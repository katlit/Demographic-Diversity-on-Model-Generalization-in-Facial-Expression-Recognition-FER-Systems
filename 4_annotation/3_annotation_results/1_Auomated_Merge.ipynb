{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08ad8688",
   "metadata": {},
   "source": [
    "# Load Automated Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1eeca19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded AffectNet - Test_face_analysis_results_anger with shape (1718, 5)\n",
      "Loaded AffectNet - Test_face_analysis_results_contempt with shape (1312, 5)\n",
      "Loaded AffectNet - Test_face_analysis_results_disgust with shape (1248, 5)\n",
      "Loaded AffectNet - Test_face_analysis_results_fear with shape (1664, 5)\n",
      "Loaded AffectNet - Test_face_analysis_results_happy with shape (2704, 5)\n",
      "Loaded AffectNet - Test_face_analysis_results_neutral with shape (2368, 5)\n",
      "Loaded AffectNet - Test_face_analysis_results_sad with shape (1584, 5)\n",
      "Loaded AffectNet - Test_face_analysis_results_surprise with shape (1920, 5)\n",
      "Loaded CK_face_analysis_results_anger_ck with shape (135, 5)\n",
      "Loaded CK_face_analysis_results_contempt_ck with shape (54, 5)\n",
      "Loaded CK_face_analysis_results_disgust_ck with shape (177, 5)\n",
      "Loaded CK_face_analysis_results_fear_ck with shape (75, 5)\n",
      "Loaded CK_face_analysis_results_happy_ck with shape (207, 5)\n",
      "Loaded CK_face_analysis_results_sadness_ck with shape (84, 5)\n",
      "Loaded CK_face_analysis_results_surprise_ck with shape (249, 5)\n",
      "Loaded FER2013_face_analysis_results_angry_fer with shape (958, 5)\n",
      "Loaded FER2013_face_analysis_results_disgust_fer with shape (111, 5)\n",
      "Loaded FER2013_face_analysis_results_fear_fer with shape (1024, 5)\n",
      "Loaded FER2013_face_analysis_results_happy_fer with shape (1774, 5)\n",
      "Loaded FER2013_face_analysis_results_neutral_fer with shape (1233, 5)\n",
      "Loaded FER2013_face_analysis_results_sad_fer with shape (1247, 5)\n",
      "Loaded FER2013_face_analysis_results_surprise_fer with shape (831, 5)\n",
      "Loaded JAFFE_face_analysis_results__jaffe with shape (213, 5)\n"
     ]
    }
   ],
   "source": [
    " import os\n",
    "import pandas as pd\n",
    "\n",
    "# Dictionary to store DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Root path where the results are stored\n",
    "root_path = \"../2_automated/results\"\n",
    "\n",
    "# Walk through each subfolder and file in the root path\n",
    "for subdir, _, files in os.walk(root_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            # Create a unique name for each DataFrame based on subfolder and file name\n",
    "            relative_path = os.path.relpath(subdir, root_path)  # Get relative path for subfolder\n",
    "            df_name = f\"{relative_path}_{file.replace('.csv', '')}\".replace(os.sep, \"_\")\n",
    "            \n",
    "            # Read the CSV file into a DataFrame\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            dataframes[df_name] = pd.read_csv(file_path)\n",
    "            \n",
    "            print(f\"Loaded {df_name} with shape {dataframes[df_name].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a88a43d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframes[\"JAFFE_face_analysis_results__jaffe\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b93af4",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f73366c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datasetName</th>\n",
       "      <th>folderName</th>\n",
       "      <th>Image</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AffectNet - Test</td>\n",
       "      <td>anger</td>\n",
       "      <td>image0000006</td>\n",
       "      <td>30</td>\n",
       "      <td>Woman</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AffectNet - Test</td>\n",
       "      <td>anger</td>\n",
       "      <td>image0000060</td>\n",
       "      <td>34</td>\n",
       "      <td>Man</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AffectNet - Test</td>\n",
       "      <td>anger</td>\n",
       "      <td>image0000061</td>\n",
       "      <td>44</td>\n",
       "      <td>Man</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AffectNet - Test</td>\n",
       "      <td>anger</td>\n",
       "      <td>image0000066</td>\n",
       "      <td>49</td>\n",
       "      <td>Man</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AffectNet - Test</td>\n",
       "      <td>anger</td>\n",
       "      <td>image0000106</td>\n",
       "      <td>30</td>\n",
       "      <td>Man</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        datasetName folderName         Image  Age Gender   Race\n",
       "0  AffectNet - Test      anger  image0000006   30  Woman  white\n",
       "1  AffectNet - Test      anger  image0000060   34    Man  white\n",
       "2  AffectNet - Test      anger  image0000061   44    Man  white\n",
       "3  AffectNet - Test      anger  image0000066   49    Man  white\n",
       "4  AffectNet - Test      anger  image0000106   30    Man  black"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Create a list to hold modified DataFrames\n",
    "modified_dfs = []\n",
    "\n",
    "# Step 2: Process each DataFrame in the dictionary\n",
    "for df_name, df in dataframes.items():\n",
    "    # Split the 'Image' column into 'folderName' and 'imageName'\n",
    "    split_image = df['Image'].str.split('_-_', n=1, expand=True)\n",
    "    df['folderName'] = split_image[0]\n",
    "    df['imageName'] = split_image[1].str.rsplit('.', n=1).str[0]  # Remove file extensions from imageName\n",
    "\n",
    "    # Extract the dataset name from the folderName (first part of the folder name)\n",
    "    df['datasetName'] = df_name.split(\"_\")[0]  # Take the prefix as the dataset name\n",
    "    \n",
    "    # Append the modified DataFrame to the list\n",
    "    modified_dfs.append(df)\n",
    "\n",
    "# Step 3: Concatenate all modified DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(modified_dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "# Rearrange columns in the desired order\n",
    "desired_column_order = ['datasetName', 'folderName', 'imageName', 'Age', 'Gender', 'Race']       \n",
    "combined_df = combined_df[desired_column_order]\n",
    "combined_df = combined_df.rename(columns={'imageName': 'Image'})\n",
    "\n",
    "\n",
    "combined_file_path = 'AutomatedAnnotations_ALL.csv'\n",
    "combined_df.to_csv(combined_file_path, index=False)\n",
    "\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a1a33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age Cohen's Kappa: 0.23\n",
      "Gender Cohen's Kappa: 0.40\n",
      "Ethnicity Cohen's Kappa: 0.68\n"
     ]
    }
   ],
   "source": [
    "# Replace NaN or None with 'Unknown' for all relevant columns\n",
    "df['Final_Age'] = df['Final_Age'].fillna('Unknown')\n",
    "df['Age'] = df['Age'].fillna('Unknown')\n",
    "df['Final_Gender'] = df['Final_Gender'].fillna('Unknown')\n",
    "df['Gender'] = df['Gender'].fillna('Unknown')\n",
    "df['Final_Ethnicity'] = df['Final_Ethnicity'].fillna('Unknown')\n",
    "df['Ethnicity'] = df['Ethnicity'].fillna('Unknown')\n",
    "\n",
    "\n",
    "# Compute Cohen's Kappa for each attribute\n",
    "age_kappa = cohen_kappa_score(df['Final_Age'], df['Age'])\n",
    "gender_kappa = cohen_kappa_score(df['Final_Gender'], df['Gender'])\n",
    "ethnicity_kappa = cohen_kappa_score(df['Final_Ethnicity'], df['Ethnicity'])\n",
    "\n",
    "# Print results\n",
    "print(f\"Age Cohen's Kappa: {age_kappa:.2f}\")\n",
    "print(f\"Gender Cohen's Kappa: {gender_kappa:.2f}\")\n",
    "print(f\"Ethnicity Cohen's Kappa: {ethnicity_kappa:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e87914",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292ea10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0c3571",
   "metadata": {},
   "source": [
    "# 22890 all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038dde5d",
   "metadata": {},
   "source": [
    "### AffectNet Test: 14,518 images\n",
    "\n",
    "diverse all age, gender and ethnicity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d9f516",
   "metadata": {},
   "source": [
    "### FER2013 Test: 7,178 images - Black and white (grayscale) images only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aea9992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af8bacfa",
   "metadata": {},
   "source": [
    "### CK (all images included): 981 images - Black and white (grayscale) images only.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b635fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4135b6dd",
   "metadata": {},
   "source": [
    "### JAFFE (all images) 213 images\n",
    "\n",
    "The JAFFE dataset was collected for controlled expression analysis rather than demographic diversity, so it lacks explicit demographic variability in age and ethnicity. This is a limitation when using JAFFE for tasks requiring diverse demographic information.\n",
    "\n",
    "Age: The JAFFE dataset generally includes images of adult women, but specific ages are not provided within the dataset itself. Itâ€™s often assumed that the subjects are adults, typically ranging from young to middle-aged.\n",
    "\n",
    "Gender: All individuals in the JAFFE dataset are female, as the dataset name itself specifies (\"Japanese Female Facial Expression\").\n",
    "\n",
    "Ethnicity: All subjects in the JAFFE dataset are of Japanese ethnicity, making the dataset homogeneous in terms of ethnicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c462ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1be041b",
   "metadata": {},
   "source": [
    "### Why Grayscale Images Are Unsuitable for Age, Gender, and Ethnicity Annotation\n",
    "\n",
    "Annotating age, gender, and ethnicity from grayscale (black and white) images is challenging and often unreliable due to the lack of color information. Color plays a crucial role in accurately assessing demographic attributes:\n",
    "\n",
    "Age Estimation: Age-related features like skin texture, wrinkles, and hair color are harder to discern without color. Grayscale images obscure these details, making it difficult to differentiate between similar age groups accurately.\n",
    "\n",
    "Gender Identification: Indicators such as makeup, lip color, and hair color, which can subtly signal gender, are lost in grayscale, reducing the accuracy of gender annotations.\n",
    "\n",
    "Ethnicity Identification: Ethnicity often relies on skin tone and undertones, which are absent in grayscale images. This makes it nearly impossible to distinguish between certain ethnic groups, leading to unreliable results.\n",
    "\n",
    "In summary, grayscale images lack the necessary detail to make accurate annotations of age, gender, and ethnicity, and attempting to do so may introduce significant bias and error into the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf82e614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
