links,texts
https://ieeexplore.ieee.org/document/4563052,"3D Facial Expression Recognition Based on Automatically Selected Features
Hao Tang and Thomas S. Huang
University of Illinois at Urbana-Champaign, Urbana, IL 61801
fhaotang2,huangg@ifp.uiuc.edu
Abstract
In this paper, the problem of person-independent fa-
cial expression recognition from 3D facial shapes is inves-
tigated. We propose a novel automatic feature selection
method based on maximizing the average relative entropy
of marginalized class-conditional feature distributions and
apply it to a complete pool of candidate features composed
of normalized Euclidean distances between 83 facial fea-
ture points in the 3D space. Using a regularized multi-class
AdaBoost classication algorithm, we achieve a 95.1% av-
erage recognition rate for six universal facial expressions
on the publicly available 3D facial expression database BU-
3DFE [1], with a highest average recognition rate of 99.2%
for the recognition of surprise. We compare these results
with the results based on a set of manually devised features
and demonstrate that the auto features yield better results
than the manual features. Our results outperform the re-
sults presented in the previous work [2] and [3], namely
average recognition rates of 83.6% and 91.3% on the same
database, respectively.
1. Introduction
3D facial shapes, which are dened by the 3D geometry
of the faces of human beings, contain important information
about human's facial expressions. Such information is in-
variant to pose and lighting conditions, which have imposed
serious hurdles on many 2D facial analysis tasks such as
2D facial expression recognition. In light of this, the explo-
ration and exploitation of 3D facial geometric information
in tackling various facial analysis problems have emerged
in the research community. Thanks to the fast-developing
and increasingly mature 3D range scanning products and
computer vision based techniques for 3D face reconstruc-
tion from one or more 2D images, the 3D facial geometric
information has been relatively extensively utilized in the
study of face recognition [4, 5]. However, unlike in the
case of face recognition, far fewer work has been done on
facial expression recognition which takes advantage of the
3D facial geometric information. The most obvious reasonfor this embarrassing situation is that there has long been a
lack of a publicly available 3D facial expression database to
serve as a common platform for the researchers in the eld.
Without such a database, neither can the researchers start
with problem investigation nor can they end in comparing
their results. Now the good news is, Yin et. al. at Bing-
hamton University have recently constructed a 3D facial
expression database for facial behavior research and made
it publicly available [1]. This is denitely the rst attempt
to make a publicly available 3D facial expression database
for the research community. It is believed that the existence
of this database will have a great impact on the facial
behavior relevant research in the next a few years to come.
Wang et. al. [2] performed the very rst work of 3D
facial expression recognition on the BU-3DFE database.
They reported that the highest average recognition rate
they had obtained on this database was 83.6% by using
elaborately extracted primitive facial surface features
and an LDA classier. They also reported that the facial
expressions of happiness and surprise were well identied
with accuracies of 95% and 90.8%, respectively. They
compared their results with the results obtained by two
2D appearance feature based methods, namely the well-
known Gabor-wavelet (GW) approach and the Topographic
Context (TC) approach. In both cases, they showed that
their results outperformed those for the 2D methods. They
demonstrated that by incorporating 3D facial geometric
information the facial expression recognition problem
can be better solved. This is highly expectable because
3D facial shapes contain rich information about facial
expressions and are more robust than 2D appearances.
Soyel and Demirel [3] also conducted 3D facial expres-
sion recognition work on the BU-3DFE database. They
applied a carefully tuned neural network architecture to ve
characteristic facial feature distances that represent the eye
opening, eyebrow height, mouth opening, mouth height,
and lip stretching. They reported that the average recogni-
tion rate of their system was 91.3% and the highest average
recognition rate reached to 98.3% in the recognition of
surprise. As of the writing of this paper, to the best of our
knowledge, this is the only work that claims better results
978-1-4244-2340-8/08/$25.00 ¬©2008 IEEE
Figure 1. Some examples in the BU-3DFE database showing six universal facial expressions with four levels of intensities.
than Wang et. al.'s work on the same database.
In this paper, we further investigate the problem of
person-independent facial expression recognition from 3D
facial shapes based on the BU-3DFE database. We propose
a novel automatic feature selection method based on max-
imizing the average relative entropy of marginalized class-
conditional feature distributions and apply it to a complete
pool of candidate features composed of normalized Eu-
clidean distances between 83 facial feature points in the 3D
space. Using a regularized multi-class AdaBoost classica-
tion algorithm with three different weak classiers, namely
Nearest Neighbor (NN), Naive Bayes (NB), and LDA, we
achieve a 95.1% average recognition rate for six universal
facial expressions, namely anger, disgust, fear, happiness,
sadness, and surprise. The highest average recognition rate
that we obtain is 99.2% for the recognition of surprise. We
compare these results with the results based on a set of man-
ually devised features and demonstrate that the auto features
yield better results than the manual features. In addition, we
show that our results outperform the results presented in the
previous work [2] and [3], namely average recognition rates
of 83.6% and 91.3% on the same database, respectively.
2. Database description
The BU-3DFE database was recently developed by Yin
et. al. at Binghamton University. It was designed to sample
3D facial behaviors with different prototypical emotional
states at various levels of intensities. There are a total of
100 subjects in the database. Among these subjects, 56
are female and 44 are male. The subjects are well dis-tributed across different ethnic or racial ancestries, includ-
ing White, Black, East-Asian, Middle-East Asian, Hispanic
Latino, and others. While being recorded, each subject was
asked to perform the neutral facial expression as well as
six universal facial expressions, namely anger (AN), dis-
gust (DI), fear (FE), happiness (HA), sadness (SA), and sur-
prise (SU). Each facial expression has four levels of inten-
sities (low, middle, high, highest or 01-04), except that the
neutral facial expression has only one intensity level (00).
Thus, there are 25 3D facial expression models for each
subject, resulting in 2500 3D facial expression models in
the database.
Associated with a 3D facial expression model are a raw
3D face mesh model, a cropped 3D face mesh model, a pair
of texture images with two-angle views (about +45oand -
45oaway from the face frontal normal), a frontal-view tex-
ture image, a set of 83 facial feature points, and a facial pose
vector. These data give a complete 3D description of a face
under a specic facial expression. A detailed description
of the BU-3DFE database can be found in [1]. In this pa-
per, we only use the 83 facial feature points marked on the
cropped 3D face mesh model, as shown in Figure 2 (a). We
refer to a cropped 3D face mesh model as a 3D facial ex-
pression model. Some examples of the 3D facial expression
models in the BU-3DFE database are illustrated in Figure 1.
3. Feature extraction
In this paper, we manually devise a set of features (here-
after referred to as the manual features, as compared to the
automatically selected features, namely the auto features)
based on the normalized Euclidean distances between the
facial feature points on the 3D facial expression models.
According to Ekman [6, 7], the six universal facial expres-
sions are coded by combinations of action units, which are
caused by the movements of particular facial feature points.
Similarly, the MPEG-4 standard [8] species that the six
universal facial expressions correspond to six unique fa-
cial congurations, which can be obtained by adjusting the
weights of certain facial animation parameters (FAPs). The
FAPs will change the relative position of the facial feature
points, resulting in distinct collocation patterns of facial fea-
ture points. In light of this, we believe that the distances be-
tween certain facial feature points offer a sparse, compact,
yet information-rich representation of the 3D facial shape.Such representation is invariant to translations and rotations
of the 3D facial geometry.
We extract a set of 24 features composed of the nor-
malized distances between a subset of the 83 facial feature
points. These distances are believed to play an important
role in determining the various facial expressions. The 24
features are shown in Figure 2 (b) and their textual descrip-
tions are given in Table 1.
Since the faces of different people tend to have differ-
ent sizes and proportions, in order to make the features
person-independent, we normalize the distances by facial
animation parameter units (FAPUs), as guided by theMPEG-4 standard. In MPEG-4, the FAPUs serve to scale
the FAPs in order that a single set of FAPs can be used with
arbitrary face models. The FAPUs are dened as fractions
of distances between certain feature points on a face model
in its neutral state and allow us to interpret the FAPs on
arbitrary face models in a consistent way. The ve FAPUs
are shown in Figure 3. The distances are then divided by
the corresponding FAPUs, given as the unit in Table 1.
4. Automatic feature selection
In a typical pattern classication system, the features
are generally devised by experts from the domain of the
problem. While expert-derived features are often compact
and useful, to derive them requires domain knowledge and
many trials and errors. An attractive scheme is that we have
a large pool of candidate features, which are relatively easy
to collect, and we want a computer algorithm to be able to
automatically select the ¬ìbest¬î features from this candidate
feature pool.
In order to perform automatic feature selection, one has
to tell the computer algorithm in what sense a feature is
considered ¬ìthe best¬î. Since our goal is to classify patterns,
the discrimination power comes in naturally. In general, the
more discrimination power a feature possesses, the more
it will contribute to the classication accuracy. Thus, intu-
itively and naturally, we want an automatic feature selection
algorithm to select from the candidate pool those features
11
13
1214
23246
41 2
7 8
15169101920212218 173
5
(a) (b)
Figure 2. (a) 83 facial feature points marked on the 3D facial ex-
pression model displayed in the texture mode. (b) 24 manually
devised features dened by the normalized Euclidean distances
between certain facial feature points on the 3D facial expression
model displayed in the shade mode.
Figure 3. A face model in its neutral state on which the FAPUs are
dened by the fractions of distances between the marked key facial
feature points. The ve FAPUs are IRISD0 - Iris diameter, ES0 -
Eye separation, ENS0 - Eye-nose separation, MNS0 - Mouth-nose
separation, and MW0 - Mouth width, respectively [8].
which have the highest discrimination power, denoted by
DP(x), where xis a feature set or a feature vector.
From a Bayesian point of view, the discrimination power
of a feature vector DP(x)depends on how much discrimi-
native information lies in the class-conditional distributions
ofx. The more differences that the class-conditional distri-
butions exhibit, the more discriminative information there
is in x. A good ¬ìmetric¬î that measures the differences be-
tween two probability distributions is the relative entropy, or
Kullback-Leibler divergence (KLD) [9]. Given two proba-
Feature Textual description Unit
1 the length of the right eyebrow ES0
2 the length of the left eyebrow ES0
3 the distance between the left and right eyebrows ES0
4 the distance between the left and right eyes ES0
5 the distance between the right inner eyebrow and the right inner eye corner ENS0
6 the distance between the left inner eyebrow and the left inner eye corner ENS0
7 the distance between the right outer eyebrow and the right outer eye corner ENS0
8 the distance between the left outer eyebrow and the left outer eye corner ENS0
9 the distance between the right inner eye corner and the right nose wing ES0
10 the distance between the left inner eye corner and the left nose wing ES0
11 the width of the right eye ES0
12 the width of the left eye ES0
13 the height of the right eye IRISD0
14 the height of the left eye IRISD0
15 the distance between right outer eye corner and the right outer mouth corner ENS0
16 the distance between the left outer eye corner and the left outer mouth corner ENS0
17 the distance between the right outer mouth corner and the right jaw root MNS0
18 the distance between the left outer mouth corner and the left jaw root MNS0
19 the distance between the right nostril and the upper mid lip MNS0
20 the distance between the left nostril and the upper mid lip MNS0
21 the distance between the right nostril and the chin MNS0
22 the distance between the left nostril and the chin MNS0
23 the width of the mouth MW0
24 the height of the mouth MNS0
Table 1. The normalized Euclidean distances between certain facial feature points on the 3D facial expression model comprise 24 manual
features. The units are given by the MPEG-4 facial animation parameter units (FAPUs).
bility distributions f(x) and g(x), the KLD is given by
D(f(x)kg(x)) =Z
xf(x)logf(x)
g(x)(1)
The properties of the KLD states that D0andD= 0
if and only if f(x) =g(x). In addition, the KLD is not
symmetric: D(f(x)kg(x))6=D(f(x)kg(x)). A commonly
used symmetric metric is given by
Ds(f(x)kg(x)) =1
2(D(f(x)kg(x)) +D(g(x)kf(x)))
(2)
Letx= [x 1;x2;:::;x D]Tdenote a set of Dcandidate
features. We rst linearly transform xsuch that
y=U(x m) (3)
where mis the sample mean vector of x,U=
[u1;u2;:::;uK]Tis an orthogonal matrix with each row be-
ing an eigenvector of the sample covariance matrix of x,
andK=min(N;D )withNbeing the number of train-
ing examples. By Equation 3, the components of xare
decorrelated, resulting in a new feature vector ywith sta-
tistically independent components y1;y2;:::;y Kunder thejointly Gaussian assumption. Suppose there are Cclasses.
The discrimination power of the feature set yis given by
DP(y) =CX
i=1CX
j=i+1Ds(fi(y)kfj(y))
=1
2CX
i=1CX
j=1D(fi(y)kfj(y)) (4)
where fi(y)andfj(y)are the class-conditional probability
distributions of the feature vector y, or the class-conditional
joint probability distributions of the features y1;y2;:::;y K.
Sincey1;y2;:::;y Kare statistically independent, we have
D(fi(y)kfj(y))
=Z
y1;y2;:::;y KKY
k=1fi(yk)logQK
k=1fi(yk)
QK
k=1fj(yk)dy1;y2;:::;y K
=KX
k=1Z
ykfi(yk)logfi(yk)
fj(yk)dyk
=KX
k=1D(fi(yk)kfj(yk)) (5)
Equation 5 indicates that given statistically independent
features, the KLD between the class-conditional joint dis-
tributions is equal to the sum of the class-conditional distri-
butions marginalized over all but one feature. Thus, from
Equation 4, we have
DP(y) =1
2CX
i=1CX
j=1KX
k=1D(fi(yk)kfj(yk))
=KX
k=1DP(yk) (6)
Equation 6 states that the discrimination power of a set
of independent features yis the sum of the discrimination
power of the individual features y1;y2;:::;y K. The intu-
ition of this statement is that once we are given a new, in-
dependent feature, we at least gain some new information
in discriminating between the classes. However, due to the
lack of sufcient training data, we cannot keep all the fea-
tures, even if the total discrimination power increases, as
deterred by the curse of dimensionality. From the additive
relationship in Equation 6, one reasonable way of doing fea-
ture selection is to keep only those features that have the
highest discrimination power and that contribute the most
to the total discrimination power of the selected feature set.
Thus, one can rst sort the features y1;y2;:::;y Kby their
discrimination power DP(yk)in the descending order and
then keep only the rst K0< K features. As we will see
in the experiment section, the discrimination power of the
features drops rather sharply, which means keeping only a
small number of the features would be sufcient.
Assuming that the class-conditional marginal feature dis-
tributions are Gaussian, namely
fi(yk) =1p
2ie (yk i)2
22
i i= 1;2;:::;C (7)
where iandiare the mean and standard deviation of the
classi. The discrimination power of the feature ykis then
DP(yk) =1
2CX
i=1CX
j=1D(fi(yk)kfj(yk))
=1
2CX
i=1CX
j=1flogj
i+2
i
22
j+(i j)2
22
j 1
2g(8)
5. Regularized AdaBoost classication
The AdaBoost algorithm, originally proposed by Yoav
Freund and Robert Schapire [10], is a proven, effective clas-
sication algorithm. AdaBoost calls a weak classier re-
peatedly in t= 1;2;:::;T rounds while at each round a
different distribution over the training examples is providedAlgorithm 1 AdaBoost for binary classication.
1:Input: Ntraining examples fxi;yig, where xi2X,
yi2 f 1; 1g,i= 1;2;:::;N ; a weak classier Cweak ;
the number of iterations T.
2:Initialize D1(i) =1
N,i= 1;2;:::;N .
3:Fort= 1;2;:::;T , do
callCweak to generate a hypothesis ht:X!
f 1;1gthat minimizes the training error with re-
spect to the distribution Dt:ht= argmin
hj2Hj
where j=PN
i=1Dt(i)[y i6=hj(xi)].
ift0:5then stop.
sett=1
2log1  t
t.
update Dt+1(i) =1
ZtDt(i)e tyiht(xi),i=
1;2;:::;N , where Ztis a normalization factor that
makes Dt+1a valid probability distribution.
4:Output: H(x) = sign(PT
t=1tht(x)).
to the weak classier. This distribution is updated to in-
dicate the importance of the examples in the training data
set for classication. That is, at each round, the weights
of every incorrectly classied examples are increased while
the weights of every correctly classied examples are de-
creased. After Trounds, the hypotheses of each round are
weighted and summed to produce a nal strong classier.
These procedures are presented in Algorithm 1.
The AdaBoost algorithm presented in Algorithm 1 aims
to solve a binary classication problem. It can be readily
extended to solve multi-class classication problems [10,
11] by slightly changing the output nal hypothesis to
H(x) = argmax
y2YTX
t=1Dt(i)[y=ht(x)] (9)
where Y=fy1;y2;:::;y Cgrepresent the labels of the C
classes.
The basic idea of the AdaBoost algorithm is that it tries
to focus more on the difcult training examples than theeasy ones. At each round, the distribution over the training
examples is updated in a way such that the weak classier is
tweaked in favor of those examples misclassied at the pre-
vious round in the hope that the misclassication error can
be reduced overall. While being known that it is less sus-
pectable to the overtting problem, the AdaBoost algorithmis sensitive to noise and outliers. Since the distribution is
adjusted at each round to give more weights to the difcult
examples, this distribution tends to become sharply peaked
at the outliers if they exist (as they are hard to be classied).
Such a distribution would wrongly guide the weak classier
Algorithm 2 Regularized multi-class AdaBoost.
1:Input: Ntraining examples fxi;yig, where xi2X,
yi2 fy1;y2;:::;y Cg,i= 1;2;:::;N ; a weak classier
Cweak ; the number of iterations T; constants mandb.
2:Initialize D1(i) =1
N,i= 1;2;:::;N .
3:Initialize E(i) = 0, i= 1;2;:::;N .
4:Fort= 1;2;:::;T , do
callCweak to generate a hypothesis ht:X!
fy1;y2;:::;y Cgthat minimizes the training er-
ror with respect to the distribution Dt:ht=
argmin
hj2Hjwhere j=PN
i=1Dt(i)[y i6=hj(xi)].
ift0:5then stop.
setE(i) = [E (i) + 1][y i6=ht(xi)],i=
1;2;:::;N .
sett=1
2log1  t
t.
update
Dt+1(i) =Dt(i)
Zt8
<
:e tifyi=ht(xi)
0 ifE(i)m; D t(i)> b
et ifyi6=ht(xi)
i= 1;2;:::;N , where Ztis a normalization factor
that makes Dt+1a valid probability distribution.
5:Output: H(x) = argmax
y2YPT
t=1Dt(i)[y=ht(x)].
to focus on the outliers. It would be advantageous to discard
the outliers and force the weak classier to focus on the
meaningful data. To this end, we regularize the AdaBoost
algorithm in the following way: 1. If a training example is
incorrectly classied for a consecutive mrounds, then we
consider it as an outlier. 2. If the current weight of this out-
lier exceeds a limit b(upper bound), we decide to discard
this outlier by setting its weight to zero. The regularized
AdaBoost algorithm is given in Algorithm 2.
6. Experiments
We conducted extensive experiments on the BU-3DFE
database using the same setup as that in the previous work
[2] and [3]. In our experiments, we used the data of 60
subjects with two high-intensity models for each of the six
universal facial expressions. For the purpose of person and
gender independency, half (30) of the 60 subjects were fe-
male and half (30) were male. The subjects we chose were
well distributed across the various races, as illustrated in
Figure 1. We randomly divided the 60 subjects into two
sets. The training set contained 54 subjects and the testset contained 6 subjects. The features we used were solely
based on the 83 facial feature points given for every 3D
facial expression model in the database. The experiments
were carried out in the follow phases.
First, we extracted the manual features for each 3D facial
expression model as described in Section 3. Each facial ex-
pression model was then represented by a 24 dimensional
feature vector. In this work, the neutral facial expression
is not classied. Rather, as a preprocessing step its fea-
tures serve as ducial measures that are subtracted from the
features of the six universal facial expressions of the corre-
sponding subject. We applied the regularized AdaBoost al-
gorithm (Algorithm 2) with three weak classiers, namely
Nearest Neighbor (NN), Naive Bayes (NB), and LDA, to
the manual features, and achieved average recognition rates
of 93.6%, 93.8%, and 91.8%, respectively. Note that these
numbers were obtained by averaging the results of 10 in-
dependent experiments run on 10 random partitions of the
training and test data sets. We believe that they represent
a generalization of the recognition rates. The average con-
fusion matrices of the experiments are given in Tables 2, 3,
4, respectively. From these confusion matrices, the high-
est average recognition rates in each of the experiments are
98.3%, 97.5%, and 95%, respectively, all for the recognition
of surprise.
% AN DI FE HA SA SU
AN 86.7 3.3 2.5 4.2 1.7 1.7
DI 0.8 94.2 1.7 0 3.3 0
FE 2.5 3.3 88.3 3.3 1.7 0.8
HA 0 0.8 0 98.3 0 0.8
SA 0 1.7 1.7 0.8 95.8 0
SU 0 0.8 0 0 0.8 98.3
Table 2. Average confusion matrix (manual features, regularized
AdaBoost with a nearest neighbor weak classier).
% AN DI FE HA SA SU
AN 91.7 1.7 2.5 0 1.7 2.5
DI 1.7 90 3.3 0.8 2.5 1.7
FE 4.2 3.3 75.8 7.5 3.3 5.8
HA 0.8 0.8 2.5 90.8 2.5 2.5
SA 0 4.2 8.3 5 80 2.5
SU 1.7 0 0.8 0 0 97.5
Table 3. Average confusion matrix (manual features, regularized
AdaBoost with a naive Bayes weak classier).
Next, we applied our automatic feature selection method
to a complete pool of normalized distances of facial fea-
ture points. The 83 facial feature points on a 3D facial ex-
pression model produced C2
83unique pairs of facial feature
points and the distance of each pair was rst normalized
% AN DI FE HA SA SU
AN 80.8 7.5 6.7 2.5 2.5 0
DI 0.8 90.8 3.3 2.5 1.7 0.8
FE 2.5 5 80.8 5 5.8 0.8
HA 0 5 1.7 89.2 2.5 1.7
SA 4.2 3.3 4.2 5 81.7 1.7
SU 1.7 1.7 0 0.8 0.8 95
Table 4. Average confusion matrix (manual features, regularized
AdaBoost with an LDA weak classier).
0 100 200 300 400 500 600 700051015202530
Featur eDiscrimination power
Figure 4. The discrimination power (DP) of the individual features
sorted in descending order. The DP curve drops sharply.
by the distance between two outer eye corners of the same
3D facial expression model in order to make the features
scale-invariant. After the same ducial subtraction as done
for the manual features, the C2
83normalized distances com-
prised a candidate feature pool, from which the automatic
feature selection method selected the features with the high-
est discrimination power. We performed principal compo-
nent analysis (PCA) on the training data set. While the orig-inal feature dimension was C
2
83, since the training data set
only contained 5426 = 648 examples, there were only
648 eigenvectors of the sample covariance matrix associ-ated with non-zero eigenvalues. The auto features were then
selected from the 648 PCA transformed features. Figure 4
shows the discrimination power of the transformed features,
sorted in the descending order. We observe that the discrim-
ination power of the features drops rather sharply. It is rea-
sonable to keep only a few features with dramatically large
discrimination power while discarding the rest. Empirically,
10 to 30 features are sufcient to yield good classication
results. The class-conditional marginal distributions of the
four top features with the highest discrimination power are
shown in Figure 5.
Finally, based on the auto features, we applied the regu-
larized AdaBoost algorithm with the same three weak clas-
siers (NN, NB, LDA) and achieved average recognition
rates of 94.8%, 90.8%, and 95.1%, respectively. The corre-
sponding average confusion matrices are given in Tables 5,
Figure 5. The class-conditional distributions of the four top fea-
tures with the largest discrimination power: 27.74, 20.46, 8.35,
and 7.89. In each subplot, the green dots are the data points, the
blue bars the histogram, and the red curve the tted Gaussian.
6, 7, respectively. A typical run of the regularized AdaBoost
algorithm is shown in Figure 6.
% AN DI FE HA SA SU
AN 94.2 1.7 2.5 0 0.8 0.8
DI 1.7 94.2 0.8 0.8 2.5 0
FE 0.8 3.3 87.5 4.2 0 4.2
HA 0 0 1.7 98.3 0 0
SA 0 1.7 3.3 0 95 0
SU 0 0.8 0 0 0 99.2
Table 5. Average confusion matrix (auto features, regularized Ad-
aBoost with a nearest neighbor weak classier).
% AN DI FE HA SA SU
AN 80 10 2.5 3.3 4.2 0
DI 2.5 89.2 3.3 2.5 1.7 0.8
FE 6.7 9.2 73.3 5.8 2.5 2.5
HA 2.5 2.5 1.7 88.3 3.3 1.7
SA 0.8 4.2 3.3 1.7 89.2 0.8
SU 0.8 1.7 2.5 0.8 0.8 93.3
Table 6. Average confusion matrix (auto features, regularized Ad-
aBoost with a naive Bayes weak classier).
A comparison of our results and the results reported in
the previous work [2] and [3] is shown in Figure 7. On
average, our results on the auto features are better than our
% AN DI FE HA SA SU
AN 85.8 7.5 3.3 2.5 0 0.8
DI 5 86.7 0.8 3.3 2.5 1.7
FE 1.7 5 79.2 3.3 8.3 2.5
HA 3.3 0.8 1.7 94.2 0 0
SA 4.2 0 2.5 1.7 90.8 0.8
SU 0.8 0.8 0 0 0.8 97.5
Table 7. Average confusion matrix (auto features, regularized Ad-
aBoost with an LDA weak classier.
2 4 6 8 10 12 14 16 18 2000.10.20.30.40.50.60.70.80.91AdaBoost with different weak classifiers
Iteration numberRecognition rate
  
AdaBoost with Nearest Neighbor
AdaBoost with Naive Bayes
AdaBoost with LDA
Figure 6. A typical run of the regularized multi-class AdaBoost
algorithm with three weak classiers. The curves represent the
recognition rate on the test set vs. the number of training iterations.
7580859095100Average recognition rate (%)Performance comparison
  
Manual features, AdaBoost.NN
Manual features, AdaBoost.NB
Manual features, AdaBoost.LDA
Auto features, AdaBoost.NN
Auto features, AdaBoost.NB
Auto features, AdaBoost.LDA
Wang et. al.'s work
Soyel and Demirel's work
Figure 7. Comparison of the results.
results on the manual features. All our results outperform
those of the previous work, except that in the case of auto
features and regularized AdaBoost with a naive Bayes weak
classier, our result is slightly worse than that of [3].
7. Conclusion
In this paper, we investigate the problem of person-
independent facial expression recognition from 3D facial
shapes. We propose a novel automatic feature selection
method based on maximizing the average relative entropyof marginalized class-conditional feature distributions
and apply it to a complete pool of candidate features
composed of normalized Euclidean distances between 83
facial feature points in the 3D space. Using a regularized
multi-class AdaBoost classication algorithm, we achieve
a 95.1% average recognition rate for six universal facial
expressions on the publicly available 3D facial expression
database BU-3DFE, with a highest average recognition rate
of 99.2% for the recognition of surprise. We compare these
results with the results based on a set of manually devised
features and demonstrate that the auto features yield better
results than the manual features. Our results outperform the
results presented in the previous work [2] and [3], namely
average recognition rates of 83.6% and 91.3% on the same
database, respectively.
References
[1] Lijun Yin, Xiaozhou Wei, Yi Sun, Jun Wang, and Matthew
Rosato, ¬ìA 3D Facial Expression Database For Facial Behav-
ior Research¬î, 7th International Conference on Automatic
Face and Gesture Recognition (FG2006), pp. 211 - 216.
[2] Jun Wang, Lijun Yin, Xiaozhou Wei, and Yi Sun, ¬ì3D Facial
Expression Recognition Based on Primitive Surface Feature
Distribution¬î, IEEE International Conference on Computer
Vision and Pattern Recognition (CVPR 2006).
[3] Soyel, H. and Demirel, H., ¬ìFacial Expression Recognition
Using 3D Facial Feature Distances¬î, ICIAR07, pp. 831-838.
[4] Bronstein, A. M.; Bronstein, M.M, and Kimmel, R., ¬ìThree-
dimensional face recognition¬î, International Journal of Com-
puter Vision (IJCV) 64 (1): 5-30, 2005.
[5] Kakadiaris, I. A.; Passalis, G. and Toderici, G. and Mur-
tuza, N. and Karampatziakis, N. and Theoharis, T., ¬ì3D face
recognition in the presence of facial expressions: an anno-
tated deformable model approach¬î, Transactions on Pattern
Analysis and Machine Intelligence (PAMI) 13 (12), 2007.
[6] P. Ekman and W. Friesen, ¬ìFacial Action Coding System: A
Technique for the Measurement of Facial Movement¬î, Con-
sulting Psychologists Press, Palo Alto, 1978.
[7] Ekman, P., Huang, T., Sejnowski, T., and Hager, J., ¬ìFinal
Report to NSF of the Planning Workshop on Facial Expres-
sion Understanding¬î, Available from HIL-0984, UCSF, San
Francisco, CA 94143.
[8] Igor S. Pandzic, Robert Forchheimer (Eds), MPEG-4 Facial
Animation: The Standard, Implementation and Applications,
John Wiley & Sons, Inc., 2002.
[9] S. Kullback, ¬ìThe Kullback-Leibler distance¬î, The Ameri-
can Statistician 41:340-341, 1987.
[10] Y . Freund and R. Schapire, ¬ìA decision-theoretic general-
ization of on-line learning and an application to boosting¬î,
Journal of Computer and System Sciences, 55(1):119¬ñ139,
August, 1997.
[11] R. Schapire and Y . Singer, ¬ìImproved Boosting Algorithms
Using Condence-rated Predictions¬î, Machine Learning
37(3): 297-336, December, 1999.
"
https://ieeexplore.ieee.org/document/1640921,"3D Facial Expression Recognition Based on Primitive Surface Feature
Distribution
Jun Wang, Lijun Yin, Xiaozhou Wei and Yi Sun
Department of Computer Science
State University of New Y ork at Binghamton, NY , 13902, USA
Abstract
The creation of facial range models by 3D imaging sys-
tems has led to extensive work on 3D face recognition [19].
However, little work has been done to study the usefulness
of such data for recognizing and understanding facial ex-pressions. Psychological research shows that the shape of ahuman face, a highly mobile facial surface, is critical to fa-cial expression perception. In this paper, we investigate theimportance and usefulness of 3D facial geometric shapes to
represent and recognize facial expressions using 3D facial
expression range data. We propose a novel approach to ex-
tract primitive 3D facial expression features, and then apply
the feature distribution to classify the prototypic facial ex-
pressions. In order to validate our proposed approach, we
have conducted experiments for person-independent facial
expression recognition using our newly created 3D facial
expression database. We also demonstrate the advantages
of our 3D geometric based approach over 2D texture based
approaches in terms of various head poses.
1. Introduction
There is a long history of interest in the problem of rec-
ognizing human emotion from facial expressions, as well
as extensive studies on face perception over the last threedecades [8, 17]. Analyzing the emotional expression of ahuman face requires a number of preprocessing steps whichattempt to detect and locate characteristic facial regions, ex-
tract facial expression features, and model facial gestures
using anatomic information about the face. Although all
these steps are equally important, current research mostlyconcentrates on the facial expression feature detection anddescription, which is also the focus of this paper.
Facial expression features are mainly represented by
three categories: (1) static versus dynamic (or temporal);(2) global versus local (or analytic); and (3) 2D versus
3D. Most research over the past thirty years has been di-
rected towards static/dynamic, analytic, 2D feature extrac-tion [9], focusing primarily on two types of features: 2D
geometric features and appearance features from 2D static
images [16, 18] or video sequences [4, 3, 31]. Geomet-ric features are described by a set of facial feature pointsused to derive facial organs‚Äô shapes or expressive regions.Appearance features refer to the features exhibited on the
skin (e.g., frowns or wrinkles) [22] or action units [8, 6].
Over the past decade, a number of techniques have been
successfully developed for facial expression recognition, in-cluding optical Ô¨Çow [28, 10] and Gabor wavelets [16], andFACS-feature based techniques [22, 1, 6, 31]. The excel-lent review of recent advances in this Ô¨Åeld can be found in
[5, 12, 17, 32]. All recent advances have been based on
2D images or videos, and most were primarily concerned
with extracting prototypic expressions from frontal or near-frontal views of a face.
Recently, additional work has been done to improve the
performance of facial expression recognition under vari-
ous imaging conditions (e.g., pose, lighting, etc). Some
researchers have successfully explored partial 3D informa-
tion for facial expression recognition, such as multiple-view
based [18] and 3D model-based techniques [2, 13, 27, 30].
These methods are still based on 2D data. Because the facial
expression actuated by the facial muscle movement resultsin the facial skin shape variation, it is ideal to model thefacial expressions explicitly in a 3D space.
Due to the limitations in describing facial surface defor-
mation when 3D features are evaluated in 2D space, 2D
images with a group of feature units may not accuratelyreÔ¨Çect complex and authentic facial expressions. More im-portantly, head pose and posture, which are precious cues inconjunction with facial action, reÔ¨Çect a person‚Äôs real emo-tion. Since people rarely express emotions without head
motion or posture spontaneity, the assumption of frontalimages of faces under good illumination is to be unreal-istic. Therefore, there is a high demand to represent and
recognize facial expressions in 3D space. In this paper, we
address the issue regarding 3D global features on the 3Df a -
cial surface in order to mitigate the problems posted by 2Dbased facial expression analysis.
Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR‚Äô06) 
0-7695-2597-0/06 $20.00 ¬© 2006  IEEE 

The3D surface features, reÔ¨Çecting the facial skin
‚Äúwave‚Äù, represent the intrinsic facial surface structure as-
sociated with the speciÔ¨Åc facial expressions. Motivated bythis fact, we propose a novel geometric feature based facialexpression descriptor in the 3-dimensional Euclidean space.
Based on the principal curvature information estimated onthe3D triangle mesh model, we apply a surface labeling
approach to classify the 3D primitive surface features into
twelve basic categories. In order to classify the speciÔ¨Åc ex-
pression, we partition the face surface into a number of ex-pressive regions, and conduct the statistics of the surface
primitive label distribution on each region separately. The
statistic histograms of the surface labels of all these regionsare combined to construct the speciÔ¨Åc facial expression fea-ture.
Finally, we validate our approach by the recognition ex-
periments on our newly constructed 3D facial expression
database. We conduct the experiments in the following
several aspects: the performance investigation of our 3D
approach; the comparison study with 2D appearance fea-
ture based methods (Gabor-Wavelet method and our Topo-graphic Context method), including the front view case and
the angle-view case. The experiments are executed in a
person-independent manner, which means that the subject
being tested has never appeared in the training set. We haveselected four classiÔ¨Åers for the classiÔ¨Åcation experiments.Since the paper focuses on 3D facial expression descrip-
tion, the classiÔ¨Åcation algorithm design is not expatiated onin this paper.
The remainder of this paper is organized as follows: In
Section 2, we introduce the surface primitive feature anal-ysis based on the computational geometry method. In Sec-tion 3, we describe the facial expression feature extractionfrom the primitive surface features distribution. Section 4reports the experimental results of the 3D facial expression
recognition, followed by a comparison with the 2D appear-
ance feature based approaches in Section 5. Finally, con-cluding remarks and discussion are given in Section 6.
2.3D Primitive Feature Analysis
The surface feature analysis is based on the triangle
meshes of faces, which are created by a 3D imaging system
[15]. The system captures a three dimensional point cloudand generates a meshed surface that models the face as 3D
triangle mesh geometry. The 3D surface data points are less
than300 microns apart, providing the capability of distin-
guishing between small differences regardless of lighting
or orientation during the original scan. Figure 1 shows an
example of 3D facial expression range models with six pro-
totypic facial expressions. In the following sub-sections, we
will give a detailed description of the primitive facial feature
estimation and labeling of the range models.
Figure 1. An example of 3D facial range mod-
els showing six prototypic expressions, from
left to right: Anger, Disgust, Fear, Happiness, Sad-
ness, and Surprise . The textured models are
shown in the upper row, and the correspond-ing shaded models are in the lower row.
2.1. Principal Curvature Analysis by Local
Surface Fitting
The shape information of a surface is ‚Äúencoded‚Äù in its
primitive geometric features (such as ridge, ravine, peak,pit, saddle, concave hill, convex hill, etc.), which may be
viewed as a digital signature of a facial expression. Thesefeatures are determined by the surface curvatures and theirprincipal directions. Curvature estimation techniques fortriangle meshes could be based on the mesh itself or al-
ternatively on a local smooth approximation. In general,there are broadly two categories: discrete and continuous[11]. The Ô¨Årst refers to approximating curvatures by formu-
lating a closed form for differential geometry operators thatwork directly on the discrete representation of the underly-ing surface. The latter involves Ô¨Åtting a surface locally, then
computing the curvatures by interrogating the Ô¨Åtted surface.
Our experiment shows that curvature estimates derived by
locally approximating the surface with a smooth polynomialfunction give better results than the discrete versions.
In order to derive the geometry features of facial sur-
face, we give an analytic form of the regression functionfor smoothly Ô¨Åtting the triangle mesh surface. Note thatalthough the triangle meshes are deÔ¨Åned in a global coordi-nate system, for easy computation, the Ô¨Åtting procedure isperformed in a local coordinate system, which is centered
at the vertex to be examined. The local coordinate systemfor the surface Ô¨Åtting is deÔ¨Åned as follows: let pbe a vertex
on a 3D model M, andn
p=(a,b, c)Tbe the unit nor-
mal at vertex p. A set of vertices that are adjacent to pis
{qi=(xi,yi,zi)T},i=1,2...,m . A local coordinate sys-
tem is deÔ¨Åned by taking the vertex pas an origin and ( nx,
ny,np) as the three axes. nxandnyare two orthogonal
axes, which are arbitrarily selected to form a tangent planeperpendicular to the normal vector n
pat crossing point p
(see Figure 2 for an example). The normal vector can be
simply estimated by the mean normal, which is obtained by
Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR‚Äô06) 
0-7695-2597-0/06 $20.00 ¬© 2006  IEEE 

np
v1
v2nx
ny
xy
zp
Figure 2. An example of a local coordinate
system (nx,ny,np)deÔ¨Åned on a nose surface
patch. v1,v2are the principal directions at
pointp.
the average of normal values of the polygons sharing the
common vertex p.
To Ô¨Åt a smooth polynomial patch onto the local surface
and estimate the principal curvatures, we transform the ver-
tices of the local region to the local coordinate system. Theresulting vertices Àúq
iand the corresponding normal vectors
ÀúnÀúqiare expressed as:
Àúqi=RT¬∑(qi‚àíp)
ÀúnÀúqi=RT¬∑nqi (1)
where the rotation matrix is obtained by R=[nxnynp].
The vertex pand its normal npare transformed to the origin
Àúp=( 0,0,0)Tand the unit vector Àúnp=( 0,0,1)Talong the
positive zaxis, respectively.
The approximating polynomial surface is at least sec-
ond order. Considering the Ô¨Åtting accuracy and the compu-tation complexity, we choose a cubic-order approximationmethod. Similar to the method in [14], we deÔ¨Åne the Ô¨Åttingfunction in the form of
z(Àúx,Àúy)=U¬∑X (2)
where the variable Uand the coefÔ¨Åcient Xare deÔ¨Åned as:
U=(1
2Àúx2ÀúxÀúy1
2Àúy2Àúx3Àúx2ÀúyÀúxÀúy2Àúy3)
X=(ABCDEFG )T(3)
Given a set of neighbor vertices {Àúqi=( Àúxi,Àúyi,Àúzi)T},
(i=1,2,..., m )and their corresponding normals {ÀúnÀúqi=(ai,bi,ci)T}, we can establish 3mequations to solve the
seven parameters of X. When the vertex phas more than
three neighbor vertices, the parameters Xcan be approx-
imated using the least-square Ô¨Åtting method. With the re-
gressed local Ô¨Åtting function z(Àúx,Àúy), the Weingarten matrix
for the surface patch becomes:
W=/bracketleftBigg‚àÇ2z(Àúx,Àúy)
‚àÇÀúx2‚àÇ2z(Àúx,Àúy)
‚àÇÀúx‚àÇÀúy
‚àÇ2z(Àúx,Àúy)
‚àÇÀúx‚àÇÀúy‚àÇ2z(Àúx,Àúy)
‚àÇÀúy2/bracketrightBigg
=/bracketleftbiggAB
BC/bracketrightbigg
(4)
After the eigenvalue decomposition, the principal direc-
tions in the local coordinates Àú v1andÀú v2can be estimated.
W=(Àú v1Àú v2)¬∑diag(Œª1Œª2)¬∑(Àú v1Àú v2)T(5)
where Œª1andŒª2are the eigenvalues and Àú v1andÀú v2are the
orthogonal eigenvectors. If |Œª1|>|Œª2|,Àúv1andÀúv2are in the
directions with the maximum curvature and the minimum
curvature, respectively. Figure 3(a)(b) shows an example ofthe principal direction estimation on a facial range model.
Since the principal directions are represented in the local
coordinate system, to obtain a global view of the principal
directions, Àú v
1andÀú v2must be rotated back to the global
coordinate system, as formulated by
v1=R¬∑Àú v1,v2=R¬∑Àú v2 (6)
It is worth noting that our surface labeling criteria rely
on the surface principal curvatures, the principal directions
as well as the surface gradient. After the transformation
from the local coordinate system to the original global co-ordinate system, we can derive the gradient using the nor-mal direction of each surface point. Let z(x,y)be the Ô¨Åt-
ting function of a surface patch centered at p, the normal
direction n
p=(a,b, c)Tcan be written in the form of
(‚àí‚àÇz(x,y)
‚àÇx,‚àí‚àÇz(x,y)
‚àÇy,‚àí1)T. The gradient magnitude /bardbl‚àáz(x,y)/bardbl
atpis then calculated as:
/bardbl‚àáz(x,y)/bardbl=/radicalBig
[‚àÇz(x,y)
‚àÇx]2+[‚àÇz(x,y)
‚àÇx]2=/radicalBig
[‚àía
c]2+[‚àíb
c]2(7)
The principal curvature analysis produced a set of at-
tributes {/bardbl‚àáz/bardbl,v1,v2,Œª1,Œª2}, which describes the sur-
face property at each vertex. Every vertex can be classiÔ¨Åed
according to a primitive feature classiÔ¨Åcation rule, which
will be explained in the next sub-section.
2.2. Primitive 3D Surface Feature Labeling
The principal curvatures Œª1,Œª2represent the maximum
and the minimum degrees of bending of a surface, v1and
v2indicate the surface principal directions, and /bardbl‚àáz/bardblre-
Ô¨Çects the steepness of the surface. Using these geometric
attributes, we are able to classify every vertex into one of
the primitive categories. In other words, we can symbolize
Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR‚Äô06) 
0-7695-2597-0/06 $20.00 ¬© 2006  IEEE 

(a)
 (b) (c)
Figure 3. From left to right: minimum curva-
ture direction map, maximum curvature di-rection map and primitive label map.
the geometric surface by classifying and labeling the primi-tive features. This symbolization process can be realized bymapping the primitive surface features from a 3D geometric
space to a discrete label space, which is formulated as:
Q:{/bardbl‚àáz/bardbl,v
1,v2,Œª1,Œª2}=‚áí{Lm},m=1,2,..., M
(8)
where Qis a pre-deÔ¨Åned classiÔ¨Åcation rule and {Lm}is
a set of surface categories with a total of Mtypes to be
identiÔ¨Åed.
There are several existing rules for primitive surface fea-
ture classiÔ¨Åcation. For example, (1) the shape index basedclassiÔ¨Åcation [7]; (2) Tanaka‚Äôs method [21], which catego-
rized eight distinct features to describe local shapes accord-
ing to the sign of two principal curvatures; (3) the topo-graphic classiÔ¨Åcation method [23, 26], which has been used
for analyzing the 3D topographic surfaces of gray level im-
ages. Depending on the classiÔ¨Åcation Ô¨Åneness, the maximaltwelve distinct primitive features can be deÔ¨Åned. They are
peak, pit, Ô¨Çat, ravine, ridge, saddle (including ridge saddle
and ravine saddle) and hill (including convex hill, concavehill, concave saddle hill, convex saddle hill and slope hill).
In order to scrutinize the facial expression surface de-
tails, we use the twelve distinct primitive surface features torepresent the facial expressions. Similar to the classiÔ¨Åcation
rule used in [23], we extend the method to the application
for the real 3D facial surface labeling. The labeling process
is based on the feature values of {/bardbl‚àáz/bardbl,v
1,v2,Œª1,Œª2}in
the global coordinate system. To do so, two thresholds, TG
andTŒª, are deÔ¨Åned. They are used to evaluate whether the
gradient magnitude and the principal curvatures are trivial
enough to be ignored as zero. The thresholds are calculated
based on the mixed error criteria [26]:
TG= max[ /epsilon1,/epsilon1¬∑z(x,y)]
TŒª= max[ /epsilon1,/epsilon1¬∑/bardblW/bardbl‚àû] (9)
where /bardblW/bardbl‚àû= max[ |A|+|B|,|B|+|C|](see Equation4) and /epsilon1is a user-speciÔ¨Åed parameter. In our method, we
use/epsilon1=0.001¬∑s, where sis the average distance from each
vertex to the center of the individual 3D facial model.
The classiÔ¨Åcation rule for twelve primitive surface labels
is expounded in Table 1. In general, if /bardbl‚àáz/bardbl<TGor there
is a zero crossing in the direction of the maximum curva-ture, one of the non-hillside labels is assigned; otherwise,
one of the hill-side labels is assigned using the rule deÔ¨Ånedin the table. Figure 3(c) shows an example of the labeling
result from a 3D facial expression range model.
Œª1 Œª2 Hillside Label Non-Hillside Label
|Œª1|<TŒª |Œª2|<TŒª Ô¨Çat slope hill
Œª1<‚àíTŒª Œª2<‚àíTŒª peak convex hill
Œª1<‚àíTŒª |Œª2|<TŒª ridge convex hill
Œª1<‚àíTŒª Œª2>TŒª ridge saddle convex saddle hill
Œª1>TŒª Œª2<‚àíTŒª ravine saddle concave hill
Œª1>TŒª |Œª2|<TŒª ravine convex hill
Œª1>TŒª Œª2>TŒª pit
Œª1>TŒª Œª2<‚àíTŒª concave saddle hill
Table 1. ClassiÔ¨Åcation rule of primitive 3D
surface labels.
3. 3D Expression Description Based on Primi-
tive Label Distribution
After the labeling process, the facial expressions can be
described by the distribution of the labels over the local orentire facial region. Intuitively, every facial expression is a
result of facial muscle actuation, reÔ¨Çecting the facial surface
variation. Such a variation results in the different distribu-tions of primitive surface labels. This fact suggests that the
primitive label distribution could directly link to a distinct
facial expression. In other words, the same type of facial
expression is expected to share the similar primitive label
distribution with a certain robustness, given a sufÔ¨Åcient res-olution of the range model.
To Ô¨Ånd an explicit representation of the fundamental
structure of facial surface details, we investigate the statisti-cal distributions of the primitive surface labels in seven ex-
pressive facial regions, which are deÔ¨Åned according to the
neuro-anatomy knowledge of conÔ¨Åguration of facial mus-
cles and their dynamics [20]. As shown in Figure 4, sixty-
four Ô¨Åducial points are deÔ¨Åned on the facial surface, and ac-cordingly, the seven expressive local regions are constructed
based on these key points. Note that the interiors of mouth
and eyes are currently not included in the seven local re-
gions. The reasons are twofold: (1) the interiors of mouth
and eyes are isolated from the facial skin. They can be
Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR‚Äô06) 
0-7695-2597-0/06 $20.00 ¬© 2006  IEEE 

(b) (a)
Figure 4. 64facial Ô¨Åducial points and 7se-
lected facial regions.
treated as three separated objects. These ‚Äúholes‚Äù are com-
plementary areas of the facial skin area. The change of their
shapes is directly reÔ¨Çected on the change of the surroundingskins which have been included in the seven expressive re-gions; (2) although tongues and eye-balls can be viewed asthe signatures of facial expressions, their appearances may
not be as stable as the facial skin due to the dark hole effect
existing in the current 3D imaging systems.
In short, the selected seven local regions cover the most
expressive areas on a human face. In each selected region,
we identify the expression signature by calculating the his-togram distribution of primitive labels. Such a distributionis described as follows:
r
i=/bracketleftbiggni1
ni,¬∑¬∑¬∑,nim
ni,¬∑¬∑¬∑,niM
ni/bracketrightbigg
(10)
where nimis the number of vertices which are labeled
by the label type Lm, andniis the total number of vertices
in the ith local region ( ni=M/summationtext
m=1nim).M=1 2 is the number
of the primitive label categories.
The combination of seven histogram distributions of the
selected entire regions generates a unique expression de-
scriptor for a speciÔ¨Åc expression. As a result, the expres-
sion is described by the primitive surface feature distribu-tion (PSFD), which is expressed by
E=[r
1,¬∑¬∑¬∑,ri,¬∑¬∑¬∑,rK] (11)
where Kis the number of expressive regions ( K=7in our
experiment).
4. Recognition Experiments
4.1. Database
We constructed a 3D facial expression database for our
experiment. The 3D range data is scanned by a 3DMD static
digitizer [15], which uses a random light pattern projectionin the speckle projection Ô¨Çash environment . The modelresolution is in the range of 20,000 polygons to 35,000
polygons, depending on the size of the face being scanned.The facial expressions with six universal emotional states,
Anger, Disgust, Fear, Happiness, Sadness and Surprise , are
sampled in four different levels of intensity (e.g., from lesspronounced to more pronounced). In our experiment, we
used the data captured from 60subjects with two high-
intensity models for each expression. The test is based onthe six prototypic expressions.
Each range facial model consists of a meshed surface
model and an associated texture image. The head pose
can be estimated by a triangle plane determined by the in-
ner corners of two eyes and a nose tip. Given the head
pose, the original scan is processed by rotating the model
to the frontal view. This pre-processing results in a work-
ing model , on which we manually labeled 64Ô¨Ådicual ver-
tices for the expression recognition experiments. Table 2
gives a summary of the data set that we used for our exper-
iments. A detail description of the database construction,
post-processing, and organization can be found in [29].
# of subjects # of expres-
sion types# of samples for each
subject of each ex-
pressiontotal # of sam-
ples
60 6 2 720
Table 2. Summary of the 3D facial expression
data set used in our experiment.
4.2. Recognition Results
Our facial expression recognition experiments are car-
ried out in a person-independent manner, which is believed
to be more challenging than a person-dependent approach
[17]. We randomly partitioned the 60subjects into two sub-
sets: one with 54subjects for training and the other with
6subjects for test. The experimental paradigm guarantees
that any subject used for testing does not appear in the train-ing set because the random partition is based on the subjectsrather than the individual expression. Four popular clas-
siÔ¨Åers: Quadratic Discriminant ClassiÔ¨Åer (QDC), Linear
Discriminant Analysis (LDA), Naive Bayesian ClassiÔ¨Åer
(NBC), and Support V ector ClassiÔ¨Åer (SVC) with RBF ker-
nel are used in the experiments. The tests are executed 20
times on each classiÔ¨Åer with different partitions to achieve
a stable generalization recognition rate. The entire process
guarantees that every subject is tested at least once for each
classiÔ¨Åer. For each round of the test, all the classiÔ¨Åers are
reset and re-trained from the initial state.
Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR‚Äô06) 
0-7695-2597-0/06 $20.00 ¬© 2006  IEEE 

ClassiÔ¨Åer QDC LDA NBC SVC
Recognition rate 74.5% 83.6% 71.7% 77.8%
Table 3. Results of person-independent ex-
pression classiÔ¨Åcation using the 3D-PSFDmethod.
Table 3 shows the average correct recognition rates of
the four classiÔ¨Åers. The LDA classiÔ¨Åer achieves the high-
est correct recognition rate with an accuracy of 83.6%. The
confusion matrix of the average case for the LDA classi-
Ô¨Åer is shown in Table 4. The expressions of Happiness
and Surprise are well identiÔ¨Åed with accuracies of 95.0%
and90.8%, respectively. Anger ,Sadness ,Fear and Disgust
have comparatively lower recognition rates. The misclassi-
Ô¨Åcation rate between Anger and Sadness is around 19.6%
(=8.3%+11.3%), while that between Fear with Happiness
is around 16.3%.
Input\Output Anger Disgust Fear Happiness Sadness Surprise
Anger 80.0% 1.7% 6.3% 0.0% 11.3% 0.8%
Disgust 4.6% 80.4% 4.2% 3.8% 6.7% 0.4%
Fear 0.0% 2.5% 75.0% 12.5% 7.9% 2.1%
Happiness 0.0% 0.8% 3.8% 95.0% 0.4% 0.0%
Sadness 8.3% 2.5% 2.9% 0.0% 80.4% 5.8%
Surprise 1.7% 0.8% 1.2% 0.0% 5.4% 90.8%
Table 4. Confusion Matrix of the average case
of LDA classiÔ¨Åer for person-independent ex-
pression recognition.
5. Comparison Study
In this section, we compare the proposed 3D primitive
feature distribution method (3D-PSFD) with two 2D ap-
pearance feature based methods. One is the well-knownGabor-wavelet (GW) approach [16] and the other is our re-
cently developed Topographic Context (TC) approach [24].
(1) In the Gabor-wavelet (GW) based approach, a set of
multi-scale and multi-orientation coefÔ¨Åcients are calculated
to describe the appearance variations in the facial region.
We applied 6√ó3complex Gabor-wavelet ( 6orientations and
3spatial resolutions) on 34Ô¨Åducial points. The coefÔ¨Åcients
of the real and imaginary parts can be computed as:
G+(x,y;œâ,Œ∏)=œâ2
œÉ2¬∑e‚àíœâ2(x2+y2)
2œÉ2¬∑{cos[œâ(xcosŒ∏+ysinŒ∏)]‚àíe‚àíœÉ2
2}
G‚àí(x,y;œâ,Œ∏)=œâ2
œÉ2¬∑e‚àíœâ2(x2+y2)
2œÉ2¬∑sin[œâ(xcosŒ∏+ysinŒ∏)] (12)where we let œÉ=œÄ. The three selected spatial scales
are{œÄ/4,œÄ/8,œÄ/16}and the 6orientations selected are
{0,œÄ/6,œÄ/3,œÄ/2,2œÄ/3,5œÄ/6}. The feature extracted at
pixel(x,y)by a certain Gabor-wavelet kernel with parame-
ters{œâ,Œ∏}is the amplitude of the real and imaginary coef-
Ô¨Åcients G=/radicalBig
G2
++G2‚àí. Therefore in total, we extracted
18√ó34 = 612 wavelet features for each image.
(2) In our existing work, we developed a Topographic
Context (TC) based approach for facial expression recog-
nition based on 2D static images. The TC expression fea-
tures are the topographic primal sketch features inherent inthe2D facial images. We use such appearance features to
describe the distinct facial expressions. Similar to our 3D
based approach, we used the feature statistics to representthe2D facial expressions. It has proved to be robust to fa-
cial landmark detection as a result of its intrinsic statisticsproperty (see details in [24]).
The comparison study with the above two methods is
conducted under various head pose conditions, including
two cases: frontal view and non-frontal views.
5.1. Case 1: Frontal View
Our frontal-view images are generated from the texture-
mapped working models. All these images are normalized
to256√ó256 pixels. Several examples are shown in the
top row of Figure 1. There are 60subjects with a total of
720 frontal-view facial images used for the test. The experi-
ments are executed in a person-independent manner similar
to the strategy used in Section 4.2.
Table 5 reports the correct recognition rates using these
two methods. We found the Gabor-wavelet approach per-
forms poorly when using the SVC classiÔ¨Åer, which is not
comparable to the results from the other three classiÔ¨Åers.
Method \ClassiÔ¨Åer QDC LDA NBC
Topographic context method 73.8% 79.2% 70.9%
Gabor-wavelets method 72.3% 74.1% 62.1%
Table 5. Results of person-independent ex-
pression classiÔ¨Åcation using GW and TC.
Comparing to the performance shown in Table 3, the
3D-PSFD method is superior to the 2D appearance feature
based methods when classifying the six prototypic facial ex-
pressions.
5.2. Case 2: Non-frontal View
In this section, we compare the performance of our 3D
geometric based approach (PSFD) with the 2D appearance
Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR‚Äô06) 
0-7695-2597-0/06 $20.00 ¬© 2006  IEEE 

feature based approaches under different head pose condi-
tions. We obtained the head pose information from the orig-
inal face scans by estimating the triangle face-plane formed
by the inner corners of two eyes and the tip of nose. Know-
ing the 3D head pose, we are able to rotate the 3D model to a
frontal view position or arbitrary pose positions without los-
ing any geometric feature information. However, if viewedin the 2D projection plane, the appearance of 2D facial im-
ages varies dramatically when the head pose is arbitrarilychanged. The difÔ¨Åculty to recover the missing appearanceinformation from 2D images makes 2D based facial expres-
sion recognition sensitive to head pose variations.
Using our face range models, we have generated facial
expression images under different views, corresponding todifferent head poses. This is done by rotating the models to
a certain degree and generating the face images in that view
by texture-mapping. We generated the face images underviews of ¬±10
‚ó¶,¬±20‚ó¶,¬±30‚ó¶, and¬±40‚ó¶for each orienta-
tion (pitch and yaw rotation), resulting in 720 images for
each view. Figure 5 shows the examples of different facial
appearances under different views.
Figure 5. Facial expression images with dif-
ferent head rotation. Top row is yaw rotation
and bottom row is pitch rotation. From left to
right, the rotation angle is ‚àí40‚ó¶,‚àí30‚ó¶,‚àí20‚ó¶,
20‚ó¶,30‚ó¶and40‚ó¶.
We choose to use the LDA classiÔ¨Åer to evaluate the
expression recognition performance under different head
poses because in most cases LDA outputs the best recog-
nition result. For all three algorithms (PSFD, GW, and TC),
the LDA classiÔ¨Åer is trained using the front view face data.
The recognition results with respect to different poses
(e.g., pitch and yaw rotations) are shown in Figure 6. From
the Ô¨Ågure, we can see that the average recognition rates
of the 2D appearance feature based methods degrade con-
siderably as head rotation increases, especially the Gabor-wavelet method. The extraction of Gabor-wavelet coefÔ¨Å-
cients is based on the selected Ô¨Åducial points. The posechange alters the distribution of these Ô¨Åducial points, and re-
sults in signiÔ¨Åcant distortion of Gabor-wavelet features. On
the contrary, the 3D-PSFD based approach makes the 3D
primitive feature distribution invariant to the pose variations
because the 3D geometric features are view-independent.
Figure 6. Comparison of the recognition per-
formance under different head orientations.
left: pitch; right: yaw.
6. Concluding Remarks
In this paper, we investigated the issue of 3D facial ex-
pression representation and recognition. To the best of our
knowledge, this is the Ô¨Årst attempt to recognize facial ex-
pressions using range data in a complete 3D space. We
have proposed to extract and label the primitive 3D surface
features, and derive their statistical distributions to repre-
sent the distinct prototypic facial expressions. We used the
primitive surface feature distribution (PSFD) as the signa-
ture to distinguish facial expressions, and conducted expres-sion recognition experiments using the person-independentstrategy. Compared to the existing 2D static image based
approaches (e.g., GW and TC methods), our 3D range data
based approach shows superior performance as a result ofthe lighting and orientation invariance of 3D geometric fea-
tures. The experiments show encouraging results in recog-nizing six prototypic facial expressions under various head-
pose conditions.
There are some limitations in the current work:
(1)Our current data set contains only static expression
models. A database including dynamic 3D facial expres-
sion sequences is needed in order to study the subtle skin
movement associated with facial expressions in 3D space.
With the emergence of dynamic 3D imaging systems [25],
it is possible to investigate the feasibility of tracking action
units in 3D space to further enhance the current FACS based
facial expression recognition technique. Because the dis-
criminability of 3D facial expressions is dependent on the
resolution of the 3D mesh, higher resolution models are re-
quired in order to improve the recognition performance.
(2) The current work involves the pre-processing of
range data which requires manual selection of the surfaceÔ¨Åducial points. In order to realize an automatic system for3D facial expression analysis, algorithms for automatic de-tection of 3D surface features must be developed. The exist-
ing approaches (e.g., free-form based [25], etc.) are promis-
Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR‚Äô06) 
0-7695-2597-0/06 $20.00 ¬© 2006  IEEE 

ing for this purpose.
The above issues give rise to our future research direc-
tions in order to improve facial expression analysis in the
3D space. In addition, we will investigate integrating 3D
geometric shape and 2D texture information to improve our
current approach.
7. Acknowledgement
This material is based upon the work supported in part by
the National Science Foundation under grants IIS-0541044,IIS-0414029, and the NYSTAR‚Äôs James D. Watson Investi-gator Program.
References
[1] M. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I.Fasel,
and J. Movellan. Recognizing facial expressions: machine
learning and application to spontaneous behavior. In IEEE
CVPR2005 , San Diego, CA. 2005.
[2] B. Braathen, M. Bartlett, G. Littlewort, E. Smith, and
J. Movellan. An approach to automatic recognition of spon-taneous facial actions. In Proc. of Int. Conf. on FGR , 2002.
[3] Y . Chang, C. Hu, and M. Turk. Probabilistic expression anal-
ysis on manifolds. In CVPR , Washington DC, USA, 2004.
[4] I. Cohen, N. Sebe, A. Garg, L. S. Chen, and T. S. Huang. Fa-
cial expression recognition from video sequences: temporal
and static modeling. CVIU , 91:160‚Äì187, 2003.
[5] R. Cowie, , E. Douglas-Cowie, N. Tsapatsoulis, G. V otsis,
S. Kollias, W. Fellenz, and J. Taylor. Emotion recognition in
human computer interaction. IEEE Signal Processing Mag-
azine , 18(1):32‚Äì80, 2001.
[6] G. Donato, M. Bartlett, J. Hager, P . Ekman, and T. Se-
jnowski. Classifying facial actions. IEEE Trans. on PAMI ,
21:974‚Äì989, 1999.
[7] C. Dorai and A. Jain. Cosmos - a representation scheme for
3d free-form objects. IEEE Trans. on PAMI , 19(10):1115‚Äì
1130, 1997.
[8] P . Ekman and W. Friesen, editors. The facial action coding
system: a technique for the measurement of facial move-ment . Consulting Psychologists Press, San Francisco, 1978.
[9] P . Ekman, T. Huang, T. Sejnowski, and J. Hager. Final re-
port to NSF of the planning workshop on facial expressionunderstanding . Human Interaction Lab., UC at San Fran-
cisco, 1993.
[10] I. Essa and A. Pentland. Coding, analysis, interpretation,
and recognition of facial expressions. IEEE Trans. on PAMI ,
19:757‚Äì763, 1997.
[11] G. Farin. Curves and Surfaces for Computer Aided Geomet-
ric Design . 5th ed., Morgan-Kaufmann, 2001.
[12] B. Fasel and J. Luttin. Automatic facial expression analysis:
Survey. Pattern Recognition , 36:259‚Äì275, 2003.
[13] S. Gokturk, J. Bouguet, C. Tomasi, and B. Girod. Model-
based face tracking for view-independent facial expressionrecognition. In Proc. of Int. Conf. on FGR , 2002.[14] J. Goldfeather and V . Interrante. A novel cubic-order algo-
rithm for approximating principal direction vectors. ACM
Trans. on Graphics , 23:45‚Äì63, 2004.
[15] Inc.3dMd. http://www.3q.com . 2005.
[16] M. Lyons, J. Budynek, and S. Akamatsu. Automatic clas-
siÔ¨Åcation of single facial images. IEEE Trans. on PAMI ,
21:1357‚Äì1362, 1999.
[17] M. Pantic and L. Rothkrantz. Automatic analysis of facial
expressions: the state of the art. IEEE Trans. on PAMI ,
22:1424‚Äì1445, 2000.
[18] M. Pantic and L. Rothkrantz. Facial action recognition for
facial expression analysis from static face images. IEEE
Trans. on SMC-Part B: Cybernetics , 34:1449‚Äì1461, 2004.
[19] P . Phillips, P . Flynn, T. Scruggs, K. Bowyer, J. Chang,
K. Hoffman, J. Marques, J. Min, and W. Worek. Overviewof the face recognition grand challenge. In IEEE Conf. on
CVPR, San Diego, CA , 2005.
[20] W. Rinn. The neuropsychology of facial expression: A
review of the neurological and psychological mechanismsfor producing facial expressions. Psychological Bulletin ,
95:52‚Äì77, 1984.
[21] H. Tanaka, M. Ikeda, and H. Chiaki. Curvature-based face
surface recognition using spherical correlation. In IEEE
Conf. on FGR , pages 372‚Äì377, 1998.
[22] Y . Tian, T. Kanade, and J. Cohn. Recognizing action units
for facial expression analysis. IEEE Trans. on PAMI , 23:1‚Äì
9, 2001.
[23] O. Trier, T. Taxt, and A. Jain. Data capture from maps based
on gray scale topographic analysis. In The Third Interna-
tional Conference on Document Analysis and Recognition ,
Montreal, Canada, 1995.
[24] J. Wang and L. Yin. Facial expression representation and
recognition from static images using topographic context. InTechnical Report, Department of Computer Science, SUNYat Binghamton , Nov., 2005.
[25] Y . Wang, X. Huang, C. Lee, S. Zhang, Z. Li, D. Samaras,
D. Metaxas, A. Elgammal, and P . Huang. High resolutionacquisition, learning and transfer of dynamic 3d facial ex-pressions. In EUROGRAPHICS 2004 , 2004.
[26] L. T. Watson, T. J. Laffey, and R. M. Haralick. Topographic
classiÔ¨Åcation of digital image intensity surfaces using gener-
alized splines and the discrete cosine transform. Computer
Vision, Graphics and Image Processing , 29:143‚Äì167, 1985.
[27] Z. Wen and T. Huang. Capturing subtle facial motions in 3d
face tracking. In ICCV , 2003.
[28] Y . Yacoob and L. Davis. Recognizing human facial expres-
sion from long image sequences using optical Ô¨Çow. IEEE
Trans. on PAMI , 16:636‚Äì642, 1996.
[29] L. Yin, X. Wei, Y . Sun, J. Wang, and M. Rosato. A 3d facial
expression database for facial behavior research. In 7th Int.
Conf. on FGR , Southampton, UK, 2006.
[30] L. Zalewski and S. Gong. Synthesis and recognition of facial
expressions in virtual 3d views. In IEEE 6th Inter. Conf. on
FGR , 2004.
[31] Y . Zhang and Q. Ji. Active and dynamic information fusion
for facial expression understanding from image sequences.
IEEE Trans. on PAMI , 27(5):699‚Äì714, May 2005.
[32] W. Zhao, R. Chellappa, P . Phillips, and A. Rosenfeld. Face
recognition: A literature survey. ACM Computing Surveys ,
35(4), Dec. 2003.
Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR‚Äô06) 
0-7695-2597-0/06 $20.00 ¬© 2006  IEEE 

"
https://ieeexplore.ieee.org/document/4813304,"3DFacial Expr ession Recognition Based onProperties ofLine Segments
Connecting Facial Featur ePoints
Hao TangandThomas S.Huang
Beckman Institute forAdvanced Science andTechnology
Coordinated Science Laboratory
Department ofElectrical andComputer Engineering
University ofIllinois atUrbana-Champaign, Urbana, IL61801 U.S.A.
fhaotang2,huang g@ifp.uiuc.edu
Abstract
The 3Dfacial geometry contains ample information
about human facial expressions. Suchinformation isinvari-
anttopose and lighting conditions, whic hhave imposed
serious hurdles onmany 2Dfacial analysis problems. In
thispaper ,weperform personandgender independent fa-
cialexpression recognition based onproperties oftheline
segments connecting certain 3Dfacial featur epoints. The
normalized distances andslopes ofthese linesegments com-
prise asetof96distinguishing featur esforrecognizing six
univer salfacial expressions, namely anger,disgust, fear,
happiness, sadness, andsurprise .Using amulti-class sup-
port vector machine (SVM) classier ,an87.1% average
recognition rateisachievedonthepublicly available 3D
facial expression database BU-3DFE [1]. Thehighest av-
eragerecognition rateobtained inourexperiments is99.2%
fortherecognition ofsurprise .Our result outperforms the
result reported intheprior work [2],whic huses elabor ately
extracted primitive facial surface featur esandanLDAclas-
sier andwhic hyields anaveragerecognition rateof83.6%
onthesame database .
1.Introduction
The3Dgeometry ofthefaceplays animportant rolein
recognizing theidentity andexpression ofaperson [1¬ñ5].
The3Dfacial geometry contains ample information about
human facial expressions. Such information isinvariant to
pose andlighting conditions, which haveimposed serious
hurdles onmany2Dfacial analysis problems including the
facial expression recognition problem [6¬ñ8].Itisthus ad-
vantageous toexploit theavailable 3Dfacial geometry in-
This workwassupported inpartbytheU.S. Government VACEPro-
gram.formation totackle thevarious problems offacial analy-
sis. While the3Dfacial geometry hasbeen relati velyex-
tensivelyexplored andutilized inthestudy offacerecog-
nition [9,10]thanks tothefast-de veloping andincreasingly
mature 3Dscanning products and3Dfacereconstruction
techniques, itishoweverfarlessused forfacial expression
recognition. The most obvious reason forthisembarrass-
ingsituation isclearly thatthere haslong been alack ofa
publicly available 3Dfacial expression database toserveas
acommon platform fortheresearchers inthisarea. With-
outsuch adatabase, neither cantheresearchers start with
problem investigation norcantheyendincomparing their
results. Nowthegood newsis,Yinet.al.atBinghamton
University haverecently constructed a3Dfacial expression
database forfacial beha vior research andmade itpublicly
available [1]. This isdenitely therstattempt tomakea
publicly available 3Dfacial expression database forthere-
search community .Itisbelie vedthatthisdatabase willun-
doubtedly haveagreat impact onthefacial beha viorrelated
research inthenextafewyears.
Wanget.al.[2]conducted therst workof3Dfacial
expression recognition ontheBU-3DFE database. They
reported thatthehighest average recognition ratetheyob-
tained onthisdatabase was83.6% byusing elaborately ex-
tracted primiti vefacial surfacefeatures andanLDAclas-
sier .Theyalso reported that thefacial expressions of
happiness andsurprise were well identied with accuracies
of95% and90.8%, respecti vely.Theycompared their re-
sults with theresults obtained bytwo2Dappearance feature
based methods, namely theGabor -wavelet(GW) approach
andtheTopographic Conte xt(TC) approach. Inboth cases,
theyshowedthattheir results outperformed those forthe2D
methods. This ishighly expectable because 3Dfacial geom-
etrycontains richinformation about facial expressions and
ismore robustthan 2Dappearances.
Inthispaper ,wefurther perform person andgender in-
978-1-4244-215 4-1/08/$25.00c2008 IEEE
Figure 1.Some examples inBU-3DFE database.
dependent facial expression recognition ontheBU-3DFE
database. Ourmethod isbased onproperties ofthelineseg-
ments connecting certain 3Dfacial feature points. Thenor-
malized distances andslopes ofthese linesegments com-
prise asetof96distinguishing features forrecognizing six
universal facial expressions, namely anger ,disgust, fear,
happiness, sadness, andsurprise. Using amulti-class sup-
portvector machine (SVM) classier ,weachie vean87.1%
average recognition rate. The highest average recognition
rateobtained is99.2% fortherecognition ofsurprise.
2.Database description
TheBU-3DFE database wasrecently developed byYin
et.al.atBinghamton University .Itwasdesigned tosample
3Dfacial beha viors with different prototypical emotional
states atvarious levelsofintensities. Inthedatabase, there
areatotal of100subjects, among which 56arefemale and
44aremale. These subjects arewell distrib uted across dif-
ferent ethnic orracial ancestries, including White, Black,
East-Asian, Middle-East Asian, Hispanic Latino, andoth-
ers. While being recorded, each subject wasaskedtoper-
form theneutral facial expression aswell assixuniversalfacial expressions, namely anger (AN), disgust (DI), fear
(FE), happiness (HA), sadness (SA), and surprise (SU).
Each facial expression hasfour different levelsofintensi-
ties(low,middle, high, highest), except thattheneutral fa-
cialexpression hasonly oneintensity level.Thus, there are
253Dfacial expression models foreach subject, resulting
in2500 3Dfacial expression models inthedatabase.
Associated with each 3Dfacial expression model area
raw3Dfacemesh model, acropped 3Dfacemesh model,
apairoftexture images with two-angle views(about +45o
and-45oawayfrom thefacefrontal normal), afrontal-vie w
texture image, asetof83facial feature points, andafa-
cialpose vector .These data giveacomplete 3Ddescription
ofafaceunder aspecic facial expression. Adetailed de-
scription oftheBU-3DFE database canbefound in[1].In
thispaper ,webase ourmethod solely onthe83facial fea-
turepoints mark edonthecropped 3Dfacemesh model, as
showninFigure 2.Wehereafter refer toacropped 3Dface
mesh model asa3Dfacial expression model. Some exam-
ples ofthe3Dfacial expression models intheBU-3DFE
database areillustrated inFigure 1.
Figure 2.The83facial feature points.
3.Featur eextraction
Inthispaper ,wecarefully devise asetoffeatures based
onproperties ofthelinesegments connecting certain facial
feature points onthe3Dfacial expression models. Accord-
ingtoEkman' sfacial action coding system (FACS)[11,12],
thesixuniversal facial expressions canbeencoded byac-
tion units, which describe themovements ofparticular fa-
cialfeature points. Similarly ,intheMPEG-4 standard [13],
thesixfacial expressions aredened byfacial animation
parameters (FAPs), which specify precisely howmuch the
facial feature points havetobemoved.Inlight ofthis, we
belie vethatcertain properties ofthelinesegments connect-
ingfacial feature points offerasparse, compact, anduseful
representation ofthe3Dfacial geometry .
Wetherefore extract asetof96features, which consist
ofthenormalized distances andslopes ofthelinesegments
connecting asubset ofthe83facial feature points. These
features arebelie vedtoplay animportant roleindetermin-
ingthevarious facial expressions. The features arepicto-
rially showninFigure 3andtheir textual descriptions are
giveninTable 1.
Since thefaces ofdifferent people havedifferent sizes
and proportions, inorder tomakethefeatures person-
independent, wenormalize thedistance features byfa-
cialanimation parameter units (FAPUs), asguided bythe
MPEG-4 standard. InMPEG-4, theFAPUs servetoscale
theFAPs inorder thatasingle setofFAPs canbeused with
arbitrary facemodels. TheFAPUs aredened asfractions
ofdistances between certain feature points onafacemodel
initsneutral state andallowtointerpret theFAPs onar-
bitrary facemodels inaconsistent way.The veFAPUs
areshowninFigure 4.The distance features aredivided
bythecorresponding FAPUs, givenastheunitinTable 1.
Inaddition, theslope features (each ofwhich being a3-
dimensional vector) aredivided bytheir respecti venorms,
resulting inunitvectors.
11
13
1214
23246
41 2
7 8
15169101920212218 173
5
2825 26
2930
3940
46 4527
3132333435363837
4142
4344
4748
(a) (b)
Figure 3.(a)Thedistance features. (b)Theslope features.
Figure 4.Afacemodel initsneutral state onwhich theFAPUs are
dened bythefractions ofdistances between themark edkeyfacial
feature points. TheveFAPUs areIRISD0 -Irisdiameter ,ES0 -
Eyeseparation, ENS0 -Eye-nose separation, MNS0 -Mouth-nose
separation, andMW0 -Mouth width, respecti vely[13].
4.Multi-class SVM classication
The support vector machine (SVM) isasuccessful su-
pervised learning technique forclassication andregression
[14,15]. Inabinary classication scenario, givenasetof
Ntraining feature vectors fxngN
n=1andtheir correspond-
ingclass labelsyT=[y1;y2;:::;yN]where yn2f 1;1g,
n=1;2;:::;N,theSVM makespredictions based ona
function oftheform
y(x;w)=NX
n=1wnK(x;xn)+b (1)
wherewT=[w1;w2;:::;wN]aretheweights andK(;)
isakernel function. The weights aredetermined bymin-
Feature Textual description Property Number ofvalues Unit
1 length ofright eyebro w distance 1 ES0
2 length oflefteyebro w distance 1 ES0
3 distance between leftandright eyebro ws distance 1 ES0
4 distance between leftandright eyes distance 1 ES0
5 distance between right inner eyebro wandright inner eyecorner distance 1 ENS0
6 distance between leftinner eyebro wandleftinner eyecorner distance 1 ENS0
7 distance between right outer eyebro wandright outer eyecorner distance 1 ENS0
8 distance between leftouter eyebro wandleftouter eyecorner distance 1 ENS0
9 distance between right inner eyecorner andright nose wing distance 1 ES0
10 distance between leftinner eyecorner andleftnose wing distance 1 ES0
11 width ofright eye distance 1 ES0
12 width oflefteye distance 1 ES0
13 height ofright eye distance 1 IRISD0
14 height oflefteye distance 1 IRISD0
15 distance between right outer eyecorner andright outer mouth corner distance 1 ENS0
16 distance between leftouter eyecorner andleftouter mouth corner distance 1 ENS0
17 distance between right outer mouth corner andright jawroot distance 1 MNS0
18 distance between leftouter mouth corner andleftjawroot distance 1 MNS0
19 distance between right nostril andupper midlip distance 1 MNS0
20 distance between leftnostril andupper midlip distance 1 MNS0
21 distance between right nostril andchin distance 1 MNS0
22 distance between leftnostril andchin distance 1 MNS0
23 width ofmouth distance 1 MW0
24 height ofmouth distance 1 MNS0
25 slope ofright eyebro w slope 3 -
26 slope oflefteyebro w slope 3 -
27 slope between right inner eyebro wandright inner eyecorner slope 3 -
28 slope between leftinner eyebro wandleftinner eyecorner slope 3 -
29 slope between right outer eyebro wandright outer eyecorner slope 3 -
30 slope between leftouter eyebro wandleftouter eyecorner slope 3 -
31 slope between right outer eyecorner andright upper mideyelid slope 3 -
32 slope between right inner eyecorner andright upper mideyelid slope 3 -
33 slope between leftouter eyecorner andleftupper mideyelid slope 3 -
34 slope between leftinner eyecorner andleftupper mideyelid slope 3 -
35 slope between right outer eyecorner andright lowermideyelid slope 3 -
36 slope between right inner eyecorner andright lowermideyelid slope 3 -
37 slope between leftouter eyecorner andleftlowermideyelid slope 3 -
38 slope between leftinner eyecorner andleftlowermideyelid slope 3 -
39 slope between right outer eyecorner andright outer mouth corner slope 3 -
40 slope between leftouter eyecorner andleftouter mouth corner slope 3 -
41 slope between right outer mouth corner andupper midlip slope 3 -
42 slope between leftouter mouth corner andupper midlip slope 3 -
43 slope between right outer mouth corner andlowermidlip slope 3 -
44 slope between leftouter mouth corner andlowermidlip slope 3 -
45 slope between right outer mouth corner andright jawroot slope 3 -
46 slope between leftouter mouth corner andleftjawroot slope 3 -
47 slope between right jawrootandchin slope 3 -
48 slope between leftjawrootandchin slope 3 -
Table 1.The normalized distances andslopes between certain facial feature points onthe3Dfacial expression model comprise a96-
dimension feature vector .Theunits ofthedistance features aregivenbytheMPEG-4 facial animation parameter units (FAPUs). Theslope
features arenormalized bytheir respecti vevector norms.
imizing theerror onthetraining setwhile maximizing the
marginbetween thetwoclasses inthefeature space implic-
itlydened bythekernel function. The optional constant
termbindicates theuseofabias intheformulation. The
keyfeature andnice property oftheSVM isthatasanat-
uralresult oftheeffective¬ìprior¬î (maximizing themargin)
manyoftheweights inEquation 1turnouttobezero. This
leads toasparse model represented bythetraining feature
vectors corresponding tothenon-zero weights (those lieon
themarginoronthewrong side ofit).These training fea-
turevectors arecalled thesupport vectors. Thesparseness
ofamodel isanappealing feature thatisextremely helpful
forcontrolling themodel comple xity toavoidover-tting
andtoreduce thecomputational time.
The SVM training seeks tond thesolution tothefol-
lowing optimization problem (primal form):
min
w;b;1
2wTw+CPN
n=1n (2)
subject to yn(wT(xn)+b)1 n
n0;n=1;2;:::;N
whose dual is
min1
2TQ eT (3)
subject to yT=0
0nC;n=1;2;:::;N
whereeT=[1;1;:::;1],C>0istheerror/mar gintrade-
offparameter ,QisanNNmatrix with elements Qij=
yiyjK(xi;xj),andK(xi;xj)=(xi)T(xj)isthe
kernel function. Through thekernel function, thetraining
feature vectorsxnaremapped into ahigher orinnite di-
mensional space inwhich theSVM seeks tond alinear
separating hyperplane with themaximum marginbetween
thetwoclasses. Averyuseful kernel function istheradial
basis function (RBF)
K(xi;xj)=e kxi xjk2(4)
where isthekernel parameter .
Thedecision function ofSVM classication isgivenby
y(x)=sgn NX
n=1ynnK(x;xn)+b!
(5)
TheSVM initsbasic form (Equation 5)isabinary clas-
sier .Weconstruct amulti-class SVM classier using the
¬ìone-against-one¬î approach. Inthecase ofK-class classi-
cation, anSVM model islearned foreach ofthe1
2K(K 1)
unique pairs ofclasses. Givenanovelinput feature vectorx
tobeclassied, wetestitagainst allthelearned SVM mod-
elsyij(x),i;j=1;2;:::;K,i6=j.Thefollo wing votingstrate gyisadopted
vote(i)=vote(i)+1 ifyij(x)0 (6)
vote(j)=vote(j)+1ifyij(x)>0
Theclass which gains themajority voteisthen thepre-
dicted class label^k:
^k=argmax
0kKv(k) (7)
5.Experiments
Weperformed extensi vefacial expression recognition
experiments ontheBU-3DFE database. The setup ofour
experiments isasfollo ws.First, werandomly chose asub-
setofthedatabase containing 60subjects ofwhich 30are
female and30aremale. The purpose ofthisistoensure
gender independenc y.Then, werandomly partitioned the
60subjects intotwoseparate sets. Thetraining setcontains
54subjects andthetestsetcontains 6subjects. Inallexper-
iments, weused thedata captured from these 60subjects
with twohigh intensity models foreach facial expression.
Foreach facial expression model inthetraining andtest
sets, weextracted a96dimensional feature vector asde-
scribed inSection 3.Inthiswork,theneutral facial expres-
sion isnotclassied. Rather ,asapreprocessing stepitsfea-
tures serveasducial measures thataresubtracted from the
features ofthesixuniversal facial expressions ofthecorre-
sponding subject. Thetraining feature vectors were used to
train amulti-class SVM classier with aC++ implementa-
tionoftheSVM: theLibSVM softw are[16]. Thetestfea-
ture vectors were evaluated against thetrained SVM clas-
sier toproduce theclassication results. Byrunning the
experiment independently for10times on10random par-
titions ofthetraining andtestsets, weachie vedanaverage
recognition rateof87.1% forthesixuniversal facial expres-
sions, which isbelie vedtobeageneralization oftherecog-
nition rate. Anumerical comparison ofourresult with the
results reported in[2]isgiveninTable 2.Weseethatthere
isa3.5% absolute increase intheaverage recognition rate
inourworkascompared tothehighest average recognition
rateobtained in[2],which used elaborately extracted prim-
itivefacial surfacefeatures andanLDAclassier .Table 3
givestheaverage confusion matrix ofourresults andTa-
ble4givestheaverage confusion matrix ofthebest results
in[2](with anLDAclassier). Figure 5givesapictorial
comparison oftheexpression-specic average recognition
rates. From Figure 5,itcanbeseen thatallourresults are
better than theresults in[2]except forthefear (FE) case.
Especially ,forthecase ofsurprise (SU), ouraverage recog-
nition ratereaches to99.2%. Inthecase offear (FE), our
result isslightly worse than thatin[2]-a0.8% decrease.
Theresults in[2] Ourresult
QDC LDA NBC SVC SVM
74.5% 83.6% 71.7% 77.8% 87.1%
Table 2.Comparison oftheaverage recognition rates.
% AN DI FE HA SA SU
AN 86.7 1.7 2.5 0 9.2 0
DI 3.3 84.2 5.8 3.3 0.8 2.5
FE 5.8 5 74.2 5.8 6.7 2.5
HA 0 0 4.2 95.8 0 0
SA 12.5 0 5 0 82.5 0
SU 0 0 0.8 0 0 99.2
Table 3.Average confusion matrix ofSVM classication.
% AN DI FE HA SA SU
AN 80 1.7 6.3 0 11.3 0.8
DI 4.6 80.4 4.2 3.8 6.7 0.4
FE 0 2.5 75 12.5 7.9 2.1
HA 0 0.8 3.8 95 0.4 0
SA 8.3 2.5 2.9 0 80.4 5.8
SU 1.7 0.8 1.2 0 5.4 90.8
Table 4.Average confusion matrix ofLDAclassication in[2].
65 70 75 80 85 90 95 100ANDIFEHASASU
Aver age recognition rate (% )Expressio nCompari son of expression!specific reco gnition rate s
  
Wan g et. al.'s result s
Our re sults
Figure 5.Comparison ofexpression-specic recognition rates.
6.Conclusion
Inthis paper ,person and gender independent facial
expression recognition isperformed ontheBU-3DFE
database. Wepropose a3Dfacial expression recognition
method based onproperties ofthelinesegments connect-
ingcertain facial feature points. Thenormalized distances
andslopes ofthese linesegments comprise asetof96dis-
tinguishing features forrecognizing sixuniversal facial ex-pressions. Using amulti-class SVM classier ,weachie ve
an87.1% average recognition rate. The highest average
recognition rate obtained is99.2% fortherecognition of
surprise. Ourresult yields a3.5% absolute increase inthe
average recognition rate ascompared tothat intheprior
work[2].
Refer ences
[1]Lijun Yin,Xiaozhou Wei,YiSun, JunWang, andMatthe w
Rosato, ¬ìA3DFacial Expression Database ForFacial Be-
haviorResearch¬î, FG2006, pp.211-216.
[2]JunWang, Lijun Yin,Xiaozhou Wei,andYiSun, ¬ì3D Facial
Expression Recognition Based onPrimiti veSurfaceFeature
Distrib ution¬î, CVPR 2006.
[3]P.Jonathon Phillips, Patrick J.Flynn, ToddScruggs, Kevin
W.Bowyer ,JinChang, KevinHoffman, JoeMarques, Jaesik
Min, andWilliam Worek, ¬ìOvervie woftheFaceRecogni-
tionGrand Challenge¬î, CVPR 2005, pp.947-954.
[4]Lijun Yinand Xiaozhou Wei,¬ìMulti-Scale Primal Fea-
tureBased Facial Expression Modeling andIdentication¬î,
FG2006, pp.603-608.
[5]L.Yin,X.Wei,P.Longo, andA.Bhuv anesh, ¬ìAnalyzing Fa-
cialExpressions Using Intensity-V ariant 3DData forHuman
Computer Interaction, ¬îICPR 2006, pp.1248-1251.
[6]M.Pantic andL.J.M. Rothkrantz, ¬ìAutomatic analysis offa-
cialexpressions: thestate oftheart¬î, T-PAMI, 22(12):1424-
1445, 2000.
[7]Cowie, R.,Douglas-Co wie, E.,Tsapatsoulis, N.,Votsis, G.,
Kollias, S.,Fellenz, W.,&Taylor ,J.Emotion recognition in
human-computer interaction. IEEE Signal Processing Mag-
azine: 18(1),32-80, 2001.
[8]Fasel, B.,Luettin, J.,Automatic facial expression analysis:
Asurvey.Pattern Recognition 36(2003) 259-275
[9]Bronstein, A.M.,Bronstein, M.M, andKimmel, R.,¬ìThree-
dimensional facerecognition¬î, IJCV ,64(1):5-30, 2005.
[10] Kakadiaris, I.A., Passalis, G.,Toderici, G.,Murtuza, N.,
Karampatziakis, N.,andTheoharis, T.,¬ì3D facerecogni-
tioninthepresence offacial expressions: anannotated de-
formable model approach¬î, T-PAMI, 13(12), 2007.
[11] P.Ekman andW.Friesen, ¬ìFacial Action Coding System: A
Technique fortheMeasurement ofFacial Movement¬î, Con-
sulting Psychologists Press, PaloAlto, 1978.
[12] Ekman, P.,Huang, T.,Sejno wski, T.,andHager ,J.,¬ìFinal
Report toNSF ofthePlanning Workshop onFacial Expres-
sion Understanding¬î, Human Interaction Lab, University of
California, SanFrancisco, CA.
[13] Igor S.Pandzic, Robert Forchheimer (Eds), MPEG-4 Facial
Animation: TheStandard, Implementation andApplications,
John Wiley&Sons, Inc., 2002.
[14] B.E.Boser ,I.M.Guyon, andV.N.Vapnik. Atraining algo-
rithm foroptimal marginclassiers. InD.Haussler ,editor ,
5thAnnual ACMWorkshop onCOL T,pp.144-152, 1992.
[15] Corinna Cortes andV.Vapnik, ¬îSupport-V ector Netw orks,
Machine Learning, 20,1995
[16] Chih-Chung Chang andChih-Jen Lin, LIBSVM :alibrary
forsupport vector machines, 2001. Softw areavailable at
http://www .csie.ntu.edu.tw/ cjlin/libsvm.
"
https://ieeexplore.ieee.org/document/1613022,"A 3D Facial Expression Database  For Facial Beh avior Research 
Lijun Yin      Xiaozhou Wei      Yi Sun      Jun Wang      Matthew J. Rosato  
Department of Computer Scie nce, State University of New York at Binghamton 
 
 
Abstract 
 
Traditionally, human facial expressions have been 
studied using either 2D static images or 2D video 
sequences. The 2D-based analysis is incapable of handing 
large pose variations. Although 3D modeling techniques 
have been extensively used fo r 3D face recognition and 3D 
face animation, barely any research on 3D facial expression recognition using 3D range data has been 
reported. A primary factor fo r preventing such research is 
the lack of a publicly available 3D facial expression 
database. In this paper, we present a newly developed 3D 
facial expression database, which includes both 
prototypical 3D facial expression shapes and 2D facial 
textures of 2,500 models from 100 subjects. This is the first attempt at making a 3D facial expression database 
available for the research community, with the ultimate 
goal of fostering the research on affective computing and 
increasing the general understanding of facial behavior 
and the fine 3D structure inherent in human facial 
expressions. The new database can be a valuable resource 
for algorithm assessment, comparison and evaluation.  
 
1. Introduction 
 
Computer facial expression analysis would be highly 
beneficial for many fields including those as diverse as 
human computer interaction, security, medicine, behavior 
science, communication, and education. Currently, all 
existing face expression analysis and recognition systems rely 
primarily on static images or dynamic videos from many 2D 
facial expression databases (e.g., [19] and Table 1). Although 
some systems have been successful, the performance 
degradation remains when handling expressions with large 
head rotation, subtle skin movement, and/or lighting change with varying postures. In order to mitigate the problems 
inherent in the 2D based analysis, we propose to establish a 
new 3D facial expression database, and conduct facial 
expression analysis in a 3D space by exploring the surface 
information, which is beyond the availability from the 2D 
plane. In the following section, we will review the existing 
work, identify the critical issues to show why analyzing facial expression in a fully 3D space is necessary. 
 
 
1.1 The State of The Art 
 
     Research on automatic techniques for analyzing human 
facial behavior has been conducted for over three decades [11, 32, 34]. There are two general approaches which have been developed relying on eith er 2D information or partial 
3D information.  
       The conventional methods for facial expression 
recognition focuses on extracting the expression data needed to describe the change of facial features, such as Action Units (AUs) which are defined in the Facial Action Coding System (FACS) [11]. A number of techniques were successfully developed using 2D static images or video 
sequences, including machine vision techniques [44, 12, 10, 
21, 4, 41, 42] and machine learning techniques [1, 20, 5, 6, 
45]. The excellent review of recent advances in this field can be found in [27, 42, 46, 28, 15].        Recently, some researchers have noticed the 
importance of exploring 3D information to improve facial 
expression recognition. Some have successfully used partial 3D information, such as multiple views [29] or 3D models for facial expression analysis [2, 17, 43, 26]. These methods are based on 2D images. They can alleviate the problems caused by different head poses to a certain degree with the 
assistance of a 3D model or with multiple views of the face. 
However, since no complete 3D individual facial geometric shapes are employed, the abilit y to handle large head pose 
variation and the ability to differentiate subtle expressions 
is inherently limited.        To the best of our knowledge, little investigation  
has been conducted on analyzing facial behavior in a 
complete 3D space even though it is believed to be a better reflection of facial behavior. In the following section, we summarize several critical issues and limitations of the existing facial expression recognition systems, and show 
the advantage of 3D facial expression analysis.  
 
1.2 Why 3D: Critical Issues and Limitations of 2D 
 
(1) 3D surface features exhib ited in facial expressions  
     The common theme in the current research on face expression recognition is that the face is a flat pattern, like a 2D geometric shape associating with certain textures. This 
view has the consequences that expression variations is 
considered only in terms of measurements made on the 
picture plane. However, the common feature of faces is the three-dimensional surface rather than a two-dimensional 
pattern. Understanding the face as a mobile, bump surface 
instead of a flat pattern may have a theoretical implication as 
well as practical applications. Psychological research shows 
that the human visual system can perceive and understand 
embedded features contained in the 3D facial surface even when such features are not exhibited in corresponding 2D 
Proceedings of the 7th International Conference on  Automatic Face and Gesture Recognition (FGR‚Äô06) 
0-7695-2503-2/06 $20.00 ¬© 2006  IEEE 

plane images. It is possible that  the viewer actually 
represents the surface shape of the face when constructing 
representations for recognition  [3]. This explains why 
human recognition of 2D facial expressions is presently so 
much better than machine recognition.         The facial expression is an entire facial behavior. 
Multi-
dimensional expression space better characterizes this 
complexity [37]. Many expressions in this space exhibit subtle 
in-depth skin motion. For example, the skin extrusion in the 
areas of the cheek, forehead, glabella (in between eyebrows), 
nasolabial (in-between nose-side and mouth corners), crow-
feet (out-corners of eyes), chin or mouth exhibits these subtle 
motions. These areas contain a high number of precious 
surface features (e.g., convex, concave, or other 3D primitive 
features), and could play a critical role in distinguishing subtle 
facial expressions. However, the 2D based approaches are hard 
to detect 3D surface features and in-depth motions (e.g., wrinkles) although they are good at detecting high-contrast 
features in salient organ areas (such as eye, nose, mouth).  
       Due to the limitations in describing facial surface 
deformation when 3D features are evaluated in 2D space, 2D images with a handful of feature units may not accurately reflect the authentic facial expressions. 
Therefore, there is a great demand for representing facial 
expressions in a 3D space in order to scrutinize the facial 
behavior at the level of subtlety explored between human-
human interactions. Such a 3D-based analysis approach 
could allow us to examine the fine structure change for 
universal and complex expressions.   
(2) Pos e / Posture:  
     People rarely express emotions without head motion or 
posture spontaneity. Nevertheless, current research on facial 
expression analysis primarily focuses on the frontal view of face images, with very limited head motion or posture change. The assumption of frontal view expressions is not 
only unrealistic, but also jeopardizes the accurate 
expression analysis because head pose and posture are 
important cues, which, in conjunction with facial action, 
reflect a person's real emotion [28]. 
Large head pose change 
will cause an illumination change on the face which may cause 
part of the face to become invisible. The head motion and 
resulting occlusion increase the difficulty to track facial 
features and pose (e.g., in-depth direction) accurately and 
reliably in the 2D plane, jeopardizing the robust detection of 
AUs. Capturing 3D head orientation and analyzing facial 
expressions in 3D space has the potential to alleviate these 
problems related to pose and posture.  
 
(3) B enchmark 3D facial ex pression databas e 
     A common testing resource is essential for research on  
facial expression analysis. Although there are a number of  
popular 2D face expression databases accessible for facial 
expression research, as of yet, no readily accessible 
database of test materials for 3D expression analysis has 
been established (see Table 1). The lack of an accredited common database (like the FERET and FRGC databases for face recognition) and evaluation methodology makes it difficult to compare, validate, resolve, and extend the issues concerned with 3D facial expression analysis. Currently, a 
number of standard  face databases , containing both 2D and 
3D data (e.g., [30, 13, 14]), are available to the face 
recognition community. However, these databases were not designed systematically for the purpose of  facial expression recognition. They do not include either a whole set of prototypic expression data or 3D face expressions at 
various levels of intensity, therefore are not sufficient for 
3D face expression research.   
Data- 
Base          Face 
    Recognition   Face Expression 
    Recognition  
2D FERET [31], FRGC [30],
CMU-PIE [39], BioID[50]
AR [48],  Yale [47],   
xm2vtsdb [18]
UT-Dallas [25, 24],
Many others [46], ...Cohn-Kanade [19], 
JAFFE [21], MMI [8], 
RU-FACS-1 [9], 
Ekman-Hager [16,10] 
USC-IMSC [23], 
UT-Dallas [25, 24], 
UA-UIUC [38],  
QU [33], PICS[49],...  
3D 3D FRGC [30, 22], 
DARPA-HumanID [14], 
PRISM-ASU [13],  
xm2vtsdb [18], ‚Ä¶  None 
 
Table 1. Survey of existing databases for research on face recognition and face expression recognition 
 
       In short, the establishment of a 3D facial expression 
database is crucial to enhancing facial behavior research. 
The lack of such essentials has impeded research in this area. In the following sections, we will introduce the database creation process and the organization of the database. We will also describe the process for data validation and assessment. Finall y, the limitation and future 
extension of the current database are addressed.
 
 
2. Creation of 3D Facial  Expression Database  
 
2.1 Capturing 3D Facial Expressions 
    The development of our database was designed to sample 
facial behaviors with seven universal emotional states. Each expression is represented by multiple intensities which reflect different levels of spontaneity.   
 
 
Figure 1. 3D face imaging system setup 
Proceedings of the 7th International Conference on  Automatic Face and Gesture Recognition (FGR‚Äô06) 
0-7695-2503-2/06 $20.00 ¬© 2006  IEEE 

(1) 3D face digitizer  
     The 3D facial range data is captured with a 3D face  
imaging system (3DMD digitizer, Figure 1) [36] using a 
random light pattern projection in the speckle projection 
flash environment. The system projects a random light 
pattern onto the subject and captures his/her shape using the 
precisely synchronized digital cameras which are set at 
various angles in an optimum configuration. The six digital cameras and two light pattern projectors are positioned on 
two sides (three cameras and one projector on each side). 
The system automatically merges all six synchronized 
cameras‚Äô viewpoints data and produces a single 3D polygon 
surface mesh. Using the stereo photogrammetry technique, the 3D face surface geometry and surface texture are acquired. Each instant shot (l ess than 2 milliseconds capture 
time) outputs a set of data, including a pair of textures with two angle views and a wire-frame model. The texture size of the two-views image is around 1300 by 900 pixels. The model resolution is in the range from 20,000 polygons to 35,000 polygons, depending on the size of subject‚Äôs face.  
                    
(2) Expression scanning at work  
     Each subject is instructed to sit in front of the 3D face capture system. They are requested to perform seven universal expressions, i.e., neutral, happiness, surprise, fear, 
sadness, disgust, and angry . Although the 3D capture 
system does not capture facial expressions dynamically, we 
require the subjects to perform each expression for a short 
period of time. The scan fires four instant shots to capture 
the four different degrees of the expression. The intensity ranges from low, middle, high , and highest , and the 
capturing at approximately ten-second intervals.      Ideally, a video clip could be used for eliciting the 
authentic expressions refl ecting naturally occurring 
emotions. However, it is difficult to elicit a wide range of 
true emotions from a short video clip, especially for sadness and fear [38]. It is worth noting that archetypal emotions are 
a rare phenomenon. As quoted by Cowie et al [33], displays 
of intense emotion or ‚Äúpure‚Äù primary emotions rarely 
happened.  
     The true emotion could be developed over a long time of 
involvement in special activ ities or events. The best 
elicitation could be from scenarios such as those shown in the reality TV shows, ‚ÄúFear Factor‚Äù, ‚ÄúSurvivors‚Äù, and ‚ÄúThe 
Apprentice‚Äù. However, it is not feasible to set up a lab 
environment to obtain such authentic and spontaneous 
expressions associated with true emotions. In everyday life, most people are likely to exhibit spontaneous emotions in a very light (low)  intensity without exaggerated appearances. 
This common observation is similar to the scenario 
exhibited in the initial stage of the expression action. With 
this consideration, the subjects were asked to perform the light (low) intensity of each expression to simulate the 
spontaneity of the emotional state.      We requested each subject to perform four stages of expressions, ranging from low intensity, middle, high, and highest intensity of a specific expression. It was up to the subject to post four stages of expressions with his/her own 
style. Upon completing the face scanning, the data were 
annotated for archival as a ground truth .  
 
(3) Statistics of participan ts and expression data 
       There were 100 subjects who participated in face scans, 
including undergraduates, graduates and faculty from our institute‚Äôs departments of Psychology, Arts, and 
Engineering (Computer Science, Electrical Engineering and Mechanical Engineering). The majority of participants were undergraduates from the Psychology Department. The resulting database consists of  about 60% female and 40% 
male subjects with a variety of ethnic/racial ancestries, 
including White, Black, East-Asian, Middle-east Asian, 
Hispanic Latino, and others.      Each subject performed seven expressions. With the exception of the neutral expression, each of the six 
prototypic expressions ( happiness, disgust, fear, angry, 
surprise and sadness ) includes four levels of intensity. 
Therefore, there are 25 instant 3D expression models for 
each subject, resulting in a total of 2,500 3D facial 
expression models in the database. Associated with each expression shape model, is a corresponding facial texture 
image captured at two views (about +45 ¬∞ and -45 ¬∞). As a 
result, the database consists of 2,500 two-view‚Äôs texture 
images and 2,500 geometric shape models. 
 
2.2 Expression Data Description and Management 
 
     The expression data includes the 3D model, texture, and 
enrollment information. Along with the raw model data, 
additional semantic and surface feature data are also archived. Figure 2 shows the data structure for archival. By 
query, the data is searchable by gender, ethnicity, 
expression (emotion state), and intensity.   
Gender Race
3D scans mesh     TexturesExpression Data3D Face Expression
Subject#
Features EP1EP2EP3 EP7 .......
.... 
 
Figure 2. Data structure of 3D facial expressions for archival 
 
(1) Data processing 
     In order to make the database useful for assessing and 
comparing algorithms using 2D-based and 3D-based facial 
expression recognition techniques, we provide both facial texture images and facial shape models as the raw data in the database.       Since the raw geometric models contain the unprocessed 
head-shoulder boundaries including necks and clothing, 
which are not ‚Äúclean‚Äù, further processing was performed to 
Proceedings of the 7th International Conference on  Automatic Face and Gesture Recognition (FGR‚Äô06) 
0-7695-2503-2/06 $20.00 ¬© 2006  IEEE 

make the data easier to use. The original raw data was 
processed by truncating the boundary to generate a face 
model with the pure face region. The cropped face region 
contains about 13,000 - 21,000 polygons. In addition, a 
frontal view texture (512 by 512 pixels) is generated using 
our 3D face shape processing and warping tool. Therefore, in total, the database is composed of 2,500 raw 3D expression models, 2,500 raw textures in two-views‚Äô faces, 
2,500 cropped models and 2,500 frontal view textures of 
the face regions.  
     In addition to the geomet ric data and texture data, a set 
of associated descriptors is also generated as an optional 
data set. 
 
(2) Associated Optional Descriptor  
(a) Feature point set:  We picked 83 feature vertices on  
each facial model (Figure 4 (row 1)). Given the set of 
feature points on the face model labeled, the feature regions 
on the face surface can be easily determined. These features could be used as a ground truth to assess algorithms for 3D 
model segmentation and 3D feature detection. 
(b) 3D face pose:  The obtained models contain various 
poses. We provide the model orientation using a normal 
vector with respect to the frontal projection plane. Given 
three vertices picked from two eye corners and a nose 
center, a triangle plane is formed. The norm of this plane 
represents the original face pose. The database includes such data for pose-related algorithm assessment.   
 
Raw data (Figure 3) Produced data (Figure 4) 
2,500 face shape models 2,500 cropped face  
regional  shape models  
2,500 face textures  
(two views)  2,500 frontal texture of  facial regions
 
 2,500 data sets of  
facial feature points  
 2,500 data sets of the  
original facial poses   
 
Table 2. Summary of the archived data including the raw data and the processed data.  
    
   In summary, the amount of 3D facial expression data 
archived in the database is list ed in the Table 2. Note that 
since the database is designed to be available to public 
research, researchers in different areas can test their 
algorithms against the database and update or expand the dataset by adding new features in the future.  
 
3. Validation and Evaluation of the Database  
 
      The quality of the 3D face expression database is 
evaluated through the validation experiments. The 
validation study addresses th e question of whether the 
interpretations by machines are equal to those given by 
observers or performers. To do so, we conducted an analysis and test against our 3D expression database. Each expression data set was analyzed three times. Firstly, by the subject who performed the expression (as ground truth). 
Secondly, by observers from the Psychology Department 
who are experts in interpreting facial expressions (as expert votes). Thirdly, using machines via our facial expression recognizer (as machine votes). The following sub-sections report the statistical results of the expert evaluation and computer recognition.  
 
3.1 Subjective votes by observers 
 
As described in Section 2, the subjects provided the 
validation results for each expression with four intensities. Given such ground truth data, we compare the results by the 
subjective votes from two psychologists of Psychology 
Department. The confusion matrix is reported in the Table 3. 
The average expert recognition rate is 94.1% for low 
intensity expressions, 95.7% for middle intensity, 96.8% for high intensity, and 98.1% for highest intensity expressions. The most likely confused expressions were sad-fear and 
disgust -angry, even for experts .  
 
In/Out Ang  Dis Fea Hap Sad Sur Neu 
Anger 94.9 2.5 1.2 0 0.3 0.2 0.9 
Disgust 2.6 95.4 0.9 0 0.9 0 0.2 
Fear 0.1 0.5 96.4 0 2.4 0.1 0.5 
Happy 0.1 0 0.1 99.4 0 0.4 0 
Sad 1.0 0.2 2.4 0 96.2 0 0.2 
Surprise 0.4 0 0.2 0.4 0 99.0 0 
Neutral 0.8 0 0.2 0 0.3 0 98.7 
 
Table 3. Confusion matrix of expert voting averagely for four intensities of expressions (%). 
 
3.2 Objective votes by machine classification 
 
   To validate the created 3D facial expression database, we 
conducted experiments on face expression recognition 
using our newly developed 3D face expression 
representation and classification algorithm. The basic algorithm is outlined as follows: (details in the report [7]).  
     Given the set of expression range models, in order to 
better characterize 3D features of the facial surface, each 
vertex on the individual model is labeled by one of the 
twelve primitive surface. Our labeling approach is based on the estimation of principal curvatures of the facial surface. It is believed that  the curvature information is a good 
reflection of local shape of the facial surface [40]. 
     In order to classify the facial expressions based on the 
3D facial expression data, we segment the 3D face surface into seven local expressive re gions (excluding interiors of 
mouth, interior of eyes and nose bridge), and conduct the histogram statistics on each region in terms of the twelve primitive surface label distribution. Each expressive region 
forms a twelve-dimension feature vector, in which each 
Proceedings of the 7th International Conference on  Automatic Face and Gesture Recognition (FGR‚Äô06) 
0-7695-2503-2/06 $20.00 ¬© 2006  IEEE 

element is defined as a ratio of the number of vertices with 
a specific label type to the number of vertices in the local region. As such, an 84-dimension feature vector is 
constructed on the entire facial region. The facial 
expression surface labels exhibit different patterns which correspond to different facial expressions. Such feature 
vectors are used for expression classification. 
     We conducted facial expression recognition using pure 3D geometric shape models from our 3D facial expression 
database. The experiment is person-independent, which 
means the query subject has never appeared in the training set. We applied linear discriminant analysis (LDA) classifier to classify the prototypic facial expressions of 
sixty subjects. The correct recognition rate is about 83.6%.  
     
a
 
b 
c  
a‚Äô  
b‚Äô  
c‚Äô   
 
Figure 3: Sample expressions: Left four (happiness) and 
right four (surprise) with four levels of intensity. a-a‚Äô are 
raw models; b-b‚Äô are cropped shape models in face 
regions. c-c‚Äô are two views‚Äô textures.  
 
4. Limitation and Development of Database 
 
    There are several limitations in  the current version of the 
database in terms of dynamics, FACS-related coding and 
the expression variety in the expression space. Limited by 
the speed of 3D imaging capture system and post 
processing load, no 3D dynamic expressions are captured in 
the current version. The number of expression types is still limited to the prototypic expression space, more spontaneous expressions need to be included for naturally occurring emotion analysis. Our future work will focus on the following aspects:  
 
 
 
 
 
 
 
 
 
 
Figure 4: Four sample subjects showing seven 
expressions (neutral, angry, disgust, fear, happiness, 
sadness, and surprise). The facial shape model and 
frontal-view textures are produced. The first row shows 
a sample set of the picked feature points. 
 
(1) Dynamics:  We will extend the database to include 
dynamic 3D facial expression sequences using a real-time dynamic range system, with a super-high resolution model representation. As such, the 3D action units coding and 
labeling could be further explored.  
 (2) Expression space:  We will include more spontaneous 
3D expression data with more affects states (such as 
boredom, skepticism, shame, etc.) through eliciting 
children/adults emotion response with the experiments 
designed and guided by our collaborated psychologists.  
 
 (3) Applications in medical and psychological research:  
We will be interested in the study of the clinically interested 
data for diagnosis purpose. For example, reading pain 
expressions when the self-report is not possible for people like non-
communicative adults, developmentally delayed 
children or newborns. We will also extend the 3D facial 
expression database to the emerging field of applications, such as using 3D expression models as a source of stimuli for the 
psychological research to diagnose, assess and rehabilitate 
Proceedings of the 7th International Conference on  Automatic Face and Gesture Recognition (FGR‚Äô06) 
0-7695-2503-2/06 $20.00 ¬© 2006  IEEE 

patients with brain or psychological disorders (e.g., alzheimer, 
etc.) [35].  
 
5. Conclusion 
 
    We have developed a new 3D facial expression database 
for the scientific research community. This is the first 
attempt to foster the research on analyzing the facial 
behavior in a complete 3D space, targeting the 
identification of more detail and subtle facial behavior. The future challenge is to fu rther develop the 3D facial 
expression database using the dynamic and spontaneous 3D 
high-resolution expression data in order to move closer to 
developing a naturally occurring facial behavior analysis 
system.  
  
Acknowledgement  
 
        This material is based upon the work supported in 
part by the National Science Foundation under grants IIS-
0541044, IIS-0414029, and the NYSTAR‚Äôs James D. 
Watson Investigator Program. We would like thank Gina 
Shroff, Peter Gerhardstein, Joseph Morrissey of the Department of Psychology, Lee Serversky and Ben Myerson of the Computer Science Department for the help during the process of creating the database.  
     
References 
 
[1] M. Bartlett, J. Hager, P. Ekman, and T. Sejnowski. Measuring facial 
expressions by computer image analysis. Psychophysiology, 36, 1999.  [2] B. Braathen, M. Bartlett, G. Littlewort, et al. An approach to automatic 
recognition of spontaneous facial actions. FGR 2002. 
[3] V Bruce, M. Burton, and T. Doyle. Faces as surfaces. Processing 
Images of Faces, 1992. 
[4] Y Chang, C. Hu, and M. Turk. Probabilistic expression analysis on 
manifolds. CVPR‚Äô04, Washington DC, 2004.  
[5] I. Cohen, F. Cozman, N. Sebe, M. Cirelo, and T. Huang. Semi-
supervised learning of classiÔ¨Åers: Theory, algorithms for Bayesian network 
classiÔ¨Åers and application to human-computer interaction. IEEE Trans. 
PAMI, 26(12), 2004.  
[6] I. Cohen, N. Sebe, A. Garg, L. Chen, and T. Huang. Facial expression 
recognition from video sequences: temporal and static modeling. CVIU, 
91(1), 2003.  
[7] J. Wang, L. Yin, et al, ‚Äú3D facial expression recognition based on 
primitive surface feature distribution‚Äù, Tech. Report, Binghamton U, 2006.   
[8] Man machine interaction group. http://www.mmifacedb.com/. Delft University of Technology, 2005.  
[9] RU-FACS-1 Database. http://mplab.ucsd.edu/databases/databases.html.  
[10] G. Donato, M. Bartlett, J. Hager, P. Ekman, and T. Sejnowski. 
Classifying facial actions. IEEE Trans. PAMI, 21(10):974-989, 1999.  
[11] P. Ekman and W. Friesen. Facial Action Coding System. New York: 
Consulting Psychologists Press, 1977.  
[12] I. Essa and A. Pentland. Coding, analysis, interpretation, and 
recognition of facial expressions. IEEE Trans. PAMI, 19(7), 1997.  
[13]  PRISM-ASU. http://prism.asu.edu/3dface/default.asp .  
[14]  USF DARPA Human ID 3D face database. Courtesy of Prof. Sudeep 
Sarkar, University of South Florida, Tampa, FL.  
[15]  B. Fasel and J. Luettin. Automatic facial expression analysis: A 
survey. Pattern Recognition, 36(1), 2003.  
[16]  W. Friesen and P. Ekman. Dictionary - Interpretation of FACS Scoring. Unpublished manuscript, UC San Francisco, 1987.  [17]  S. Gokturk, J. Bouguet, C. Tomasi, and B. Girod. Model-based face tracking for view-independent facial expression recognition. In FGR 2002. 
[18] K Messer, J Matas, J Kittler, et al. Xm2vtsdb: The extended m2vts 
database. In International Conference of AVBPA, March 1999. 
http://www.ee.surrey.ac.uk/Research/VSSP/xm2vtsdb/   
[19] T. Kanade, J.Cohn, and Y. Tian. Comprehensive database for facial 
expression analysis. FGR‚Äô00, France, 2000.  
[20]  G. Littlewort, M. Bartlett, I. Fasel, J. Susskind, and J. Movellan. 
Dynamics of facial expression extracted automatically from video. In 
CVPR Workshop on FPIV'04, 2004.  
[21]  M. Lyons, et al. Automatic classiÔ¨Åcation of single facial images. 
IEEE Trans. PAMI, 21(12):1357-1362, 1999.  
[22] K. Chang and K. Bowyer and P. Flynn. An evaluation of multimodal 
2D+3D face biometrics, IEEE Trans. on PAMI. 27(4): 619-624. 2005. 
[23]  U. Neumann. Facial expression analysis and synthesis (NSF report). 
http://imsc.usc.edu/research/project/facialexp/ . 
[24]  A. O'Toole. Psychological and neural perspectives in human face 
recognition. In The Handbook of Face Recognition, 2004, Springer-Verlag. 
Editors: S. Li and A. Jain.  
[25]  A. O'Toole, J. Harms, et al. A video database of moving faces and 
people. IEEE Trans. PAMI, 27(5), 2005.  [26]  L. Zalewski and S. Gong. Synthesis and recognition of facial 
expressions in virtual 3D views. In FGR‚Äô04, 2004.  
[27]  M. Pantic and L. Rothkrantz. Automatic analysis of facial 
expressions: the state of the art. IEEE Trans. PAMI, 22(12), 2000. 
[28]  M. Pantic and L. Rothkrantz. Toward an affect-sensitive multimodal 
human-computer interaction. Proceedings of IEEE, 91(9):1370-1390, 2003.  
[29]  M. Pantic and L. Rothkrantz. Facial action recognition for facial 
expression analysis from static face images. IEEE Trans. on SMC Part B: 
Cybernetics, 34(3):1449-1461, 2004.  
[30]  P. Phillips, P. Flynn, T. Scruggs, K. Bowyer, J. Chang, K. Hoffman, 
J. Marques, J. Min, and W. Worek. Overview of the face recognition grand 
challenge. CVPR05, San Diego, CA, 2005.  
[31]  P. Phillips, H. Moon, P. Rauss, et al., The FERET evaluation metho-
dology for face recognition algorithm. IEEE Trans. PAMI, 22 (10), 2000.  
[32]  R. Picard. Affective computing: challenges. Inter. Journal of Human 
Computer Studies, 59(1-2):55-64, 2003.  
[33] E. Douglas-Cowie, R. Cowie and M. Schroder. A new emotion 
database: considerations, sources and scope. Proc. of the ISCA ITRW on Speech and Emotion, Newcastle, 2000, pp. 39-44. 
[34] R.Cowie, E. Douglas-Cowie, et al.  Emotion Recognition in Human 
Computer Interaction. IEEE Signal Processing Magazine 18 (1). 2001. 
[35] A. Rizzo. Virtual reality and disability: emergence and challenge. 
Disability and Rehabilitation, 24(11), 2002.  
[36] 3DMD Inc., http://www.3dmd.com , 2005. 
[37] J. Russell. Is there universal recognition of emotion from facial 
expression? Psychological Bulletin, 115(1):102-141, 1994.  
[38]  N. Sebe, M. Lew, I. Cohen, Y Sun, T. Gevers, and T. Huang. 
Authentic facial expression analysis. In FGR 2004. 
[39] T. Sim, S. Baker, and M. Bsat. The CMU pose, illumination and 
expression database. IEEE Trans. PAMI, 25(12), 2003.  
[40]  H. Tanaka, M. Ikeda, and H. Chiaki. Curvature-based face surface 
recognition using spherical correlation. In FGR‚Äô1998.  
[41] F. Bettinger and T.F.Cootes. A Model of Facial Behavior. In FGR‚Äô04. 
[42] Y Tian, T. Kanade, and J. Cohn. Recognizing action units for facial 
expression analysis. IEEE Trans. on PAMI, 23(2), 2001.  
[43]  Z. Wen and T. Huang. Capturing subtle facial motions in 3D face 
tracking. In IEEE Inter. Conf. on Computer Vision, 2003.  
[44]  Y. Yacoob and L. Davis. Recognizing human facial expressions from 
long image sequences using optical Ô¨Çow. IEEE Trans. PAMI, 18 (6), 1996.  
[45]  Y Zhang and Q. Ji. Active and dynamic information fusion for facial 
expression understanding from image sequences. IEEE Trans. PAMI, 27 
(5):699-714, May 2005.  
[46]  W. Zhao, R. Chellappa, P. Phillips, and A. Rosenfeld. Face 
recognition: A literature survey. ACM Computing Surveys, 35(4), 2003.  
[47] http://cvc.yale.edu/projects/yalefaces/yalefaces.html  
[48] http://rvl1.ecn.purdue.edu/~aleix/aleix_face_DB.html  
[49] PICS database, http://pics.psych.stir.ac.uk/index.html  
[50]http://www.humanscan.de/support/downloads/facedb.php  
Proceedings of the 7th International Conference on  Automatic Face and Gesture Recognition (FGR‚Äô06) 
0-7695-2503-2/06 $20.00 ¬© 2006  IEEE 

"
https://ieeexplore.ieee.org/document/5975141,"A Multimodal Database for
Affect Recognition and Implicit Tagging
Mohammad Soleymani, Member ,IEEE , Jeroen Lichtenauer,
Thierry Pun, Member ,IEEE , and Maja Pantic, Fellow ,IEEE
Abstract ‚ÄîMAHNOB-HCI is a multimodal database recorded in response to affective stimuli with the goal of emotion recognition and
implicit tagging research. A multimodal setup was arranged for synchronized recording of face videos, audio signals, eye gaze data,
and peripheral/central nervous system physiological signals. Twenty-seven participants from both genders and different cultural
backgrounds participated in two experiments. In the first experiment, they watched 20 emotional videos and self-reported their felt
emotions using arousal, valence, dominance, and predictability as well as emotional keywords. In the second experiment, short videos
and images were shown once without any tag and then with correct or incorrect tags. Agreement or disagreement with the displayed
tags was assessed by the participants. The recorded videos and bodily responses were segmented and stored in a database. The
database is made available to the academic community via a web-based system. The collected data were analyzed and single
modality and modality fusion results for both emotion recognition and implicit tagging experiments are reported. These results show the
potential uses of the recorded modalities and the significance of the emotion elicitation protocol.
Index Terms ‚ÄîEmotion recognition, EEG, physiological signals, facial expressions, eye gaze, implicit tagging, pattern classification,
affective computing.
√á
1I NTRODUCTION
ALTHOUGH the human emotional experience plays a
central part in our lives, our scientific knowledge about
human emotions is still very limited. Progress in the field ofaffective sciences is crucial for the development ofpsychology as a scientific discipline or application that
has anything to do with humans as emotional beings. More
specifically, the application of human-computer interactionrelies on knowledge about the human emotional experi-ence, as well as on knowledge about the relation betweenemotional experience and affective expression.
An area of commerce that could obviously benefit from
an automatic understanding of human emotional experi-ence is the multimedia sector. Media items such as movies
and songs are often primarily valued for the way in which
they stimulate a certain emotional experience. While itmight often be the affective experience that a person islooking for, media items are currently primarily tagged by
their genre, subject or their factual content. Implicit affectivetagging through automatic understanding of an individual‚Äôs
response to media items would make it possible to rapidly
tag large quantities of media, on a detailed level and in a
way that would be more meaningful to understand how
people experience the affective aspects of media content [1].
This allows more effective content retrieval, required to
manage the ever-increasing quantity of shared media.
To study human emotional experience and expression in
more detail and on a scientific level, and to develop andbenchmark methods for automatic recognition, researchers
are in need of rich sets of data of repeatable experiments [2].
Such corpora should include high-quality measurements ofimportant cues that relate to the human emotional experi-ence and expression. The richness of the human emotionalexpressiveness poses both a technological as well as a
research challenge. This is recognized and represented by
an increasing interest into pattern recognition methods forhuman behavior analysis that can deal with the fusion ofmeasurements from different sensor modalities [2]. How-ever, obtaining multimodal sensor data is a challenge in
itself. Different modalities of measurement require different
equipment, developed and manufactured by differentcompanies, and different expertise to set up and operate.The need for interdisciplinary knowledge as well astechnological solutions to combine measurement data from
a diversity of sensor equipment is probably the main reason
for the current lack of multimodal databases of recordingsdedicated to human emotional experiences.
To contribute to this need for emotional databases and
affective tagging, we have recorded a database of multi-modal recordings of participants in their response to
affectively stimulating excerpts from movies and images
and videos with correct or incorrect tags associated withhuman actions. The database is freely available to the42 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 3, NO. 1, JANUARY-MARCH 2012
.M. Soleymani and T. Pun are with the Computer Vision and Multimedia
Laboratory, Computer Science Department, University of Geneva, BattelleCampus, Building A, Rte. de Drize 7, Carouge (GE) CH-1227, Switzer-land. E-mail: {mohammad.soleymani, thierry.pun}@unige.ch.
.J. Lichtenauer is with the Department of Computing, Imperial College
London, 180 Queen‚Äôs Gate, London SW7 2AZ, United Kingdom.E-mail: j.lichtenauer@imperial.ac.uk.
.M. Pantic is with the Department of Computing, Imperial College London,
180 Queen‚Äôs Gate, London SW7 2AZ, United Kingdom, and the Faculty ofElectrical Engineering, Mathematics and Computer Science (EEMCS),University of Twente, Drienerlolaan 5, Enschede 7522 NB, The Nether-
lands. E-mail: m.pantic@imperial.ac.uk.
Manuscript received 12 Nov. 2010; revised 1 July 2011; accepted 6 July 2011;
published online 28 July 2011.Recommended for acceptance by B. Schuller, E. Douglas-Cowie, and A. Batliner.
For information on obtaining reprints of this article, please send e-mail to:
taffc@computer.org, and reference IEEECS Log NumberTAFFCSI-2010-11-0112.Digital Object Identifier no. 10.1109/T-AFFC.2011.25.
1949-3045/12/$31.00 /C2232012 IEEE Published by the IEEE Computer Society
academic community, and is easily accessible through a
web-interface.1The recordings for all excerpts are anno-
tated through an affective feedback form, filled in by the
participants immediately after each excerpt. A summary of
the MAHNOB-HCI database characteristics is given inTable 1. The recordings of this database are preciselysynchronized and its multimodality permits researchers to
study the simultaneous emotional responses using different
channels. Two typical sets including responses to emotionalvideos and implicit tagging or agreement with displayedtags can be used for both emotion recognition as well asmultimedia tagging studies. Emotion recognition and
implicit tagging baseline results are given for researchers
who are going to use the database. The baseline results set atarget for the researchers to reach.
In Section 2, we give an overview of existing affective
databases, followed by descriptions of the modalities wehave recorded in our database in Section 3. Section 4explains the experimental setup. The first experimentparadigm, some statistics and results of classifications ofemotions are presented in Section 5 and for the secondexperiment in Section 6. A discussion on the use of thedatabase and recommendations for recordings of suchdatabases are given in Section 7, followed by our conclu-sions in Section 8.
2B ACKGROUND
Creating affective databases is an important step in emotion
recognition studies. Recent advances in emotion recognition
have motivated the creation of novel databases containing
emotional expressions. These databases mostly includespeech, visual, or audio-visual data [5], [6], [7], [8].The visual modality of the emotional databases includesface and/or body gestures. The audio modality carries
acted or genuine emotional speech in different languages.
In the last decade, most of the databases consisted only of
acted or deliberately expressed emotions. More recently,
researchers have begun sharing spontaneous and naturalemotional databases such as [6], [7], [9]. We only review the
publicly available spontaneous or naturalistic databases
and refer the reader to the following review [2] for posed,
audio, and audio-visual databases.
Pantic et al. created the MMI web-based emotional
database of posed and spontaneous facial expressions withboth static images and videos [5], [10]. The MMI database
consists of images and videos captured from both frontal and
profile view. The MMI database includes data from 61 adults
acting different basic emotions and 25 adults reacting to
emotional videos. This web-based database gives an option of
searching in the corpus and is downloadable.
2
One notable database with spontaneous reactions is the
Belfast database (BE) created by Cowie et al. [11]. The BE
database includes spontaneous reactions in TV talk shows.
Although the database is very rich in body gestures and
facial expressions, the variety in the background makes the
data a challenging data set of automated emotion recogni-
tion. The BE database was later included in a much largerensemble of databases in the HUMAINE database [6]. The
HUMAINE database consists of three naturalistic and six
induced reaction databases. Databases vary in size from 8 to
125 participants and in modalities, from only audio-visual
to peripheral physiological signals. These databases were
developed independently at different sites and collected
under the HUMAINE project.
The ‚ÄúVera am Mittag‚Äù (VAM) audio-visual database [7]
is another example of using spontaneous naturalistic
reactions during a talk show to develop a database. Twelve
hours of audio-visual recordings from a German talk show,
‚ÄúVera am Mittag,‚Äù were segmented and annotated. The
segments were annotated using valence, activation, anddominance. The audio-visual signals consist of the video
and utterances from 104 different speakers.
Compared to audio-visual databases, there are fewer
publicly available affective physiological databases. Healeyand Picard recorded one of the first affective physiologicaldata sets at MIT, which has reactions of 17 drivers underdifferent levels of stress [4]. Their recordings include
electrocardiogram (ECG), galvanic skin response (GSR)
recorded from hands and feet, electromyogram (EMG) fromthe right trapezius, as well as the respiration pattern. Thedatabase of stress recognition in drivers is publicly availablefrom Physionet.
3
The Database for Emotion Analysis using Physiological
Signals (DEAP) [9] is a recent database that includes
peripheral and central nervous system physiological signals
in addition to face videos from 32 participants. The face
videos were only recorded from 22 participants. EEGsignals were recorded from 32 active electrodes. Peripheral
nervous system physiological signals were EMG, electro-
ocologram (EOG), blood volume pulse (BVP) using plethys-
mograph, skin temperature, and GSR. The spontaneousSOLEYMANI ET AL.: A MULTIMODAL DATABASE FOR AFFECT RECOGNITION AND IMPLICIT TAGGING 43
1. http://mahnob-db.eu.TABLE 1
MAHNOB-HCI Database Content Summary
2. http://www.mmifacedb.com/.
3. http://www.physionet.org/pn3/drivedb/.
reactions of participants were recorded in response to music
video clips. This database is publicly available on theInternet.
4The characteristics of the reviewed databases are
summarized in Table 2.
3M ODALITIES AND APPARATUS
3.1 Stimuli and Video Selection
Although the most straightforward way to represent an
emotion is to use discrete labels such as fear or joy, label-based representations have some disadvantages. Specifi-cally, labels are not cross-lingual: Emotions do not haveexact translations in different languages, e.g., ‚Äúdisgust‚Äùdoes not have an exact translation in Polish [12]. Psychol-
ogists therefore often represent emotions or feelings in an
n-dimensional space (generally 2 or 3D). The most famoussuch space, which is used in the present study andoriginates from cognitive theory, is the 3D valence-arousal-dominance or pleasure-arousal-dominance (PAD)
space [13]. The valence scale ranges from unpleasant to
pleasant. The arousal scale ranges from passive to active orexcited. The dominance scale ranges from submissive (or‚Äúwithout control‚Äù) to dominant (or ‚Äúin control, empow-ered‚Äù). Fontaine et al. [14] proposed adding a predictabilitydimension to PAD dimensions. Predictability level de-
scribes to what extent the sequence of events is predictable
or surprising for a viewer.
In a preliminary study, 155 video clips containing movie
scenes manually selected from 21 commercially producedmovies were shown to more than 50 participants; each
video clip received 10 annotations on average [15]. The
preliminary study was con ducted utilizing an online
affective annotation system in which the participantsreported their emotions in response to the videos playedby a web-based video player.
In the preliminary study, the participants were thus
asked to self-assess their emotion by reporting the feltarousal (ranging from calm to excited/activated) andvalence (ranging from unpleasant to pleasant) on ninepoints scales. SAM Manikins were shown to facilitate theself-assessments of valence and arousal [16]. Fourteen video
clips were chosen based on the preliminary study from the
clips which received the highest number of tags in differentemotion classes, e.g., the clip with the highest number ofsad tags was selected to induce sadness. Three otherpopular video clips from online resources were added to
this set (two for joy and one for disgust). Three past weather
forecast reports (retrieved from youtube.com) were alsoused as neutral emotion clips. The videos from online
resources were added to the data set to enable us to
distribute some of the emotional video samples with themultimodal database described below. The full list ofvideos is given in Table 3.
Ultimately, 20 videos were selected to be shown which
were between 34.9 and 117 s long ( M¬º81:4s;SD ¬º22:5s).
Psychologists recommended videos from 1 to 10 minuteslong for elicitation of a single emotion [17], [18]. Here, the
video clips were kept as short as possible to avoid multiple
emotions or habituation to the stimuli while keeping themlong enough to observe the effect.
3.2 Facial Expressions and Audio Signals
One of the most well-studied emotional expression chan-
nels is facial expressions. A human being uses facial
expressions as a natural mean of emotional communication.
Emotional expressions are also used in human-human
communication to clarify and stress what is said, to signal
comprehension, disagreement, and intentions, in brief, to
regulate interactions with the environment and other
persons in the vicinity [19], [20]. Automatic analysis offacial expression is an interesting topic from both scientific
and practical point of view. It has attracted the interest of
many researchers since such systems will have numerous
applications in behavioral science, medicine, security, and
human-computer interaction. To develop and evaluate suchapplications, large collections of training and test data are
needed [21], [22]. In the current database, we are interested
in studying the spontaneous responses of participants while44 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 3, NO. 1, JANUARY-MARCH 2012
TABLE 2
The Summary of the Characteristics of the Emotional Databases Reviewed
TABLE 3
The Video Clips Listed with Their Sources
The listed emotional keywords were chosen by polling over participants‚Äô
self-reports in the preliminary study. 4. http://www.eecs.qmul.ac.uk/mmv/data sets/deap/.
watching video clips. This can be used later for emotional
implicit tagging of multimedia content.
Fig. 1 shows the synchronized views from the six different
cameras. Two types of cameras have been used in the
recordings: one Allied Vision Stingray F-046C, color camera(C1), and five Allied Vision Stingray F-046B, monochrome
cameras (BW1 to BW5). All cameras recorded with a
resolution of 780/C2580pixels at 60 frames per second. The
two close up cameras above the screen give a near-frontal
view of the face in color Fig. 1a or monochrome Fig. 1b. The
monochrome views have a better sharpness and less motionblur than the color camera. The two views from the bottom of
the screen, Figs. 1c and 1d, give a close up view that may be
more useful during down-facing head poses, and make it
possible to apply passive stereo imaging. For this, the
intrinsic and extrinsic parameters of all cameras have beencalibrated. Linear polarizing filters were applied with the
two bottom cameras in order to reduce the reflection of the
computer screen in eyes and glasses. The profile view Fig. 1ecan be used to extract backward-forward head/body move-
ments or to aid the extraction of facial expressions, together
with the other cameras. The wide-angle view Fig. 1f capturesthe upper body, arms and hands, which can also carry
important information about a person‚Äôs affective state.
Although we did not explicitly ask the participants to
express or talk during the experiments, we expected some
natural utterances and laughter in the recorded audio signals.
The audio was recorded for its potential to be used for videotagging, e.g., it has been used to measure the hilarity of videos
by analyzing a user‚Äôs laughter [23]. However, the amount of
laughter and audio responses in the database from partici-pants is not enough for such studies and therefore the audio
signals were not analyzed. The recorded audio contains two
channels. Channel one (or ‚Äúleft‚Äù if interpreted as a stereostream) contains the audio signal from a AKG C 1000 S MkIII
room microphone, which includes the room noise as well asthe sound of the video stimuli. Channel two contains the
audio signal from an AKG HC 577 L head-worn microphone.
3.3 Eye Gaze Data
The Tobii X1205eye gaze tracker provides the position of
the projected eye gaze on the screen, the pupil diameter, the
moments when the eyes were closed, and the instantaneousdistance of the participant‚Äôs eyes to the gaze tracker device.The eye gaze data were sampled at 60 Hz due to instabilityof the eye gaze tracker system at 120 Hz. The blinkingmoments are also extractable from eye gaze data by finding
the moments in the eye gaze responses where the
coordinates are equal to /C01. Pupil diameter has been
shown to change in different emotional states [24], [25].Examples of eye gaze responses are shown in Fig. 2.
3.4 Physiological Signals
Physiological responses (ECG, GSR, respiration amplitude,and skin temperature) were recorded with a 1,024 Hzsampling rate and later downsampled to 256 Hz to reducethe memory and processing costs. The trend of the ECG and
GSR signals was removed by subtracting the temporal low
frequency drift. The low frequency drift was computed bysmoothing the signals on each ECG and GSR channels witha 256 points moving average.
GSR provides a measure of the resistance of the skin by
positioning two electrodes on the distal phalanges of themiddle and index fingers and passing a negligible currentthrough the body. This resistance decreases due to an
increase of perspiration, which usually occurs when one is
experiencing emotions such as stress or surprise. Moreover,Lang et al. discovered that the mean value of the GSR isrelated to the level of arousal [26].
ECG signals were recorded using three sensors attached
on the participants‚Äô body. Two of the electrodes wereplaced on the chest‚Äôs upper right and left corners below theclavicle bones and the third electrode was placed on theabdomen below the last rib for setup simplicity. This setup
allows precise identification of heart beats and conse-
quently to compute heart rate (HR).
Skin temperature was recorded by a temperature sensor
placed participant‚Äôs little finger. The respiration amplitudewas measured by tying a respiration belt around theabdomen of the participant.
Psychological studies regarding the relations between
emotions and the brain are uncovering the strong implica-tion of cognitive processes in emotions [27]. As a result, theEEG signals carry valuable information about the partici-
pants‚Äô felt emotions. EEG signals were recorded using
active AgCl electrodes placed according to the international10-20 system. Examples of peripheral physiological re-sponses are shown in Fig. 2.
4E XPERIMENTAL SETUP
4.1 Experimental Protocol
As explained above, we set up an apparatus to record facial
videos, audio and vocal expressions, eye gaze, and
physiological signals simultaneously. The experiment wasSOLEYMANI ET AL.: A MULTIMODAL DATABASE FOR AFFECT RECOGNITION AND IMPLICIT TAGGING 45
Fig. 1. Snapshots of videos captured from six cameras recording facial
expressions and head pose.
5. http://www.tobii.com.
controlled by the Tobii studio software. The Biosemi active
II system6with active electrodes was used for physiological
signals acquisition. Physiological signals including ECG,
EEG (32 channels), respiration amplitude, and skin
temperature were recorded while the videos were shown
to the participants. In the first experiment, five multiple
choice questions were asked during the self-report for each
video. For the second experiment, where the feedback was
limited to yes and no, two big colored buttons (red andgreen) were provided.
Thirty participants with different cultural backgrounds
volunteered to participate in response to a campus wide call
for volunteers at Imperial College, London. Out of the
30 young healthy adult participants, 17 were female and 13
were male; ages varied between 19 to 40 years old
(M¬º26:06;SD ¬º4:39). Participants had different educa-
tional background, from undergraduate students to post-
doctoral fellows, with different English proficiency from
intermediate to native speakers. The data recorded from
three participants (P9, P12, P15) were not analyzed due to
technical problems and unfinished data collection. Hence,
the analysis results of this paper are only based on the
responses recorded from 27 participants.
4.2 Synchronized Setup
An overview of the synchronization in the recording setup
is shown in Fig. 3. To synchronize between sensors, wecentrally monitored the timings of all sensors, using aMOTU 8pre
7audio interface (‚Äúc‚Äù in Fig. 3) that can sampleup to eight analog inputs simultaneously. This allowed the
derivation of the exact temporal relations between events ineach of the eight channels. By recording the external cameratrigger pulse signal (‚Äúb‚Äù in Fig. 3) in a parallel audio track
(see the fifth signal in Fig. 4), each recorded video frame
could be related to the recorded audio with an uncertaintybelow 25/C22s. More details about the data synchronization
can be found in [28].
The gaze tracking data and physiological signals were
recorded with separated capture systems. Because neither of46 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 3, NO. 1, JANUARY-MARCH 2012
Fig. 3. Overview of our synchronized multisensor data capture system,
consisting of (a) a physiological measurement device, (b) videocameras, (c) a multichannel A/D converter, (d) an A/V capture PC,(e) microphones, (f) an eye gaze capture PC, (g) an eye gaze tracker,and (h) a photo diode to capture the pulsed IR-illumination from the eye
gaze tracker. Camera trigger was recorded as audio and physiological
channels for synchronization.
6. http://www.biosemi.com.
7. http://www.motu.com/products/motuaudio/8pre.
Fig. 2. Natural expressions to a fearful (on the left) and disgusting (on the right) video. The snapshots of the stimuli videos with eye gaze overlaid
and without eye gaze overlaid, frontal captured video, raw physiological signals, and raw eye gaze data are shown. In the first row, the red circlesshow the fixation points and their radius indicates the time spent in each fixation point. The red lines indicate the moments where each of thesnapshots was captured.
them allowed the recording of the actual sensor trigger
signals, they required alternative synchronization strategies.
The physiological data were captured with a multichannel
A/D converter (‚Äúa‚Äù in Fig. 3) that allowed recording one
binary input signal alongside the data. This input was used to
connect the camera trigger signal. Since the accurate timing
of each camera frame is known, this allowed synchronizing
the physiological data with all the other modalities.
The eye gaze tracker (‚Äúg‚Äù in Fig. 3) synchronized with
the CPU cycle counter of its dedicated capture PC (‚Äúf‚Äù) with
an accuracy of approximately one millisecond. To synchro-
nize the respective CPU cycle counter to the audio interface,
we developed an application that periodically (twice per
second) outputs binary time stamp signals with the current
time, through the serial port output (see the third signal in
Fig. 4), with an error below 10/C22s. To get a more accurate
timing accuracy than the 1 ms accuracy of the time stamps
of the gaze tracking data, the infrared strobe illumination of
the gaze tracker was recorded using a photo diode (‚Äúh‚Äù inFig. 3 and the fourth signal in Fig. 4). This allowed the
correction of the gaze data time stamps with the temporal
resolution as high as 10 microseconds.
The start moments of the stimuli data were time stamped
using the same synchronized CPU cycle counter as the eye-gaze data. An uncertainty in timing of the stimuli data is
introduced by the video player software, as well as by the
latency of the audio system, graphics card, and the screen.
Furthermore, the accuracy of the time codes of the
fragments may introduce further errors in synchronizing
the recorded data with the actual stimuli. The room
microphone was placed close to the speaker that produced
the stimuli sound. Therefore, the recorded ambient sound
provides an implicit synchronization, as it includes the
sound of the stimuli.
4.3 Practical Considerations
Although the protocol and setup was done carefully,
problems arose during recordings. The data recorded from
participants 9 and 15 are not complete due to technical
problems. The physiological responses of participant 12 aremissing due to recording difficulties. The physiological
responses to each stimuli were recorded each in a separate
file in Biosemi data format (BDF) which is an extension of
European data format (EDF) and easily readable in different
platforms. For each trial, the response to the 15 seconds
neutral video is stored separately. All the files containingphysiological signals include the signals recorded 30 s
before the start and after the end of their stimuli. In
accordance with the Biosemi recording methodology, we
did not record a reference electrode with EEG signals.Therefore, EEG signals need rereferencing to a virtual
reference, for example, the average reference. The stimuli
videos were all encoded in MPEG-4 Xvid format and MPEGlayer three format with 44,100 Hz sampling frequency in an
audio video interleave container (AVI). The frames were
encoded in 1;280/C3800to match our display resolution.
5E MOTION RECOGNITION EXPERIMENT
In this section, we present the emotion recognition experi-
mental paradigm, analysis methods, and experimentalresults. Three modalities, including peripheral and central
nervous system physiological signals and information
captured by eye gaze tracker, were used to recognize
emotions from participants‚Äô responses.
5.1 Emotion Experiment Paradigm
The participants were informed about the experiment, and
their rights, in a verbal introduction, by e-mail, and through
a consent form. Participants were trained to use the
interface before the experiment and during the setup time.The participants were also introduced to the meaning of
arousal, valence, dominance, and predictability in the self-
assessment procedure, and to the nature of the videocontent. The five questions which were asked during self-
reporting were
1.emotional label/tag,
2.arousal,
3.valence,
4.dominance,
5.predictability [14].
The emotional labels included neutral, anxiety, amusement,
sadness, joy, disgust, anger, surprise, and fear. To simplify
the interface for the first experiment a keyboard wasprovided with only nine numerical keys and the partici-
pants answered each question by pressing one of the keys.
Questions 2 to 5 were on a nine point scale.
In emotional-affective experiments the bias from the
emotional state needs to be reduced. For this purpose, a
short neutral clip was shown to the participants before eachemotional video. The neutral clip was randomly selected
from the clips provided by the Stanford psychophysiology
laboratory [18]. The 20 emotional video clips were played
from the data set in random order. After watching a video
clip, the participant filled in the self-assessment form. Intotal, the time interval between the start of a trial and the
end of the self-reporting phase was approximately two and
half minutes. This interval included playing the neutral clip,playing the emotional clip, and performing the self-
assessment. Running of the whole protocol took, on
average, 50 minutes, in addition to 30 minutes set up time.
5.2 Emotional Features
5.2.1 EEG and Physiological Signals
The following peripheral nervous system signals were
recorded: GSR, respiration amplitude, skin temperature,SOLEYMANI ET AL.: A MULTIMODAL DATABASE FOR AFFECT RECOGNITION AND IMPLICIT TAGGING 47
Fig. 4. Five tracks recorded in parallel by MOTU 8pre audio interface.
From top to bottom: 1) room microphone, 2) head microphone,
3) serial port time stamp output (transmitted at 9,600 bps), showing
2 time stamp signals, 4) measured infrared light in front of eye tracker,5) camera trigger.
and ECG. Most of the current theories of emotion [29] agree
that physiological activity is an important component of an
emotion. Heart rate and heart rate variability (HRV)
correlate with emotional changes. Pleasantness of stimulican increase peak heart rate response [26], and HRVdecreases with fear, sadnes s, and happiness [30]. In
addition to the HR and HRV features, spectral featuresderived from HRV were shown to be a useful feature in
emotion assessment [31]. Skin temperature was also
recorded since it changes in different emotional states[32]. Regarding the respiration amplitude, slow respirationis linked to relaxation, while irregular rhythm, quickvariations, and cessation of respiration correspond to morearoused emotions like anger or fear [33], [30]. In total,102 features were extracted from peripheral physiological
responses based on the proposed features in the literature
[34], [33], [30].
In addition to the peripheral nervous system re-
sponses, electroencephalogram signals were acquired.The power spectral features were extracted from EEGsignals. The logarithms of the spectral power from theta(4H z <f< 8H z), slow alpha ( 8H z <f< 10 Hz ), alpha
(8H z <f< 12 Hz ), beta ( 12 Hz <f< 30 Hz ), and gamma
(30 Hz <f) bands were extracted from all 32 electrodes as
features. In addition to power spectral features, thedifference between the spectral power of all the symme-trical pairs of electrodes on the right and left hemisphereswas extracted to measure the possible asymmetry in thebrain activities due to the valence of perceived emotion
[35], [36]. The asymmetry features were extracted from all
mentioned bands except slow alpha. The total number ofEEG features of a trial for 32 electrodes is 14/C24√æ32/C2
5¬º216features. The total number of EEG features of a
trial for 32 electrodes is 216 features. (Table 4 lists thefeatures extracted from the physiological signals.)
5.2.2 Eye Gaze Data
After removing the linear trend, the power spectrum of thepupil diameter variation was computed. Standard deviationand spectral features were extracted from the pupil
diameter. The Hippus effect is the small oscillations of the
eye pupil diameter between 0.05 and 0.3 Hz and withamplitude of 1 mm [37], [38]. The Hippus effect has beenshown to be present when one is relaxed or passive. In thepresence of mental activity the effect will disappear. TheHippus effect is extracted by the first two power spectral
features which are covering up to 0.4 Hz.
Eye blinking was extracted by counting the number of
times when eye gaze data were not available, i.e., themoments when eyes were closed. The rate of eye blinking isshown to be correlated with anxiety. From the eye blinks,the eye blinking rate, the average and maximum blinkduration were extracted as features. In addition to the eyeblinking features the amount of time each participant spent
with his/her eyes closed was also used as a feature to detect
possible eye closing due to unpleasant emotions.
Although participants were asked not to move during
experiments, there were small head movements whichmanifested themselves in the distance between participants‚Äôeyes and the eye gaze tracker. The distance of participantsto the screen and its changes provide valuable information
about participants‚Äô posture. The total change in the distanceof a participant to the gaze tracker, gaze distance, was
calculated to measure the possible approach and avoidance.
The amount of time participants spent per trial getting closeto or far from the screen was computed as well. Thesefeatures were named approach and avoidance ratio to
represent the amount of time participants spent getting
close to or going far from the screen. The frequency of theparticipants‚Äô movement toward the screen during each trial,
approach rate, was also extracted. Approach and with-
drawal are closely related to emotional experiences [39].
The statistical features were extracted from eye gaze
coordinates along the horizontal and vertical axes, namely,the standard deviation, Kurtosis, and skewness of horizon-tal and vertical projected eye gaze. Moreover, the power
spectral density in different bands was extracted to
represent different oscillations the eye gaze pattern (seeTable 5). These bands were empirically chosen based on the
spectral changes in eye gaze movements. Ultimately,
38 features were extracted from the eye gaze data. Thefeatures extracted from eye gaze data are listed in Table 5.
5.3 Rating Analysis
Regarding the self-reports, we computed the multiraterCohen‚Äôs kappa for different annotations. A fair agreementwas found on emotional keywords with the average
/C20¬º0:32. For arousal and valence rating, the cross correlation
values were computed which was ( M¬º0:40;SD ¬º0:26) for48 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 3, NO. 1, JANUARY-MARCH 2012
TABLE 4
This Table Lists All 102 Features
Extracted from Physiological Signals
Number of features extracted from each channel is given in brackets.
arousal and ( M¬º0:71;SD ¬º0:12) for valence. The key-
word-based feedbacks were used to generate each partici-
pant‚Äôs ground truth. The histograms of emotional self-reports‚Äô keywords and ratings given to all videos are shownin Fig. 5. In Fig. 5, it is visible that the emotions which were
not initially targeted (see Table 3) have the least frequencies.
5.4 Emotion Recognition Results
In order to give the reader some baseline classification results,
emotion recognition results from three modalities and fusionof ebest modalities are presented. Two classification schemeswere defined: first, along the arousal dimension, three classesof calm, medium aroused, and excited, and second along the
valence dimension, unpleasant, neutral valence and pleasant.
The mapping between emotional keyword and classes whichare based on [14] and are given in Table 6.
A participant independent approach was taken to check
whether we can estimate a new participant‚Äôs felt emotion
based on others. For each video from the data set, the groundtruth was thus defined by the feedback given by each
participant individually. The keyword-based feedback was
then translated into the defined classes. According to this
definition, we can name th ese classes calm, medium
aroused, and excited/activated for arousal and unpleasant,
neutral valence, and pleasant for valence (see Table 6).
To reduce the between participant differences, it is
necessary to normalize the features. Each feature was
separately normalized by mapping to the range ¬Ω0;1/C138on
each participant‚Äôs signals. In this normalization the mini-
mum value for any given feature is subtracted from the same
feature of a participant and the results were divided by the
difference between the maximum and minimum values.
A leave-one-participant-out cross validation technique
was used to validate the user independent classification
performance. At each step of cross validation, the samples of
one participant were taken out as test set and the classifier
was trained on the samples from the rest of the participants.
This process was repeated for all participants‚Äô data. An
implementation of the SVM classifier from libSVM [40] withRBF kernel was employed to classify the samples using
features from each of the three modalities. For the SVM
classifier, the size of the kernel, /C13, was selected between
¬Ω0:01;10/C138, based on the average F1 score using a 20-fold cross
validation on the training set. The Cparameter that regulates
the tradeoff between error minimization and margin
maximization is empirically set to 1. Prior to classification,
a feature selection was used to select discriminative features
as follows: First, a one-way ANOVA test was done on the
training set for each feature with the class as the independent
variable. Then, any feature for which the ANOVA test was
not significant ( p> 0:05) was rejected.
Here, we used three modalities which are peripheral
physiological signals, EEG, and eye gaze data. From these
three modalities, the results of the classification over the two
best modalities were fused to obtain the multimodal fusion
results. If the classifiers provide confidence measures on their
decisions, combining decisions of classifiers can be doneusing a summation rule. The confidence measure summation
fusion was used due to its simplicity and its proven
performance for emotion recognition according to [34].
The data from the 27 participants which had enough
completed trials was used. Five hundred thirty-two samples
of physiological responses and gaze responses were
gathered over a potential data set of 27/C220¬º540samples;SOLEYMANI ET AL.: A MULTIMODAL DATABASE FOR AFFECT RECOGNITION AND IMPLICIT TAGGING 49
Fig. 5. This bar chart shows the frequency of the emotional keywords
assigned to all videos.TABLE 6
The Emotional Keywords Are Mapped into Three Classes
on Arousal and ValenceTABLE 5
This Table Lists the Features Extracted
from Eye Gaze Data for Emotion Recognition
Number of features extracted from each channel is given in brackets.
the eight missing ones were unavailable due to not having
enough valid samples in eye gaze data.
The F1 scores and recognition rates for the classification
in different modalities are given in the Table 7 and Fig. 6.
In Table 7, two random level results are also reported. Thefirst random level results represent a random classifier
with uniform distribution, whereas the second random
classifier (weighted) uses the training set distribution torandomly choose the class. The confusion matrices for eachmodality and their fusion show how they performed on
each emotion class (Table 8). In these confusion matrices
the row represents the classified label and each columnrepresents the ground truth for those samples. For allcases, classification on gaze data performed better than
EEG and peripheral signals. This is due to the fact that eye
gaze is more correlated with the shown content and similarvisual features induce similar emotions. The peripheral
physiological responses have a high variance between
different participants which makes interparticipant classi-fication difficult to perform. Therefore, the classificationusing peripheral physiological features gave the worst
results among these three modalities. This can be reduced
in future studies by using better methods to reduce thebetween participants‚Äô variance. The high arousal, ‚Äúacti-vated,‚Äù class was the most challenging class. While EEG
and peripheral physiological modalities were completely
unable to classify the samples, eye gaze also did not obtainits superior accuracy for this class (see Fig. 5). This might
have been caused by the lower number of responses for the
emotions assigned to this class. The fusion of the two bestmodalities, eye gaze and EEG, ultimately outperformed all
single modalities.
6I MPLICIT TAGGING EXPERIMENT
In this section, we describe the implicit tagging experiment
paradigm, analysis methods, and experimental results. Two
modalities were used to predict the correctness of displayed
tags, namely, facial expression (captured by a camera) and
the eye gaze location on the screen (captured by an eye gaze
tracker). The results presented in this section are limited tothe static images only. The sequences with tagged videos
were not processed. In total, we analyzed 756 data
sequences of 27 participants with the goal of recovering
the correctness of the displayed tags. The average sequence
length was 5s. The method utilized for facial expression
analysis and its results were previously published in [41].
6.1 Implicit Tagging Experiment Paradigm
In this second experiment, 28 images and 14 video fragments
were subsequently shown on their own and accompanied by
a word tag. Once using a correct tag and once using an
incorrect tag. The videos were chosen from the Hollywood
human actions database (HOHA) [3] and were between 12
and 22 seconds long ( M¬º17:6s;SD ¬º2:2s). For each trial,
the following procedure was taken:
1.Untagged Stimulus: The untagged stimulus was
displayed (still images were shown during 5 sec-onds). This allowed the participant to get to know thecontent of the image/video.
2.Tagged Stimulus: The same stimulus was displayed
with tag (still images were shown during 5 seconds).The participants‚Äô behavior in this period containedtheir reaction to the displayed tag.50 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 3, NO. 1, JANUARY-MARCH 2012
Fig. 6. This bar chart shows the F1 score for classification results of
each class from different modalities.TABLE 7
The Recognition Rate and F1 Scores of
Emotion Recognition for Different Modalities and
Fusion of the Two Best Modalities, EEG and Eye Gaze
TABLE 8
Confusion Matrices of Different Classification Schemes (Row: Classified Label; Column: Ground Truth)
The numbers on the first row and the first column of tables (a), (b), (c), and (d) represent: 1. calm, 2. medium aroused, 3. activated, and for tables (e),
(f), (g), and (h) represent: 1. unpleasant 2. neutral valence 3. pleasant. The confusion matrices relate to classification using (a), (e) peripheralphysiological signals, (b), (f) EEG signals, (c), (g) eye gaze data, (d), (h) EEG and eye gaze decision-level fusion.
3.Question: A question was displayed on the screen to
ask whether the participant agreed with the suggestedtag. Agreement or disagreement was expressed bypressing a green or a red button, respectively.
Only the color video capturing the frontal view of partici-
pants‚Äô faces was used in the analysis (see Fig. 1a). The length
of each trial was about 11 seconds for images, and slightlymore than double the stimulus‚Äô length in case of videos. Therecording of each participant was segmented in three sets of28 small clips, according to the order in which the images/
videos were presented. Each fragment corresponds to the
period between the time point when the stimulus appearsand the point when the participant has given his/herfeedback. Running of the second experiment‚Äôs protocol took
in average 20 minutes, excluding the setup time.
6.2 Facial Expression Analysis
To extract facial features, the Patras-Pantic particle filter [42]
was employed to track 19 facial points. The initial positions
of the 19 tracked points were manually labeled for each
video and then automatically tracked for the rest of thesequence. After tracking, each frame of the video wasrepresented as a vector of the facial points‚Äô 2D coordinates.
For each frame, geometric features, f1 to f20, were then
extracted based on the positions of the facial points. Theextracted features are:
.Eyebrows : Angles between the horizontal line
connecting the inner corners of the eyes and theline that connects inner and outer eyebrow (f1, f2),the vertical distances from the outer eyebrows to theline that connects the inner corners of the eyes (f3, f4)(see Fig. 7a).
.Eyes : Distances between the outer eyes‚Äô corner and
their upper eyelids (f5, f9), distances between theinner eyes‚Äô corner and their upper eyelid (f6, f10),distances between the outer eyes‚Äô corner and their
lower eyelids (f8, f12), distances between the inner
eyes‚Äô corner and their lower eyelids (f7, f11), verticaldistances between the upper eyelids and the lowereyelids (f13, f14) (see Fig. 7b).
.Mouth : Distances between the upper lip and mouth
corners (f15, f16), distances between the lower lipand mouth corners (f18, f18), distances between themouth corners (f19), vertical distance between theupper and the lower lip (f20) (see Fig. 7c).
The line that connects the inner eye corners was used as a
reference line since the inner eye corners are stable facial
points, i.e., changes in facial expression do not induce any
changes in the position of these points. They are also thetwo most accurately tracked points. For each sequence,
the listed 20 features were extracted for all the frames. Thedifference of these 20 features with their values in a neutral
frame was used in further processing.
6.3 Analysis of Eye Gaze
Gaze fixations are the coordinates of the points on the
display on which the eye gaze stayed fixed for a certainperiod of time. Each fixation is composed of its duration as
well as the two-dimensional coordinates of the projection of
the eye gaze on the screen. An example of an eye gazepattern and fixations points on an image is shown in Fig. 8.The features extracted from eye gaze are listed in Table 9.
6.4 Classification Methods
We have chosen Hidden Markov Models (HMMs) toclassify the facial expression sequences according to thecorrectness of the displayed tags. HMMs have beencommonly used for modeling dynamic sequences. For a
more detailed description of the utilized HMM framework
for facial expression analysis, see the early work on facial-expression-based implicit tagging [41].
As shown in [43], a temporal facial movement consists of
four states:
1.neutral‚Äîthere are no signs of muscular activation;
2.onset‚Äîthe muscular contraction begins and in-
creases in intensity;
3.apex‚Äîa plateau where the intensity reaches a stable
level;
4.offset‚Äîthe relaxation of muscular action.
Based on these states, the number of modeled states in the
HMM was set to four. We chose the ergodic topology, inSOLEYMANI ET AL.: A MULTIMODAL DATABASE FOR AFFECT RECOGNITION AND IMPLICIT TAGGING 51
Fig. 8. An example of displayed images is shown with eye gaze fixation
and scan path overlaid. The size of the circles represents the time spentstaring at each fixation point.
TABLE 9
This Table Lists the 19 Features Extracted
from Eye Gaze Data for Implicit Tagging
Fig. 7. The tracked points and features.
which all the states are connected with each other. This
means that it is possible to jump from any of the four statesto any other. The initial state and transition probabilitieswere randomly chosen. The emission probabilities were
modeled by a mixture of two Gaussians with diagonal
covariance matrices. For the implementation of the utilizedHMM, the HMM toolbox for MATLAB was used. For eachparticipant, two HMMs were trained: one for the partici-pant‚Äôs facial responses when the image with a correct tagwas displayed and the other for when the incorrect tag was
shown. The correctness of the displayed tags was predicted
by comparing the likelihood of these two HMMS. Theperformance of the classification was investigated by a10-fold cross validation.
Adaboost was employed for the classification of eye gaze
data. The general idea of Adaboost is to combine a group of
weak classifiers to form a strong classifier. A set of classifierswere trained sequentially and then combined [44]. The latergenerated classifiers focused more on the mistakes of theearlier classifiers. Let √∞x
1;y1√û;√∞x2;y2√û;...;√∞xn;yn√ûdenote the
instances we have. For the gaze data, each ~xiis a
19-dimensional feature vector and yi¬º/C0 1;√æ1is its asso-
ciated label. The weak classifiers used in this paper aredecision stumps in the following form:
c√∞x; i; p; /C21 √û¬º1i f px
i<p/C21
/C01 otherwise ;/C26
√∞1√û
in which i; p; /C21 are the parameters of a decision stump. iis the
feature chosen, pis a polarity flag with value 1 or /C01,/C21is
the decision threshold, xiis an instance, and xiis the value of
theith feature of x. The decision stump simply chooses a
feature from the instance and then compares its value to a
threshold. The algorithm takes two parameters: the data set
and the number of iterations T. It generates a sequence of
weak classifiers and combines them with weights
H√∞x√û¬ºX
t¬º1/C11tht√∞x√û: √∞2√û
At each iteration, Adaboost chooses the decision stump
that minimizes the weighted classification error, which is
equivalent to choosing the most discriminative feature.
The weight for the weak classifier /C11t¬º1
2ln√∞1/C0/C15t
/C15t√ûdecreases
when the error increases. The weights are then updatedbased on the classification result. A larger weight will beassigned to the misclassified samples at each iteration.
This increases the importance of the misclassified sample
in the next iteration. For the gaze data, 19 features werechosen to model the gaze pattern.
6.5 Facial Expression Results
At the single participant level, the best predication rate was
not better than 62 percent. This led to using a strategy to
predict the correctness of the displayed tag by the weightedsum of the predictions made from the behavior of differentparticipants. In this combination, the prediction rate on thetraining set was used as the combining weight of the
prediction based on a single participant. This strategy
attenuates the negative effect of participants with lessconsistent behavior in the measured features.The results of the fusion of the participants‚Äô responses
are presented in Table 10. Nis the number of classifiers, of
which each was trained on a single participant‚Äôs responses.The classifiers whose weights are among the top Nare
fused using our proposed method.
The best result is achieved by using only one classifier,
which means only using one participant‚Äôs data. Note thatthe results of N¬º1andN¬º2should always be the same
because the classifier with the second largest weight cannotchange a binary decision made by the top 1 classifier. Thebest result of 58 percent is worse than the best predictionrate on a single participant, which is 62 percent. The
prediction rate decreases as the number of participants
increases, which implies that the number of effectiveparticipants is very limited.
6.6 Eye Gaze Results
Using the gaze responses at the single participant level,
the best prediction rate using Adaboost was 66 percent.Here, the same combination strategy was employed tocombine multiple participants‚Äô responses to detect thecorrectness of tags.
The result of combining Adaboosts from different
participants is presented in Table 10. Nis again the
number of participants used. When Nis equal to 1 or 2,
the results are slightly lower than the best result on the
single participant. This might be due to the selection ofoverfitted participants from the training set. The perfor-mance of those ineffective participants can be regarded asnearly random guess.
As noted previously, the results for N¬º1,N¬º2are the
same. As Ngoes to 4, the prediction rate increases to
73 percent. Unlike the facial data, combining differentparticipants‚Äô gaze responses improves the overall predic-tion rate. When Nis larger than 4, the result starts to
deteriorate. This indicates that the number of effectiveparticipants is also limited here.
6.7 Modality Fusion
After combining the responses from different participants,the combined classifiers of facial expression and eye gazewere fused at the decision level using a weighted sum ofconfidence measures. The assigned weights were found
based on the performance of fusion on the training set.
Since better results were obtained with gaze features, theweight of gaze confidence was set to one and the weight offacial analysis confidence was between ¬Ω0;1/C138.
The prediction rate improved from 73.2 to 75 percent by
combining the facial and gaze results. The best result wasachieved when the facial analysis confidence weight wasbetween 0.1 and 0.35, which gives us an estimate of therelative importance of the two modalities. These results52 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 3, NO. 1, JANUARY-MARCH 2012
TABLE 10
Prediction Rate of Fusing HMM for Facial Expression
Analysis and Fusing Adaboost for Eye Gaze Analysis
Nis the number of combined classifiers.
show that a participant‚Äôs facial and eye gaze responses
convey information about the correctness of tags associated
with multimedia content.
7D ISCUSSIONS AND RECOMMENDATIONS
To our knowledge, MAHNOB-HCI is the first databasewhich has five modalities precisely synchronized, namely,
eye gaze data, video, audio, and peripheral and centralnervous system physiological signals. This database can be
of interest to researchers in different fields, from psychol-
ogy and affective computing to multimedia. In addition to
emotion recognition from a single modality or multiple
modalities, the relations between simultaneous emotion-
related activity and behavior can be studied. The high
accuracy of synchronization of this database allows study-
ing the simultaneous effects of emotions on EEG and other
modalities, and fusing them in any desired way, fromdecision-level fusion (DLF) down to combined processing at
the signal level. As the results reflect, not all recorded
modalities are as correlated with the stimuli as the others.
For example, there are not enough audio events and the
peripheral physiological signals do not give the same
emotion recognition results comparing to the eye gaze
and EEG signals.
In any emotional experiment, having more trials gives
the opportunity for single participant studies. At the same
time, longer sessions make participants tired and unable to
feel the stimuli emotions. Considering this tradeoff, we
found that an upper limit of 20 video fragments was
acceptable. Although 20 videos are enough to compute
significant correlations, this number of samples is notsufficient for single-participant emotion recognition.
Inducing emotions and recording affective reactions is a
challenging task. Special attention needs to be paid to several
crucial factors, such as stimuli that are used, the laboratory
environment, as well as the recruitment of participants. Our
experience can be distilled into a list of recommendations
that will enable the development of additional corpora to
proceed smoothly.
The experiment environment in the laboratory should be
kept isolated from the outside environment. The partici-
pants should not be able to see the examiners or hear noise
from the outside. The light and temperature should be
controlled to avoid variation in physiological reactions due
to uncontrolled parameters.
Choosing the right stimuli material is an important factor
in any affective study. They should be long enough to
induce emotions and short enough to prevent boredom.
Furthermore, to be sure variation in stimuli length does notintroduce variance in the measurements between emotional
and nonemotional stimuli, we suggest the stimuli durations
be equal. The mixture of contradicting emotions can make
problems for self-assessments. We recommend using videos
which do not induce multiple emotions. A correct partici-
pant recruitment can make a big difference in the results.
Due to the nature of affective experiments, a motivated
participant with the right knowledge for filling the
questionnaire in is desirable. The rewards can make theparticipants more motivated and responsible. However,
cash compensation might attract participants who are notmotivated or lack desired communication skills. Therefore,
rewarded recruitment should be done by carefully con-
sidering the desired qualifications. Contact lenses usually
cause participants to blink more, which introduces a higher
level of artifacts on EEG signals. Therefore, participants
with visual correction should avoid using contact lenses as
much as possible. Thick glasses affect the eye gaze trackerperformance. In the experiments in which both these
modalities are recorded, recruiting participants with no
visual correction is advised. Properly attending to partici-
pants takes an important part of one‚Äôs attention, which can
easily lead to forgetting parts of complicated technical
protocols. Therefore, operation of the recording equipment
during the experiments should be made as simple as
possible (preferably just by pressing a single button).
Alternatively, the tasks of controlling and monitoring
correct data collection and attending to participants can
be divided between multiple laboratory assistants with
carefully defined procedures.
8C ONCLUSIONS
A multimodal affective database has been recorded and
made available to the affective computing community. Thelarge collection of modalities recorded (multicamera video
of face, head, speech, eye gaze, pupil size, ECG, GSR,
respiration amplitude, and skin temperature) and the highsynchronization accuracy between them makes this data-base a valuable contribution to the ongoing development
and benchmarking of emotion-related algorithms that
exploit data fusion, as well as to studies on humanemotion and emotional expression. Emotion recognitionand implicit tagging results from different modalities set a
baseline result for researchers who are going to use the
database in the future.
ACKNOWLEDGMENTS
The work of Soleymani and Pun was supported in part by
the Swiss National Science Foundation and in part by the
European Community‚Äôs Seventh Framework Programme
(FP7/2007-2011) under grant agreement Petamedia
no 216444. The data acquisition part of this work and the
work of Pantic and Lichtenauer were supported by the
European Research Council under the ERC Starting Grant
agreement no. ERC-2007-StG-203143 (MAHNOB). The
authors would like to thank J. Dobo /C20s, Prof. D. Grandjean,
and Dr. G. Chanel for their valuable scientific contributions
to the experiments‚Äô protocol. They also acknowledge the
contributions of J. Jiao for the analysis on the implicit
tagging.
REFERENCES
[1] M. Pantic and A. Vinciarelli, ‚ÄúImplicit Human-Centered Tagging,‚Äù
IEEE Signal Processing Magazine, vol. 26, no. 6, pp. 173-180, Nov.
2009.
[2] Z. Zeng, M. Pantic, G.I. Roisman, and T.S. Huang, ‚ÄúA Survey of
Affect Recognition Methods: Audio, Visual, and SpontaneousExpressions,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence,
vol. 31, no. 1, pp. 39-58, Mar. 2009.
[3] I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld, ‚ÄúLearning
Realistic Human Actions from Movies,‚Äù Proc. IEEE Conf. Computer
Vision and Pattern Recognition, pp. 1-8, June 2008.SOLEYMANI ET AL.: A MULTIMODAL DATABASE FOR AFFECT RECOGNITION AND IMPLICIT TAGGING 53
[4] J.A. Healey and R.W. Picard, ‚ÄúDetecting Stress during Real-World
Driving Tasks Using Physiological Sensors,‚Äù IEEE Trans. Intelligent
Transportation Systems, vol. 6, no. 2, pp. 156-166, June 2005.
[5] M. Pantic, M. Valstar, R. Rademaker, and L. Maat, ‚ÄúWeb-Based
Database for Facial Expression Analysis,‚Äù Proc. IEEE Int‚Äôl Conf.
Multimedia and Expo, pp. 317-321, 2005.
[6] E. Douglas-Cowie, R. Cowie, I. Sneddon, C. Cox, O. Lowry, M.
McRorie, J.-C. Martin, L. Devillers, S. Abrilian, A. Batliner, N.
Amir, and K. Karpouzis, ‚ÄúThe HUMAINE Database: Addressingthe Collection and Annotation of Naturalistic and InducedEmotional Data,‚Äù Proc. Second Int‚Äôl Conf. Affective Computing and
Intelligent Interaction, A. Paiva et al., pp. 488-500, 2007.
[7] M. Grimm, K. Kroschel, and S. Narayanan, ‚ÄúThe Vera am Mittag
German Audio-Visual Emotional Speech Database,‚Äù Proc. IEEE
Int‚Äôl Conf. Multimedia and Expo, pp. 865-868, Apr. 2008.
[8] G. McKeown, M.F. Valstar, R. Cowie, and M. Pantic, ‚ÄúThe
SEMAINE Corpus of Emotionally Coloured Character Interac-tions,‚Äù Proc. IEEE Int‚Äôl Conf. Multimedia and Expo, pp. 1079-1084,
July 2010.
[9] S. Koelstra, C. Mu ¬®hl, M. Soleymani, A. Yazdani, J.-S. Lee, T.
Ebrahimi, T. Pun, A. Nijholt, and I. Patras, ‚ÄúDEAP: A Database forEmotion Analysis Using Physiological Signals,‚Äù IEEE Trans.
Affective Computing, vol. 3, no. 1, pp. 18-31, Jan.-Mar. 2012.
[10] M.F. Valstar and M. Pantic, ‚ÄúInduced Disgust, Happiness and
Surprise: An Addition to the MMI Facial Expression Database,‚ÄùProc. Int‚Äôl Conf. Language Resources and Evaluation, WorkshopEMOTION, pp. 65-70, May 2010.
[11] E. Douglas-cowie, R. Cowie, and M. Schro ¬®der, ‚ÄúA New Emotion
Database: Considerations, Sources and Scope,‚Äù Proc. ISCA Int‚Äôl
Technical Research Workshop Speech and Emotion, pp. 39-44, 2000.
[12] J.A. Russell, ‚ÄúCulture and the Categorization of Emotions,‚Äù
Psychological Bull., vol. 110, no. 3, pp. 426-450, 1991.
[13] J.A. Russell and A. Mehrabian, ‚ÄúEvidence for a Three-Factor
Theory of Emotions,‚Äù J. Research in Personality, vol. 11, no. 3,
pp. 273-294, Sept. 1977.
[14] J.R.J. Fontaine, K.R. Scherer, E.B. Roesch, and P.C. Ellsworth, ‚ÄúThe
World of Emotions Is Not Two-Dimensional,‚Äù Psychological
Science, vol. 18, no. 12, pp. 1050-1057, 2007.
[15] M. Soleymani, J. Davis, and T. Pun, ‚ÄúA Collaborative Personalized
Affective Video Retrieval System,‚Äù Proc. Third Int‚Äôl Conf. Affective
Computing and Intelligent Interaction and Workshops, Sept. 2009.
[16] J.D. Morris, ‚ÄúObservations: Sam: The Self-Assessment Manikin;
An Efficient Cross-Cultural Measurement of Emotional Re-sponse,‚Äù J. Advertising Research, vol. 35, no. 8, pp. 63-38, 1995.
[17] A. Schaefer, F. Nils, X. Sanchez, and P. Philippot, ‚ÄúAssessing the
Effectiveness of a Large Database of Emotion-Eliciting Films: A
New Tool for Emotion Researchers,‚Äù Cognition and Emotion,
vol. 24, no. 7, pp. 1153-1172, 2010.
[18] J. Rottenberg, R.D. Ray, and J.J. Gross, ‚ÄúEmotion Elicitation Using
Films,‚Äù Handbook of Emotion Elicitation and Assessment, series in
affective science, pp. 9-28, Oxford Univ. Press, 2007.
[19] The Psychology of Facial Expression, J. Russell and J. Fernandez-Dols,
eds. Cambridge Univ. Press, 1997.
[20] D. Keltner and P. Ekman, Facial Expression of Emotion, second ed.,
pp. 236-249. Guilford Publications, 2000.
[21] T. Kanade, J.F. Cohn, and T. Yingli, ‚ÄúComprehensive Database for
Facial Expression Analysis,‚Äù Proc. IEEE Fourth Int‚Äôl Conf. Automatic
Face and Gesture Recognition, pp. 46-53, 2000.
[22] M. Pantic and L.J.M. Rothkrantz, ‚ÄúToward an Affect-Sensitive
Multimodal Human-Computer Interaction,‚Äù Proc. IEEE, vol. 91,
no. 9, pp. 1370-1390, Sept. 2003.
[23] S. Petridis and M. Pantic, ‚ÄúIs This Joke Really Funny? Judging the
Mirth by Audiovisual Laughter Analysis,‚Äù Proc. IEEE Int‚Äôl Conf.
Multimedia and Expo, pp. 1444-1447, 2009.
[24] M.M. Bradley, Miccoli, Laura, Escrig, A. Miguel, Lang, and J.
Peter, ‚ÄúThe Pupil as a Measure of Emotional Arousal andAutonomic Activation,‚Äù Psychophysiology, vol. 45, no. 4, pp. 602-
607, July 2008.
[25] T. Partala and V. Surakka, ‚ÄúPupil Size Variation as an Indication of
Affective Processing,‚Äù Int‚Äôl J. Human-Computer Studies, vol. 59,
nos. 1/2, pp. 185-198, 2003.
[26] P.J. Lang, M.K. Greenwald, M.M. Bradley, and A.O. Hamm,
‚ÄúLooking at Pictures: Affective, Facial, Visceral, and BehavioralReactions,‚Äù Psychophysiology, vol. 30, no. 3, pp. 261-273, 1993.
[27] R. Adolphs, D. Tranel, and A.R. Damasio, ‚ÄúDissociable Neural
Systems for Recognizing Emotions,‚Äù Brain and Cognition, vol. 52,
no. 1, pp. 61-69, June 2003.[28] J. Lichtenauer, J. Shen, M. Valstar, and M. Pantic, ‚ÄúCost-Effective
Solution to Synchronised Audio-Visual Data Capture UsingMultiple Sensors,‚Äù technical report, Imperial College London,2010.
[29] D. Sander, D. Grandjean, and K.R. Scherer, ‚ÄúA Systems Approach
to Appraisal Mechanisms in Emotion,‚Äù Neural Networks, vol. 18,
no. 4, pp. 317-352, 2005.
[30] P. Rainville, A. Bechara, N. Naqvi, and A.R. Damasio, ‚ÄúBasic
Emotions Are Associated with Distinct Patterns of Cardiorespira-tory Activity,‚Äù Int‚Äôl J. Psychophysiology, vol. 61, no. 1, pp. 5-18, July
2006.
[31] R. McCraty, M. Atkinson, W.A. Tiller, G. Rein, and A.D. Watkins,
‚ÄúThe Effects of Emotions on Short-Term Power Spectrum Analysisof Heart Rate Variability,‚Äù The Am. J. Cardiology, vol. 76, no. 14,
pp. 1089-1093, 1995.
[32] R.A. McFarland, ‚ÄúRelationship of Skin Temperature Changes to
the Emotions Accompanying Music,‚Äù Applied Psychophysiology and
Biofeedback, vol. 10, pp. 255-267, 1985.
[33] J. Kim and E. Andre ¬¥, ‚ÄúEmotion Recognition Based on Physiolo-
gical Changes in Music Listening,‚Äù IEEE Trans. Pattern Analysis
and Machine Intelligence, vol. 30, no. 12, pp. 2067-2083, Dec. 2008.
[34] G. Chanel, J.J.M. Kierkels, M. Soleymani, and T. Pun, ‚ÄúShort-Term
Emotion Assessment in a Recall Paradigm,‚Äù Int‚Äôl J. Human-
Computer Studies, vol. 67, no. 8, pp. 607-627, Aug. 2009.
[35] S.K. Sutton and R.J. Davidson, ‚ÄúPrefrontal Brain Asymmetry: A
Biological Substrate of the Behavioral Approach and InhibitionSystems,‚Äù Psychological Science, vol. 8, no. 3, pp. 204-210, 1997.
[36] R.J. Davidson, ‚ÄúAffective Neuroscience and Psychophysiology:
Toward a Synthesis,‚Äù Psychophysiology, vol. 40, no. 5, pp. 655-665,
Sept. 2003.
[37] V.F. Pamplona, M.M. Oliveira, and G.V.G. Baranoski, ‚ÄúPhoto-
realistic Models for Pupil Light Reflex and Iridal Pattern
Deformation,‚Äù ACM Trans. Graphics, vol. 28, no. 4, pp. 1-12, 2009.
[38] H. Bouma and L.C.J. Baghuis, ‚ÄúHippus of the Pupil: Periods of
Slow Oscillations of Unknown Origin,‚Äù Vision Research,
vol. 11,
no. 11, pp. 1345-1351, 1971.
[39] R.J. Davidson, P. Ekman, C.D. Saron, J.A. Senulis, and W.V.
Friesen, ‚ÄúApproach-Withdrawal and Cerebral Asymmetry: Emo-
tional Expression and Brain Physiology I,‚Äù J. Personality and Social
Psychology, vol. 58, no. 2, pp. 330-341, 1990.
[40] C.-C. Chang and C.-J. Lin, ‚ÄúLIBSVM: A Library for Support Vector
Machines,‚Äù Science, vol. 2, pp. 1-39, 2001.
[41] J. Jiao and M. Pantic, ‚ÄúImplicit Image Tagging via Facial
Information,‚Äù Proc. Second Int‚Äôl Workshop Social Signal Processing,
pp. 59-64, 2010.
[42] I. Patras and M. Pantic, ‚ÄúParticle Filtering with Factorized
Likelihoods for Tracking Facial Features,‚Äù Proc. IEEE Sixth Int‚Äôl
Conf. Automatic Face and Gesture Recognition, pp. 97-102, May 2004.
[43] S. Petridis, H. Gunes, S. Kaltwang, and M. Pantic, ‚ÄúStatic vs.
Dynamic Modeling of Human Nonverbal Behavior from
Multiple Cues and Modalities,‚Äù Proc. Int‚Äôl Conf. Multimodal
Interfaces, pp. 23-30, 2009.
[44] Y. Freund and R.E. Schapire, ‚ÄúA Decision-Theoretic General-
ization of On-Line Learning and an Application to Boosting,‚Äù Proc.
European Conf. Computational Learning Theory, pp. 23-37, 1995.
Mohammad Soleymani received both the
BSc and MSc degrees from the Departmentof Electrical and Computer Engineering, Uni-
versity of Tehran, Iran. He is now a doctoral
student and research assistant in the Compu-ter Vision and Multimedia Laboratory, Compu-ter Science Department, University of Geneva,Switzerland. His research interests includeaffective computing and multimedia informationretrieval. He is a member of the IEEE and the
HUMAINE association.54 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 3, NO. 1, JANUARY-MARCH 2012

Jeroen Lichtenauer received the MSc and PhD
degrees in electrical engineering from DelftUniversity of Technology, Delft, The Nether-lands, in 2003 and 2009, respectively. He is aresearch associate with the Intelligent Behaviour
Understanding Group, Department of Comput-
ing, Imperial College London, London, UnitedKingdom. His research interests include real-timesignal processing, real-time computer vision.
Thierry Pun received the EE Engineering
degree in 1979 and the PhD degree in 1982.
He is a full professor and the head of the
Computer Vision and Multimedia Laboratory,Computer Science Department, University ofGeneva, Switzerland. He has authored orcoauthored more than 300 full papers as wellas eight patents in various aspects of multimediaprocessing; his current research interests lie in
affective computing and multimodal interaction.
He is a member of the IEEE.Maja Pantic is a professor in affective and
behavioural computing at Imperial College Lon-don, Department of Computing, United King-dom, and at the University of Twente,Department of Computer Science, The Nether-
lands. She has received various awards for her
work on automatic analysis of human behavior,including the European Research Council Start-ing Grant Fellowship and the Roger NeedhamAward 2011. She currently serves as the editor
in chief of Image and Vision Computing Journal and as an associate
editor for both the IEEE Transactions on Systems, Man, and
Cybernetics Part B and the IEEE Transactions on Pattern Analysis
and Machine Intelligence . She is a fellow of the IEEE.
.For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.SOLEYMANI ET AL.: A MULTIMODAL DATABASE FOR AFFECT RECOGNITION AND IMPLICIT TAGGING 55

"
https://ieeexplore.ieee.org/document/5523955,"682 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 12, NO. 7, NOVEMBER 2010
A Natural Visible and Infrared Facial
Expression Database for Expression
Recognition and Emotion Inference
Shangfei Wang , Member, IEEE , Zhilei Liu, Siliang Lv, Yanpeng Lv, Guobing Wu, Peng Peng, Fei Chen, and
Xufa Wang
Abstract‚Äî To date, most facial expression analysis has been
based on visible and posed expression databases. Visible images,
however, are easily affected by illumination variations, while
posed expressions differ in appearance and timing from naturalones. In this paper, we propose and establish a natural visibleand infrared facial expression database, which contains bothspontaneous and posed expressions of more than 100 subjects,recorded simultaneously by a visible and an infrared thermalcamera, with illumination provided from three different direc-tions. The posed database includes the apex expressional imageswith and without glasses. As an elementary assessment of theusability of our spontaneous database for expression recogni-tion and emotion inference, we conduct visible facial expressionrecognition using four typical methods, including the eigenfaceapproach [principle component analysis (PCA)], the Ô¨Åsherfaceapproach [PCA + linear discriminant analysis (LDA)], the ActiveAppearance Model (AAM), and the AAM-based + LDA. We alsouse PCA and PCA+LDA to recognize expressions from infraredthermal images. In addition, we analyze the relationship betweenfacial temperature and emotion through statistical analysis. Ourdatabase is available for research purposes.
Index Terms‚Äî Emotion inference, expression recognition, facial
expression, infrared image, spontaneous database, visible image.
I. I NTRODUCTION
FACIAL expression is a convenient way for humans to
communicate emotion. As a result, research on expres-
sion recognition has become a key focus area of personalized
human-computer interaction [1], [2]. Most current research
focuses on visible images or videos and good performance hasbeen achieved in this regard. Whereas varying light exposure
can hinder visible expression recognition, infrared thermal
images, recording the temperature distribution formed by face
vein branches, are not sensitive to imaging conditions. Thus,
thermal expression recognition is a useful and necessary com-plement to visible expression recognition [3]. Besides, a change
Manuscript received December 13, 2009; revised March 24, 2010 and June
23, 2010; accepted June 24, 2010. Date of publication July 26, 2010; date of
current version October 15, 2010. This paper is supported in part by National
863 Program (2008AA01Z122), in part by Anhui Provincial Natural Science
Foundation (No.070412056), and in part bySRF for ROCS, SEM. The associate
editor coordinating the review of this manuscript and approving it for publica-
tion was Dr. Caifeng Shan.
The authors are with the School of Computer Science and Technology,
University of Science and Technology of China, Hefei 230027, China (e-mail:
sfwang@ustc.edu.cn; leivo@mail.ustc.edu.cn; lsliang@mail.ustc.edu.cn;
lvyp@mail.ustc.edu.cn; guobing@mail.ustc.edu.cn; dbpeng@mail.ustc.
edu.cn; feichen@mail.ustc.edu.cn; xfwang@ustc.edu.cn).
Digital Object IdentiÔ¨Åer 10.1109/TMM.2010.2060716in facial temperature is a clue that can prove helpful in emotion
inference [4], [5]. Furthermore, most existing research has beenbased on posed expression databases, which are elicited by
asking subjects to perform a series of emotional expressions
in front of a camera. These artiÔ¨Åcial expressions are usually
exaggerated. Spontaneous expressions, on the other hand, may
be subtle and differ from posed ones both in appearance andtiming. It is, therefore, most important to establish a natural
database to allow research to move from artiÔ¨Åcial to natural
expression recognition, ultimately leading to more practical
applications thereof.
This paper proposes and establishes a natural visible and in-
frared facial expression database (NVIE) for expression recog-
nition and emotion inference. First, we describe in detail the de-
sign, collection, and annotation of the NVIE database. In addi-
tion, we conduct facial expression analysis on spontaneous vis-
ible images with front lighting using several typical methods,including the eigenface approach [principle component anal-
ysis (PCA)], the Ô¨Åsherface approach [PCA + linear discriminant
analysis (LDA)], the Active Appearance Model (AAM), and
the combined AAM-based + LDA (referred to as AAM+LDA).
Thereafter, we use PCA and PCA +LDA to recognize expres-
sions from spontaneous infrared thermal images. In addition, we
analyze the relationship between facial temperature and emo-tion through an analysis of variance (ANOV A). The evaluation
results verify the effectiveness of our spontaneous database for
expression recognition and emotion inference.
II. B
RIEF REVIEW OF EXISTING NATURAL
AND INFRARED DATABASES
There are many existing databases dealing with facial ex-
pressions, an exhaustive survey of which is given in [1] and [6].
Here, we only focus on natural and infrared facial expressiondatabases. Due to the difÔ¨Åculty of eliciting affective displaysand the time-consuming manual labeling of spontaneous ex-
pressions, only a few natural visible expression databases
exist. These are listed in Table I, together with details of size,elicitation method, illumination, expression descriptions, and
modality [visual (V) or audiovisual (A V)]. From Table I, we
can see that researchers use one of three possible approaches toobtain spontaneous affective behavior, i.e., human-human con-versation, human-computer interaction, or emotion-inducing
videos [22]. Since this paper only focuses on facial expressions,
and not speech or language, using emotion-inducing videos is
1520-9210/$26.00 ¬© 2010 IEEE
WANG et al. : A NATURAL VISIBLE AND INFRARED FACIAL EXPRESSION DATABASE FOR EXPRESSION RECOGNITION AND EMOTION INFERENCE 683
TABLE I
DATABASES OF NATURAL FACIAL EXPRESSIONS
TABLE II
DATABASES OF INFRARED FACIAL EXPRESSIONS
a suitable approach, especially as the datasets will not include
any facial changes caused by speech.
Because expression recognition in the thermal infrared
domain has received relatively little attention compared withrecognition in visible-light imagery, no thermal expressiondatabases exist. There are, however, a few thermal face
databases (listed in Table II) that include some posed thermal
expression images. For each database, we provide informa-tion on the size, wave band, elicitation method, illumination,thermal information, and expression descriptions.
Based on Tables I and II, it is obvious that current natural
expression databases focus only on visible spectrum imagery,while the existing infrared databases consist only of posedimages. Therefore, we propose and establish a natural visible
and infrared facial expression database, enriching the existing
databases for expression recognition and emotion inference.
III. I
MAGE ACQUISITION SETUP
To set up the database, we Ô¨Årst built a photographic room.
Since videos are used to induce the subjects‚Äô emotions, we chosea quiet room as the experiment environment to ensure that the ef-fect of the screened videos was not compromised. The room was
9.6 m*4.0 m*3.0 m, with a single door and two windows. The
facial expression recording system included a camera system,illumination system, glasses, and thermometer. The details ofthe system, depicted in Fig. 1, are described below.
A. Camera Setup
To record both visible and infrared videos, we used two
cameras: a DZ-GX25M visible camera capturing 30 frames
per second, with resolution 704*480, and a SAT-HY6850 in-
frared camera capturing 25 frames per second, with resolution320*240 and wave band 8‚Äì14
. The lenses of the two cam-
eras were placed 0.75 m in front of the subject, with a distance
of 0.1 m between the two cameras. The cameras were placed
at a height of 1.2 m above the ground. To begin the process,both the video and the two cameras were turned on at the sametime. Then, a sun visor was placed in front of both cameras,
Fig. 1. Design of the photographic room.
and removed very quickly from above. Thus, the removal of thesun visor was recorded in both the visible and thermal videos.Before segmenting the emotional frames, frames containingevidence of the sun visor were carefully compared to Ô¨Ånd
two frames in which the position of the sun visor was almost
the same. Appropriate time stamps were calculated for each,allowing the remaining parts of the visible and infrared videosto start simultaneously.
To prevent any discomfort and to guarantee that the emotion
was spontaneous, we did not require subjects to keep their headsÔ¨Åxed in one position. Once the screening had started, the exper-imenter stayed in the room to supervise the whole process.
Temperature information recorded by the infrared camera
gradually becomes inaccurate after a long period of recordingdue to the increased heat of the sensor. Thus, timely calibration
is needed. To minimize any disruption to the subject and to
prevent any loss of frames, the experimenters manually invokedthe auto-calibration before each experiment and while neutralvideos (depicted in Fig. 2) were being screened.
B. Illumination Setup
This system controls the indoor light and the different direc-
tions of the light source. The indoor lighting was controlled bydaylight lamps on the ceiling. Both the door and the curtains
684 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 12, NO. 7, NOVEMBER 2010
Fig. 2. Video clips.
were kept closed during the screening of the videos, keeping
the lighting difference between day and night as small as pos-sible. Another three Ô¨Çoor lamps were used to create the dif-ferent illumination conditions. The distance between the lamps
and the subject was one meter. As shown in Fig. 1, the front
lamp was placed in front of the subject, while the left and rightlamps were at 45 degree angles to the front lamp. Bearing inmind that bright lights may cause the subjects some discomfort,
36-W lamp bulbs were used. To create front illumination, all
three lamps were turned on, whereas for left or right illumina-tion, the front lamp, together with either the left or right lamp,respectively, was turned on.
C. Glasses
When performing facial expression recognition, the wearing
of glasses can disguise important information. To make theNVIE database more practical, we added facial expressionswith glasses. In the spontaneous experiment, subjects were not
speciÔ¨Åcally requested to wear glasses, but could do so if they
chose to. In the posed experiment, all subjects were required topose for two complete sets of expressions under each lightingcondition. In the Ô¨Årst set, subjects wore glasses, whereas in
the other, they posed without glasses. Subjects could choose to
wear their own glasses or the pair we provided.
D. Environmental Temperature
Although thermal emissivity from the facial surface is rela-
tively stable under illumination variations, it is sensitive to the
temperature of the environment. We, therefore, recorded the
temperature of the room during the experiments. Room temper-ature ranged between 18 and
, with a mean of
 and
a standard deviation of
 .
IV . D ATAACQUISITION
A. Subjects
A total of 215 healthy students (157 males and 58 females),
ranging in age from 17 to 31, participated in our experiments.
All of the subjects had normal auditory acuity and were men-
tally stable. Each signed an informed consent before the exper-iment and received compensation for participating after com-pleting the experiment. Each subject participated in three spon-
taneous experiments, for each of the three illumination condi-
tions, and one posed experiment. However, some of the subjectscould only express two or fewer expressions and some thermal
and visible videos were lost. Ultimately, for the spontaneous
database, we obtained images of 105 subjects under front illu-mination, 111 subjects under left illumination, and 112 subjectsunder right illumination, while 108 subjects contributed to the
posed database.B. Stimuli
In our experiments, we induced the subjects‚Äô emotions by
screening deliberately selected emotional videos. All the videos
were obtained from the Internet, including 13 happy, 8 angry,
45 disgusted, 6 fearful, 7 sad, 7 surprised, and 32 emotionallyneutral videos, as judged by the authors. Each emotional videowas about 3‚Äì4 min long, while neutral videos were about 1‚Äì2
min long.
C. Experiment Procedure
Two kinds of facial expressions were recorded during our ex-
periments: spontaneous expressions induced by the Ô¨Ålm clips
and posed ones obtained by asking the subjects to perform a se-ries of expressions in front of the cameras. Each kind of facialexpression was recorded under three different illumination con-
ditions, namely left, front, and right illumination.
Details of this procedure are given below.First, the subjects were given an introduction to the experi-
mental procedure, the meaning of arousal and valence, and how
to self-assess their emotions.
Second, the subjects seated themselves comfortably, and then
we moved their chairs forward or backwards to ensure a distanceof 0.5 m between the chair and the screen.
Third, a 19-inch LCD screen was used to display the video
clips, each of which contained six segments (as depicted inFig. 2), corresponding to the six types of emotional videos. Ineach segment, a particular type of emotional video was played
and the subject‚Äôs expressions recorded as synchronous videos.
To reduce the interaction of different emotions induced by thedifferent emotional video clips, neutral clips were shown be-
tween segments. Meanwhile, subjects were also asked to re-
port the real emotion experienced by considering emotional va-lence, arousal, the basic emotion category, and its intensity on aÔ¨Åve-point scale. These self-reported data were used as the emo-
tion label for the recorded videos.
Fourth, once the video had terminated, each subject was asked
to display six expressions, namely happiness, sadness, surprise,fear, anger, and disgust, both with and without glasses.
V. D
ESIGN OF THE NVIE D ATABASE
First, we manually Ô¨Ånd the onset and apex of an expression in
the visible facial videos. Then both visible and thermal videos
during these periods are segmented into frames. Thus, our NVIEdatabase includes two sub-databases: a spontaneous databaseconsisting of image sequences from onset to apex and a posed
database consisting of apex images, with each containing both
the visible and infrared images that were recorded simultane-ously and under three different illumination conditions, namely,illumination from the left, front, and right. The posed database
also includes expression images with and without glasses.
Because it is difÔ¨Åcult to determine automatically what kind
of facial expression will be induced by a certain emotional Ô¨Ålm,
Ô¨Åve students in our lab manually labeled all the visible apex
facial images in the spontaneous database according to the in-tensity of the six categories (happiness, sadness, surprise, fear,anger, and disgust), arousal, and valence on a three-point scale.
The category with the highest average intensity was used as the
WANG et al. : A NATURAL VISIBLE AND INFRARED FACIAL EXPRESSION DATABASE FOR EXPRESSION RECOGNITION AND EMOTION INFERENCE 685
Fig. 3. Example images of a subject showing posed and spontaneous anger.
Fig. 4. Visual and infrared images of a subject expressing anger.
Fig. 5. Example images of a subject expressing happiness with illumination
from the left, front, and right.
label for the visible and thermal apex facial images. The average
arousal and valence were also adopted as labels. The kappa co-efÔ¨Åcient of the labeling was 0.65, indicating a good consistency.
As we know, expression is not emotion. For example, we may
feel sad, but our expression can be neutral. The data collectedfrom the self assessment reports, described in Section IV-C,were used to provide an emotional annotation of the corre-
sponding image sequences.
The following sections describe the variations in our database
and present some example images.
A. Posed versus Spontaneous
Both posed and spontaneous expressions were recorded in
our database, allowing researchers to investigate differences fur-ther. Fig. 3 shows an example of a posed and spontaneous facial
expression in the database.
B. Infrared versus Visible
Both visible and infrared images were collected. Fig. 4 shows
an expression of anger in both a visible and infrared image.
C. Illumination Direction
Using the light system described in Section III-B, we captured
the facial expressions with illumination from three different di-rections. Fig. 5 shows the images captured under the different
illumination conditions.
Fig. 6. Example images of a subject with and without glasses, expressing hap-
piness.
Fig. 7. Images of a subject showing the six expressions (happiness, disgust,fear, surprise, sadness, and anger).
D. Glasses
Images of a subject with and without glasses are shown in
Fig. 6.
E. Six Facial Expressions
In the posed database, each subject displayed six facial ex-
pressions. In the spontaneous emotion database, the expressionswere evaluated by Ô¨Åve experimenters. Fig. 7 shows example im-ages of the six spontaneous facial expressions.
VI. V
ALIDATION AND EV ALUATION OF THE DATABASE
The main aim of this section is to present an elementary as-
sessment of the usability of the spontaneous sub-database with
front lighting for expression recognition and emotion inference,
and to provide reference evaluation results for researchers usingthe database. We conduct visible facial expression recognitionusing four typical methods, namely the PCA, PCA+LDA,
AAM, and AAM+LDA. We also use PCA and PCA +LDA to
recognize expressions from infrared thermal images. In addi-tion, we analyze the relationship between facial temperatureand emotion through statistical analysis.
Not all subjects displayed all six types of emotion we aimed
to elicit. Thus, only three expressions (i.e., disgust, fear, andhappiness), which were induced successfully in most cases,are used for expression recognition. Furthermore, since we did
not restrict head movement in our data acquisition experiment,
686 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 12, NO. 7, NOVEMBER 2010
Fig. 8. Face labeling (a) with and (b) without face conÔ¨Åguration.
thereby ensuring more natural expressions, some sequences
with non-frontal facial images were discarded. Ultimately,236 apex images of 84 subjects were selected for visible and
thermal expression recognition, including 83 images depicting
disgust, 62 images depicting fear, and 91 images depictinghappiness. In addition, the expressions were divided into fourclasses in the arousal-valence space: arousal positive/valence
, arousal positive/valence
 ,
arousal negative/valence
 , and arousal nega-
tive/valence
 . Of the 236 images, 100 images
were classiÔ¨Åed as
 , 132 images as
 , 0 images as
, and 4 images as
 .
Although some subjects did not display sad, angry, and sur-
prised expressions, they really did feel these, which is why allsix types of emotion for 20 participants (13 male and 7 female)
from the NVIE spontaneous sub-database are used for emotion
inference based on facial temperature as described below.
A. Expression Recognition From Visible Images
1) Analysis Methods: Before feature extraction, prepro-
cessing work activities including manual eye location, angle
rectiÔ¨Åcation, and image zoom were performed to normalize
the apex images to
rectangles. Then, four baseline
algorithms are used to extract image features, namely the PCA[23], PCA+LDA [24], AAM [26], and AAM+LDA [24], [26].
Details of these algorithms can be found in [23], [24], and [26],
respectively. For the PCA and PCA+LDA methods,
and
are both set as 64 to reduce the computational complexity,
yet retain sufÔ¨Åcient information of the images. Furthermore,
four methods are used to normalize the grey-level values of
the images, that is, histogram equalization (HE), regionalhistogram equalization (RHE), gamma transformation (GT),and regional gamma transformation (RGT) [25]. Then, the
non-facial region was removed using an elliptic mask. For
each image of a speciÔ¨Åc person in the selected 236 images, weapplied PCA with the training set containing all the remainingimages, except the other images of different expressions for the
same person. The grey-level values of all pixels of images are
the feature sets used for PCA and LDA.
For the AAM and AAM+LDA methods, W and H are both
set as 400 to allow the facial features to be located relatively
precisely. We consider two labeling options: with face conÔ¨Åg-
uration points (WFC: 61 points in total) and without (NFC: 52points in total), as shown in Fig. 8. From the 236 images, weTABLE III
CONFUSION MATRIX OF CATEGORICAL EXPRESSION
RECOGNITION ON VISIBLE IMAGES (%)
selected 72 images, which included 24 images for each expres-
sion, to build the appearance model. We applied this model tothe 236 images to obtain their appearance parameters.
After feature extraction,
-nearest neighbors was used as the
classiÔ¨Åer. Euclidean distance and leave-one-subject-out cross-validation was adopted, and
was set to 1 here.
2) Experimental Results and Analysis: Tables III and IV
show the performance of these algorithms with respect to our
database, including confusion matrices and average recognitionrates.
The following observations are evident from Tables III and
IV.
‚Ä¢ With respect to categorical expressions, happiness has a
higher recognition rate than disgust and fear. Accordingto the confusion matrices, very few instances of happiness
are incorrectly recognized as disgust or fear, and vice versa.
On the other hand, instances of disgust and fear are morelikely to be incorrectly identiÔ¨Åed as the other. Different ex-pressions are displayed using different facial characteris-
tics, especially in the mouth and eye regions. In our exper-
iments, when most subjects smiled, they lifted the cornersof their mouths and kept their eyes half-closed. However,when the subjects expressed disgust or fear, some closed
their mouths, while others opened their mouths slightly
or even widely. Moreover, some closed their eyes, whileothers kept their eyes wide open. In other words, facialmovements of the subjects are similar in expressing happi-
ness, but they differ from person to person when expressing
disgust or fear. This may explain why happiness is moreeasily recognized.
WANG et al. : A NATURAL VISIBLE AND INFRARED FACIAL EXPRESSION DATABASE FOR EXPRESSION RECOGNITION AND EMOTION INFERENCE 687
TABLE IV
CONFUSION MATRIX OF DISCRETE DIMENSIONAL
EXPRESSION RECOGNITION ON VISIBLE IMAGES (%)
‚Ä¢ With respect to discrete dimensional expressions, it is hard
to say whether
 or
 is more recognizable.
Among the incorrect cases,
 and
 are mostly
recognized as one another. However, all
 instances
are incorrectly recognized as
 or
 . The main
reason for this is that most instances belong to
 or
, and only four instances belong to
 .
‚Ä¢ With respect to the processing algorithms without LDA,
AAM performs better than PCA. The reason may be that
PCA uses grey-level values of each image as features,while AAM uses geometric characteristics of each image,that is, shape and texture information, which better repre-
sents facial movements. LDA improves recognition rates
in all PCA cases, although it has the opposite effect in mostAAM cases. This shows that the effect of LDA depends
on the property of the data processed [15]. The differ-
ence in recognition rates for different feature extractionalgorithms is larger than that for different preprocessingalgorithms within one processing algorithm, which means
that feature extraction algorithms have a greater inÔ¨Çuence
on the results than preprocessing algorithms. In general,NFC+AAM+KNN is the best combination of prepro-cessing and processing algorithms.
B. Expression Recognition From Infrared Thermal Images
1) Analysis Methods: The preprocessing procedure for IR
images is similar to that for visible images, except that theTABLE V
CONFUSION MATRIX OF CATEGORICAL EXPRESSION
RECOGNITION ON INFRARED IMAGES (%)
TABLE VI
CONFUSION MATRIX OF DISCRETE DIMENSIONAL
EXPRESSION RECOGNITION ON INFRARED IMAGES (%)
manual eye location is less accurate than in visible images,
because the eyes in IR images are not as clear as those in visible
images. Because some subjects wore glasses, we removed the
eye region using a 64x15 rectangular mask for the infraredfacial images. After preprocessing, the PCA and PCA+LDAmethods are used to extract features of the images. Then,
K-nearest neighbors is used as the classiÔ¨Åer. Euclidean distance
and leave-one-subject-out cross validation is adopted, and K isset to 1 here.
2) Experimental Results and Analysis: Both apex images and
difference images (Dif.), which are created from the difference
between the grey-level of the corresponding pixels of apex andneutral images, are used in the experiments. Tables V and VI
show the results.
‚Ä¢ With respect to categorical expressions, it is hard to say
which one is the best discriminator in the thermal infrareddomain. However, with respect to discrete dimensional ex-
pressions,
is more recognizable than
 , and
all
 instances are incorrectly recognized. The main
reason for this is that most instances belong to
 or
, and only four instances belong to
 .
‚Ä¢ With respect to the processing algorithms, LDA improves
recognition rates in discrete dimension expression recogni-tion, but causes the performance to deteriorate in categoryrecognition. The reason may be similar to that given above
[15]. The effect of different source images is negligible.
However, since the overall recognition rates are not very high,
new methods that are suitable for thermal infrared images needto be developed.
C. Emotion Inference From Thermal Images
1) Analysis Methods: Physiological changes due to auto-
nomic nervous system activity can impact the temperature pat-
688 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 12, NO. 7, NOVEMBER 2010
Fig. 9. Face segmentation.
terns of the face, which may be used to predict rated emotions
[4], [5], [27]. In this section, we analyze the relationship be-tween emotion and facial temperature using infrared thermaldifference images. First, in order to retain the original tempera-
ture data for analysis, we manually segmented the difference in-
frared original images into Ô¨Åve regions, namely forehead, eyes,nose, mouth, and cheeks, as shown in Fig. 9, and ensured thatthe facial segmentation size and ratio of each participant‚Äôs neu-
tral and emotional infrared images were consistent. Then, for
each facial sub-region, we extracted the temperature data as thedifference between the neutral and emotional infrared facial im-ages, and calculated the Ô¨Åve statistical parameters given below
to reÔ¨Çect the temperature variance. As most of the participants
wore glasses, thereby masking the thermal features of the eyeregion, the eye regions were not taken into account in this anal-ysis.
‚Ä¢ MEAN‚Äîthe mean of each difference temperature matrix
‚Ä¢ ABS‚Äîthe mean of the absolute values of each difference
temperature matrix
‚Ä¢ ADDP‚Äîthe mean of the positive values of each difference
temperature matrix
‚Ä¢ ADDN‚Äîthe mean of the negative values of each differ-
ence temperature matrix
‚Ä¢ V AR‚Äîthe variance of each difference temperature matrix
Having obtained the Ô¨Åve statistical parameters, three ANOV A
analyses were conducted. First, an ANOV A was conductedto ascertain which statistical parameter is most useful forreÔ¨Çecting temperature change associated with changes in emo-
tion. Second, an ANOV A was applied to the facial regions with
different emotional states to ascertain in which facial regionsthe change in temperature due to the different emotions isthe greatest. Third, an ANOV A analysis was applied to the
emotional states in different facial regions to analyze which
emotional state differs most in each facial sub-region.
2) Experimental Results and Analysis:
Experiment for Parameter Selection: The factors in this
analysis are the six emotional states (i.e., sadness, anger, sur-
prise, fear, happiness, and disgust) and the four facial sub-re-gions (forehead, nose, mouth, and cheeks). Using a multiple
comparison of the post hoc test, the signiÔ¨Åcant differences be-
tween the emotional states for the different statistical parametersare given in Table VII.
From Table VII, the following observations can be made.TABLE VII
SIGNIFICANT DIFFERENCES BETWEEN DIFFERENT EMOTION
STATES UNDER DIFFERENT STATIC PARAMETERS
‚Ä¢ When the statistical parameter V AR is used, the signiÔ¨Å-
cance of the differences between the six emotional statesis relatively higher than for all the other statistical parame-ters at the 0.05 level. Therefore, in the subsequent analyses,
we only consider the V AR statistic.
‚Ä¢ When V AR is used, the differential validities of the six
emotional states are sorted as follows: Sadness-Fear,Anger-Fear, Fear-Disgust, Sadness-Surprise, Fear-Hap-
piness, and Anger- Surprise. Changes in the thermal
distribution on the face for different emotional states arecaused by facial muscle movements, in addition to thetransition of emotional states and/or other physiological
changes [29]. However, for some emotional states, the
thermal distribution on the face is not sufÔ¨Åciently uniqueto be distinguished from that of another state. This may becaused by ambiguity or overlap of the emotional states.
ANOVA Analysis of the Different Facial Regions With Dif-
ferent Emotion States Using VAR: The factors in this analysis
are the facial sub-regions (forehead, nose, mouth, and cheeks),while the dependent variable is V AR. The impact is measured
as the mean V AR values for different facial sub-regions, shown
in Fig. 10 and Table VIII.
‚Ä¢ From Fig. 10, we can conclude that, when using the V AR
statistic, the degree of impact of the forehead and cheek
regions is more signiÔ¨Åcant than the other two regions for
most emotional states, e.g., surprise, fear and happiness.Furthermore, the impact of the mouth region on all emo-tional states is the smallest compared with other regions.
This means that when the emotion changes, the tempera-
ture variance in the forehead and cheek regions is muchlarger than that in the other regions, while the temperaturevariance in the mouth region is the smallest. The impor-
tance of the forehead region in the analysis of emotions
using thermal images coincides with the research results ofSugimoto and Jenkins [30], [31]. The V AR values for the
cheek region are relatively large for most emotional states,
thereby reÔ¨Çecting the uniformity of this region‚Äôs tempera-ture change, while the results for the nose and mouth re-gions are opposite, thereby reÔ¨Çecting the non-uniformity
WANG et al. : A NATURAL VISIBLE AND INFRARED FACIAL EXPRESSION DATABASE FOR EXPRESSION RECOGNITION AND EMOTION INFERENCE 689
Fig. 10. Impact of different facial regions on different emotional states.
TABLE VIII
MATRIX SHOWING SIGNIFICANT DIFFERENCES BETWEEN
FACIAL REGIONS WITHDIFFERENT EMOTION STATES
of the temperature change in these regions. This result
is vastly different to that obtained when using visible fa-
cial expressions to reÔ¨Çect the various emotions, in that the
change in forehead and cheek is not signiÔ¨Åcant when thefacial expression changes. This result is, therefore, mean-ingful in the research of emotion inference using changes
in facial temperature.
‚Ä¢ From Table VIII, only one facial sub-region pair, Fore-
head-Mouth, is signiÔ¨Åcantly different for the emotion fear.We can conclude that the differences between different fa-
cial sub-regions are not signiÔ¨Åcant for any of the emotional
states when using the V AR statistics. This indicates thatthe overall change in the facial temperature occurs duringthe emotional state transfer. In some individual cases, for
certain emotional states, a change in temperature may be
caused by the combined inÔ¨Çuence of psychological factorsor physiological factors related to facial muscular move-ments. This will be studied in depth in our future research.
ANOVA Analysis of the Different Emotion States in Dif-
ferent Facial Regions Using VAR: In this ANOV A analysis, the
factors are the emotion states, with the dependent variable beingV AR. The degree of impact is measured as the mean of the V AR
values for different emotional states in each facial sub-region.
The analysis results are given in Fig. 11 and Table IX.
‚Ä¢ From Fig. 11, we can conclude that, when using the V AR
statistic, the sorted degree of impact of these emotional
states, i.e., Fear-Surprise-Happiness, is similar in the fore-
head and cheek regions, but not in the nose and mouth re-gions. This means that the degree of impact of the emo-tional states fear, surprise, and happiness in most of the
facial sub-regions is more signiÔ¨Åcant than that of the other
emotional states. The exception of the nose and mouth re-gions may be due to a change in the breathing rate or anon-stationary physiological signal for different emotional
states [27], [28]. Besides, muscular movement in the mouth
Fig. 11. Impact of the different emotional states in the different facial sub-
regions.
TABLE IX
MATRIX SHOWING SIGNIFICANT DIFFERENCES BETWEEN DIFFERENT
EMOTIONAL STATES FOR EACH FACIAL SUB-REGION
region for different facial expressions may also be a con-
tributing factor [29].
‚Ä¢ From Table IX, we can conclude that when using the
V AR statistic, three emotional state pairs are signiÔ¨Å-
cantly different in the forehead region (Sadness-Fear,
Anger-Fear, and Fear-Disgust), two emotional state pairsin the mouth region (Sadness-Surprise and Anger-Sur-prise), and two emotional state pairs in the cheek regions
(Sadness-Surprise and Sadness-Fear) at the 0.05 level. All
these differences may be caused by the combined inÔ¨Çu-ence of the emotional state change, the factor of the facialmuscular movements, and various other psychological
factors [29].
VII. C
ONCLUSION
The NVIE database developed in this study for expression
recognition and emotion inference has four main characteris-tics. 1) To the best of our knowledge, it is the Ô¨Årst natural ex-
pression database containing both visible and infrared videos.
As such, it can be used for visible, infrared, or multi-spectralnatural expression analysis. 2) It contains the facial temperatureof subjects, thereby providing the potential for emotion recog-
nition. 3) Both posed and spontaneous expressions by the same
subject are recorded in the database, thus supplying a valuableresource for future research on their differences. 4) Both lightingand glasses variations are considered in the database. This is
useful for algorithm assessment, comparison, and evaluation.
690 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 12, NO. 7, NOVEMBER 2010
We carried out an elementary assessment of the usability of
the spontaneous sub-database, using four baseline algorithmsfor visible expression recognition and two baseline algorithms
for infrared expression recognition, and provided reference
evaluation results for researchers using the database. We alsoanalyzed the relationship between facial temperature and emo-tion, and provided useful clues for emotion inference from
thermal images.
A
CKNOWLEDGMENT
The authors would like to thank all the subjects who partici-
pated in the experiments. The authors also would like to thankthe Editor and the anonymous reviewers for their insightful
comments.
R
EFERENCES
[1] Z. Zeng, M. Pantic, G. I. Roisman, and T. S. Huang, ‚ÄúA survey of af-
fect recognition methods: Audio, visual, and spontaneous expressions,‚Äù
IEEE Trans. Pattern Anal. Mach. Intell. , vol. 31, no. 1, pp. 39‚Äì58, Jan.
2009.
[2] B. Fasel and J. Luettin, ‚ÄúAutomatic facial expression analysis: A
survey,‚Äù Pattern Recognit. , vol. 36, pp. 259‚Äì275, 2003.
[3] M. M. Khan, R. D. Ward, and M. Ingleby, ‚ÄúClassifying pretended and
evoked facial expressions of positive and negative affective states using
infrared measurement of skin temperature,‚Äù Trans. Appl. Percept. , vol.
6, no. 1, pp. 1‚Äì22, 2009.
[4] M. M. Khan, M. Ingleby, and R. D. Ward, ‚ÄúAutomated facial expres-
sion classiÔ¨Åcation and affect interpretation using infrared measurement
of facial skin temperature variations,‚Äù ACM Trans. Autonom. Adapt.
Syst. , vol. 1, pp. 91‚Äì113, 2006.
[5] A. T. Krzywicki, G. He, and B. L. O‚ÄôKane, ‚ÄúAnalysis of facial
thermal variations in response to emotion‚Äîeliciting Ô¨Ålm clips,‚Äù Proce.
SPIE‚ÄîInt. Soc. Optic. Eng. , vol. 7343, no. 12, pp. 1‚Äì11, 2009.
[6] [Online]. Available: http://emotion-research.net/wiki/Databases.
[7] M. Pantic and M. Stewart Bartlett , Machine Analysis of Facial Expres-
sions, in Face Recognition , K. D. a. M. Grgic, Ed. Vienna, Austria:
I-Tech Education and Publishing, 2007, pp. 377‚Äì416.
[8] A. J. O‚ÄôToole, J. Harms, S. L. Snow, D. R. Hurst, M. R. Pappas, J. H.
Ayyad, and H. Abdi, ‚ÄúA video database of moving faces and people,‚Äù
IEEE Trans. Pattern Anal. Mach. Intell. , vol. 27, no. 5, pp. 812‚Äì816,
May 2005.
[9] N. Sebe, M. S. Lew, I. Cohen, Y. Sun, T. Gevers, and T. S. Huang,
‚ÄúAuthentic facial expression analysis,‚Äù in Proc. 6th IEEE Int. Conf.
Automatic Face and Gesture Recognition , 2004.
[10] E. Douglas-Cowie, R. Cowie, and M. Schroeder, ‚ÄúThe description of
naturally occurring emotional speech,‚Äù in Proc. 15th Int. Conf. Pho-
netic Sciences , Barcelona, Spain, 2003, pp. 2877‚Äì2880.
[11] G. I. Roisman, J. L. Tsai, and K. S. Chiang, ‚ÄúThe emotional integration
of childhood experience: Physiological, facial expressive, and self-re-
ported emotional response during the adult attachment interview,‚Äù De-
velopment. Psychol. , vol. 40, no. 5, pp. 776‚Äì789, 2004.
[12] M. S. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. Fasel, and J.
Movellan, ‚ÄúRecognizing facial expression: Machine learning and ap-
plication to spontaneous behavior,‚Äù in Proc. IEEE Int. Conf. Computer
Vision and Pattern Recognition (CVPR ‚Äô05) , 2005, pp. 568‚Äì573.
[13] R. Gajsek, V. Struc, F. Mihelic, A. Podlesek, L. Komidar, G. Socan,
and B. Bajec, ‚ÄúMulti-modal emotional database: AvID,‚Äù Informatica
33, pp. 101‚Äì106, 2009.
[14] K. R. Scherer and G. Ceschi, ‚ÄúLost luggage emotion: A Ô¨Åeld study of
emotion-antecedent appraisal,‚Äù Motivation and Emotion , vol. 21, pp.
211‚Äì235, 1997.
[15] K. Delac, M. Grgic, and S. Grgic, ‚ÄúIndependent comparative study
of PCA, ICA, and LDA on the FERET data set,‚Äù Int. J. Imag. Syst.
Technol. , vol. 15, no. 5, pp. 252‚Äì260, 2005.
[16] [Online]. Available: http://www.image.ntua.gr/ermis/.
[17] [Online]. Available: http://emotion-research.net/download/vam.
[18] D5i: Final Report on WP5, IST FP6 Contract no. 507422, 2008.[19] [Online]. Available: http://www.equinoxsensors.com/prod-
ucts/HID.html.
[20] [Online]. Available: http://www.cse.ohio-state.edu/OTCBVS-
BENCH/Data/02/download.html.
[21] [Online]. Available: http://www.terravic.com/research/facial.htm.[22] J. Rottenberg, R. R. Ray, J. J. Gross, J. A. Coan, and J. J. B. Allen ,
Handbook of Emotion Elicitation and Assessment . New York: Ox-
ford Univ. Press, 2007.
[23] M. A. Turk and A. P. Pentland, ‚ÄúFace recognition using eigenfaces,‚Äù in
Proc. IEEE Int. Conf. Computer Vision Pattern Recognition , 1991, pp.
586‚Äì591.
[24] P. N. Belhumeur, J. P. Hespanha, and D. J. Kriegman, ‚ÄúEigenfaces vs.
Fisherfaces: Recognition using class speciÔ¨Åc linear projection,‚Äù IEEE
Trans. Pattern Anal. Mach. Intell. , vol. 19, no. 7, pp. 711‚Äì720, Jul.
1997.
[25] S. Shan, W. Gao, B. Cao, and D. Zhao, ‚ÄúIllumination normalization for
robust face recognition against varying lighting conditions,‚Äù in Proc.
IEEE Int. Workshop Analysis and Modeling of Faces and Gestures
,
2003, pp. 157‚Äì164.
[26] T. F. Cootes, G. J. Edwards, and C. J. Taylor, ‚ÄúActive appearance
models,‚Äù IEEE Trans. Pattern Analysis Mach. Intell. , vol. 23, no. 6,
pp. 681‚Äì685, Jun. 2001.
[27] B. R. Nhan and T. Chau, ‚ÄúInfrared thermal imaging as a physiological
access pathway: A study of the baseline characteristics of facial skin
temperatures,‚Äù Physiol. Measure. , vol. 30, no. 4, pp. N23‚ÄìN35, 2009.
[28] H. Tanaka, H. Ide, and Y. Nagashuma, ‚ÄúAn attempt of feeling analysis
by the nasal temperature change model,‚Äù in Proc. 2000 IEEE Int. Conf.
Systems, Man, and Cybernetics , 2000, vol. 1, pp. 1265‚Äì1270.
[29] Y. Sugimoto, Y. Yoshilomi, and S. Tomita, ‚ÄúA method for detecting
transitions of emotional states using a thermal facial image based on a
synthesis of facial expressions,‚Äù Robot. Autonomous Syst. , vol. 3I, no.
3, 2000.
[30] A. Merla and G. L. Romani, ‚ÄúThermal signatures of emotional arousal:
A functional infrared imaging study,‚Äù in Proc. IEEE 29th Annu. Int.
Conf. , 2007, pp. 247‚Äì249.
[31] S. Jenkins, R. Brown, and N. Rutterford, ‚ÄúComparing thermographic,
EEG, and subjective measures of affective experience during simulated
product interactions,‚Äù Int. J. Design , vol. 3, no. 2, pp. 53‚Äì65, 2009.
[32] S.-J. Chung, ‚ÄúLexpression et la perception de l.emotion extraite de la
parole spontanee: Evidences du coreen et de l‚Äôanglais,‚Äù , Univ. Sor-
bonne Nouvelle, Paris, France, 2000.
Shangfei Wang (M‚Äô02) received the M.S. degree in
circuits and systems and the Ph.D. degree in signaland information processing from the University of
Science and Technology of China (USTC), Hefei,
Anhui, China, in 1999 and 2002.
From 2004 to 2005, she was a postdoctoral
research fellow with Kyushu University, Fukuoka,
Japan. She is currently an Associate Professor with
the School of Computer Science and Technology,
USTC. Her research interests cover computation
intelligence, affective computing, multimedia
computing, information retrieval, and artiÔ¨Åcial environment design. She has
authored or coauthored over 40 publications.
Zhilei Liu received the B.S. degree in information
and computing science from Shandong University of
Technology, Zibo, Shandong, China, in 2008. He is
pursuing the M.S. degree in the School of Computer
Science and Technology of the University of Science
and Technology of China, Hefei, Anhui, China.
His research interest is affective computing.
Siliang Lv received the B.S. degree in computer sci-
ence and technology from the University of Science
and Technology of China (USTC), Hefei, Anhui,
China, in 2008. He is pursuing the M.S. degree in
the School of Computer Science and Technology of
USTC. His major is affective computing.
WANG et al. : A NATURAL VISIBLE AND INFRARED FACIAL EXPRESSION DATABASE FOR EXPRESSION RECOGNITION AND EMOTION INFERENCE 691
Yanpeng Lv received the B.S. degree in software en-
gineering from Shandong University, Jinan, China,
in 2008. He is currently pursuing the M.S. degree
in computer science in the University of Science and
Technology of China, Hefei, Anhui, China.
His research interest is affective computing.
Guobing Wu received the B.S. degree in computer
science and technology from Anhui University,
Hefei, Anhui, China. He is pursuing the M.S. degreein the School of Computer Science and Technology
of the University of Science and Technology of
China, Hefei, Anhui, China.
His research interest is affective computing.
Peng Peng received the B.S. degree in computer sci-
ence and technology from the University of Science
and Technology of China, Hefei, Anhui, China, in
2010. He is pursuing the M.S. degree in the School
of Computing Science at Simon Frazer University,
Burnaby, BC, Canada.
Fei Chen received the B.S. degree in computer sci-
ence and technology from the University of Science
and Technology of China, Hefei, Anhui, China, in
2010. He is pursuing the M.S. degree in the School of
Computing Science at the University of Hong Kong.
Xufa Wang received the B.S. degree in radio elec-
tronics from University of Science and Technology
of China, Hefei, China, in 1970.
He is currently a Professor of the School of Com-
puter Science and Technology, University of Science
and Technology of China, Hefei, Anhui, China, and
the Director of the Key Lab of Computing and Com-
municating Software of Anhui Province. He has pub-
lished Ô¨Åve books and over 100 technical articles in
journals and proceedings in the areas of computation
intelligence, pattern recognition, signal processing,
and computer networks.
Prof. Wang is an Editorial Board Member of the Chinese Journal of Elec-
tronic , the Journal of Chinese Computer Systems , and the International Journal
of Information Acquisition .
"
https://ieeexplore.ieee.org/document/5647898,"                                 978-1-4244-6516-3/10/$26.00 ¬©2010 IEEE                                 1750
2010 3rd International Congress on Image and Signal Processing (CISP2010)                       
A New Method For Facial Expression Recognition 
Based On Sparse Representation Plus LBP  
 
Ming-Wei Huang 
School of Information Engineering 
WUYI University  
Jiangmen, China Zhe-wei Wang Ôºå Zi-Lu Ying 
 
 
 
 
Abstract ‚ÄîSparse Representation-based Classification (SRC) is a 
newly introduced algorithm for face recognition, notable for its robust performance to occlusions and corruptions. Local Binary Patterns (LBP) is a very powerful method to describe the texture 
and shape of images. In this paper, we propose a novel method 
for facial expression recognition based on sparse representation of LBP features. Extensive experiments on Japanese Female Facial Expression (JAFFE) database are conducted. The 
experiment results show that the new method has a better 
performance than using Sparse Representation-based Classification solely on facial recognition, and is also better than those traditional algorithms such as Principal Component Analysis (PCA) and Linear discriminant analysis (LDA). 
Keywords-Facial expression recognition; Sparse representation; 
LBP; SRC; PCA; LDA  
I. INTRODUCTION  
Facial expressions imply much information about human 
emotions and play an important role in human communications. With the advance of information technology, the wide spread use of Internet, mobile telephony and other 
inexpensive computer equipment has made human computer 
interaction (HCI) a usual activity in everyday life [1]. In order to facilitate a more intelligent and smart human machine interface of multimedia products [1], the automatic facial expression recognition (FER), which had been studied world wide in the last twenty years, has become a hot spot in the computer vision and pattern recognition community. Despite much effort had been made on, and many algorithms, such as PCA and LDA, have been introduced to, FER remains a tough problem, for its relative low discriminative rate and weak robustness to occlusions and corruptions. 
In this paper we propose a new approach to facial 
expression recognition based on Sparse Representation and LBP. Sparse Representations a newly developed signal processing tool [2]. Sparse Representation-based Classification is a new algorithm for digital image classification based on sparse representation theory [3]. SRC is notable for its robustness to corruption and occlusion in face recognition. Local Binary Patterns (LBP) [4, 5] is a very powerful method to describe the texture and shapes of an image. Therefore it appeared to be suitable for feature extraction in facial expression recognition. In this paper we use LBP to extract the features of expression images and then we use SRC to classify images into categories of expressions. This algorithm can attain 
a recognition rate of 62.86% for FER, better than using SRC solely, and also better than some traditional algorithms, such as PCA and LDA. 
In this paper, SRC+LBP is used for facial expression 
recognition. Traditional algorithms for FER such as PCA and LDA are used to compare with SRC+LBP algorithm. The robustness of SRC algorithm for FER to noise is studied. The rest of the paper is organized as follows. In section 2, the fundamental theory of SRC is introduced. LBP is described in section 3. The new proposed algorithm LBP+SRC is introduced in section 4. Section 5 describes extensive experiments of LBP+SRC algorithm on FER. And some conclusions are given in section 6. 
II. S
PARSE REPRESENTATION -BASED CLASSIFICATION  
The original goal of Sparse Representation algorithm was 
not for classification itself, but rather for representation or compression of signals. Sparse representation of signals potentially uses much lower sampling rates than the Shannon-Nyquist bound. The Sparse representation theory has shown that sparse signals can be exactly reconstructed from a small number of linear measurements [6, 7]. Therefore, the algorithm performance was measured in terms of sparsity of the representation and fidelity to the original signals. In SRC the 
measurements are the training samples themselves. Given
n 
training samples: 12, ,...,nvv v . We can construct the matrix 
[]12, ,..., ,mn
nAv v v√ó=‚àà \  the test samplemy‚àà\ can be 
linearly represented by all training samples as 
myA x=‚àà \  (1) 
The equation yA x= is usually over-determined. We can 
find the sparest solution by choosing the minimum 0A-norm 
solution: 
0 0x = argmin x subject to Axy= (2) 
where x is 0A-norm of x. If , mn>  however, the problem 
of finding the solution of (2) is NP-hard. To solve the NP-hard 
equation, Donoho [8] and some other scholars [9] proved that if 
                                                                                                                                          1751
the solution0xis sparse enough, the solution of the 0A-
minimization problem (2) is equivalent to the following 1A-
minimization problem: 
11arg min subject to xx A x y== (3) 
If the solution is sparse and has t nonzero entries, it can be 
efficiently solved by homotopy algorithms  in 3()Ot n +  time, 
linear in the size of the training set [10].  
In the case of dealing data with noise, the Sparse  
Representation model (1) can be modified to account for small possibly dense 
noise  by writing  
yzAx=+  (4) 
where  mz‚àà\is a noise factor with bounded energy  
2z Œµ<. 
The Sparse x can still be approximately recovered by solving 
the following stable  1A-minimization problem: 
1
sA() : 112arg min subject to - xx A x y Œµ =‚â§ (5) 
This convex optimization problem can be solved via 
second-order cone programming [11]. The solution of 1
sA()  is 
guaranteed to approximately recovery sparse solutions in 
ensembles of random matrices A.   
Given a test sample y from one of the classes in the 
training set, we first calculate its sparse representation l
1xvia 
(3) or (5). The nonzero entries in the estimate l
1xwill all be 
associated with the columns of A from a single object class i, 
and then we can assign the test sample y to that class.  
However, noise and modeling error may lead to small 
nonzero entries associated with multiple object classes. In this 
case, we instead classify y based on how well the coefficients 
associated with all training samples of each object reproduce y.  
For each class i, letiŒ¥:nn‚Üí\\  be the characteristic 
mapping that selects the coefficients associated with the thi 
class. For1nx‚àà\, 
inxŒ¥ ‚àà\ÔºàÔºâ  is a new vector only whose only 
nonzero entries are the entries in x that are associated with 
class i.  
Using only the coefficients associated with the thi class, 
one can approximate the given test sample yasl
1()iiyA x Œ¥= . 
We then classify y based on these approximations by 
assigning it to the object class that minimizes the residual 
between y and
iy: 
1 2min ( ) ( )iiiry y A x Œ¥‚àí  (6) 
III. THE PRINCIPLE OF LBP   
LBP is a very powerful method to describe the texture and 
shape of a digital image. Ojala et al. [4] first introduced LBP operator and showed its high discriminative power for texture 
classification. At a given pixel position (),ccxy, LBP is 
defined as an ordered set of binary comparisons of pixel 
intensities between the center pixel and its eight surrounding pixels (Fig. 1). The decimal form of the resulting 8-bit word (LBP code) can be expressed as follows: 
7
0(, ) ( ) 2n
cc n c
nLBP x y s i i
==‚àí‚àë (7) 
where ci corresponds to the grey value of the center pixel 
(),ccxy, in to the grey values of the 8 surrounding pixels, and 
function s(x) is defined as:  
10()00if xsxif x‚â• ‚éß=‚é®< ‚é© (8) 
 
Figure 1.  The example of basic LBP operator.  
By definition, the LBP operator is unaffected by any 
monotonic gray-scale transformation which preserves the pixel intensity order in a local neighborhood. Note that each bit of the LBP code has the same significance level and that two successive bit values may have a totally different meaning. Actually, The LBP code may be interpreted as a kernel structure index. 
Later the LBP operator was extended to use neighborhood 
of different sizes [5]. Their 
,PRLBP  notation refers to P equally 
spaced pixels on a circle of radius R. In this paper, we use the 
,PRLBP  operator which is illustrated in Fig. 2.  
 
Figure 2.  Circularly neighbour-sets for three different values of P and R 
Another extension to the original operator is the definition 
of so called uniform patterns. A local binary pattern is called uniform if the binary pattern contains at most two bitwise transitions from 0 to 1 or vice versa when the bit pattern is considered circular. For example, the patterns 00000000 (0 transitions) and 11001111 (2 transitions) are uniform whereas the patterns 11001001 (4 transitions) and 01010011 (6 transitions) are not. In the computation of the LBP histogram, uniform patterns are used so that the histogram has a separate bin for every uniform pattern and all non-uniform patterns are 
assigned to a single bin. For uniform patterns with
P sampling 
points and radius Rthe notion 2
,u
PRLBP  is used. The superscript 
                                                                                                                                          1752
2u stands for using only uniform patterns. Ojala et al. noticed 
that in their experiments with texture images, uniform patterns 
account for a bit less than 90 % of all patterns when using the (8, 1) neighborhood and for around 70 % in the (16, 2) neighborhood. The image after LBP uniform pattern is illustrated in Fig. 3. 
 
Figure 3.  The instance of 2
,u
PRLBP  processing of facial expression image. 
To extract the features of the facial expression, the images 
are divided into local regions01 1,m RRR‚àí""  and texture 
descriptors are extracted from each region independently. For 
every region a histogram with all possible labels is constructed. This means that every bin in a histogram represents a pattern and contains the number of its appearance in the region. A 
histogram of the labeled image 
),(yxfl can be defined as: 
{ }
,(, ) { (, ) }
,0 , ,1 , 0 , , 1 .il j
xyH T f xy i T xy R
in jm== ‚àà
=‚àí = ‚àí‚àë
"""" (9) 
Where, n is referred to the number of label generated by LBP 
operator. The m is refereed to the local region number of the 
divided image. The function of ( ) TB can be defined as: 
1
()
0Bis true
TB
Bis false=‚éß‚é®‚é©  (10) 
The feature vector is then constructed by concatenating the 
regional histograms to a one big histogram (Fig. 4).  
 
Figure 4.  The example of regional histograms and global histograms 
construction. 
IV. SRC+LBP  ALGORITHM FOR FER 
In this section, we introduce the new method for facial 
expression recognition.  The proposed algorithm involves two vital portions: facial representation and classifier design. Facial representation is to derive a set of features from original face images to effectively represent faces.  The optimal features should minimize within-class variations while maximize between-class variations [12]. In our proposed algorithm, the first portion is using LBP to extract image feature, and the 
second part is using Sparse Representation as classifier for FER.  
The flow of the algorithm is shown in Fig. 5. First, n facial 
expression images were inputted as training samples. The next step is features extraction. After the processing of LBP operator, the images yield regional histograms (features). Then, each of the facial expression images constructs their feature vectors by combining the histograms of all blocks of each sub-image into one histogram. One benefit of feature extraction, which carries over to the proposed SRC algorithm is to reduce data dimension and computational cost. For raw facial expression 
images, the corresponding linear system 
yA x= is very large. 
For example, if the images are given at the typical resolution, 
64√ó64 pixels, the dimension m is in order of 4096, but after 
the LBP preprocessing, the dimension is reduced to 885. These 
histograms are feature vectors12,n vv v"" . 
 
 
Figure 5.  The FEA  algorithm flow 
Since the most feature transformations involve only linear 
operations, the projection from image space to feature space 
can be represented as a matrix dmR√ó‚àà\ with dm . Applying 
R to both sides of (1) yields Ôºö 
 
0dy Ry RAx =‚àà\  (11) 
In practice, the dimension d of the feature space is chosen 
to be much smaller than n, which is the number of training 
images. In this case, the system of equations dyR x=‚àà\is 
underdetermined. Nevertheless, as the desired solution 0xis 
sparse, we can recover it by solving the following reduced 1A-
minimization problem Ôºö 

1 12arg min x x subject to RAx y Œµ =‚àí ‚â§ (12) 
for a given error tolerance  0Œµ>. Thus, in Algorithm of SRC, 
the matrix Aof training images is now replaced by the matrix 
dnRA√ó‚àà\of -dimensionald features; the test image yis replaced 
by its features y. And rigorously follow the step of the above-
mentioned SRC algorithm; we can get the result of the 
classification of the sample y. Image normalizatio n Input JAFFE database  
Feature extraction 
based on LBP 
SRC classifie r 
End 
                                                                                                                                          1753
V. EXPERIMENTS  
The experiments of the proposed algorithm on facial 
expression recognition are carried out on JAFFE database. The database contains 213 images in which ten persons expressing three or four times the seven basic expressions (anger, disgust, fear, joy, neutral, sadness and surprise). We select 3 copies of the image of each individual per expression (210 images in all) for our experiments.  
Before extracting features, all images were preprocessed: 
aligned the position of face images using eye coordinates and 
cropped with a mask to exclude non-fac
e area. Then, images 
are resized to 6 4√ó64 pixels. Some sample images are shown in 
Fig. 6. For FER experiments, we divide the database into ten 
equally sized sets (each set is consist of only one person‚Äôs facial expression image): nine sets are used for training and the remaining one for testing. The above process is repeated so that each of the ten roughly equally sized sets is used once as the test set. The average result over all ten cycles is considered as the recognition rate of one trial.
  
 
  
  
  
    
Figure 6.  The samples of JAFFE images after preprocessing  
When solving equation (12), we find that the error tolerance 
Œµ is a very important undetermined parameter. It determines 
the accuracy of the classification algorithms.  We did an 
experiment on different value of Œµ, the results are shown in 
TABLE I.  
TABLE I.   FACIAL EXPRESSION RECOGNITION RATES ON DIFFERENT 
VALUE OF Œµ 
Œµ 0.01 0.005 0.003 0.0001 
Accuracy 60.95% 62.86% 62.38% 55.24% 
 
We can see that, the recognition rate improved  with the 
error tolerance Œµdecreasing at first, but then the recognition 
rate turned bad when the error tolerance Œµ become extremely 
small. The best recognition rate can be obtained when the error 
tolerance Œµ is 0.005. 
Finally, we compare the performance of our proposed 
algorithm to SRC, Principle Component Analysis (PCA), Kernel PCA (KPCA), Linear Discriminant Analysis (LDA) [13], and Gabor Histogram Feature+MVBoost [14]. All those four methods use SVM as classifiers. The results are shown in TABLE II. 
TABLE II.  THE FACIAL EXPRESSION RECOGNITION RATE COMPARISON 
OF SEVERAL ALGORITHM ABOUT FEATURE FORMATION AND SELECTION  
SRC+LBP SRC PCA 
[13] LDA 
[13] KPC A 
[13] Gabor Histogram 
Feature+MVBoost 
[14] 
62.9% 61.4% 53.8% 55.7% 53.6% 58.7%
 The best recognition rate of SRC+LBP reaches 62.9%, 
higher than SRC, also higher than traditional algorithms, such as PCA, LDA  and some freshly introduced algorithms, such as those based on Gabor Histogram Feature and MVBoost. 
VI.
 CONCLUTIONS  
A Novel approach for facial expression recognition based 
on Sparse Representation and LBP was proposed. Tested on JAFFE database, the proposed algorithm obtained satisfactory results. The performance of the proposed algorithm is better than that of the traditional algorithms such as PCA, KPCA and LDA etc, and also superior to some freshly introduced algorithms, such as FER based on Gabor Histogram Feature and MVBoost.   
FER, which is filled of excitement and challenge, is 
thriving in the pattern recognition and computer vision community. Many scientific workers and scholars in the world are devoted to it with the prospect of applying it to commerce. After decades of development, FER arrival to the commercial application era, not only happens in lab. Remarkable examples are Sony‚Äôs T200 camera and Hanson Robotics‚Äô apish robot. In our experiment, LBP as a way of feature extraction combined 
with Sparse Compressing Classification has a great success on 
facial expression recognition. However, the average performance of our proposed algorithm is still relatively low for commercial appliance. There are a lot of research works to do with FER.   
A
CKNOWLEDGMENT  
This paper was supported by Guangdong NSF (032356), 
Guangdong NSF (07010869), Open Project Fund of National 
Key Laboratory on Machine Perception, Peking University (0505) and Open Project Fund of National Key Laboratory on CAD &CG, Zhejiang University (A0703)  
R
EFERENCES  
[1] George Votsis, Nikolaos Doulamis, Anastasios Doulais, 
NicolasTsapatsoulis and Steanos Kollias, ""A Neural-Network-based Approach to Adaptive Human Computer Interaction"" Lecture Notes in Computer Science, Spinger-Verlang Press, Vol. 2130, pp. 1054-1059 
2001. 
[2] Donoho. D.L, ‚ÄúCompressed sensing‚Äù, IEEE Transaction on Information 
Theory, Vol 52, No. 4, pp. 1289-1306, Apr. 2006. 
[3] Wright. J, Yang. A.Y, Ganesh. A, Sastry. S.S, and Ma.Y, ‚ÄúRobust Face 
Recognition via Sparse Representation‚Äù, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol 31, No. 2, pp. 210-227, 
Feb.2009. 
[4] T. Ojala, M. Pietik¬®ainen and D. Harwood. ‚ÄúA comparative study of 
texture measures with classification based on feature distributions‚Äù,J. 
PatternRecognition vol. 29, No.1 pp. 51-59,  1996
.. 
[5] Timo Ahonen, Abdenour Hadid, and Matti Pietikainen, ‚ÄúFace 
Recognition with Local Binary Patterns‚ÄùM.  Lecture Notes in Computer 
Science,  Vol. 3021, pp.469-474,May.2004. 
[6] Candes. E.J, ‚ÄúCompressive sampling‚Äù, International Congress of 
Mathematicians, Aug. 2006. 
[7] Candes. E.J, Romberg. J, and Tao.T, ‚ÄúRobust uncertainty principles: 
Exact signal reconstruction from highly incomplete frequency information‚Äù, IEEE Transactions on Information Theory, Vol 52, No. 2, 
pp. 489-509, Feb. 2006. 
[8] Donoho. D.L, ‚ÄúFor Most Large Underdetermined Systems of Linear 
Equations the Minimal 
1A-Norm Solution Is Also the Sparest Solution‚Äù, 
                                                                                                                                          1754
Communication on Pure and Applied math, Vol 59, No. 6, pp. 797-829, 
May. 2006. 
[9] Candes. E.J, and Tao. T, ‚ÄúNear-Optimal Signal Recovery from Random 
Projections: Universal Encoding Strategies?‚Äù, IEEE Transaction on 
Information Theory, Vol 52, No. 12, pp. 5406-5426, Dec. 2006. 
[10] Donoho. D.L, and Tsaig. Y, ‚ÄúFast Solution of 1A-Norm Minimization 
Problems When the Solution May Be Sparse‚Äù, 
http://www.stanford.edu/tsaig/research.html.  
[11] S. Chen, D. Donoho, and M. Saunders, ‚ÄúAtomic Decomposition by 
Basis Pursuit,‚Äù SIAM Rev., vol. 43, no. 1, pp. 129-159, 2001. 
[12] Caifeng shan, Shaogang Gong, Peter W. McOwan, ‚ÄúFacial expression 
recognition based on Local Binary Patterns: A comprhensive study‚Äù, J. 
Image and Vision Computing, Vol. 27, No 6, pp 803-816, 2009. 
[13] Ying Zilu, Zhang Guoyi, ‚ÄúFacial Expression Recognition Based on 
NMF and SVM‚Äù, International Forum on Information Technology and 
Applications, Chengdu, China, pp. 612-615, 2009. 
[14] Liu Xiaomin, Zhang Yujin, ‚ÄúFacial Expression Recognition Based on 
Gabor Histogram Feature and MVBoost‚Äù, J.Journal of Computer 
Research and Development, Vol. 44, No.7, pp. 1089-1096, 2007. 
"
http://ieeexplore.ieee.org/document/7775081,"1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE
Transactions on Information Forensics and Security
 
 1 
  
 
Abstract‚ÄîAccura te biometric  identification under  real 
environments is one of the most critical and chall enging tasks to 
meet growing demand for higher security. This paper proposes a 
new framework to efficiently and accurately match periocular 
images that are automatically acquired under less -constrained 
environments. Our framework, referred to as semantics -assisted 
convolutional neural networks (SCNN) in this paper, incorporates 
explicit semantic information to automatically recover 
comprehensive periocular features. This strategy enables superior 
matc hing accuracy with the usage of  relatively smaller  number  of 
training samples which is often an issue with s everal biometrics. 
Our reproduci ble experimental results on four different publicly 
available databases suggest that  the SCNN based  periocular 
recognition approach  can achieve outperforming results, both i n 
achievable accuracy and matching time, for less -constrained 
periocular matching. Additional experimental results presented in 
this paper also indicate that the effectiveness of proposed SCNN 
architecture is not only limited to periocular recognition but it can 
also be useful for generalized image classification.  Without 
increasing the volume of training data, the SCNN is able to 
automatically extract more discriminative features from the input 
data than a single CNN, therefore can consistently improve the  
recognition performance . The experimental results in this paper 
validate such an approach to enable faster and more accurate 
periocular recognition  under less constrained environments.  
 
Index Terms ‚ÄîPeriocular recognition, deep learning, 
convolution neural  network , training data augmentation.  
I. INTRODUCTION  
ERIOCULAR  recognition is an emerging biometric 
modality that has attracted noticeable  interest in recent 
years and a lot of research effort ha ve been devoted to advance 
accuracy from the automated algori thms. The periocular region 
usually refers to the region around the eye, although there is no 
strict definition  or standard fro m research bodies like NIST [41]. 
Periocular recognition is believed to be u seful when accurate 
iris recognition cannot be ensured , such as under visible 
illumination [8], unconstrain ed environment [9] or when the 
whole face is not available , as illustrated  from some real -life 
samp les in Figure 1. It has also been shown that the periocular 
region is more resistant to expression variation s [10] and aging 
[11] as compared  with the face. In addition to serving  as an 
independent biometric modality, periocular information  can 
also be simultaneously combined with iris [2], [13] and/or face 
[15] to improve the overall recognition performa nce. However  
matching  periocular images, particularly under less constrained 
environment , is a challenging problem as this  region itself 
contains less information than the entire  face and often 
accompanied by high intra -class variations along with occlusio ns like from glasses, hair, etc. 
In recent years, Convolutional Neural Network (CNN) has 
gained popular ity for its strong ability to extract comprehensive 
feature s from the input data, especially for visual pattern s. It 
has demonstrated its robustness to the real -life intra -class 
spatial variation s. The CNN has many successful applications 
like hand -written character recognition [6], object detection 
[16], large -scale image classification [17] and face recognition 
[18]-[19], where CNN has significantly outperform ed 
traditional methods using handcrafted features or other learning 
based approaches. Therefore we have b een motivated to use 
CNN to achieve better performance  for the challenging 
periocular recognition problem.   
A. Our Wor k and Contributions  
Automated periocular recognition under less constrained 
environment has shown promising performa nce and underlined 
the ne ed for further research. Several databases, acquired under 
visible and near -infrared illuminations, have been introduced in 
the public domain [30], [34]-[36] and it can be observed that 
researchers require/use training samples from respective 
databases, primarily to select  or learn  best set of parameters.  
The performance achieved on these less -constrained databases 
is encouraging but requires further work. This paper atte mpts to 
address these two limitations for the automated periocular 
recognition.  
      In addition to  successfully investigating the strengths of 
CNN for the less -constrained periocular recognition , this paper 
introduc es the Semantics -Assisted CNN (SCNN) a rchitecture 
to fully exploit the discriminative information within limited Accurate Periocular Recognition under Less Constrained Environment  
Using Semantics -Assisted Co nvolutional Neural Network  
Zijing Zhao, Ajay Kumar  
P    
(a) 
   
 
 
 
 
 
 
  
(b)                                     (c)                                 (d) 
 
Figure 1: Periocular recognition is useful when (a) iris texture is degraded or  
when the faces are covered for (b) protection from environment, (c) during 
sickness or (d) during demonstrations or riot s [42]. 

1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE
Transactions on Information Forensics and Security
 
 2 
number of training samples. The key contributions of our work 
can be summarized as in the following.  
Our approach for periocular recognition using SCNN does 
not require training sam ples from target datasets , while 
achieving outperforming results, which is a key advantage over 
state-of-the-art approaches [2] and [10]. In our experiments, t he 
SCNN is trained with one database an d tested on totally 
independent/separate databases. The testing and training sets 
have mutually exclusive subjects and hi ghly different image 
quality as well as imaging condition s and/or equipment‚Äôs . The 
SCNN architecture  can also enable recovery of  more 
comprehensive periocular features from the limited training 
samples . Another key advantage of  proposed method in this 
paper is its computational simplicity, i.e., our trained model 
requires much less computational time for feature extraction 
and matching c ompared with other methods . Unlike earlier 
works, the trained models and executable files of our work are 
made publicly available [40] so that other researchers can easily 
reproduce our results or evaluate on new data bases . Finally, t he 
use of  SCNN is not only limited to the periocular recognition 
but can also be useful for  general image classification  task. By 
attaching branch CNN(s) that are trained with semantic 
supervision from the training data, the SCNN architecture can 
be easily used to extend and improve existing CNN based 
approaches  while limiting the general requirement of increase 
in training data for such performance improvement. The SCNN 
enables the deep neural network to fully learn the training data 
in conjunction with the semantical correlation and therefore can 
benefit  the final classification  task, especially when the size of 
training data is limited to build a very deep network. The 
structure of SCNN is easy to implement, and semantic 
annotation of the training samples is often included with the 
release of many public databases . 
B. Related Work  
In 2009, Park et al. [8] have investigated the feasibility of using 
periocular region for human recognition under various 
conditions. Bharadwaj  et al. [22] also support the usefulness of 
periocular recognition when iris recognition fails. There is also 
research work focusing on cross -spectrum periocular matching 
[5], where techniques of neur al network have been used. 
State -of-the art  work for periocular recognition includes [2], 
where good performance is obtained by fusing periocular and 
iris features/scores together . However, DSIFT feature  
extraction and the K -means clustering  used by this w ork for the 
periocular region are highly  time consuming . Another 
state-of-the-art approach by Smereka  et al. [10] proposes  the 
Periocular Probabilistic Deformation Model (PPDM), which is 
a variant o f their previous work, and promising performance 
has been reported. The PPDM uses a probabilistic inference 
model to evaluate the matching scores from correlation filters 
on patches of the image pair. However, this patch -based 
matching scheme is sensitive to scale variance among samples, 
which often exists in the challenging forensic  and security  
scenario s. More importantly, approaches in both [2] and [10] 
employ some samples in the target dataset fo r training or 
selection of parameter s, while our objective has been to 
develop a more effective approach , that does not require any 
samples from target datasets for training , that can deliver outperforming results  and is computationally simpler . 
As for the  development of CNN, LeCu n‚Äôs early work [6] for 
handwritten character recognition is one of the most typical 
applications of CNN in computer vision. Gradient based 
learning was used in that work so that CNN can learn from the 
training data effectively. In recent years  CNN becomes very 
popular due to its powerful feature extraction ability for visual 
pattern and robustness for challenging scenarios (typically for 
large intra -class variance ), and CNN based methods hold 
state-of-the-art performance for many computer vision tasks, 
such as image classification [17], object detection [16], etc. In 
2014, Sun et al. [18] and Taigman  et al. [19] have presented 
successful application of CNN on face recognition, which 
showed superior results even compared with human 
performance. Above two approaches have shown great 
potential of using CNN on biometrics, which is the primary 
motivation for us to develop the proposed CNN based method 
for the highly challenging periocular recognition problem.  
However, most of the existing CNN based methods require 
huge amount of data for training, which is the major bottleneck 
for its quick use for  many other computer vision tasks. This has 
motivated us to explore alternate strategies to considerably 
compensate lack of large dataset often required for the training.  
II. METHODOLOGY  
As discussed earlier , we were motivated to incorporate  CNN 
for the challe nging periocular recognition problem due to its 
known ability to extract comprehensive feature from image.  In 
this section we will first introduce the theoretical background of 
CNN and the practical architecture of our SCNN model in 
Section II.A, followed by detailing the application for the 
periocular recognition proble m in Section II.B and II.C.  
A. Semantics -Assisted Convolution Neural Network (SCNN)  
1) Basic Introduction to  CNN  
CNN is a biologically -inspired variants of multilayer  
perceptron  (MLP) and well-known  as one of typical deep 
learning architectures. CNN has shown strong ability to learn 
effective feature representation from input data especially for 
image/video unde rstanding tasks, such as handwritten character 
recognition [6], large -scale image classification [17], face 
recognition [18]-[19], etc. In t he following, we will briefly 
introduce the basic knowledge of a typical CNN architecture 
that is used in our and many other work.   
CNN is usually composed of convolution layers, pooling 
layers and fully connected  (FC) layers. At the output of each 
layer, there is often a nonlinear activate function, such as  
sigmoid, ReLU [1], etc. In our work, we adopt the basic CNN 
structure similar to AlexNet [7] and is shown in Figure 2 (say 
the periocular recognition problem as an example). The input 
image is passed through several convolutional units and then a 
few fully connected layers. The output of the last FC layer with 
N (number of classes) nodes would represent probabilistic 
prediction to the class labels.  
Each of the convolutio n unit s is composed of three 
components - a convolution layer, a max -pooling layer and a 
1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE
Transactions on Information Forensics and Security
 
 3 
ReLU (Rectified Linear Unit) activation function, as shown in  
Figure 2. For the convolutio nal layer, each channel of its output 
is computed as:  
 
( ) ( ) ( )( * )i ij ij j
jÔÄΩÔÄ´ÔÉ• y b k x  (1) 
where 
()iy  is the i-th channel of the output map, 
()jx  is the j-th 
channel of the input map, 
()ijb  is called the bias term, 
()ijk  is 
the convolution  kernel between 
()iy  and 
()jx , and * denote s 
the 2D convolution operation. 
()ijb  and 
()ijk  will be learned by 
back -propagation so that the convolution kernels are trained  to 
extract most useful features that are discriminative among 
different subjects.  
The pooling layer extracts one maximum or average value 
from each patch of the input channel. In our application, we use 
max-pooling with non-overlapping patches . As a resul t, the 
input maps, after convolution, are down -sampled with a scale 
determined by the pooling kernel. The pooling operation 
aggregates low -level features from the input to high -level 
representation and thus could achieve spatial invariance  among 
different samples.   
At the output of each pooling layer and the first FC layer (e.g., 
L7 in Figure 2), we choose the ReLU ( Rectified  Linear Unit)  [1] 
as the activation function:  
 
max( ,0)iiyyÔÇ¢ÔÄΩ   (2) 
The ReLU activation ensures the nonlinearity of the feature 
extraction process and is more efficient for training, compared 
with the traditi onal activation functions like sigmoid or t anh 
employed in other approaches [14]. 
The FC layers process the input as in conventional  neural 
networks:   
i i j ij
jy b x wÔÄΩ ÔÄ´ ÔÉóÔÉ•   (3) 
where 
jx  is the j-th element of the vectorized  input map to th e 
current  layer, 
iy  is the i-th element of the output map, which is 
also a vector. 
ib  and 
ijw  are elements of the bias and weights 
to be learned through training. The last FC layer, as usually 
configured in classific ation problem, is not followed by ReLU 
but a softmax  function:  
 
i
jy
i y
jey
eÔÇ¢ÔÇ¢ÔÄΩÔÉ•  (4) 
The use of softmax  function  in the final output of the network 
results in a 
1NÔÇ¥  vector with positive elements which are 
summed up to one. Each element then is treated as the 
probabilistic prediction of the class  label. The cross -entropy 
loss function is to be minimized, which is formulated as:  
 
( ) logt LyÔÇ¢ÔÇ¢ ÔÇ¢ÔÇ¢ÔÄΩÔÄ≠y   (5) 
where t is the ground truth label of the training sample. The loss 
function is minimized via back -propagation so that the 
predictions of the ground truth class of the training samples will 
approach to unity .  
2) Limitation of Contemporary CNN Based Approach  
In order t o achieve superior performance using CNN based 
method s, a common way is  to add more  layers to make the 
networ k deeper and more comprehensive, and/or devote more 
labeled training data because CNN is usually trained in a 
supervised  manner . For instance, the famous CNN architecture 
GoogLeNet [17] has 22 layers and later comes the Microso ft‚Äôs 
deep network with 152 layers [21]. Apparently , common 
researchers or companies could hardly afford to train such deep 
networks due to the lack of enough computational power. Also, 
as the network goes deeper, the need for t raining data grows 
accordingly, while in many research areas, it is difficult to 
acquire  enough labeled training samples like ImageNet [23]. 
Table 1 provides examples of several typical deep learning 
based approaches  and their employed train ing data. In reference 
[18] in Table 1, for instance, where the developed CNN is no t 
very deep (nine layers) , a total of ~200,000 face images from 
more than 10,0 00 people were used for training to achieve 
10 x 226 x 226
L1
Convolution10 x 113 x 113
L2
Pooling
+
ReLU20 x 103 x 103
L3
Convolution20 x 52 x 52
L4
Pooling
+
ReLU40 x 46 x 46
L5
Convolution40 x 23 x 23
L6
Pooling
+
ReLU1 x 250
L7: Features
Fully connected
+
ReLU1 x N
L8: Prediction
Fully connected
+
softmax15
1522
11
112
27
72
2 
Figure 2: Structure of the employed deep convolutional neuron network.  
 
 
 Table 1: Examples of several deep learning based approaches and their required  
number of training images.  
Approach  Task  Size of Training Data  
No. of Classes  No. of Samples  
CVPR [17] Image classification  1,000 1,281,167  
ICML [24]  Handwritten digits 
recognition  10 60,000  
T-PAMI [25]  Object detection  200 456,567  
CVPR [18] Face recognition  10,177  202,599  
CVPR [19] Face recognition  4,030  ~ 4,400,000  
 
 
 
 
 
1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE
Transactions on Information Forensics and Security
 
 4 
superior performance . However for other  popular biometrics 
modalities like iris or periocular, in the best of our knowledge, 
there is currently no single public database with that many 
images .  
Therefore, we are motivated to improve the performance of 
existing CNN based architecture in another way - to enhance 
CNN with supervision from explicit semantic information. 
When human recognizes  objects , for example  while 
recognizing a f ace image, one would analyze  not on ly the 
overall visual pattern  but also the semantic information, such as 
gender, ethnicity , age, etc., to judge whether the face image 
belongs to a  certain known person. Therefore, it is reasonable to 
believe that semantic information is helpful for the visual 
identification task. For a CNN that is trained with the identity 
label only, it is possible that the network is already capable of 
acquiring  semantic information . For instance , for the 
well-known deep learning model for face recognition, 
DeepID2+ , researchers discovered that although the network 
was trained using subject identities, certain neurons turn out to 
exhibit selectiveness to attributes like gender, ethnicity, age, etc. 
These semantic attributes contribute to discriminating identities  
[43]. However , such useful  semantic information is expected to 
be implicit ly learned by the CNN . It is not easy to answer the 
following questions : 
(1) How man y types of semantic information can be acquired ? 
Since the discriminative capacit y of a certain CNN is 
limited, we c annot guarantee that all the semantic 
information we prefer to have has already been included.  
(2) To what exten t the semantic information c an be analyzed by 
the trained CNN? Does it really help in the final 
identification ta sk, or could it be further improved?  
Above problems arise  due to the nature of training popularly 
employed for the CNN , i.e., the loss function is usually only 
related to the class labels, therefore it is hard to reveal how the 
semantic information c an be implicitly  acquired . In order t o 
address this issue, we propose to empower the CNN with the 
ability to analyze semantic information explic itly. The idea is 
very simple and illustrated in Figure 3.  
3) Semantics -Assisted CNN  
As illustrated in Figure 3, we simply add a branch, which is also 
a CNN, to the existing CNN. The attached CNN is not trained 
using the identity of the training data but the semantic groups. 
For example, we could train CNN2 using the gend er 
information of the training sample, i.e., let the CNN2 be able to 
estimate the gender instead of identity, and train CNN3 using 
the ethnicity information. After the CNNs are trained, we can 
combine the output of each CNN in the way of feature fusion. 
We refer to such extended structure of the CNN as 
Semantics -Assisted CNN  (SCNN  for sh ort). Despite the 
simplicity of t his idea , it can inherently improve the original 
CNN by adding more discriminative power to it, w hich has 
been shown from  the experiments de scribed in Section 3. 
Theoretically , the SCNN has  the following benefits:  
ÔÅ¨ Instead of letting the semantic information be learned 
from the identities by the CNN in an unpredictable and 
uncontrollable way, SCNN allows us to explicitly  recover  the preferred s emantic information that can be  helpful for  
the identification t ask. As a result, the feature 
representation  from the SCNN  is accompanied by more 
reliable semantic information that is closer to mechanism 
in human visual system.   
ÔÅ¨ The training s cheme for SCN N can reuse the same set of 
training data but just labeled in another way than the 
simple identities. Since the labeling scheme is variable, 
the branches of SCNN learn the training data from 
different points of view, which is equivalent to increasing 
the d ata volume without really adding the number of 
training samples. This c an relax the constraints on the 
requirements of  enormous training data for deep neural 
networks to som e ext ent, i.e., instead of purs uing for 
superior performance from a single CNN, we enhance the 
joint performance of branches of CNNs with fewer 
amounts of training dat a. 
ÔÅ¨ The SCNN architecture and training scheme is naturally 
compatible for most of the existing CNN based 
approaches . What we need is just to train some 
independent CNNs with  semantic grouping labels and 
judiciously combine the features from multiple CNNs  to 
benefit from such training , as the semantic annotations of 
training samples are also available for many public 
databases. In a ddition, the architecture of SCNN is highly 
friendly for paralle l computing platforms.  
B. Application for Periocular Recognition  
As discussed earlier , CNN has been successfully used for the 
face recognition in sever al state -of-the-art approaches [18]-[19]. 
Considering t hat the periocular region is actually a part of face  
and also presents  some  structural information (eyebrow, 
eyelids, eyeball, etc.), it is reasonable to expect that CNN  can 
be effective  for the  periocular recognition  problem. Howev er, 
as compared with such related  work, we are constrained by lack  
of large -scale periocular databases that are usually required to 
sufficiently train a deep neural network. Therefore  we 
developed and investigated SCNN for the periocular 
recognition proble m. 
1) Network Structure and Supervision Information  
The detailed SCNN structure used for the periocular 
recognition is shown in Figure 4. In order t o examine the 
CNN n ‚Äì Trained by Semantic InformationCNN 2 ‚Äì Trained by Semantic InformationCNN 1 ‚Äì Trained by Identity
Joint 
Feature or 
Prediction... 
 
Figure 3: Structure of the proposed Semantics -Assisted CNN (SCNN). While 
first branch is trained by the label of the intended tasks, other branch CNNs 
are trained using different semantic information, t hen the branches are joint in 
the end to get a comprehensive feature representation or perform score fusion.  
 
1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE
Transactions on Information Forensics and Security
 
 5 
impact of adding  branch to an existing CNN, we simply 
designed one bran ch that is trained with semantic information, 
denoted  as CNN2 in  Figure 4. While CNN 1 is like the ones 
commo nly tra ined with the subject identities from  the training 
samples , CNN2 is designated to be train ed with the side (left or  
right)  and the gender information. More specifically, we 
labeled the training data as follows , also shown in Figure 5:  
ÔÉ¨
ÔÉ≠
ÔÉÆ
 0 - Left and Male,  
1 - Right and Male,  
2 - Left and Female,  
3 - Right and Female.  
The reason fo r using left/right and gender information is that 
human s also tend to incorporate such judgment by visuall y 
inspecting the presented periocular image s, although such 
accuracy may not reach cent percent level. Therefore there is 
some scientif ic basis to believe that CNN can learn to 
distinguish above semantic information from the periocular  
pattern s and assist in the identification task. Another reason for 
using gender information is that the genders of subjects are 
often  included i n the metad ata of  many publicly available 
dataset s, such as UBIpr [36]. Therefore we can directly use 
those labels to train CNN2. Other possible and useful semantic 
information include iris color (light/dark), ethnicity , shape of 
eyebrow,  etc.  
 Using such additional semantic information to supervise the network  make s the overall architecture and learning process of 
SCNN similar to multi -label learning [44] to some extent. 
However, the principal difference is t hat, the introduction of 
semantic labeling in our model aims to assist/supplement the 
prediction of subject identity labels, i.e., they are inequally 
important, while in traditional multi -label learning, the multiple 
labels are usually in equal positions. In addition, the learning 
processes of identities and other semantic information are 
separately undertaken to maximally ensure the explicitness of 
semantic learning and compatibility to other CNN based model, 
while in general multi -label learning, features  are usually 
jointly learned for predicting different lables. Nevert heless, in 
spite of the diffrent iation between the identity labels and other 
supportive  labels, the  s emantic learning process ( e.g., CNN2 
itself) can also be conducted in the manner of mu lti-label 
learning alternatively.  
2) Training Protocol and Data Augmentation  
Among the original training samples, the last sample of each 
subject is selected to form the validation set, which is tested in 
every certain amount of iterations to observe whether  the 
training process is converging i n a right  direction or not.  
Furthermore, it is observed that the periocular images from 
the training set  are well aligned and scaled to a similar level, 
while the samples from independent test datasets and real 
applica tions may have misalignments and scale  variations . 
Such inconsistency can also be observed from the image 
samples in Figure 6.  
If the deep network is trained with the well aligned and 
scaled images, it may not be effectively gene ralized to other 
datasets or d ata acquired  by real applications.  In order to  
address such problem s, we firstly augmented the training data 
with a different scale to simulate scale inconsistency in the test  
environment. Then we applied random cropping durin g the 
training process to ensure that the network can accommodate  
spatial variations  among the periocular images.  The scale 
augmentation and random cropping process is also  illustrated in 
Figure 7. As illustrated in this, each ori ginal of the image in 
CNN 2CNN 1
Feature or 
Score Fusion10 x 226 x 226
L1
Convolution10 x 113 x 113
L2
Pooling
+
ReLU20 x 103 x 103
L3
Convolution20 x 52 x 52
L4
Pooling
+
ReLU40 x 46 x 46
L5
Convolution40 x 23 x 23
L6
Pooling
+
ReLU1 x 250
L7: Features
Fully connected
+
ReLU15
1522
11
112
27
72
2
10 x 226 x 226
L1
Convolution10 x 113 x 113
L2
Pooling
+
ReLU20 x 103 x 103
L3
Convolution22
11
112
2
1 x 200
L5: Features
Fully connected
+
ReLU20 x 52 x 52
L4
Pooling
+
ReLU 
Figure 4: Structure of the employed SCNN for the periocular recognition.  
 
 
 
Semantical Annotation
Left Right
Male Female Male Female
Label: 0                       1                       2                       3
 
Figure 5: Semantical labeling used in our implantation  to train CNN2.  
 
1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE
Transactions on Information Forensics and Security
 
 6 
training set is automatically  cropped from its center with a size 
of 
0.6 0.6whÔÇ¥ , where w and h are its original width and height 
respectively. The original images and its cropped patch are 
resized to
300 240ÔÇ¥ , then padded with symmetric  edges filled 
with zeroes  to a size of 
300 300ÔÇ¥ . So far one original 
periocular image could generate two training samples.  As a 
result , we have 6,270 samples for training and 448 samples for 
validation w hile training for each side of the periocular  image s. 
Furthermore, during the training process, each training sample 
would be cropped by a 
240 240ÔÇ¥  window randomly placed 
within the image region before entering the first layer of the 
netw ork. Such  random ized cropping  process  from one training 
sample could produce abundant samples that have random ized 
misalignments with others. In this way,  the network can be 
enforced to learn to extract features that are robust to the 
misalignment s.  
3) Visualization of Trained SCNN  
Once the networks have been trained, CNN1  is expected to 
lock-into features that are directly relevant to the subject 
identities, while CNN2 is expected to analyz e the features that 
are more related to side and the gender differe nce. In order to 
observe the difference among f eatures extract ed by the two 
CNNs , we have visualize d the filter kernels from the first two 
convolutional layers  of trained CNN1 and C NN2  in Figure 8.  
We can visuall y observe from Figure 8 that: 1) Overall both 
CNNs were not trained sufficiently. Compared with 
convolutional kernels trained with large amount  of samples (e.g., those in [7]), a number  of kernels here remain flat or noisy, 
for which it is less likely to extract useful information.  
Insufficiently trained network parameters usually results in 
certain levels of over -fitting.  2) Despite the over -fitting concern , 
the convolutional filter ker nels of CNN1 and CNN2 are quite 
different. Critical  kernels in CNN2 are sharper and present 
more visual salience , therefore might  be more sensitive to small 
texture,  edges  or corners  than the filters in CNN1. This 
indicates CNN2 can provide complementary i nformation that 
CNN1 was not able to learn due to lack of  sufficient training 
data. Although the features extracted by CNN2 are not directly 
related to the subject identities, it is reasonable to expect that 
those visual features could assist  CNN1 to form a more 
comprehensive visual representation of the periocular image, 
therefore help to distinguish different subject s finally.  
Original Imagewh
0.6w0.6h
300
300240
300 240
Bi-scaling
240
240
240
240Training
TrainingPre-processed In real-time
Random cropping 
Figure 7: Illustrat ion of scale augmentation and random cropping. Each 
original image is augmented to two samples with different scales, and each 
augmented sample would be cropped by a smaller window that is randomly 
placed before entering the network for each epoch  of the t raining process.  
    
(a) UBIpr (training)  
   
(b) UBIRIS.v2 (testing)  
   
(c) FRGC (testing)  
   
(d) FOCS (training and testing)  
   
(e) CASIA.v4 -dista nce (testing)  
Figure 6: Sample images from the databases we used in the experiments. 
Scale variance and misalignment are common in the testing environment.  
 
CNN1:  
First convolutional layer (L1)  Second  convolutional layer ( L3) 
 
 
 
CNN2:  
First convolutional layer (L1)  Second  convolutional layer ( L3) 
 
 
 
Figure 8: Visualization of the filter kernels from the fir st two convolutional 
layers of trained CNN1 and CNN2 respectively . 
 
 
 

1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE
Transactions on Information Forensics and Security
 
 7 
C. Featur e Vector  and Verification Score  Generation  
The CNNs we use are trained in a classification protocol, i.e., 
the category  or identity of the input data is known and fixed. 
Therefore  this network can be directly used in some 
classification or id entification tasks. However, in biometrics, 
one-to-one matching for probably unseen subj ects is the key 
problem and needs to be evaluated.  Therefore, we need to 
generalize the trained model to separated subjects that are not 
included in the training set, and formulate  one-to-one matching 
scheme.  
Similar to [18], we use the output of second last layer (L7 in 
CNN1  and L5 in CNN2) as the feature representation of the 
input data. While the last layer represents the class prediction 
during the training process, the second last layer should contain 
the most relevant and aggregated information that can 
contribute to dis tinguishing the classes or identities. Therefore, 
it is reasonable to use the output of the second last layer as the 
feature representation and generalize the model to unseen 
subje cts. Once we get the layer output vect ors, we first 
normalize  them by l2 norm, then apply  PCA to reduce the 
dimensionality of the vector.  For the SCNN architecture, we 
simply concatenate the two independently normalized output 
vectors to form a longer vector before PCA. In our experiments, 
the dimension of output vectors after PCA  is set to 80, for both 
the single CNN and SCNN cases. Then the joint Bayesian 
scheme [33] is utilized to  predict the similarity between a pair 
of feature vectors. The joint Bayesian is primarily designed for 
face verification,  in which a face (equivalent to the periocular 
feature vector here) is represented by:  
 
f=Œº+Œµ  (6) 
where 
f  is the observation, in this paper the feature vector  
after PCA , 
Œº is the identity of the subject, 
Œµ  is the intra -class 
variation. 
Œº  and 
Œµ  are assumed to be two independent 
Gaussian variables following  
( , )NÔÅ≠0S  and 
( , )NÔÅ•0S  
respectively , then the covariance of two observation  is: 
 
1 2 1 2 1 2 cov( , ) cov( , ) cov( , ) ÔÄΩÔÄ´ ff Œº Œº Œµ Œµ  (7) 
The joint distribution of a pair of observations 
12{ , }ff  is 
considered . Let 
IH  denote the intra -person hypothesis 
indicating that two observations are from the same person, and 
EH
 the extra -person hypothesis. Under 
IH , since 
1Œº  and 
2Œº  
are the same, 
1Œµ and 
2Œµ  are independent, the covariance matrix 
of the  distribution 
12( , | )I PHff  is: 
 
+
+IÔÅ≠ ÔÅ• ÔÅ≠
ÔÅ≠ ÔÅ≠ ÔÅ•ÔÉ©ÔÉπÔÄΩÔÉ™ÔÉ∫
ÔÉ´ÔÉªS S S
Œ£S S S  (8) 
On the other hand, under 
EH , 
1Œº and 
2Œº  are also independent, 
therefore the covariance matrix has become:  
 
+
+IÔÅ≠ÔÅ•
ÔÅ≠ÔÅ•ÔÉ©ÔÉπÔÄΩÔÉ™ÔÉ∫
ÔÉ´ÔÉª0
0SS
Œ£SS  (9)  With above conditional joint probabilities, the log likelihood 
ratio which tells the difference between intra - and extra -person 
probabilities can be obtained in a closed form:  
 
T T T 12
1 2 1 1 2 2 1 2
12( , | )( , ) 2( , | )I
EPHrPHÔÄΩ ÔÄΩ ÔÄ´ ÔÄ≠fff f fŒëffŒëf f Gfff  (10) 
where  
 
( ) ( )ÔÅ≠ÔÅ•ÔÄΩ ÔÄ´ ÔÄ≠ ÔÄ´Œë S S F G  (11) 
 
1
ÔÅ≠ ÔÅ• ÔÅ≠
ÔÅ≠ ÔÅ≠ ÔÅ•ÔÄ≠ÔÄ´ ÔÄ´ ÔÉ©ÔÉπ ÔÉ©ÔÉπÔÄΩÔÉ™ÔÉ∫ ÔÉ™ÔÉ∫ÔÄ´ ÔÄ´ ÔÉ´ÔÉª ÔÉ´ÔÉªS S S F G G
S S S G F G  (12) 
 The covariance matrix 
ÔÅ≠S  and 
ÔÅ•S  can be estimated using 
an EM based algorithm as detailed in [33], and the log 
likelihood ratio 
12( , )rff  is used as the similarity score in our 
one-to-one matching scenario.  
III. EXPERIMENTS  AND RESULTS  
In this section we provide the details on the experiments and 
analyze the results. The experimental de tails on the periocular 
identification are firstly provided and this is followed by details 
on supporting experiments for the image classification.  
A. Periocular Recognition  
1) Training and Testing Datasets and Protocol  
We use follo wing publicly available datab ases for the 
experiments . Two different databases were employed for 
training the deep neural networks and three separate databases 
were employed for the testing . 
 
ÔÅ¨ UBIpr [36] - for training  
We employed UBIpr periocular database [26] for training the 
SCNN  for the visible spectrum . This database originally 
contains 5,126 images for each of left and right perioculars 
from 344 subjects. However, we are also employing a subset of 
UBIRIS.v2 database  [4] for separate test experiments , which 
has some overlapping subjects with the UBIpr database. In 
order to ensure that subjects of training set and testing set are 
mutually exclusive,  we removed these overlapping subjects 
from UBIpr database before we perform training on the 
network. As a result, we only have 3359 periocular images 
from  each of the two sides of 224 subjects. Such a scale is 
relatively small as compared w ith those in  the training 
protocols in other typical deep learnin g work like ImageNet [27] 
or LFW [28]. Therefore, the application scenario is good for 
validating the ability of SCNN f or learning comprehensive  
information from limited size of training data.  
 
ÔÅ¨ UBIR IS.v2 [4] 
The UBIRIS.v2 database is prima rily released for evaluation of 
at-a-distance iris segmentation and recognition algorithms 
under visible illumination and challenging imaging 
environment. Since the eye images in this da tabase contain 
surrounding regions of the eye, it is possible to perform  
periocular recogn ition on the UBIRIS.v2 database. Similar to 
as in [2], we use a subset of 1,000 images from this database 
1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE
Transactions on Information Forensics and Security
 
 8 
that is released in NICE.I comp etition [29]. This subset 
contains left and right eye images together from 161 subjects 
that are captured from 3m to 8m, bringing serious scale 
inconsistency. Some images only contain the eye region 
without eyebrow and other su rrounding texture which makes 
the task of perioc ular recognition  highly challenging . Some 
sample images are shown in Figure 6(b). 
 
ÔÅ¨ FRGC [30] 
The dataset of Face Recognition Grand Challenge (FRGC) is 
released by the National Institute of Standards and Technology 
(NIST) and has been primarily for the evaluation of new 
algorithms for the automated face recognition. Similar to  as in  
[2], we automatically extrac ted the periocula r region from the  
original face images of FRGC using publicly available face and 
eye detector [31]-[32]. A subset of 540 right eye images  from 
163 subjects , same as also the ones used in [2], were employed 
in the experiments.  Some sample images are reproduced in 
Figure 6(c).  
 
ÔÅ¨ FOCS [34] - for training  and testing  
The Face and Ocular Challenge Series  (FOCS) dataset is also 
released by NIST and contains face, ocular images and videos. 
We employ ed the ‚ÄúOcularStillChallenge1 ‚Äù section , which  
consist s of 4,792 left and 4,789 right periocular images from 
136 subjects that are cropped from face video clips acquired 
under near -infrared (NIR) spectrum. The periocular samples 
from thi s dataset, as shown in Figure 6, suffer from serious 
illumination inconsistency and misalignment s, therefore this 
dataset is considered as highly challenging.  We used 3,262 left 
and 3,259 right periocular images of the first 80 subjects to train 
the CNNs and used the remaining images from 56 subjects for 
testing. Again, such a scale of training samples and subjects is 
small compared with other typical deep learning tasks.  
 
ÔÅ¨ CASIA.v4 -distance [35] 
CASIA.v4 is t he first publicly available long -range iris  and face 
database acquired u nder NIR illumination , which is released by 
the Center for Biometrics and Security Research (CBSR) from 
the Chinese Acade my of Sciences (CASIA). The full database 
contains  2,567 images from 142 subjects in single session. The 
standoff distance of the subjects to the camera is from 3 meters 
away. Similar to FRGC, w e used publicly available eye 
detector [31]-[32] to automatically segment left periocular  
images which are used in our experiments. The first eight 
samples of each subject , excluding a few badly segmented 
images , were used for the periocular matching experimen t.  
Above  datasets were selected  for evaluation  because of the 
availability of periocular images acquired under less constrained environments that are close to  real world scenarios. 
The selected subsets from FRGC and UBIRIS.v2 contain 
multi -session data an d exhibit obvious scale/illumunation 
variation. Samples in FOCS database suffer from sig nificant 
illumination degradation  and misalignment. Images from  
CASIA.v4 -distance are  more consistent than the other three 
databases, but were acquired at a distance an d some contain 
artifacts like glasses and/or hair, therefore also represent less 
constrained scenari os. In addition, networks for visible and 
NIR spectrums were trained separately due to the significant 
difference between the image properties.  
It is import ant to clarify that during our (reproducible [40]) 
experiments, the SCNN is tested in totally cross -database  
manner , i.e., not only the su bjects from the training and test  set 
sets are totally separated, the databases themselves are 
independent  from traini ng for three sets  of experiments. 
However, the methods we are going to compare  with, [2] and 
[10], both requir e some  samples of the target databases for the 
training. In order to compare with the be st performance of [2] 
and [10] as well as to ensure the fairness in such comparison, 
we still divide the target datasets into training and testing sets, 
as summarized  in Table 2. For example, 96 samples of the first 
19 subjects in UBIRIS.v2 were used to train the models [2] and 
[10], the remaining were used for test as in [2], [10] and also for 
our method. Such a configuration is highly disadvantageous to 
our methods because the inter -database variance is always a 
key factor for the performance of all learning based methods. 
However, our method has still been ab le to achieve 
outperforming results as detailed later.  
We perform periocular mat ching using  the all -to-all protocol, 
i.e., every image is matched to al l the other images in the testing 
set, and all the generated matching scores are taken into 
calculation o f the receiver operating characteristic (ROC) curve.  
Such a protocol is considered to be highly challenging because 
one bad sample may result in several poor genuine scores, 
which drops the overall matching performance.  
2) Effectiveness of SCNN  
We firstly ex amine the i mpact of the added branch that has been 
trained with the semantic information. We have compared the 
performance of a single CNN, i.e., only CNN1 in Figure 3, with 
the performance of the extended SCNN. The  results from t he 
verification  experiments are illustrated i n Figure 9.  
We can observe from Figure 9 that the SCNN consistently  
achieves better performance than that of original or single CNN. 
This observation suggests  that the newly added CNN2 which is 
trained with semantic supervision has been successful in 
contributing to some useful information that is not reinforced in 
CNN1, and therefore improving the overall discriminative Table 2: Summary of the employed databases for training and testing.  
Spectrum  Visible  Near Infrared (NIR)  
Division  Training set  Testing set  Training set  Testing  set 
Dataset  UBIpr  UBIRIS.v2  FRGC  FOCS  FOCS  CASIA.v4 -distance  
Standoff  distance  4 ‚Äì 8m 3 - 8m N/A N/A N/A ‚â•3m 
No. of subjects * 224 171(19/152)  163 (13/150)  80 56 141 (10/131)  
No. of image s* left: 3,359  
right: 3,359  1,000 (96/904)  540 (40/500)  left: 3,2 62 
right: 3,259  1,530  1,077 (79/998)  
* In the bracket ( a/b) means a subjects  or images  were used for training for methods [2] and [10] (not for our method) , remaining  b subjects  or images  were 
used for testing.  
 
 
 
 
 
1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE
Transactions on Information Forensics and Security
 
 9 
power of the network. In theory, we can add more branches that 
are trained with different semantic information ( e.g., iris color) 
to further improve the final recognition accuracy. However, the 
need for computational power would also increase  and the 
trade -off may need to be made according to the applications. In 
our example, since CNN2 shown in Figure 4 has a relatively 
simpl ified structure, the additional training cost is minor .  
3) Comparison with Earlier  Work on Periocular Recognition  
We also compared the performance o f our approach with 
state-of-the-art approaches [2], [10] on the periocular 
recognition problem. While [2] is our previous work, w e have 
caref ully implemented the method s in [10] with the help of the 
original  authors. The t est protocol s were  kept exactly  the same 
for different approaches during the experimental process and 
therefore the comparis ons of ROC /CMC curves are  fair. 
However  several  factors can be firstly clarified here to ensure 
clarity in understanding the experimental c omparisons.   1) For U BIRIS.v2, we use the 1,000 image set that was 
employed f or the NICE.I  competition . This subset is  the same 
as was used in [2] but different  from the one in [10]. In [10], test 
images were gathered from the full dataset, but only those 
acquired from 6 -8 meters were used, while the 1,000 image set 
in [2] included samples acquired from 3 -8 meters. Due to the 
relatively consistent imaging distance, the subset used in [10] 
involves much less scale variance than those  in [2] and also in 
this paper . As a result , the performance from our experiment 
using exact method in [10] is not reproduced as good as what 
appears to be in [10] and this is reasonable  due to the difference 
in selection of image s as explained above . 
2) For FRGC, we also used the same subset as in [2] but 
different from the one  used in [10]. As described before , the 
subset we used contains 540 periocular images which were  
automatically  segmented  from the original fa ce images and 
therefore may suffer from some misalignment. Moreover, 
images in this subset were acquired from various sessions  with 
certain time lapse  and different imaging environments , which 
increases the difficulty for accurate recognition. However,  the   
(a) UBIRIS.v2                                                                                 (b) FRGC  
 
(c) FOCS                                                                           (d) CASIA.v4 -distance  
Figure 9: ROC curves of the periocular verification using SCNN and comparison with single CNN and other state -of-the-art methods for different databases.  
10-310-210-110000.10.20.30.40.50.60.70.80.91
False Accept RateVerification Rate
  
SCNN (EER: 10.98%)
CNN1 (EER: 13.76%)
TIFS'15 (EER: 34.96%)
TIP'13 (EER: 24.49%)
10-310-210-110000.10.20.30.40.50.60.70.80.91
False Accept RateVerification Rate
  
SCNN (EER: 10.93%)
CNN1 (EER: 11.88%)
TIFS - EER: 27.55%
TIP (EER: 19.46%)
10-310-210-110000.10.20.30.40.50.60.70.80.91
False Accept RateVerification Rate
  
SCNN (EER: 10.47%)
CNN1 (EER: 11.79%)
TIFS'15 [10] (EER: 20.09%)
TIP'13 [2] (EER: 23.90%)
10-310-210-11000.50.60.70.80.91
False Accept RateVerification Rate
  
SCNN (EER: 6.61%)
CNN1 (EER: 7.27%)
TIFS'15 (EER: 10.46%)
TIP'13 (EER: 8.27%)
1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE
Transactions on Information Forensics and Security
 
 10 
subset used in  [10] only consist s of images captured in 
consistent  illumination and background in single session , and 
the periocular regions were manually  segmented. Therefore, it 
is also a reasonable explanation for the drop  in performance in 
our reproduced results, over the ones shown in [10] using 
manual segmentation.   
3) For FOCS, we used fixed division of training and testing sets 
as shown in Table 2, while the original setup in [10] used 5-fold 
cross validation for  the entire  dataset. Although the  subsets 
used in our experiment  and their original experiment  are not 
exactly the same, the quality of images is observed to be quite 
similar . Therefore our reproduced resul t is very close to those 
appearing in reference [10]. 
The verification  results  (ROC)  for above comparison s are 
also shown in  Figure 9, while the identification results (CMC) 
are shown in  Figure 10. It can be observed from the 
experimental results in these two figures that the proposed 
approach using SCNN consistently outperforms  the two  
state-of-the-art approaches.   
In order to ascertain statistical significance of the 
improve ments, we have conducted the significance test for the 
ROC curves usi ng the method described in [12], which judges Table 3: Results of significance test for comparison of ROCs using method  [12]. 
p-value indicates the probability of the null hypothesis that two methods have no 
difference statistically.  
Comparison  p-value  
UBIRIS.v2  FRGC  CASIA.v4
-distance  FOCS  
SCNN &  TIP‚Äô13 [2] < 1e-4 < 1e-4 < 1e-4 < 1e-4 
SCNN &  TIFS ‚Äô15 [10] < 1e-4 < 1e-4 < 1e-4 < 1e-4 
*  The computed z-statistics are too large that the corresponding p-values 
exceed double precision, therefore expressed as  < 1e-4. 
 
 
 
 
 Table 4: Comparison of time required to match two periocular images by 
different approaches, f rom Matlab im plementation running on a computer with 
Linux OS, 16 GB RAM, 3.4 GHz Intel i7-4770  CPU (4 cores) and NVIDIA 
GeForce GTX 670  GPU.  
Approach  Major Time Consuming 
Operations  Matching Time (s)  
GPU  CPU  
proposed  convolution, matrix 
multiplication  0.013  0.183  
TIP‚Äô13 [2] DSIFT feature extraction , 
K-means clustering  / 15.478  
TIFS ‚Äô15 
[10] Gabor feature extraction , 
correlation filter matching  / 1.441  
 
 
 
 
  
(a) UBIRIS.v2                                                                                 (b) FRGC  
 
(c) FOCS                                                                            (d) CASIA.v4 -distance  
Figure 10: CMC curves of the periocular verification using SCNN and comparison with state -of-the-art methods for different databases.  
1001011020.70.750.80.850.90.951
RankRecognition Rate
  
SCNN (Rank 1 = 82.43%)
TIP'13 [2] (Rank1 = 74.56%)
TIFS'13 [10] (Rank1 = 71.08%)
1001011020.70.750.80.850.90.951
RankRecognition Rate
  
SCNN (Rank 1 = 91.13%)
TIP'13 [2] (Rank 1 = 85.36%)
TIFS'15 [10] (Rank 1 = 78.34%)
1001010.70.750.80.850.90.951
RankRecognition Rate
  
SCNN (Rank 1 = 96.93%)
TIFS'15 [10] (Rank 1 = 91.83%)
TIP'13 [2] (Rank 1 = 81.16%)
1001011020.970.9750.980.9850.990.9951
RankRecognition Rate
  
SCNN (Rank 1 = 98.90%)
TIP'13 [2] (Rank 1 = 98.82%)
TIFS'15 [10] (Rank 1 = 97.61%)
1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE
Transactions on Information Forensics and Security
 
 11 
from the area under the curve (AUC). Table 3 shows the 
significance level  (p-value)  of the difference of the SCNN 
based method over the comparative methods [2] and [10]. The 
results indicate  that, by the commonly used confidence level  of 
95%, our approach significantly outperforms th ese two 
methods ( p-value < 0.05) on all the employed datasets.  
It may be noted that [10] perform ed poorly on the 
UBIRIS.v2 set because it ad opts the patch based matching 
scheme while , as explained above,  the 1,000 -image set of 
UBIRIS.v2 used in our experiment suffers from serious scale  
varia tions among the samples, which results in significant loss 
of patch correspondence.  The approach from [2] which uses 
DSIFT features is more robust to scale varian ce, however the 
extraction of DSIFT fea ture is esp ecially time consuming. In 
contrast, our approach not only performs better than both of  the 
baseline approaches on different  databases, but is also 
computationally simpler for the deployment using the train ed 
network . Table 4 presents  the summary of the average time 
required for the feature extraction for the considered 
state-of-art approaches. These tests were performed usi ng the 
Matlab  wrapper and C++ implementation  running on a 
computer with Linux OS, 16 GB RAM, 3.4 GHz Intel¬Æ Core‚Ñ¢ 
i7-4770  CPU (4 cores) and NVIDIA ¬Æ GeForce GTX 670  GPU.  
It can be observed that the proposed approa ch is much faster  
due to the  straightforward architecture and the use of GPU 
could further reduce the c omputational time.   
B. Image Classification  
In order  to examine that the proposed SCNN architecture is not 
only effective  for the periocular recognition but can also be 
useful f or more general problems, we performed experiment for 
image classification on the CIFAR -10 dataset  [37].  
The CIFAR -10 dataset contains 60,000 32 √ó32 color images 
from 10  classes. Among these  images, 50,000 images are for 
training and 10,000 are for testing.  Figure 11 shows some 
randomly selected samples from each class.  As we can see from  
Figure 11, although the number of classes is not large, the 
intra-class variation  is significant and the resolution is also 
small er, which brings certain challenge for cla ssifying those 
images. The CIFAR -10 has therefore emerged as  a popular 
dataset for evaluating image cl assification algori thms along 
with others like ImageNet a nd CIFAR -100, etc.  
Since the SCNN is devel oped to enhance existing CNN 
based approaches, we select a baseline  CNN to  ascertain the 
improvement . We adopt the CNN originated from 
Krizhevsky ‚Äôs cuda -convnet  [38], re -implemented and  
introduced in the Caffe tutorial [39]. Although the selected 
CNN i s not the state-of-the-art for CIFAR -10 in terms of 
performance, we chose  it because this model  is publicly 
available under Caffe, the deep learning frame work employed  
in the paper, and it is also quick to train. For simple annotation, 
we refer to this net work as cuda -convnet . By following the 
tutorial, we can quickly get an accuracy of about 75% on the 
CIFAR -10 test set. The n we trained a branch CNN to learn the 
semantic features of the images in CIFAR -10 in order to build 
the SCNN a rchitecture. We define one possible groups of 
semantic inform ation for the classes in the CIFAR -10 dataset as 
follows , also shown in  Figure 12. 
ÔÉ¨
ÔÉ≠
ÔÉÆ artificial  
ÔÅª rectangular , has wheel: (aut omobile, truck)  
no/invisible wheel: (airplane, ship)  
Natural  
ÔÅª round , short: ( cat, dog, bird , frog ) 
slim, long : (deer, horse)  
 
With above division, the entire  dataset is grouped into four 
semantical classes. It may be n oted that this is not the unique or 
the optimal division, but it is an easy -to-understand scheme  to 
start with.   In order to  obtain  a branch CNN that was trained to 
acquire  above semantic features, we simply duplicate the 
structure of the base cuda -convne t but replace the last fully 
connected layer having 10 neurons with a new fully connected 
layer with four neurons, since the task now is to recognize the 
four semantic groups. We then just repeat, as described in Caffe 
tutorial , but train the new network w ith newly labeled data . We 
refer to this new CNN as cuda -convnet -s. Again, above 
configuration is made because of the ease to execute and one airplane  
 auto- 
mobile  
bird 
cat 
deer 
dog 
frog 
horse  
ship 
truck  
 
Figure 11: Sample images from each class of CIFAR -10 dataset.  
 
Semantical Annotation
Artificial Natural
Retangular,
Has wheelNo/invisible
wheelRound , 
ShortSlim, 
Long
Label: 0                       1                       2                       3
 
Figure 12: The semantical group label ing used in our experiment to train 
cuda -convnet -s. 
 
 
 
1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE
Transactions on Information Forensics and Security
 
 12 
has many choice s for actual applications. We then built an 
SCN N with the architecture as in  
Figure 13. As shown in th is figure, we combine the branch CNN 
and the original one to obtain  an extended structure. The 
components highlighted in red are retrained after the 
combination to aggregate the long concatenated features, and 
this process can be considered as a kind of finetuning. Since the 
number of layers to be retrained is small, the finetuning is very 
fast.  Table 5 shows the classification results on the test set 
using the original  cuda -convnet and the extended SCNN.   
 We c an observe from the results that the proposed SCNN can 
achieve an improvement of 2.11% over the original result. 
Although this may not be considered as  a very large 
improvement, the achieved results reinforce the motivation for 
SCNN is to mak e solid and consistent enhancement on existing 
CNN based approaches, especially for the scen ario when  the 
training data may not be enough to feed a complex n etwork. In 
the CIFAR -10 dataset, the number of images per class is 
actually quite large a nd therefore the effect of SCNN is not 
significant, but it still offers a noticable improvement with 
minor addition in the complexity. Moreover, as discussd  above, 
the experimental setup  is reproducible and made  to execute in a  
straigh tforward  manner. Therefore  it is reasonable to expect  
certain space for further  improve ment .  
IV. CONCLUSIONS  
This paper has presented automated periocular recognition 
using CNN with outperforming results and significantly 
smaller complexity. In particular,  we propose d a robust and 
more accurate framework for the periocular recognition  using  
the semantics -assisted convolutional neural network (SCNN). 
By training on e or more branches of CNNs with semantical 
information corresponding to training data, the SCNN  is 
capable of recovering more comprehensive features from the 
image s and therefore achiev e superior performance. Our 
experimental results on four publicly available databases 
suggest that t he proposed approach  can achieve outperforming 
results while requi ring much smaller computational time for the 
matching process.  The SCNN architecture can also be generalized for other image classification tasks, which can 
improve the performance over the single CNN based 
approach es. The source and executable  files of ou r approach 
are made publicly available [40] t o encourage other researchers 
to easily reproduce our results  and further advance research on 
accurate periocular recognition . 
 It may be noted that a t the current stage , we decouple the 
identity supervision and  other semantic supervision , in order to 
ensure h igh level of explicitness of  semantic learning  and 
compatibility to existing CNN based approach es. However, it is 
believed that a well -designed network structure may explicitly 
incorporate semantic informati on itself and facilitate  efficient 
training in an end -to-end training manner. It will be our future 
work to investigate improved architecture which enables joint 
learning of semantic information explicitly as well as 
preserving the network integrity.  
ACKNOWLEDGEMENT  
Authors thankfully acknowledge the support and help received 
from the authors of reference [10] in reproducing results from 
their approach  employed for comparisons in this paper.  
REFERENCES  
[1] V. Nair and G. Hinton, ""Rectified linear units imp rove restricted 
boltzmann machines"", Proc .  27th Int l. Conf . Machine Learning  (ICML) , 
2010, pp. 807 -814, 2010. . 
[2] C. W. Tan and A. Kumar, ""Towards Online Iris and Periocular 
Recognition Under Relaxed Imaging Constraints"", IEEE Transactions on 
Image Processin g, vol. 22, no. 10, pp. 3751 -3765, 2013. . 
[3] L. Nie, A. Kumar and S. Zhan, ""Periocular Recognition Using 
Unsupervised Convolutional RBM Feature Learning"", Proc.  22nd In tl. 
Conf. on Pattern Recognition , ICPR 2014, Stockholm, pp. 399 -404, 2014  
[4] H. Proenca, S. Fi lipe, R. Santos, J. Oliveira and L. Alexandre, ""The 
UBIRIS.v2: A Database of Visible Wavelength Iris Images Captured 
On-the-Move and At -a-Distance"", IEEE Transactions on Pattern 
Analysis and Machine Intelligence , vol. 32, no. 8, pp. 1529 -1535, 2010.   
[5] A. Sh arma, S. Verma, M. Vatsa and R. Singh, ""On cross spectral 
periocular recognition"", Proc. Intl. Conf.  Image  Processing , ICIP 2014, 
pp. 5007 -5011.  2014.  
[6] Y. Lecun, L. Bottou, Y. Bengio and P. Haffner, ""Gradient -based learning 
applied to document recognition"" , Proc . IEEE , vol. 86, no. 11, pp. 
2278 -2324, 1998 . 
[7] A. Krizhevsky , I. Sutskever, and  G. Hinton,  ‚ÄúImagenet classification with 
deep convolutional neural networks ‚Äù, In Advances in neural information 
processing systems , 2012, pp. 1097 -1105 . 
[8] U. Park, A. Ross, and A . K. Jain , ""Periocular biometrics in the visibl e 
spectrum: A feasibility study "", in Biometrics: Theory, Applications, and 
Systems  (BTAS) , 2009 . IEEE 3rd International Conference on . 2009 , pp. 
1-6. 
[9] G. Santos  and H. Proenca , ""Periocular biometrics: An e merging 
technol ogy for unconstrained scenarios "", in Computational Intelligence 
in Biometrics and Identity Management (CIBIM), 2013 IEEE Workshop 
on, 2013, pp. 14 -21. 
[10] J. Smereka, V. Boddeti and B. Vijaya Kumar, ""Probabilistic Deformation 
Models for Challeng ing Periocular Image Verification"", IEEE 
Trans.Inform.Forensic Secur. , vol. 10, no. 9, pp. 1875 -1890, 2015.  
[11] F. Juefei -Xu, K. Luu,  M. Savvides, T . D. Bui  and C . Y. Suen , 
""Investigating age invariant face recognition based on periocular 
biometrics."" In Biome trics (IJCB), 2011 International Joint Conference 
on, 2011, pp. 1 -7. 
[12] E. DeLong, D. DeLong and D. Clarke -Pearson, ""Comparing the Areas 
under Two or More Correlated Receiver Operating Characteristic Curves: 
A Nonparametric Approach"", Biometrics , vol. 44, no.  3, p. 837, 1988.  
[13] D. L. Woodard , S. Pundlik , P. Miller , R. Jillela, and A. Ross , ""On the 
fusion of periocular and iris b iometrics in non -ideal imagery "", Pattern 
Recognition (ICPR), 2010 20th IEEE International Conference on , 2010 , 
pp. 201-204. 
cuda-convnet
L1 L2 L3 L4 L5 L6
...
L7
...
...
L8 Prediction
to 10 
classes
cuda-convnet-s
L1 L2 L3 L4 L5 L6
...
L7 
 
Figure 13: The structure of SCNN used in the experiment for CIFAR -10 
dataset . The cuda -convnet  is from the original Caffe tutorial, and the 
cuda -convn et-s is newly trained by the semantic information.  
 
 
 Table 5: Results of classification on the CIFAR -10 testing set using original 
existing cuda -convnet  and the proposed SCNN enhancement on the 
cuda -convnet . 
Approach  Accuracy  
cuda-convnet  74.95%  
cuda -convnet -SCNN  77.06%  
 
 
 
 
 
1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE
Transactions on Information Forensics and Security
 
 13 
[14] A. Kumar, ""N eural network based defection of local textile defects,"" 
Pattern Recognition,  vol. 36, pp. 1645 -1659, Jul y 2003  
[15] R. Jillela and A . Ross. ""Mitigating effects of plastic surgery: Fu sing face 
and ocular biometrics "", in Biometrics: Theory, Applications and Syst ems 
(BTAS), 2012 IEEE Fifth International Conference on , 2012 , pp. 
402-411. 
[16] R. Girshick , J. Donahue , T. Darrell  and J.  Malik , ""Rich feature hierarchies 
for accurate object dete ction and semantic segmentation "", in Computer 
Vision and Pattern Recognition (CV PR), 2014 IEEE Conference on , 2014 , 
pp. 580 -587. 
[17] C. Szegedy , W. Liu, Y . Jia, P . Sermanet, S . Reed, D . Anguelov, D. Erhan, 
V. Vanhoucke, and A . Rabinovich , ""Going deeper with c onvolutions "", in 
Computer Vision and Pattern Recognition (CVPR), 201 5 IEEE 
Confer ence on , 2015, pp. 1 -9. 
[18] Y. Sun, X . Wang, and X . Tang , ""Deep learning face representation  from 
predicting 10,000 classes "", in Computer Vision and Pattern Recognition 
(CVPR), 2014 IEEE Conference on , 2014 , pp. 1891 -1898.  
[19] Y. Taigman , M. Yang , M. A. Ranzato  and L. Wolf , ""Deepface: Closing 
the gap to human -level p erformance in face verification "", in Computer 
Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on , 2014 , 
pp. 1701 -1708.  
[20] N. Srivastava , G. Hinton , A. Krizhevsky , I. Sutskever  and R.  
Salakhutdi nov, ""Dropout: A simple way to prevent neural networks fro m 
overfitting "", The Journal of Machine Learning Research , vol. 15, no. 1 pp. 
1929 -1958 , 2014 . 
[21] K. He, X. Zhang , S. Ren and J.  Sun, ""Deep Residual  Learning for Image 
Recognition "", The IEEE Conference on Computer Vision and Pattern 
Recognition (CVPR) , June, 2016 . 
[22] S. Bharadwaj , H. S. Bhatt , M. Vatsa and R. Singh,  ""Periocular biometrics: 
When iris recogn ition fails "", in Biometrics: Theory, Applications, and 
Systems  (BTAS) , 2010 Fourth IEEE International C onference on , 2010 , 
pp. 1 -6. 
[23] J. Deng , W. Dong , R. Socher , L. J. Li, K. Li, and L. Fei-Fei, ""Imagenet: A 
large -scale hierarchical image database "", in Computer Visi on and Pattern 
Recognition  ( CVPR ), 2009  IEEE Conference on , 2009 , pp. 248 -255. 
[24] L. Wan , M. Zeiler, S. Zhang , Y. Lec un, and R. Fergus , ‚ÄúRegularization of 
neural networks using dropconnect ‚Äù, Proc . 30th Int l. Conf. on Machine 
Learning , ICML 2013, 1058 -1066 , 2013 . 
[25] K. He, X. Zhang, S. Ren and J. Sun, ""Spatial Pyramid Pooling in Deep 
Convolutional Networ ks for Visual Recognition"", IEEE Transactions on 
Pattern Analysis and Machine Intelligence , vol. 37, no. 9, pp. 1904 -1916, 
2015.  
[26] C. N. Padole and H. Proenca. ""Periocular recognition: Analysis of 
performance degradation factors.""  in Biometrics (ICB), 2012 5 th IAPR 
International Conference on . IEEE, 2012 , pp. 439 -445. 
[27] ImageNet Large Scale Visual Recognition Challenge (ILSVRC) : 
http://www.image -net.org/challenges/LSVRC/  
[28] G. B. Huang, M. Ramesh,  T. Berg and E. Learned -Miller , Labeled faces 
in the wild: A database for studying face recognition in unconstrained 
environments . vol. 1. no. 2, Technical Report 07 -49, University of 
Massachusetts, Amherst, 2007.  
[29] H. Proen√ßa and L . A. Alexandre , ""The NICE. I: Noisy  iris challenge 
evaluation -part I "", in Biometrics: Theory, A pplications, and Systems  
(BTAS ) 2007 First IEEE International Conference on , 2007 , pp. 1 -4. 
[30] ""FRGC dataset"", 2016. [Online]. Available: 
http:/ /www.nist.gov/itl/iad/ig/frgc.cfm . [Accessed: 29 - Mar- 2016].   
[31] G. Bradski, ‚ÄúThe OpenCV Library,‚Äù Dr. Dobb‚Äôs Journal of Software 
Tools, 2000.  
[32] P. Viola and M. Jones, ‚ÄúRapid object detection using a boo sted cascade of 
simple features ‚Äù, in Computer Vision and  Pattern Recognition (CVPR), 
Proceedings of the 2001 IEEE Computer Society Conference on , 2001, 
vol. 1, pp. I -511. 
[33] D. Chen , X. Cao, L. Wang , F. Wen and J.  Sun, ""Bayesian face  revisited: A 
joint formulation "", in Computer Vision -ECCV 2012 , Springer Berlin 
Heidelberg, 2012 , pp. 5 66-579. 
[34] ""FOCS  dataset"", 2016. [Online]. Available: 
http://www.nist.gov/itl/iad/ig/focs.cfm . [Accessed: 29 - Mar- 2016] . 
[35] ""CASIA.v4  dataset"", 2016. [Online]. Available: 
http://biometrics.idealtest.org/dbDetailForUser.do?id=4 . [Accessed: 29 - 
Mar- 2016] .  
[36] ""UBIpr  dataset"", 2016. [Online]. Available: 
http://socia -lab.d i.ubi.pt/~ubipr/ . [Accessed: 29 - Mar- 2016] .  
[37] ""CIFAR -10 dataset "", 2016. [Online]. Available: 
https://www.cs.toronto.edu/~kriz/cifar.html . [Accessed: 29 - Mar- 2016] .  [38] ""cuda -convnet -   High -perfor mance C++/CUDA implementation of 
convolutional neural networks - Google Project Hosting"", 
Code.google.com, 2016. [Online]. Available: 
https://code.google.com/p/cuda -convnet/ . [Accessed: 29 - Mar- 2016] .  
[39] ""Caffe | CIFAR -10 tutorial"", Caffe.berkeleyvision.org, 2016. [Online]. 
Available: 
http://caffe.berkeleyvision.org/gathered/examples/cifar10.html . 
[Accessed: 29 - Mar- 2016].   
[40] Weblink to download codes for methods in this paper, 
http://www.comp.polyu.edu.hk/~csajaykr/scnn.rar  
[41] ""Biometric Evaluations Homepage"", Nist.gov , 2016. [Online]. Available: 
http://www.nist.gov/itl/iad/ig/biometric_evaluations.cfm . [Accessed: 29 - 
May- 2016].  
[42] ""Stock Photo - epa01034158 Demonstrators cover their faces during 
clashes at the end of the far leftist 'No Bush -No Wa r' rally"",  Alamy , 2016. 
[Online]. Available: 
http://www.alamy.com/stock -photo -epa01034158 -demonstrators -cover -t
heir-faces -during -clashes -at-the-97583185.html . [Accessed: 30 -May- 
2016].  
[43] Y. Sun , X. Wang and X. Tang,  ‚ÄúDeeply learned face representations ar e 
sparse, selective, and robust,‚Äù in Computer Vision and Pattern 
Recognition, 2015 IEEE conference on , pp. 2892 -2900 , 2015 . 
[44] G. Tso umakas and I. Katakis, ""Multi -Label Classification: An 
Overview"", International Journal of Data Warehousing and Mining , vol. 
3, no. 3, pp. 1 -13, 2007.  
 
 
 
 
 
 
"
https://ieeexplore.ieee.org/document/9727163,Error
http://ieeexplore.ieee.org/document/8014813," 
 
  
Abstract  
 
A key challenge of facial expression recognition (FER) 
is to develop effective representations to balance the 
complex distribution of intra- and inter- class variations. 
The latest deep convolutional networks proposed for FER 
are trained by penalizing the misclassification of images via 
the softmax loss. In this paper, we show that better FER 
performance can be achieved by combining the deep metric 
loss and softmax loss in a unified two fully connected layer 
branches framework via joint optimization. A generalized 
adaptive (N+M )-tuplet clusters loss function together with 
the identity-aware hard-negative mining and online 
positive mining scheme are proposed for identity-invariant 
FER. It reduces the computational burden of deep metric 
learning, and alleviates the difficulty of threshold 
validation and anchor selection. Extensive evaluations 
demonstrate that our method outperforms many state-of-art 
approaches on the posed as well as spontaneous facial 
expression databases. 
 
1. Introduction 
Facial expression is one of the most expressive nonverbal 
communication channels for humans to convey their 
emotional state [5]. Therefore, automatic facial expression 
recognition (FER) is important in a wide range of 
applications including human-computer interaction (HCI), 
digital entertainment, health care and intelligent robot 
systems [20]. 
Researchers have achieved great progress in recognizing 
the posed facial expressions collected under tightly 
controlled environment. Since the most promising face-
related applications occur in more natural conditions, it is 
our goal to develop a robust system that can operate well in 
the real word. Despite the significant efforts, FER remains 
a challenge in the presence of pose and illumination 
variations as well as inter-subject variations (i.e., identity-
specific attributes) [42]. These identity-specific factors 
degrade the FER performance of new identities unseen in 
the training data. Since spontaneous expressions only 
involve subtle facial muscle movements, the extracted expression-related information from different classes can 
be dominated by the sharp-contrast identity-speci ¬øc 
geometric or appearance features which are not useful for 
FER. As shown in Fig. 1, example x1 and  x3 are of happy 
faces whereas x2 and x4 are not of happy faces. ›Ç·à∫›î‡Øú·àªare the 
image representations using the extracted features. For 
FER, we desire that two face images with the same 
expression label are close to each other in the feature space, 
while face images with different expressions are farther 
apart from each other, i.e., the distance D2 between 
examples x1 and x3 should be smaller than D1 and D3, as in 
Fig. 1(b). However, the learned expression representations 
may contain irrelevant identity information as illustrated in 
Fig. 1(a). Due to large inter-identity variations, D2 usually 
has a large value while the D1 and D3 are relatively small.  
To further improve the discriminating power of the ex-
pression feature representations, and address the large intra-
subject variation in FER, a potential solution is to incorpo-
rate the deep metric learning scheme within a convolutional 
neural network (CNN) framework. The fundamental phi-
losophy behind the widely-used triplet loss function [7] is 
to require one positive example closer to the anchor exam-
ple than one negative example with a fixed gap ƒ≤. Thus, dur-
ing one iteration, the triplet loss ignores the negative exam-
ples from the rest of classes. Moreover, one of the two ex-
amples from the same class in the triplets can be chosen as  
Adaptive Deep Metric Learning for Identity-Aware Facial Expression Recognition
 
Xiaofeng Liu1,2,4*, B.V.K Vijaya Kumar1, Jane You3, Ping Jia2 
1Department of Electrical and Computer Engineerin g, Carnegie Mellon University, Pittsburgh, PA 
2 Changchun Institute of Optics, Fine Mechanics and Physics, Chinese Academy of Science 
3 Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China 
4 University of Chinese Academy of Sciences, Beijing, China  
liuxiaofeng@cmu.edu, kumar@ece.cmu.edu, csyjia@comp.polyu.edu.hk, jiap@ciomp.ac.cn  
Figure 1.  Illustration of representations in feature space learned by
(a) existing methods, and (b) the proposed method. 
2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops
Unrecognized Copyright Information
DOI 10.1109/CVPRW.2017.79522
2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops
2160-7516/17 $31.00 ¬© 2017 IEEE
DOI 10.1109/CVPRW.2017.79522

 
 
 the anchor point. However, there exist some special cases 
that the triplet loss function with impropriate anchor may 
judge falsely, as illustrated in Fig. 3(a). This means the per-
formance is quite sensitive to the anchor selection in the tri-
plets input. We adapted the idea from the ( N+1)-tuplet loss 
[37] and  coupled clusters loss (CCL) [22] to design a 
(N+M )-tuplet clusters loss function which incorporates a 
negative set with N examples and a positive set with M ex-
amples in a mini-batch. A reference distance T is introduced 
to force the negative examples to move away from the cen-
ter of positive examples and for the positive examples to 
simultaneously map into a small cluster around their  center  
c‡¨æ. The circles of radius ·à∫‹∂‡µÖ‡¥ì
‡∞Æ·àª and·à∫‹∂‡µÜ‡¥ì
‡∞Æ·àª centered at the 
c‡¨æform the boundary of the negative set and positive set re-
spectively, as shown in Fig. 3(d). By doing this, our ap-
proach can handle complex distribution of intra- and inter-
class variations, and free the anchor selection trouble in 
conventional deep metric learning methods. Furthermore, 
the reference distance T and the margin ƒ≤ can be learned 
adaptively via the propagation in the CNN instead of the 
manually-set hyper-parameters. We also propose a simple 
and efficient mini-batch construction scheme that uses dif-
ferent expression images with the same identity as the neg-
ative set to avoid the expensive hard-negative example 
searching, while mining the positive set online. Then, the 
(N+M )-tuplet clusters loss guarantees all the discriminating 
negative samples are efficiently used per update to achieve 
an identity-invariant FER. 
We jointly optimize the softmax loss and ( N+M )-tuplet 
clusters loss to explore the potential of both the expression 
labels and identity labels information. Considering the dif-
ferent characteristics of each loss function and their tasks, 
we design two branches of fully connected (FC) layers, and 
a connecting layer to balance them. The features extracted 
by the expression classification branch can be fed to the fol-
lowing metric learning processing. This enables each 
branch to focus better on their own task without embedding 
much information of the other. As shown in Fig. 2, the in-
puts are two facial expression image set: one positive set 
(images of the same expression from different subjects) and 
one negative set (images of other expressions with the same identity of the query example). The deep features and dis-
tance metrics are learned simultaneously in a network.  
The three major contributions in this paper are: 1) We 
propose a generalized ( N+M )-tuplet clusters loss function 
with adaptively learned reference threshold which can be 
seamlessly factorized into a linear-fully connected layer for 
an end-to-end learning. 2)  With the identity-aware negative 
mining and online positive mining scheme, we learn dis-
tance metrics with fewer input passes and distance calcula-
tions, without sacrificing the performance for identity-in-
variant FER. 3) We optimize the softmax loss and ( N+M )-
tuplet clusters loss jointly in a unified two-branch FC layer 
metric learning CNN framework based on their character-
istics and tasks. In experiments, we demonstrate that the 
proposed method achieves promising results not only out-
performing several state-of-art approaches in posed facial 
expression dataset (e.g., CK+, MMI), but also in spontane-
ous facial expression dataset (namely, SFEW). 
2. Related work 
FER focus on the classification of seven basic facial ex-
pressions which are considered to be common among hu-
mans [40]. Much progress has been made on extracting a 
set of features to represent the facial images [13]. Geomet-
ric representations utilize the shape or relationship between 
facial landmarks. However, they are sensitive to the facial 
landmark misalignments [35]. On the other hand, appear-
ance features, such as Gabor filters, Scale Invariant Feature 
Transform (SIFT), Local Binary Patterns (LBP), Local 
Phase Quantization (LPQ), Histogram of Oriented Gradi-
ents (HOG) and the combination of these features via mul-
tiple kernel learning are usually used for representing facial 
textures [3, 15, 49, 51]. Some methods such as active ap-
pearance models (AAM) [41] combine the geometric and 
appearance representations to provide better spatial infor-
mation. For a comprehensive survey, we refer readers to 
[34]. Due to the limitations of handcrafted filters, extracting 
purely expression-related features is difficult.  
    The developments in deep learning, especially the suc-
cess of CNN, have made high-accuracy image classification 
possible in recent years. It has also been shown that care-
fully designed neural network architectures perform well in 
FER [29]. Despite its popularity, current softmax loss-
based network does not explicitly encourage intra-class 
compactness and inter-class separation. The emerging deep 
metric learning methods have been investigated for person 
recognition and vehicle re-identification problems with 
large intra-class variations, which suggests that deep metric 
learning may offer more pertinent representations for FER. 
Compared to traditional distance metric learning, deep met-
ric learning learns a nonlinear embedding of the data using 
the deep neural networks.  The initial work is to train a Si-
amese network with contrastive loss function [4]. The pair-
wise examples are fed into two symmetric sub-networks to 
predict whether they are from the same class. Without the 
Positive  examples
Negative  examples
Two-branch FC layer
Joint Metric  Learning  Network
(N+M )-tuplet cluster loss &Softmax lossPositive Set
Negative Set
Figure 2. Frame work of our facial expression recognition model 
used for training. The deep convolutional network aims to map the 
original expression images into a feature space that the images o f 
the same expression tend to form a cluster while other images tend 
to locate far away. 
523
523
  
 interactions of positive pairs and negative pairs, the Sia-
mese network may fail to learn effective metrics in the pres-
ence of large intra- and inter-class variations. One improve-
ment is the triplet loss approach [7], which achieved prom-
ising performance in both re-identification and face recog-
nition problems. The inputs are triplets, each consisting of 
a query, a positive example and a negative example. Spe-
cifically, it forces the difference of the distance from the 
anchor point to the positive example and from the anchor 
point to the negative example to be larger than a fixed mar-
gin ﬂ¨. Recently, some of its variations with faster and stable 
convergence have been developed. The most similar model 
of our proposed method is the ( N+1)-tuplet loss [37]. We 
use ›î‡¨æand ›î‡¨øto denote the positive and negative examples 
of a query example ›î ,meaning that ›î‡¨æis the same class of 
›î ,while ›î‡¨øis not. Considering ( N+1) tuplet which includes 
›î ,›î‡¨æand N-1 negative examples ·àº›î‡Øù‡¨ø·àΩ‡Øù‡≠Ä‡¨µ‡Øá‡¨ø‡¨µ, the loss is: 
 
‹Æ·âÄ›î«°›î‡¨æ«°‡µõ›î‡Øù‡¨ø‡µü‡Øù‡≠Ä‡¨µ‡Øá‡¨ø‡¨µ«¢›Ç·âÅ‡µå  
›É›ã›à‡µ´Õ≥‡µÖœÉ‡Øù‡≠Ä‡¨µ‡Øá‡¨ø‡¨µ¬á¬ö¬í·à∫‹¶·à∫›Ç«°›Ç‡¨æ·àª‡µÖ…í‡µÜ‹¶·à∫›Ç«°›Ç‡Øù‡¨ø·àª·àª‡µØ          (1) 
 
where›Ç·à∫‡µâ·àª  is an embedding kernel defined by the CNN, 
which takes ›î and generates an embedding vector ›Ç·à∫›î·àª . 
We write it as ›Ç for simplicity, with ›Ç inheriting all 
superscripts and subscripts. ·à∫‡µâ«°‡µâ·àªis defined as the 
Mahalanobis or Euclidean distance according to different 
implementations. The philosophy in this paper also shares 
commonality with the coupled clusters loss [22], in which 
the positive example center c‡¨æ is set as the anchor. By 
comparing each example with this center instead of each 
other mutually, the evaluation times in a mini-batch are 
largely reduced.  
    Despite their wide use, the above-mentioned frameworks still suffer from the expensive example mining to provide 
nontrivial pairs or triplets, and poor local optima. In 
practice, generating all possible pairs or triplets would 
result in quadratic and cubic complexity, respectively and 
the most of these pairs or triplets are less valuable in the 
training phase. Also, the online or offline traditional mini-
batch sample selection is a large additional burden. 
Moreover, as shown in Fig. 3(a), (b) and (c), all of them are 
sensitive to the anchor point selection when the intra- and 
inter-class variations are large. The triplet loss, ( N+1)-tuplet 
loss and CCL are 0, since the distances between the anchor 
and positive examples are indeed smaller than the distance 
between the anchor and negative examples for a margin …í. 
This means the loss function will neglect these cases during 
the back propagation. We need much more input passes 
with properly selected anchors to correct it. The fixed 
threshold in the contrastive loss was also proven to be sub-
optimal for it failed to adapt to the local structure of data. 
Li et al. proposed [21] to address this issue by learning a 
linear SVM in a new feature space. Some works [9, 43] 
used shrinkage-expansion adaptive constraints for pair-
wise input, which optimized by alternating between SVM 
training and projection on the cone of all positive 
semidefinite (PSD) matrices, but their mechanism cannot 
be implemented directly in deep learning. 
    A recent study presented objective comparisons between 
the softmax loss and deep metric learning loss and showed 
that they could be complementary to each other [12]. 
Therefore, an intuitive approach for improvement is 
combining the classification and similarity constraints to 
form a joint CNN learning framework. For example, [39, 
47] combining the contrastive and softmax losses together 
to achieve a better performance, while [52] proposed to 
combine triplet and softmax loss via joint optimization. 
These models improve traditional CNN with softmax loss 
because similarity constraints might augment the 
information for training the network. The difficult learning 
objective can also effectively avoid overfitting. However, 
all these strategies apply the similarity as well as 
classi¬øcation constraints directly on the last FC layer, so 
that harder tasks cannot be assigned to deeper layers, (i.e., 
more weights) and interactions between constraints are 
implicit and uncontrollable. Normally, the softmax loss 
converges much faster than the deep metric learning loss in 
multi-task networks. This situation has motivated us to 
construct a unified CNN framework to learn this two loss 
function simultaneously in a more reasonable way. 
3.  (N+M )-tuplet clusters loss 
We give a simple description of our intuition to introduce 
a reference distance T to control the relative boundary ( T‡µÜ
…í
Õ¥) and ·à∫‹∂‡µÖ‡¥ì
‡∞Æ·àª for the positive and negative examples 
respectively, as shown in Fig. 3(d). We rewrite the ( N+1)-
tuplet loss function in Eq.(1) as follows: 
 Figure 3. Failed case of (a) triplet loss, (b) ( N+1)-tuplet loss, an d
(c) Coupled clusters loss. The proposed ( N+M )-tuplet clusters loss
is illustrated in (d). 
524
524
 
 
 &55?5X@
XA;N@;0n 
241jXA;N@;	o#00?jk)j7
j)j7
k#o00X@pp 
n241jXA;N@;	#00?k)j7
	o)j7
k#o00X@pp     
(2) 
Indeed, the 	00?jd
b term used to pull the 
positive example together and the 	d
bjo00Xp 
term used to push the negative examples away have an 
‚ÄúOR‚Äù relationship. The relatively large negative distance 
will make the loss function ignore the large absolute 
positive distance. One way to alleviate large intra-class 
variations is to construct an ‚ÄúAND‚Äù function for these two 
terms.  
We also extend the trip let loss to incorporate N negative 
examples and M negative examples. Considering a multi-
classification problem, the triplet loss and CCL only 
compare the query example with one negative example, 
which only guarantees the embedding vector of the query 
one to be far from a selected negative class instead of every 
class. The expectation of these methods is that the final 
distance metrics will be balan ced after sufficient number of 
iterations. However, towards the end of the training, 
individual iteration may exhibit zero errors due to the lack 
of discriminative negative ex amples causing the iterations 
to be unstable or slow in convergence.  
The identity labels in FER database largely facilitate the 
hard-negative mining to alleviate the effect of the inter-
subject variations. In practi ce, for a query example, we 
compose its negative set with all the different expression 
images of the same person. Moreover, randomly choosing 
one or a group of positive examples is a paradigm of the 
conventional deep metric methods, but some extremely 
hard positive examples may distort the manifold and force 
the model to be over-fitting. In the case of spontaneous 
FER, the expression label may erroneously be assigned due 
to the subjectivity or varied expertise of the annotators [2, 
50]. Thus, an efficient online mining for M randomly-
chosen positive examples should be designed for large 
intra-class variation datasets. We find the nearest negative 
example and ignore those positive examples with a larger 
distance. Algorithm 1 shows the detail. In summary, the 
new loss function is expressed as follows: 
 
&5W?
WA;M5X@
XA;N0n;
MWA;M
o#0?jk)j_
<p 
                         ja
fXA;N
o)j 7
k#o0X@jppp           (3)  
 
The simplified geometric inte rpretation is illustrated in 
Fig. 3(d). Only if the distances from online mined positive 
examples to the updated c? smaller than o)ki
bp and the  
distances to the updated c+ than o)ji
bp, the loss can get a 
zero value. This is much more consistent with the principle 
used by many data cluster and discriminative analysis 
methods. One can see that the conventional triplet loss and its variations become the special cases of the ( N+M)-tuplet 
clusters loss under our framework. 
 
For a batch consisting of X queries, the input passes 
required to evaluate the necessary embedding feature 
vectors in our application are X, and the total number of 
distance calculations can be (j'+. Normally, the 
N and M are much smaller than X. In contrast, triplet loss 
requires Q= passes and Q=times calculations, ( N+1)-tuplet 
loss requires +j+ passes and +j+< times 
calculations. Even for a dataset with a moderate size, it is 
intractable to load all possible meaningful triplets into the 
limited memory for model training. 
    By assigning different values for T and 7, we define a 
flexible learning task with adjustable difficulty for the 
network. However, the two hyper-parameters need manual 
tuning and validation. In the spirit of adaptive metric 
learning for SVM [21], we form ulate the reference distance 
to be a function mmrelated with each example instead of 
a constant. Since the Mahalanobis distance matrix M in 
Eq.(4) itself is quadratic, and can be calculated 
automatically via a linear fu lly connected layer as in  [36], 
we assume 0;0< as a simple quadratic form, i.e., 
T0;,0<=a
b6^!6j8^j- , where 6^n0;^0<^<S, 
!n!UaUa!UaUb
!UbUa!UbUb<Sl<S 8^n8Ua^8Ub^<S, - , 
0;0<<S are the representations of two images in 
the feature space. 
 
           D(0;,0<)=0;k0<M<n0;k0<Oo0;k0<p           (4) 
 
Due to the symmetry property with respect to 0; and 0<, we 
can rewrite T0;,0< as follows: 
 
     T0;,0<=a
b0;^0;ja
b0<^3j0;^0<j.^0;j0<j-    (5) 
 Algorithm 1  Online positive mining 
Input 
query example and its randomly selected  
positive set 5W?
WA;M, and negative set q5X@rXA;N 
  1. map examples to feature plane with CNN to get: 
0W?
WA;M,3/q0X@rXA;N 
  2. calculate the positive cluster center c?=a
eWA;M0W? 
  3. calculate the distance from c? to each  
      positive and negative example  Dfi?,c?, #fj@,c? 
  4. search for the nearest negative distance: 
#o5[TR\]^@c?p 
  5. ignore those positive examples satisfying: 
 Dfi?,c?#o5[TR\]^@c?p 
  6. update c?=a
eWA;M0W? 
Output 
Online mined M* positive examples and updated c? 
525
525
 
 
where = !UaUa=!UbUband  = !UaUb= !UbUa are both the 
/l/  real symmetric matrices (not necessarily positive 
semi-de Ô¨Ånite), c = Ua = Ub is a d-dimensional vector, and 
bis the bias term. Then, a new quadratic formula 
(0,0)=To0,0pkD(0,0)is defined to combine the 
reference distance function an d distance metric function. 
Substituting Eq.(4) and Eq.(5) to H(0;,0<), we get: 
H(0;,0<)=;
<0;^k0;j;
<0<^k0< 
j0;^ojp0<j.^0;j0<j- (6)
     
  H(0;,0<)=a
b0;^0;ja
b0<^0<j0;^0<j.^0;j0<j-     (7)   
where A=k  and B=j Suppose A is positive 
semi-definite (PSD) and B is negative semi-definite (NSD), 
A and B can be factorized as BOB andCOC. Then 
o0;0<p can be formulated as follows:  
H(0;,0<)=;
<0;^KOB0;j;
<3^BOB0<j0;^COC0< 
j.^0;j0<j- 
 
=;
<B0;^B0;j;
<B0<^B0<jC0;^C0< 
j.^0;j.^0<j-                                 (8)                                    
Motivated by the above, we propose a general, 
computational feasible loss function. Following the 
notations in the preliminaries and denote oBC.pO as W, 
we have: 
&*5W?
WA;M5X@
XA;N0n 
;
MWA;M
o Ho0ij,cj)+_
<pj;
NXA;N
o%o0X@.jpj_
<p 
 (9) 
Given the mined N+M*  training examples in a mini-batch, 
2mis a label function. If the example 5Yis from the 
positive set, 25Ynk, otherwise, 25Yn1. Moreover, 
we simplify the  i
bto be the constant 1, and changing it to 
any other positive value results only in the matrices being 
multiplied by corresponding factors. Our hinge-loss like 
function is: &*5W?
WA;M5X@
XA;N0n 
              a
fceYA;N?M
25Y*H(0Y,c?)+           (10) 
 
We optimize Eq.(12) using the standard stochastic gradient 
descent with momentum. The desired partial derivatives of 
each example are computed as: 
 
                      `L
`Phna
fceYn;NjM`L
`Qgh`Qgh
`Ph                   (11) 
 
                            `L
`Qghn`L
`Qghca`Qghca
`Qgh                            (12) 
 
where  +YZ represents the feature map of the example 5Y at 
the 2^Vlayer. Eq.(11) shows that the overall gradient is the 
sum of the example-based gradie nts. Eq.(12) shows that the 
partial derivative of each ex ample with respect to the 
feature maps can be calculated recursively. So, the 
gradients of network parameters can be obtained with back 
propagation algorithm.  
    In fact, as a straightforward generalization of 
conventional deep metric learning methods, the ( N+M )-
tuplet clusters loss can be easily used as a drop-in 
replacement for the triplet loss and its variations, as well as 
used in tandem with other performance-boosting 
approaches and modules, in cluding modified network 
architectures, pooling functions, data augmentations or 
activation functions. 
4.Network architecture 
The proposed two-branch FC layer joint metric learning 
architecture with softmax loss and ( N+M )-tuplet clusters 
loss, denoted as 2B( N+M )Softmax, is illustrated in Fig. 4. 
The convolutional groups of our network are based on the 
inception FER network presented in [28]. We adopt the 
parametric rectified linear unit (PReLU) to replace the 
conventional ReLU for its good performance and 
generalization ability when given limited training data. In 
addition to providing the sparsity to gain benefits discussed 
in Arora et al. [1], the inception layer also allows for 
Figure 4. The proposed network structure. In the testing phase, only the convolutional groups and expression classification bra nch with 
softmax are used to recognize a single facial expression image. 	

""
# $


	
 	  
  
  


 
 		 
 		
  
		
 		 
 		
  
		
 		 
 		
  
		




 





!
  Expression 
labelIdentity 
label“ÅML“Ç
“ÅEC“Ç
526
526
 
 
 improved recognition of local features. The locally applied 
smaller convolution filters seem to align the way that 
human process emotions with  the deformation of local 
muscles. Note that we did not specifically search for the 
architectures that obtain the absolute best accuracies on 
some datasets. Our goal is to confirm our generalized metric 
learning loss function and the unified two-branch FC layer 
joint learning framework perform well.   
    Combing the ( N+M )-tuplet clusters loss and softmax loss 
is an intuitive improvement to reach a better performance. 
However, conducting them directly on the last FC layer is 
sub-optimal. The basic idea of  building a two-branch FC 
layers after the deep convolution groups is combining two 
losses  in different level of tasks. We learn the detailed 
features shared between the sa me expression class with the 
expression classification (EC)  branch, while exploiting 
semantic representations via the metric learning (ML) 
branch to handle the significant appearance changes from 
different subjects. The connecting layer embeds the 
information learned from the expression label-based detail 
task to the identity label- based semantical task, and 
balances the scale of weights in two task streams. This type 
of combination can effectively alleviate the interference of 
identity-specific attributes. The inputs of connecting layer 
are the output vectors of the former FC layers- FC 2 and FC 3, 
which have the same dimension denoted as Dinput. The 
output of the connecting layer, denoted as FC 4 with 
dimension Douput, is the feature vector fed into the second 
layer of the ML branch. The connecting layer concatenates 
two input feature vectors into a larger vector and maps it 
into a Doutput dimension space: 
 
                    $"">n D$""<$""=n 9D$""<j :D$""=        (13) 
 
where P is a o#EFHJIl#GJIHJIp matrix, P1 and P2 are 
#EFHJIl#GJIHJImatrices. 
    Regarding the sampling strategy, every training image is 
used as a query example in an epoch. In practice, the 
softmax loss will only be calculated for the query example. 
The importance of two loss functions is balanced by a 
weight Œ±. During the testing stage, this framework takes one 
facial image as input, and gene rates the classification result 
through the EC branch with the softmax loss function. 
5.Experiments and analysis 
5.1.Preprocessing 
     For a raw image in the database, face registration is a 
crucial step for good performance. The bidirectional 
warping of Active Appearance Model (AAM) [30] and a 
Supervised Descent Method (SDM) called IntraFace model 
[45] are used to locate the 49 facial landmarks. Then, face 
alignment is done to reduce in-plane rotation and crop the 
region of interest based on the coordinates of these landmarks to a size of 60 l60. The limited images of FER 
datasets is a bottleneck of deep model implementation. 
Thus, an augmentation procedure is employed to increase 
the volume of training data and alleviate the chance of over-
Ô¨Åtting. We randomly crop the 48 l48 size patches, flip them 
horizontally and transfer them to grayscale images. All the 
images are processed with the standard histogram 
equalization and linear plane fitting to remove unbalanced 
illumination. Finally, we normalize them to a zero mean 
and unit variance vector. In the testing phase, a single center 
crop with the size of 48 l48 is used as input data. 
5.2.Implementation Details 
Following the experimental pr otocol in [28,48], we pre-
train our convolutional groups a nd EC branch FC layers on 
the FER2013 database [9] fo r 300 epochs, optimizing the 
softmax loss using stochastic gradient decent with a 
momentum of 0.9. The initial network learning rate, batch 
size, and weight decay paramete r are set to 0.1, 128, 0.0001, 
respectively. If the training loss increased more than 25% 
or the validation accuracy does not improve for ten epochs, 
the learning rate is halved a nd the previous network with 
the best loss is reloaded. Then the ML branch is added and 
the whole network is trained by 204,156 frontal viewpoints 
(-45 to 45) face images selected from the CMU Multi-pie 
[10] dataset. There contains  337 people displaying disgust, 
happy, surprise and neutron. The size of both the positive 
and negative set are fixed to 3 images. The weights of two 
loss functions are set equally. We select the highest 
accuracy training epoch as our pre-trained model.  
    In the fine-tuning stage, the positive and negative set size 
are fixed to 6 images (for CK+ and SFEW) or 5 images (for 
MMI). For a query example, the random searching is 
employed to select the othe r 6 (or 5) same expression 
images to form the positive set. Identity labels are required 
for negative mining in our method. CK+ and MMI have the 
subject IDs while the SFEW need manually label. In 
practice, an off-the-shelf face recognition method can be 
used to produce this informa tion. When the query example 
lacks some expression images from the same subject, the 
corresponding expression images sharing the same ID with 
the any other positive examples are used. The tuplet-size is 
set to 12, which means 12 l(6+6) =144 (or 12 l(5+5) =120) 
images are fed in each training iteration. We use Adam [19] 
for stochastic optimization and other hyper-parameters 
such as learning rate are tuned accordingly via cross-
validation. All the CNN archit ectures are implemented with 
the widely used deep learning tool ‚ÄúCaffe [14].‚Äù 
5.3.Experimental Evaluations 
To evaluate the effectiveness of the proposed method, 
extensive experiments have been conducted on three well-
known publicly available facial e xpression databases: CK+,  
MMI and SFEW. For the fair comparison, we follow the 
527
527
 
 
protocol used by previous wo rks [28,48]. Three baseline 
methods are employed to demons trate the superiority of the 
novel metric learning loss and two-branch FC layer network 
respectively, i.e., adding the ( N+M )-tuplet clusters loss or 
(N+1)-tuplet loss with softmax loss after the EC branch, 
denoted as 1B( N+1)Softmax or 1B( N+M )Softmax, and 
combining the ( N+1)-tuplet loss with softmax loss via the 
two-branch FC layer structure, as 2B( N+1)Softmax. We do 
not compare with the triplet loss here, because the number 
of triplets grows cubically with the number of images, 
which makes it impractical and inef Ô¨Åcient. With randomly 
selected triplets, the loss failed to converge during the 
training phase. 
The extended Cohn-Kanade database (CK+) [26] 
includes 327 sequences collected from 118 subjects, 
ranging from 7 different expressi ons (i.e., anger, contempt, 
disgust, fear, happiness, sadness, and surprise). The label is 
only provided for the last frame (peak frame) of each 
sequence. We select and label the last three images, and 
obtain 921 images (without neutral). The Ô¨Ånal sequence-
level predictions are made by selecting the class with the 
highest possibility of the three images. We split the CK+ 
database to 8 subsets in a stri ct subject independent manner, 
and an 8-fold cross-validation is employed. Data from 6 
subsets is used for training and the others are used for 
validation and testing. The confusions matrix of the 
proposed method evaluated on the CK+ dataset is reported 
in Table 1. It can be obser ved that the disgust and happy 
expressions are perfectly recognized while the contempt 
expression is relatively harder for the network because of 
the limited training examples and subtle muscular 
movements. As shown in Table 3, the proposed 
2B(N+M )Softmax outperforms the human-crafted feature-
based methods, sparse coding-based methods and the other 
deep learning methods in comparison. Among them, the 
3DCNN-DAP, STM-Explet and DTAGN utilized temporal 
information extracted from se quences. Not surprisingly, it 
also beats the baseline methods obviously benefit from the 
combination of novel deep metric learning loss and two-
branch architecture. 
The MMI database [32] includes 31 subjects with frontal-
view faces among 213 image sequences which contain a full temporal pattern of expressions, i.e., from neutral to one 
of six basic expressions as tim e goes on, and then released. 
It is especially favored by the video-based methods to 
exploit temporal information. We collect three frames in the 
middle of each image sequence and associate them with the 
labels, which results in 624 images in our experiments. We 
divide MMI dataset into 10 subsets for person-independent 
ten-fold cross validation. The sequence-level predictions 
are obtained by choosing the class with the highest average 
score of the three images. The confusion matrix of the 
proposed method on the MMI database is reported in Table 
2. As shown in Table 3, th e performance improvements in 
this small database without causing overfitting are 
impressive. The proposed method outperforms other works 
that also use static image-based features and can achieve 
comparable and even better results than those video-based 
approaches. 
 
Table 3. Recognition accuracy comparison on the CK+ database 
[26] in terms of seven expressions, and MMI database [32] in 
terms of six expressions. 
 
Methods CK+  MMI  
MSR [33] 91.4% N/A 
ITBN [44] 91.44% 59.7% 
BNBN [25] 96.7% N/A 
IB-CNN [11] 95.1% N/A 
3DCNN-DAP [23] 92.4% 63.4% 
STM-Ex plet [24] 94.19% 75.12% 
DTAGN [16] 97.25% 70.2% 
Inception [28] 93.2% 77.6% 
1B(N+1)Softmax 93.21% 77.72% 
2B(N+1)Softmax 94.3% 78.04% 
1B(N+M)Softmax 96.55% 77.88% 
2B(N+M )Softmax 97.1% 78.53% 
 
The static facial expressions in the wild (SFEW) database 
[6] is created by extracting frame s from the film clips in the 
AFEW data corpus. There are 1766 well-labeled images 
(i.e., 958 for training, 436 for validation and 372 for testing) 
being assigned to be one of the 7 expressions. Different 
from the previous two datasets, it targets for unconstrained 
facial expressions, which ha s large variations reflecting 
real-world conditions. The conf usion matrix of our method 
on the SFEW validation set is reported in Table 4. The 
Table 1.  Average confusion matrix obtained from proposed
method on the CK+ database [26]. 
Table 2.  Average confusion matrix obtained from proposed
method on the MMI database [32]. 
 
 Predict 
AN CO DI FE HA SA SU 
 AN 91.1% 0% 0% 1.1% 0% 7.8% 0% 
CO 5.6% 90.3% 0% 2.7% 0% 5.6% 0% 
DI 0% 0% 100% 0% 0% 0% 0% 
FE 0% 4% 0% 98% 2% 0% 8% 
HA 0% 0% 0% 0% 100% 0% 0% 
SA 3.6 0% 0% 1.8% 0% 94.6% 0% 
SU 0% 1.2% 0% 0% 0% 0% 98.8% 
CO
D
FE
HA
Actual  Predict 
AN DI FE HA SA SU 
 AN 81.8% 3% 3% 1.5% 10.6% 0% 
DI 10.9% 71.9% 3.1% 4.7% 9.4% 6% 
FE 5.4% 8.9% 41.4% 7.1% 7.1% 30.4% 
HA 1.1% 3.6% 0% 92.9% 2.4% 0% 
SA 17.2% 7.8% 0% 1.6% 73.4% 0% 
SU 7.3% 0% 14.6% 0% 0% 79.6% 
Actual 
528
528
 
 
recognition accuracy of disgus t and fear are much lower 
than the others, which is also observed in other works. As 
illustrated in Table 5, the CNN-based methods dominate the ranking list. With the augmentation of deep metric learning and two-branch FC layer network, the proposed method works well in the real world environment setting. Note that Kim et al. [18] employed 216 AlexNet-like CNNs with 
different architectures to boost the Ô¨Ånal performance. Our 
network performs about 25M operations, almost four times 
fewer than a single AlexNet. With the smaller size, the 
evaluation time in testing phase takes only 5ms using a 
Titan X GPU, which makes it applicable for real-time applications. 
Overall, we can see that joint optimizing the metric 
learning loss and softmax loss can successfully capture 
more discriminative expre ssion-related features and 
translate them into the significant improvement of FER 
accuracy. The (N+M )-tuplet clusters loss not only inherits 
merits of conventional deep metric learning methods, but 
also learns features in a more efficient and stable way. The 
two-branch FC layer can further give a boost in 
performance. Some nice properties of the proposed method 
are verified by Fig. 5, where the training loss of2B(N+M )Softmax converges after about 40 epochs with a 
more steady decline and reaches a lower value than those 
baseline methods as we expect. As Fig. 6 illustrates, the 
proposed method and the baseline methods achieve better performance in terms of th e validation accuracy on the 
training phase. Table 5. Recognition accuracy comparison on the SFEW database 
[6] in terms of seven expressions. 
  
Methods Validation  
Kim et al. [18] 53.9% 
Yu et al. [48] 55.96%
Ng et al. [31] 48.5% 
Yao et al. [46] 43.58% 
Sun et al. [38] 51.02% 
Zong et al.  [53] N/A 
Kaya et al.[17] 53.06% 
Mao et al.[27] 44.7% 
Mollahosseini [28] 47.7% 
1B(N+1)Softmax 49.77% 
2B(N+1)Softmax 50.75% 
1B(N+M )Softmax 53.36% 
2B(N+M )Softmax 54.19% 
6.Conclusion 
We derive the ( N+M )-tuplet clusters loss and combine it 
with softmax loss in a unified two-branch FC layer joint 
metric learning CNN architecture to alleviate the attribute variations introduced by different identities on FER. The efficient identity-aware negative-mining and online positive-mining scheme are employed. After evaluating performance on the posed and spontaneous FER dataset, we show that the proposed met hod outperforms the previous 
softmax loss-based deep learning approaches in its ability 
to extract expression-related features. More appealing, the 
(N+M )-tuplet clusters loss function has clear intuition and 
geometric interpretation for gene ric applications. In future 
work, we intend to explore the use of it to the person or vehicle re-identifications. 
 
Acknowledgements  The funding support from Youth Innovation Promotion Association, CAS (2017264), Innovative Foundation of CIOMP, CAS (Y586320150) and Hong Kong Government 
General Research Fund GRF (Ref. No.152202/14E) is 
greatly appreciated. 
Table 4. Average confusion matrix obtained from proposed 
method on the SFEW validation set [6]. 
 Predict 
AN DI FE HA NE SA SU 
 AN 66.24% 1.3% 0% 6.94% 9.09% 5.19% 10.69% 
DI 21.74% 4.35% 4.35% 30.34% 13.04% 4.35% 21.74% 
FE 27.66% 0% 6.38% 8.51% 10.64% 19.15% 27.66% 
HA 0% 0% 0% 87.67% 6.85% 1.37% 4.11% 
NE 5.48% 0% 2.74% 1.37% 57.53% 5.48% 27.4% 
SA 22.81% 0% 1.75% 7.02% 8.77% 40.35% 19.3% 
SU 1.16% 0% 2.33% 5.81% 17.44% 0% 73.26% 
 
F
H
N
Actual 
Figure 5. The training loss of different methods on SFEW validation set. 
 Figure 6. The validation accuracies of different methods on 
SFEW validation set. 
  #&#&(#'#'(#(#((#)
# $ #% #& #' #( #) # 



	  

*$! 
*$! 
$*! 
%*! 
#$%&'()
# $ #% #& #' #( #) # 
	  

$*$! 
%*$! 
*! 
*! 
529
529
 
 
 References 
[1]S. Arora, A. Bhaskara, R. Ge  and T. Ma. Provable bounds for 
learning some deep representations. arXiv preprint 
arXiv:1310.6343 , 2013. 
[2]E. Barsoum, C. Zhang, C. C. Ferrer and Z. Zhang. Training 
deep networks for facial expression recognition with crowd-
sourced label distribution. In ICMI , pages 279-283, 2016. 
[3]T. Baltrusaitis, M. Mahmoud and P. Robinson. Cross-dataset 
learning and person-speci Ô¨Åc normalisation for automatic 
action unit detection. In Automatic Face and Gesture 
Recognition (FG) , pages 6:1-6, 2015. 
[4]S. Chopra, R. Hadsell and Y. LeCun. Learning a similarity 
metric discriminatively, with application to face veri Ô¨Åcation. 
In CVPR , 2005. 
[5]J. F. Cohn and P. Ekman. Measuring facial action. The new 
handbook of methods in nonverbal behavior research , pages 
9‚Äì64, 2005. 
[6]A. Dhall, Abhinav, O. V. R Murthy, R. Goecke, J. Joshi and 
T. Gedeon. Video and image based emotion recognition 
challenges in the wild: Emotiw 2015. In ICMI , pages 423‚Äì
426, 2015. 
[7]S. Ding, L. lin, G Wang and H.  Chao. Deep Feature Learning 
with Relative Distance Comparison for Person Re 
identification. Pattern Recognition , 48: 2993-3003, 2015. 
[8]Y. Dong, B Du, L. Zhang, L.  Zhang and D. Tao. LAM3L: 
Locally adaptive maximum margin metric learning for visual 
data classification.  Neurocomputing, 235:1‚Äì9, 2017.  
[9]I. J. Goodfellow, D. Erhan, P.  L. Carrier, A. Courville, M. 
Mirza, B. Hamner, W. Cukiersk i, Y. Tang, D. Thaler, D Lee, 
Y Zhou, C Ramaiah, F Feng, R Li and X Wang. Challenges 
in representation learning: A report on three machine 
learning contests. In  International Conference on Neural 
Information Processing , page 117‚Äì124, 2013. 
[10]R. Gross, I. Matthews, J. C ohn, T. Kanade and S. Baker. 
Multi-pie. Image and Vision Computing , 28(5):807‚Äì813, 
2010. 
[11]S. Han, Z. Meng, A. S. Khan and Y. Tong, Incremental 
Boosting Convolutional Neural Network for Facial Action 
Unit Recognition, In NIPS , pages 109‚Äì117, 2016. 
[12]S. Horiguchi, D. Ikami and K. Aizawa. Significance of 
softmax-based features over metric learning-based features. 
In ICLR , 2017. 
[13]S. Jain, C. Hu and J.K. Aggarwal. Facial expression 
recognition with temporal modeling of shapes. In CVPRW , 
pages 1642‚Äì1649, 2011. 
[14]Y. Jia, E. Shelhamer, J. Dona hue, S. Karayev, J. Long, R. 
Girshick, S. Guadarrama and T. Darrell. Caffe: 
Convolutional architecture for fast feature embedding. arXiv 
preprint arXiv:1408.5093 , 2014. 
[15]B. Jiang, M. Valstar and M. Pantic. Action unit detection 
using sparse appearance descriptors in space-time video 
volumes. In Automatic Face and Gesture Recognition (FG) , 
pages 314-321, 2011. 
[16]H. Jung, S. Lee, J. Yim, S. Park and J. Kim. Joint fine-tuning 
in deep neural networks for facial expression recognition. 
In ICCV , pages 2983‚Äì2991, 2015. 
[17]H. Kaya and A. Salah. Combining modality-specific extreme 
learning machines for emoti on recognition in the wild. 
Journal on Multimodal User Interfaces  10(2):139-149, 2016. 
[18]B.K. Kim, J. Roh, S.Y. Le e and J. Roh. Hierarchical 
committee of deep convolutional neural networks for robust facial expression recognition. Journal on Multimodal User 
Interfaces , 10(2): 173-189, 2016. 
[19]D. Kingma and J. Ba. Adam: A method for stochastic 
optimization. In ICLR , 2015. 
[20]I. Kotsia, S. Zafeiriou and S. Fotopoulos. Affective gaming: 
A comprehensive survey. In CVPRW , pages 663‚Äì670, 2013. 
[21]Z. Li, S Chang, F Liang, T.S.  Huang, L Cao and J R. Smith. 
Learning locally-adaptive decision functions for person 
verification. In  CVPR , pages 3610-3617. 2013. 
[22]H. Liu, Y. Tian, Y. Yang, L.  Pang and T. Huang. Deep 
relative distance learning: Tell the difference between similar 
vehicles. In CVPR , pages 2167-2175, 2016. 
[23]M. Liu, S. Li, S. Shan, R.  Wang and X. Chen. Deeply 
learning deformable facial action parts model for dynamic 
expression analysis. In  ACCV , pages 143‚Äì157, 2014. 
[24]M. Liu, S. Shan, R. Wa ng and X. Chen. Learning 
expressionlets on spatio-temporal manifold for dynamic 
facial expression recognition. In  CVPR , pages 1749‚Äì1756, 
2014. 
[25]P. Liu, S. Han, Z. Meng a nd Y. Tong. Facial expression 
recognition via a boosted deep belief network. In CVPR , 
pages 1805‚Äì1812, 2014. 
[26]P. Lucey, J.F. Cohn, T. Kanade , J. Saragih, Z. Ambadar and 
I. Matthews. The extended cohn-kanade dataset (ck+): A 
complete expression dataset for action unit and emotion-
speciÔ¨Åed expression. In CVPRW , pages. 94-101, 2010. 
[27]Q. Mao, Q. Rao, Y. Yu and M.  Dong. Hierarchical Bayesian 
Theme Models for Multi-pose Facial Expression Recognition. 
IEEE Transactions on Multimedia , 19(4): 861 ‚Äì 873, 2017. 
[28]A. Mollahosseini, D. Chan a nd M.H. Mahoor. Going deeper 
in facial expression recognition using deep neural networks. 
In WACV,  pages 1‚Äì10, 2016. 
[29]A. Mollahosseini, B. Hassani, M.J. Salvador, H. Abdollahi, 
D. Chan and M.H. Mahoor. Facial expression recognition in 
the world wild web. In CVPR,  pages 58-65, 2016. 
[30]A. Mollahosseini and M. H. Mahoor. Bidirectional warping 
of active appearance model. In CVPRW , pages 875‚Äì880, 
2013. 
[31]H.W. Ng, V.D. Nguyen, V. V onikakis and S Winkler. Deep 
learning for emotion recognition on small datasets using 
transfer learning. In ICMI , pages 443‚Äì449, 2015.  
[32]M. Pantic, M Valstar, R Rademaker, and L Maat. Web-based 
database for facial expression analysis. In  ICME, pages5,  
2005.  
[33]S. Rifai, Y Bengio, A. Courville, P. Vincent and M Mirza. 
Disentangling factors of variation for facial expression 
recognition. In  ECCV , pages 808‚Äì822, 2012. 
[34]E. Sariyanidi, H. Gunes and A. Cavallaro. Automatic 
analysis of facial affect: A survey of registration, 
representation and recognition. Pattern Analysis and 
Machine Intelligence, IEEE Transactions on,  37:1113‚Äì1133, 
2015. 
[35]J. Shen, S. Zafeiriou, G. G.  Chrysos, J. Kossaifi, G. 
Tzimiropoulos and M. Pantic. The first facial landmark 
tracking in-the-wild challenge: Benchmark and results. In 
ICCVW , pages 1003-1011, 2015. 
[36]H. Shi, Y. Yang, X. Zhu, L. Zhen, W. Zheng and S.Z. Li. 
Embedding deep metric for person re-identification: a study 
against large variations. In ECCV , pages 732-748, 2016. 
[37]K. Sohn. Improved deep metric learning with multi-class n-
pair loss objective. In NIPS , pages 1849-1857, 2016. 
530
530
 
 
 [36] H. Shi, Y. Yang, X. Zhu, L. Zhen, W. Zheng and S.Z. Li. 
Embedding deep metric for person re-identification: a study 
against large variations. In ECCV , pages 732-748, 2016. 
[37] K. Sohn. Improved deep metric learning with multi-class n-
pair loss objective. In NIPS , pages 1849-1857, 2016. 
[38] B. Sun, L. Li, G. Zhou, X. Wu, J. He, L. Yu, D. Li and Q. 
Wei. Combining multimodal features within a fusion 
network for emotion recognition in the wild. In ICMI , pages 
497‚Äì502, 2015. 
[39] Y. Sun, Y. Chen, X. Wang and X. Tang, Deep learning face 
representation by joint identi ¬øcation-veri ¬øcation. In NIPS , 
pages 1988‚Äì1996, 2014. 
[40] Y. Tian, T. Kanade and J. F. Cohn. Facial expression 
analysis. In Handbook of face recognition , pages 247‚Äì275, 
2005. 
[41] G. Tzimiropoulos and M. Pantic. Optimization problems for 
fast aam fitting in-the-wild. In  ICCV , pages 593-600, 2013. 
[42] M.F. Valstar, M. Mehu, B. Jiang, M. Pantic and K. Scherer. 
Meta-analysis of the first facial expression recognition 
challenge.   IEEE Trans. Syst., Man Cybern. B, 
Cybern,  42(4): 966‚Äì979, 2012. 
[43] Q. Wang, W. Zuo, L. Zhang and P. Li. Shrinkage expansion 
adaptive metric learning. In  ECCV , pages 456‚Äì471, 2014. 
[44] Z. Wang, S Wang and Q Ji. Capturing complex spatio-
temporal relations among facial  muscles for facial expression 
recognition. In CVPR , pages 3422‚Äì3429, 2013. 
[45] X. Xiong and F. De la Torre. Supervised descent method and 
its applications to face alignment. In CVPR , pages 532‚Äì539, 
2013. 
[46] A. Yao, J. Shao, N. Ma and Y. Chen. Capturing au-aware 
facial features and their latent relations for emotion 
recognition in the wild. In ICMI , pages 451‚Äì458, 2015. 
[47] D. Yi, Z. Lei, S. Liao and S.Z. Li, Learning face 
representation from scratch. arXiv preprint arXiv:1411.7923 , 
2014. 
[48] Z. Yu and C. Zhang. Image based static facial expression 
recognition with multiple deep network learning. In ICMI , 
pages 435‚Äì442, 2015. 
[49] A. Y√ºce, H. Gao and J. P. Thiran. Discriminant multi-label 
manifold embedding for facial action unit detection. 
In Automatic Face and Gesture Recognition (FG), pages 9:1-
6, 2015. 
[50] S. Zafeiriou, A. Papaioannou, I. Kotsia, M. Nicolaou and G. 
Zhao. Facial Affect ‚ÄúIn-the-Wild‚Äù: A Survey and a New 
Database. In CVPRW , pages 1487-1498, 2016. 
[51] L. Zhang, D. Tjondronegoro and V. Chandran. Random 
Gabor based templates for facial expression recognition in 
images with facial occlusion.  Neurocomputing, 145:451-464, 
2014. 
[52] X. Zhang, F. Zhou, Y. Lin and S. Zhang, Embedding label 
structures for ¬øne-grained feature representation. In CVPR , 
pages 1114‚Äì1123, 2016. 
[53] Y. Zong, W. Zheng, X. Huang, J. Yan, and T. Zhang. 
Transductive transfer lda with riesz-based volume lbp for 
emotion recognition in the wild. In ICMI , pages 491‚Äì496, 
2015. 
531
531
"
https://ieeexplore.ieee.org/document/8291717,"2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2018.2805861, IEEE Access
 
VOLUME XX, 2017 1 2169 -3536 ¬©  201 7 IEEE. Translations and content mining are permitted for academic research only.  
Personal use is also permitted, but republication/redistribution requires IEEE permission.  
See http://www.ieee.org/publications_standards/publications/rights/index.html for m ore information.  Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000. 
Digital Object Identifier 10.1109/ACCESS.2017.Doi Number 
Adaptive Feature Mapping for Customizing 
Deep Learning Based Facial Expression 
Recognition Model 
Bing-Fei Wu 1, Fellow, IEEE and Chun-Hsien Lin1, Student Member, IEEE 
1National Chiao Tung University , Hsinchu, 30010 Taiwan  
Corresponding author: Chun-Hsien Lin (e-mail: clifflin@cssp.cn.nctu.edu.tw ). 
This work was supported by the Ministry of Science Technology under Grant no. MOST 106-2622-E-009-009 -CC2.  
ABSTRACT  Automated facial expression recognition can greatly improve the human-machine interface. 
The machine can provide better and more personalized services when it knows the human's emotion. This 
kind of improvement is an important progress in this artificial intelligence era. Many deep learning 
approaches have been applied in recent years due to their outstanding recognition accuracy after training 
with large amounts of data. The performance is limited, however, by the specific environmental conditions 
and variations in different persons involved. Hence, this paper addresses the issue of how to customize the 
generic model without label information from the testing samples. Weighted Center Regression Adaptive 
Feature Mapping (W- CR-AFM) is mainly proposed to transform the feature distribution of testing samples 
into that of trained samples. By means of minimizing the error between each feature of testing sample and 
the center of the most relevant category , W-CR-AFM can bring the features of testing samples around the 
decision boundary to the centers of expression categories; therefore, their predicted labels can be corrected. 
When the model which is tuned by W- CR-AFM is tested on extended Cohn-Kanade (CK+) [2], Radboud 
Faces Database (RaFD) [3], and Amsterdam Dynamic Facial Expression Set (ADFES) [4], our approach 
can improve the recognition accuracy by about 3.01%, 0.49%, and 5.33% respectively. Compared to the 
competing deep learning architectures with the same training data, our approach shows the better 
performance. 
INDEX TERMS  Cross domain adaption, Facial expression recognition, Computer vision, Pattern 
recognition, Image processing 
I. INTRODUCTION 
Facial expression recognition plays a vital role in the 
artificial intelligen ce era. According to the human‚Äôs emotion  
information, machines can provide personalized services. 
Many applications, such as virtual reality, personalized 
recommendations, customer satisfaction, and so on, depend 
on an efficient and reliable way to recognize the facial 
expressions. This topic has attracted many researchers for 
years, but it is still a challenging topic since expression 
features vary greatly with the head poses, environments, and 
variations in the different persons involved .  
To mitigate these variations, some approaches modified 
the handcraft ed features to gain the better performance, like 
[32] and [33]. Qirong Mao et al.  [34] make a Bayesian model 
by means of multiple head poses to conquer the feature 
variation caused by head poses. However, the handcraft ed features have shown their limitations in practical applications, 
so deep learning methods are utilized to make the models 
learn to extract the complicated features from large amounts 
of facial expression data [5][6][7][8][9]. Most of the standard 
database for facial expression recognition are not candid 
since they are built under the controlled environment with 
coached expressions. Therefore, [5][8][9] apply data mining 
technique to search for the facial images on the internet to 
make the model more realistic. For deep learning neural 
networks, there is no clear rule to determine the architecture 
and learning parameters, so image pre-processing is often 
adopted to improve the neural network‚Äôs  performance. Andre 
et al.  [10] apply the spatial normalization, local intensity 
normalization, and facial image cropping to the 
Convolutional Neural Network (CNN). Mapped binary 
pattern method is utilized in [11]. They all have the better 
2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2018.2805861, IEEE Access
 
VOLUME XX, 2017 9 result after applying the pre-processing. In addition, some 
other approaches combine common machine learning models 
to gain the robustness and higher performance. Duc et al.  [12] 
take the second- to-last output layer as the encoded features, 
and utilize Support Vector Machine (SVM) to be the label 
predictor. Dennis et al.  [13] propose a 2-channel CNN, and 
the first convolutional layer in one of the channels is trained 
by Convolutional Auto-Encoder (CAE) to learn the better 
capability in order to extract better features. In order to 
mitigate the effect of head pose, a CNN learns the pose-
robust features by regressing the features extracted from the 
Principal Component Analysis Network (PCANet) which has 
been trained by the frontal facial images with various 
expressions [14]. Different from traditional learning 
algorithms in CNN, the model in [15] learns the correlations 
among the training data. To mitigate the person-specific 
differences, Zibo Meng et al.  [35] and Chongsheng Zhang et 
al. [36] propose a way to train an identity-aware structure to 
extract the person-specific features for recognizing the facial 
expressions. Rather than trying to recognize a single image, 
[16][17][18] predict the expressions by passing a video, 
which seems more reasonable in practice; nevertheless, 
labeling video data is more labor intensive. 
The approaches mentioned are all static after the learning 
procedure. If applying enough data for training, they can do 
well in general cases while the performance would be 
relatively low in the specific testing, like the experiment 
results in [9]. Moreover, CNNs are also weak with cross-
domain data [6]. Consequently, adaptive learning may be a 
possible solution to tailor the generic model in specific cases. 
Feature adaption proposed in [19] tries to find a mapping in 
the universal Reproducing Kernel Hilbert Space (RKHS). 
With the mapping, the features of samples can be transferred 
into a new space where the feature distributions of testing 
and training samples are similar. Sinno  et al.  [20] realize the 
domain adaption by transfer component analysis while Tzu-
Ming et al.  [21] aim to use closest common space learning 
for associating cross-domain data. Besides, Wen-Sheng et al.  
[22] train the generic SVM by re-weighting the trained 
samples which are most relevant to the testing data. These 
methods have to calculate the relations between both training 
and testing data sets, so the computational complexity is so 
high that it is difficult to apply them to deep learning models.  
In this research, three types of Adaptive Feature Mapping 
(AFM) are proposed to transfer the feature space of testing 
samples to that of training samples as close ly as possible. 
Since AFM learns the data sequentially, it can be deployed to 
deep learning models easily, and does improve the 
performance. 
There are two main contributions in this paper. First, a 
novel pre-processing method is proposed for the general 
facial image processing, and it does improve the 
performance. Second, the domain adaption methods, AFMs, 
for deep learning models with large number of training data 
is proposed, which can fine-tune the parameters efficiently and gain better recognition accuracy in the specific 
applications. 
This paper is organized as follows. Section II introduces 
the preparation of testing and training data. Section III and 
IV address the method of image pre-processing and the CNN 
architecture. Section V explains how AFMs work and their 
design principles. Section VI shows the experiments and the 
discussions. Section VII is the conclusion. 
II.  FACIAL EXPRESSION DATABASE 
This section addresses the usage of the facial expression 
database and the process of preparing the training and testing 
data. Only seven common facial expressions, anger, disgust, 
fear, happiness, sadness, surprise, and neutral, are considered 
in this paper. Other expressions are ignored even if they are 
collected in the public domain database. 
A. EXTENDED COHN-KANADE 
Extended Cohn-Kanade (CK+), [2], has been widely utilized 
to research facial expression recognition for years. In each 
expression category for a person, there are about 15 images 
in a sequence, and the expression intensity changes from low 
to high. The first one or two images are regarded as the 
neutral expression while the last one or two images are 
selected as the expressions in full effect. Consequently, there 
are 630 images from CK+. The samples are shown in Fig. 1. 
B. RADBOUD FACES DATABASE 
The Radboud Faces Database (RaFD), [3], is a high quality 
database of faces, which contains pictures of 8 emotional 
expressions, including Caucasian males and females, 
(a) (b) (c) (d) (e) (f) (g)
 
FIGURE  1.  Samples in CK+. (a) Anger. (b) Disg ust. (c) Fear. (d) 
Happiness. (e) Sadness. (f) Surprise. (g) Neutral.  
  
(a) (b) (c) (d) (e) (f) (g)
 
FIGURE  2.  Samples in RaFD . (a) Anger. (b) Disgust. (c) Fear. (d) 
Happiness. (e) Sadness. (f) Surprise. (g) Neutral.  
  
2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2018.2805861, IEEE Access
 
VOLUME XX, 2017 9 Caucasian children, both boys and girls, and Moroccan 
Dutch males. Head poses vary from left side to right, and 
each pose is shot with three eye gazing directions. Compared 
to CK+, RaFD is more challenging to the recognition model. 
The samples are shown in Fig. 2. 
C. AMSTERDAM DYNAMIC FACIAL EXPRESSION SET 
Around 10 emotional expressions are collected in the 
Amsterdam Dynamic Facial Expression Set (ADFES), [4]. 
Most of them are videos with head pose variations, and the 
expression intensity also changes from low to high, like in 
CK+. The facial images are captured with fixed time steps 
when the expressions start to become obvious. The samples 
are shown in Fig. 3. 
D. PROPRIETARY DATABASE 
To make the deep model more robust and general, a home-
grown/proprietary database is built to train the model. 372 
videos are downloaded from YouTube, including movies, 
film reviews, variety shows, and some short videos. After 
that, a face detection method proposed by D. E. King, [25], is 
employed to capture the face images with the time intervals 
set to 1, 2, or 3 second(s) to avoid repeating the images with 
similar expressions for one person. Then, 100,000 facial 
images are produced. Only the images that represent their 
corresponding categories are manually picked to be the 
training and testing samples. This database ended up with 
17,655 images. Some samples are shown in Fig. 4. 
E. TRAINING AND TESTING DATA REARRANGEMENT Since CK+ is a well-known benchmark in facial expression 
recognition and the number of images is small, it will not be 
placed in the training set but instead in the testing set only in 
order to objectively show the performance of the proposed 
approach. Out of RaFD and ADFES, 10 and 4 person s‚Äô 
images are chosen to be the testing data respectively; 
therefore, people in the training and the testing sets are 
definitely different. Altogether, the number of testing images 
is 630 from CK+, 616 from RaFD, and 562 from ADFES 
while the number of training data is 23,591 including 17,655 
from the proprietary database, 3,377 from RaFD and 2,559 
from ADFES. The configuration of the testing and training 
data is listed in TABLE I. 
To balance the image count throughout all categories, the 
category with less images is supplemented by copying the 
randomly selected images before combining the training data. 
In this way, the total number of images in every category will 
be the same.  
Researches show that if the data is augmented in a 
reasonable way, the model can perform much better, [30]. 
Thus, the training set is mirrored and also augmented by two 
Gamma transformation, three Gaussian blur, and three 
sharpening filter, so one image is extended to 42 images. As 
a result, the total number of training data is increased to 
2,315,544, and the resolution is set to 
6 4 6 4ÔÇ¥  pixels in 
grayscales. TABLE  I 
THE CONFIGURATION OF THE TESTING AND TRAINING DATA 
Type  Database  Anger  Disgust  Fear  Happiness  Sadness  Surprise  Neutral  Total  
Testing CK+  63 91 40 130 42 150 114 630 
RaFD  88 90 87 90 89 83 89 616 
ADFES  80 80 80 80 82 80 80 562 
Train ing RaFD  494 494 466 493 485 452 493 3,377 
ADFES  367 373 367 367 380 343 362 2,559  
Proprietary  407 244 411 6,545  2,105  941 7,002  17,655  
 
(a) (b) (c) (d) (e) (f) (g)
 
FIGURE  3.  Samples in ADFES . (a) Anger. (b) Disgust. (c) Fe ar. (d) 
Happiness. (e) Sadness. (f) Surprise. (g) Neutral.  
  
(a) (b) (c) (d) (e) (f) (g)
 
FIGURE  4.  Samples in proprietary  database  (a) Anger. (b) Disgust. (c) 
Fear. (d) Happiness. (e) Sadness. (f) Surprise. (g) Neutral.  
  
2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2018.2805861, IEEE Access
 
VOLUME XX, 2017 9 III. IMAGE PRE-PROCESSING 
Previous researches, [10] and [11], have shown that if the 
image is pre-processed appropriately, the recognition 
performance can be improved. In this chapter, a proposed 
pre-processing method which contains spatial normalization 
and feature enhancement is introduced. 
A. SPATIAL NORMALIZATION 
The purpose of spatial normalization is to adjust the 
alignment of the position and rotation angle of the detected 
facial images. An example is shown in Fig. 5. 
A face alignment algorithm [23] is utilized to detect some 
landmarks on the face. The tip of the nose will be shifted to 
the center of the image so that the placement offset can be 
mitigated. 
B. FEATURE ENHANCEMENT 
Local Binary Pattern (LBP) [30] may be an efficient way to 
extract the features from images. Nonetheless, it may lose a 
lot of intrinsic information. Jiwen et al. [24] try to fix this 
problem by finding a mapping from the Neighbor-Center 
Difference Vector (NCDV) into the binary space so that the 
patterns can better represent the images in the original 
database, but it needs more computing effort. 
Neighbor-Center Difference Image (NCDI) is presented to 
enhance the edges efficiently and retain the original 
information. The concept is the same as NCDV. NCDIs are 
extracted by subtracting the center pixel from the 
neighboring pixels, so the pixel values fall in the range from 
255ÔÄ≠
 to 
255 . An NCDI collects the subtraction results of 
the selected channel from all patch es to reconstruct the image. 
Thus, eight images which have been sharpened in eight 
different directions are produced if the 8-channel NCDI is 
applied, Fig. 6. 
After enhancing the edges, the facial contour and 
background become sharper, but they have nothing to do 
with facial expressions. Hence, facial image cropping should 
be applied. Since the facial contour is often confused with the background, the detected landmarks may drift between the 
facial contour and background. It is not recommended to 
crop the facial image by connecti ng the landmarks, which is 
considered as polygon cropping. Except for the contour, 
other landmarks are more stable. An elliptical region which 
regresses the suitable landmarks, as shown in Fig. 7, is the 
better way to crop the facial image effectively. The ellipse 
function is 
 
ÔÄ® ÔÄ©22
1 2 3 4 5 6, f x y a x a x y a y a x a y a ÔÄΩ ÔÄ´ ÔÄ´ ÔÄ´ ÔÄ´ ÔÄ´a , (1) 
where 
ÔÅõ ÔÅù1 2 6T
a a aÔÄΩa
  and 
2
1 3 240a a aÔÄ≠ÔÄæ . To 
regress these landmarks, the cost function is defined as 
 
ÔÄ® ÔÄ©
ÔÄ® ÔÄ© ÔÄ® ÔÄ©22
1 3 2 1 3
1,
1
,4
22N
nn
nf x y a a a a a
NÔÅ§
ÔÄΩÔÅóÔÄΩ
ÔÄ≠ ÔÄ≠ ÔÄ´ ÔÄ´ ÔÉ•a x y
a ,  (2) 
where 
ÔÅõ ÔÅù12T
Nx x xÔÄΩx
  and 
ÔÅõ ÔÅù12T
Ny y yÔÄΩy
  
are the selected positions of the landmarks, 
N  is the sample 
number, and 
ÔÅ§  is the hyper parameter used to regularize the 
optimization. By setting the gradient of the cost function to 
zero, 
ÔÄ® ÔÄ© ,0 ÔÉëÔÅó ÔÄΩ a x y , the equation becomes 
 
ÔÄ® ÔÄ©ÔÄ≠ÔÄΩDŒ®aŒõ , (3) 
 
4 3 2 2 3 2 2
3 2 2 3 2 2
2 2 3 4 2 3 2
3 2 2 2
1
2 2 3 2
221i i i i i i i i i
i i i i i i i i i i i i
N
i i i i i i i i i
i i i i i i i i i i
i i i i i i i i i
i i i i i ix x y x y x x y x
x y x y x y x y x y x y
x y x y y x y y y
x x y x y x x y x
x y x y y x y y y
x x y y x yÔÄΩÔÉ©ÔÉπ
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫
ÔÄΩÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫ÔÉ´ÔÉªÔÉ•D , (4) 
 
2
0 0 0 0 0
0 0 0 0 0
2
0 0 0 0 0
0 0 0 0 0 0
0 0 0 0 0 0
0 0 0 0 0 0N
N
NÔÅ§
ÔÅ§
ÔÅ§ÔÉ©ÔÉπ
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫ÔÄ≠ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫
ÔÄΩÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫
ÔÉ´ÔÉªŒ® , (5) 
 
0 0 0 0
22TNNÔÅ§ÔÅ§ÔÉ©ÔÉπÔÄΩÔÉ™ÔÉ∫ÔÉ´ÔÉªŒõ , (6) 
 
FIGURE  5.  Spatial normalization in image pre -processing . 
  
 
FIGURE 7.  Ellipse regression. The red landmarks are the suitable 
points used to find the cropping boundary, the blue curve, by ellipse 
regression.  
 
 
FIGURE 6.  Neighbor -center difference images (NCDIs).  
  
2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2018.2805861, IEEE Access
 
VOLUME XX, 2017 9 where 
ÔÄ≠DŒ®  is a symmetric matrix, so 
ÔÄ® ÔÄ©1ÔÄ≠ÔÄ≠DŒ®  exists if it 
is of full rank. Through 
ÔÄ® ÔÄ©1ÔÄ≠ÔÄΩÔÄ≠aD Œ® Œõ , there is an 
analytical solution to find the ellipse. 
The schematic diagram is shown in Fig. 7. Only the pixels 
in the ellipse and those lower than the highest landmarks of 
the eyebrows are kept, as Fig. 8 shows. The differences 
between ellipse cropping and polygon cropping are shown in 
Fig. 9. 
The pre-processing procedure follows the steps below . 
Detect the face and find the bounding box [25]. Resize the 
facial image into 
6 4 6 4ÔÇ¥  pixels. Then, extract the landmarks 
on the face, and perform the spatial normalization and feature 
enhancement last. 
IV. DEEP CONVOLUTIONAL NEURAL NETWORK 
Based on Caffe framework [26], a CNN model is designed 
from the concept of [6] and [27]. There are parallel 
structures in the network to extract the features using 
different sizes of windows. The model consists of nine 
convolutional layers, two max pooling layers, one mean 
pooling layer, three fully connected layers, and a Local 
Response Normalization (LRN) layer. The activation 
functions are all set to rectified linear functions. Other 
details and the configuration of the model are shown in Fig. 
10. 
Like [12], the output of Full connection layer (L12) is 
regarded as the encoded features. The combination of Full 
connection layer (L13) and Softmax output layer (L14) is 
regarded as a classifier. Except for the classifier, the whole 
structure is a feature extractor for the input image. The 
Convolutional Feature Extractor (CFE) is defined as from 
Convolution layer (L1) to Mean pooling layer (L10) while 
the Fully Connected Feature Extractor (FCFE) is defined as 
from Full connection layer (L11) to Full connection layer 
(L12). 
 
FIGURE 10.  Convolutional neural network architecture. C, H, W are the 
output channel, height, and weight respectively. K, S, P stand for the  
kernel size, stride, and patch of convolution or pooling. N is the local 
size while A and B are the scale and the exponent parameters of LRN. 
D is the dropout ratio.  
 
FIGURE 8.  Ellipse cropping of NCDIs.  
 
 
FIGURE 9.  Comparison between ellipse cropping and polygon cropping. 
The images in the top row are the results of ellipse cropping NCDIs, and 
the images in the bottom row are the results of polygon cropping NCDIs.  
 
2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2018.2805861, IEEE Access
 
VOLUME XX, 2017 9 V. ADAPTIVE FEATURE MAPPING 
This section address es the design principle and the 
mechanism of AFM. In the following description, the trained 
and testing data sets are denoted as 
12ss s s s
NÔÉ©ÔÉπÔÄΩÔÉ´ÔÉªX x x x
 and 
12tt t t t
NÔÉ©ÔÉπÔÄΩÔÉ´ÔÉªX x x x
  
respectively, and 
sN  is the number of trained samples while 
tN
 is the batch size of the testing samples. The feature 
extractor consists of CFE and FCFE, and it is denoted as 
ÔÄ® ÔÄ©h x W
. 
W  is the parameter set of the whole feature 
extractor while 
x  is the input sample. In this research, 
x  is 
the 8-channel NCDIs. 
A. COST FUNCTION 
The main purpose of AFM is to tune the parameters of the 
feature extractor for the testing samples so that the tuned 
feature extractor can make the feature distribution of the 
testing samples similar to that of the trained samples. See Fig. 
11. That is, 
ÔÄ® ÔÄ©ÔÄ®ÔÄ©ÔÄ® ÔÄ©ÔÄ® ÔÄ©tspp ÔÇª h x W h x W
 , where 
W  is the 
generic parameter set, and 
W
  is the new parameter set for 
the testing samples. To accomplish this, the discrepancy 
between the means of the trained and testing samples must be 
minimized. According to [19][20][22], the cost function can 
be written as 
 
ÔÄ® ÔÄ©
ÔÄ® ÔÄ©ÔÄ®ÔÄ© ÔÄ® ÔÄ©ÔÄ® ÔÄ©2
11,
11tsts
NN
ts
ij
ijtsE
NNÔÅ™ÔÅ™
ÔÄΩÔÄΩÔÅàÔÄΩ
ÔÄ≠ ÔÉ•ÔÉ•W X X
h x W h x W
 , (7) 
where 
ÔÅà  stands for RKHS which can be defined by a kernel, 
ÔÄ® ÔÄ© ÔÄ® ÔÄ© ÔÄ® ÔÄ©ÔÄ® ÔÄ©ÔÄ® ÔÄ© ÔÄ® ÔÄ©ÔÄ®ÔÄ© ,T
t s t s
i j i jk ÔÅ™ÔÅ™ÔÄΩ h x W h x W h x W h x W
, to 
describe the relation between 
ÔÄ® ÔÄ©t
ih x W  and 
ÔÄ® ÔÄ©s
jh x W . The 
linear kernel is chosen for simplicity, so the kernel is 
ÔÄ® ÔÄ© ÔÄ® ÔÄ© ÔÄ® ÔÄ©ÔÄ® ÔÄ© ÔÄ® ÔÄ©2
,t s t s
i j i jk ÔÄΩÔÄ≠ h x W h x W h x W h x W
. Based on 
[20], the cost function shall be 
 
ÔÄ® ÔÄ©ÔÄ® ÔÄ© ,tsE T rÔÄΩ W X X K L
 ,  (8) 
 
,,
,,s s s t
t s t tÔÉ©ÔÉπ
ÔÄΩÔÉ™ÔÉ∫
ÔÉ´ÔÉªKK
K
KK , (9)  
ÔÄ® ÔÄ© ÔÄ® ÔÄ© ÔÄ® ÔÄ©
ÔÄ® ÔÄ© ÔÄ® ÔÄ© ÔÄ® ÔÄ©
ÔÄ® ÔÄ© ÔÄ® ÔÄ© ÔÄ® ÔÄ©1 1 1 2 1
2 1 2 2 2
,
12, , ,
, , ,
, , ,s
s
s s s ss s s s s s
N
s s s s s s
N
ss
s s s s s s
N N N Nk k k
k k k
k k kÔÉ©ÔÉπ
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫ÔÄΩ
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫ÔÉ´ÔÉªh h h h h h
h h h h h h
K
h h h h h h
 , (10) 
 
ÔÄ® ÔÄ© ÔÄ® ÔÄ© ÔÄ® ÔÄ©
ÔÄ® ÔÄ© ÔÄ® ÔÄ© ÔÄ® ÔÄ©
ÔÄ® ÔÄ© ÔÄ® ÔÄ© ÔÄ® ÔÄ©1 1 1 2 1
2 1 2 2 2
,
12, , ,
, , ,
, , ,t
t
t t t tt t t t t t
N
t t t t t t
N
tt
t t t t t t
N N N Nk k k
k k k
k k kÔÉ©ÔÉπ
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫ÔÄΩ
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫ÔÉ´ÔÉªh h h h h h
h h h h h h
K
h h h h h h
 , (11) 
 
ÔÄ® ÔÄ© ÔÄ® ÔÄ© ÔÄ® ÔÄ©
ÔÄ® ÔÄ© ÔÄ® ÔÄ© ÔÄ® ÔÄ©
ÔÄ® ÔÄ© ÔÄ® ÔÄ© ÔÄ® ÔÄ©1 1 1 2 1
2 1 2 2 2
,
12, , ,
, , ,
, , ,t
t
s s s ts t s t s t
N
s t s t s t
N
st
s t s t s t
N N N Nk k k
k k k
k k kÔÉ©ÔÉπ
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫ÔÄΩ
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫
ÔÉ™ÔÉ∫ÔÉ´ÔÉªh h h h h h
h h h h h h
K
h h h h h h
 , (12) 
 
,,T
t s s tÔÄΩ KK , (13) 
where 
t
ih  and 
s
jh  are the abbreviations of 
ÔÄ® ÔÄ©t
ih x W
  and 
ÔÄ® ÔÄ©s
jh x W
 respectively. The elements of the matrix 
L  is 
21
sNÔÄ≠
 if 
,s
ijÔÉé x x X , else 
21
tNÔÄ≠  if 
,t
ijÔÉé x x X , otherwise, 
1
stNN
. Since we are only concerned with the cross relations 
between the features of trained and testing samples, other 
terms can be eliminated. Therefore, the cost function can be 
modified as 
 
ÔÄ® ÔÄ©
ÔÄ® ÔÄ© ÔÄ® ÔÄ©2
,
11,
tsts
NN
ij ts
ij
ij stE
NNÔÅ°
ÔÄΩÔÄΩÔÄΩ
ÔÄ≠ ÔÉ•ÔÉ•W X X
h x W h x W
 , (14) 
where 
,ijÔÅ°  are the weights to represent the relevance 
between 
ÔÄ® ÔÄ©t
ih x W   and 
ÔÄ® ÔÄ©s
jh x W . A large 
,ijÔÅ°  value is 
required when the features of trained and testing samples are 
relevant, i.e. error becomes small. On the contrary, a smaller 
,ijÔÅ°
 value is required when the features of trained and testing 
samples are less relevant, i.e. error becomes larger. If 
,ijÔÅ°  is 
not appropriate, the cost function will oscillate drastically and 
 
FIGURE 11.  The purpose of AFM. There are red and blue categories in the example. The crosses and circles are the trained data. The triangles are the 
new testing subjects, and the color on each subject represents the ground truth.  
 
2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2018.2805861, IEEE Access
 
VOLUME XX, 2017 9 may not converge during the training process. Moreover, the 
computational complexity will increase as the number of 
trained samples increases. The winner-take-all strategy, 
therefore, is considered, so the cost function can be re-written 
as 
 
ÔÄ® ÔÄ© ÔÄ® ÔÄ© ÔÄ® ÔÄ©2
11
,
2tN
t s t s
ir
itE
NÔÄΩÔÄΩÔÄ≠ÔÉ•W X X h x W h x W
 , (15) 
 
ÔÄ® ÔÄ© ÔÄ® ÔÄ©2
a rg m ints
irrÔÉ¶ÔÉ∂ÔÄΩÔÄ≠ ÔÉßÔÉ∑
ÔÉ®ÔÉ∏h x W h x W
 , (16) 
where 
r  is the index of the trained sample which is most 
relevant to the 
thi  new sample in distance measure, and 
ÔÅõ ÔÅù1,srNÔÉé
. By minimizing the nearest trained sample, the 
oscillation is damped, and the computational complexity is 
reduced. 
Since some bad samples may limit the performance of 
AFM , the probability distribution on the output of the 
classifier can be taken into consideration to regularize the 
cost function. By simply multiplying the prediction 
confidence, the cost function can be written as 
 
ÔÄ® ÔÄ©
ÔÄ® ÔÄ©ÔÄ® ÔÄ©ÔÄ® ÔÄ© ÔÄ® ÔÄ©2
1,,
1
2t
s
rt s s
N
s t s
r i ry
itE
f
NÔÄΩÔÄΩ
ÔÉóÔÄ≠ ÔÉ•W X X y
h x W h x W h x W
 , (17) 
where the entries of 
12sTs s s s
Ny y yÔÉ©ÔÉπÔÄΩÔÉ´ÔÉªy
  are the 
labels of the trained samples. 
ÔÄ® ÔÄ©12kT
Nf f fÔÉ©ÔÉπÔÄΩÔÉ´ÔÉªf
  is 
the classifier, and 
kN  is the number of categories. This 
form of AFM is defined as Weighted Adaptive Feature 
Mapping (W-AFM). 
The classifier in CNN is a linear transformation. After a 
CNN model is well trained, the features extracted by the 
model must be linearly separable. Therefore, there must be 
a unique center in each category. If the centers of the 
categories are considered, the person-specific bias can be 
mitigated further. The cost function, then, can be modified 
as 
 
ÔÄ® ÔÄ©
ÔÄ® ÔÄ©ÔÄ® ÔÄ©ÔÄ® ÔÄ©2
1,,
1
2t
ss
rrt s s
N
s t c
riyy
itE
f
NÔÄΩÔÄΩ
ÔÉóÔÄ≠ ÔÉ•W X X y
h x W h x W h
 , ( 18) 
where 
s
rc
yh  stands for the feature center of category 
s
ry , and 
such form of AFM is regarded as Weighted Center 
Regression Adaptive Feature Mapping (W- CR-AFM). 
These cost functions can be easily solved by the 
stochastic gradient descent which is written as follow: 
 
ÔÄ® ÔÄ©1,,k k t s s kEÔÅ®ÔÅ¨ÔÄ´ÔÉ©ÔÉπ ÔÄΩ ÔÄ≠ ÔÉó ÔÉë ÔÄ´ÔÉ´ÔÉªW W W X X y W
 ,  (19) 
where 
ÔÅ®  is the learning rate, and 
ÔÅ¨  is the regularizing 
factor to prevent the parameters from going out of bound. 
ÔÄ® ÔÄ© ,,t s sEÔÉë W X X y
 is the gradient of the cost function. B. SYSTEM OPERATION 
After training the CNN model, the extracted features of 
training samples shall be stored as the feature database. In the 
testing phase, AFM can tune the weights based on the 
relationship between features of testing samples and the 
feature database in order to transform the features of testing 
samples into a new space so that its distribution can be 
similar to that of the feature database. Most of the parameters 
are distributed in the fully connected layers, so AFM is only 
applied to tune FCFE for higher efficiency. See Fig. 12. The 
premise of AFM is that the feature distribution of the testing 
samples is assumed to be similar to that of the training 
samples. The features around the decision boundary, 
therefore, shall be moved to the centers of categories. This 
way, the misclassified labels can be corrected. In addition, 
misclassified trained samples must be removed in advance so 
that the newly mapped features can be better. To make it 
more reliable, the testing samples with lower confidence of 
prediction can be ignored. 
VI. EXPERIMENTS 
A.  IMAGE PRE-PROCESSING COMPARISON 
TABLE II shows the results of the proposed model with 
different pre-processing methods. As can be seen in TABLE 
II, the spatial normalization does not always seem to help the 
recognition accuracy since the edges of bounding box may 
appear and become the main feature of the image after spatial 
normalization is applied, which impairs the recognition 
function, causing the accuracy to be lower . Also, the model 
has been trained with many candid images from YouTube, so 
it can extract some features that are not affected by rotation. 
CFE ClassifierImage 
pre-processing
Feature databaseAFMFCFE
CFEImage 
pre-processing
HappinessClassifier FCFE
LossTesting phase
Training phase 
FIGURE 12.  System architecture.  
TABLE  II 
IMAGE PRE-PROCESSING COMPARISON  
Pre-processing  Accuracy (%)  
CK+ RaFD  ADFES  
None  82.22  90.26  85.59  
SN 81.75  94.16  83.10  
SN + FE  86.83  95.78  87.37  
SN stands for spatial normalization while FE sta nds for feature 
enhancement.  
2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2018.2805861, IEEE Access
 
VOLUME XX, 2017 9 Thus, the recognition accuracy can be higher than when 
spatial normalization is applied. These may be the reasons 
why the spatial normalization appears ineffective in Table II . 
The feature enhancement operation not only makes the facial 
edges more distinct but also removes the areas that are irrelevant to facial expressions, so the accuracy can be 
increased by about 4.61% in CK+, 5.52% in RaFD, and 
1.78% in ADFES. These results demonstrate that the 
proposed pre-processing method is really effective. 
7072747678808284868890
16 32 48 64 80 96 112 128 144 160 176 192 208 224 240 256 512Accuracy (%)
Batch SizeAFM
W-AFM
W-CR-AFM
GM 
FIGURE 13.   The improved accuracy of CK+ in different batch size.  
 
9394959697
16 32 48 64 80 96 112 128 144 160 176 192 208 224 240 256 512Accuracy (%)
Batch SizeAFM
W-AFM
W-CR-AFM
GM
 
FIGURE 14.   The i mproved accuracy of RaFD in different batch size.  
 
858687888990919293
16 32 48 64 80 96 112 128 144 160 176 192 208 224 240 256 512Accuracy (%)
Batch SizeAFM
W-AFM
W-CR-AFM
GM
 
FIGURE 15.   The improved accuracy of ADFES in different batch size.  
 
2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2018.2805861, IEEE Access
 
VOLUME XX, 2017 9 B. THE EFFECT OF ADAPTIVE FEATURE MAPPING 
The proposed CNN model with spatial normalization and 
feature enhancement is regard ed as our Generic Model (GM). 
As for AFM, the learning rate 
ÔÅ®  is set to 0.001 while the 
regularizing factor 
ÔÅ¨  is set to 0.0005. The training iteration 
is set to 1000. The batch size ranges from 16 to 5 12. The 
trained samples and testing samples are mirrored, and the 
trained samples which are misclassified  should be removed 
in advance while the testing samples whose prediction 
confidence is lower than 90% are not taken into 
consideration when tuning the model with AFM . The results 
are shown in Fig. 13, Fig. 14, and Fig. 15. In most cases, W-
AFM performs better than AFM, and W- CR-AFM is the best 
of all. The performance will be more stable when the batch 
size is large enough, otherwise the effect may be limited.  
According to the experiments shown in Fig. 13, Fig. 14, 
and Fig. 15, the best result of each AFM is listed in TABLE 
III. Based on the result of GM, the improved recognition 
accuracy is in TABLE IV.  The category of happiness can 
always be predicted correctly in these three databases 
because its feature is obvious and its training data is ample. 
After applying AFM, most predicted labels are corrected. 
Compared to other categories, the number of images in 
anger, disgust and fear is less, so the ability of extracting 
features for these expressions is poor; hence, most features 
of testing samples do not fall into the center of the category, 
and will be drawn to other categories. Besides, the 
expression of anger is usually not explicit, so it is 
sometimes confused with neutral expression even if AFM 
or W-AFM is utilized. The main feature of surprise is the 
exaggerated mouth while the minor feature is the eyes, but 
the feature of eyes is difficult to extract properly because it varies greatly with the person. Hamid et al.  [29] have 
proven that the mouth is the primary feature for facial 
expressions. However, some surprised faces do not clearly 
express the feature on the mouth in ADFES, so they are 
misclassified into neutral expression if AFM or W-AFM is 
applied.  
For W- CR-AFM, since it minimizes the distance 
between the feature of the testing sample and the center of 
the most relevant category rather than the most relevant 
feature of the trained sample, the person-specific bias can 
be mitigated much more. Moreover, the feature distribution 
of neutral expression contains the largest area in the featur e 
space, so the features of neutral expression that are far away 
from the center of neutral expression category will be 
brought to other categories. That is why the recognition 
accuracy of neutral expression is reduced after applying W-
CR-AFM while accuracy of other expressions are raised. 
According to the experiments, all three types of AFM 
can assist in improving the performance of a model in 
specific cases. For the overall recognition accuracy, W- CR-
AFM works the best. 
C. BENCHMARK COMPARISON 
Some other deep learning approaches in facial expression 
recognition are introduced and compared with ours. They 
are trained with our training data for fair comparison.   
To make GoogLeNet [27] and AlexNet [28] perform better , 
they have been trained with ImageNet previously. The 
second- to-last layer of the trained AlexNet is utilized to 
train a SVM [12]. To present the original performance of 
the competing models, the architectures and training 
parameters are set based on the original works. Since CK+ 
[2] is not included in the training data, it is reasonable that TABLE  IV 
IMPROVEMENT IN EACH DATABASE  
Database  
Category  CK+  (%) RaFD  (%) ADFES  (%) 
AFM  W-AFM  W-CR-AFM  AFM  W-AFM  W-CR-AFM  AFM  W-AFM  W-CR-AFM  
Anger  -7.94 -4.76 +3.17 +1.14 +1.14 +1.14 +7.50 +8.75 +10.00  
Disgust  +1.10 +4.40 +21.98  +1.11 +1.11 +2.22 0.00 +3.75 +11.25  
Fear  +10.00  +10.00  +35.00  -2.30 -2.30 +1.15 -2.50 -2.50 +1.25 
Happiness  0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
Sadness  +2.38 +2.38 0.00 +3.37 +3.37 +6.75 0.00 0.00 +3.66 
Surprise  +2.00 +2.00 +5.33 0.00 0.00 +1.20 -7.50 -5.00 +2.50 
Neutral  +1.75 0.00 -21.93  0.00 -1.13 -8.99 +25.00  +25.00  +8.75 
Total  +0.95 +1.42 +3.01 +0.49 +0.32 +0.49 +3.20 +4.27 +5.33 
 TABLE  III 
ACCURACY IN EACH DATABASE  
Database  
Category  CK+  (%) RaFD  (%) ADFES  (%) 
GM AFM  W-AFM  W-CR-AFM  GM AFM  W-AFM  W-CR-AFM  GM AFM  W-AFM  W-CR-AFM  
Anger  77.78  69.84  73.02  80.95  98.86  100.00  100.00  100.00  88.75  96.25  97.50  98.75  
Disgust  69.23  70.33  73.63  91.21  97.78 98.89  98.89  100.00  88.75  88.75  92.50  100.00  
Fear  42.50  52.50  52.50  77.50  87.36  85.06  85.06  88.51  71.25  68.75  68.75  72.50  
Happiness  100.00  100.00  100.00  100.00  100.00  100.00  100.00  100.00  100.00  100.00  100.00  100.00  
Sadness  83.33  85.71  85.71  83.33  88.76 92.13  92.13  95.51  93.90  93.90  93.90  97.56  
Surprise  94.67  96.67  96.67  100.00  98.80  98.80  98.80  100.00  97.50  90.00  92.50  100.00  
Neutral  97.37  99.12  97.37  75.44  98.88  98.88  97.75  89.89  71.25  96.25  96.25  80.00  
Total  86.83  87.78  88.25  89.84  95.78  96.27  96.10  96.27  87.37  90.57  91.64  92.70  
 
2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2018.2805861, IEEE Access
 
VOLUME XX, 2017 9 the recognition accuracy in TABLE V is lower than the 
results of state- of-the-arts. If the models are trained by CK+ 
[2], the recognition accuracy is expected to be much higher. 
The results in TABLE V show that our approach 
performs better than others. The parameter quantities of 
GoogLeNet, AlexNet and the CNN designed by Heechul et 
al. [5] are around 40MB, 222MB, and 5MB respectively. 
Although our parameter quantity, around 3.5MB, which is 
much lower than others, the performance can be 
comparable to these state- of-the-art architectures through 
the use of proposed pre-processing method. Besides, AFMs 
can adapt the testing samples so that the model can perform 
better than other approaches. 
VII. CONCLUSION 
Two main contributions are presented in this paper. One 
contribution is that the proposed pre-processing method can 
assist the CNN model to gain the higher accuracy rate in the 
applications of facial image processing. The other 
contribution is that three types of AFMs can reformulate the 
features of new samples which do not have label information 
so that some misclassified samples can be corrected, which 
means it can tune a generic model to adapt to a specific 
condition. Moreover, AFMs can be deployed to real-time 
systems since it learns batch by batch rather than calculating 
all of the training and testing data in one batch. The concept 
drift problem is restrained because AFMs map the features of 
the testing samples to a static feature distribution. With the 
pre-processing and AFMs, a light CNN can outperform the 
state- of-the-art architectures. 
ACKNOWLEDGMENT 
This work was supported by the Ministry of Science 
Technology under Grant no. MOST 106-2622-E- 009-009 -
CC2. 
REFERENCES 
[1] R. E. Jack, O. G. B. Garrod, H. Yu, R. Caldara, and P. G. Schyns, 
‚ÄúFacial expressions of emotion are not culturally universal,‚Äù Proc. 
National Academy of Sci. of the United States of America , vol. 109, 
no. 19, pp. 7241 ‚Äì7244, 2012. 
[2] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. 
Matthews, ‚ÄúThe extended cohn -kanade dataset (CK+): A complete 
dataset for action unit and emotion- specified expression,‚Äù In Comput. 
Vision and Pattern Recognition (CVPR) Workshops , 2010. [3] O. Langner, R. Dotsch, G. Bijlstra, D. H. J. Wigboldus, S. T. Hawk, 
and A. van Knippenberg, ‚ÄúPresentation and validation of the 
Radboud Faces Database,‚Äú Cognition and Emotion , vol. 24, no. 8, pp. 
1377 ‚Äì1388, 2010. 
[4] J. van der Schalk, S. T. Hawk, A. H. Fischer, and B. J. Doosje, 
‚ÄúMoving faces, looking places : The Amsterdam Dynamic Facial 
Expressions Set (ADFES),‚Äù Emotion , vol. 11, no. 4, pp. 907 ‚Äì920, 
2011. 
[5] W. Li, M. Li, and Z. Su, ‚ÄúA deep -learning approach to facial 
expression recognition with candid images,‚Äù MVA2015 IAPR Int. 
Conf. Mach. Vision Appl. , May 18 ‚Äì22, 2015, Tokyo, Japan. 
[6] A. Mollahosseini, D. Chan, and M. H. Mahoor, ‚ÄúGoing deeper in 
facial expression recognition using deep neural networks,‚Äù  IEEE 
Winter Conf. Appl. of Comput. Vision (WACV) , pp. 1 ‚Äì10, 2016. 
[7] H. Jung, S. Lee, S. Park, B. Kim, J. Kim, I. Lee, and C. Ahn, 
‚ÄúDevelopment of deep learning -based facial expression recognition 
system,‚Äù 21st Korea-Japan Joint Workshop on Frontiers of Comput. 
Vision (FCV) , pp. 1 ‚Äì4, 2015. 
[8] A. Mollahosseini, B. Hassani, M. J. Salvador, H. Abdollahi, D. Chan, 
and M. H. Mahoor, ‚ÄúFacial expression recognition from world wild 
web,‚Äù IEEE Conf. on Comput. Vision and Pattern Recognition 
(CVPR)  Workshops, pp. 59 ‚Äì65, 2016. 
[9] X. Peng, Z. Xia, L. Li, and X. Feng, ‚ÄúTowards facial expression 
recognition in the wild: a new database and deep recognition 
system,‚Äù IEEE Conf. Comput. Vision and Pattern Recognition 
(CVPR)  Workshops, pp. 93 ‚Äì99, 2016. 
[10] A. T. Lopes, E. de Aguiar, and T. Oliveira- Santos, ‚ÄúA facial 
expression recognition system using convolutional networks,‚Äù 28th 
SIBGRAPI Conf. Graphics, Patterns and Images , pp. 273 ‚Äì280, 2015. 
[11] G. Levi and T. Hassner, ‚ÄúEmotion recognition in the wild via 
convolutional neural networks and mapped binary patterns,‚Äù Proc. 
2015 ACM Int. Conf. Multimodal Interaction , pp. 503 ‚Äì510, 2015. 
[12] D. M. Vo and T. H. Le, ‚ÄúDeep generic features and SVM for facial 
expression recognition,‚Äù 3rd National Foundation for Science and 
Technology Development Conf. Inf. and Comput. Sci. (NICS) , pp. 
80‚Äì84, 2016. 
[13] D. Hamester, P. Barros, and S. Wermter, ‚ÄúFace expression 
recognition with a 2- channel convolutional neural network,‚Äù Int. 
Joint Conf. Neural Netw. (IJCNN) , pp. 1 ‚Äì8, 2015. 
[14] F. Zhang, Y. Yu, Q. Mao, J. Gou, and Y. Zhan, ‚ÄúPose -robust feature 
learning for facial expression recognition,‚Äù J. Frontiers of Comput. 
Sci.: Selected Publications from Chinese Universities , vol. 10, no. 5, 
pp. 832 ‚Äì844, 2016. 
[15] Y. Guo, D. Tao, J. Yu, H. Xiong, Y. Li, and D. Tao, ‚ÄúDeep neural 
networks with relativity learning for facial expression recognition,‚Äù 
IEEE Int. Conf. Multimedia & Expo Workshops (ICMEW) , pp. 1 ‚Äì6, 
2016. 
[16] T. Zhang, W. Zheng, Z. Cui, Y. Zong, J. Yan, and K. Yan, ‚ÄúA deep 
neural network driven feature learning method for multi-view facial 
expression recognition,‚Äù IEEE Trans. Multimedia , vol. 18, no. 12, pp. 
2528 ‚Äì2536, 2016. 
[17] H. Jung, S. Lee, J. Yim, S. Park, and J. Kim, ‚ÄúJoint fine -tuning in 
deep neural networks for facial expression recognition,‚Äù IEEE Int. 
Conf. Comput. Vision (ICCV) , pp. 2983 ‚Äì2991, 2015. 
[18] Y. H. Byeon and K. C. Kwak, ‚ÄúFacial expression recognition using 
3D convolutional neura l network,‚Äù Int. J. Advanced Comput. Sci. and 
Appl. , vol. 5, no. 12, pp. 107‚Äì 112, 2014. 
[19] R. Zhu, G. Sang, and Q. Zhao, ‚ÄúDiscriminative feature adaptation for 
cross- domain facial expression recognition,‚Äù Int. Conf. Biometrics 
(ICB), pp. 1 ‚Äì7, 2016. 
[20] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang, ‚ÄúDomain adaptation 
via transfer component analysis,‚Äù IEEE Trans. Neural Netw. , vol. 22, 
no. 2, pp. 199 ‚Äì210, 2011. 
[21] T. M. H. Hsu, W. Y. Chen, C. A. Hou, Y. H. H. Tsai, Y. R. Yeh, and 
Y. C. F. Wang, ‚ÄúUnsupervised domain adaptation with imbalanced 
cross- domain data,‚Äú IEEE Int. Conf. Comput. Vision (ICCV) , pp. 
4121 ‚Äì4129, 2015. 
[22] W. S. Chu, F. D. la Torre, and J. F. Cohn, ‚ÄúSelective transfer 
machine for personalized facial expression analysis,‚Äù IEEE Trans. 
Pattern Anal. Mach. Intell. , vol. 6, no. 1, pp. 1 ‚Äì15, 2016. TABLE  V 
BENCHMARK COMPARISON  
Approach  Accuracy (%)   
CK+  RaFD  ADFES   
GoogLeNet  [27] 85.71  95.45  86.48   
AlexNet  [28] 85.87  95.29  84.01   
AlexNet + SVM  [12] 86.83  95.13 88.43  
CNN  [7] 80.16  94.16  87.01   
Our GM  86.83  95.78  87.37   
Our GM +  AFM  87.78  96.27  90.57   
Our GM + W-AFM  88.25  96.10  91.64   
Our GM + W-CR-AFM  89.84  96.27  92.70   
 
2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2018.2805861, IEEE Access
 
VOLUME XX, 2017 9 [23] V. Kazemi, J. Sullivan, ‚ÄúOne millisecond face alignment with an 
ensemble of regression trees,‚Äù IEEE Conf. Comput. Vision and 
Pattern Recognition (CVPR) , pp. 1867 ‚Äì1874, 2014. 
[24] J. Lu, V. E. Liong, and J. Zhou, ‚ÄúCost -sensitive local binary feature 
learning for facial age estimation,‚Äù  IEEE Trans. Image Process. , vol. 
24, no. 12, pp. 5356 ‚Äì5368, 2015. 
[25] D. E. King, ‚ÄúMax -margin object detection,‚Äù Comput. Vision and 
Pattern Recognition (cs.CV) , arXiv:1502.00046, 2015. 
[26] Y. Q. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. 
Girshick, S. Guadarrama, and T. Darrell, ‚ÄúCaffe: Convolutional 
Architecture for Fast Feature Embedding,‚Äù J. arXiv preprint 
arXiv:1408.5093 , 2014. 
[27] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. 
Erhan, V. Vanhoucke, A. Rabinovich, ‚ÄúGoing deeper with 
convolutions,‚Äù IEEE Conf. Comput. Vision and Pattern Recognition 
(CVPR) , pp. 1 ‚Äì9, 2015. 
[28] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImageNet 
classification with deep convolutional neural net works,‚Äù Advances in 
Neural Inf. Process. Syst. , pp. 1097 ‚Äì1105, 2012. 
[29] H. Sadeghi, A. A. Raie, M. R. Mohammadi, ‚ÄúFacial expression 
recognition using geometric normalization and appearance 
representation,‚Äù 8th Iranian Conf. Mach. Vision and Image Process. 
(MVIP) , pp. 159 ‚Äì163, 2013. 
[30] P. Simard, D. Steinkraus, and J. C. Platt, ‚ÄúBest practices for 
convolutional neural networks applied to visual document analysis,‚Äù 
in Seventh Int. Conf. Document Analysis and Recognition , pp. 958 ‚Äì
963, 2003. 
[31] D. C. He and L. Wang, ""Texture unit, texture spectrum, and texture 
analysis,""  IEEE Trans. Geosci. Remote Sens. , vol. 28, no. 4, pp. 509 ‚Äì
512, 1990. 
[32] Y. Ding, Q. Zhao, B. Li, and X. Yuan, ‚ÄúFacial expression recognition 
from image sequence based on LBP and Taylor expansion,‚Äù IEEE 
Acce ss, vol. 5, pp. 19409 ‚Äì19419, 2017. 
[33] B. Ryu, A. R. Rivera, J. Kim, and O. Chae, ‚ÄúLocal directional ternary 
pattern for facial expression recognition,‚Äù IEEE Trans. Image 
Process. , vol. 26, no. 12, pp. 6006‚Äì 6018, Dec. 2017. 
[34] Q. Mao, Q. Rao, Y. Yu, and M. Dong, ‚ÄúHierarchical Bayesian theme 
models for multipose facial expression recognition,‚Äù IEEE Trans. 
Multimedia , vol. 19, no. 4, pp. 861 ‚Äì873, Apr. 2017. 
[35] Z. Meng, P. Liu, J. Cai, S. Han, and Y. Tong, ‚ÄúIdentity -aware 
convolutional neural network for facial expressi on recognition,‚Äù 
IEEE Int. Conf. Automatic Face & Gesture Recognition, pp. 558 ‚Äì
565, 2017. 
[36] C. Zhang, P. Wang, K. Chen, and J. K. K√§m√§r√§inen , ‚ÄúIdentity -aware 
convolutional neural networks for facial expression recognition,‚Äù J. 
of Syst. Eng. and Electron. , vol. 28, no. 4, pp. 784-792, Aug. 2017. 
Bing-Fei Wu  (M‚Äô92 -SM‚Äô02 -F'12) received the 
B.S. and M.S. degrees in control engineering from National Chiao Tung 
University (NCTU), Hsinchu, Taiwan, in 1981 and 1983, respectively, and 
the Ph.D. degree in electrical engineering from the University of Southern 
California, Los Angeles, USA, in 1992. Since 1992, he has been with the 
Department of Electrical and Computer Engineering, where he was 
promoted to be a Professor in 1998 and Distinguished Professor in 2010, 
respectively. He serves as the Director of the Institute of Electrical and 
Control Engineering, NCTU in 2011. He founded and served as the Chair of Taipei Chapter of IEEE Systems, Man and Cybernetics Society (SMCS) 
in 2003, and is also the Chair of the Technical Committee on Intelligent 
Transportation Systems of IEEE SMCS since 2011. Currently, he is an 
Associate Editor of the IEEE Transactions on Systems, Man and 
Cybernetics: Systems and Editor- in-Chief of International Journal of 
Computer Science and Artificial Intelligence. Dr. Wu is an IEEE Fellow, 
IET Fellow and CACS Fellow. His research interests include image 
recognition, vehicle driving safety and control, intelligent robotic systems, 
intelligent transportation systems, and multimedia signal analysis. 
Dr. Wu receives many research honors, including the Outstanding 
Research Award of  Ministry of Science and Technology, Taiwan, in 2015; 
the Technology Invention Award of Y. Z. Hsu Scientific Award from Y. Z. 
Hsu Foundation in 2014; the National Invention and Creation Award of 
Ministry of Economic Affairs, Taiwan, in 2012 and 2013, respectively; the 
Outstanding Research Award of Pan Wen Yuan Foundation in 2012; the 
Best Paper Award in The 12th International Conference on ITS 
Telecommunications, 2012; the Best Technology Transfer Contribution 
Award from National Science Council, Taiwan, in 2012; and the 
Outstanding Automatic Control Engineering Award from the Chinese 
Automatic Control Society in 2007. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Chun-Hsien Lin (S‚Äô15) was born in Taipei, 
Taiwan. He has received his B.S. degree in Electrical and Computer 
Engineering in 2015 and M.S. degree in Electrical and Control 
Engineering in 2017 from National Chiao Tung University, Taiwan. 
Currently, he is earning his Ph.D. degree at NCTU. His research interests 
include computer vision, machine learning, neural networks, and control 
systems. 
 

"
https://ieeexplore.ieee.org/abstract/document/8025197,"Addressing Bias in Machine Learning Algorithms: A Pilot Study on Emotion Recognition for Intelligent Systems Ayanna Howard1*, Cha Zhang2, Eric Horvitz2 1School of Electrical and Computer Engineering Georgia Institute of Technology Atlanta, GA‚àó  2Microsoft Research Redmond, WA  Abstract: Recently, there has been an explosion of cloud-based services that enable developers to include a spectrum of recognition services, such as emotion recognition, in their applications.  The recognition of emotions is a challenging problem, and research has been done on building classifiers to recognize emotion in the open world.  Often, learned emotion models are trained on data sets that may not sufficiently represent a target population of interest. For example, many of these on-line services have focused on training and testing using a majority representation of adults and thus are tuned to the dynamics of mature faces. For applications designed to serve an older or younger age demographic, using the outputs from these pre-defined models may result in lower performance rates than when using a specialized classifier. Similar challenges with biases in performance arise in other situations where datasets in these large-scale on-line services have a non-representative ratio of the desired class of interest.  We consider the challenge of providing application developers with the power to utilize pre-constructed cloud-based services in their applications while still ensuring satisfactory performance for their unique workload of cases.  We focus on biases in emotion recognition as a representative scenario to evaluate an approach to improving recognition rates when an on-line pre-trained classifier is used for recognition of a class that may have a minority representation in the training set. We discuss a hierarchical classification approach to address this challenge and show that the average recognition rate associated with the most difficult emotion for the minority class increases by 41.5% and the overall recognition rate for all classes increases by 17.3% when using this approach. I.!INTRODUCTION Poor representation of people of different ages and skin colors in training data can lead to performance problems and biases for real-world classification tasks involving the visual attributes of people‚Äîsuch as detecting facial expressions or pose.  These performance problems and biases are directly correlated with the problem of class-imbalance in the datasets used to train these machine learning algorithms. There have been a number of efforts that have tried to resolve this issue with training on imbalanced datasets [1], including using different forms of re-sampling [2], adjusting the decision threshold [3], or a mixture-of-experts approach which combines the results of many classifiers [4]. The difficulty with imbalance is recognizing why and when imbalanced data sets are problematic. It can be difficult to distinguish between data associated with a low incidence class and noisy data when training a classifier. This is especially problematic when building models using cloud-based services that utilize training data drawn from readily available sources, such as photos crawled from the web. Tens-of-thousands of cases might be downloaded for such training, and the resulting datasets may be dominated by high prevalence age and skin color categories. Such broad scans of data lead to three challenges for application developers, including machine learning practitioners and roboticists. The first challenge is                                                              * Research performed while Visiting Researcher at Microsoft Research that, by using a cloud-based service, application developers typically cannot directly influence its behavior since they do not have access to the services‚Äô internal processes. The second challenge is that the categories which are derived from a representative quantity of majority data (and thus have likelihoods representative of real-world data streams) may lead to incorrect outcomes when a person in a minority age and skin color category uses the system. In this instance, the learning converges based on theoretically acceptable outcomes but its real-world outcomes may not be socially acceptable. The third challenge deals with the amplification of the problem when the data source is or is almost completely influenced by the dominant class, which may not be representative of the world culture, thus leading to the perpetuation of biased beliefs.  Although the derivations of these problems are different, these situations can bias the classification results, especially when designing learning algorithms for emotion recognition in intelligent systems. Emotion recognition is a growing area of interest, from interpreting moods for effective caregiver robots to recognizing a child‚Äôs anxiety level for therapeutic robots. Emotion recognition is the process of identifying human emotion, typically via facial expressions, linguistics, voice, or even body gestures. Most machine learning algorithms for emotion recognition use images to track facial expressions in 
order to identify basic emotions such as Happy, Angry, Fear, or Surprise. In fact, in a review of online emotion recognition APIs [5], over 50% of the intelligent software packages available used facial expressions to recognize emotions.   Generalization of these algorithms for optimizing performance in the open world is typically achieved by training on large sets of unconstrained data collected ‚Äòin the wild‚Äô. The term ‚Äòin the wild‚Äô means images have been captured under natural conditions with varying parameters such as environments and scenes, diverse illumination conditions, head poses and with occlusions. Unfortunately, most of these image sets, by the nature of their collection process, will have small collections of low incidence classes, such as those associated with a younger or older age demographic, or with an ethnic minority. For example, in the yearly Emotion Recognition in the Wild (EmotiW) challenge [6], researchers are provided a dataset to benchmark the performance of their methods on in-the-wild data. The dataset represents the typical expression classes of Angry, Disgust, Fear, Happiness, Neutral, Sadness and Surprise, but focuses primarily on scenes with adults. In this paper, we examine the bias issue found in learning algorithms for intelligent systems by focusing on the emotion recognition problem. We first present baseline outcomes for a cloud-based emotion recognition algorithm applied to images associated with a minority class, in this instance, children‚Äôs facial expressions. We then present a hierarchical approach that combines outputs from the cloud-based emotion recognition algorithm with a specialized learner, and show that this methodology can increase overall recognition results by 17.3%. We also verify that this hierarchical algorithm, when applied to an additional corpus of test data, shows similar improvements in the recognition rate. This additional corpus is used to assess the performance of the hierarchical approach in generalizing to new unseen images. We conclude by discussing future work needed to address the problem of bias stemming from poor representation of people in a large training set. II.!RELATED WORK Although there are a number of research efforts focused on children that incorporate the recognition of emotions to enable their functionality, most have not done a systematic analysis of accuracy with respect to their emotion recognition algorithms. For example, with socially-interactive robots, a number of research robots such as Darwin [7], the iCub [8], Avatar [9], and the GRACE robot [10], use emotions to engage children in therapy or learning. Their analysis though is based on overall performance on child engagement and not on emotion recognition. Of those research efforts that have focused on emotion recognition performance [11], very few have focused on children.  In [12], researchers used their own video data collection of children to develop methods to analyze changes in facial expressions of 50 children age 3 to 9, with a focus on problem solving scenarios. Their accuracy measures were not directly based on emotions but rather centered on Facial Action Units, which, when blended together, can be used to represent emotions. In their application, they developed separate linear support vector machines to detect the presence or absence of one of 19 facial actions. The training set consisted of 10000 image frames and the testing set consisted of 200 randomly sampled frames from this set, resulting in a recognition rate for the facial action units that ranged from 61% to 100% depending on the unit. In [13], an approach for learning children‚Äôs affective state was presented using three different types of neural network structures, namely a multi-stage radial basis function neural network, a probabilistic neural network, and a multi-class classification support vector machine (SVM).  Using the Dartmouth Database of Children Faces [14], they subdivided images of children age 5 to 9 into a training set of 1040 images and a testing set of 242 images. The training set was clustered into three affective classes: positive (Happy, Pleased, Surprised), negative (Disgust, Sad, Angry) and Neutral. They achieved a maximum overall recognition rate of 85% on the untrained facial test images using the multi-class classification SVM. In [15], a method was discussed for automatic recognition of facial expressions for children. They validated their methodology on the full Dartmouth Database of Children Faces of 1280 images and their 8 emotions classes. The full image set was used for both training and testing using a support vector machine, a C4.5 decision tree, random forest and the multi-layer perceptron method. The SVM achieved the maximum overall recognition rate of 79%. They then tested on the NIMH child emotional faces picture set (NIMH-ChEFS) database [16] with 482 images to assess the generalization accuracy of the classifiers on new unseen images and achieved a maximum overall recognition rate of 68.4% when using the SVM that was trained on the Dartmouth dataset. These represent several efforts that have focused on the explicit evaluation of emotion recognition algorithms as applied to the domain of children‚Äôs emotional cues. In the next section, we discuss baseline results derived from using a cloud-based classifier on publically available datasets of children‚Äôs facial expressions.  III.!BASELINE RESULTS  Recently, several cloud-based services have offered programming libraries that enable developers to include emotion recognition capabilities in their imaging applications [17]. These include, among others, Google‚Äôs Vision API (https://cloud.google.com/vision) and the Microsoft Emotion API (https://www.microsoft.com/cognitive-services), a component of Microsoft‚Äôs Cognitive Services. These cloud-based emotion recognition algorithms optimize their performance in the open world by training on large sets of unconstrained data sets collected ‚Äòin the wild.‚Äô To establish a baseline on the capabilities of learning and inference for a minority class, we evaluate the emotion recognition results associated with children‚Äôs facial expressions using the Microsoft Emotion API, a deep learning neural network [18]. The emotions detected using the Microsoft Emotion API are Angry, Contempt, Disgust, Fear, Happy, Neutral, Sad, and Surprise. We selected four datasets of children‚Äôs faces that are publically available for research purposes and with published  
   Neutral Contempt Surprise    Sad Disgust Fear    Fear Angry Happy Fig. 1.!Example stimuli of children associated with the facial expression databases: Top: The Radboud Faces Database, Middle: The Dartmouth Database of Children‚Äôs Faces, Bottom: NIMH-ChEFS Database human inter-rater reliability measures associated with the emotion labels. The four datasets were the NIMH Child Emotional Faces Picture Set (NIMH-ChEFS) [16], the Dartmouth Database of Children‚Äôs Faces [14], the Radboud Faces Database [19], and the Child Emotions Picture Set (CEPS) [20] (Figure 1). The NIMH Child Emotional Faces Picture Set (NIMH-ChEFS) contains 534 images of children ranging in age from 10 to 17 years old with a mean age of 13.6 years old.  The picture set includes 39 girls and 20 boys covering 5 emotions (Fear, Angry, Happy, Sad and Neutral). We selected an inter-rater reliability value for inclusion in our evaluation at 75% of the raters correctly identifying the intended emotion, which excluded 52 pictures from the original set leaving a final set of 482 pictures. Using inter-rater reliability for inclusion provides a way of quantifying the degree of agreement between two or more coders. We selected 75% as this is the benchmark established for having good agreement among raters when rating among 5-7 categories.  The Dartmouth Database of Children‚Äôs Faces contains 1280 images of children ranging in age from 5 to 16 years old with a mean age of 9.72 years old. The picture set includes 40 girls and 40 boys covering 7 emotions (Neutral, Happy, Sad, Angry, Fear, Surprise, and Disgust). For evaluation, we selected the inter-rater reliability cut-off value for inclusion at 75%, which excluded 370 pictures from the original set leaving a final set of 910 pictures. The Radboud Faces Database (RaFD) includes 240 images covering 8 emotions (Neutral, Angry, Sad, Fear, Disgust, Surprise, Happy, and Contempt).  There are 4 boys and 6 girls in the dataset. Based on a 75% inclusion criteria, we excluded 57 pictures from the original set leaving a final set of 183 pictures. Lastly is the Child Emotions Picture Set (CEPS), which contains 273 images of children, ranging in age from 6 to 11 years old with a mean age of 8.9 years old. The dataset includes 9 girls and 8 boys covering 7 emotions (Happy, Sad, Angry, Disgust, Fear, Surprise, and Neutral). Since we did not have access to the individual inter-rater reliability values associated with this dataset, we used the researcher‚Äôs inclusion critera, which resulted in a final set of 225 images. We seek to characterize the performance of existing machine learning algorithms on cases from these distinct data sets to develop an understanding of how well the deep learning neural network (i.e. the Microsoft Emotion API) performs on recognizing children‚Äôs emotional states. To evaluate the performance of the deep learning algorithm, we parsed each of the final image sets into the Emotion API and tabulated the recognition results. Table I shows the performance for each of the datasets. TABLE I. !DEEP LEARNING RECOGNITION RATES ACROSS THE DIFFERENT STIMULI SETS (IN %): (FE)AR, (AN)GRY, (HA)PPY, (SA)D, (NE)UTRAL, (SU)RPRISED, (DI)SGUST, (CO)NTEMPT  Fe An Di Ha Ne Sa Su Co NIMH-ChEFS  13 43  100 100 48   Dartmouth 25 35 55 100 99 64 91  Radboud 33 54 100 100 100 95 100 50 CEPS 5 50 10 95 92 52 81    With respect to the overall recognition rates, which incorporate the variable data dimensions of the image sets (Table II), the overall emotion recognition rate was 62% for the NIMH-ChEFS dataset, 81% for the Dartmouth dataset, 83% for the Radboud dataset, and 61% for the CEPS dataset. If we compare these results with specialized learning algorithms that have been trained specifically on children‚Äôs facial images and as discussed in the related work section, we see that the deep learning algorithm, based on training in the wild, has results that are comparable to the specialized results that used comparatively smaller training sets but with an emphasis on a minority class (Table III). Yet, if we look at the overall emotion recognition rates for adults, the rates should be closer to 88% [30]. Our goal therefore is to capitalize on the power of the cloud-based emotion recognition algorithm while improving the overall recognition rate.  TABLE II. !NUMBER OF IMAGES ASSOCIATED WITH EACH STIMULI SET: (FE)AR, (AN)GRY, (HA)PPY, (SA)D, (NE)UTRAL, (SU)RPRISED, (DI)SGUST, (CO)NTEMPT   Fe An Di Ha Ne Sa Su Co NIMH-ChEFS  102 94   108 98 80     Dartmouth 20 83 101 302 145 135 124   Radboud 21 26 25 30 24 20 29 8 CEPS 20 30 31 56 24 33 31     

TABLE III. !DEEP LEARNING ‚ÄòIN THE WILD‚Äô APPROACH VERSUS SPECIALIZED LEARNING METHODOLOGIES  Dartmouth Database  with emotions grouped into positive, negative, and neutral affective states Albu [13] 85% Emotion API 84%  Dartmouth Database NIMH-ChEFS Khan [15] 79% 68% Emotion API 81% 62% III.!METHODOLOGY AND RESULTS When we examine the results from the deep learning neural network, we note that Fear has a significantly lower recognition rate than the other emotions across all of the different datasets. If we look at the confusion matrix associated with Fear (Table IV), we also note that Fear is most often confused with Surprise. Facial expressions of Fear and Disgust have repeatedly been found in the emotion recognition literature to be less well recognized than those of other basic emotions [21]. In fact, it has been shown that Surprise is the most frequent error made when trying to recognize expressions of Fear in children [22]. If we look at the basic facial action units that make up these two expressions, it becomes obvious why this confusion occurs. Facial Action Units (AUs) represent the 44 anatomically distinct muscular activities that activate when changes occur in an individual‚Äôs facial appearance (Table V). Based on comprehensive comparative human studies, Ekman and Friesen [23, 24] have labeled these muscle movements and identified those believed to be associated with emotional expressions (Table VI) [25]. When examining the facial action units associated with Fear and Surprise (Table V), we confirm that Surprise is actually a subset of Fear (i.e. Surprise ‚äÜ  Fear); and over 60% of the AUs in Fear are also found in the set of Surprise AUs.  As such, Surprise becomes easier to recognize than Fear as a default since there are less distinctive cues to identify. TABLE IV. !CONFUSION MATRIX ASSOCIATED WITH DEEP LEARNING RESULTS   Surprise Neutral Happy Sad Fear Fear      NIMH-ChEFS 74 (83.7%) 11 (10.2%) 0 3 13 (6.1%) Dartmouth 11    (55%) 0 3    (15%) 1      (5%) 5  (25%) Radboud 13 (61.9%) 0 0 1   (4.8%) 7 (33.3%) CEPS 9      (45%) 4     (20%) 3    (15%) 3    (15%) 1    (5%) Surprise      Dartmouth 113 (91.1%) 3    (2.4%) 8   (6.5%) 0 0 Radboud 29  (100%) 0 0 0 0 CEPS 25 (80.6%) 3    (9.7%) 3   (9.7%) 0 0 TABLE V. !FACIAL ACTION UNITS INVOLVED IN EMOTION STIMULI Emotion AUs associated with Emotion Angry 4, 5 and/or 7,  22, 23, 24 Fear 1, 2, 4, 5, 7, 20, 25 or 26 Surprise 1, 2, 5, 25 or 26  TABLE VI. !FACIAL ACTION UNITS AND FACIAL FEATURE IMAGES FROM THE CHILDREN FACIAL EXPRESSION DATASETS Action Unit Description Facial Feature Image 1 Inner Brow Raiser  2 Outer Brow Raiser  4 Brow Lowerer  5 Upper Lid Raiser  7 Lid Tightener  20 Lip Stretcher  22 Lip Funneler  23 Lip Tightener  24 Lip Pressor  25 Lips Apart  26 Jaw Drop   Thus, as a first step in illustrating a process for improving the recognition rates of a generalized machine learning algorithm, we focus on improving the overall recognition rates by improving recognition of the Fear emotion. Figure 2 depicts the overall algorithmic flow of the approach. Given a facial image, facial landmarks are extracted and used to compute a number of anthropometric features. The anthropometric features are then fed into two Support Vector Machines (SVMs) for binary classification, one to distinguish between Fear and Surprise with an explicit bias toward Fear and one to distinguish between Surprise and Not-Surprise with a balanced bias. The design of this construct is to increase the bias toward the minority class (in the first SVM) while ensuring that the recognition of the majority class is not drastically reduced (in the second SVM). We train the SVMs on 50% of the data from three of the datasets of children‚Äôs faces and evaluate the results on all four datasets, including the remaining untrained dataset.  

 Fig. 2.!Feature-based learning approach for emotion recognition of children‚Äôs facial expressions A. Extraction of Anthropometric Features Most methods that focus on developing image-based age classification methods typically use features associated with face anthropometry [26].  A face anthropometric model is based on measurements of size and proportions of the human face, i.e. human face ratios. Although age classification is not our direct target application, it does provide some measure for distinguishing between age groups (i.e. children versus adults) based on facial features. We thus utilize various human face ratios as the input into our specialized learning algorithm. In [27], it was shown that four feature distances are sufficient to represent the mathematical relationships among all of the various landmarks for age classification. Since we are interested in emotion classification, we compute all principal ratios, namely: Width of Left Eye, Height of Left Eye, Length of the Nose Bridge, Distance between the Nose Tip and Chin, Width of Mouth, Height of Open Mouth, and Offset Distance between the Inner and Outer Corner of the Eyebrow. These anthropometric features are computed based on extracted face landmarks, as shown in Figure 3, and Equations (1)-(7). ""#$%‚Ñé'()*(%+,*=|+,*)*(%/00*12‚àí+,*)*(%45%*12|   (1) 6'7*81#$9*)*09%‚Ñé=|6'7*:''%:#9‚Ñé%2‚àí6'7*:''%)*(%2|   (2) 6'7*;#<%'=‚Ñé#0=|=‚Ñé#0>'7#%#'0?@‚àí?6'7*;#<@|   (3) A*#9‚Ñé%'()*(%+,*=|+,*)*(%8'%%'B@‚àí+,*)*(%;'<@|  (4)                                                              1 !https://www.projectoxford.ai/face ""#$%‚Ñé'(C'5%‚Ñé=|C'5%‚Ñé:#9‚Ñé%2‚àíC'5%‚Ñé)*(%2|  (5) A*#9‚Ñé%'(4<*0C'5%‚Ñé=|D<<*1)#<8'%%'B@‚àíD0$*1)#<;'<@|  (6) +,*81'EA*#9‚Ñé%4((7*%=|+,*F1'E)*(%/00*1@‚àí   ??+,*F1'E)*(%45%*1@|?? (7) Where x and y represent the pixel location in (x,y) screen coordinates and ChinPosition is estimated as the pixel coordinates associated with the bottom center of the Face Rectangle provided by the Face API, which indicates where in the image a face is located. Once computed, all ratios are normalized based on the calculated width and height of the Face Rectangle.                 Fig. 3.!Face Landmarks extracted using the Face API developed by Microsoft Oxford Project1 Once computed, these features are used to train two SVMs for emotion classification. B. SVMs for Classification of Fear versus Surprised Given that there is a bias for Surprise versus Fear associated with children‚Äôs facial expressions, our first task was to develop a specialized classifier that biases the results toward the minority class, in this case, the Fear class. Support Vector Machines (SVMs) are supervised learning models that can be used for classification of classes associated with a set of training examples [28]. For our application, we want to better differentiate between the Fear minority class and the Surprise majority class. Thus, any facial expression that was classified (correctly or incorrectly) as Surprised by the deep learning algorithm, we want to re-evaluate with a specialized learner. To enable this process, all emotions labeled as Surprise are fed to the first-level SVM and reclassified into one of two classes: Fear or Surprise. We thus design the first-level SVM to learn the mapping: G‚Ü¶I, where J‚àä:L,,‚àä¬±1,0=7   (8) In this case, x represents the vector containing anthropometric features, y=1 represents the Surprise class, and y=-1 represents the Fear class.  

For our application, we trained the first-level SVM on 50% of the feature vectors classified as Fear or Surprise by the deep learning algorithm and extracted from the Radboud Faces Database, the Dartmouth Database of Children‚Äôs Faces and the NIMH-ChEFS Database.  We did not train on the Child Emotions Picture Set as we wished to assess the capabilities of the new algorithm when faced with unseen facial characteristics. We then biased the class decision threshold of the first-level SVM by selecting the minimum threshold value that maximized the true positive rate associated with Fear.  This, by default, increases the false positive rate of Fear while potentially reducing the true positive rate associated with Surprise. Thus, after parsing the images through the first-level SVM, the minority class has a significantly higher recognition rate but the majority class recognition rate, on average, is reduced as shown in Table VII. The higher recognition rates for the minority class are valid even for the CEPS database which contains data that was not in the training sets of of the SVM classifiers. All recognition rates incorporate the variable data dimensions of the image sets (Table II). TABLE VII. !EMOTION RECOGNITION RATES AFTER TRAINING: ML ‚Äì DEEP LEARNING ALGORITHM, SVM ‚Äì FIRST-LEVEL SUPPORT VECTOR MACHINE  Fear Surprise Change in Overall Rec. Rate    ML ML+ SVM ML ML+ SVM NIMH-ChEFS  13% 47%   34% Dartmouth 25% 91% 91% 79% -1.2% Radboud 33% 77% 100% 100% 31.8% CEPS 5% 70% 81% 68% 17.6%  The goal of the second-level SVM is to increase the recognition rate of the majority class to pre-bias levels while still keeping the recognition rate associated with the minority class higher than its original recognition rate. From Table V, we note that Angry and Fear have more Action Units in common than Angry and Surprise. Thus, to reduce the effect of the Action Unit overlap between Surprise and Fear, we trained the second-level SVM on the recognition of two primary classes ‚Äì (Fear ‚à® Angry) and Surprise. We then associated the derived anthropometric feature vector from each image to one of two classes: Surprised and Not-Surprised, where Not-Surprised represented the union of Fear and Angry. In this case, for the mapping: G‚Ü¶I, where J‚àä:L,,‚àä¬±1,0=7, y=1 is associated with the Surprise class, and y=-1 is associated with the Fear or Angry class. In practice, only those feature vectors that were classified as Fear by the first-level SVM are processed by the second-level SVM. This approach results in an average increase in the recognition of Fear by 41.5% and an increase in the overall recognition rate by 17.3%.   As Table VIII shows, the overall recognition rate increases after parsing through the second-level SVM, even though the recognition rate for the minority class falls to a lower value than in the first-level SVM.  Of special interest, we note that, although recognition of Fear has increased greatly for the Dartmouth database, the recognition rate for Surprise is slightly lower than the original Surprise recognition rate, even after parsing through the second-level SVM. Of all the datasets, the Dartmouth dataset had a large imbalance between Surprise and Fear, in fact this dataset had 6x more images belonging to Surprise than Fear (Table II). As such, this result is not surprising as the balance that we are trying to achieve in our approach is to ensure that the minority class has an increase in benefit with respect to recognition rates, while ensuring that the reduction in benefits to the majority class is not major. If the motivation is, instead, to ensure that the recognition rate for the minority class is maximized, the only requirement is to ignore the second-level SVM and utilize only the output resulting from parsing through the first-level SVM. In the next section, we make some interesting observations about these results and provide discussion on ways to generalize this approach to the broad class of generalized learning algorithms. TABLE VIII. !EMOTION RECOGNITION RATES AFTER TRAINING: ML ‚Äì DEEP LEARNING ALGORITHM, SVM ‚Äì SECOND-LEVEL SUPPORT VECTOR MACHINE  Fear Surprise Change in Overall Rec. Rate    ML ML+ SVM ML ML+ SVM NIMH-ChEFS  13% 47%   34% Dartmouth 25% 70% 91% 83% -0.6% Radboud 33% 71% 100% 100% 32.8% CEPS 5% 55% 81% 81% 19.6%  IV.!DISCUSSION  We conclude this paper with a discussion on the presented results and highlight some areas for future efforts that could address the limitations associated with building classifiers when there is imbalanced representation in their training sets.  Recently, there has been an upsurge of attention given to generalized machine learning algorithms and the practices of inequality and discrimination that are potentially being built into them [29]. We know that imbalances exist and thus, our goal in this paper is to present an approach that enables us to capitalize on the power of generalized learning algorithms, while incorporating a process that allows us to tune those results for different target demographics.  Bias in machine learning algorithms will occur anytime there is a large majority class coupled with other minority classes having lower incidence rates, such as those associated with a younger or older age demographic, or an ethnic minority. The challenge is to develop a process for ensuring the overall positive results of the generalized learning approach is maintained, while also increasing the outcomes associated with any minority classes. In this paper, we address this issue by developing a hierarchical approach that couples the results from the generalized learning algorithm with results from a specialized learner. Although we focus on the issue of emotion recognition for intelligent systems, and address emotion 
recognition associated with children‚Äôs facial expressions, this concept can be applied to similar classification applications. The steps involved are (1) identifying the set(s) of minority classes, (2) developing specialized learners that address the minority class via special focus on the class, and (3) developing a specialized learner that combines signals from both the minority and majority class models. As shown in the results, if at any point, we determine that it is more important to have a maximum outcome rate associated with the minority class, regardless of the outcome rate associated with the majority class, only steps (1) and (2) are necessary. That question on the inclusiveness of a classifier touches on ethics of equity in the performance of  algorithms. Although the presented approach shows validity in addressing the issue of bias, there are still a number of threads that need to be investigated. Future work in this domain includes validating the approach with a focus on a different minority class, validating the approach with a focus on a different classification problem, and validating the approach with different generalized machine learning algorithms. We will also target improving the classification rate of both Fear and Disgust, since both of these expressions are hard to detect, and would provide further evidence of the impact of this methodology. We hope that this work will contribute to raising the sensitivity to the potential challenges in the performance and bias of classifiers when making inferences about people of different ages and skin colors. There are opportunities  for additional research to identify and address these challenges. V.!REFERENCES [1]!Kotsiantis, S., Kanellopoulos, D. and Pintelas, P. ‚ÄúHandling imbalanced datasets: A review,‚Äù GESTS International Transactions on Computer Science and Engineering, Vol. 30, pp. 25-36, 2006. [2]!Chawla, N.V., Hall, L. O., Bowyer, K. W. and Kegelmeyer, W. P., ‚ÄúSMOTE: Synthetic Minority Oversampling Technique,‚Äù Journal of Artificial Intelligence Research, Vol. 16, pp. 321‚Äì357, 2002. [3]!Joshi, M. V., Kumar, V. and Agarwal, R.C. ‚ÄúEvaluating boosting algorithms to classify rare cases: comparison and improvements,‚Äù In First IEEE International Conference on Data Mining, pp. 257-264, 2001. [4]!Provost, F. and Fawcett, T. ‚ÄúRobust classification for imprecise environments,‚Äù Machine Learning, Vol. 42, pp. 203-231, 2001.  [5]!Doerrfeld, B. ‚Äú20+ Emotion Recognition APIs That Will Leave You Impressed, and Concerned,‚Äù http://nordicapis.com/20-emotion-recognition-apis-that-will-leave-you-impressed-and-concerned, 2015.  [6]!Dhall, A., Ramana Murthy O.V., Goecke, R., Joshi J. and Gedeon, T. ‚ÄúVideo and Image based Emotion Recognition Challenges in the Wild: EmotiW 2015,‚Äù ACM International Conference on Multimodal Interaction (ICMI), 2015. [7]!Brown, L. and Howard A. ‚ÄúGestural Behavioral Implementation on a Humanoid Robotic Platform for Effective Social Interaction,‚Äù IEEE Int. Symp. on Robot and Human Interactive Communication (RO-MAN), pp. 471 ‚Äì 476, 2014. [8]!Metta, G., Sandini, G., Vernon, D. Natale, L. and Nori, F. ‚ÄúThe iCub humanoid robot: an open platform for research in embodied cognition,‚Äù 8th workshop on performance metrics for intelligent systems, pp. 50-56, 2008. [9]!Cloutier, P., Park, H.W., MacCalla, J. and Howard, A. ‚ÄúIt‚Äôs All in the Eyes: Designing Facial Expressions for an Interactive Robot Therapy Coach for Children,‚Äù 8th Cambridge Workshop on Universal Access and Assistive Technology, Cambridge, UK, 2016. [10]!Simmons R., et al. ‚ÄúGRACE: An Autonomous Robot for the AAAI Robot Challenge,‚Äù AI Magazine, Vol. 24(2), pp. 51-72, 2003. [11]!Sankur, B., Ulukaya, S. and √áeliktutan, O. ‚ÄúA Comparative Study of Face Landmarking Techniques,‚Äù EURASIP J. Image and Video Processing, Vol. 13, 2013. [12]!Littleworth, G., Bartlett, M.S., Salamanca, L.P. and Reilly, J. ‚ÄúAutomated Measurement of Children‚Äôs Facial Expressions during Problem Solving Tasks,‚Äù IEEE Int. Conference on Automatic Face and Gesture Recognition, pp. 30‚Äì35, 2011. [13]!Albu, F., Hagiescu, D., Vladutu, L. and Puica, M. ‚ÄúNeural network approaches for children's emotion recognition in intelligent learning applications,‚Äù in Proc. of EDULEARN 2015, Barcelona, Spain, pp. 3229-3239, 2015. [14]!Dalrymple, KA, Gomez, J, and Duchaine, B. ‚ÄúThe Dartmouth Database of Children‚Äôs Faces: Acquisition and Validation of a New Face Stimulus Set,‚Äù Urgesi C, ed.PLoS ONE. 2013. Vol. 8(11), 2013. [15]!Khan, R.A., Meyer, A. and Bouakaz, S. ‚ÄúAutomatic Affect Analysis: From Children to Adults,‚Äù International Symposium on Visual Computing, ISVC 2015, pp. 304-313, 2015. [16]!Egger, H.L., Pine, D.S., Nelson, E., et al. ‚ÄúThe NIMH Child Emotional Faces Picture Set (NIMH-ChEFS): A new set of children‚Äôs facial emotion stimuli,‚Äù International Journal of Methods in Psychiatric Research, Vol. 20(3), pp. 145-156, 2011. [17]!Schmidt, A. ‚ÄúCloud-Based AI for Pervasive Applications,‚Äù IEEE Pervasive Computing, Vol. 15(1), pp. 14-18, 2016. [18]!Barsoum, E., Zhang, C., Canton Ferrer, C. and Zhang, Z. ‚ÄúTraining Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution,‚Äù ACM International Conference on Multimodal Interaction (ICMI), Tokyo, Japan, 2016. [19]!Langner, O., Dotsch, R., Bijlstra, G., Wigboldus, D.H.J., et. al. ‚ÄúPresentation and validation of the Radboud Faces Database,‚Äù Cognition & Emotion, Vol. 4(8), pp. 1377‚Äî1388, 2010. [20]!Romani-Sponchiado, A., Sanvicente-Vieira, B., Mottin, C., Hertzog-Fonini, D., Arteche, A. ‚ÄúChild Emotions Picture Set (CEPS): Development of a database of children‚Äôs emotional expressions,‚Äù Psychology & Neuroscience, Vol. 8(4), pp. 467-478, 2015. [21]!Gagnon, M., Gosselin P., Hudon-ven der Buhs, I., Larocque, K., Milliard, K. ‚ÄúChildren‚Äôs recognition and discrimination of Fear and Disgust facial expressions,‚Äù Journal of Nonverbal Behavior, Vol. 34(1), pp. 27‚Äì42, 2010. [22]!Gosselin, P., Roberge, P. and Lavalle¬¥e, M. C. ‚ÄúThe development of the recognition of facial emotional expressions comprised in the human repertoire,‚Äù Enfance, Vol. 4, pp. 379‚Äì396, 1995.  [23]!Ekman, P. and Friesen, W. Facial action coding system: A technique for the measurement of facial movement. Palo Alto, Ca.: Consulting Psychologists Press, 1978. [24]!Friesen, W. and Ekman, P. EMFACS-7: Emotional Facial Action Coding System. Unpublished manual, University of California, California, 1983. [25]!Matsumoto, D. and Ekman, P. ‚ÄúFacial expression analysis,‚Äù Scholarpedia, Vol. 3(5), pp. 4237, 2008. [26]!Grd, P. ‚ÄúTwo-dimensional face image classification for distinguishing children from adults based on anthropometry,‚Äù Thesis submitted to University of Zagreb, 2015. [27]!Alom M.Z., Piao, M-L., Islam M.S., Kim, N. and Park, J-H. ‚ÄúOptimized Facial Features-based Age Classification,‚Äù World Academy of Science. Engineering and Technology Conference, Vol. 6, pp. 319‚Äì324, 2012. [28]!Joachims, T. ‚ÄúMaking Large-Scale SVM Learning Practical. Advances in Kernel Methods - Support Vector Learning,‚Äù B. Sch√∂lkopf and C. Burges and A. Smola (ed.), MIT-Press, 1999.  [29]!Crawford, K. ‚ÄúArtificial Intelligence‚Äôs White Guy Problem,‚Äù New York Times ‚Äì Opinion, http://www.nytimes.com/2016/06/26/ opinion/sunday/artificial-intelligences-white-guy-problem.html, 2016. [30]!Brodny, G., Ko≈Çakowska, A., Landowska, A., Szwoch, M., Szwoch, W. and Wr√≥bel, M. ‚ÄúComparison of selected off-the-shelf solutions for emotion recognition based on facial expressions,‚Äù 9th Int. Conf. on Human System Interactions (HSI), pp. 397-404, 2016.  
"
https://ieeexplore.ieee.org/document/8014982/,"Aff-Wild: Valence and Arousal ‚Äòin-the-wild‚Äô Challenge
Stefanos Zafeiriou‚ãÜ,3Dimitrios Kollias‚ãÜ‚àóMihalis A. Nicolaou‚Ä†
Athanasios Papaioannou‚ãÜGuoying Zhao3
Irene Kotsia1,2
‚ãÜDepartment of Computing, Imperial College London, UK
‚Ä†Department of Computing, Goldsmiths, University of London, UK
3Center for Machine Vision and Signal Analysis, University of Oulu, Finland
1School of Science and Technology, International Hellenic University, Greece
2Department of Computer Science, Middlesex University, UK
‚ãÜ{s.zafeiriou, dimitrios.kollias15 }@imperial.ac.uk,‚Ä†m.nicolaou@gold.ac.uk
Abstract
The Affect-in-the-Wild (Aff-Wild) Challenge proposes a
new comprehensive benchmark for assessing the perfor-
mance of facial affect/behaviour analysis/understanding
‚Äòin-the-wild‚Äô. The Aff-wild benchmark contains about 300
videos (over 2,000 minutes of data) annotated with re-
gards to valence and arousal, all captured ‚Äòin-the-wild‚Äô (the
main source being Youtube videos). The paper presents the
database description, the experimental set up, the baseline
method used for the Challenge and Ô¨Ånally the summary of
the performance of the different methods submitted to the
Affect-in-the-Wild Challenge for V alence and Arousal esti-
mation. The challenge demonstrates that meticulously de-
signed deep neural networks can achieve very good perfor-
mance when trained with in-the-wild data.
1. Introduction
Behavioral modeling and analysis constitute a crucial as-
pect of Human Computer Interaction. Emotion recognition
is a key issue, dealing with multimodal patterns, such as
facial expressions, head pose, hand and body gestures, lin-
guistic and paralinguistic acoustic cues, as well as phys-
iological data. However, generating machines which are
able to recognize human emotions is a difÔ¨Åcult problem, be-
cause the emotion patterns are complex, time-varying, user
and context dependent, especially when considering uncon-
trolled environments, i.e., ‚Äòin-the-wild‚Äô.
Current research in automatic analysis of facial affect
aims at developing systems, such as robots and virtual hu-
mans, that will interact with humans in a naturalistic way
‚àóThe Ô¨Årst two authors contributed equally in the paper.under real-world settings. To this end, such systems should
automatically sense and interpret facial signals relevant to
emotions, appraisals and intentions. Furthermore, since
real-world settings entail uncontrolled conditions, where
subjects operate in a diversity of contexts and environments,
systems that perform automatic human behaviour analysis
should be robust to video recording conditions, the diver-
sity of contexts and the timing of display.
The past twenty years research in automatic analysis of
facial behaviour was mainly limited to posed behavior cap-
tured in highly controlled recording conditions [ 29,35,33,
24]. Some representative datasets, which are still used in
many recent works [ 18], include the Cohn-Kanade database
[33,24], the MMI database [ 29,35], the Multi-PIE database
[17] and the BU-3D and BU-4D databases [ 41,40]. Nev-
ertheless, it is now accepted by the community that the
facial expressions of naturalistic behaviour could be radi-
cally different from the posed ones [ 9,32,44]. Hence, ef-
forts have been made in order to collect subjects displaying
naturalistic behaviour. Examples include the recently col-
lected EmoPain [ 4] and UNBC-McMaster [ 27] for analy-
sis of pain, the RU-FACS database consisting of subjects
participating in a false opinion scenario [ 5] and the SE-
MAINE [ 27] corpus which contains recordings of subjects
interacting with a Sensitive ArtiÔ¨Åcial Listener (SAL) under
controlled conditions. All the above databases have been
captured in well-controlled recording conditions and mainly
under a strictly deÔ¨Åned scenario (e.g., pain estimation).
Representing human emotions has been a basic topic of
research in psychology. The most frequently used emotion
representation is the categorical one, including the seven ba-
sic categories, i.e., Anger, Disgust, Fear, Happiness, Sad-
ness, Surprise and Neutral [ 13][10]. It is, however, the di-
mensional emotion representation [ 39,31] which is more
2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops
2160-7516/17 $31.00 ¬© 2017 IEEE
DOI 10.1109/CVPRW.2017.2481980

Figure 1: The 2-D Emotion Wheel
appropriate to represent subtle, i.e., not only extreme, emo-
tions appearing in everyday human computer interactions.
The 2-D V alence and Arousal Space is the most usual di-
mensional emotion representation. Figure 1shows the 2-D
Emotion Wheel [ 30], with valence ranging from very pos-
itive to very negative and arousal ranging from very active
to very passive.
There are various signs of a humans emotions, such as
facial expressions, gestures, paralinguistic speech features
and physiological measurements. In the Challenge we fo-
cus on facial affect as measured by valence and arousal an-
notations. In particular, we make a considerable effort to
go beyond the current practices in facial behaviour analy-
sis and collect and annotate the Ô¨Årst large scale in-the-wild
database of facial affect1. To achieve this, we capitalise
on the abundance of data available in video-sharing web-
sites, such as Y ouTube [ 42]2, and select videos that display
the affective behavior of people, for example videos that
display the behaviour of people when watching a trailer,
a movie, a disturbing clip or reactions to pranks etc. To
this end we have collected 298 videos displaying reactions
of 200 subjects. To the best of our knowledge this is the
largest database containing videos of facial behaviour ‚Äùin-
the-wild‚Äù. For a recent survey on facial behaviour analysis
in-the-wild with an emphasis on deep learning methodolo-
gies the interested reader may refer to [ 43]. This database
has been annotated by 6-8 lay experts with regards to two
continuous emotion dimensions, i.e. valence, which records
1Currently, there are many challenges in behaviour analysis, including
the series of A VEC [ 37,36,34] challenges. Nevertheless, A VEC uses only
data captured in controlled conditions and under very speciÔ¨Åc scenarios.
The only challenge that uses ‚Äòin-the-wild‚Äô data is the series of [ 15,14,16].
Nevertheless, the samples come from movies and the annotation is limited
to the universal expressions.
2The collection has been conducted under the scrutiny and approval of
Imperial College Ethical Committee (ICREC). The majority of the chosen
videos were under Creative Commons License (CCL). For those videos
that were not under CCL, we have contacted the person who created them
and asked for their approval to be used in this research.how positive or negative an emotion is, and arousal which
measures the power of the activation of the emotion.
In the rest of the paper, we Ô¨Årst describe the generated
Aff-Wild database (Section 2), afterwards we describe the
annotation procedure (Section 3) and then we present the
results of the challenge (Section 3). Subsequently, in Sec-
tion 4.1 we make a reference to all methods and respec-
tive papers submitted to the Challenge, summarize the ob-
tained results in valence and arousal estimation and declare
the winning method. Finally Conclusions are presented in
Section 5.
2. The Aff-Wild Database
We created a database consisting of 298 videos, with a
total length of more than 30 hours. The aim was to col-
lect spontaneous facial behaviors under arbitrary recording
conditions. To this end the videos were collected using the
Y outube video sharing web-site. The keyword that was used
to retrieve the videos was ‚Äùreaction‚Äù; they display subjects
reacting to a variety of stimuli (e.g., tasting something hot or
disgusting). Examples include subjects reacting on an un-
expected plot twist of a movie or series, a trailer of a highly
anticipated movie, etc. The subjects display both positive
and negative emotions (or combinations of them). In other
cases, subjects display emotions while performing an activ-
ity (e.g., riding a rolling coaster). In some videos, subjects
react on a practical joke, or on positive surprises (e.g., a
gift). Most of the videos were in YUV 4:2:0 format, with
some of them being in A VI format; all have been annotated
in terms of valence and arousal. Six to eight subjects have
annotated the videos following a methodology similar to the
one proposed in [ 11]. That is, an on line annotation proce-
dure was used, according to which annotators were watch-
ing each video and provided their annotations through a joy-
stick. V alence and arousal ranged continuously in [ ‚àí1,+1].
We have annotated all subjects that are present in a video.
In total we have 200 subjects, with 130 of them being male
and 70 of them female. Figures 2and 3demonstrate some
frames of the Aff-Wild database.
In Figures 4,5we present two characteristic examples of
facial images, cropped from two different videos, with their
respective video frame number and the valence and arousal
annotation for each of them. We also present a visual repre-
sentation of these values on the 2-D emotion space, showing
the change of the reactions/behavior of the person among
these time instances of the video. Time evolution is indi-
cated, by using a larger size for the more recent frames and
a smaller size for the older ones.
It can be veriÔ¨Åed that annotations correspond well to the
facial expression displaying in the video frames. It should,
however, be added that it is often difÔ¨Åcult to say which is the
true emotional state of the acting person from a static frame.
That is why, working with many annotators and selecting
1981
Figure 2: Some representative frames from the Aff-Wild database.
Figure 3: Some challenging frames from the Aff-Wild database.
the ones that are more consistent between them is necessary
to get more accurate annotation of the underlying emotion.
Table 1: Number of Subjects in the Aff-Wild Database
Database no of males no of females
Train 106 48
Test 24 22
Table 2: Attributes of the Aff-Wild Database
Attribute Description
Length of videos 0.10-14.47 min
No of annotators 6-8
Total no of videos 252(train)+46(test) = 298
Video format A VI , MP4
3. Annotation and data processing
3.1. Annotation tool
For data annotation, we developed our own application
which was similar to others like Feeltrace [ 11] and Gtrace
[12]. In our application we used a setting with one time-
continuous annotation for each affective dimension, like in
Gtrace. We did not want to judge valence and arousal at the
same time, like in Feeltrace, because it would be too cogni-
tively demanding to reach a high quality on both. The user
at Ô¨Årst selects if (s)he wants to annotate valence or arousal.
Then, the interface of our application asks the user to log
in using an identiÔ¨Åer, his/her name, and to select an appro-
priate joystick. After that, the screen is split into two parts:a scrolling list of all videos is given on the left side and on
the right side there is a scrolling list of all annotated videos.
After one selects a video to annotate, a screen appears that
shows the video and a slider of values ranging in [‚àí1,1].
Then the video can be annotated by moving the joystick ei-
ther up or down. At the same time our application samples
the annotations at a variable time rate. Figure 6shows the
graphical interface of our tool when annotating valence (the
tool for arousal is similar).
3.2. Annotation guidelines
Each annotator was instructed orally and received in-
structions through a multi page document, explaining in de-
tail the procedure to follow for the annotation task. This
document included a short list of some well identiÔ¨Åed emo-
tional cues for both arousal and valence, in order to provide
a common introduction on emotions to the annotators, even
though they were rather instructed to use their own feeling
for the annotation task3. Before starting the annotation of
the data, each annotator watched the whole video so as to
know what to expect regarding all emotions being depicted
in the video.
3.3. Data pre-processing
VirtualDub [ 22] was used in order to trim the raw
Y ouTube videos, mainly at the start and the end of them,
so as to remove useless content (e.g., an advertisement).
Then another pre-processing step was applied in order to
locate the faces in all frames of the videos. In more detail,
3All annotators were computer scientists who were working on face
analysis problems and all had a working understanding of facial expres-
sions.
1982
Figure 4: Annotated Facial Expressions (Person A)
Figure 5: Annotated Facial Expressions (Person B)
we extracted a total of 1,180,000 frames using the Menpo
software [ 2]. From each frame, we detected the faces using
the method described in [ 26]. We also developed a match-
ing process between the annotation time stamps and the
cropped faces time instances. In particular, for each frame
time instance, we searched for the nearest neighbor, in the
annotation time stamp sequence and then linked the latter
valence and arousal annotation values to the corresponding
frame time stamp. In cases where we had two annotation
timestamps with same time distance from a frame, we com-
puted the average of those two timestamps and attributed
this value to that frame. Finally, we extracted facial land-
marks for all frames using the best performing method in
[8].
Figure 6: The GUI of the annotation tool when annotating
valence (the GUI for arousal is exactly the same).
3.4. Annotation Post-processing
We further extended our annotation tool so that it plays a
speciÔ¨Åc video and at the same time plots the correspond-
ing valence and arousal annotated values. Every expert-
annotator (i.e., the annotators that are working directly on
the problem of valence and arousal estimation) watched
again all videos and checked if all annotations were in ac-
cordance with the videos, depicting well the emotion ex-
pressed at all times at the videos. In this way, a further
validation of annotations was achieved. Using this proce-
dure some frames were dropped, especially at the end of
the videos. After the annotations have been validated, we
computed, for every video, cross-correlations between all
annotators. Furthermore, we computed the correlation be-
tween the annotation and the tracked facial landmarks. As a
consequence, we ranked the annotators‚Äô correlation for each
video. Two more experts then watched all videos and, for
every video, selected the most correlated best annotations
(between 2 to 4 annotations). We then computed the mean
of these annotations; in other words we selected the mean
of annotations that were mostly correlated and were given
good evaluation by the experts. Figure 7shows a small part
of a video (2000 frames) and the 4 most highly correlated
annotations for valence.
Figure 8provides a histogram for the annotated values
for valence and arousal in the generated database. As it can
be observed the annotation is currently biased towards pos-
itive valence and arousal (something we aim at addressing
in future runs of the challenge).
4. Experiments with a Baseline
Due to the fact that it was the Ô¨Årst time that estimation
of valence and arousal was attempted in in-the-wild videos
the standards approaches, e.g. using Support V ector Re-
gression (SVR) etc. on image features [ 28], resulted in
very poor performance. Hence, we implemented a deep
neural network approach as the baseline. Currently deep
1983
Figure 7: The 4 most highly correlated valence annotations
over a part of a video
Figure 8: Histogram of Annotations
neural networks provide the state-of-the-art in many tasks
in computer vision, speech analysis and natural language
processing that require learning from massive data. They
have also achieved good performances in emotion recogni-
tion challenges and contests [ 15]. Deep Convolutional Neu-
ral Networks (CNNs) [ 20][21] include convolutional layers
with feature maps composed of neurons with local receptive
Ô¨Åelds, shared weights and pooling layers, which are able to
automatically extract non-linear features. The baseline ar-
chitecture was based on the structure of the CNN-M [ 7] net-
work. We used the pre-trained on the FaceV alue dataset [ 3]
CNN-M network as starting structure and performed trans-
fer learning of its convolutional and pooling parts on our de-
signed network. In particular, we used two fully connected
layers, the second being the output layer providing the va-lence and arousal predictions. We either froze the CNN part
of the network and performed Ô¨Åne-tuning of the weights of
the fully connected layers, or performed Ô¨Åne-tuning of the
weights of the whole network. The exact structure of the
network is shown in Table 3. Note that the activation func-
tion in the convolutional and batch normalisation layers is
the ReLu one; this is also the case in the Ô¨Årst fully con-
nected one. The activation function of the second fully con-
nected layer is linear. It should be also mentioned that the
utilized deep learning architecture has been implemented
on the TensorFlow platform [ 1]. For the pre-trained net-
work we took the one in MatConvNet [ 38] and transformed
it into a format recognisable by TensorFlow. Also note that
we follow the TensorFlow‚Äôs platform notation for the sizes
of all parameters of the convolutional and pooling layers.
Table 3: Baseline Architecture based on CNN-M showing
the sizes for the parameters of the convolutional and pooling
layers and the no of hidden units in the fully connected ones
Layer Ô¨Ålter ksize stride padding no of units
conv 1 [7, 7, 3, 96] [1, 2, 2, 1] ‚ÄôV ALID‚Äô
batch norm
max pooling [1, 3, 3, 1] [1, 2, 2, 1] ‚ÄôV ALID‚Äô
conv 2 [5, 5, 96, 256] [1, 2, 2, 1] ‚ÄôSAME‚Äô
batch norm
max pooling [1, 3, 3, 1] [1, 2, 2, 1] ‚ÄôSAME‚Äô
conv 3 [3, 3, 256, 512] [1, 1, 1, 1] ‚ÄôSAME‚Äô
batch norm
conv 4 [3, 3, 512, 512] [1, 1, 1, 1] ‚ÄôSAME‚Äô
batch norm
conv 5 [3, 3, 512, 512] [1, 1, 1, 1] ‚ÄôSAME‚Äô
batch norm
max pooling [1, 2, 2, 1] [1, 2, 2, 1] ‚ÄôSAME‚Äô
fc 1 4096
fc 2 2
For training the network (in mini batches) we used
the Adam optimizer algorithm. The Mean Squared Error
(MSE) was used as the error/cost function. The hyper-
parameters being used were: the batch size which was 80,
the constant learning rate being 0.001 and the number of
hidden units in the Ô¨Årst fully connected layer which was
4096. We also used biases in the fully connected layers.
The weights of the fully connected layers were initialised
from a Truncated Normal distribution with a zero mean and
variance equal to 0.1 and the biases were initialised to 1.
Training was performed on a single GeForce GTX TITAN
X GPU and the training time was about 4-5 days. No data
augmentation techniques were used, because the database
was already large enough. For training the baseline the
training data have been split to two sets. One was used for
training the network and the other for validation.
Table 4summarizes the obtained MSE and Concordance
Correlation CoefÔ¨Åcient (CCC) V alues over the whole gen-
erated database by our baseline network. It should be men-
1984
tioned that the CCC is deÔ¨Åned as
œÅc=2sxy
s2x+s2y+( ¬Øx‚àí¬Øy)2(1)
wheresxandsyare the variances of the predicted and
ground truth values respectively, ¬Øxand¬Øyare the correspond-
ing mean values and sxyis the respective covariance value.
From the results we deduced that the task is very challeng-
ing and requires meticulously designed deep learning archi-
tectures in order to be tackled.
Table 4: Concordance (CCC) and Mean Squared Error
(MSE) evaluation of valence & arousal predictions provided
by the CNN M baseline architecture
CCC MSE
V alence Arousal V alence Arousal
CNN M 0.15 0.10 0.13 0.14
4.1. The AFF-Wild Challenge
The training data (i.e., videos and annotations) of AFF-
wild challenge were made publicly available on the 30th of
January 2017. The test videos (without annotations) were
made available around 22nd of March. The participants
could submit an entry to the challenge until 2nd of April.
Ten different research groups were initialy interested
in the Aff-Wild challenge. These groups downloaded the
datasets and enquired about different issues of the chal-
lenge. Six of them made experimentation and submitted
their results to the Workshop portal. Based on the perfor-
mance they obtained on the test data, three of them Ô¨Ånally
submitted a paper to the workshop. These are brieÔ¨Çy re-
ported below, while Table 5compares the derived results
(in terms of CCC and MSE) by all three methods.
Table 5: Concordance (CCC) and Mean Squared Error
(MSE) of valence & arousal predictions provided by the 3
methods
Methods CCC MSE
V alence Arousal V alence Arousal
MM-Net 0.196 0.214 0.134 0.088
FA TAUV A-Net 0.396 0.282 0.123 0.095
DRC-Net 0.042 0.291 0.161 0.094
In [23] (Method MM-Net), a variation of the deep con-
volutional residual neural network is Ô¨Årst presented for af-
fective level estimation of facial expressions . Then multi-
ple memory networks are used to model temporal relations
between the video frames. Finally, ensemble models are
used to combine the predictions of the multiple memory
networks, showing that the latter steps improve the initially
obtained performance, as far as MSE is concerned, by more
than 10%.In [6] (Method FA TAUV A-Net), a deep learning frame-
work is presented, in which a core layer, an attribute layer,
an AU layer and a V-A layer are trained sequentially. The
facial part-based response is Ô¨Årstly learnt through attribute
recognition Convolutional Neural Networks, and then these
layers are applied to supervise the learning of AUs. Finally,
AUs are employed as mid-level representations to estimate
the intensity of valence and arousal.
In [25] (Method DRC-Net), three neural network-based
methods are presented and compared, which are based on
Inception-ResNet modules redesigned speciÔ¨Åcally for the
task of facial affect estimation. These methods are: Shallow
Inception-ResNet, Deep Inception-ResNet, and Inception-
ResNet with LSTMs. Facial features are extracted in differ-
ent scales and simultaneously both the valence and arousal
are estimated in each frame. Best results in the experiments
are obtained by the Deep Inception-ResNet method.
All participants applied deep learning methods to the
problem. The winning method of our Challenge is
FA TAUV A-Net Method, since it achieved the best results
in the majority of the metrics used.
4.2. Additional Experiments
As organizers we could not participate in the Aff-W chal-
lenge. Nevertheless, we believe that it would be interesting
to report results with a method that we were developing dur-
ing the course of the challenge. This method was based on
an end-to-end architecture composed of CNNs and Recur-
rent Neural Networks (CNN-RNN) which was trained and
tested using the Aff-Wild benchmark. In particular, we used
different pre-trained CNN and performed transfer learning
and Ô¨Åne-tuning for designing the CNN part of the network.
By including an RNN part and retraining the resulting end-
to-end architecture we obtained our best results for valence
and arousal estimation. More details about this method are
reported in [ 19] (Method V A-CRNN). As can be seen, we
have been able to achieve better results than those achieved
by the participants of the challenge. These results are shown
in Table 6. The above results demonstrate that even though
the task is very challenging an elaborate deep learning ap-
proach could achieve very good results (around 0.57 CCC
for valence and 0.43 for arousal)
Table 6: Concordance (CCC) and Mean Squared Error
(MSE) of valence & arousal predictions provided by our
method
Method CCC MSE
V alence Arousal V alence Arousal
V A-CRNN 0.57 0.43 0.08 0.06
1985
5. Conclusion
The Affect-in-the-Wild (Aff-W) Challenge targets at
bench-marking the efforts made in the research Ô¨Åeld of fa-
cial affect analysis in-the-wild. To develop the Aff-wild
benchmark we collected and annotated the Ô¨Årst large scale
‚Äòin-the- wild‚Äô database of facial affect, consisting of more
than 30 hours of videos showing reactions of about 200 per-
sons, both males and females. In this paper, we described
how the data have been collected and annotated. We give
statistics of the benchmark. Furthermore, we describe the
baseline system that we developed for the challenge. The
performance of the baseline system demonstrates that the
data are very challenging. Hence, they require meticulously
designed deep learning approaches to be designed and im-
plemented for the task. The different approaches which
have been submitted to this challenge show that it is possi-
ble to develop new methods and obtain performance much
higher than the baseline. Our current efforts are concen-
trated to obtain and annotate more videos, as well as an-
notate the previous videos with more attributes (e.g., facial
action units).
6. Acknowledgments
The work of Stefanos Zafeiriou has been partially
funded by the FiDiPro program of Tekes (project num-
ber: 1849/31/2015). The work of Dimitris Kollias was
funded by a Teaching Fellowship of Imperial College Lon-
don. We would like also to acknowledge the contribution of
the Y outube users that gave us the permission to use their
videos (especially Zalzar and Eddie from The1stTake).
References
[1] M. Abadi, A. Agarwal, P . Barham, E. Brevdo, Z. Chen,
C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghe-
mawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y . Jia,
R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man ¬¥e,
R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,
J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P . Tucker,
V . V anhoucke, V . V asudevan, F. Vi ¬¥egas, O. Vinyals, P . War-
den, M. Wattenberg, M. Wicke, Y . Y u, and X. Zheng. Tensor-
Flow: Large-scale machine learning on heterogeneous sys-
tems, 2015. Software available from tensorÔ¨Çow.org.
[2] J. Alabort-i-Medina, E. Antonakos, J. Booth, P . Snape, and
S. Zafeiriou. Menpo: A comprehensive platform for para-
metric image alignment and visual deformable models. In
Proceedings of the ACM International Conference on Multi-
media , MM ‚Äô14, pages 679‚Äì682, New Y ork, NY , USA, 2014.
ACM.
[3] S. Albanie and A. V edaldi. Learning grimaces by watching
tv. In Proceedings of the British Machine Vision Conference
(BMVC) , 2016.
[4] M. S. Aung, S. Kaltwang, B. Romera-paredes, B. Martinez,
A. Singh, M. Cella, M. F. V alstar, H. Meng, A. Kemp, A. C.Elkins, N. Tyler, P . J. Watson, A. C. Williams, M. Pantic,
and N. Berthouze. The automatic detection of chronic pain-
related expression: requirements, challenges and a multi-
modal dataset. IEEE Transactions on Affective Computing ,
2016.
[5] M. S. Bartlett, G. Littlewort, M. Frank, C. Lainscsek,
I. Fasel, and J. Movellan. Fully automatic facial action recog-
nition in spontaneous behavior. In Automatic Face and Ges-
ture Recognition, 2006. FGR 2006. 7th International Con-
ference on , pages 223‚Äì230. IEEE, 2006.
[6] W.-Y . Chang, S.-H. Hsu, and J.-H. Chien. Fatauva-net : An
integrated deep learning framework for facial attribute recog-
nition, action unit (au) detection, and valence-arousal estima-
tion. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition Workshop , 2017.
[7] K. ChatÔ¨Åeld, K. Simonyan, A. V edaldi, and A. Zisserman.
Return of the devil in the details: Delving deep into convo-
lutional nets. arXiv preprint arXiv:1405.3531 , 2014.
[8] G. G. Chrysos, E. Antonakos, P . Snape, A. Asthana, and
S. Zafeiriou. A comprehensive performance evaluation
of deformable face tracking‚Äù in-the-wild‚Äù. arXiv preprint
arXiv:1603.06015 , 2016.
[9] C. Corneanu, M. Oliu, J. Cohn, and S. Escalera. Survey on
rgb, 3d, thermal, and multimodal approaches for facial ex-
pression recognition: History, trends, and affect-related ap-
plications. IEEE transactions on pattern analysis and ma-
chine intelligence , 2016.
[10] R. Cowie and R. R. Cornelius. Describing the emotional
states that are expressed in speech. Speech communication ,
40(1):5‚Äì32, 2003.
[11] R. Cowie, E. Douglas-Cowie, S. Savvidou*, E. McMahon,
M. Sawey, and M. Schr ¬®oder. ‚Äôfeeltrace‚Äô: An instrument
for recording perceived emotion in real time. In ISCA tuto-
rial and research workshop (ITRW) on speech and emotion ,
2000.
[12] R. Cowie, G. McKeown, and E. Douglas-Cowie. Tracing
emotion: an overview. International Journal of Synthetic
Emotions (IJSE) , 3(1):1‚Äì17, 2012.
[13] T. Dalgleish and M. Power. Handbook of cognition and emo-
tion . John Wiley & Sons, 2000.
[14] A. Dhall, R. Goecke, J. Joshi, K. Sikka, and T. Gedeon. Emo-
tion recognition in the wild challenge 2014: Baseline, data
and protocol. In Proceedings of the 16th International Con-
ference on Multimodal Interaction , pages 461‚Äì466. ACM,
2014.
[15] A. Dhall, R. Goecke, J. Joshi, M. Wagner, and T. Gedeon.
Emotion recognition in the wild challenge 2013. In Pro-
ceedings of the 15th ACM on International conference on
multimodal interaction , pages 509‚Äì516. ACM, 2013.
[16] A. Dhall, O. Ramana Murthy, R. Goecke, J. Joshi, and
T. Gedeon. Video and image based emotion recognition chal-
lenges in the wild: Emotiw 2015. In Proceedings of the 2015
ACM on International Conference on Multimodal Interac-
tion , pages 423‚Äì426. ACM, 2015.
[17] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker.
Multi-pie. Image and Vision Computing , 28(5):807‚Äì813,
2010.
1986
[18] H. Jung, S. Lee, J. Yim, S. Park, and J. Kim. Joint Ô¨Åne-tuning
in deep neural networks for facial expression recognition. In
Proceedings of the IEEE International Conference on Com-
puter Vision , pages 2983‚Äì2991, 2015.
[19] D. Kollias, M. Nicolaou, I. Kotsia, G. Zhao, and S. Zafeiriou.
Recognition of affect in the wild using deep neural networks.
InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition Workshop , 2017.
[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
classiÔ¨Åcation with deep convolutional neural networks. In
Advances in neural information processing systems , pages
1097‚Äì1105, 2012.
[21] Y . LeCun, K. Kavukcuoglu, and C. Farabet. Convolutional
networks and applications in vision. In Circuits and Sys-
tems (ISCAS), Proceedings of 2010 IEEE International Sym-
posium on , pages 253‚Äì256. IEEE, 2010.
[22] A. Lee. Welcome to virtualdub. org!-virtualdub. org, 2002.
[23] J. Li, Y . Chen, S. Xiao, J. Zhao, S. Roy, J. Feng, S. Yan, and
T. Sim. Estimation of affective level in the wild with multi-
ple memory networks. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition Workshop ,
2017.
[24] P . Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and
I. Matthews. The extended cohn-kanade dataset (ck+): A
complete dataset for action unit and emotion-speciÔ¨Åed ex-
pression. In Computer Vision and Pattern Recognition Work-
shops (CVPRW), 2010 IEEE Computer Society Conference
on, pages 94‚Äì101. IEEE, 2010.
[25] M. Mahoor and B. Hasani. Facial affect estimation in the
wild using deep residual and convolutional networks. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshop , 2017.
[26] M. Mathias, R. Benenson, M. Pedersoli, and L. V an Gool.
Face detection without bells and whistles. In European Con-
ference on Computer Vision , pages 720‚Äì735. Springer, 2014.
[27] G. McKeown, M. V alstar, R. Cowie, M. Pantic, and
M. Schr ¬®oder. The semaine database: Annotated multimodal
records of emotionally colored conversations between a per-
son and a limited agent. Affective Computing, IEEE Trans-
actions on , 3(1):5‚Äì17, 2012.
[28] M. A. Nicolaou, S. Zafeiriou, and M. Pantic. Correlated-
spaces regression for learning continuous emotion dimen-
sions. In Proceedings of the 21st ACM international con-
ference on Multimedia , pages 773‚Äì776. ACM, 2013.
[29] M. Pantic, M. V alstar, R. Rademaker, and L. Maat. Web-
based database for facial expression analysis. In Multimedia
and Expo, 2005. ICME 2005. IEEE International Conference
on, pages 5‚Äìpp. IEEE, 2005.
[30] R. Plutchik. Emotion: A psychoevolutionary synthesis .
Harpercollins College Division, 1980.
[31] J. A. Russell. Evidence of convergent validity on the dimen-
sions of affect. Journal of personality and social psychology ,
36(10):1152, 1978.
[32] E. Sariyanidi, H. Gunes, and A. Cavallaro. Automatic anal-
ysis of facial affect: A survey of registration, representation,
and recognition. Pattern Analysis and Machine Intelligence,
IEEE Transactions on , 37(6):1113‚Äì1133, 2015.[33] Y .-l. Tian, T. Kanade, and J. F. Cohn. Recognizing action
units for facial expression analysis. Pattern Analysis and
Machine Intelligence, IEEE Transactions on , 23(2):97‚Äì115,
2001.
[34] M. V alstar, J. Gratch, B. Schuller, F. Ringeval, D. Lalanne,
M. Torres Torres, S. Scherer, G. Stratou, R. Cowie, and
M. Pantic. Avec 2016: Depression, mood, and emotion
recognition workshop and challenge. In Proceedings of the
6th International Workshop on Audio/Visual Emotion Chal-
lenge , pages 3‚Äì10. ACM, 2016.
[35] M. V alstar and M. Pantic. Induced disgust, happiness
and surprise: an addition to the mmi facial expression
database. In Proc. 3rd Intern. Workshop on EMOTION
(satellite of LREC): Corpora for Research on Emotion and
Affect , page 65, 2010.
[36] M. V alstar, B. Schuller, K. Smith, T. Almaev, F. Eyben,
J. Krajewski, R. Cowie, and M. Pantic. Avec 2014: 3d
dimensional affect and depression recognition challenge.
InProceedings of the 4th International Workshop on Au-
dio/Visual Emotion Challenge , pages 3‚Äì10. ACM, 2014.
[37] M. V alstar, B. Schuller, K. Smith, F. Eyben, B. Jiang, S. Bi-
lakhia, S. Schnieder, R. Cowie, and M. Pantic. Avec 2013:
the continuous audio/visual emotion and depression recogni-
tion challenge. In Proceedings of the 3rd ACM international
workshop on Audio/visual emotion challenge , pages 3‚Äì10.
ACM, 2013.
[38] A. V edaldi and K. Lenc. Matconvnet: Convolutional neural
networks for matlab. In Proceedings of the 23rd ACM inter-
national conference on Multimedia , pages 689‚Äì692. ACM,
2015.
[39] C. Whissel. The dictionary of affect in language, emotion:
Theory, research and experience: vol. 4, the measurement
of emotions, r. Plutchik and H. Kellerman, Eds., New York:
Academic , 1989.
[40] L. Yin, X. Chen, Y . Sun, T. Worm, and M. Reale. A high-
resolution 3d dynamic facial expression database. In Auto-
matic Face & Gesture Recognition, 2008. FG‚Äô08. 8th IEEE
International Conference On , pages 1‚Äì6. IEEE, 2008.
[41] L. Yin, X. Wei, Y . Sun, J. Wang, and M. J. Rosato. A 3d fa-
cial expression database for facial behavior research. In Au-
tomatic face and gesture recognition, 2006. FGR 2006. 7th
international conference on , pages 211‚Äì216. IEEE, 2006.
[42] L. Y ouTube. Y outube. Retrieved , 27:2011, 2011.
[43] S. Zafeiriou, A. Papaioannou, I. Kotsia, M. Nicolaou, and
G. Zhao. Facial affect‚Äúin-the-wild. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion Workshops , pages 36‚Äì47, 2016.
[44] Z. Zeng, M. Pantic, G. I. Roisman, and T. S. Huang. A sur-
vey of affect recognition methods: Audio, visual, and spon-
taneous expressions. Pattern Analysis and Machine Intelli-
gence, IEEE Transactions on , 31(1):39‚Äì58, 2009.
1987
"
https://ieeexplore.ieee.org/document/6595975,"Affectiva-MIT Facial Expression Dataset (AM-FED): Naturalistic and
Spontaneous Facial Expressions Collected In-the-Wild
Daniel McDuff‚Ä†‚Ä°, Rana El Kaliouby‚Ä†‚Ä°, Thibaud Senechal‚Ä°, May Amr‚Ä°, Jeffrey F. Cohn¬ß, Rosalind Picard‚Ä†‚Ä°
‚Ä°Affectiva Inc., Waltham, MA, USA
‚Ä†MIT Media Lab, Cambridge, MA, USA
¬ßRobotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA
djmcduff@media.mit.edu, {kaliouby,thibaud.senechal,may.amr }@affectiva.com,
jeffcohn@cs.cmu.edu, picard@media.mit.edu
Abstract
Computer classiÔ¨Åcation of facial expressions requires
large amounts of data and this data needs to reÔ¨Çect the
diversity of conditions seen in real applications. Public
datasets help accelerate the progress of research by provid-
ing researchers with a benchmark resource. We present a
comprehensively labeled dataset of ecologically valid spon-
taneous facial responses recorded in natural settings over
the Internet. To collect the data, online viewers watched
one of three intentionally amusing Super Bowl commercials
and were simultaneously Ô¨Ålmed using their webcam. They
answered three self-report questions about their experience.
A subset of viewers additionally gave consent for their data
to be shared publicly with other researchers. This subset
consists of 242 facial videos (168,359 frames) recorded in
real world conditions. The dataset is comprehensively la-
beled for the following: 1) frame-by-frame labels for the
presence of 10 symmetrical F ACS action units, 4 asymmet-
ric (unilateral) F ACS action units, 2 head movements, smile,
general expressiveness, feature tracker fails and gender; 2)
the location of 22 automatically detected landmark points;
3) self-report responses of familiarity with, liking of, and
desire to watch again for the stimuli videos and 4) base-
line performance of detection algorithms on this dataset.
This data is available for distribution to researchers online,
the EULA can be found at: http://www.affectiva.com/facial-
expression-dataset-am-fed/.
1. Introduction
The automatic detection of naturalistic and spontaneous
facial expressions has many applications, ranging from
medical applications such as pain detection [ 1], or monitor-
ing of depression [ 4] and helping individuals on the autism
spectrum [ 10] to commercial uses cases such as advertis-ing research and media testing [ 14] to understanding non-
verbal communication [ 19]. With the ubiquity of cameras
on computers and mobile devices, there is growing interest
in bringing these applications to the real-world. To do so,
spontaneous data collected from real-world environments is
needed. Public datasets truly help accelerate research in an
area, not just because they provide a benchmark, or a com-
mon language, through which researchers can communicate
and compare their different algorithms in an objective man-
ner, but also because compiling such a corpus and getting
it reliably labeled, is tedious work - requiring a lot of effort
which many researchers may not have the resources to do.
There are a number of publicly available labeled
databases for automated facial analysis, which have helped
accelerate research in automated facial analysis tremen-
dously. Databases commonly used for facial action unit
and expression recognition include; Cohn-Kanade (in its
extended edition know as CK+) [ 11], MMI [ 23], RU-
FACS [ 2], Genki-4K [ 24] and UNBC-McMaster Pain
archive [ 12]. These datasets are reviewed in Section 2.
However, all (except the Genki-4K and UNBC-McMaster
Pain archives) were captured in controlled environments
which do not reÔ¨Çect the the type of conditions seen in real-
life applications. Computer-based machine learning and
pattern analysis depends hugely on the number of training
examples [ 22]. To date much of the work automating the
analysis of facial expressions and gestures has had to make
do with limited datasets for training and testing. As a result
this often leads to over-Ô¨Åtting.
Inspired by other researchers who made an effort to share
their data publicly with researchers in the Ô¨Åeld, we present
a database of spontaneous facial expressions that was col-
lected in naturalistic settings as viewers watched video con-
tent online. Many viewers watched from the comfort of
their homes, which meant that the facial videos contained
a range of challenging situations, from nonuniform lighting
2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops
978-0-7695-4990-3/13 $26.00 ¬© 2013 IEEE
DOI 10.1109/CVPRW.2013.130867
2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops
978-0-7695-4990-3/13 $26.00 ¬© 2013 IEEE
DOI 10.1109/CVPRW.2013.130875
2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops
978-0-7695-4990-3/13 $26.00 ¬© 2013 IEEE
DOI 10.1109/CVPRW.2013.130875
2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops
978-0-7695-4990-3/13 $26.00 ¬© 2013 IEEE
DOI 10.1109/CVPRW.2013.130881

and head movements, to subtle and nuanced expressions. To
collect this large dataset, we leverage Internet crowdsourc-
ing, which allows for distributed collection of data very ef-
Ô¨Åciently. The data presented are natural spontaneous re-
sponses to ecologically valid online media (video advertis-
ing) and labels of self-reported liking, desire to watch again
and familiarity. The inclusion of self-reported labels is es-
pecially important as it enables systematic research around
the convergence or divergence of self-report and facial ex-
pressions, and allows us to build models that predict behav-
ior (e.g, watching again).
While data collection is a major undertaking in and of
itself, labeling that data is by far a much grander chal-
lenge. The Facial Action Coding System (FACS) [ 7]i st h e
most comprehensive catalogue of unique facial action units
(AUs) that correspond to each independent motion of the
face. FACS enables the measurement and scoring of facial
activity in an objective, reliable and quantitative way, and is
often used to discriminate between subtle differences in fa-
cial motion. One strength of FACS is the high level of detail
contained within the coding scheme, this has been useful in
identifying new behaviors [ 8] that might have been missed
if a coarser coding scheme were used.
Typically, two or more FACS-certiÔ¨Åed labelers code for
the presence of AUs, and inter-observer agreement is com-
puted. There are a number of methods of evaluating the re-
liability of inter-observer agreement in a labeling task. As
the AUs differ in how easy they are identiÔ¨Åed, it is important
to report agreement for each individual label [ 3]. To give a
more complete perspective on the reliability of each AU la-
bel, we report two measures of inter-observer agreement for
the dataset described in this paper.
The main contribution of this paper is to present a Ô¨Årst
in the world data set of labeled data recorded over the inter-
net of people naturally viewing online media, the AM-FED
dataset contains:
1.Facial Videos: 242 webcam videos recorded in real-
world conditions.
2.Labeled Frames: 168,359 frames labeled for the pres-
ence of 10 symmetrical FACS action units, 4 asymmet-
ric (unilateral) FACS action units, 2 head movements,
smile, expressiveness, feature tracker fails and gender.
3.Tracked Points: Automatically detected landmark
points for 168,359 frames.
4.Self-report responses: Familiarity with, liking of and
desire to watch again for the stimuli videos
5.Baseline ClassiÔ¨Åcation: Baseline performance of
smile, AU2 and AU4 detection algorithms on this
dataset and baseline classiÔ¨Åer outputs.To the authors knowledge this dataset is the largest set
labeled for asymmetric facial action units AU12 and AU14.
In the remainder of this paper we describe the data col-
lection, labeling and label reliability calculation, and the
training, testing and performance of smile, AU2 and AU4
detection on this dataset.
2. Existing Databases
The Cohn-Kanade (in its extended edition known as
CK+) [ 11] has been one of the mostly widely used re-
source in the development of facial action unit and ex-
pression recognition systems. The CK+ database, contains
593 recordings (10,708 frames) of posed and non-posed se-
quences, which are FACS coded as well as coded for the six
basic emotions. The sequences are recorded in a lab setting
under controlled conditions of light and head motion.
The MMI database contains a large collection of FACS
coded facial videos [ 23]. The database consists of 1395
manually AU coded video sequences, 300 also have onset-
appex-offset annotions. A majority of these are posed and
all are recorded in laboratory conditions.
The RU-FACS database [ 2] contains data from 100 par-
ticipants each engaging in a 2.5 minute task. In the task,
the participants had to act to hide their true position, and
therefore one could argue that the RU-FACS dataset is not
fully spontaneous. The RU-FACS dataset is not publicly
available at this time.
The Genki-4K [ 24] dataset contains 4000 images la-
beled as either ‚Äúsmiling‚Äù or ‚Äúnon-smiling‚Äù. These images
were collected from images available on the Internet and do
mostly reÔ¨Çect naturalistic smiles. However, these are just
static images and not video sequences making it impossi-
ble to use the data to train systems that use temporal infor-
mation. In addition, the labels are limited to presence or
absence of smiles and therefore limiting their usefulness.
The UNBC-McMaster Pain archive [ 12] is one of the
largest databases of AU coded videos of naturalistic and
spontaneous facial expressions. This is labeled for 10 ac-
tion units and the action units are coded with levels of in-
tensity making it very rich. However, although of natural-
istic and spontaneous expressions the videos were recorded
with control over the lighting, camera position, frame rate
and resolution.
Multi-PIE [ 9] is a dataset of static facial expression im-
ages using 15 cameras in different locations and 18 Ô¨Çashes
to create various lighting conditions. The dataset includes 6
expressions plus neutral. The JAFFE [ 13] and Semaine [ 18]
datasets contain videos with labeled facial expressions.
However, Multi-PIE, JAFFE and Semaine were collected
in controlled laboratory settings and are not FACS labeled,
but rather have ‚Äúmessage judgement‚Äù labels, and so are not
readily available for training AU detectors.
868
876
876
882
O‚ÄôToole et al. [ 20] present a database including videos
of facial expressions shot under controlled conditions.
3. Data Collection
Figure 1shows the web-based framework that was used
to crowdsource the facial videos and the user experience.
Visitors to the website opt-in to watch short videos while
their facial expressions are being recorded and analyzed.
Immediately following each video, visitors get to see where
they smiled and with what intensity. They can compare their
‚Äúsmile track‚Äù to the aggregate smile track. On the client-
side, all that is needed is a browser with Flash support and
a webcam. The video from the webcam is streamed in real-
time at 14 frames a second at a resolution of 320x240 to
a server where automated facial expression analysis is per-
formed, and the results are rendered back to the browser for
display. There is no need to download or install anything on
the client side, making it very simple for people to partici-
pate. Furthermore, it is straightforward to easily set up and
customize ‚Äúexperiments‚Äù to enable new research questions
to be posed. For this experiment, we chose three successful
Super Bowl commercials: 1. Doritos (‚ÄúHouse sitting‚Äù, 30
s), 2. Google (‚ÄúParisian Love‚Äù, 53 s) and 3. V olkswagen
(‚ÄúThe Force‚Äù, 62 s). Viewers chose to view one or more of
the videos.
On selecting a commercial to watch, visitors are asked
to 1) grant access to their webcam for video recording and
2) to allow MIT and Affectiva to use the facial video for
internal research. Further consent for the data to be shared
with the research community at large is also sought, and
only videos with consent to be shared publicly are shown
in this paper. This data collection protocol was approved by
the MIT Committee On the Use of Humans as Experimental
Subjects (COUHES) prior to launching the site. A screen-
shot of the consent form is shown in Figure 2. If consent
is granted, the commercial is played in the browser whilst
simultaneously streaming the facial video to a server. In
accordance with MIT COUHES, viewers could opt-out if
they chose to at any point while watching the videos, in
which case their facial video is immediately deleted from
the server. If a viewer watches a video to the end, then
his/her facial video data is stored along with the time at
which the session was started, their IP address, the ID of
the video they watched and self-reported responses (if any)
to the self report questions. No other data is stored. A sim-
ilar web-based framework is described in [ 16]. Participants
were aware that their webcam was being used for record-
ing, however, at no point within the interaction were they
shown images from their webcam. This may have had an
impact on their behavior but the majority of videos contain
naturalistic and spontaneous responses.
We collected a total of 6,729 facial videos from 5,268
people who completed the experiment. We disregard videosMEDIAVideo of webcam
footage storedVideo processed to 
calculate smile 
intensity
REPORT3.4. 5.
6.
Flash capture of webcam 
footage.  Frames sent 
(320x240, 15 fps) to 
server. Media  clip played 
simultaneously.SERVER
CLIENT
User can answer self-report 
questions and receives a 
graphical display of the 
calculated smile intensityCONSENT2.
Participant asked if 
they will allow access 
to their webcam 
stream.Data passed 
back to client
1.
Participant visits 
site and chooses 
to watch a 
commerical.
Figure 1. Overview of what the user experience was like and the
web-based framework that was used to crowdsource the facial
videos. The video from the webcam is streamed in real-time to
a server where automated facial expression analysis is performed,
and the results are rendered back to the browser for display. All
the video processing was done on the server side.
Figure 2. The consent forms that the viewers were presented with
before watching the video and before the webcam stream began.
for which the face tracker was unable to identify a face in
at least 90% of frames; this left 3,268 videos (20.0%). As
mentioned earlier, the participants were given the option to
make their face video available for research purposes. For
489 (7.3%) of the videos this was checked. Due to the con-
siderable effort required in coding 242 of these videos have
been hand labeled and are available for public release. We
refer to the public portion of the data collected as the AM-
FED dataset. All videos were recorded with a resolution
of 320x240 and a frame rate of 14 fps. The data contain
many challenges from an automated facial action and ges-
ture recognition perspective. Firstly, the lighting is very
varied both in terms of illumination and contrast making
appearance vary markedly. Secondly, there is considerable
range in the pose and scale of the viewers‚Äô faces as there
were no restrictions applied to the position of the camera
and the viewer‚Äôs were not shown any images from their we-
bcam. Figure 1 (top) shows a selection of frames from the
dataset as examples of the diversity. The properties of the
larger dataset from which the public data is taken can be
found in [ 15]. This demonstrates that the data contains sig-
niÔ¨Åcantly more varied data, in terms of lighting and pose
and position of the participants, than in the CK+ and MMI
databases. The gender of subjects and whether they are
wearing glasses in the video are labeled in the dataset. The
details are provided in Table 1.
869
877
877
883
Table 1. Demographic, glasses wearing and facial hair information
about videos in the dataset.
Gender Glasses Facial hair
Male Female Present Present
140 102 86 37
Table 2. DeÔ¨Ånitions of the labels for the dataset and the number
of frames and videos in which each label was present (agreed by
majority of labelers). Positive examples of each of the labels are
shown in Figure 5
Label DeÔ¨Ånition Frames
PresentVideos
Present
Gender Gender of the viewer - 242
AU2 Outer eyebrow raise 2,587 50
AU4 Brow lowerer 2,274 22
AU5 Upper lid raiser 991 11
AU9 Nose wrinkler 3 1
AU10 Upper lip raiser 26 1
AU14 Symmetrical dimpler 1,161 27
AU15 Lip corner depressor 1 1
AU17 Chin raiser 1,500 30
AU18 Lip pucker 89 7
AU26 Jaw drop 476 6
AU57 Head is forward 253 22
AU58 Head is backward 336 37
Expressiveness Non-neutral face (may contain
AUs that are not labeled)68,028 208
Smile Smile (distinct from AU12) 37,623 180
Trackerfail Frames in which the track failed
to accurately Ô¨Ånd the correct
points on the face18,060 76
Unilateral left
AU12Left asymmetric AU12 467 6
Unilateral
right AU12Right asymmetric AU12 2,330 14
Unilateral left
AU14Left asymmetric dimpler 226 8
Unilateral
right AU14Right asymmetric dimpler 105 4
Negative
AU12AU12 and AU4 together - dis-
tinct from AU12 in smile62 2
110100100010000100000 No. of Frames /Videos  each AU is Present
AU02AU04AU05AU10AU14AU17AU18AU26ForwardBackwardExpressiveSmile
Trackerfail Uni-L-AU14Uni-L-AU12 Uni-R-AU14Uni-R-AU12negAU12Frames
Videos
Figure 3. Number of frames in which each label is present (with
agreement for >= 50% of labelers).
Figure 4. Screenshot of the video labeling tool ViDL used to label
the videos in the dataset.
4. FACS Coding
Each of the videos were independently labeled, frame-
by-frame, by at least three FACS trained coders chosen
from a pool 16 coders (labeling stage). All 16 coders had
undergone FACS training and three were FACS certiÔ¨Åed.
The labels were subsequently labeled by another indepen-
dent FACS trained individual (QA stage) and discrepancies
within the coding reviewed (relabeling stage). For label-
ing we used a web-based, distributed video labeling system
(ViDL) which is speciÔ¨Åcally designed for labeling affective
data [ 6]. A version of ViDL developed by Affectiva was
used for the labeling task. Figure 4shows a screenshot of
the ViDL interface. The labelers were working indepen-
dently for the labeling. The coders labeled for presence (bi-
nary labels) of AU2, AU4, AU5, AU9, AU12 (unilateral and
bilateral), AU14 (unilateral and bilateral), AU15, AU17,
AU18 and AU26. Smiles are labeled and are distinct from
the labels for AU12 as AU12 may occur in an expression
that would not necessary be given the label of smile (e.g. a
grimace). The expressiveness label describes the presence
of any non-neutral facial expression. The trackerfail label
indicates a frame in which the automated Nevenvision fa-
cial feature tracker (licensed from Google, Inc.), for which
the detected points are provided with the dataset, were not
accurately tracking the correct locations on the face. This
gives a total of 168,359 FACS coded frames. DeÔ¨Ånitions
of the labels and the number of frames in which they were
labeled present by a majority of the labelers are shown in
Table 2. Although AU9 and AU15 were coded for, there
were only 1 or 2 examples identiÔ¨Åed by a majority of the
coders. Therefore we do not evaluate the reliability of AU9
and AU15. In the smile and action unit classiÔ¨Åcation sec-
tion of this paper, we assume a label is present if over 50%
of the labelers agree it is present and assume that a label is
not present if 100% of the labelers agree it is not present.
We do not use the frames that do not satisfy these criteria
for the classiÔ¨Åcation task.
870
878
878
884
AU2 AU4 AU5 AU9
AU12 (left)
AU12 (right) AU12 (neg.) Smile AU14 (left) AU14 (right) AU14 (sym.)AU15
AU17 AU18 AU26
 AU58 AU57
AU10
Figure 5. Cropped examples of frames with positive labels for each
of the action units coded. Smile and negative AU12 are labeled
separately instead of labeling symmetrical AU12.
4.1. Reliability of Labels
A minimum of three coders labeled each frame of the
data and agreement between the coders was not necessarily
100%. The labels provided in the archive give the break-
down of all the labelers judgements. We present the reli-
ability of the FACS coding. The reliability for each set of
AU labels in a particular video, p, is the mean correlation
between all pair-wise combinations of the coders labels for
that video sequence. Then the ‚Äúeffective reliability‚Äù is eval-
uated using the Spearman-Brown measure of effective reli-
ability [ 21]. The Spearman-Brown measure of reliability is
calculated as:
RS‚àíB=Np
1+(N‚àí1)p(1)
Where Nis the number of ‚Äútests‚Äù, in this case the number
of coders. The effective reliability accounts for the fact that
theoretically employing more that one coder will mean that
random errors within the coding begin to cancel out and
therefore the effective reliability is greater than the mean
reliability for a particular video.
The weighted-mean Spearman-Brown reliability, across
all 242 video sequences, for each of the labels is shown
in Figure 6. The weighted-mean reliability was calculated
by giving the reliability for each video-AU combination a
weighting relative to the number of agreed positive exam-
ples in that video. As such, a video with very few positive
labels that has poor reliability score is down-weighted rela-
tive to one that has many positive examples.
As the reliability measure calculated above does not re-
ward agreement by labelers in videos that do not contain any
examples of an action unit (i.e. they all label absence of an
action for the whole video) we also calculated the percent-
age agreement across all pairs of labelers and all frames for
each of the labels. The mean percentage agreement across
all AUs was 0.98, the minimum was for AU26 = 0.87.AU02AU04 AU05 AU10AU14
AU17
AU18AU26ForwardBackward
ExpressiveSmile
Trackerfail Uni-L-AU14Uni-L-AU12 Uni-R-AU14 Uni-R-AU12negAU12Mean Spearman‚àíBrown Reliability
00.10.20.30.40.50.60.70.8
Figure 6. Bar graph showing the mean in the Spearman-Brown
reliability for each of the labels
12 34
5678 91011
121314
1516171819
202122
Figure 7. Locations of the 22 landmark points automatically la-
beled using the Nevenvision tracker that are provided with the
dataset.
5. Fiducial Points
The data is also provided with the frame-by-frame lo-
cations of 22 automatically detected landmark points on
the face. The points were detected using the Nevenvision
tracker. The locations of the points are shown in Figure 7.
In some cases the tracker could not identify a face. For these
frames the automatic labels (landmark points, smile and ac-
tion unit classiÔ¨Åer outputs) are assigned -1 to indicate that
no face was identiÔ¨Åed.
6. Self-report Responses
Following viewing a commercial viewers could option-
ally answer three multiple choice questions: ‚ÄúDid you like
the video?‚Äù, ‚ÄúHave you seen it before?‚Äù and ‚ÄúWould you
watch this video again?‚Äù. A screenshot of the questions is
shown in Figure 8. Viewers were not required to answer
the questions and the page would time-out after a time.
The responses to the questions (if any) are provided with
the dataset. For the publicly available data 234 people an-
swered the likability question, 219 people answered the fa-
miliarity question and 194 people answered the desire ques-
tion. Some preliminary analysis of the relationship between
the facial responses and self-report labels collected can be
found in [ 16,17].
7. Experiments
Whilst this is not a paper focused on AU detection we
provide baseline performance for automated AU detection.
871
879
879
885
Figure 8. The self-report questions the viewers were presented
with after watching the commercial.
The action units for which we present results are AU2 (outer
eyebrow raise), AU4 (brow lowerer), and smile (labelers la-
beled for presence of a ‚Äúsmile‚Äù rather than AU12). The out-
put of each of the classiÔ¨Åers is a probability estimate of the
presence of each action unit. The baseline results are set
using the following method. The tracker was used to auto-
matically detect the face and track 22 facial landmark points
within each frame of the videos. The locations of the fa-
cial landmarks are shown in Figure 7. The landmarks were
used to locate the face ROI and the segmented face images
were rescaled to 120x120 pixels. An afÔ¨Åne warp was per-
formed on the bounded face region to account for in-planar
head movement. For the smile detection the landmarks were
used to locate a region around the mouth and histogram of
orientated gradients (HOG) [ 5] features are computed for
the region. The classiÔ¨Åers each use a Support V ector Ma-
chine (SVM) with RBF kernel, these showed signiÔ¨Åcantly
better performance than random forest classiÔ¨Åers. SVMs
have been shown to perform well on smile detection in the
past [ 24]. For the AU2 and AU4 classiÔ¨Åers the landmarks
were used to locate a region around the eyes and HOG fea-
tures were computed for that region.
The AU classiÔ¨Åers were trained and validated on exam-
ples from other datasets collected over the web and simi-
lar in nature to the data available in the AM-FED dataset.
The training and validation sets were independent from
one another and were also person-independent. For testing
the complete set of public frames in this dataset (168,359
frames) were taken and those for which there was greater
than 50% agreement of the present of each action unit or
100% agreement of the absence of each action unit used.
For training, validation and testing in the design of the
classiÔ¨Åers 16,000 frames were used for the AU2 classi-
Ô¨Åer, 58,000 frames were used for the AU4 classiÔ¨Åer and
114,000 frames for the smile classiÔ¨Åer. In the validation
stage the classiÔ¨Åer parameters were selected by maximizing
the area under the receiver operating characteristic (ROC)
curve. During validation the HOG parameters and the size
of facial ROI were optimized. For the SVM classiÔ¨Åer the
spread of the RBF kernel ( Œ≥) and the penalty parameter ( C)
were optimized.
ROC curves were calculated for each of the AU algo-
rithms, these are shown in Figure 10respectively. The
decision-boundary was varied to calculate the ROC curves
shown. The area under the ROC curve for the smile, AU2
and AU4 classiÔ¨Åers was 0.90, 0.72 and 0.70.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91
False Positive RateTrue Positive Rate
SmileAU4AU2
Figure 10. Receiver Operating Characteristic (ROC) curves for the
performance of the smile, AU2 and AU4 classiÔ¨Åers on videos from
the dataset. Smile AUC = 0.90, AU2 AUC=0.72, AU4 AUC=0.70.
The baseline performance shows that accurate AU detec-
tion is possible on this challenging, naturalistic and sponta-
neous data. However, this paper does not focus on the task
of AU detection and there remains room for improvement.
In particular the detection of action units is difÔ¨Åcult in low
illumination conditions. Greater details of the variations in
conditions within the larger dataset from which the labeled
public data is taken can be found in [ 15].
8. Distribution Details
Participants provided informed consent for use of their
video images for scientiÔ¨Åc research purposes. Distribution
of the dataset is governed by the terms of their informed
consent. The data may be used for research purposes and
images from the dataset used in academic publications. All
of the images in the dataset may be used for research pub-
lications. Approval to use the data does not allow recip-
ients to redistribute it and they must adhere to the terms
of conÔ¨Ådentiality restrictions. The license agreement de-
tails the permissible use of the data and the appropriate ci-
tation, it can be found at: http://www.affectiva.com/facial-
expression-dataset-am-fed/. Use of the dataset for commer-
cial purposes is strictly prohibited.
9. Conclusions and Future Work
The main contribution of this paper is to present a Ô¨Årst in
the world publicly available dataset of labeled data recorded
over the Internet of people naturally viewing online media.
The AM-FED contains, 1) 242 webcam videos recorded
in real-world conditions, 2) 168,359 frames labeled for the
presence of 10 symmetrical FACS action units, 4 asymmet-
ric (unilateral) FACS action units, 2 head movements, smile,
872
880
880
886
0 10 20 30 40 50 6000.20.40.60.81Smile Probability 
Mean Hand Labels Thresholded Hand Labels Classifier Output
0 5 10 15 20 25 30
Time (s) 00.20.40.60.81AU2 Probability
0 1 02 03 04 05 06 000.20.40.60.81AU4 Probability
0 10 20 30 40 50  
  
01 02 03 0 4 0 50 
 
01 0 2 0 3 04 0 50 
Time (s)
AU4AU2Smile
Figure 9. Example comparisons between classiÔ¨Åer predictions (green) and manually coded labels (blue and black dashed) for six videos
within the dataset. Threshold of hand labels based on >= 0.5 agreement between coders. Frames from the sequences are shown above.
Top) Smile classiÔ¨Åcation example, middle) AU2 classiÔ¨Åcation example, bottom) AU4 classiÔ¨Åcation example. The viewer‚Äôs distance from
the camera, their pose and the lighting varies considerably between videos.
general expressiveness, feature tracker fails and gender, 3)
locations of 22 automatically detect landmark points, 4)
baseline performance of detection algorithms on this dataset
and baseline classiÔ¨Åer outputs for smile. 5) self-report re-
sponses of familiarity with, liking of and desire to watch
again for the stimuli videos. This represents a rich and ex-
tensively coded resource for researchers working in the do-
mains of facial expression recognition, affective computing,
psychology and marketing.
The videos in this dataset were recorded in real-world
conditions. In particular, they exhibit non-uniform frame-
rate and non-uniform lighting. The camera position relative
the viewer varies from video to video and in some cases the
screen of the laptop is the only source of illumination. The
videos contain viewers from a range of ages and ethnicities
some with glasses and facial hair.
The dataset contains a large number of frames with
agreed presence of facial action units and other labels. The
most common are smiles, AU2, AU4 and AU17 with over
1,000 examples of these. The videos were coded for the
presence of 10 symmetrical FACS action units, 4 asymmet-
ric (unilateral) FACS action units, 2 head movements, smile,
general expressiveness, feature tracker fails and gender. The
rater reliability (calculated using the Spearman-Brown reli-ability metric) was good for a majority of the actions. How-
ever, in cases where there were only a few examples of a
particular action the rate reliability metrics suffered. The
labels with the greatest reliability were smile = 0.78, AU4 =
0.72 and expressiveness = 0.71. The labels with the lowest
reliability was unilateral AU14 (unilateral) and AU10. This
is understandable as the unilateral labels are challenging
especially in frames where the lighting is non-uniform in
which case the appearance of an asymmetric expression can
be ampliÔ¨Åed. AU10 is also relatively rare, only 26 frames
with majority agreed presence, and these come from only
one video sequences. Therefore small differences in coders
agreement might cause the reliability to be low.
We calculate baseline performance for smile, AU2 and
AU4 detection on the dataset, the area under the ROC curves
were 0.90, 0.72 and 0.70 respectively. This demonstrates
that accurate facial action detection is possible but that there
is room for improvement as there are a number of challeng-
ing examples. In addition, the labels provide the possibility
of testing many other AU classiÔ¨Åers on real-world data.
We hope that the release of this dataset will encourage
researchers to test new action unit detection, expression de-
tection and affective computing algorithms on challenging
data collected ‚Äúin-the-wild‚Äù. We hope that it will also serve
873
881
881
887
as a benchmark, enabling researchers to compare the per-
formance of their systems against a common dataset and
that this will lead to greater performance for state-of-the-art
systems in challenging conditions.
Acknowledgments
Richard Sadowsky, Oliver Wilder-Smith, Zhihong Zeng,
Jay Turcot and Khoulood Ayman provided support with the
crowdsourcing system and labeling.
References
[1] A. Ashraf, S. Lucey, J. F. Cohn, T. Chen, Z. Ambadar,
K. Prkachin, P . Solomon, and B. Theobald. The painful face:
pain expression recognition using active appearance models.
InProceedings of the 9th international conference on Multi-
modal interfaces , pages 9‚Äì14. ACM, 2007. 1
[2] M. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. Fasel,
and J. Movellan. Automatic recognition of facial actions in
spontaneous expressions. Journal of Multimedia , 1(6):22‚Äì
35, 2006. 1,2
[3] J. F. Cohn, Z. Ambadar, and P . Ekman. Observer-based mea-
surement of facial expression with the Facial Action Coding
System . Oxford: NY , 2005. 2
[4] J. F. Cohn, T. Kruez, I. Matthews, Y . Yang, M. Nguyen,
M. Padilla, F. Zhou, and F. De la Torre. Detecting depression
from facial actions and vocal prosody. In Affective Comput-
ing and Intelligent Interaction and Workshops, 2009. ACII
2009. 3rd International Conference on , pages 1‚Äì7. IEEE,
2009. 1
[5] N. Dalal and B. Triggs. Histograms of oriented gradients for
human detection. In Computer Vision and Pattern Recogni-
tion, 2005. CVPR 2005. IEEE Computer Society Conference
on, volume 1, pages 886‚Äì893. Ieee, 2005. 6
[6] M. Eckhardt and R. Picard. A more effective way to label
affective expressions. In Affective Computing and Intelligent
Interaction and Workshops, 2009. ACII 2009. 3rd Interna-
tional Conference on , pages 1‚Äì2. IEEE, 2009. 4
[7] P . Ekman and W. Friesen. Facial action coding system. 1977.
2
[8] P . Ekman and E. Rosenberg. What the face reveals: Basic
and applied studies of spontaneous expression using the Fa-
cial Action Coding System (F ACS) . Oxford University Press,
USA, 1997. 2
[9] R. Gross, I. Matthews, J. F. Cohn, T. Kanade, and S. Baker.
Multi-pie. Image and Vision Computing , 28(5):807‚Äì813,
2010. 2
[10] R. Kaliouby and P . Robinson. Real-time inference of com-
plex mental states from facial expressions and head ges-
tures. Real-time vision for human-computer interaction ,
pages 181‚Äì200, 2005. 1
[11] P . Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and
I. Matthews. The Extended Cohn-Kanade Dataset (CK+):
A complete dataset for action unit and emotion-speciÔ¨Åed ex-
pression. In Computer Vision and Pattern Recognition Work-
shops (CVPRW), 2010 IEEE Computer Society Conference
on, pages 94‚Äì101. IEEE, 2010. 1,2[12] P . Lucey, J. F. Cohn, K. Prkachin, P . Solomon, and
I. Matthews. Painful data: The unbc-mcmaster shoulder pain
expression archive database. In Automatic Face & Gesture
Recognition and Workshops (FG 2011), 2011 IEEE Interna-
tional Conference on , pages 57‚Äì64. IEEE, 2011. 1,2
[13] M. Lyons, S. Akamatsu, M. Kamachi, and J. Gyoba. Coding
facial expressions with gabor wavelets. In Automatic Face
and Gesture Recognition, 1998. Proceedings. Third IEEE In-
ternational Conference on , pages 200‚Äì205. IEEE, 1998. 2
[14] D. McDuff, R. El Kaliouby, K. Kassam, and R. Picard. Af-
fect valence inference from facial action unit spectrograms.
InComputer Vision and Pattern Recognition Workshops,
2010 IEEE Computer Society Conference on . IEEE. 1
[15] D. McDuff, R. El Kaliouby, and R. Picard. Crowdsourced
data collection of facial responses. In Proceedings of the 13th
international conference on Multimodal Interaction . ACM,
2011. 3,6
[16] D. McDuff, R. El Kaliouby, and R. Picard. Crowdsourcing
facial responses to online videos. IEEE Transactions on Af-
fective Computing , 3(4):456‚Äì468, 2012. 3,5
[17] D. McDuff, R. El Kaliouby, and R. W. Picard. Predicting on-
line media effectiveness based on smile responses gathered
over the internet. In Automatic Face & Gesture Recognition,
2013 IEEE International Conference on . IEEE, 2013. 5
[18] G. McKeown, M. V alstar, R. Cowie, M. Pantic, and
M. Schroder. The semaine database: annotated multimodal
records of emotionally colored conversations between a per-
son and a limited agent. Affective Computing, IEEE Trans-
actions on , 3(1):5‚Äì17, 2012. 2
[19] D. Messinger, M. Mahoor, S. Chow, and J. F. Cohn. Au-
tomated measurement of facial expression in infant‚Äìmother
interaction: A pilot study. Infancy , 14(3):285‚Äì305, 2009. 1
[20] A. J. O‚ÄôToole, J. Harms, S. L. Snow, D. R. Hurst, M. R. Pap-
pas, J. H. Ayyad, and H. Abdi. A video database of moving
faces and people. Pattern Analysis and Machine Intelligence,
IEEE Transactions on , 27(5):812‚Äì816, 2005. 3
[21] R. Rosenthal. Conducting judgment studies: Some method-
ological issues. The handbook of methods in nonverbal be-
havior research , pages 199‚Äì234, 2005. 5
[22] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio,
R. Moore, A. Kipman, and A. Blake. Real-time human pose
recognition in parts from single depth images. In CVPR ,
volume 2, page 3, 2011. 1
[23] M. V alstar and M. Pantic. Induced disgust, happiness and
surprise: an addition to the mmi facial expression database.
InProceedings of the 3rd International Workshop on EMO-
TION: Corpora for Research in Emotion and Affect , page 65,
2010. 1,2
[24] J. Whitehill, G. Littlewort, I. Fasel, M. Bartlett, and J. Movel-
lan. Toward practical smile detection. Pattern Analysis and
Machine Intelligence, IEEE Transactions on , 31(11):2106‚Äì
2111, 2009. 1,2,6
874
882
882
888
"
https://ieeexplore.ieee.org/document/6553734,"AU-aware Deep Networks for Facial Expression Recognition 
Mengyi Liu, Shaoxin Li, Shiguang Shan, and Xilin Chen, 
Ahstract- In this paper, we propose to construct a deep 
architecture, AU-aware Deep Networks (AUDN), for facial 
expression recognition by elaborately utilizing the prior knowl¬≠
edge that the appearance variations caused by expression can be 
decomposed into a batch of local facial Action Units (AUs). The 
proposed AUDN is composed of three sequential modules: the 
first module consists of two layers, i.e., a convolution layer and 
a max-pooling layer, which aim to generate an over-complete 
representation encoding all expression-specific appearance vari¬≠
ations over all possible locations; In the second module, an 
AU-aware receptive field layer is designed to search subsets of 
the over-complete representation, each of which aims at best 
simulating the combination of AUs; In the last module, multi¬≠
layer Restricted Boltzmann Machines (RBM) are exploited to 
learn hierarchical features, which are then concatenated for 
final expression recognition. Experiments on three expression 
databases CK+, MMI and SFEW demonstrate the effectiveness 
of AUDN in both lab-controlled and wild environments. All our 
results are better than or at least competitive to the best known 
results. 
I. INTRODUCTION 
In the recent years, Facial expression recognition has 
attracted much attention due to its potential applications, 
such as human-computer interaction, multimedia, surveil¬≠
lance, and so on. However, the exploration of the specific 
facial expression features is still an open problem. Studies in 
physiology and psychology indicate that the most descriptive 
regions for expression are located around certain facial 
parts (e.g. mouth, nose, eyes, and brows) . Among them, 
Facial Action Coding System (FACS) [1] is a typical work, 
which was designed to decompose each expression into 
several facial Action Units (AUs) which correspond to above¬≠
mentioned facial parts. Based on the definition of FACS, 
facial expressions can be more precisely described compared 
to rigid categorization [2]. 
Generally, the previous works on expression recognition 
can be categorized into two classes: AU-based methods [3], 
[4] and appearance-based methods [5], [6]. In [3], they first 
detected a number of pre-defined AUs, and then encoded 
their combinations as specific expression according to FACS. 
Since the definitions of AUs are ambiguous in semantics, 
it is difficult to achieve accurate AU detection in practice. 
On the other hand, many appearance-based methods identify 
an image or a sequence as one of the basic expression 
categories according to appearance features (e.g. local binary 
patterns (LBP) [6], Gabor [7], SIFT [8], HOG [9]). Although 
these methods have obtained satisfactory performance in 
Mengyi Liu, Shaoxin Li, Shiguang Shan and Xilin Chen are with Key 
Lab of Intelligent Information Processing of Chinese Academy of Sciences 
(CAS), Institute of Computing Technology, CAS, Beijing 100190, China, 
{mengyi.liu, shaoxin.li, shiguang.shan, xilin.chen}@vipl.ict.ac.cn. some cases, the hand-crafted descriptors applying on a whole 
face in their schemes are lack of semantic interpretation in 
expression. As different facial expressions have explicit local 
variations, which are corresponded to AUs, it's intuitive to 
make use of these distinctive spatial regions. To this end 
as well as tackling the difficult AUs detection, [10] and 
[11] proposed feature selection schemes to automatically 
search for descriptive feature dimensions of local patches 
where certain AUs locates. This strategy is shown to be 
effective. However, there exist two problems: Firstly, hand¬≠
crafted features are still used in their methods, i.e. LBP, 
and Haar, which lack in explicit semantic meaning; Sec¬≠
ondly, selected features are directly concatenated for final 
expression recognition, with no further mechanism to learn 
more higher-level representation, which may be essential 
for bridging the semantic gap between AUs and certain 
expressions. Based on above analysis, in this paper we utilize 
the prior knowledge about facial AUs but propose to solve the 
above problems as follows: First, producing more efficient 
representation by convolving input image with a batch of 
learned descriptors instead of hand-crafted descriptor, which 
can explicitly encode expression-specific appearance, such 
as frown, grin and glare; Second, adding a multi-layer 
learning process after AU-aware feature selection to extract 
incrementally higher-level features layer-by-Iayer. As both 
solutions can be characterized as several network layers, we 
formulate our framework as a multi-layer Deep Networks 
[12] due to its hierarchical representation ability on feature 
learning tasks. 
In the deep networks framework, for the purpose of AU¬≠
aware feature selection, we first introduce the concept of 
""receptive field"". As many deep learning methods [14], [15], 
[16] have achieved high performance of object recognition 
using large architectures with numerous features (hidden 
nodes), one critical concern in such large architectures 
is to specify the connections of nodes (features) between 
adjacent layers. Traditionally, the connections are limited 
by restricting higher level units to receive only lower-level 
inputs from certain subsets (e.g. based on spatial locality 
[17], [18]), which is called ""receptive fields"". However, for 
various recognition scenarios, the size and shape of receptive 
fields may be data-dependent which make it difficult to pre¬≠
define the fields without prior knowledge. To handle this, 
[19] proposes to select receptive fields automatically by 
grouping sets of most similar features during pre-training 
of deep networks. Such unsupervised scheme focus most on 
the relationship among the features, rather than exploring the 
relevance between features and categories, which makes the 
single receptive field inefficient on description and discrim-
Face Image Feature Maps Over Complete 
Representation 
Receptive 
Fields Logistic 
RBM RBM Regression 
ÔøΩ,A,,A, 
Hierarchical Feature Linear 
SVM 
Classifier 
Linear 
SVM 
Classifier 
Linear 
SVM 
Classifier 
ÔøΩ-------ÔøΩ yÔøΩ------- -----'}\'-------,yr-____ -,I '---y-----l 
Over-complete Representation AU-aware Receptive Fields Hierarchical Feature Learning 
Fig. 1. The pipeline of the proposed method. 
ination. A. Over-complete Representation 
In order to construct the receptive fields automatically 
as well as utilize the co-occurrence information contained 
in disconnected facial regions, we introduce a supervised 
feature selection scheme to group subsets of low-level repre¬≠
sentations, which could simulate the different combinations 
of AUs. In this paper, we call such subsets ""AU-aware 
Receptive fields (AURFs)"" and this selection procedure is 
added into our whole framework for effective mid-level 
feature learning. An overview of the proposed method is 
presented in Fig.I. There are three sequential modules each 
of which consists of one or more network layers. As the 
proposed deep architecture can learn features according to 
the interpretation of facial AUs, we call it AU-aware Deep 
Networks (AUDN). Our AUDN is tested on two well known 
lab controlled expression database CK+ [21], MMI [22], and 
a wild expression database SFEW [23]. Compared to several 
hand-crafted appearance features, with the same linear SVM 
classifier [24], the learned features not only achieve state-of¬≠
the-art performance but also bears intuitive physiology and 
psychology appeal. 
II. THE PROPOSED AUDN 
This section details the three modules in the proposed AU¬≠
aware Deep Networks (AUDN). As is demonstrated in Fig.l, 
firstly, convolution layer and max-pooling layer are used to 
generate an over-complete representation. This representation 
can explicitly depict specific appearance presented in specific 
region. Then a feature selection scheme is used to find 
AURFs, which describe the combinations of local appearance 
variations. Finally, multi-layer RBM [20] is applied to each 
AURF respectively to learn hierarchical features for facial 
expression recognition. As accurate AUs detection is hard to achieve in static 
facial images, we try to design an over-complete feature 
representation over all possible spatial regions by convolving 
the dense-sampling facial patches with special filters. Previ¬≠
ously, many patch-based learning methods tend to generate 
localized filters that simulate the function of simple cells 
in the primary visual cortex [25], [26]. These works justify 
the use of such filters in the standard model for object 
recognition. In this paper, we also attempt to learn a bank 
of specific filters, from all possible local patches from large 
number of expression images. 
Among the plenty of algorithms for unsupervised learning, 
K-means has enjoyed wide adoption for generating code¬≠
books of ""visual words"" [27], [28], which can be used to 
define higher-level features. This method is also applied to 
build layers in our framework. Suppose the patch size is u¬≠
by-u pixels, to obtain an over-complete representation, we 
set K > u2 in K-means clustering and learn K centroids c( k) (k = 1, 2, ... , K) from all patches after normalizing and 
whitening. Then each centroid is considered as a filter to 
convolve with the patches in the whole facial images. For 
an input image with t-by-t patches, each 2D grid of t-by-t 
responses for a single filter is generally called a ""map"". In 
the end we will get an t-by-t-by-K dimension representation 
after the convolutional layer (see Fig.2). 
Depending on the first layer representation, it is hard 
to learn features invariant to image transformations (e.g. 
translation). This problem can be generally handled by 
incorporating ""pooling"" layers. In [16], it has been found 
that max-pooling can lead to faster convergence, superior 
invariant features selection, and improve generalization. Here 
the features go through our max-pooling layers are given by 
the maximum activation over adjacent, disjoint spatial blocks 
on each filter map. 
Fig. 2. Examples of filters and corresponding maps in convolutional layer. 
B. AU-aware Receptive Fields (AURFs) 
In this module, we focus on selecting groups of AU¬≠
aware receptive fields from the outputs of max-pooling layer. 
As each feature in the outputs represents the presence of 
a single pattern which simulates specific AU, the selected 
receptive fields can depict complex combinations of appear¬≠
ance variations, which is expected to be consistent with 
the interpretation of FACS. To explore the expression-driven 
AU-aware features, we attempt to apply a greedy maximal 
relevance feature selection: iteratively selecting features with 
the highest relevance to the target expression category. How¬≠
ever, in such greedy searching scheme, the combinations 
of individually best features do not necessarily lead to best 
classification performance [29]. For example, the features of 
adjacent regions that contained similar appearance is likely 
to have similar label-relevance score, but the combination of 
them can offer no more descriptive information to classifi¬≠
cation. On this consideration, some researchers proposed to 
select features with the minimal redundancy as an auxiliary 
condition [30], [31], [32], and one of these, based on criteria 
of minimal-redundancy-maximal-relevance (mRMR) [33], is 
a made-to-order scheme and proved to be effective in our 
experiments. 
In our approach, relevance is characterized in terms of 
mutual information, which is widely used to measure depen¬≠
dency of variables. Given two random variables x and y, 
their mutual information is defined in accordance with their 
probabilistic density functions p(x), p(y), and p(x, y): 
( ) II p(x,y) I x; y = p(x, y) log p(x)p(y) dxdy. (1) 
Max-relevance is to find a feature set S with m features {Xi}, 
which has the largest mean relevance to expression labels c. 
The scheme can be formularized as: 
1 max D(S, c), D = 1ST L I(Xi; c). 
xiES (2) 
As the features only satisfying (2) may have unexpected 
redundancy, the following min-redundancy condition can be Fig. 3. Selected patches in AURF when m = 1,2,8,16. 
added to select relatively more diverse features: 
min R(S), R = 1;12 L I(Xi;Xj). (3) 
Xi,XjES 
Combining criterion presented in expression (2) and (3), 
an greedy search methods is used to find the local-optimal 
features defined by max(D -R). Suppose we already have 
Sk-l, the set contained k-l features. Our goal is to find the 
kth feature from the rest P = X -Sk-l. The incremental 
algorithm optimizes the following condition: 
1 max[I(xj; c) --k-"" I(Xj; Xi)]' (4) 
xjEP - 1 L...J 
xiESk-1 
Our overall AU-aware receptive fields selection algorithm 
is shown in Algorithm. 1. And examples of local patches 
corresponding to selected features are shown in Fig.3. 
Algorithm 1 : AU-aware Receptive Fields Selection 
Input 
Over-complete representation X = {Xi Ii = 1,2, ... , M}; 
Expression labels of over-complete representation C; 
Selected feature dimension in each receptive field m; 
Number of AU-aware receptive fields N; 
Output 
Receptive fields RF(n) = {Xni Ii = 1,2, ... , m}; 
Algorithm 
1: Initialize selected feature set S = ¬¢, the set of features 
to be selected P = X -S; 
2: for n = 1,2, ... , N do 
3: Set RF(n) = ¬¢; 
4: Select feature Xs from P which has maximal 
mutual information with c: 
argmax I(xs; c) 
xsEP 
5: Update RF(n) = RF(n) U{xs}, S = SU{xs}, 
P=X -S; 
6: for k = 2, ... , m do 
7: Select feature Xs from P based on equation (4): 
8: Update RF(n) = RF(n) U{xs}, S = SU{xs}, 
P=X-S; 
9: end for 
10: end for 
C. Hierarchical Feature Learning 
In this module, we attempt to learn even higher-level 
expression features from each AU-aware receptive field. 
Recent neuroscience findings have provided insight into 
the principles governing information representation in the 
mammalian brain, which motivated the emergence of varies 
deep machine learning methods [12], [15], [18] focusing on 
computational models for information representation. One 
of the approaches in [20] applies a restricted Boltzmann 
machine (RBM) to model each new layer of higher-level 
features, which guarantees an increase on the lower-bound 
of the log likelihood of the data. The Single RBM is a two¬≠
layer (i.e. visible layer and hidden layer), undirected graphic 
model without lateral connections. The nodes in visible layer 
and hidden layer are represented as Vi, hi respectively. If the 
visible units are real values, the configuration of Vi, hi is 
characterized by an energy function as follows: 
E(v h) = ÔøΩ ""v2_ ""v¬∑W .. h¬∑-""b¬∑h¬∑-"" c¬∑v¬∑ (5) , 2 ÔøΩ"" ÔøΩ"" ""J J ÔøΩ J J ÔøΩ""""' i i,j j i 
where Wij characterizes the association between visible 
and hidden nodes and Ci, bj are the biases of visible layer and 
hidden layer respectively. Probabilistically, this is interpreted 
as 
P(v,h) = exp(-ÔøΩ(v,h)), Z = Lexp(-E(v, h)). (6) 
v,h 
The hidden nodes are conditionally independent given the 
visible layer nodes, and vice versa. The parameters of RBM 
can be optimized by performing stochastic gradient ascent on 
maximizing the log-likelihood of training data. As computing 
the exact gradient of log-likelihood is intractable, Contrastive 
Divergence (CD) approximation is used [34] which works 
fairly well in practice. The three-layers module in this section 
is formed by stacking RBMs as this way: First an RBM is 
trained on the receptive field layer. Then, after the training 
first layer RBM, the weights are frozen and hidden layer 
act as the input of next layer, and so forth. in the end, a 
supervised fine tuning step is performed to adjust the weights 
for improvement on particular task. 
III. DEEP NETWORK DETAILS 
This section details the network structure and parameters 
used in our experiments. As preprocessing, all the faces 
in images are detected automatically by Viola-Jones face 
detector [35], and then normalized to 32x32 based on the 
location of eyes. 
For the first layer, we sample 6-by-6 pixel patches with 
a stride of 1 pixel on the 32-by-32 pixel raw images. Thus 
each image contains 27-by-27 small patches and K (here 
we set K = 100) filters can be learned from all these 
patches in training set. Then each 32-by-32 image obtains a 
27-by-27-by-K representation after convolution. To achieve 
some extent translation invariance, we apply max-pooling 
over adjacent, disjoint 3-by-3 patches on each map (an image 
responses for a single filter). In the end this yields 9-by-9-
by-IOO features as an over-complete representation for each 
expression image. After extracting the representation in the first module, we 
apply mRMR for AU-aware receptive fields selection. For 
the parameters referred in Algorithm. 1 , we set the feature 
dimension in each AURF as m = 500, and the number of 
AURFs N in each dataset depends on the features relevance 
to class labels. Our experiments show that CK+ contains 
less noise than the other two database (e.g. non-uniform 
expression, wearing accessories, lighting, and so on), so the 
relevance of features are much higher than that in MMI or 
SFEW. In practice, based on the mean relevance of each 
receptive field, we set N = 9 in CK + and N = 3 in both 
MMI and SFEW. 
In the last module, we apply two additional RBM layers 
to learn higher-level features in each AURF. The input 
layer's node number equals to the dimension of each AURF 
(m = 500). The node number of hidden layers are 400 and 
300 respectively. After unsupervised pre-training, the two 
layers RBM can be further supervisedly fine-tuned using 
logistic regression. At last, for each AURF, the features in 
the first visible layer and each hidden layer are concatenated 
to construct final hierarchical feature for facial expression 
recognition using linear SVM classifier. 
IV. EXPERIMENTS 
In this section, we evaluate the learned hierarchical fea¬≠
tures for facial expression recognition. All comparisons are 
performed on three datasets: CK+ [21], MMI [22], and a wild 
expression database SFEW [23]. The experimental results 
of our method achieve or outperform the state-of-the-art 
performance. 
A. Experiments on CK + 
The CK+ database consists of 593 sequences from 123 
subjects, which is an extended version of Cohn-Kanade 
(CK) [36] database. The validating emotion labels were only 
assigned to 327 sequences which were found to meet criteria 
for one of 7 discrete emotion (Anger (An): 45, Contempt 
(Co): 18, Disgust (Di): 59, Fear (Fe): 25, Happiness (Ha): 
69, Sadness (Sa): 28, and Surprise (Su): 83) based on FACS. 
In our experiments, we make use of all these sequences from 
118 subjects. For each sequence, the first image (neutral face) 
and three peak frames were used for prototypic expression 
recognition which is similar to the settings in [6], [11]. Based 
on the subject ID given in the dataset, we construct 10 
person independent subsets by sampling in ID ascending 
order with step size equals 10 and adopt lO-fold cross¬≠
validation. What's more, to avoid parameter sensitivity, only 
linear SVM classifier is used in all of our experiments. For 
comparison, Table.! lists the recognition accuracies of several 
methods, including hand-crafted-feature-based ones and the 
state-of-the-art performance. 
In the table, ""OR"" represents the method based on fea¬≠
tures of Over-complete Representation; ""AURF"" represents 
the method based on selected AU -aware Receptive Fields 
(AURFs); and ""AUDN"" represents the method based on the 
final features achieved by our deep networks. The CSPL in 
TABLE I TABLE II TABLE III 
EXPRESSION RECOG NITION ACCURACY ON EXPRESSION RECOG NITION ACCURACY ON EXPRESSION RECOG NITION ACCURACY ON 
CK+ DATABASE. MMI DATABASE. SPEW DATABASE. 
Methods Accuracy Methods Accuracy Methods Accuracy 
LBP 
SIFf 
HOG 
Gabor 83.87%(linear) 81.89%(RBF) 
86.39%(linear) 87.31 %(RBF) 
89.53%(linear) 88.61 %(RBF) 
88.61 %(linear) 85.09%(RBF) LBP 
SIFf 
HOG 
Gabor 52.93%(linear) 50.37%(RBF) 
57.80%(linear) 61.46%(RBF) 
63. 17%(linear) 65.24%(RBF) 
56.10%(linear) 57.56%(RBF) LBP 
SIFT 
HOG 
Gabor 21.29%(linear) 23.71 %(RBF) 
20.45%(linear) 21.14%(RBF) 
19.52%(linear) 22.71 %(RBF) 
19.29%(linear) 19. 14%(RBF) 
CSPL [11] 
OR 
AURF 
AVON 89.89%(unknown) 
91.44%(linear) 
92.22%(linear) 
92.05%(linear) CSPL [11] 
OR 
AURF 
AVON 73.53%( unknown) 
68.41 %(linear) 
69.88%(linear) 
74.76%(linear) Baseline [23] 
OR 
AVRF 
AVON 19.00%(RBF) 
24.98%(linear) 
23.00%(linear) 
26.14%(linear) 
TABLE IV TABLE V TABLE VI 
THE CONFUSION MATRIX OF AUDN ON 
CK+ DATABASE. THE CONFUSION MATRIX OF AUDN ON 
MMI DATABASE. THE CONFUSION MATRIX OF AUDN ON 
SPEW DATABASE. 
Ne 
An 
Co 
Di 4.52 
Fe 9.33 
Ha 0 
Sa 20.24 2.38 0 Di Fe Ha Sa Su 
0.61 0.61 0 0.61 0.61 
o 0.74 0 
1.85 5.56 0 
o 
4 
Su 1.2 0 1.2 0 An 
Ne 
An 
Di 
Fe 10.71 
Ha 4.76 
Sa 20.83 15.63 3.13 
Su 15 0 
[10] gets the accuracy of 89.89%, which represents state-of¬≠
the-art performance in CK + database. It should be pointed 
out that CSPL only handled six expression categories from 
96 subjects, while our experiment considers 8 categories 
(including Contempt and Neutral (denoted as ""Ne"")), which 
is much more challenging and practical compared to many 
existing methods. 
We also compared our ""learned feature"" with the hand¬≠
crafted ones. The experiment settings (the same on Table.l, 
Table.II and Table.III) are: Image size is 32x32 pixels as 
same as before. LBP (944 dimensions): 16 patches with size 
of 8x8 pixels and 59 dimensions uniformed feature on each 
patch; SIFT (1152 dimensions): 9 lattice points with 128 
dimensions feature on each; HOG (1568 dimensions): 49 
overlapped blocks with size of 8x8 pixels, 4 cells and 8 
histogram bins for each block. Gabor (24576 dimensions): 
convolutional images with 3 spatial scales and 8 orientations. 
We performed both linear SVM and RBF SVM on all the 
features. The grid search for parameter estimation for RBF 
is performed over c = 2k, k = 0, 1, ... , 9; g = 2l, l = 
-5, -4, ... , O. Some results may have gaps with the existing 
ones [6], [7], the two main reasons are: (1) The protocols 
are different. we performed strict person independent test by 
lO-nonoverlapping-fold cross-validation. (2) The image size 
is 32x32 pixels, which is much smaller than other methods. 
For classification details, we also show the confusion 
matrix of AUDN in Table.lV. We can see that the recognition 
rates of Contempt and Sadness are not as promising as 
that of , for example, Happiness and Surprise. This may 
be caused by the unevenly distribution of data, i.e. some 
of the categories have much fewer samples. The probability 
of a sample assigned to such minor categories could be 
overwhelmed by those of major categories. Fe Ha Sa Su An Di Fe Ha Ne Sa Su 
1.95 0.98 5.85 0.98 An 24.11 8.93 15.18 10.71 16.07 16.96 8.04 
0 15.05 0 Di 16.47 14.12 9.41 15.29 18.82 16.47 9.41 
6.25 8.33 0 Fe .23 5.05 20.20 19.19 9.09 10.10 13.13 
Ha 15.79 3.51 
Ne 22.00 8.00 
Sa 10.10 8.08 
7.5 Su 9.89 9.89 
B. Experiments on MMI 
The MMI database [22] includes 30 subjects of both sexes 
and ages from 19 to 62. In the datasets, 213 sequences 
have been labeled with six basic expressions, in which 205 
sequences are captured frontal view. We use the data from 
all these 205 sequences as in [11]. Similar to the settings on 
CK+, the neutral face and three peak frames in each sequence 
have been used and lO-fold cross-validation is conducted 
in the same way. We still perform our experiments on all 
seven expression categories including neutral, which is more 
difficult compared to six categories problem in [11]. What's 
more, in both CK+ and MMI experiments, the images we 
used are 32x32 pixels which are much smaller than 96x96 
in [11], nevertheless, better results are achieved using the 
proposed method. Table.II demonstrate the comparisons of 
several methods and the confusion matrix of AUDN are 
shown in Table. V. 
Compared to the results on CK+, the recognition perfor¬≠
mance degrade significantly on MMI database due to its 
challenging conditions: The subjects posed expressions non¬≠
uniformly, and many of them wear accessories (e.g. glasses, 
moustache). Thus the traditional hand-crafted features cannot 
offer discriminative information as such noises exist. How¬≠
ever, our scheme apply the data-dependent dictionary rather 
than fixed descriptors, which is more robust dealing with the 
complex conditions. It shows that gradually better results 
have been achieved during our learning process. 
C. Experiments on SFEW 
For further validation, we apply our method to a much 
more challenging scenario: the expression in the wild. The 
Static Facial Expression in the Wild (SFEW) database 
which has been extracted from movies is different from 
the available facial expression datasets generated in highly 
controlled lab environments. It is the first attempt to build 
database depicting real-world or simulated real-world con¬≠
ditions for expression analysis [23]. According to Strictly 
Person Independent (SPI) Protocol for SPEW, the database 
is divided into two sets. Each set contains seven subfolders 
corresponding to the seven expression categories. The sets 
were created in strict person independent manner that there 
is no overlap between training and testing set. In total, there 
are 700 images (346 in Set!, 354 in Set2) and 95 subjects. 
The experiment is set to be two-fold (Fold1: train on Set! 
and test on Set2; Fold2: train on Set2 and test on Set!) and 
we take the average accuracy of two folds for measurement. 
As shown in Table.III and Table.VI, our method outper¬≠
forms the baseline 19% [23] significantly. However, due 
to the tough imaging conditions, the faces normalized by 
automatically detected eyes location suffer from severe mis¬≠
alignment, so that none of the algorithms can work well as 
on CK + or MMI. 
V. CONCLUSIONS AND FUTURE WORKS 
In this paper, we propose to construct a deep architec¬≠
ture especially for facial expression recognition, which is 
called ""AUDN"". Inspired by the interpretation of FACS, we 
applying an mRMR feature selection on an over complete 
representation, the obtained AURFs are shown to be able 
to simulate specific AUs. Additional multi-layer RBMs can 
extract higher-level features in each AURF and further fine 
tuned by logistic regression. Linear SVM classifier is ap¬≠
plied to hierarchical feature and final result is obtained by 
averaging recognition results of each AURF. The proposed 
AUDN achieve or outperform state-of-the-art performance on 
three facial expression database covering both lab control and 
wild scenario. In future, we will try to search more complex 
combination of feature rather than simple non-overlapping 
AURF to further boost the performance of our method. 
VI. ACKNOWLEDGMENTS 
This paper is partially supported by Natural Science 
Foundation of China under contracts No. 61025010 and No. 
61222211. 
REFERENCES 
[1] P. Ekman and W. V. Friesen. Facial action coding system. Consulting 
Psychologists Press,I,1978. 
[2] C. E. Izard. The face of emotion. New York: Appleton-Century¬≠
Crofts, 1971. 
[3] Y. Tian, T. Kanade, and J. F. Cohn. Recognizing upper face action 
units for facial expression analysis. CVPR, 2000. 
[4] Y. Tong, W. Liao, and Q. n. Facial action unit recognition by 
exploiting their dynamic and semantic relationships. TPAMI, 29(10): 
1683-1699, 2007. 
[5] M. Lyons, J. Budynek, and S. Akamatsu. Automatic classification of 
single facial images. TPAMI, 21(12): 1357-1362, 1999. 
[6] C. Shan, S.Gong, and P. W. McOwan. Facial expression recognition 
based on local binary patterns: A comprehensive study. Image and 
Vision Computing, 27: 803-816, 2009. 
[7] G. Littlewort, M. S. Bartlett, 1. Fasel, J. Susskind, and J. Movellan. 
Dynamics of facial expression extracted automatically from video. 
Image and Vision Computing, 2006. [8] U. Tariq, K. Lin, Z. Li, X. Zhou, Z. Wang, V. Le, T. S. Huang, X. 
Lv, and T. X. Han. Emotion recognition from an ensemble of features. 
FG,20 11. 
[9] Z. Zeng, L. Yin, X. Wei, X. Zhou, and T. S. Huang. Multi-view facial 
expression recognition. FG, 2008. 
[10] P. Yang, Q. Liu, and D. N. Metaxas. Exploring facial expression with 
compositional features. CVPR, 2010. 
[11] L. Zhong, Q. Liu, P. Yang, B. Liu, J. Huang, and D. N. Metaxas. 
Learning active facial patches for expression analysis. CVPR, 2012. 
[12] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality 
of data with neural networks. Science, 313(5786): 504-507, 2006. 
[13] A. Rao, and N. Thiagarajan. Recognizing facial expressions from 
videos using Deep Belief Networks. Technical Report, 2009. 
[14] D. Ciresan, U. Meier, J. Masci, L. M. Gambardella, and J. Schmidbu¬≠
ber. High-performance neutral networks for visual object classification. 
Pre-print, 2011. http://arxiv.orglabs/ll02.0183. 
[15] A. Coates and A. Y. Ng. The importance of encoding versus training 
with sparse coding and vector quantization. ICML, 2011. 
[16] D. Scherer, A. Miler, and S. Behnke. Evaluation of pooling operations 
in convolutional architectures for object recognition. ICANN, 2010. 
[17] Y. LeCun, F. Huang, and L. Bottou. Learning methods for generic 
object recognition with invarience to pose and lighting. CVPR, 2004. 
[18] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep 
belief networks for scalable unsupervised learning of hierarchical 
representations. ICML, 2009. 
[19] A. Coates and A. Y. Ng. Selecting receptive fields in deep networks. 
NIPS, 2011. 
[20] G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm 
for Deep Belief Nets. Neural Computation, 18(7): 1527-1554, 2006. 
[21] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, and Z. Ambadar. The 
Extended Cohn-Kanade dataset (CK+): A complete dataset for action 
unit and emotion-specified expression. CVPRW, 2010. 
[22] M. F. Valstar and M. Pantic. Induced disgust, happiness and surprise: 
an addition to the MMI facial expression database. LREC, 2010. 
[23] A. Dhall, R. Goecke, S. Lucey, and T. Gedeon. Static facial expression 
analysis in tough conditions: Data, evaluation protocol and benchmark. 
ICCVW, 2011. 
[24] R. E. Fan, K. W. Chang, C. J. Hsieh, X. R. Wang, and 
C. J. Lin. LIBLINEAR: A Library for Large Linear Classifi¬≠
cation, JMLR, 9(2008): 1871-1874, 2008. Software available at 
http://www.csie.ntu.edu.tw/cj linlliblinear. 
[25] T. Serre, L. Wolf, and T. Poggio. Object recognition with features 
inspired by visual cortex. CVPR, 2007. 
[26] J. Mutch and D. G. Lowe. Object class recognition and localization 
using sparse features with limited receptive fields. IJCV, 56(6): 503-
511, 2008. 
[27] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial 
pyramid matching for recognizing natural scene categories. CVPR, 
2006. 
[28] J. Winn, A. Criminisi, and T. Minka. Object categorization by learned 
universal visual dictionary. ICCV, 2005. 
[29] A. K. Jain, R. P. W. Duin, and J. Mao. Statistcial pattern recognition: 
A review. TPAMI, 22(1): 4-37, 2000. 
[30] P. Pudil, J. Novovicova, and J. Kittler. Floating search methods in 
feature selection. PRL, 15(11): 1119-1125, 1994. 
[31] J. Jaeger, R. Sengupta, and W. L. Ruzzo. Improved gene selection for 
classification of microarrays. PSB, 2003. 
[32] C. Ding and H. Pengo Minimum redundancy feature selection from 
micro array gene expression data. CSB, 2003. 
[33] H. Peng, F. Long, and C. Ding. Feature selection based on mutual 
information: criteria of max-dependency, max-relevance, and min¬≠
redundancy. TPAMI, 27(8): 1226-1237,2005. 
[34] G. E. Hinton. Training products of experts by minimizing contrastive 
divergence. Neural Computation, 14: 1771-1800,2002. 
[35] P. Viola and M. Jones. Robust real-time object detection. IJCV, 2001. 
[36] T. Kanade, J. Cohn, and Y. Tian. Comprehensive database for facial 
expression analysis. FG, 2000. 
"
https://ieeexplore.ieee.org/abstract/document/7965717/,"Automatic Facial Expression Recognition Based on a 
Deep Convolutional-Neural-Network Structure  
Ke Shan, Junqi Guo*, Wenwan You, Di Lu, Rongfang Bie 
College of Information Science and Technology 
Beijing Normal University 
Beijing, P.R.China 
guojunqi@bnu.edu.cn 
Abstract ‚ÄîFacial expression recognition, which many 
researchers have put much effort in, is an important portion of affective computing and artificial intelligence. However, human 
facial expressions change so subtly that recognition accuracy of 
most traditional approaches largely depend on feature extraction. Meanwhile, deep learning is a hot research topic in the field of machine learning recently, which intends to simulate the 
organizational structure of human brain‚Äôs nerve and combine low-
level features to form a more abstract level. In this paper, we employ a deep convolutional neural network (CNN) to devise a facial expression recognition system, which is capable to discover deeper feature representation of facial expression to achieve 
automatic recognition. The proposed system is composed of the 
Input Module, the Pre-processing Module, the Recognition Module and the Output Module. We introduce both the Japanese Female Facial Expression Database(JAFFE) and the Extended 
Cohn-Kanade Dataset(CK+) to simulate and evaluate the 
recognition performance under the influence of different factors (network structure, learning rate and pre-processing). We also introduce a K-nearest neighbor (KNN) algorithm compared with 
CNN to make the results more convincing. The accuracy 
performance of the proposed system reaches 76.7442% and 80.303% in the JAFFE and CK+, respectively, which demonstrates feasibility and effectiveness of our system. 
Keywords‚ÄîFacial Expression Recognition; Deep Learning; 
Convolutional Neural Network 
I. I NTRODUCTION
 Early in the 1990s, Picard predicted that Affective 
Computing would be an important direction for future artificial 
intelligence research [1]. In 1971, the American psychologist 
Ekman and Friesen defined seven categories of basic facial expression, which are Happy, Sad, Angry, Fear, Surprise, Disgust and Neutral [2]. In 1991, A.Pentland and K.Mase held the first attempt to use optical flow method to determine the 
direction of movement of facial muscles. Then, they extracted 
the feature vectors to achieve four kinds of automatic expression recognition including Happy, Angry, Disgust, Surprise and got nearly 80% accuracy [3]. 
 In 2006, Hinton and Salakhutdinov published an article in 
""Science"" [5], opening the door to a deep learning era. Hinton suggested that the neural network with multiple hidden layers had good ability for learning characteristics. It can improve the accuracy of prediction and classification by obtaining different degrees of abstract representation of the original data. So far, the 
deep learning algorithm has achieved good performance in speech recognition, collaborative filtering, handwriting recognition, computer vision and many other fields [4]. 
 The concept of Convolutional Neural Network (CNN) was 
presented by Yann LeCun et al. in [7] in the 1980s, where a neural network architecture was composed of two kinds of basic layers, respectively called convolutional layers (C layers) and subsampling layers (S layers). However, many years after that, 
there was still not a major breakthrough of CNN. One of the 
main reasons was that CNN could not get ideal results on large size images. But it was changed when Hinton and his students used a deeper Convolutional Neural Network to reach the optimal results in the world on ImageNet in 2012. Since then, 
more attention has been paid on CNN based image recognition. 
In this paper, we present a method to achieve facial 
expression recognition based on a deep CNN. Firstly we implement face detection by using Haar-like features and histogram equalization. Then we construct a four-layer CNN architecture, including two convolutional layers and two subsampling layers (C-S-C-S). Finally, a Softmax classifier is 
used for multi-classification. 
 The structure of the paper is as follows: Section 2 introduces 
the whole system based on CNN, including the input module, the image pre-processing module, the recognition algorithm 
module and the output module. In Section 3, we simulate and evaluate the recognition performance of the proposed system 
under the influence of different factors such as network structure, learning rate and pre-processing. Finally, a conclusion is drawn..  
II. F
ACIAL EXPRESSION RECOGNITION SYSTEM BASED ON CNN
A. System Overview
This section starts with the overall introduction of CNN-
based facial expression recognition system. System flow is showed in Fig. 1. 
* The corresponding author 
978-1-5090-5756-6/17/$31.00 ¬©2017 IEEE
SERA 2017, June 7-9, 2017, London, UK123
We employ the Extended Cohn-Kanade Dataset (CK+) [8] 
and  the Japanese Female Facial Expression Database (JAFFE) [9] for the simulation, which are both standard facial expression database categorized for 7 kinds of expressions. First of all, the 
Input Module obtains the input image 2D data. The Pre-
processing Module includes 2 steps: face detection and histogram equalization. Thus we can get the main part of the human face and minimize the difference of lighting conditions in backgrounds. Recognition Module is based on convolutional neural network (CNN) algorithms and multiple classifiers Softmax. The Output Module shows MSE convergence figure and calculate the recognition accuracy. If the recognition 
accuracy does not meet the requirement, re-adjust the network 
parameters and begin a new round of training until the accuracy is satisfying. Details of each module are described as follow. 
B. Image Pre-processing 
We employ two standard facial expression databases for the 
simulation, which are both widely acknowledged by academia. JAFFE contains 213 images of 10 Japanese women, while CK+ covers the expression images of all races of people and has 328 pictures totally. Before the recognition, some pre-processing 
work need to be done firstly. In our image pre-processing 
procedure, we run a two-step process to reduce the interference in the original images, which are Face Detection and Histogram Equalization. 
1) Face detection based on Haar-like feature The first step of image pre-processing is face detection. In 
the face detection part, detection results are based on the Haar-like feature in OpenCV, which is one of the most classic features for face detection. It was originally proposed by Papageorgiou et al. [10] [11] and also known as the rectangular feature. Haar-like feature templates are divided into three categories, namely edge features, linear features and center surround features. On this basis, Haar-like feature templates Viola and Jones [12] used are shown in Fig. 2. 
A feature template is composed of black area and white area. 
After placing it on a certain part of the image, we can get the feature value by the subtraction between all the pixels added within the white rectangle coverage and that within the black rectangle coverage. Accordingly, the goal of these black and white rectangles is to quantify facial features to get the distributing information of the face and finally to distinguish the non-face portion and the face portion. 
 A demonstration of our detection results is showed in Fig. 3. 
We can see that the Haar-like feature is effective in catching the useful portion of facial expression and removing most of the meaningless background information. Therefore, it can reduce the amount of data we need to deal with, as well as effectively 
avoid the interference of different backgrounds and other objects 
in the picture on the recognition results. 
2) Histogram equalization 
After acquiring the very face portion of the image, other 
troublesome issues should also be considered. Due to the different lighting conditions when taking pictures, the portions of human face will also show in different brightness, which will Face 
Detection
Histogram 
EqualizationCK+ Input Module
Pre-processing
Module
Recognition
Moduleconvolutional 
layer (C1)
subsampling 
layer(S1)
convolutional 
layer(C2)
subsampling 
layer(S2)
Softmax 
multiple 
classifierRasterizeAdjust 
Parameters
Output ModuleRecognition 
AccuracyMSE 
convergence 
FIG.Accuracy is 
satisfying?End YesNoJAFFE
 
Fig. 1.  System Flow Diagram.  
Fig. 2.  Demonstration of Haar-like feature templates. 
 
Original Images 
 
After Detection and Extraction 
Fig. 3.  Face detection based on Haar-like feature. 
124
inevitably cause large interference on recognition results. Thus, 
we decide to conduct histogram equalization(HE) before recognition. Histogram equalization is a simple but effective algorithm in image processing, which can make the gray values distribution in different images more uniform and reduce interference caused by different lighting conditions. 
As is showed in Fig. 4, distributions of gray value in different 
picture of the same expression are very inconsistent before equalization, which causes large interference to recognition algorithm. After histogram equalization, gray value of each image uniformly covers the entire range of gradation, image contrast is improved and gray distribution of different pictures is more unified.  
Fig. 5 shows more clearly that brighter face portion is 
optimized by histogram equalization. Thus the important features are better presented and all the images are unified as 
possible. We can conclude that histogram equalization is 
effective in reducing interference caused by different lighting 
conditions. The following experiments also prove it.  
C. Structure of CNN-based Recognition Algorithm 
Convolutional Neural Networks (CNN) is composed of two 
basic layers, respectively called convolutional layer (C layer) 
and subsampling layer (S layer). Different from general deep 
learning models, CNN can directly accept 2D images as the input data, so that it has unique advantage in the field of image recognition. 
A classic CNN model is showed as Fig. 6. 2D images are 
directly inputted into the network, and then convoluted with several adjustable convolutional kernels to generate corresponding feature maps to form layer C1. Feature maps in 
layer C1 will be subsampled to reduce their size and form layer S1. Normally, the pooling size is 2√ó2. This procedure also repeats in layer C2 and layer S2. After extracting enough features, the two-dimensional pixels are rasterized into 1D data and inputted to the traditional neural network classifier. In practical applications, we generally use Softmax as the final multiple classifier. 
Entering a convolutional layer, the feature map of upper 
layer is divided into lots of local areas and convoluted respectively with trainable kernels. After the convolutions are processed by activation function, we will get new output feature 
maps. Let the 
l-th layer is a convolutional layer, the j-th 
output in this layer can be expressed as: 
 1(* )
jll l l
j ii j j
iMX fX K b‚àí
‚àà=+¬¶   (1) 
Wherein, jM presents the local area connected by the j-th 
kernel, l
ijK is a parameter of convolutional kernel, l
jb is bias, 
()f‚Ä¢ is the Sigmoid function. 
In subsampling layer, the most commonly used method is 
mean pooling in a 2√ó2 area. That is, average 4 points in the area 
as a new pixel value. 
Parameter estimation in CNN still uses the gradient 
algorithm of back propagation. However, according to the characteristics of CNN, we should make some modifications in several particular steps. 
Suggest that the residual error vector spread to the raster 
layer is 
rd. Specifically, it can be written as: 
 111 112[, , ]T
rj m ndd d d= ‚Ä¶,   (2) 
Because the rasterization is a 2D-to-1D transfer, residual 
error vector reverse pass to the subsampling layer is simply needed to re-organized from 1D to 2D matrix. 
When reversing pass from S layer to C layer, different 
pooling method is corresponding to different process of residual error back propagation. In mean pooling, we just average the residual error in current point to 4 points of upper layer. Suggest  
Fig. 4.  Histograms contrast before and after histogram equalization. 
 
Fig. 5.  Contrast between the face portion before and after HE. 
 
Fig. 6.  Structure of CNN. 
125
that a residual error in a point of S layer isqŒî. After up-
sampling, error transferred to C layer can be presented as: 
 ()p q upsample Œî= Œî   (3) 
There are trainable parameters in C layer. Therefore, C layer 
has two tasks in back propagation: reverse residual error and update its parameters. According to the BP algorithm and 
consider the convolution operation, we can get the formula that 
update parameter 
pŒ∏ in convolutional layer p is: 
 '180(( )* 180( ))q
pErot X rot pŒ∏‚àÇ=Œî‚àÇ¬¶   (4) 
 where rot180 refers to a 180-degree rotation to a matrix. 
If a feature map q' in the former S layer is connected to a set 
C in convolutional layer p, the residual error spread to q' is: 
 ''( * 180( ))qp p q
pCrot X Œ∏
‚ààŒî= Œî¬¶   (5) 
After all the parameters are updated, the network completes 
a round of training. This process should be carried out for all training samples until the whole network meet the training requirements. 
 In this paper, we employ a C-S-C-S constructed CNN. The 
size of convolutional kernel in C layer is 5 √ó 5, and the initial values are random numbers between -1 and 1. S layer do the mean pooling within the 2 √ó 2 area of each feature map. The logical relationship of the recognition algorithm between the various parts and functions is shown in Fig.7. The algorithm consists of several basic parts, respectively 
achieve the function of data input, parameter initialization, network training and testing. 
First of all, the network infrastructure is completely 
configured and parameters of layers are initialized by the initialization module. When initialization is finished, input the training data and training labels into the network. After the training, testing data together with testing labels are inputted into Testing Module. By comparing output judging results of testing data with the testing labels, we can finally get the recognition accuracy. 
III. P
ERFORMANCE EVALUATION  
A. Performance Evaluation vs. CNN Structures 
Differences of network structures could cause great impact 
on the recognition performance. Generally, we need to rely on experience and continuous testing to get the best network structure for a particular classification task. For feasibility, we fix the four-layer structure as C-S-C-S and make the number of 
feature maps of every convolutional layer changeable. In order 
to better control the variables, the following results is in the case that learning rate »ò (0 <»ò¬î1) equals 0.5. 
TABLE I  Recognition accuracy of different CNN structures in JAFFE (%)  
 10 12 14 
4 13.9535 65.1163 69.7674 
6 13.9535 76.7442 69.7674 
8 58.1395 58.1395 67.4419 
As it can be seen from Table ƒâ, although more feature maps 
can extract more types of expression features theoretically, too 
many features will cause unnecessary interference and decrease the recognition ability of the network. Thus, proper structure of convolution layer is important for obtaining good recognition results. By the experiments, JAFFE get the highest accuracy when C1 has 6 and C2 has 12 feature maps. 
TABLE II  Recognition accuracy of different CNN structures in CK+ (%) 
 10 12 14 
4 21.2121 77.2727 77.2727 
6 77.2727 78.7879 77.7879 
8 71.2121 72.7273 21.2121 
Similarly, in CK +, we can get the best results when C1 has 
6 and C2 has 12 or 14 feature maps showed in the Table ƒä. 
Based on the two databases, we finally selected 6-12 structure 
for network training. All of the recognition results showed below are based on this structure. 
B. Performance Evaluation vs. Learning Rates 
Learning rate »ò is the measure to variation of parameter 
updating, thus the value is controlled between 0 and 1(0 < »ò¬î1). 
If »ò is too large, the variation of network parameters on every 
updating will be too sharp, which will affect the stability of the updated parameters. Even worse, it can eventually lead to non-C1 C2 C1 C2 
Convolutional
kernel and bias in 
C layerPooling 
method in 
S layerParameters 
in classic NN
Forward 
propagationCalculate
error & back
propagationGradient 
Method to 
modify the 
weightsParameter 
Initialization
Training
Reach the number 
of iterations?No
Yes
Testing
Recognition 
Accuracy
Train_data
Train_label
Test_data
Test_labelInput
Test_data
Output 
judging 
resultsTest_labelInput
 
Fig. 7.  Schematic of recognition algorithm.   
126
convergence error with the increase of training times. 
Meanwhile, if »ò is too small, the process of convergence will 
take a long time and consume too much calculating resources. Therefore, the value of »ò needs to be selected appropriately 
according to the actual training environment.  
For better recognition performance, we run the tests on 
different values of »ò. We selected five discrete values between 
0-1 and get the recognition results in both JAFFE and CK+, 
which are respectively displayed in the Table ƒã and the Table 
ƒå as below. 
TABLE III  Recognition accuracy on different learning rates in JAFFE (%)  
»ò Accuracy 
0.1 69.7674 
0.3 72.093 
0.5 76.7442 
0.7 74.4186 
0.9 72.093 
TABLE IV  Recognition accuracy on different learning rates in CK+ (%) 
»ò Accuracy 
0.1 75.7576 
0.3 75.7576 
0.5 78.7879 
0.7 80.303  
0.9 77.2727 
The best recognition result is obtained when »ò is 0.5 in 
JAFFE, and 0.7 in CK+. Higher or lower learning rate will decrease the recognition performance of CNN. 
In order to further analyze the impact of different learning 
rates on the recognition performance, we draw the partial magnification of MSE (Mean Square Error) convergence curve 
of CK+ as an example. 
As we can see more intuitively from Fig. 8, when »ò is too 
large (»ò=0.9,0.7), the sharp change of weights results in the 
oscillation of MSE at the beginning of training. As »ò decreases, 
the error convergence tends to be stable. The smaller »ò leads to 
the slower MSE convergence. 
C. Performance Evaluation vs. Image Pre-processing  
To reflect the effect of pre-processing, we discuss the 
recognition performance before and after the histogram 
equalization (HE). The result in JAFFE is showed as Table ƒç. 
TABLE V  Recognition accuracy before and after HE in JAFFE (%) 
 0.1 0.3 0.5 0.7 0.9 
After 69.7674 72.093 76.7442 74.4186 72.093 
Before 69.7674 65.1163 67.4419 51.1628 58.1395 
The same evaluation in CK+ is displayed in Table ƒé as well: 
TABLE VI  Recognition accuracy before and after HE in CK+ (%) 
 0.1 0.3 0.5 0.7 0.9 
After 75.7576 75.7576 78.7879  80.303 77.2727 
Before 71.2121 72.7272 75.7576 77.2727 75.7576 
The results show that the recognition accuracy without 
histogram equalization is reduced due to brightness interference, compared to the accuracy after equalization on every value of learning rate. It proves that histogram equalization does indeed improve recognition performance of the network. 
For deeper analysis, we draw the MSE convergence curve in 
the case of training process with and without histogram equalization. As a representative, Fig.9 shows the MSE convergence curve before and after histogram equalization in 
CK+, where »ò = 0.7. HE »ò »ò
HE
 
Fig. 9.  MSE convergence curve before and after HE in CK+.  
 
Fig. 8.  Partial magnification of MSE convergence curve in CK+. 
127
Obviously, MSE converges more slowly when training the 
network with original images, which means that there is a lot of interference before the histogram is equalized. Much computational resources and training times are consumed to correct the interference caused by brightness differences. By the appropriate pre-process like histogram equalization, we can render the MSE converge faster and finally get the better training results. 
D. Performance Comparison 
For a more comprehensive display of CNN‚Äôs performance 
on facial expression recognition task, we introduce a traditional classification method KNN (K-Nearest Neighbor) to make a comparison.  
For equality, the pre-processing of images for KNN is the 
same as CNN‚Äôs. We get the recognition results in different K values as below. 
TABLE VII   Recognition accuracy of KNN on different K values in JAFFE (%)  
k Accuracy 
3 58.1395 
5 62.7907 
7 65.1163  
9 62.7907 
11 48.8372  
13 53.4884 
TABLE VIII   Recognition accuracy of KNN on different K values in CK+ (%) 
k Accuracy 
10 74.2424 
15 77.2727  
20 74.2424 
25 72.7273 
30 72.7273  
35 71.2121 
Thus, we can compare the two best recognition results of 
CNN and KNN, which is shown in Table ƒë. 
TABLE IX  The comparison of best recognition accuracy of CNN and KNN (%) 
 CNN KNN 
JAFFE 76.7442 65.1163 
CK+ 80.303 77.2727 As is shown, CNN‚Äôs performance is significantly better than 
KNN‚Äôs in the task of facial expression recognition. KNN is simply based on the spatial location of known data to judge the test data, while CNN can learn deeper features of data and get more reliable recognition results. In a conclusion, the CNN is obviously more suitable for facial expression recognition. 
IV. C
ONCLUSIONS  
In this paper, we have proposed a system based on a CNN 
algorithm to achieve human facial expression recognition. The whole system is composed of Input Module, Pre-processing Module, Recognition Module and Output Module. First of all, we build a theoretical model of the system, and describe the details of every module, especially the CNN algorithm module. Then we introduce two classic facial expression databases JAFFE and CK+ to simulate the recognition process on MATLAB, and analyze recognition performance in different situations. A KNN algorithm is also employed to make comparison with CNN, which demonstrates that the CNN algorithm is more suitable for facial expression recognition.  
R
EFERENCES  
[1] Picard R. W.. Affective computing. MIT Press. 
[2] Ekman P, Friesen WV. Constants across cultures in the face and 
emotion[J]. Journal of personality and social psychology, 1971,17(2): 
124. 
[3] Mase K. Recognition of facial expression from optical flow. IEICE Trans 
E[J]. Ieice Transactions on Information & Systems, 1991, 74(10). 
[4] X. W. Chen and X. Lin, ""Big Data Deep Learning: Challenges and 
Perspectives,"" in IEEE Access, vol. 2, no. , pp. 514-525, 2014.doi: 
10.1109/ACCESS.2014.2325029. 
[5] Hinton G E; Salakhutdinov R R. Reducing the dimensionality of data with 
neural networks [J]. Science, 2006, 313:504-507. DOI: 
10.1126/science.1127647. 
[6] Hubel DH, Wiesel TN. Receptive fields, binocular interaction and 
functional architecture in the cat‚Äôs visual cortex. The Journal of 
Physiology. 1962;160(1):106-154.2. 
[7] Lecun, Y. ""Generalization and Network Design Strategies."" 
Connectionism in Perspective 1989. 
[8] Lucey, P., Cohn, J. F., Kanade, T., Saragih, J., Ambadar, Z., & Matthews, 
I. (2010). The Extended Cohn-Kanade Dataset (CK+): A complete expression dataset for action unit and emotion-specified expression. Proceedings of the Third International Workshop on CVPR for Human 
Communicative Behavior Analysis (CVPR4HB 2010), San Francisco, 
USA, 94-101. 
[9] Michael J. Lyons, Shigeru Akemastu, Miyuki Kamachi, Jiro Gyoba. 
Coding Facial Expressions with Gabor Wavelets, 3rd IEEE International Conference on Automatic Face and Gesture Recognition, pp. 200-205 
(1998). 
[10] Pageorgiou C., Oren M., Poggio T.. A general framework for object 
detection. International Conference on Computer Vision. 1998. 555-562. 
[11] Oren M., Pageorgiou C., Ppggio T.. Example ‡≠çbased object detection in 
images by components. IEEE Transaction on Pattern Analysis and 
Machine Intelligence.2001.23(4): 349-361. 
[12] Viola P., Jones M.. Rapid object detection using a boosted cascade of 
simple features. IEEE Conference on Computer Vision and Pattern 
Recognition. 2001:511-518. 
 
128
"
https://ieeexplore.ieee.org/document/7747479,"This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
IEEE TRANSACTIONS ON CYBERNETICS 1
Automatic Facial Expression Recognition System
Using Deep Network-Based Data Fusion
Anima Majumder, Laxmidhar Behera, Senior Member, IEEE , and Venkatesh K. Subramanian
Abstract ‚ÄîThis paper presents a novel automatic facial
expressions recognition system (AFERS) using the deep net-
work framework. The proposed AFERS consists of four steps:
1) geometric features extraction; 2) regional local binary pat-
tern (LBP) features extraction; 3) fusion of both the featuresusing autoencoders; and 4) classiÔ¨Åcation using Kohonen self-organizing map (SOM)-based classiÔ¨Åer. This paper makes threedistinct contributions. The proposed deep network consisting ofautoencoders and the SOM-based classiÔ¨Åer is computationallymore efÔ¨Åcient and performance wise more accurate. The fusionof geometric features with LBP features using autoencoders pro-vides better representation of facial expression. The SOM-basedclassiÔ¨Åer proposed in this paper has been improved by makinguse of a soft-threshold logic and a better learning algorithm. Theperformance of the proposed approach is validated on two widelyused databases (DBs): 1) MMI and 2) extended Cohn‚ÄìKanade(CK+). An average recognition accuracy of 97.55% in MMI DBand 98.95% in CK+ DB are obtained using the proposed algo-rithm. The recognition results obtained from fused features arefound to be distinctly superior to both recognition using individ-ual features as well as recognition with a direct concatenation ofthe individual feature vectors. Simulation results validate that theproposed AFERS is more efÔ¨Åcient as compared to the existingapproaches.
Index Terms ‚ÄîAutoencoder, deep network, facial expression
recognition system, self-organizing map (SOM), support vectormachine (SVM).
I. I NTRODUCTION
FACIAL expressions recognition system has received sig-
niÔ¨Åcant attention among researchers in recent decades
mainly because of it is diversiÔ¨Åed applications, such as human
computer interactions, multimedia, surveillance, treatment
of mentally retarded patients, and lie detection. The study of
Mehrabian [1] stated that to understand emotion or intention of
a person, 55% of the information are conveyed through facialexpressions alone, 38% through vocal cues, and the remaining
7% via verbal cues. This encourages the researchers to explore
deeply in the area of facial expressions recognition and anal-ysis (FERA). Ekman et al. [2] asserted after extensive study
Manuscript received March 28, 2016; revised July 9, 2016 and
August 8, 2016; accepted October 28, 2016. This paper was recommended
by Associate Editor H. Yin.
The authors are with the Department of Electrical Engineering,
Indian Institute of Technology Kanpur, Kanpur 208016, India (e-mail:
animam@iitk.ac.in ;lbehera@iitk.ac.in ;venkats@iitk.ac.in ).
This paper has supplementary downloadable multimedia material available
at http://ieeexplore.ieee.org provided by the authors.
Color versions of one or more of the Ô¨Ågures in this paper are available
online at http://ieeexplore .ieee.org .
Digital Object IdentiÔ¨Åer 10.1109/TCYB.2016.2625419over facial expressions, that facial expressions are universal
and innate. They also concluded that six basic expressions,namely, happiness, sadness, disgust, anger, surprise, and fear
are universal in nature. During the last few decades, much
attention is given in automating features extraction method-ologies [3], [4]. Features extracted from image sequences can
be classiÔ¨Åed into either different action units (AUs) or canbe classiÔ¨Åed directly to determine the six basic expressions.The key challenges in FERA are features extraction and clas-
siÔ¨Åcation, as it is very difÔ¨Åcult to select a feature extraction
technique that works efÔ¨Åciently in varying conditions, such asdifferent skin colors, age, gender, and lighting conditions.
Based on the existing literature, the features can be broadly
classiÔ¨Åed into two main categories: 1) geometric features and
2) appearance features. Geometric features require accurate
and reliable facial feature detection and tracking methods,as they are often sensitive to noise. On the other hand, the
appearance features are difÔ¨Åcult to generalize across differ-
ent persons [ 5]. Yet, both the features play signiÔ¨Åcant role
in effective recognition of facial expressions. As we look at
an expressive face, our brain not only collects information
from appearance features or wrinkles, it also performs reason-ing of the information from facial geometric features, such as
degree of closeness of eyes, lips, and widening of eyebrows.
Schyns et al. [6] explained in their paper about the decod-
ing and decorrelation process of facial information occurs
in human brain. According to this paper, the brain encodes
both the geometric and appearance features at different scalesduring its various processing stages.
As the geometric features and appearance features are com-
ing from two different features extraction approaches, they are
not likely to be statistically related in any predictable man-
ner. Particularly, the nonlinearities in the function mappings,from local binary pattern (LBP) feature space to emotion space
and from the geometric features space to emotion space are
not of same order. The conventional feature fusion methodssimply concatenate several kinds of features together. Such
approach is simple, but the systems which use this kind of
feature fusion may not perform better, or even it may performworse [7]. The reason for such behavior is uneven measure
or representation of different kinds of features. Moreover, theelement values of different features can be signiÔ¨Åcantly unbal-anced. Thus, a mere concatenation of the two feature vectors
results in poor recognition performance. In this paper, we pro-
pose a feature fusion framework using autoencoders which canlearn the correlation and generate a better representation of
both the features. The autoencoder has a simple architecture
2168-2267 c/circlecopyrt2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
Seehttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
2 IEEE TRANSACTIONS ON CYBERNETICS
with an extra-ordinary capability of enhancing a given feature
at every successive hidden layer.
The conventional deep learning network [ 8]u s i n gM L P
learns by extracting features progressively from image pix-els which is usually of very high dimension. The learningtakes place in two stages: 1) stacked restricted Boltzmann
machine (RBM) (unsupervised) and 2) error back-propagation
(supervised Ô¨Åne tuning). For instance, each face detectedimage of the MMI and the extended Cohn‚ÄìKanade (CK +)
databases (DBs) contains 240 √ó240 and 300 √ó300 pixels,
respectively. The conventional deep learning network will pro-cess input data of sizes 240 √ó240 and 300 √ó300, respectively,
while these data will be further processed through many hid-den layers of the network until one reaches the output layer.
The supervised Ô¨Åne-tuning of the entire network using back-
propagation incurs huge computation as massively parallelnonlinear steps are involved [ 9].
In contrast, input data size for the Ô¨Årst layer of the proposed
deep network is 258 √ó1 (the geometric feature vector has
the dimension 22 √ó1 and the appearance feature vector has
the dimension 236 √ó1). This input data size is negligi-
ble as compared to the data size that will be used in theconventional deep network [ 8]. The proposed network has
only three layers that include two autoencoder layers and aself-organizing map (SOM)-based classiÔ¨Åer. The SOM-basedclassiÔ¨Åer processes input data of size 230 √ó1. Thus the
proposed deep-network as given in this paper has a sim-ple architecture and is computationally more efÔ¨Åcient. Theautoencoders used in this paper has following architecture. In
the Ô¨Årst layer, two separate autoencoders individually process
geometric and appearance features. The concatenated learned
representations obtained in the Ô¨Årst layer are given as input to
the autoencoder at the second layer to learn the higher ordercorrelations between two different types of features. Such a
data fusion technique provides better facial attributes with the
lower dimensional feature set. When compared with PCA,the autoencoder performs much better data representation
in a lower dimension, especially for nonlinear input data
type [ 10]. The fused feature is further applied to SOM-based
classiÔ¨Åer which is an improvement over our previous
approach [11 ].
The contributions of the proposed work are summarized as
follows.
1) The proposed deep network consisting of autoencoders
and the SOM-based classiÔ¨Åer has a simple architecture.
The network is easy to train as compared to a deep learn-
ing architecture with stacked RMB followed by MLP
which is trained using back-propagation.
2) The fusion of geometric features and appearance fea-
tures using autoencoders is a novel idea to get betterrepresentation of the facial attributes.
a) Instead of considering whole face image, 236 LBP
features are extracted from automatically detectedfour key facial regions. This approach reduces
computational burden while preserving location
information.
b) Geometric features of dimension 22 (with
directional displacement information of faciallandmarks) and appearance feature of dimension236 (using regional LBP) are fused using two lay-ers of autoencoders to get a better representation
of facial attributes.
3) A soft thresholding technique has been introduced at
the output nodes of the SOM-based classiÔ¨Åer whichreduces false prediction rate. An improved learning algo-
rithm has been proposed to train the parameters of theclassiÔ¨Åer. The combined effect of the proposed soft
thresholding technique and the improved learning algo-
rithm has enhanced the recognition performance of theSOM-based classiÔ¨Åer signiÔ¨Åcantly.
4) The proposed technique is implemented and validated
on two popular DBs: 1) MMI and 2) CK +.T h e
fusion algorithm is also implemented with support vectormachine (SVM) classiÔ¨Åer on both the DBs and the com-parative performance evaluation has been done through
extensive simulations.
The remainder of this paper is organized as follows. A
brief review of related previous work has been presentedin Section II. Two different types of features (geometric-
based and regional LBP) extraction approaches are explained
in Sections III-A and III-B , respectively. The detailed
explanation of the data fusion technique is given inSection III-C .
SOM-based classiÔ¨Åcation approach is explained
in Section III-D . In Section IV, evaluation performances of two
different DBs; CK +and MMI are presented with discussions.
Finally, Section Vconcludes this paper.
II. R ELATED WORK
A. Previous Work on Feature Extraction and
ClassiÔ¨Åcation Techniques
Much progress has been made in geometric and appear-
ance features extraction approaches [12 ]‚Äì[17] for automatic
facial expressions recognition system (AFERS). Two wellestablished geometric or Ô¨Åducial points detection methods
are active shape model (ASM) [12 ] and active appearance
model (AAM) [13]. However, there is one important limita-
tion in both the approaches. Each training face image in ASM
and AAM needs to be manually labeled with 68 landmarks.It becomes a tedious job to label 68 landmarks manually in
each of the training image contained in large DB. Some of the
popular appearance features extraction methods are LBPs [ 18],
Gabor Ô¨Ålters [19 ], linear discriminant analysis (LDA) [ 20] and
histograms of oriented gradients [21]. Although Gabor Ô¨Ålter-
based AFERS gives very good recognition accuracies [ 19], it
is associated with high computational cost. As Gabor Ô¨Ålters
use Ô¨Åve different scales and eight different orientations [ 22],
a face image of size 240 √ó240 has the Gabor feature dimen-
sion as 240 √ó240√ó40. This is a huge dimensional feature
that adds heavily to the computational cost. In contrast, the
LBP [ 14] extracts comparatively a low-dimensional feature
vector using nonlinear operations and hence has advantagesover approaches, such as Gabor Ô¨Ålters and LDA. Given afeature vector, the next step in AFERS is to select a proper
dimensionality reduction and classiÔ¨Åcation technique. Some of
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
MAJUMDER et al. : AFERS USING DEEP NETWORK-BASED DATA FUSION 3
the well established dimensionality reduction and classiÔ¨Åca-
tion techniques are RBMs [10], SVMs [23 ], and deep neural
networks [24 ], [25]. Zhi et al. [26] used sparse and graph-
preserving properties to reduce feature vector dimensions forfacial expressions recognition. SOM has been extensively usedin [27] and [ 28] as an unsupervised clustering technique, as it
has the inherent capability of providing a topological orderingof the classes. Lawrence et al. [27] used local image sampling,
an SOM and a convolutional neural network to recognize face.
Some extension of SOM-based clustering and dimensionality
reduction approaches are visualization induced SOM (ViSOM)and growing ViSOM [ 28]. High level emotion recognition has
been recently reported in the literature, such as recognition ofsign languages [15] and detection of deception [ 17].
B. Previous Work on Data Fusion Techniques
A multimodal approach to feature extraction can lead to
the development of a robust AFERS as facial expres-sions are subjected to diverse personalities, gender,race, color, and lighting conditions. Fusion of differ-
ent kind of features has been adopted in the following
works: [ 15], [19], [22], [29 ], and [30]. Yu et al. [29]
introduced a new dimensionality reduction technique basedon spectral embedding for fusion of multiple features.Zavaschi et al. [30] proposed an ensemble of classiÔ¨Åers using
LBP and Gabor features. They further use multiobjectivegenetic algorithm (MOGA) based on accuracy and sizeof ensemble. Both LBP and Gabor Ô¨Ålter-based features
give appearance or texture information, while geometric
or shape information is missing in their work. Moreover,use of MOGA makes the processing very slow due to the
involvement of Gabor features which is very high dimensional
data. Senechal et al. [19] introduced a method of combining
geometric features (coefÔ¨Åcients of AAM) and local Gabor
binary pattern histograms using multikernel learning (MKL)
algorithm. They use one MKL SVM algorithm for each AU.Again, use of Gabor Ô¨Ålters leads to higher computation cost
during training, as for every face image, they are generating
3√ó6 images with three spatial frequencies and six orien-
tations. Liu et al. [15] integrated low level (geometric and
appearance) and high level features (eyebrow gesture and headnods, head shakes events) through multiscale, spatio-temporal
analysis for recognition of nonmanual grammatical markers
in American Sign Language.
C. Proposed Work in the Context
In the introduction, it has been emphasized that human cog-
nition makes use of both geometric and appearance features
to categorize facial expressions. It is found that the fusion
of geometric and appearance features that can provide a low-
dimensional meaningful feature space has not been explored
in depth in the literature. This paper solves this importantproblem using two layers of autoencoders in a deep-network
framework for the Ô¨Årst time. In addition, the proposed deep
network includes an SOM-based classiÔ¨Åer that Ô¨Ånds emotioncategories from the meaningful feature space obtained from
the autoencoders. This SOM-based classiÔ¨Åer is the improvedversion of our previous work [ 11]‚Äîthis previous work was
based
on 26-D geometric feature vector. Introduction of the
soft threshold logic instead of hard thresholding and theimprovement in the learning algorithm are twofold modiÔ¨Å-
cation in the present classiÔ¨Åer as compared to the previouswork [ 11]. This paper makes another fundamental contribu-
tion in automatic LBP-based feature extraction, where LBP isapplied to the four key regions of the face. This approach thushandles redundant information in a face in a computationally
effective manner while preserving local location information.
The proposed approach has thus distinct advantages over [ 31]
in terms of generating a low-dimensional feature vector.
III. P
ROPOSED FACIAL EXPRESSIONS
RECOGNITION SYSTEM
The proposed AFERS algorithm involves the following
steps: geometric features extraction, regional LBP features
extraction, data fusion using autoencoders, and SOM-based
classiÔ¨Åer.
A. Geometric Feature Extraction
A geometric feature vector xg‚àà R22consisting of eye
projection ratios and directional displacement information of
facial points is used in this paper. Fig. 2shows an automatic
extraction of four key facial regions using facial geometricinformation. The steps for extracting geometric features froma given image are as follows.
1) Detect face, Get face height H
face.
2) Detect both the eyes.
3) Locate eyes‚Äô centers (x1,y1)and(x2,y2). Estimate eyes‚Äô
region which also include eyebrow regions. Get the eye
region height.
4) Using the eyes‚Äô center and the face height, estimate the
nose and lips regions.
In successive frames, the facial regions are detected and the
features are extracted from each detected region. In a video
sequence, where either face detection algorithm or eye detec-tion algorithm fails, the previous frame‚Äôs information is used to
estimate the location of face and eyes‚Äô regions. This approach
increases the detection accuracy of key facial regions and morevideo frames have been included in this paper which were dis-
carded previously in our earlier work [11]. The Algorithm 1
below explains how the facial regions are detected in the
successive frames. Once the key regions are detected, the
eyes, eyebrows, nostrils, and lips are segmented using various
vision-based algorithms proposed in [11 ]. After segmenting
the regions, the features are extracted from them. The eye fea-
tures are extracted by taking the ratio of horizontal and vertical
projection of the segmented eye region that gives degree of an
eye opening. In this paper, we consider that every sequence
starts with a neutral expression and the Ô¨Årst frame is con-sidered as the reference frame. The facial landmarks whose
directional displacement features are to be calculated.
1) Left corner point, right corner point, and upper mid point
of left and right eyebrows, totally six points with 12displacement information along xandydirection.
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
4 IEEE TRANSACTIONS ON CYBERNETICS
Algorithm 1 Facial Regions Detection in Successive Frames
1:forframe i‚Üê1t o Ndo
2: Detect face using Viola Jones‚Äô [ 32] object detection algorithm.
3: Located both left and right eyes using using Viola Jones‚Äô object
detection algorithm.
4: ifThe face and eyes are detected in the Ô¨Årst frame then
5: Assign the detected face and eye regions as Rface
dectandReyedect
6: Detect the face in the next frame.
7: ifFace is not detected then
8: Assign the estimated face region Rface
estm as the previous frame‚Äôs
face ROI (region of interest) Rface
dect.
9: end if
10: Estimate the regions of eyes using the eyes‚Äô ROI of the previous
frame. Reye
estm=Reye
dect11: Apply the Viola Jones‚Äô object detection algorithm to detect both
the eyes.
12: ifEither or both the eyes are not detected then
13: Assign the estimated regions detected eyes‚Äô region.
14: end if
15: Once the eyes‚Äô regions are set, apply the eye segmentation
algorithm proposed in our previous work [ 11].
16: The eyebrows and lips regions are estimated based on the location
of eyes‚Äô centers and face height as explained in our previous work [ 11].
17: else Discard the video sequence and start with new video sequence.
18: end if
19:end for
Fig. 1. Selective AAM landmarks to extract geometric feature vector from
CK+DB.
2) Left corner point, right corner point, upper mid point,
and lower mid point of lips. These four points again give
eight displacement information.
Along with the displacement information of each key facial
points, these features also maintain the direction and location
information of the features. With reference to our previous
work [ 11], the feature vector dimension has been reduced by
four. It has been observed, that the upper and lower mid points
of the eyebrows contain almost same information. Thus, to
reduce the unnecessary dimension, we drop information oflower mid points of both the eyebrows. The landmark points
detection approach for MMI DB involves color-based tech-
niques which are described in detail in [ 11]. However, the
CK+ DB contains a large number of gray scale images which
are associated with 68 landmarks. We extract the similar geo-metric features using some of the facial landmarks labeled in
the DB. Fig. 1is a pictorial description of the facial landmarks
chosen for extracting 22-D geometric features. In CK +DB,
every sequence starts with a neutral expressions and ends withpeak expressions. Using this concept, we choose the landmarksFig. 2. Estimation of key facial regions using face height and eyes‚Äô location.
Given the face height Hface, eyes‚Äô center (x1,y1), (x 2,y2)and eyes‚Äô height,
the expected regions of the nose, eyebrows, and lips are calculated.
of the Ô¨Årst frame as the reference and the 22-D features are
extracted in each of the successive images as follows.
1) The differences between (x,y)coordinates of landmark
points on the reference frame and successive frames arecalculated for eyebrows and lips features.
2) For eyes, the distance between the ycomponent of upper
and lower points are considered as horizontal projection(H
proj)and distance between xcomponent of two corner
points are considered as vertical projection (Vproj).
3) The projection ratio is given as (Hproj/Vproj), which is
considered as the feature for the eye.
B. Automatic Extraction of Regional LBP Features
LBP has been considered as a powerful texture features
extraction technique since its introduction. Ojala et al. [18]
proposed the LBP for texture features extraction. The basicassumption was that texture has locally two complementary
aspects: 1) pattern and 2) strength [33 ]. Given a gray scale
image I(x,y), with intensity value g
c=I(x,y)at the location
(x,y), the LBP is calculated as follows. For an evenly spaced
circular neighborhood with Psampling points and radius R
around the center pixel (x,y), the gray value of pixel at the
pth sampling point is gp, given by
gp=I/parenleftbig
xp,yp/parenrightbig
,p=0,..., P‚àí1( 1 )
where xpand ypare the coordinate values of the sampling
point. The thresholding binary operator S(gp‚àígc)can be
given as
S/parenleftbig
gp‚àígc/parenrightbig
=/braceleftBigg
1i f gp‚â•gc
0 otherwise.
Then the LBP P,Rfeatures are computed by taking deci-
mal equivalent of the Pbit binary operator S(gp‚àígc).
Ojala et al. [18] proved that only a subset of the 2Ppatterns
extracted using basic LBP operator is sufÔ¨Åcient to describe
most of the texture information within the image. In thispaper, we use uniform patterns in-stead of basic LBP patterns.
Further explanation of the LBP is available in [ 34]. When the
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
MAJUMDER et al. : AFERS USING DEEP NETWORK-BASED DATA FUSION 5
Fig. 3. Examples of resultant LBP images of four basic facial regions. The
upper row shows the facial regions and lower row shows the corresponding
LBP images.
LBP histogram of dimension 59 is extracted from the whole
face image, the features encode only the presence of micro
patterns. No information about their location is available.
Moreover, when the whole face is considered, it introducesunwanted nonlinearities to the data which may create difÔ¨Åcul-
ties in learning a good classiÔ¨Åer. To overcome the problem,
we introduce a concept of four key regions of the face whichare detected automatically by our algorithm. The regions also
cover all the possible subregions. The four key regions are:
1) left eye region including left eyebrow and a part of fore-head; 2) right eye region including right eyebrow and a part of
forehead; 3) nose region containing surrounding cheek regions;
and 4) the lips region containing cheek and chin regions.The regions are extracted using the basic facial geometry.
LBP are extracted from all the four basic facial regions. Each
region (R
1,...R4)contributes a uniform LBP feature vector
of dimension 59. When all the feature vectors are concatenated
in order, we obtain a new feature vector of dimension 236, that
preserves approximate location and shape information of the
facial region. The concatenated histogram can be deÔ¨Åned as
Hk,rj=/summationdisplay
x,yS{LBP(x,y)=k} (2)
where rjis the region j‚àà{1,2,3,4}and kis the bin,
k‚àà{0,..., 58}.F i g . 3shows an example of four key facial
regions and the corresponding LBP images.
C. Data Fusion Using Autoencoders
An autoencoder is an unsupervised learning algorithm
which applies back-propagation method to approximate the
input feature vector. The main purpose of the autoencoder is
to learn a better representation of a feature set in a compressed
form. Fig. 4shows an example of an autoencoder. For a given
input vector x=x1,x2,..., x6, the autoencoder tries to learn
a function such that the network output h(x) approximates the
input vector x. The hidden representation of xis given by ÀÜh(x)
ÀÜhj(x)=f/parenleftbig
aj(x)/parenrightbig
where aj(x)=bj+/summationdisplay
kWjkxk (3)
ÀÜhj(x)is the output of the hidden unit jafter applying activa-
tion function f(.). Generally, the activation function f(.)is a
sigmoid function. The reconstructed output ÀÜxis obtained by
using a decoding function. It can be given as
ÀÜx=g/parenleftbig
ÀÜak/parenrightbig
where ÀÜak=ck+/summationdisplay
jW‚àó
jkÀÜhj(x). (4)Fig. 4. Architecture of an autoencoder with single hidden layer.
Fig. 5. Proposed data fusion technique based on autoencoders.
Again, the function g(.) is mostly chosen as a sigmoid func-
tion. It is empirically observed [ 24] that after training RBM,
W‚àóbecomes equivalent to WT. In the decoding part of the
autoencoder W‚àóis set to WTwhich is similar to RBM [35 ].
This approach does not allow the network to learn a triv-ial identity function. Choosing W
‚àó=WTalso gives better
results even if the number of hidden nodes is larger than theinput nodes. The parameters b
jand ckare set to 1. In the
proposed approach, we train the model with geometric and
regional LBP features individually using two separate autoen-
coders. The represented features of Ô¨Årst layer are concatenatedand given as input to the autoencoder of the second layer. The
Ô¨Ånal represented features of the second layer (output of the
hidden nodes at second layer) are considered as the fused fea-
ture. Fig. 5is a pictorial illustration of proposed data fusion
technique. In the Ô¨Årst step geometric features x
g‚ààR22and
appearance features xl‚ààR236are separately passed through
two different autoencoders. In the next step the hidden lay-
ers‚Äô outputs of two autoencoders are concatenated and passedthrough another autoencoder. Output of the hidden unit jat
Ô¨Årst layer for the geometric feature vector x
gand regional
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
6 IEEE TRANSACTIONS ON CYBERNETICS
LBP feature vector xlcan be calculated as
ÀÜhz
j(x)=1
1+e‚àíaz
j(x)(5)
where
az
j(x)=bzj+/summationdisplay
kWz
jkxk (6)
where azis the hidden unit‚Äôs output at node jbefore applying
the activation function. zis the data type (either geomet-
ric type or regional LBP type), bzjis the bias term for the
feature vector x. In case of the geometric features the input
feature vector is xgand for the regional LBP features it is xl.
Considering ÀÜhg‚ààRMas the output of the hidden layer for the
geometric feature vector xgandÀÜhl‚ààRNas the output for the
regional LBP feature xlafter applying the activation function,
the concatenated feature vector x2(which is an input vector
for the second layer) is hence given by
x2=/bracketleftBig
ÀÜhg
1,ÀÜhg2,...,ÀÜhg
M,ÀÜhl
1,ÀÜhl2,...,ÀÜhl
N/bracketrightBig
(7)
where Mand Nare the total number of hidden nodes of
the autoencoders for geometric and regional LBP features,
respectively. The Ô¨Ånal fused feature vector
x=ÀÜhf(x2) (8)
is generated using (5). In an autoencoder the weights and bias
are updated using stochastic gradient descent algorithm. The
update rules for the weight W(l)and the bias b(l)at layer l
of the network with a training sample {x(i),y(i)}where i‚àà
(1,2,3,...,M )is given as
W(l)(i+1)=W(l)(i)‚àíŒ±‚àÇ
‚àÇW(l)(i)J/parenleftBig
W(l),b(l);x(i),y(i)/parenrightBig
(9)
b(l)(i+1)=b(l)(i)‚àíŒ±‚àÇ
‚àÇb(l)(i)J/parenleftBig
W(l),b(l);x(i),y(i)/parenrightBig
(10)
where the J(W,b;x(i),y(i))is the cost function with respect
to a single training example (x(i),y(i)). The cost function is
deÔ¨Åned as
J/parenleftBig
W(l),b(l);x(i),y(i)/parenrightBig
=1
2/bardblW(l)Th(x)‚àíx/bardbl2‚àíŒª
2/summationdisplay
j/summationdisplay
k/parenleftBig
w(l)
jk/parenrightBig2
(11)
where the Ô¨Årst term is the sum square error term [error betweentheÀÜx=W
Th(x) and x, the input at layer l‚àí1]. The sec-
ond term is regularization term or the weight decay term. The
parameter Œªis the weight decay parameter. The readers can
refer [36] for more details about the parameters update rule.Once the fused feature vector ÀÜh
f(x)is obtained after train-
ing the autoencoders, the fused feature vector is classiÔ¨Åedinto six basic expressions using both SOM-based classiÔ¨Åerand SVM. The SOM-based classiÔ¨Åer which is an improvement
over our previous work [ 11] is described in Section III-D .W e
have used the library [ 37] for implementing the autoencoder
where an exercise for implementing sparse autoencoder (usingMATLAB) is given.Fig. 6. SOM-based classiÔ¨Åcation technique. In the SOM, each node
represents the local characteristics of the input feature space, where weight
vector wŒ≥, matrix AŒ≥, and bias yŒ≥are the related parameters for the node Œ≥.
D. SOM-Based ClassiÔ¨Åer
ClassiÔ¨Åcation techniques based on SOM have been cate-
gorized into two different kinds: 1) hard classiÔ¨Åcation [ 38]
and 2) soft classiÔ¨Åcation [39], [40 ]. Linear vector quanti-
zation (LVQ) and K-nearest neighbor methods are mainly
used as a precursor to SOM for hard classiÔ¨Åcation tech-
niques. However, one major disadvantage of the LVQ-based
approach is that, it needs to generate codebook vectors fora given problem which is often very difÔ¨Åcult. In case of
K-nearest neighbor-based approach, the input vector is classi-
Ô¨Åed to a category which has the highest likelihood for each
reference vector. It is difÔ¨Åcult to directly estimate the likeli-
hood for a category by computing the probability from the
n-dimensional region to which the input vector belongs [ 38].
Hard classiÔ¨Åcation techniques are thus gradually taken over bysoft classiÔ¨Åcation techniques. Li and Eastmann [ 40] proposed
an SOM-based soft classiÔ¨Åcation technique using two non-
parametric algorithms, called SOM commitment and SOM
typicality. However, the approach again has some limitations:it gives the same soft answer to all the pixels associated with
one node and it does not take into account the distance infor-
mation between the labeled pixels and the prototype of eachnode. Other variant of SOM-based classiÔ¨Åcation techniques
can be found in [41 ] and [ 42]. The proposed SOM-based
classiÔ¨Åer as presented in this section is a soft classiÔ¨Åcation
technique which is an improvement of our previous work [ 11].
The SOM [ 43] is a topology preserving network, where the
neural lattice discretizes the entire feature space to a Ô¨Ånitenumber of small discrete zones, each zone is represented by a
single neuron as shown in Fig. 6. The emotional feature space
is discretized in terms of Kohonen 2-D lattice of size M√óN.
Given a fused feature vector xcorresponding to a facial
image, the output of the classiÔ¨Åer yrepresents the emotion
class, such as happiness, sadness, disgust, anger, surprise,and fear. yis thus a 6-D vector. Happiness is represented as
y=[1‚àí1‚àí1‚àí1‚àí1‚àí1]
Twhere Ô¨Årst term is ON while all
other terms are OFF. Other facial categories are representedin a similar manner. For example, in case of sadness, the sec-
ond term will be ON while all other terms will be OFF. Thefused feature vector as derived from the autoencoders is a
230-D vector, i.e., x‚ààR
230. The input‚Äìoutput mapping from
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
MAJUMDER et al. : AFERS USING DEEP NETWORK-BASED DATA FUSION 7
the facial fused feature space to emotion categories can be
mathematically expressed as
y=f(x),y‚ààR6;x‚ààR230. (12)
This highly nonlinear mapping can be linearized using Ô¨Årstorder Taylor series expansion around each neuron. Given thateachŒ≥th neuron is associated with a vector w
Œ≥‚ààR230in the
lattice, the linear model associated with this neuron can beexpressed as
y
out
Œ≥=yŒ≥+A/parenleftbig
x‚àíwŒ≥/parenrightbig
where, AŒ≥=‚àÇf
‚àÇx|x=w Œ≥‚ààR6√ó230
yŒ≥=f/parenleftbig
wŒ≥/parenrightbig
(ideally ). (13)
In the SOM-based classiÔ¨Åer, neighbors participate in deci-
sion making although winning neuron has the bigger say. The
collective response of the network given an input xis given as
yout=/summationtextM√óN
Œ≥=1hi,Œ≥yout
Œ≥/summationtextM√óN
Œ≥=1hi,Œ≥(14)
where hi,Œ≥is the neighborhood function for the best matching
unit (BMU) iand the neuron Œ≥.T h eB M U iis obtained by
i=arg min
Œ≥/bardblx‚àíwŒ≥/bardbl. (15)
In this approach, the collective response model (14 )i s
expressed using locally valid linear models associated with
each neuron Œ≥. This model ( 14) thus approximates the global
nonlinear mapping y=f(x)by learning local linear map-
ping. This approach has twofold beneÔ¨Åts‚Äîfast learning andbetter accuracy. The learning problem is to update param-
eters w
Œ≥,yŒ≥,AŒ≥given training pairs {x,yd}. Neighborhood
function hi,Œ≥(n)and the spread œÉ(n)fornth iteration are cal-
culated as in (14) and(18) of our previous work [11]. A
sigmoid activation function is used at each node of the net-
work to classify the input feature vector into six fundamental
expressions. Instead of setting a hard threshold value [ 11]f o r
discrete outputs from the network, we indeed Ô¨Ånd the greatest
value Grover all the sigmoid function (tan hyperbolic) outputs.
The node which achieves the Gr, is set to 1 and the rest are
set to ‚àí1. Essentially, the node that has given the maximum
activation is chosen as the recognized class. Let the sigmoid
function output of kth class is denoted by ysig
k
ysigk=eyout
k‚àíe‚àíyoutk
eyout
k+e‚àíyoutk(16)
ifysig
k=Gr,then yk=1
else yk=‚àí1 (17)
This may be described as a winner dominates policy. In [ 11],
the hard-threshold was set to 0.5. It was observed that the
response of multiple output nodes crossed this hard-thresholdin many occasions. It was also observed that sometimes no
response at the output nodes crossed this hard-threshold. By
incorporating the proposed approach as shown in (17), the
model guarantees that only one response at the output nodes
will be ON while the rest will be OFF. This feature of theproposed SOM-based classiÔ¨Åer is necessary as every facial
image in the considered two-DBs is labeled to only onecategory of emotion. It has been experimentally observed
that parameter updates based on the minimization of least
square error provides better accuracy, we have considered thefollowing least square cost function:
E=
6/summationdisplay
k=1Ek=1
26/summationdisplay
k=1/parenleftBig
yd
k‚àíysig
k/parenrightBig2
. (18)
The update of the parameter wŒ≥is retained same as in SOM,
which is given as [ 11, eq. (15)]. To update the parameters AŒ≥
andyŒ≥the steps are given below. The derivatives of the cost
function with respect to the parameters at each output node k
are given as
‚àÇE
‚àÇyŒ≥=‚àí/parenleftBig
yd
k‚àíysig
k/parenrightBig
hi,Œ≥
/parenleftBig/summationtextM√óN
Œ≥=1hi,Œ≥/parenrightBig/parenleftbigg
1‚àí/parenleftBig
ysigk/parenrightBig2/parenrightbigg
(19)
‚àÇE
‚àÇaŒ≥=‚àí/parenleftBig
yd
k‚àíysig
k/parenrightBig
hi,Œ≥/parenleftbig
x‚àíwŒ≥/parenrightbig
/parenleftBig/summationtextM√óN
Œ≥=1hi,Œ≥/parenrightBig/parenleftbigg
1‚àí/parenleftBig
ysigk/parenrightBig2/parenrightbigg
(20)
where aŒ≥is the kth row of the matrix Aat neuron Œ≥in
the SOM network. The update of parameters yŒ≥and AŒ≥
are as given in (25)‚Äì(27) of our previous work [11]. The
proposed Kohonen SOM-based classiÔ¨Åer uses the combined
beneÔ¨Åt of both supervised and unsupervised learning algo-
rithms. The parameters update laws in (19 ) and ( 20) show that
target error (yd
k‚àíysig
k)(supervised) as well as neighborhood
index hi,Œ≥(unsupervised) inÔ¨Çuence the parameters updates.
The update of the parameter wŒ≥for each neuron Œ≥within the
neighborhood hi,Œ≥is done in unsupervised approach, whereas,
the parameter updates for yŒ≥andAŒ≥use both supervised and
unsupervised terms. Thus, the network is a combination of
both supervised and unsupervised techniques. This is our novel
contribution in development of the SOM-based classiÔ¨Åer.
IV . E XPERIMENTAL RESULTS
Experiments are performed on CK +[44] and MMI [ 45]D B s .
Six basic expressions deÔ¨Åned earlier are used for both the DBs.CK+ DB comes with labeled emotions and corresponding 68
landmarks tracked using AAM [ 13]. There are 327 sequences
in total in the DB. Each sequence has one of the seven expres-sions (anger, contempt, disgust, fear, happiness, sadness, andsurprise). The sequence starts with a neutral expression and
ends with peak expression. It has been observed that in some
of the image sequences, the landmark Ô¨Åle is missing. Also insome of the images, emotion labeling is missing. Since we
have considered six basic emotions, such as happiness, sad-
ness, disgust, anger, surprise, and fear in MMI DB, we take
only those images from CK+ DB which have one of these
six expressions. Some of the initial images in a sequence are
almost but not totally neutral; to eliminate those expressions,
we set a threshold on the norm of the feature vector for the
geometric feature vector. Only the corresponding images areused for extracting LBP feature vector. Total 3718 images con-
taining 123 subjects are used from CK+ DB. In the MMI DB,
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
8 IEEE TRANSACTIONS ON CYBERNETICS
not all the video clips start with a neutral expressions. Since
we need at least the Ô¨Årst frame to be of neutral expressionsto compute geometric features, we take only those video clips
which have one of the six emotions labeled and neutral frame
in the beginning. Thus, from MMI DB, we use 20 differentsubjects with 96 image sequences. The face image from MMI
and CK+ DBs are normalized to a standard scale (240√ó240)
and(300√ó300), respectively, after the face detection is per-
formed. The normalized face is then transformed to removeslight orientation changes. The angle for rotation is obtained
using two eyes‚Äô centers and the nose tip detected automati-cally [11]. To remove any kind of biases and to bring the feature
vectors into a common scale, both the appearance and the geo-
metric features are normalized using standard normalization
techniques. All the experiments are performed with tenfold
cross validation. To be more precise, the dataset is partitionedinto ten equally sized groups. Only one group is considered
as testing set while remaining nine groups are considered as
training set. This process is repeated for all the ten sets. Theconfusion matrices of all the ten sets are averaged to get the
Ô¨Ånal confusion matrix. Details of experimental results for both
CK+ and MMI DBs are presented below.
A. Experimental Results on MMI and CK+ Databases
The proposed data fusion technique presented in
Section III-C is used to obtain fused feature vector.
The number of hidden units for geometric feature vector in
both MMI and CK+ DBs is chosen as 20. In case of LBP
feature vector, the hidden units for CK +and MMI DBs
are set to 170 and 150, respectively. In the second layer of
the autoencoder the number of hidden units for both CK +
and MMI DBs is chosen as 230. The choice of number of
hidden units at each layer is based on parameters tuning. The
best performance is obtained using these above mentionedparameter values. An SOM-based network of size 10 √ó8,
with the initial learning rate Œ∑
I=0.9 and Ô¨Ånal learning rate
Œ∑F=0.005 is used for the experimental purpose. In case
of SVM we use LibSVM for the experiment. The approach
is one-against-one and the kernel is taken as RBF, number
of generation used is 10 and Gamma =0.1. We also tested
with varying parameters and with different kernels types
(polynomial and linear). Among them, the best performance
is obtained using RBF kernel for the above stated parameters.Recognition accuracies of both CK +and MMI DBs for all
the types of data, are shown as confusion matrices in tabularform.
1) Results of MMI Database: Table Ishows the confusion
matrix of MMI DB for the 22-D geometric facial features
when classiÔ¨Åed using the SOM-based classiÔ¨Åer. An aver-
age recognition accuracy of 91 .17% is obtained with highest
recognition accuracy 96.67% in case of anger and lowestaccuracy 86.13% in case of sadness. Table IIshows the con-
fusion matrix of MMI DB for regional LBP features. Results
shows an average recognition accuracy of 90.3% with high-
est recognition accuracy 96 .3% for happiness and 82.34% for
disgust.
The confusion matrix of MMI DB for fused data is shown
in Table IV. An average recognition accuracy of 97.55% withTABLE I
SOM R ESULTS FOR 22-D G EOMETRIC FEATURES (MMI DB)
TABLE II
SOM R ESULTS FOR 236-D LBP F EATURES (MMI DB)
TABLE III
SVM R ESULTS FOR 230-D F USED FEATURES (MMI DB)
highest recognition accuracy 98 .71% in case of surprise and
lowest 96 .28% in case of anger is achieved. It is observed
that classiÔ¨Åcation from the fused data outperforms that from
the individual features in terms of average recognition accu-
racy. It exceeds geometric-based features by 6.38 and regionalLBP features by 7.28%. The confusion matrix of the fused
feature vector of MMI DB using SVM classiÔ¨Åer is given
in Table III. We observe an average recognition accuracy
of 93.2%. When tested on individual feature vectors of, the
SVM shows 89.75% and 90.05% average recognition accu-
racies for geometric and regional LBP features, respectively.The plots shown in Fig. 7summarizes the SOM -based recog-
nition results all the three types of features. The plots clearlyshow the superiority of fused features recognition results overthe two types of individual features.
2) Results of CK+ Database: The confusion matrices for
the 22-D geometric features and 236-D regional LBP fea-tures for the CK +DB are shown in Tables VandVIbelow.
It is observed that using only geometric-based features, we
obtain an average recognition accuracy of 96.06% which out-
performs the average recognition accuracy of regional LBP
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
MAJUMDER et al. : AFERS USING DEEP NETWORK-BASED DATA FUSION 9
TABLE IV
SOM R ESULTS FOR 230-D F USED FEATURES (MMI DB)
Fig. 7. Comparison between SOM-based recognition results of MMI DB for
different types of features.
TABLE V
SOM R ESULTS FOR 22-D G EOMETRIC FEATURES (CK+DB)
features of dimension 236 by 2.47%. The geometric-based fea-
tures extracted for CK+ DB are based on AAM. The regional
LBP features gives an average recognition accuracy of 93.59%
with highest recognition accuracy 96. 64% in case of surprise
and lowest recognition accuracy 84.09% in case of fear. The
geometric-based approach gives highest recognition accuracy
of 98.17% in case of surprise and 94 .89% in case of happi-
ness. The confusion matrix of fused features using CK+ DB
is tabulated in Table VIII. It is observed that the recognition
accuracy of fused features is higher than the individual fea-
tures. An average recognition accuracy of 98 .95% is achieved
with highest recognition accuracy 99.44% in case of happi-
ness and a lowest recognition accuracy of 98 .54% is obtained
in case of fear.TABLE VI
SOM R ESULTS FOR 236-D LBP F EATURES (CK+DB)
TABLE VII
SVM R ESULTS FOR 230-D F USED FEATURES (CK+DB)
TABLE VIII
SOM R ESULTS FOR 230-D F USED FEATURES (CK+DB)
TABLE IX
AVERAGE RECOGNITION ACCURACY IN %U SING SOM
Table VIIshows the recognition accuracies of the fused fea-
tures for CK+ DB when classiÔ¨Åed using SVM. We observe
an average recognition accuracy of 95 .01% for CK+ DB.
When tested on individual feature vectors of CK +DB, the
SVM shows average recognition accuracies of 93 .5% and
89.3% for geometric and regional LBP features, respectively.
Fig. 8summarizes the recognition performance of the SOM-
based classiÔ¨Åcation for all the three types of features. The
plots clearly shows that the fused feature leads the individual
features.
3) Extensive Performance Validation: To validate the
performance of the proposed approach, we did several
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
10 IEEE TRANSACTIONS ON CYBERNETICS
Fig. 8. Comparison between SOM-based recognition results of CK +DB
for different types of features.
TABLE X
AVERAGE RECOGNITION ACCURACY IN %U SING SVM
experimental studies. First, tested on concatenated feature vec-
tor that simply concatenates the 22-D geometric feature vector
and the 236-D LBP feature vector. Both the features were nor-
malized before concatenation. The randomized concatenated
feature vector is classiÔ¨Åed using the SOM-based classiÔ¨Åer. The
average recognition results of both the DBs given in Table IX
show that the performance of the proposed fused feature vec-
tor is signiÔ¨Åcantly better than just simple concatenated feature
vector. Several experiments are carried out to demonstrate thevarying performances for different number of hidden nodes
at the last layer. The testing results for both MMI and CK +
DBs are given as supplementary material due to page limita-
tion. The performances with more hidden layers are also being
analyzed, but did not Ô¨Ånd any signiÔ¨Åcant improvement over
the stated approach; in-fact the performance deteriorates onincreasing the number of hidden layers. For CK+ and MMI
DBs we tested with one more hidden layer having 230 nodes.The other parameters are kept same (for both MMI and CK +
DBs) as those stated earlier in this paper for the best fusion
results. We observed an average recognition accuracy of 89 .3%
for MMI DB and 92.54% for CK+ DB. The plot shown in
Figs. 7and8summarizes the recognition performances of all
the four types of features (geometric features alone, regional
LBP features alone, fused features, and concatenated features).
In a nutshell we can see the average recognition accuracies
of SOM and SVM classiÔ¨Åers in Tables IXand X, respec-
tively. It is observed that the recognition accuracy of fused
features is better than both geometric-based and regional LBP
features. We also observe that the recognition accuracy of theconcatenated features is much lower than the individual fea-
tures when used alone. A comparative study of the proposedTABLE XI
PERFORMANCE COMPARISON OF FUSED FEATURE VECTOR TAKEN
FROM MMI AND CK+DBSWITHEXISTING WORK
TABLE XII
SOM R ESULTS FOR 22-D G EOMETRIC FEATURES (MMI DB)
USING THRESHOLD AS 0.5 ATNETWORK OUTPUT
fusion technique with some recent research work have been
presented in Table XI. In this paper, the dataset taken from
MMI DB, uses 20 subjects containing 96 video sequences,
whereas only 12 subjects containing 81 video sequences were
used in our previous study [ 11]. To have a fair compari-
son with our previous work [ 11] we perform the following
experiment. This comparison uses the present dataset, takenfrom MMI DB with 20 subjects. The geometric feature vec-
torx
g‚ààR22is used to train the SOM-based model proposed
in our previous work [11]. Table XIIgives the recognition
results in the form of confusion matrix. Table Ishown pre-
viously, presents a confusion matrix for the same geometric
feature vector xg‚ààR22using the proposed soft thresholding
technique. Tenfold cross validation technique is used for the
experiments. It is observed that, application of the proposed
soft thresholding method at each of the output nodes improvesthe recognition performance. An average recognition accuracy
of 87.55% is achieved when hard threshold is applied. On
application of the soft thresholding, the recognition accuracyis 91.17%, the improvement is quite signiÔ¨Åcant.
V. C
ONCLUSION
This paper has presented a deep-network-based AFERS,
where Ô¨Årst two layers of autoencoders effectively fuse two
entirely different kinds of features (geometric and appear-
ance) for better representation of the facial data. Effectivefusion of two different kind features using auto-encoders as
presented in this paper is Ô¨Årst of its kind. The SOM-based
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
MAJUMDER et al. : AFERS USING DEEP NETWORK-BASED DATA FUSION 11
classiÔ¨Åer in the third layer combines beneÔ¨Åts of both super-
vised and unsupervised learning algorithms. This classiÔ¨Åer isan improved version of the previous work [ 11]. It is observed
that the use of hard thresholding technique [ 11] sometimes
misleads the class prediction. The proposed soft threshold-ing criteria presented in this paper deals with this problem
effectively. The mathematical principle of modeling a non-
linear mapping in terms of locally valid linear mappings asgiven in ( 14) has been provided in this version. Parameter
update laws have been modiÔ¨Åed while minimizing a least-square function. The learning steps as given in (19 ) and ( 20)
are different as compared to the previous work [ 11]. Another
important feature of the present deep-network architecture isthat it has a simple architecture consisting of three layers and
the network has to process the data of maximum dimension
of 258 √ó1.
The proposed approach is rigorously validated on two
widely used DBs, namely, MMI and CK +. The recogni-
tion results of fused data are compared with the recognitionperformance of individual features and also with that of
concatenated feature vector. It is evident that the recogni-
tion results using fused features outperforms the recognitionresults of individual features and the concatenated features.
Recognition performance of the proposed data fusion tech-
nique is also studied by putting SVM classiÔ¨Åer in place ofSOM-based classiÔ¨Åer. The performance of the fused features
using the proposed SOM-based classiÔ¨Åer gives signiÔ¨Åcant
improvement over the SVM, with an 3 .94% increase in recog-
nition accuracy for CK+ DB and 4.36% for MMI DB. It is
also observed that the proposed data fusion technique enhancesrecognition performance irrespective of the classiÔ¨Åcation tech-
nique used. The proposed facial expression recognition system
is found to be successful in recognizing each of the sixexpressions in all the experiments with a very high accu-
racy, however, its performance in real-life environment is
yet to be investigated. Real-life DBs, like LFW [ 51] can be
used to evaluate the performance of the proposed recogni-
tion system, once it is trained with existing labeled facial
expressions DBs. The presented geometric features extrac-tion approach considers the Ô¨Årst frame in every sequence
to be neutral expression, which may not be always true
in real-life DBs. In such scenerios, an average normalizedface can be used as reference frame to extract geometric
features. The proposed appearnace features are independent
of any reference frame and hence can be extracted directlyfrom automatically detected facial regions. Once the individ-
ual features are extracted, the proposed data fusion framework
can be used directly to generate better represented fusedfeature vector. The real-life images must be preprocessed
and normalized before extracting the features to make them
robust to various facial transformations. Facial planar rota-
tion can be corrected by calculating rotation angle between
two detected eye centers. Interested readers may refer [52 ]t o
deal with partial facial occlusions if any. An online train-
ing mechanism of the proposed data fusion framework can
also be explored for real-life applications, such as teach-ing a robot to learn facial expressions by imitating human
expressions [53].A
CKNOWLEDGMENT
The authors would like to thank the Council of ScientiÔ¨Åc
and Industrial Research, for providing scholarship support
toward the completion of this paper. They also would like
to thank S. Dutta for his valuable discussions and help in
implementing this paper.
REFERENCES
[1] A. Mehrabian, Nonverbal Communication . New Brunswick, NJ, USA:
Aldine, 2007.
[2] P. Ekman, W. V . Friesen, and J. C. Hager, Facial Action Coding System .
Salt Lake City, UT, USA: Human Face, 2002.
[3] L. Ma and K. Khorasani, ‚ÄúFacial expression recognition using construc-
tive feedforward neural networks,‚Äù IEEE Trans. Syst., Man, Cybern. B,
Cybern., vol. 34, no. 3, pp. 1588‚Äì1595, Jun. 2004.
[4] C.-T. Tu and J.-J. J. Lien, ‚ÄúAutomatic location of facial feature points
and synthesis of facial sketches using direct combined model,‚Äù IEEE
Trans. Syst., Man, Cybern. B, Cybern. , vol. 40, no. 4, pp. 1158‚Äì1169,
Aug. 2010.
[5] S. Moore and R. Bowden, ‚ÄúLocal binary patterns for multi-view facial
expression recognition,‚Äù Comput. Vis. Image Understand., vol. 115,
no. 4, pp. 541‚Äì558, 2011.
[6] P. G. Schyns, L. S. Petro, and M. L. Smith, ‚ÄúTransmission of facial
expressions of emotion co-evolved with their efÔ¨Åcient decoding in the
brain: Behavioral and brain evidence,‚Äù Plos One, vol. 4, no. 5, p. e5625,
2009.
[7] Y . Fu, L. Cao, G. Guo, and T. S. Huang, ‚ÄúMultiple feature fusion by sub-
space learning,‚Äù in Proc. Int. Conf. Content Based Image Video Retrieval ,
Niagara Falls, ON, Canada, 2008, pp. 127‚Äì134.
[8] G. E. Hinton, S. Osindero, and Y .-W. Teh, ‚ÄúA fast learning algorithm
for deep belief nets,‚Äù Neural Comput. , vol. 18, no. 7, pp. 1527‚Äì1554,
2006.
[9] C. Szegedy et al., ‚ÄúIntriguing properties of neural networks,‚Äù arXiv
preprint arXiv:1312.6199 , Dec. 2013.
[10] G. E. Hinton and R. R. Salakhutdinov, ‚ÄúReducing the dimensionality of
data with neural networks,‚Äù Science, vol. 313, no. 5786, pp. 504‚Äì507,
2006.
[11] A. Majumder, L. Behera, and V . K. Subramanian, ‚ÄúEmotion recogni-
tion from geometric facial features using self-organizing map,‚Äù Pattern
Recognit., vol. 47, no. 3, pp. 1282‚Äì1293, 2014.
[12] T. F. Cootes, C. J. Taylor, D. H. Cooper, and J. Graham, ‚ÄúActive shape
models-their training and application,‚Äù Comput. Vis. Image Understand.,
vol. 61, no. 1, pp. 38‚Äì59, 1995.
[13] T. F. Cootes, G. J. Edwards, and C. J. Taylor, ‚ÄúActive appearance
models,‚Äù in Computer Vision‚ÄîECCV98 . Heidelberg, Germany: Springer,
1998, pp. 484‚Äì498.
[14] T. Ojala, M. Pietik√§inen, and T. Maenpaa, ‚ÄúMultiresolution gray-scale
and rotation invariant texture classiÔ¨Åcation with local binary patterns,‚ÄùIEEE Trans. Pattern Anal. Mach. Intell. , vol. 24, no. 7, pp. 971‚Äì987,
Jul. 2002.
[15] J. Liu et al., ‚ÄúNon-manual grammatical marker recognition based on
multi-scale, spatio-temporal analysis of head pose and facial expres-
sions,‚Äù Image Vis. Comput. , vol. 32, no. 10, pp. 671‚Äì681, 2014.
[16] L. Zhong, Q. Liu, P. Yang, J. Huang, and D. N. Metaxas, ‚ÄúLearning
multiscale active facial patches for expression analysis,‚Äù IEEE Trans.
Cybern., vol. 45, no. 8, pp. 1499‚Äì1510, Aug. 2015.
[17] X. Yu et al., ‚ÄúIs interactional dissynchrony a clue to deception? insights
from automated analysis of nonverbal visual cues,‚Äù IEEE Trans. Cybern. ,
vol. 45, no. 3, pp. 492‚Äì506, Mar. 2015.
[18] T. Ojala, M. Pietik√§inen, and D. Harwood, ‚ÄúA comparative study of tex-
ture measures with classiÔ¨Åcation based on featured distributions,‚Äù Pattern
Recognit., vol. 29, no. 1, pp. 51‚Äì59, 1996.
[19] T. Senechal et al., ‚ÄúFacial action recognition combining heterogeneous
features via multikernel learning,‚Äù IEEE Trans. Syst., Man, Cybern. B,
Cybern., vol. 42, no. 4, pp. 993‚Äì1005, Aug. 2012.
[20] M. H. Siddiqi, R. Ali, A. M. Khan, Y .-T. Park, and S. Lee, ‚ÄúHuman facial
e 
xpression recognition using stepwise linear discriminant analysis and
hidden conditional random Ô¨Åelds,‚Äù IEEE Trans. Image Process., vol. 24,
no. 4, pp. 1386‚Äì1398, Apr. 2015.
[21] Y . Pang, H. Yan, Y . Yuan, and K. Wang, ‚ÄúRobust CoHOG feature extrac-
tion in human-centered image/video management system,‚Äù IEEE Trans.
Syst., Man, Cybern. B, Cybern. , vol. 42, no. 2, pp. 458‚Äì468, Apr. 2012.
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
12 IEEE TRANSACTIONS ON CYBERNETICS
[22] W. Liu and Z. Wang, ‚ÄúFacial expression recognition based on
fusion of multiple Gabor features,‚Äù in Proc. 18th Int. Conf. Pattern
Recognit. (ICPR), vol. 3. Hong Kong, 2006, pp. 536‚Äì539.
[23] M. F. Valstar, I. Patras, and M. Pantic, ‚ÄúFacial action unit detection
using probabilistic actively learned support vector machines on tracked
facial point data,‚Äù in Proc. IEEE Comput. Soc. Conf. Comput. Vis.
Pattern Recognit. Workshops (CVPR) Workshops , San Diego, CA, USA,
Jun. 2005, p. 76.
[24] H. Larochelle, Y . Bengio, J. Louradour, and P. Lamblin, ‚ÄúExploring
strategies for training deep neural networks,‚Äù J. Mach. Learn. Res. ,
vol. 10, pp. 1‚Äì40, Jan. 2009.
[25] P. Liu, S. Han, Z. Meng, and Y . Tong, ‚ÄúFacial expression recognition
via a boosted deep belief network,‚Äù in Proc. IEEE Conf. Comput. Vis.
Pattern Recognit. , Columbus, OH, USA, 2014, pp. 1805‚Äì1812.
[26] R. Zhi, M. Flierl, Q. Ruan, and B. W. Kleijn, ‚ÄúGraph-preserving sparse
nonnegative matrix factorization with application to facial expressionrecognition,‚Äù IEEE Trans. Syst., Man, Cybern. B, Cybern. , vol. 41, no. 1,
pp. 38‚Äì52, Feb. 2011.
[27] S. Lawrence, C. L. Giles, A. C. Tsoi, and A. D. Back, ‚ÄúFace recognition:
A convolutional neural-network approach,‚Äù IEEE Trans. Neural Netw.,
vol. 8, no. 1, pp. 98‚Äì113, Jan. 1997.
[28] W. Huang and H. Yin, ‚ÄúVisom for dimensionality reduction in
face recognition,‚Äù in Advances in Self-Organizing Maps. Heidelberg,
Germany: Springer, 2009, pp. 107‚Äì115.
[29] K. Yu, Z. Wang, M. Hagenbuchner, and D. D. Feng, ‚ÄúSpectral
embedding based facial expression recognition with multiple features,‚ÄùNeurocomputing, vol. 129, pp. 136‚Äì145, Apr. 2014.
[30] T. H. H. Zavaschi, A. S. Britto, Jr., L. E. S. Oliveira, and A. L. Koerich,
‚ÄúFusion of feature sets and classiÔ¨Åers for facial expression recognition,‚ÄùExpert Syst. Appl., vol. 40, no. 2, pp. 646‚Äì655, 2013.
[31] G. Zhao and M. Pietikainen, ‚ÄúDynamic texture recognition using local
binary patterns with an application to facial expressions,‚Äù IEEE Trans.
Pattern Anal. Mach. Intell. , vol. 29, no. 6, pp. 915‚Äì928, Jun. 2007.
[32] P. Viola and M. J. Jones, ‚ÄúRobust real-time face detection,‚Äù Int. J.
Comput. Vis. , vol. 57, no. 2, pp. 137‚Äì154, 2004.
[33] M. Pietik√§inen, A. Hadid, G. Zhao, and T. Ahonen, ‚ÄúLocal binary pat-
terns for still images,‚Äù in Computer Vision Using Local Binary Patterns ,
vol. 40. London, U.K.: Springer, 2011, pp. 13‚Äì47.
[34] C. Shan, S. Gong, and P. W. McOwan, ‚ÄúFacial expression recognition
based on local binary patterns: A comprehensive study,‚Äù Image Vis.
Comput. , vol. 27, no. 6, pp. 803‚Äì816, 2009.
[35] R. Salakhutdinov, A. Mnih, and G. Hinton, ‚ÄúRestricted Boltzmann
machines for collaborative Ô¨Åltering,‚Äù in Proc. 24th Int. Conf. Mach.
Learn. , Corvallis, OR, USA, 2007, pp. 791‚Äì798.
[36] A. Ng, ‚ÄúSparse autoencoder,‚Äù CS294A Lecture Notes , Stanford Univ.,
Stanford, CA, USA, 2011, p. 72.
[37] A. Ng. (2011). Sparse Autoencoder Exercise . Accessed on Jun. 30, 2014.
[Online]. Available: http://uÔ¨Çdl.stanford.edu/wiki/index.php/
Exercise:Sparse_Autoencoder
[38] Y . Ito and S. Omatu, ‚ÄúCategory classiÔ¨Åcation method using a self-
organizing neural network,‚Äù Int. J. Remote Sens., vol. 18, no. 4,
pp. 829‚Äì845, 1997.
[39] F. Giacco, C. Thiel, L. Pugliese, S. Scarpetta, and M. Marinaro,
‚ÄúUncertainty analysis for the classiÔ¨Åcation of multispectral satelliteimages using SVMs and SOMs,‚Äù IEEE Trans. Geosci. Remote Sens.,
vol. 48, no. 10, pp. 3769‚Äì3779, Oct. 2010.
[40] Z. Li and J. R. Eastman, ‚ÄúCommitment and typicality measures
for the self-organizing map,‚Äù Int. J. Remote Sens., vol. 31, no. 16,
pp. 4265‚Äì4280, 2010.
[41] H. Han, X.-L. Wu, and J.-F. Qiao, ‚ÄúNonlinear systems modeling
based on self-organizing fuzzy-neural-network with adaptive compu-
tation algorithm,‚Äù IEEE Trans. Cybern. , vol. 44, no. 4, pp. 554‚Äì564,
Apr. 2014.
[42] C.-H. Wang, C.-Y . Chen, and K.-N. Hung, ‚ÄúToward a new task assign-
ment and path evolution (TAPE) for missile defense system (MDS) usingintelligent
adaptive SOM with recurrent neural networks (RNNs),‚Äù IEEE
Trans. Cybern. , vol. 45, no. 6, pp. 1134‚Äì1145, Jun. 2015.
[43] T. Kohonen, ‚ÄúThe self-organizing map,‚Äù Proc. IEEE , vol. 78, no. 9,
pp. 1464‚Äì1480, Sep. 1990.
[44] P. Lucey et al., ‚ÄúThe extended Cohn‚ÄìKanade dataset (CK+): A
complete dataset for action unit and emotion-speciÔ¨Åed expression,‚Äù
inProc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.
Workshops (CVPRW) , San Francisco, CA, USA, 2010, pp. 94‚Äì101.
[45] M. Pantic, M. F. Valstar, R. Rademaker, and L. Maat, ‚ÄúWeb-based
database for facial expression analysis,‚Äù in Proc. IEEE Int. Conf.
Multimedia Expo (ICME) , Amsterdam, The Netherlands, Jul. 2005,
pp. 317‚Äì321.[46] Z. Wang, S. Wang, and Q. Ji, ‚ÄúCapturing complex spatio-temporal rela-
tions among facial muscles for facial expression recognition,‚Äù in Proc.
IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Portland, OR, USA,2013, pp. 3422‚Äì3429.
[47] M. Liu, S. Shan, R. Wang, and X. Chen, ‚ÄúLearning expressionlets on
spatio-temporal manifold for dynamic facial expression recognition,‚Äù inProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Columbus,OH, USA, 2014, pp. 1749‚Äì1756.
[48] J. Li and E. Y . Lam, ‚ÄúFacial expression recognition using deep neural
networks,‚Äù in Proc. IEEE Int. Conf. Imaging Syst. Techn. (IST) , 2015,
pp. 1‚Äì6.
[49] S. Elaiwat, M. Bennamoun, and F. Boussaid, ‚ÄúA spatio-temporal
RBM-based model for facial expression recognition,‚Äù Pattern Recognit. ,
vol. 49, pp. 152‚Äì161, Jan. 2016.
[50] A. Mollahosseini, D. Chan, and M. H. Mahoor, ‚ÄúGoing deeper in facial
expression recognition using deep neural networks,‚Äù in Proc. IEEE
Winter Conf. Appl. Comput. Vis. (WACV) , Lake Placid, NY , USA, 2016,
pp. 1‚Äì10.
[51] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller, ‚ÄúLabeled
faces in the wild: A database for studying face recognition in uncon-strained environments,‚Äù Dept. Comput. Sci., Univ. Massachusetts,Amherst, MA, USA, Tech. Rep. 07-49, 2007.
[52] K. Seshadri and M. Savvides, ‚ÄúTowards a uniÔ¨Åed framework for pose,
expression, and occlusion tolerant automatic facial alignment,‚Äù IEEE
Trans. Pattern Anal. Mach. Intell. , vol. 38, no. 10, pp. 2110‚Äì2122,
Oct. 2016.
[53] S. Boucenna, P. Gaussier, P. Andry, and L. Hafemeister, ‚ÄúA robot
learns the facial expressions recognition and face/non-face discrimi-nation through an imitation game,‚Äù Int. J. Soc. Robot. , vol. 6, no. 4,
pp. 633‚Äì652, 2014.
Anima Majumder received the B.Tech. degree from
the North Eastern Regional Institute of Science
and Technology, Itanagar, India, in 2005, and the
M.Tech. degree from the Shri Guru Govind SinghjiInstitute of Engineering and Technology, Nanded,India, in 2007.
She was a Senior Software Engineer with Robert
Bosch Engineering and Business Solutions Ltd.,Bengaluru, India. She is currently a Ph.D. Scholarwith the Department of Electrical Engineering,
Indian Institute of Technology Kanpur, Kanpur,
India. Her current research interests include computer vision, machine learn-
ing, and image processing.
Laxmidhar Behera (S‚Äô92‚ÄìM‚Äô03‚ÄìSM‚Äô03) received
the B.Sc. and M.Sc. degrees in engineering fromthe National Institute of Technology Rourkela,Rourkela, India, in 1988 and 1990, respectively,
and the Ph.D. degree from the Indian Institute of
Technology Delhi, New Delhi, India, in 1996.
He is currently a Professor with the Department
of Electrical Engineering, Indian Institute of
Technology Kanpur, Kanpur, India. He was a
Reader with the Intelligent Systems Research Center,University of Ulster, U.K., from 2007 to 2009. He
was also a Visiting Researcher/Professor with FHG, Germany, and ETH
Zurich, Z√ºrich, Switzerland. He has over 200 papers to his credit published
in refereed journals and presented in conference proceedings. His currentresearch interests include intelligent control, robotics, semantic signal/music
processing, neural networks, control of cyber-physical systems, and cognitive
modeling.
Venkatesh K. Subramanian received the B.E.
degree from Bangalore University, Bengaluru, India,
in 1987, and the M.Tech. and Ph.D. degrees from
the Indian Institute of Technology Kanpur (IITK),Kanpur, India, in 1989 and 1995, respectively.
He is currently a Professor with the Department
of Electrical Engineering, IITK. His current researchinterests include signal processing, image and videoprocessing, signal and system theory, and computer
vision with applications in robotics.
"
https://ieeexplore.ieee.org/document/6998925/,"1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation
information: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing
IEEE TRANSACTIONS ON  AFFECTIVE COMPUTING,   MANUSCRIPT ID  1 
 Automatic  Facial Expression Recognition  
Using Features of Salient Facial  Patches  
S L Happy , Student Member , IEEE , and Aurobinda Routray,  Member, IEEE  
Abstract ‚Äî Extraction of discriminative feature s from salient facial patches plays a vital role in effectiv e facial expression 
recognition. The accurate detection of  facial landmark s improves the localization of the salient patch es on face images . This paper  
propose s a novel framework for expression recognition by using appearance features of selected facial  patches . A few prominent  
facial  patches , depending on the position of facial landmarks,  are extracted  which are active during emotion  elicitation . The se 
active  patches are further processed  to obtain  the salient patches which contain discriminative features  for classification of  each 
pair of expressions , thereby selecting different facial patches as salient for different pair of expression classes . One-against -one 
classification method is adopted using these features . In addition, an automated learning -free f acial landmark detection technique  
has been proposed , which  achieve s similar performances as that of other state -of-art landmark detection methods , yet requires 
significantly less execution time . The proposed method is found to perform  well consistently in different resolutions , hence, 
providing a solution for expression recognition in low resolution images . Experiments o n CK+ and JAFFE  facial expression 
database s show the effectiveness of the proposed system . 
Index Terms ‚ÄîFacial expression analysis , facial landmark detection, feature selection,  salient facial patches , low resolution 
image .  
‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî    ÔÅµ   ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî  
1 INTRODUCTION
acial expression, being a fundamental mode of com-
municating human emotions, finds its applicati ons in 
human -computer interaction (HCI), health -care, sur-
veillance, driver safety, deceit detection etc. T remendous 
success being achieved in the field s of face detection and 
face recognition, affective computing has received sub-
stantial  attention among th e researchers in the domain of 
computer  vision.  Signals, which can be used for affect 
recognition , include facial expression, paralinguistic fea-
tures of speech, body language, physiological signals (e.g. 
Electromyogram (EMG), Electrocardiogram (ECG), Elec-
trooculogram (EOG), Electroencephalography (EEG), 
Functional Magnetic Resonance Imaging (fMRI) etc.). A re-
view of signals and methods  for affective computing  is re-
ported in [1], according to which, most of the research on 
facial  expression analysis are based on detection of basic 
emotions  [2]: anger, fear, disgust, happiness, sadness, and 
surprise. A number of  novel methodologies for facial ex-
pression recognition  have  been proposed  over the last dec-
ade.  
Effective  expression analysis hugely depends upon the 
accurate repre sentation  of facial features . Facial Action 
Coding System (FACS)  [3] represents face by measuring  all 
visually observable facial movements in terms of Action 
Units (AUs)  and associates them  with the facial expres-
sions.  Accurat e detection of AUs depends upon proper 
identification and tracking of different facial muscles irre-spective of pose, face shape, illumination, and image reso-
lution . According to Whitehill et al. [4], the detection of all 
facial  fiducial points is even more challenging than expres-
sion recognition itself.  Therefore, most of the existing algo-
rithms are ba sed on geometric  and appearance based fea-
tures. The models  based on geometric features  track the 
shape and size of the face and f acial components such as 
eyes, lip corners , eyebrows  etc., and categorize the expres-
sions  based on relative position of these  facial components . 
Some researchers ( e.g., [5], [6], [7], [8]) used shape models 
based on a set of characteristic points on the face to classify 
the expressions. However, these  methods usually require 
very accurate and reliable detection as well as  tracking of 
the facial landmarks  which are difficult to achieve  in many 
practical situations. Moreover, the distance between  facial 
landmarks vary from person to person, thereby  making the 
person independent expression recognition system less re-
liable.  Facial expressions involve  change in local texture. In 
appearan ce-based methods  [9], a bank of filters  such as Ga-
bor wavelets, Local Binar y Pattern (LBP) etc.  are applied to 
either the whole -face or specific face regions to encode the  
texture . The super ior performance of appearance based 
methods to the geometry  based features  is reported in [4]. 
The appearance -based methods generate s high dimen-
sional vector whi ch are further represented in lower di-
mensional subspace by applyi ng dimensionality reduction 
technique s, such as  principal component analysis (PCA),  
linear discriminant  analysis (LDA)  etc. Finally, the classifi-
cation is performed in learned subspace.  Although the 
time and space costs are higher  in appearance based meth-
ods, the preserv ation of discriminative inform ation makes 
them  very popular.  
Extraction of facial features by dividing the face region 
into sev eral blocks achieves better accuracy  as reported  by 
xxxx-xxxx/0x/$xx.00 ¬© 2014  IEEE  F 
‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî  
ÔÇ∑ S L Happy and A. Routray are with the Department of Electrical Engineer-
ing, Indian Institute of Technology, Kharagpur, India.  
E-mail: {happy, aroutray}@iitkgp.ac.in . 
 
1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation
information: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing
2 IEEE TRANSACTIONS ON  AFFECTIVE COMPUTING,   MANUSCRIPT ID  
 many researchers  ( [9], [10], [11], [12], [13], [14], [15]). How-
ever, this approach fails with improper face alignm ent and 
occlusions.  Some e arlier works  [16], [17] on extraction of 
features from specific face regions  mainly determine  the 
facial regions which contributes more toward discrimina-
tion of expressions  based on the training data. However, in 
these approaches, the positions and sizes of the facial 
patches vary  according to the training data . Therefore, it is 
difficult to conceive a generic system  using these ap-
proaches. In this paper, we propose  a novel facial land-
mark detection technique as well as a salient patch based 
facial expression recognition framework with significant 
performance  at different image resolutions.  The proposed  
method localizes face as well as the facial landmark points  
in an image , thereby extracting some salient patches that 
are estimated during training stage. The  appearance fea-
tures from these patches are fe d to a multi -class classifier 
to classify the images into six basic expression classes . It is 
found that t he proposed facial landmark detection system 
performs similar to the state -of-the-art methods  in near 
frontal images  with lower  computational complexity. The 
appearance features with lower number of  histogram bins 
are used to reduce the computation. Empirically the salient 
facial patches are selected  with predefined positions and 
sizes, which  contribute significantly toward s classification  
of one expression from others.  Once the salient patches are 
selected, the e xpression recognition becomes eas y irrespec-
tive of  the data . Affective computing aims at effective emo-
tion recognition in low resolution images. The experi-
mental results shows that the proposed system performed 
better in low resolution images.  
The paper is organized as follows. Section 2 presents a 
review of earlier w orks. The proposed framework is  pre-
sented in Section 3 . Section 4 and 5 discusses the facial 
landmark detection and feature extraction technique re-
spectively. Experimental results and discussion are pro-
vided in Section 6. Section  7 concludes the paper.  
2 RELATED WORK 
For better performance in facial expression recognition, 
the importance  of detection of facial landmark s is undeni-
able.  Face alignment is a n essential step  and is usually car-
ried out by detection and horizontal positioning of eyes.  
Facial landmar k detection is followed by feature extrac-
tion. Selection of feature s also affects the classification ac-
curacy.  In [18], an active Infra -Red illumination along with 
Kalman filtering is used for accurate  tracking facial com-
ponent s. Performance is improved by the use of both geo-
metric and appearance features. Here the initial positions 
of facial landmarks are figured out using face geometry, 
given the position of eyes, which is not convenient. Tian et 
al. [19] also used relative distance ( lip corner, eye, brow 
etc.) and transient features (wrinkles, furrows etc.) for rec-
ognizing AUs present in lower face. However, the use of 
Canny edge detector for extracting appearance features is 
not flexible in differ ent illumination and determining the 
presence of furrows using threshold is uncertain.  Uddin et 
al. [20] report ed good performance by using image differ-
ence method for observing changes in expressions . The major issue is  the la ndmark selection  which  is carried out  
manually by matching the eye and mouth regions. In [21], 
a relative geometrical distance based approach is de-
scribed which uses computationally expensive Gabor fil-
ters for landmark detectio n and tracking. They used com-
bined SVM and HMM models as classifiers . 
Deformable models, to fit into new data instances, have 
become popular for facial landmark detection. Active 
shape models (ASM) determine shape, scale and pose by 
fitting an appropriate point distribution model (PDM) to 
the object of interest. Active appearance models (AAM)  
[22] combines both shape and texture models to represent 
the object, hence providing superior result to ASM. AAM 
is widely used (  [23], [24], [25], [26], [27]) for detection and 
tracking of non -rigid facial landmarks. However, its per-
formance is poor in person independent scenarios. Manual 
placement of the landmark points in training data for con-
struction of the shape model is a  tedious task and  time con-
suming process  in these models. Constrained Local Model 
(CLM) framework proposed by Cristina cce et al. [28] has 
been proved as a better tool for person independent facial 
landmark detection. All the above  said deformable models  
use PCA to learn the variability of shapes  and textures  of-
fline. CLM algorithm is further m odified by Saragih et al. 
[29] who proposed Regularized Landmark Mean Shift  
(RLMS) algorithm with improved landmark localization 
accuracy. Asthana et al. [30] proposed Discriminative Re-
sponse Map Fi tting (DRMF) method for the CLM frame-
work for the generic face fitting scenario in both controlled 
and natural imaging conditions.  Though satisfactory re-
sults has been  achieved using these deformable models, 
high computational cost  is an obstacle in using them  in 
real-time applications . Chew et al. [31] established the fact 
that, appearance based models work robustly even with 
smal l alignment errors, and perform  the same as that of a 
close to perfect alignment.  Therefore, slight  error in land-
mark detection will not hamper the purpose.  In our exper-
iments, we used a computationally inexpensive learning -
free meth od for landmar k detection that serves the pur-
pose as efficiently as  recent DRMF based CLM method  
[30]. 
An effective feature ideally discriminates between the 
expressions while minimizing the intra -class variance, and 
should be easily extracted from raw images of different 
resolutions. Among the appearance features, Gabor -wave-
let representations ha ve been widely adopted in face image 
analysis  [32], [33] due to their superior performance. How-
ever, the computation of Gabor -features  is both time and 
memory intensive; besides, they are sensitive to scaling . 
Recently the Local Binary Patterns (LBP) proved them-
selves  as an effective appearance features for facial image 
analysis  [10], [34], [35]. Jabid et al. [11] developed local fa-
cial descriptor based on Local Description Patterns  (LDP) 
codes and obtained  better performance  than LBP features. 
Recently Dhall et al. [36] reported higher performance of 
Local P hase Quantization  (LPQ) in facial expression recog-
nition.  In [12], Local Directional Pattern Variance (LDPv) 
is proposed which encodes contrast information using lo-
cal va riance of directional responses.  However, S han et al. 
1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation
information: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing
HAPPY AND ROUTRAY:  AUTOMATIC FACIAL EXP RESSION RECOGNITION USING FEATURES OF SA LIENT FAC IAL PATCHES  3 
 [37] found LBP features  to be robust for analysis of low res-
olution images. Therefore, w e used  the LBP histogram s as 
appearance features .  
PCA  ( [38], [39]) and LDA (  [40], [41], [42]) are used as a 
tool for dimensionality reduction  as well as classification 
in expression  recognition. In [43], authors reported the 
higher performance of PCA -LDA fusion method.  An en-
crypted domain based facial expression recognition sys-
tem is proposed in [44] which uses local fisher discriminant  
analysis to achieve  accuracy as good as in normal images. 
Expression subspace is introduced in [45] which explains 
that the same expressions lie on the same subspace and 
new expressions can be generated from one image by pro-
jecting it into different emotion subspaces.  
Most of the prop osed methods use  full face image, 
while a few use  features extracted from specific facial 
patches. In [13], face image is divided into several sub re-
gions  (7x6) and local features (7x6x59  dimensional fea-
tures ) are extracted. Th en, the discriminative LBP histo-
gram bins are selected by using Adaboost technique for 
optimum classification. Similar approaches are reported in 
[14], [15], and [46]. In such cases, s mall misalignment 
would cause displacement of the sub region  locations, 
thereby increas ing error in classification. Moreover,  for dif-
ferent person s the size and shape of  facial organs are not 
the same, so , it cannot be assured that the same facial posi-
tion always present in one particular block in all images. 
Hence, local patch selection based approach is adopted  in 
our experiments . In [47], authors divided the face into 64 
sub regions  and explored the common faci al patches which 
are active for most expressions and special facial patches 
which are active for specific expressions. Using multi task 
sparse learning method, they used feature s of a few num-
ber of facial patches to classify facial expressions . Song et 
al. [16] used eight facial patches based on specific landmark  
position s to observe the skin deformations caused  by ex-
pressions . The authors have  used binary classifiers to gen-
erate a Boolean variable for presence or absence of ski n 
wrinkles. However, the se patches do not include the tex-
ture of lip corners, which is important for expression  
recognition . Moreover, the occlusion of forehead by hair may result in false recognition.  In [17], authors extracte d 
Gabor features of different scales from the face image and 
trained using Adaboost to select the salient patches for 
each expression. However, the salient patch size and posi-
tion is different when trained with different databases. 
Therefore, a unique crit eria cannot be established for 
recognition of expressions in unknown images.  
Some issues related to r eal-time detection of facial land-
marks and expression recognition  remain unaddressed so 
far. Most of the research es in this field  are carried out on 
different datasets with  suitable performance criteria befit-
ting to the database . For example , selection of prominent 
facial areas improves the performance . However , in most 
of the literature , the si ze and position of these facial  
patches are reported to be different for different databases. 
Therefore, our experiments attempt to identify  the salient 
facial areas  having generalized discriminative features for 
expression classification . Selection of sal ient patches re-
taining  discriminating  features between each pair of facial 
expressions improve d the accuracy . The size and location 
of patches are  kept same for different databases for the 
purpose of generalization. In addition, the proposed 
framework has the potential to recognize expressions in 
low-resolution images.  
3 PROPOSED METHODOLOGY  
Change s in facial expression s involve contraction and 
expansion of facial muscles which alters the position of fa-
cial landmarks. Along with the facial muscles, the textur e 
of the area also changes. This paper attempts to  under-
stand the contribution of different facial areas toward au-
tomatic expression recognition.  In other words, the paper 
explores  the facial patches which generates discriminative 
features to separate two expressions  effectively .  
The overview of the proposed method is shown in Fig. 
1. Observations from  [47], [16] suggest that  accurate facial 
landmark detection and extractio n of appearance features 
from active face regions improve  the performance  of ex-
pression recognition . Therefore , the first ste p is to localize 
 
Fig. 1. Overview of the proposed system  

1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation
information: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing
4 IEEE TRANSACTIONS ON  AFFECTIVE COMPUTING,   MANUSCRIPT ID  
 the face followed by detection of  the landmarks.  A learn-
ing-free approach  is proposed  in which the eyes and nose 
are detected  in the face image  and a coarse region of inter-
est (ROI) is marked around  each . The l ip and eyebrow cor-
ners are detected from respective  ROIs. Locations of active 
patches are defined with respect to the  location of land-
marks.  Fig. 2 shows the steps involved in automated facial 
landmark detection and active patch extraction. In training 
stage, all the active facial patches are evaluated and the 
ones  having  features  of maximum variation  between pair s 
of expressions are sel ected . These selected  features are fur-
ther projected into lower dimensional subspace and classi-
fied into different expressions using  a multi -class classifier.  
The training phase includes  pre-processing , selection of fa-
cial patches,  extraction of  appearance  feature s and learning 
of the  multi -class  classifiers . In an unseen image, the pro-
cess first  detects the facial landmarks, then extracts the fea-
tures from the selected salient patches , and finally classi-
fies the expressions.  
4 FACIAL LANDMARK DETECTION  
The facial patches which are active during different fa-
cial expression s are studied  in [47]. It is reported that  some  
facial patches are common  during elicitation of all basic ex-
pressions and some are confined to a single expression . 
The result s indicate  that these active patches are positioned  
below  the eye s, in between the eyebrows, around  the nose 
and mouth corners. To extract these patches from face im-
age, we need to locate the facial components  first followed 
by the extraction o f the patches around these organs. Un-zueta et al . [48] proposed a robust, learning -free, light-
weight generic face mod el fitting method for localization 
of the facial organs. Using local gradient analysis, this 
method finds the facial features and adjusts the deforma-
ble 3D face model so that its projection on image will match 
the facial feature points. In this paper,  such a learning -free 
approach wa s adopted  for localization of facial landmarks.  
We have extracted  the active  facia l patches with respect to 
the position of eyes, eyebrows, nose , and lip corners  using 
the geometrical statistics  of the face.  
4.1 Pre-processing  
A low pass filtering was performed using a 3x3 Gauss-
ian mask to remove noise from the facial images followed 
by face detection for face localization. We used Viola -Jones 
technique  [49] of Haar -like features w ith Adaboost learn-
ing for face detection . It has lower computational complex-
ity and was sufficiently accurate for detection of near -
frontal and near -upright face images. Using integral image 
calculation, it can detect face regardless of scale and loca-
tion in real time . The localized  face was extracted and 
scaled to bring it to a common resolution. This made  the 
algo rithm  shift invariant, i.e. insensitive to the location of 
the face on image. Histogram equalization was carried out 
for lighting corrections . 
4.2 Eye and Nose Localization  
To reduce the computational complexity  as well as the 
false detection rate , the coarse region of interest s (ROI) for 
eyes and nose were selected  using geometrical positions  of 
face. Both the eyes were detected separately using Haar  
classifier s trained for each eye . The H aar classifier returns  
the vertices  of the rectangular area of detected eyes . The 
eye centers are computed as the mean of these coordinates . 
Similarly, nose position was also detected using Haar  cas-
cades. In our expe riment, for more than 9 8% cases these 
parts  were detected properly . In case the eyes or nose was 
not detected using Haar classifier s, the system relies on the 
landmark coordinates detected by anthropometric  statis-
tics of face . The position of eyes were used for up-right face 
alignment  as the position s of eyes do not change with facial 
expressions.  
4.3 Lip Corner Detection  
Inspired by the work of Nguyen et al. [50], we used fa-
cial topographies  for detection of lip and eyebrow corners. 
The ROIs for lips and eyebrows were selected as a function 
of face width positioned with respect to the facial organs. 
The ROI for mouth was extracted using the position of  
Fig. 2. Framework for automated facial landmark detection and active patch extraction, (a) face detection , (b) coarse ROI selection for eyes 
and nose, (c) eyes and nose detection followed by coarse ROI selection for eyebrows and lips, (d) detection of corners of lip  and eyebrows, 
(e) finding the facial landmark locations, (f) extraction of active facial patch es. 
 
Fig. 3. Lip corner localization, (a) lips ROI, (b) applying horizontal 
Sobel edge detector, (c) applying Otsu threshold, (d) removing spu-
rious, (e) applying morphological operations to render final con-
nected component for lip corner localization.  

1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation
information: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing
HAPPY AND ROUTRAY:  AUTOMATIC FACIAL EXP RESSION RECOGNITION USING FEATURES OF SA LIENT FAC IAL PATCHES  5 
 nose as reference  (Fig. 3a). The upper  lip always produce s 
a distinct edge  which can be detected using a horizontal 
edge detector. Sobel edge detector  [51] was used for this 
purpose. In images with different expressions, a lot of 
edge s were obtained which was further  threshold  by using 
Otsu  method  [52]. In this process, a binary image was ob-
tained  containing many connected regions. Using con-
nected component analysis, the spurious components hav-
ing an area less than a threshold were removed . Further, 
morphological dilation operation was carried out on the 
resulting  binary image. Finally, the connected component 
with largest area which was just below the nose region was 
selected as upper lip region. Fig. 3 shows diff erent stages 
of the process . The  algorithm steps are  given  below.  
Algorithm 1. Lip corner detection  
Given:  aligned face ROI and nose position  
 1: select coarse lips ROI using face width  and nose position  
 2: apply Gaussian blur to the lips ROI 
 3: apply  horizontal sobel operator for edge detection  
 4: apply Otsu -thresholding  
 5: apply morphological dilation operation  
 6: find the connected components  
 7: remove the spurious connected components using 
threshold technique  to the number of pixels  
 8: scan the image from the top  and select  the first con-
nected component as upper lip position  
 9: locate the left  and right most positions of connected 
component as lip corners  
Sometimes, due to shadow below the nose, the upper 
lip could not be segmented properly . A case is shown in 
Fig. 4. In such cases, the upper lip wa s not segmented as a 
whole  and the connected component obtained at th e end 
resembled  half of the upper lip . Hence, the extreme ends 
of this connected component  did not satisfy the bilateral 
symmetry property , i.e. the lip corners should have been  at 
more or less equal distance s from vertical central line of 
face. These situ ations we re detected by putting a threshold 
to the  ratio of  distance between the  lip corners to the max-
imum of distance s of the lip corner s from the vertical cen-
tral line . In such cases, the second connected component 
below the nose  was considered as the o ther part of upper 
lip. Thus the lip corners we re detected with the help of  two 
connected components.  By using the above said methods, 
false detection of lip corner points we re minimized.   
4.4 Eyebrow  Corner  Detection  
With the knowledge of position s of eyes, t he coarse 
ROIs of eyebrows were selected. The eyebrow s were de-tected following the same steps a s that of upper lip detec-
tion. However, we observed that performing an adaptive 
threshold operation before applying horizontal sobel oper-
ator improve d the accura cy of eyebrow corner localization.  
The use of horizontal edge detector reduce d the false de-
tection of eyebrow position s due to partial occlusion by 
hair. The inner eyebrow  corner  was detected accurately in 
most of the images . Fig. 5 shows intermediate steps in eye-
brow corner detection.  
4.5 Extraction of Active Facial Patches  
During an expression, t he local patches we re extracted 
from the face image depending upon the position o f active 
facial muscles. We have considered the appearance of fa-
cial regions  exhibiting considerable variations  during one 
expression. For example, wrinkle in upper nose region is 
prominent in disgust expression and absent in other ex-
pressions. Similarly, regions around lip corners undergo 
significant change s and its appearance features are dissim-
ilar for different expressions. From our observations, sup-
ported by the research of Zhong et al. [47], we used the ac-
tive facial patch es as shown in Fig. 6 for our experiment.  
Fig. 4. Lip corner localization in a case where upper lip is not entirely 
connected , (a-c) same as Fig. 3, (d-e) selection of two connected 
components by scanning from top, (f) localized lip corner.  
 
Fig. 5. Eyebrow corner localization, (a) rectangles showing search ROI 
and plus marks showing the detection result, (b & f) eye ROIs, (c & g) 
applying adaptive threshold on ROIs, (d & h) applying horizontal sobel 
edge detector followed by Otsu threshold and morphological operations, 
(e & i) final connected components for corner  localization.  
 
Fig. 6. Position of facial patches  

1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation
information: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing
6 IEEE TRANSACTIONS ON  AFFECTIVE COMPUTING,   MANUSCRIPT ID  
 The patches does not have  very  fixed position on the face 
image. Rather, their location depends upon the positions 
of facial landmarks. The size of all facial patches were kept 
equal and w as approximately one -ninth  of the width of the 
face. Here onwards, we will refer the patches by the num-
bers assigned to it. As shown in Fig. 6, ùëÉ1, ùëÉ4, ùëÉ18, and  ùëÉ19 
were directly extracted from the positions of lip corners 
and inner eyebrows respectively.  ùëÉ16 was at the center of 
both the eyes; and ùëÉ17 was the patch above  ùëÉ16. ùëÉ3 and  ùëÉ6 
were located in the midway of eye and nose.  ùëÉ14 and  ùëÉ15 
were located just below eyes. ùëÉ2, ùëÉ7, and ùëÉ8 were clubbed 
together and located at one side of nose position. ùëÉ9 was 
located just below ùëÉ1. In a similar  fashion  ùëÉ5, ùëÉ11, ùëÉ12, 
and  ùëÉ13 were located. ùëÉ10 was located at the center of posi-
tion of ùëÉ9 and ùëÉ11. 
5 FEATURE EXTRA CTION  AND CLASSIFICATION  
LBP wa s widely used as a robust i llumination invariant 
feature descriptor. This operator generates a binary num-
ber by comparing the neighbouring  pixel values with the 
center pixel value  [53]. The pattern with 8 neighborhoods 
is given by  
ùêøùêµùëÉ (ùë•,ùë¶)=‚àëùë†7
ùëõ=0(ùëñùëõ‚àíùëñùëê)2ùëõ 
where  ùëñùëê is the pixel value at coordinate (ùë•,ùë¶) and ùëñùëõ are the 
pixel value s at coordinate s in the neighborhood of  (ùë•,ùë¶), 
and  
ùë†(ùë•)={1,ùë•‚â•0
0,ùë•<0 
The histogram s of LBP image  can be utilize d as feature  
descriptors , given by  
ùêªùëñ=‚àëùêº
ùë•,ùë¶{ ùêøùêµùëÉ (ùë•,ùë¶)=ùëñ}, ùëñ=0,1,‚Ä¶,ùëõ‚àí1 
where ùëõ is the number of labels produced by LBP operator. 
Using different bin width s, the histograms  can be grouped 
to discover  different features . For instance, LBP with 8 
neighboring points produces 256 labels. If we collect its 
histogram s in 32 bins, then  we are basically grouping the 
patterns [0,7], [8,15],  [16,23],  ‚Ä¶, and [248,255]  together.  
This is same as ignoring the least significant bits of the un-
signed integ er, i.e. using patterns of one side of local neigh-
borhood . Fig. 7 shows the pattern generated due to 32 his-
togram bins where the upper row neighbors do not con-
tribute towards the pattern label . We used 16, 32 and 256 
bin histogram s in the experiments.   
In addition, uniform LBP and rotation invariant uni-
form LBP values  [53] are also used in our experiment  and 
their performance s are compare d. Uniform ity measure  (ùëà) 
corresponds to the number of bitwise transition s from 0 to 
1 or vice-versa  in a pattern  when the bit pattern is traversed circularly . For instance, the pattern (0000 0001 )2 and 
(00100110) 2 have ùëà values  2 and 4 respectively. The pattern 
is called uniform  (LBPu2) when  ùëà‚â§2. This reduces the 
leng th of the 8-neighborhood patterns  to 59 -bin histo-
grams . The effect of rotation can be removed by assigning 
a unique identifier to each rotation invariant pattern , given 
by  
ùêøùêµùëÉùëüùëñùë¢ 2={‚àëùë†7
ùëõ=0(ùëñùëõ‚àíùëñùëê),ùëñùëì ùëùùëéùë°ùë°ùëíùëüùëõ  ùëñùë† ""ùë¢ùëõùëñùëìùëú ùëüùëö""
9,          ùëúùë°‚Ñéùëíùëüùë§ùëñùë†ùëí 
 
Thus, the rotational invariant  uniform LBP with 8 neigh-
borhood produces  10 histogram bins.   
5.1 Learning Salient Facial Patches Across 
Expressions  
In most of the literature s, all the facial features are con-
catenated to rec ognize the expression. However, this gen-
erates a feature vector  of high dimension . We observed 
that the features from a fewer  facial patches can replace the 
high  dimensional features  without significant dimin ution 
of the recognition accuracy . From human pe rception, not 
all facial patches are responsible for recognition of one ex-
pression. The facial patches responsible for recognition of 
each expression can be  used separately to recognize that 
particular expression. Based on this hypothesis, we evalu-
ated the  performance of each facial patch for recognition of 
different expressions.  
Further , some expressions share similar movements  of 
facial muscles;  features of such patches are redundant  
while classifying the expressions . Therefore, after extract-
ing the activ e facial patches, we selected the salient facial 
patches responsible for discrimination between  each pair 
of basic expressions. A facial patch is considered to be dis-
criminative between two expressions, if the features ex-
tracted from this patch can classif y the two expressions ac-
curately. Note that not all active  patches are salient for 
recognition of all expressions. For all possible pair of ex-
pressions ( ‚Ää6C2 ), all the 19 active patches were evaluated by 
conducting  a ten-fold cross validation test . The patches 
that result maximum discrimination  were selected  for rep-
resenting the expressions . 
The LBP histogram features in lower resolution images 
are sparse  in nature because of the smaller patch area. LDA 
was applied for projecting the se features to th e discrimi-
nating dimensions and to choose the salient patches ac-
cording to their discriminative performance . LDA finds 
the hyper -plane that minimizes the intra -class scatter ( ùëÜùë§), 
while maximizing the inter -class scatter ( ùëÜùëè). It is also used 
as a tool for interpretation of importance of the features. 
Hence  it can be considered as a transformation into a lower 
dimensi onal space  for optimal discrimination between 
classe s. The intra -class scatter ( ùëÜùë§) and inter -class scatter 
(ùëÜùëè) are given by  
ùëÜùëè=‚àëùëÅùëñùëõ
ùëñ=1(ùë•ùëñÃÖ‚àíùë•ÃÖ)(ùë•ùëñÃÖ‚àíùë•ÃÖ)ùëá 
ùëÜùë§=1
ùëÅùëñ‚àí1‚àë‚àë(ùë•ùëñ,ùëó‚àíùë•ùëñÃÖ)(ùë•ùëñ,ùëó‚àíùë•ùëñÃÖ)ùëáùëÅùëñ
ùëó=1ùëõ
ùëñ=1  
Fig. 7. Patterns generated by one side of local neighborhood which 
produces 32 histogram bins  

1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation
information: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing
HAPPY AND ROUTRAY:  AUTOMATIC FACIAL EXP RESSION RECOGNITION USING FEATURES OF SA LIENT FAC IAL PATCHES  7 
 where ùë•ùëñ,ùëó is the ùëóth feature vector in ùëñth class. Here ùëõ num-
ber of classes, having ùëÅùëñ number of images in ùëñth class, 
have ùë•ÃÖ, the mea n vector of all the training data, and ùë•ùëñÃÖ, the 
mean of ùëñth class . LDA aims at maximization of ùëÜùëè while 
minimizing  ùëÜùë§, i.e. maximization of ratio of determinant of 
ùëÜùëèto the determinant of  ùëÜùë§. 
Œ¶ùëôùëëùëé=argùëöùëéùë•
Œ¶|Œ¶ùëáùëÜùëèŒ¶|
|Œ¶ùëáùëÜùë§Œ¶| 
This ratio is called as Fishers criterion. This can be com-
puted by s olving  the generalized eigenvalue  problem 
given  as: 
ùëÜùëèŒ¶ùëôùëëùëé‚àíùëÜùë§Œ¶ùëôùëëùëéŒõ=0 
‚áíùëÜùë§‚àí1ùëÜùëèŒ¶ùëôùëëùëé=Œ¶ùëôùëëùëéŒõ 
where Œõ is the diagonal eige nvalue matrix and Œ¶ùëôùëëùëé is the 
set of discriminant vectors of ùëÜùëè and ùëÜùë§ corresponding to 
the ùëõ‚àí1 largest generalized eigenvalues. Thus, Fisher cri-
terion is maximized when the projection matrix Œ¶ùëôùëëùëé is 
composed of eigenvectors of  ùëÜùë§‚àí1ùëÜùëè, subject to  ùëÜùë§ being  
non-singular. As suggested by Belhumeur et al. [54], PCA 
was applied to the signal prior to  LDA. By doing so, the 
signal wa s projected to lower dimensional space assuring 
the non -singula rity of within class scatter matrix . Moreo-
ver, this PCA -LDA fusion [43] improves the performance. 
Therefore, we applied PCA on the training set for dimen-
sionality  reduction  followed by LDA .  
We calculated the saliency of all facial patch es for all 
pair of expressions  and it wa s expressed in terms of sali-
ency scores . The saliency of a patch represents  the ability 
of the features from the patch  to accurately  classify  a pair 
of expressions . The saliency score of a patch between a p air 
of expressions is the classification accuracy of the features 
from that patch  in classifying the two expressions . Here 
PCA -LDA wa s used for classification purpose to deter-
mine the saliency  score. In a similar fashion, saliency score 
of all patches for each pair of expressions we re calculated.  
We used one -against -one strategy  for expression classifica-
tion purpose . While classifying between a pair of expres-
sions, the features were extracted  from those facial  patches  
which have high  saliency score.  The fea ture vector s from 
the salient patches we re concatenated to construct a higher 
dimensional feature vector. Thus, the dimension of the fea-
ture vector depends  upon the number of patches selected 
for classification purpose.   
We applied PCA to reduce the dimens ionality of the 
feature  vector . Thus, by projecting the feature vector s from  
salient patches  to the optimal sub -space  obtained by above 
method, we can find the  lower dimensional vector  with 
maximum discrimination for different classes . The weight 
vectors , corresponding to the salient patches of each pair 
of expression  classes , generated during the training stage 
were used  during testing.  
5.2 Multi -class Classification  
SVM wa s used for classification of extracted features 
into differe nt expression categories. SVM [55] is a popular 
machine learning algorithm which maps the feature vector 
to a different plane, usually to a higher dimensional plane , 
by a non -linear mapping , and finds a linear decision  hyper 
plane  for classification of tw o classes . Since SVM is a binary classifier, we implemented  one-against -one (OAO) tech-
nique  for multi -class classification  [56]. In OAO approach , 
a classifier is trained between each pair of classes; hence 
‚ÄäùêæC2 number of  classifiers we re constructed in total , where 
ùêæ is the number of classes . Using voting strategy, a vector 
can be classified to the class  having the highest number of 
votes.  After  sever al experiments with linear, polynomial , 
and radial basis function (RBF)  kernel s, we selected  RBF 
kernels  for its superior  classification performance .  
6 EXPERIMENTS AND DISCUSSION  
The proposed method was evaluated by using two 
widely used facial expression databases , i.e., Japanese Fe-
male Facial Expressions (JAFFE)  [57] and Cohn -Kanade  
(CK+) [58]. We have used ten -fold cross validation to eval-
uate the performance of the proposed method. As dis-
cussed earlier, face detection was carried out on all images 
followed by scalin g to bring the face to a common resolu-
tion. Facial landmarks were detected and  salient facial 
patches were extracted  from  each face image . During train-
ing stage, a  SVM classifier wa s trained bet ween each pair 
of expressions . Here the training  data we re the  concate-
nated LBP histogram features extracted from the salient 
patches containing discriminative characteristics between 
the given pair of expression classes. Similarly, ‚Ää6C2 numbers  
of SVM classifiers we re constructed and used for evaluat-
ing the performance on the test -set. 
6.1 Experiments on the Cohn -Kanade Database  
The Cohn -Kanade database contains both male and fe-
male  facial  expressi on image sequences  for the six basi c 
emotions . In our experiments, the last image from each  se-
quence was selected where the expression is at its peak  in-
tensity . The number of instances for each expression  varies 
according to its availability. In our experiments on  CK+ da-
tabase, we used  329 images in total : anger (41), disgust (45), 
fear ( 53), happiness ( 69), sadness ( 56), and surprise ( 65).  
 
Fig. 8. The recognition rate in images with different face resolutions 
in CK+ database  0.80.820.840.860.880.90.920.94RECOGNITION ACCURACY
RESOLUTION16bin 32bin 256bin
LBPu2 LBPriu2
1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation
information: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing
8 IEEE TRANSACTIONS ON  AFFECTIVE COMPUTING,   MANUSCRIPT ID  
 6.1.1  Analysis  of histogram binwidth  and face 
resolution  
We determined the optimal resolution and bin width  of 
the histogram empirically. The expression recognition per-
formance of different feature vectors  were studied for face 
resolution s starting from 48x48 to 192x192. The low reso-
lution images were o btained by down sampling the im-
ages. The classifiers were trained and evaluated at differ-
ent resolutions of face images. In our experiment s, we ob-
served  minimum  accuracy of 82 % at 48x48 face resolution  
as shown in  Fig. 8. In [59], it is reported that with  a face 
image of 48x64  resolution , some  facial landmarks, such as, 
lip and eye corners  are difficult to detect . Therefore, it is 
uncertain if expressions can be recognized at this resolu-
tion. However, f rom our experiment, we obtained pretty 
good accuracy at all resolutions.  This establishes the ro-
bustness of  the appearance features extracted from the sa-
lient patches at different resolutions.  Since we have imple-
mented a voting method, the result is based on votes of ‚Ää6C2 
classifiers, threby, reducing  the classification error due to  
a single classifier.  
We also observed that the use of LBPu2 features  pro-
duced  better  accuracy compared to other features at all res-
olutions. The performance of the other f eature vectors are 
alike . The uniform patterns removes the noisy estimates in 
the image by accumulating them into one histogram bin, 
thereby increasing the recognition accuracy.  
6.1.2  Performance improvement : use of block - 
histograms  
By implementing block -based feature extraction tech-
nique , more local f eature s were add ed to the feature vec-
tor. This process makes the  feature vector to be  a combina-
tion of local as well as global features.  Empirically we  ob-
served that the performance wa s imp roved when each se-
lected patch wa s further divided into four equal blocks. In 
our experiments, the feature vector was obtained by  con-
catenatin g the  features obtained from each block  of the sa-
lient patch es and the results are  shown in  Fig. 9. It was ob-
served that the 16 -bin histograms, 32 -bin hist ograms, 256 -
bin histograms, and uniform LBP features performed alike  
at all resolutions . We performed the experiments on a 
standard face resolution of 96x96 . At this r esolution, all the features  except LBPriu2  had similar performance s. Feature  
vector with low dimension reduces the computational 
complexity . Therefore, we selected the histogram feature s 
with 16 -bins as the optimal trade -off between speed and 
accuracy.   Table 1 shows t he confusion matrix of six emo-
tions based on the proposed method.  The quality of the 
overall classification is evaluated  by calculating the  macro -
average  [60] of precision, recall and F-score . The proposed 
system attained a  balanced  F-score  of 94.39% with 94.1% 
recall  and 94.69%  precision.   
As observed from Table 1, surprise expression achieved 
best recognition rate which is usually characterized by 
open mouth  and upward eyebrow movement . The system 
performed worst for anger expression and classification er-
ror was maximum  between a nger and sadness  since they 
involve similar and subtle changes.  Here onwards, all the 
experiments  are based on face resolution of 96x96.  
6.1.3  Optimum number of salient patches  
Number of patches used for classification also affects 
the performance in terms of speed and accuracy. Fig. 10 
shows average of accuracies of all expressions  with respect 
to the number of salient patches used for classification with 
a face resolution of  96x96.  It is apparent  from Fig. 10 that 
the use of features from all the 19 patches can classify all 
expressions with an accuracy of 93.87%.  It is clear that  even 
the use of  appearance features of a single salient patch can 
discriminate between each pair  expressions efficiently  
with  recognition rate of 91.19% . This implies that the use 
of rest of the features from other patches contribu te mini-
mum toward s the discriminative features. More the num-
ber of patches used, more is the size of  the feature vector. TABLE 1 
THE CONFUSION MATRIX USING PROPOSED METHOD ON  
CK+ DATABASE  
 
 
Fig. 9. Improvement in the recognition rate at different resolutions by 
using block histograms   
Fig. 10. The recognition rate using different number of salient patches  
0.850.870.890.910.930.95RECOGNITION ACCURACY
RESOLUTION16bin 32bin 256bin
LBPu2 LBPriu2
0.890.90.910.920.930.940.950.96
12345678910111213141516171819Recognition accuracy
Number of patches
1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation
information: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing
HAPPY AND ROUTRAY:  AUTOMATIC FACIAL EXP RESSION RECOGNITION USING FEATURES OF SA LIENT FAC IAL PATCHES  9 
 This  increases the computational burde n. Therefore, in-
stead of using all the facial patches , we can rely on some 
salient facial patches  for expression recognition. This will 
improve the computational complexity as well as  robust-
ness of the features especially  when a face is partially oc-
cluded.  In our experiments , we used top four salient 
patches in our experiment s which results an accurac y close 
to 95%. Note that the combination of patches varies across 
different expression pairs . 
6.1.4  Performance comparison  
The proposed method wa s compared with the results 
obtained by other approaches reported in the literature. 
Lack of the knowledge of the data and evaluation protocol 
used by different literature s makes the comparison task 
difficult.  However , we compared the performance of the 
system with literatures that adopted  similar protocols  in 
CK+ dataset . Table 2 compares the performance of the pro-
posed method wit h the state -of-the-art methods. From Ta-
ble 2, Uddin et al. [20] reported high est recognition perfor-
mance for disgust, fear, and happiness  probably due to  the 
use of temporal features through Hidden M arkov Model . 
However, the perform ance of the proposed system  is com-
parable with the other systems  as it achieved an average 
recognition rate of 94.09%.  Nevertheless , the high recogni-
tion rate is obtained using features from specific facial 
patches  and without using features of temporal do main .  
6.2 Experiments on JAFFE Database  
While testing  on JAFFE database, we used the same pa-rameters obtained for Cohn -Kanad e database . In our ex-
periments on JAFFE database, we used 183 images in total: 
anger (30), disgust (32), fear (29), happiness (31), sadness 
(31), and surprise (30). The confusion matrix , as in Table 3, 
shows  the co nsistent performance of the proposed method . 
An overall accuracy  of 91.8 % was obtained.  From the ex-
periments on JAFFE database, it was observed that t he pro-
posed system recognises all expressions with 91.8% recall 
and 92.63% precision  achieving  an average F-score  of 
92.22% . The system performed worst for sadness expres-
sion as it misclassified sadness as anger .  
6.3 Experiments on Fused Database  
For generalization, we have fused the samples of two 
databases together to train the classifier [62]. Sample Level 
fusion was performed by putting the images  of both data-
bases  together . The training set was constructed by ran-
domly elect ing 90% of the data from each expression of 
each database . The rest data were  used as testing set. The 
model s were trained, and their performances were evalu-
ated on samples of individual databases in testing set. This 
experiment was repeated for ten times. By learning the fea-
tures from different databases, the classifier performs bet-
ter in various situations . All samples were treated with 
 
Fig. 11. Comparison between performance of the proposed landmark 
detection method and the DRMF based CLM model in  BIOID data-
base.  TABLE 2 
PERFORMANCE COMPARISON OF DIFFERENT STATE-OF-THE-
ART APPROACHES ON CK+  DATABASE  
 [20] [61] [47] [16] [17] Pro-
posed 
system  
An 82.5 87.03  71.39  90.56  87.1 87.8 
Di 97.5 91.58  95.33  86.04  90.2 93.33  
Fe 95 90.98  81.11  84.61  92 94.33  
Ha 100 96.92  95.42  93.61  98.07  94.2 
Sa 92.5 84.58  88.01  90.24  91.47 96.42  
Su 92.5 91.23  98.27  92.3 100 98.46  
Avg  93.33  90.38  88.255  89.56  93.14  94.09  
An = anger, Di = disgust, Fe = fear, Ha = happiness, Sa = 
sadness, Su = surprise, and Avg = Average recognition 
rate.  
TABLE 3 
THE CONFUSION MATRIX USING PROPOSED METHOD ON  
JAFFE DATABASE  
 TABLE 4 
COMPARISON BETWEEN PE RFORMANCE AND TIME C OMPLEXITY 
OF THE PROPOSED LAND MARK DETECTION METHO D AND THE 
DRMF  BASED CLM  MODEL O N CK+  DATABAS E 
 
(Recognition accuracy is obtained by u sing the proposed salient 
patch extraction based method. The execution time analysis is based 
on unoptimized MATLAB code in a Dual -Core Intel Pentium i5 CPU 
with 3.2 GHz. ) 

1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation
information: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing
10 IEEE TRANSACTIONS ON  AFFECTIVE COMPUTING,   MANUSCRIPT ID  
 equal probability of selec tion for training or testing . There-
fore,  it is e xpected that the database with more samples 
should dominate in  performance . However, t he proposed  
method performed well o n both databases with a s ignifi-
cant accuracy of 89 .64% and 8 5.06% on CK+ and JAFFE  da-
tabases respectively . The top four salient patches for clas-
sification of each pair of expressions is provided in Table 
5. 
6.4 Performance of the landmark detector  
We have used the images of BIOID  [63] dataset without 
spectacles for evaluating the performance of the proposed 
landmark detection algorithm. It contains manual ly la-
belled facial landmarks  which serves the purpose of 
ground truth during training and testing . The average Eu-
clidian distance error from point to point for each land-
mark  location is used as the distance measure, which is 
given as : 
ùëí=1
ùëõùë†‚àëùëëùëñùëõ
ùëñ=1 
Here ùëëùëñs are the Euclidean distance  errors for the land-
marks, ùëõ is the number of landmarks,  and ùë† is the distance 
between the eye s pupils  used as  the scaling factor . Fig. 11 
shows the cumulative distribution of the detection accu-
racy by using proposed method.  Its performance wa s com-
pared with the performance of the recent DRMF method 
based CLM model [30]. The performa nce of expression 
recognition by the two landmark detection methods wa s 
also compared . After detection of facial landmarks,  the ac-
tive patches  were extracted and the procedure s as dis-
cussed in section 5 were followed for expression classifica-
tion. As shown in  Table 4, both the methods produce al-
most similar ac curacy in CK+ database while the proposed 
landmark detection method takes very small time in com-
parison to the other one. Although the DRMF based CLM 
method finds the facial organ locations accurately, it some-
times fails to fit to their shapes in different  expressions, es-
pecially on lip s. When the mouth is open or tightly closed, 
or when the lip corners are pulled down, it at times gener-
ates average lip -shape instead of completely fitting to lips. 
Thus, lip corner patches we re extracted at incorrect loca-
tions resulting poor classification. However, the proposed 
landmark detection method accurately finds the facial landmarks in most of the images thereby extracting fea-
tures from appropriate patch locations. The slight misa-
lignment error is automatically taken  care by the appear-
ance feature.  
7 CONCLUSION  
This paper has presented a computationally efficient  fa-
cial expression recognition system for accurate classifica-
tion of  the six universal expressions.  It investigates the rel-
evance of different facial patches in  the recognition of dif-
ferent facial expressions.  All major  active  regions on face 
are extracted which are responsible for the face defor-
mation  during an expression.  The position and size of 
these active regions are predefined. The system analyses 
the acti ve patches and determines the salient  areas on face 
where the features are discriminative for different expres-
sions.  Using the appearance features from the salient 
patches, the system performs the one -against -one classifi-
cation task and determines the expr ession based on major-
ity vote.  
In addition, a facial landmark detection method is de-
scribed which detects some facial points accurately with 
less computat ional cost .  Expression  recognition is carried 
out using the proposed landmark detection method  as well 
as the recently proposed CLM model based on DRMF  
method . In both cases, recognition accuracy is almost sim-
ilar, whereas computational cost of the propos ed learning -
free method is significantly less. Promising results has 
been obtained by using block based LBP histogram fea-
tures  of the salient patches. Extensive experiments has 
been carried out on two facial expression databases and the 
combined dataset . Experiments are conducted using vari-
ous binwidths of LBP histograms, uniform LBP and rota-
tion invariant  LBP features. Low dimensional features are 
preferred , with sufficient recognition accuracy,  to decrease 
computational complexity . Therefore, t he 16-bin L BP his-
togram features  are used  from four salient patches at face  
resolution of 96x96 for obtaining  best performance with a 
suitable trade -off between speed and accuracy.  Our system 
found the classification between anger and sadness trou-
blesome  in all datab ases. The system appears to perform 
well in CK+ dataset with a n F-score  of 94.39% . Using the TABLE 5 
THE SALIENT PATCHES DERIVED FROM FUSION OF CK+  AND JAFFE  DATABASES  
 

1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation
information: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing
HAPPY AND ROUTRAY:  AUTOMATIC FACIAL EXP RESSION RECOGNITION USING FEATURES OF SA LIENT FAC IAL PATCHES  11 
 salient  patches obtained by training on CK+ dataset , the 
system achieves a n F-score  of 92.22% in JAFFE dataset. 
This proves the generic performance of the system.  The 
performance of the proposed system is comparable with 
the earlier  works with similar approach, nevertheless  our 
system is fully automated .  
Interestingly, the local features at the salient patches 
provide consistent performance at different resolutions . 
Thus, the proposed method can be employed  in real -world 
applications with low resolution imaging in real -time. Se-
curity cameras, for instance , provide low resolution im-
ages which can be analysed effectively using the proposed 
framework.   
Instead of the w hole face, the proposed method classi-
fies the emotion by assessing a few  facial patches. Thus, 
there is a chance of improvement in performance with par-
tially occluded images which has not been addressed  in 
this study. This analysis is confined to databases  without 
facial hair s. There is a possibility  of improvement by using 
different appearance features. Dynamics of expression in 
temporal domain is also not considered in this study.  It 
would  be interesting to explore the system  incorporated 
with  motion feat ures from different facial patches. The ex-
ecution time reported for the proposed algorithm  is based 
on an un -optimi zed MATLAB code. However, the optimal 
implementation of the proposed framework will signifi-
cantly improve the computational cost and real -time ex-
pression recognition can be achieved with substantial ac-
curacy.  Further analysis and efforts are required to im-
prove the performance by addressing some of the above 
mentioned issues . 
8 ACKNOWLEDGMENT  
The authors  would like to thank Prof. Jeffery Cohn for  
the use of the Cohn ‚ÄìKanade database, and Dr. Michael J. 
Lyons for the use of the JAFFE database.  The authors grate-
fully acknowledge the contribution of the reviewers‚Äô com-
ments.  
9 REFERENCES  
 
[1]  R. A. Calvo and S. D'Mello, ""Affect detecti on: An 
interdisciplinary review of models, methods, and their 
applications,"" IEEE Transactions on Affective Computing, vol. 1, 
no. 1, pp. 18-37, 2010 .  
[2]  C. Izard, ""Innate and Universal Facial Expressions: Evidence 
from Developmental and Cross -Cultural  Research,"" 
Psychological Bull., vol. 115, pp. 288-299, 1994 .  
[3]  P. Ekman, W. V. Friesen and J. C. Hager, ""FACS Manual,"" Salt 
Lake City, UT: A Human Face, May 2002 .  
[4]  J. Whitehill, M. S. Bartlett and J. Movellan, ""Automatic facial 
expression recog nition,"" in Social Emotions in Nature and Artifact , 
Oxford University Press, 2013 .  
[5]  Y. Chang, C. Hu and M. Turk, ""Manifold of facial expression,"" 
IEEE International Workshop on Analysis and Modeling of Faces and 
Gestures, p. 28‚Äì35, 2003 .  
[6]  M. Pa ntic and I. Patras, ""Dynamics of facial expression: Recognition of facial ac - tions and their temporal segments from 
face profile image sequences,"" IEEE Transactions on Systems, 
Man, and Cybernetics, vol. 36, no. 2, p. 433‚Äì449, 2006 .  
[7]  M. Pantic and L . Rothkrantz, ""Facial action recognition for facial 
expression analysis from static face images,"" IEEE Transactions 
on Systems, Man, and Cybernetics, vol. 34, no. 3, p. 1449 ‚Äì1461 , 
2004 .  
[8]  I. Cohen, N. Sebe, A. Garg, L. Chen and T. Huang, ""Facial 
expre ssion recognition from video sequences: Temporal and 
static modeling,"" Comput. Vis. Image Understand., vol. 91, p. 160‚Äì
187, 2003 .  
[9]  S. M. Lajevardi and Z. M. Hussain, ""Automatic facial expression 
recognition: feature extraction and selection,"" Signal,  Image and 
Video Processing, vol. 6, no. 1, pp. 159-169, 2012 .  
[10]  G. Zhao and M. Pietikainen, ""Dynamic texture recognition 
using local binary patterns with an application to facial 
expressions,"" IEEE Trans. Pattern Anal. Mach. Intell., vol. 29, no. 
6, p. 915‚Äì928, 2007 .  
[11]  T. Jabid, M. Kabir and O. Chae, ""Robust facial expression 
recognition based on local directional pattern,"" ETRI Journal, 
vol. 32, pp. 784-794, 2010 .  
[12]  M. H. Kabir, T. Jabid and O. Chae, ""A Local Directional Pattern 
Variance  (LDPv) based Face Descriptor for Human Facial 
Expression Recognition,"" 7th IEEE Int. Conf. on Advanced Video 
and Signal Based Surveillance, pp. 526-532, 2010  .  
[13]  C. Shan and R. Braspenning, ""Recognizing facial expressions 
automatically from video,"" in Handbook of ambient intelligence and 
smart environments , 2010 , pp. 479-509. 
[14]  C. Shan, S. Gong and P. W. McOwan, ""Robust facial expression 
recognition using local binary patterns,"" IEEE International 
Conference on Image Processing, 2005 .  
[15]  C. Shan and T. Gritti, ""Learning Discriminative LBP -Histogram 
Bins for Facial Expression Recognition,"" British Machine Vision 
Conference, 2008 .  
[16]  M. Song, D. Tao, Z. Liu, X. Li and M. Zhou, ""Image ratio features 
for facial expression recognition applic ation,"" IEEE Transactions 
on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 40, no. 
3, pp. 779-788, 2010 .  
[17]  L. Zhang and D. Tjondronegoro, ""Facial expression recognition 
using facial movement features,"" IEEE Transactions on Affective 
Comput ing, vol. 2, no. 4, pp. 219-229, 2011 .  
[18]  Y. Zhang and Q. Ji, ""Active and dynamic information fusion for 
facial expression understanding from image sequences,"" IEEE 
Transactions on Pattern Analysis and Machine Intelligence, vol. 27, 
no. 5, pp. 699 -714, 2005 .  
[19]  Y. Tian, T. Kanade and J. F. Cohn, ""Recognizing lower face 
action units for facial expression analysis,"" IEEE International 
Conference on Automatic Face and Gesture Recognition, pp. 484-490, 
2000 .  
[20]  M. Z. Uddin, J. J. Lee and T. -S. Ki m, ""An enhanced independent 
component -based human facial expression recognition from 
video,"" IEEE Transactions on Consumer Electronics, vol. 55, no. 4, 
pp. 2216 -2224 , 2009 .  
[21]  M. F. Valstar and M. Pantic, ""Combined support vector 
machines and hidden m arkov models for modeling facial action 
temporal dynamics,"" Human‚ÄìComputer Interaction, pp. 118-127, 
1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation
information: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing
12 IEEE TRANSACTIONS ON  AFFECTIVE COMPUTING,   MANUSCRIPT ID  
 2007 .  
[22]  T. Cootes, G. Edwards and C. Taylor, ""Active appearance 
models,"" IEEE Trans. on Pattern Analysis and Machine Intelligence, 
vol. 23, no. 6, pp . 681--685, 2001 .  
[23]  S. Lucey, I. Matthews, C. Hu, Z. Ambadar, F. D. l. Torre and J. 
Cohn, ""AAM derived face representations for robust facial 
action recognition,"" 7th International Conference on Automatic Face 
and Gesture Recognition, 2006 .  
[24]  A. B. Ashraf, S. Lucey, T. C. Jeffrey F. Cohn, Z. Ambadar, K. M. 
Prkachin and P. E. Solomon, ""The painful face ‚Äìpain expression 
recognition using active appearance models,"" Image and Vision 
Computing, vol. 27, no. 12, pp. 1788 -1796 , 2009 .  
[25]  B. Abboud, F. Davoine and M. Dang, ""Facial expression 
recognition and synthesis based on an appearance model,"" 
Signal Processing: Image Communication, vol. 19, no. 8, pp. 723-
740, 2004 .  
[26]  A. Asthana, J. Saragih, M. Wagner and R. Goecke, ""Evaluating 
aam fitting methods for facial expression recognition,"" 3rd 
International Conference on Affective Computing and Intelligent 
Interaction and Workshops, pp. 1-8, 2009 .  
[27]  C. Martin, U. Werner and H. -M. Gross, ""A real -time facial 
expression recognition system based on active appearance 
models using gray images and edge images,"" 8th IEEE 
International Conference on Automatic Face & Gesture Recognition, 
2008 .  
[28]  D. Cristinacce and T. F. Cootes, ""Feature Detection and Tracking 
with Constrained Local Models,"" in British Machine Vision 
Conference , 2006 .  
[29]  J. M. Saragih, S. Lucey and J. F. Cohn., ""Deformable model 
fitting by regularized landmark mean -shift,"" International 
Journal of Computer Vision, vol. 91, no. 2, pp. 200-215, 2011 .  
[30]  A. Asthana, S. Zafeir iou, S. Cheng and M. Pantic, ""Robust 
discriminative response map fitting with constrained local 
models,"" in IEEE Conference on Computer Vision and Pattern 
Recognition , 2013 .  
[31]  S. W. Chew, P. Lucey, S. Lucey, J. Saragih, J. F. Cohn, I. 
Matthews and S.  Sridharan, ""In the pursuit of effective affective 
computing: The relationship between features and registration,"" 
IEEE Transactions on Systems, Man, and Cybernetics, Part B: 
Cybernetics, vol. 42, no. 4, pp. 1006 -1016 , 2012 .  
[32]  M. Bartlett, G. Littlew ort, M. Frank, C. Lainscsek, I. Fasel and J. 
Movellan, ""Recognizing facial expression: machine learning and 
application to spontaneous behavior,"" in IEEE Computer Society 
Conf. on Computer Vision and Pattern Recognition , 2005 .  
[33]  M. Lyons, J. Budynek and S. Akamatsu, ""Automatic 
classification of single facial images,"" IEEE Trans. on Pattern 
Analysis and Machine Intelligence, vol. 21, no. 12, pp. 1357  - 1362  , 
Dec 1999 .  
[34]  A. Hadid, M. Pietikainen and T. Ahonen, ""A discriminative 
feature space for detecting and recognizing faces,"" in IEEE 
Computer Society Conf. on Computer Vision and Pattern 
Recognition , 2004 .  
[35]  S. L. Happy, A. George and A. Routray, ""A real time facial 
expression classification system using Local Binary Patterns,"" 
4th Int. Co nf. on Intelligent Human Computer Interaction, 2012 .  
[36]  A. Dhall, A. Asthana, R. Goecke and T. Gedeon, ""Emotion recognition using PHOG and LPQ features,"" in IEEE 
International Conference on Automatic Face and Gesture Recognition 
and Workshops , 2011.  
[37]  C. Shan, S. Gong and P. W. McOwan, ""Facial expression 
recognition based on local binary patterns: A comprehensive 
study,"" Image and Vision Computing, vol. 27, no. 6, pp. 803-816, 
2009 .  
[38]  K. I. Kim, K. Jung and H. J. Kim, ""Face recognition usi ng kernel 
principal component analysis,"" IEEE Signal Processing Letters, 
vol. 9, no. 2, pp. 40-42, 2002 .  
[39]  A. J. Calder, A. M. Burton, P. Miller, A. W. Young and S. 
Akamatsu, ""A principal component analysis of facial 
expressions,"" Vision research, vol. 41, no. 9, pp. 1179 -1208 , 2001 .  
[40]  H.-B. Deng, L. -W. Jin, L. -X. Zhen and J. -C. Huang, ""A new facial 
expression recognition method based on local gabor filter bank 
and pca plus lda,"" International Journal of Information Technology, 
vol. 11, no. 11, pp. 86-96, 2005 .  
[41]  Z. Zhang, Y. Yan and H. Wang, "" Discriminative filter based 
regression learning for facial expression recognition,"" 20th IEEE 
International Conference on Image Processing (ICIP), pp. 1192  - 1196  
, 2013  .  
[42]  C. Shan, S. Gong and P. W. McOwan, ""A comprehensive 
empirical study on linear subspace methods for facial 
expression analysis,"" IEEE Conf. on Computer Vision and Pattern 
Recognition Workshop, pp. 153-153, 2006 .  
[43]  S.-K. Oh, S. -H. Yoo and W. Pedrycz, ""De sign of face recognition 
algorithm using PCA -LDA combined for hybrid data pre -
processing and polynomial -based RBF neural networks: Design 
and its application,"" Expert Systems with Applications, vol. 40, no. 
5, pp. 1451 -1466 , 2013 .  
[44]  Y. Rahulamathavan , R.-W. P. J. A. Chambers and D. J. Parish, 
""Facial Expression Recognition in the Encrypted Domain Based 
on Local Fisher Discriminant Analysis,"" IEEE Transactions on 
Affective Computing, vol. 4, no. 1, pp. 83-92, 2013 .  
[45]  H. Mohammadzade and D. Hatzin akos, ""Projection into 
expression subspaces for face recognition from single sample 
per person,"" IEEE Transactions on Affective Computing, vol. 4, no. 
1, pp. 69-82, 2013 .  
[46]  S. Moore and R. Bowden, ""Local binary patterns for multi -view 
facial expressi on recognition,"" Computer Vision and Image 
Understanding, vol. 115, no. 4, pp. 541-558, 2011 .  
[47]  L. Zhong, Q. Liu, P. Yang, B. Liu, J. Huang and D. N. Metaxas, 
""Learning active facial patches for expression analysis,"" in IEEE 
Conference on Computer Vi sion and Pattern Recognition (CVPR) , 
2012 .  
[48]  L. Unzueta, W. Pimenta, J. Goenetxea, L. P. Santos and F. 
Dornaika, ""Efficient generic face model fitting to images and 
videos,"" Image and Vision Computing, vol. 32, no. 5, pp. 321-334, 
2014 .  
[49]  P. Vi ola and M. Jones, ""Rapid object detection using a boosted 
cascade of simple features,"" in IEEE Conference on Computer 
Vision and Pattern Recognition , 2001 .  
[50]  D. Nguyen, D. Halupka, P. Aarabi and A. Sheikholeslami, ""Real -
time face detection and lip fe ature extraction using field -
programmable gate arrays,"" IEEE Transactions on Systems, Man, 
and Cybernetics, Part B: Cybernetics, vol. 36, no. 4, pp. 902-912, 
1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See
http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation
information: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing
HAPPY AND ROUTRAY:  AUTOMATIC FACIAL EXP RESSION RECOGNITION USING FEATURES OF SA LIENT FAC IAL PATCHES  13 
 2006 .  
[51]  R. C. Gonzalez and R. E. Woods, Digital Image Processing (3rd 
Edition), Pearson Educ ation, 2008 .  
[52]  N. Otsu, ""A threshold selection method from gray -level 
histograms,"" IEEE Transactions on Systems, Man and Cybernetics, 
vol. 9, no. 1, pp. 62-66, 1979 .  
[53]  T. Ojala, M. Pietikainen and T. Maenpaa, ""Multiresolution gray -
scale and rot ation invariant texture classification with local 
binary patterns,"" IEEE Transactions on Pattern Analysis and 
Machine Intelligence, vol. 24, no. 7, pp. 971-987, 2002 .  
[54]  P. N. Belhumeur, J. P. Hespanha and D. Kriegman, ""Eigenfaces 
vs. fisherfaces: Rec ognition using class specific linear 
projection,"" IEEE Transactions on Pattern Analysis and Machine 
Intelligence, vol. 19, no. 7, pp. 711-720, 1997 .  
[55]  C. Cortes and V. Vapnik, ""Support -vector networks,"" Machine 
learning, vol. 20, no. 3, pp. 273 -297, 1995 .  
[56]  C.-W. Hsu and C. -J. Lin, ""A comparison of methods for 
multiclass support vector machines,"" IEEE Transactions on 
Neural Networks, vol. 13, no. 2, pp. 415-425, 2002 .  
[57]  M. J. Lyons, M. Kamachi and J. Gyoba, ""Japanese Female Facial 
Expressi ons (JAFFE),"" Database of digital images, 1997 . 
[58]  P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar and I. 
Matthews, ""The Extended Cohn -Kande Dataset (CK+): A 
complete facial expression dataset for action unit and emotion -
specified expression,"" in 3rd IEEE Workshop on CVPR for Human 
Communicative Behavior Analysis , 2010 .  
[59]  Y. Tian, T. Kanade and J. F. Cohn, ""Facial expression 
recognition,"" in Handbook of face recognition , London, Springer , 
2011 , pp. 487-519. 
[60]  M. Sokolova and G. Lapal me, ""A systematic analysis of 
performance measures for classification tasks,"" Information Processing & Management, vol. 45, no. 4, pp. 427-437, 2009 .  
[61]  P. A, N. H.A., S. M.G. and Y. N., ""Gauss ‚ÄìLaguerre wavelet 
textural feature fusion with geometrical  information for facial 
expression identification,"" EURASIP Journal on Image and Video 
Processing, 2012 .  
[62]  Z. Zhang, C. Fang and X. Ding, ""Facial expression analysis 
across databases,"" IEEE International Conference on Multimedia 
Technology, pp. 317 -320, 2011 .  
[63]  O. Jesorsky, K. Kirchberg and R. Frischholz, ""Robust face 
detection using the hausdorff distance,"" in 3rd International 
Conference on Audio - and Video -Based Biometric Person 
Authentication , Halmstad, Sweden, 2001 .  
 
 
 S L Happy has r eceived  the B.Tech. (Hons.) 
degree from Institute of Technical Education 
and Research (ITER), India in 2011. Now he is 
pursuing the M. S. degree from Indian Institute 
of Technology Kharagpur, India. His research 
interests include pattern recognition, com-
puter vision and facial expression analysis.  
Aurobinda Routray  has received his Masters 
degrees in 1991 from IIT Kanpur, India  and his 
PhD in 1999 from  Sambalpur University, India. 
He has also worked as a postdoctoral re-
searcher at  Purdue University, USA,  during  
2003 -2004 . He is currently working as a profes-
sor in the Department of Electrical Engineering, Indian Insti-
tute of Technology, Kharagpur. His research interest s include 
non-linear and statistical signal processing, signal based fault 
detection and di agnosis, real time and embedded signal pro-
cessing, numerical linear algebra, and data driven diagnostics.  
 

"
https://ieeexplore.ieee.org/document/7451244,"1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 1
BAUM-1: A Spontaneous Audio-Visual Face 1
Database of Affective and Mental States 2
Sara Zhalehpour, Onur Onder, Zahid Akhtar, Cigdem Eroglu Erdem, Member, IEEE 3
Abstract
In affective computing applications, access to labeled spontaneous affective data is essential for testing the
designed algorithms under naturalistic and challenging conditions. Most databases available today are acted
or do not contain audio data. We present a spontaneous audio-visual affective face database of affective and
mental states. The video clips in the database are obtained by recording the subjects from the frontal view using
a stereo camera and from the half-proÔ¨Åle view using a mono camera. The subjects are Ô¨Årst shown a sequence
of images and short video clips, which are not only meticulously fashioned but also timed to evoke a set of
emotions and mental states. Then, they express their ideas and feelings about the images and video clips they
watch in an unscripted and unguided way in Turkish. The target emotions, include the six basic ones (happiness,
anger, sadness, disgust, fear, surprise) as well as boredom and contempt. We also target several mental states,
which are unsure (including confused, undecided), thinking, concentrating, interested (including curious), and
bothered. Baseline experimental results on the BAUM-1 database show that recognition of affective and mental
states under naturalistic conditions is quite challenging. The database is expected to enable further research on
audio-visual affect and mental state recognition under close-to-real scenarios.
Index Terms
facial expression recognition, audio-visual affective database, emotional corpora, mental state recognition,
affective computing, spontaneous expressions, emotion recognition from speech, dynamic facial expression
database.4
F5
1
INTRODUCTION
6
W
e would like to acknowledge the support received from the Turkish ScientiÔ¨Åc and Technical Research Council (T ¬®UBÀôITAK) under project
110E056.
This work was done when all authors were with the Department of Electrical and Electronics Engineering, Bahcesehir University, Ciragan
Cad., Besiktas, Istanbul, Turkey.
E-mail: cigdem.eroglu@eng.bahcesehir.edu.tr, onurndr@yahoo.com
S. Zhalehpour is currently with INRS-EMT, Montreal, Canada.
E-mail: sara.zhalehpour@emt.inrs.ca
Z. Akhtar is currently with the Department of Mathematics and Computer Science, University of Udine, Via delle Scienze 206, 33100 Udine,
Italy.
E-mail: zahid.akhtar@uniud.it
Manuscript received April xx, xxxx; revised September xx, xxxx.
March 16, 2016 DRAFT
1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 2
Non-verbal messages constitute an important part of human-to-human communication, since they enhance 7
or modify our verbal messages and convey information about our emotional and mental states. Major compo- 8
nents of non-verbal messages are facial expressions, body/head gestures and the paralinguistic properties of 9
speech. The importance of emotions in our daily interactions motivated researchers to design and implement 10
automatic algorithms to recognize emotional expressions with the ultimate goal of achieving intelligent human- 11
machine interaction [1], [2]. As a result, the Ô¨Åeld of affective computing has attracted a lot of attention from 12
researchers in the last decade due to its wide range of application areas including multi-modal human computer 13
interaction, security [3] (lie-detection etc.), education, health-care [4], [5], marketing and advertising. 14
There are six basic facial expressions that have been shown to be universal, which are happiness, surprise, 15
anger, sadness, fear and disgust [6]. It has been reported that humans can recognize emotions such as happiness 16
and surprise easily even from low resolution images [7]. However, their recognition accuracy is very low for 17
anger and sadness, and the worst for fear and disgust. Trained observers are reported to achieve an average 18
facial expression recognition accuracy rate of 87% [7]. 19
Access to annotated affective databases is a prerequisite for researchers to train and test the performance of 20
their affect recognition algorithms. Collecting and annotating affective databases is a challenging task, especially 21
if natural (spontaneous) multi-modal expressions are desired. Below we Ô¨Årst brieÔ¨Çy review the state of the art 22
on affective databases available in the literature and then state the contribution and scope of this work. 23
1.1 Prior Work 24
Most of the affective databases that are accessible by researchers are acted and uni-modal [2], [8], [9], [10], 25
[11]. Predominantly, acted datasets are recorded under very constrained conditions and resulting expressions 26
are exaggerated. One of the most popular acted databases is the extended Cohn-Kanade database (CK+) [8], 27
which encompasses 123 subjects with 327 labeled sequences pursuant to six basic emotions (anger, happiness, 28
sadness, disgust, surprise and fear) [6] and contempt. The Ô¨Årst frame of each sequence contains a neutral 29
expression and the last frame of the sequence reÔ¨Çects the expression at its apex. Another image-based acted 30
facial expression database is the Jaffe database [12], that contains 219 images of 10 Japanese females exhibiting 31
the six basic emotions. The MMI database [13] consists of mostly posed videos of facial expressions showing 32
complete temporal patterns (i.e. starting from onset, which is followed by apex and offset phases). Some of the 33
videos contain spontaneous laughter. Bosphorus database [10] is a 3D facial expression database incorporating 34
various head poses and occlusions. 35
There are several audio-visual acted databases in the literature. The GEMEP dataset [9], [14] is comprised 36
of videos of 10 actors coached by a professional director. During the recording session, the actors were asked 37
speciÔ¨Åcally to utter combinations of meaningless phoneme sequences. It is worth mentioning that, out of 7000 38
audio-visual affective video clips reÔ¨Çecting 18 emotions, only 289 samples representing Ô¨Åve emotions (anger, 39
fear, joy, relief, sadness) were selected for the GEMEP-FERA challenge [14]. Another audio-visual acted database 40
is eNTERFACE [15], which contains video clips of 44 subjects uttering selected scripts in English while reÔ¨Çecting 41
the six basic emotions. 42
March 16, 2016 DRAFT
1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 3
TABLE 1
Overview of Audio-Visual Affective Databases. The acronyms in the table are: SBE: Six Basic Emotions (Happiness, Sadness,
Anger, Fear, Disgust, Surprise) and Neutral, NBE: Non-Basic Emotions, MS: Mental States
Database Posed
vs. Language Annotation Number
of Number
of
Non-Posed subjects video
clips
BAUM-1 Non-Posed T
urkish SBE,
2 NBE, 3 MS 31 1222
BAUM-2
[24] Non-Posed English,
Turkish SBE,
1 NBE 286 1047
AFEW
[22] Non-Posed English SBE 330 1426
Belfast
[21] Non-Posed English SBE 100 239
Semaine
[17] Non-Posed English 3
BE, 10 NBE 150 959
IEMOCAP
[20] Posed English 3
BE, 1 NBE 10 1039
GEMEP
[14] Posed None SBE,
12 NBE 10 7000
Fanelli
[27] Posed,
3D English SBE
+ MS 14 1109
eNTERF
ACE [15] Posed English SBE 44 1166
Assembling
datasets that encompass spontaneous or naturalistic expressions is a strenuous and tedious job 43
[16], [17], [18], [19]. The SEMAINE [17] database was collected under constrained lab settings and contains 44
naturalistic expressions from 150 subjects, who are in a conversation with a ‚Äúsensitive artiÔ¨Åcial listener‚Äù. 45
Another induced emotional expressions database is the FEED database [18], which contains 18 subjects. The 46
scripted affective dyadic conversations were induced by 10 professional actors in the IEMOCAP database 47
[20], which resulted in more than 10 hours of audio-visual recordings. In particular, facial motions of the 48
actors were captured using a system utilizing markers, while the utterances were labeled along the three 49
dimensions indicating valence, activation, and dominance. Recently, DISFA [19] has been introduced, which 50
is a spontaneous facial action intensity database. Different from most of the data sets, the Belfast naturalistic 51
database [21] is comprised of clips accumulated manually from TV programs as well as clips recorded through 52
dyadic discussions. 53
There are recent efforts towards collecting naturalistic databases from movies such as the AFEW [22] 54
and BAUM-2 databases [23], [24]. Although using movie clips to gather affective databases do not replace 55
pure spontaneity, they are more naturalistic and challenging as compared to acted databases. Several other 56
approaches use crowdsourcing to collect facial expressions from the internet [25] in response to short video 57
clips or to score the captured facial expressions [26]. However, they mainly concentrate on positive emotions 58
and do not consider mental states. Also, very few efforts for naturalistic 3D audio-visual [27] or 3D facial 59
expression databases [28] have been rendered using costly 3D scanners. 60
In Table 1, we give a summary of the audio-visual databases available in the literature. It can be observed 61
that BAUM-1 is the only non-posed database in the literature that contains the six basic emotions as well as 62
several non-basic emotions and mental states in a language different from English. 63
March 16, 2016 DRAFT
1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 4
1.2 Contribution and Scope 64
As summarized in the previous section, there are a few audio-visual naturalistic affective databases in the 65
literature, which are mostly in English and do not contain any expressions related to the mental states. In this 66
work, we present a re-acted spontaneous audio-visual face database of affective and mental states in Turkish. 67
The subjects watch a sequence of still images and short video clips, which are meticulously devised and timed 68
to evoke a set of emotions and mental states. Emotion elicitation using video clips is a well-validated method 69
[29], [30]. The subjects express their feelings and ideas about the stimuli they have watched on the screen 70
in their own words, without using predetermined scripts. The database contains recordings reÔ¨Çecting the six 71
basic emotions (happiness, anger, sadness, disgust, fear, surprise ) as well as boredom and contempt . The 72
database also contains several mental states, namely unsure (confused, undecided), thinking, concentrating, 73
andbothered. The subjects are not guided in any way about how to perform the facial expressions. The database 74
consists of simultaneous recordings of subjects using two cameras, both of which can record in high deÔ¨Ånition 75
format. The Ô¨Årst one is a stereo camera, which is placed in front of the subject at the top of the screen. The 76
second one is a mono camera, which is placed to capture a half proÔ¨Åle view of the subject. The categorical 77
annotations of the recordings also include a score indicating the intensity of the emotion on a scale of 0 to 5. 78
The organization of the paper is as follows. In Section 2, we describe the data acquisition process in detail 79
including emotion elicitation and recording. In Section 3, we give the details of the post-processing steps 80
including segmentation, annotation and organization of the database. In Section 4, we outline the used method 81
for multi-modal recognition of affective and mental states for the purpose of establishing baseline results on 82
the database. In Section 5, experimental results on BAUM-1 database are provided and compared with the 83
results on the eNTERFACE [15] database. Finally in Section 6, we provide conclusions and future directions for 84
research. 85
2 M ETHOD FOR DATAACQUISITION 86
The BAUM-1 database has been recorded in a studio, which was designed speciÔ¨Åcally as described below. In 87
the studio, subjects Ô¨Årst watch a stimuli video on a monitor in front of them and then express their feelings in 88
their own words while they are being recorded with multiple cameras. 89
2.1 Recording Setup 90
In the designed studio a green curtain has been used as background (see Fig. 1(a), (b)). A Sony HDR-TD20 91
Stereo HD camera is used for recording the frontal view of the face and a Sony HDR-XR200 Mono camera is 92
used for recording from a half proÔ¨Åle view with an angle of approximately 45 degrees. Illumination is provided 93
by three 1000 Watt tungsten (Red Head) lights directed towards the ceiling to provide a smooth lighting. Two 94
spherical lights are located carefully on both sides of the face to minimize shadows on the face of the subject. 95
For recording the audio, a Rode NTG 2 shotgun (directional) microphone has been used. A monitor is located 96
in front of the subject at eye level to watch the stimuli video (see Fig. 1(c)). A clap-board has been used to assist 97
in the synchronization of audio and video streams during post-processing. 98
March 16, 2016 DRAFT
1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 5
(a)
 (b)
 (c)
Fig.
1. (a),(b) The setup of the studio showing the location of stereo and mono cameras, microphone and lights. (c) The subjects
watch the stimuli shown on the monitor and express their ideas and feelings about the stimuli in their own words.
2.2 Recording of Acted Clips 99
Before starting the spontaneous session, we asked the subjects to utter several sentences in Turkish with certain 100
emotions and imagining speciÔ¨Åc scenarios, which is a similar procedure as in the eNTERFACE dataset [15]. We 101
call the acted part of the database as BAUM-1a. In the acted recordings, we targeted 8 emotions and mental 102
states, which are listed below together with the imagined scenarios: 103
Happiness: You just learned that you have won the lottery and you are telling it to a friend of yours. 104
Sadness: You have to explain your friend that his father has passed away. 105
Fear: You are kidnapped and the kidnappers are holding a gun towards you. You have to beg for your 106
life. 107
Anger: You have just caught the thief who has stolen your wallet. 108
Disgust: You have discovered an insect in your soup. 109
Confusion: You didn‚Äôt understand the lecture and asking the lecturer to explain again. 110
Boredom: You have been waiting for a bus for at least an hour. 111
Interest (Curiosity): You want to learn about your friend‚Äôs secret. 112
2.3 Elicitation of Emotional and Mental States for Spontaneous Recordings 113
In BAUM-1 database, our main focus was to obtain spontaneous audio-visual expressions of emotional and 114
mental states. In order to bring a subject into the mood of the emotion, we Ô¨Årst asked each subject to watch 115
a carefully designed ‚Äústimuli video‚Äù, which contains a sequence of images and video clips selected to evoke 116
the target emotions and mental states. The target emotions and mental states are: happiness, anger, sadness, 117
disgust, fear, surprise, boredom, contempt, unsure (confused etc.), thinking, concentrating, and bothered. 118
Emotion elicitation using Ô¨Ålms is a well-validated procedure in the literature [29], [31]. While watching a 119
certain portion of the stimuli video (or shortly after that), the subjects are asked to explain their feelings and 120
ideas about the video or the image just shown. Subjects are also recorded while they are watching the video 121
to capture spontaneous facial expressions. After the recording process, the whole recording is segmented and 122
annotated, which will be explained in more detail in Section 3. 123
The total duration of the stimuli video is approximately 35 minutes and there are 30 second intervals in 124
between video clips or images for the subjects to express their own feelings and ideas in their own words. 125
March 16, 2016 DRAFT
1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 6
The total recording session lasts about 50 minutes for each subject. The audio-visual stimuli video consist of 126
29 images and videos that are expected to elicit the desired emotions and mental states. These 29 images and 127
video clips (scenes) have been carefully selected from a larger set of candidates retrieved from the internet and 128
the International Affective Picture System [32] as well as several works of Escher [33] (to elicit confusion). The 129
selection from this larger set has been done by making a demonstration of the whole collection to a jury, which 130
consisted of 19 students from the department of psychology, who were asked to give a score to each video on 131
a scale of 0 to 5. The stimuli, which received an average score below 2 were eliminated and highest scoring 132
stimuli were retained. The ordering for the elicitation of the emotions were designed in such a way that the 133
most negative and intense video clips (such as an autopsy scene) were shown towards the end. There were also 134
neutral clips in between emotion transitions so that the subjects had enough time to recover from one emotion 135
and were ready to enter the mood of the next emotion. Images were also displayed long enough to enable 136
elicitation of the target emotion or mental state. In Table 2, we summarize the contents of each video clip or 137
image in the stimuli video together with the target emotion or mental state. 138
2.4 Participants and the Spontaneous Recording Process 139
The data was collected from 31 subjects, 17 of which are female, which are shown in Fig. 2. All subjects are 140
native speakers of Turkish, and have an age range of 19-65. Prior to each recording session, we explained the 141
whole procedure to the subjects in detail and warned them about possible disturbing scenes (e.g. autopsy scene), 142
indicating that they can quit the session at any point. Each subject also signed a consent form, which states that 143
the subject has understood and accepted the procedure and indicates whether all recordings of the subject can 144
be used and shared for research purposes. All subjects except one of them (subject 5) gave permission for their 145
images to be used in publications. The subjects completed the 50 minute long recording process by watching the 146
stimuli video on their own and expressed their thoughts and feelings with their own words when prompted. 147
Several limitations of the used emotion elicitation procedure has been observed. First, the subjects may not 148
express completely spontaneous emotions since they are aware of the cameras and they are being recorded. 149
Hence, some subjects may tend to suppress their emotions, whereas some others may tend to exaggerate. We 150
also had a difÔ¨Åculty in elicitation of fear since it is difÔ¨Åcult to frighten the subjects in a secure ofÔ¨Åce environment, 151
which is inline with the observations stated in [29]. Another issue is the ‚Äúdiscreteness‚Äù of the emotions [29], 152
which means inducing a single emotion/mental state, with no traces of others. It was observed that this was 153
especially difÔ¨Åcult since several emotions or mental states co-exist, which can be observed from the labels given 154
by the annotators. This is explained in more detail in Sections 3.2 and 3.3 below. 155
3 P OSTPROCESSING AND ANNOTATION 156
After the video of a subject is recorded, it is divided into smaller segments, which are then annotated. Below 157
we give the details of the segmentation and annotation processes. 158
March 16, 2016 DRAFT
1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 7
TABLE 2
Contents and ordering of the stimuli video. Target emotions or mental states are also listed.
V
ideo/picture number Content Target emotional or mental state
1 Horses
illusion (image) Unsure (confusion, undecided)
2 Stair paradox (image) Unsure, Concentrating, Thinking
3 Wheel illusion (image) Unsure, Concentrating, Thinking
4 Lines illusion (image) Unsure, Concentrating, Thinking
5 Puppies (video) Happiness/Amusement
6 Funny advertisement (video) Happiness/Amusement
7 Funny stand-up show (video) Happiness/Amusement
8 Space (image) Neutral
9 Parking a car (video) Contempt
10 Children (image) Happiness
11 Crazy sportsmen (video) Surprise
12 Bored man (image) Boredom
13 A video of family Ô¨Åght Anger/Sadness
14 Child and vulture (image) Sadness
15 Sick newborn (image) Sadness
16 Dead child and father (image) Sadness/Anger
17 Murdering of a cat (video) Anger/Sadness
18 Fisherman (image) Neutral/Boredom
19 Thinking person (image) Neutral/Boredom
20 Man pointing a gun (image) Anger
21 Shark (image) Fear
22 A vomiting man (image) Disgust
23 Waterfalls (image) Neutral
24 Car accident (video) Sadness
25 Drunk driver (video) Sadness/Anger
26 Badly injured hand (image) Disgust
27 Clips from horror movies Fear/Bothered
28 Autopsy video Fear/Bothered
29 Illusionist cutting his wife Surprise/Fear
3.1
Segmentation and Organization of the Database 159
During post-processing, Ô¨Årst the recorded stereo and mono video and audio streams are synchronized using 160
the clap-board information in the audio and video streams. The precision of the synchronization is about 1 161
frame, which corresponds to 1/30 seconds. Then the recording is segmented into short video clips using Sony 162
Vegas software (see Fig. 3) so that there is a single emotion or mental state expression in a clip. There might be 163
a few clips which contains simultaneous expression of two emotions (e.g., happily surprised). The segmented 164
clips are then rendered by fusing the audio and video channels and then saved by giving a name indicating 165
the subject and clip number. Original stereo (frontal) and mono (half-proÔ¨Åle) videos were recorded in high 166
deÔ¨Ånition resolution ( 10801920) and video clips were rendered in standard deÔ¨Ånition ( 576720) resolution. 167
March 16, 2016 DRAFT
1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 8
Fig.
2. The 31 subjects who volunteered for the recordings of BAUM-1 database. All subjects (except subject 5) gave their consent for
their images to be used in publications.
(a) (b)
Fig.
3. (a) Segmentation of the recording into clips using Sony Vegas software. (b) GTrace annotation tool is used for annotating the
clips [35].
A mono frontal view version of the database is also prepared, which consists of the right view of the stereo pair 168
downsampled to a resolution of 480854[34]. An example recording can be seen in Fig. 4. We also provide 169
subtitles in English using .srt Ô¨Åles for each video clip. 170
3.2 Annotation 171
BAUM-1a database contains clips containing expressions of Ô¨Åve basic emotions (happiness, sadness, anger, 172
disgust, fear) along with expressions of boredom, confusion (unsure) and interest (curiosity). BAUM-1s database 173
contains clips reÔ¨Çecting six basic emotions and also expressions of boredom, contempt, confusion, thinking, 174
concentrating, bothered, and neutral. We used some of the mental state labels indicated by the taxonomy in [36] 175
to annotate the mental states. 176
Each clip in the database is annotated by Ô¨Åve annotators using the GTrace tool [35] (see Fig.3(b)), which has 177
a simple and friendly user interface. Prior to the annotations, the annotators were made familiar with the facial 178
March 16, 2016 DRAFT
1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 9
(a)
 (b)
(c)
Fig.
4. An example recording from the BAUM-1 database. (a) A frontal stereo recording. (b) A mono recording from half proÔ¨Åle. (c)
The speech channel.
expressions by using the Mind Reading Software [37]. The annotator watches the clip as many times as he/she 179
wants, selects the emotion or mental state that is dominant in the clip and Ô¨Ånally gives a score between 0 and 180
5, which represents the intensity of the emotional or mental state expression in the clip. Finally, each clip is 181
given a label by using majority voting over the Ô¨Åve annotators. The scores of the selected label are averaged to 182
determine the Ô¨Ånal score for the clip. 183
3.3 Inter-Annotator Agreement 184
We used the Kappa statistic [38], [39] to estimate the amount of inter-annotator agreement in the BAUM-1 185
database, which assesses differences between the agreement among annotators (i.e. ‚Äúobserved‚Äù agreement) and 186
agreement that would occur by chance alone (i.e. ‚Äúexpected‚Äù agreement). The Kappa statistic ranges between -1 187
to 1, where 1 indicates perfect agreement, while 0 indicates agreement by chance. The negative values specify 188
systematic disagreement between annotators. The expression for calculation of Kappa ( ) is as follows: 189
=Po Pc
1 Pc; (1)
wher
ePodenotes the relative observed agreement among raters and Pcdenotes the hypothetical probability 190
of agreement by chance. The Kappa value was calculated between each pair of annotators and then averaged. 191
The average Kappa statistic yielded a value of 0.67 for the BAUM-1a database, which can be considered as 192
substantial agreement [39] between annotators. The highest agreement was between annotator 1 and annotator 193
3 (see Fig.5), with a value of 0.76. The average Kappa value for BAUM-1s was 0.51, with a maximum pairwise 194
agreement of 0.57, which can be interpreted as moderate agreement. We noticed that annotator 4 had the least 195
agreement with the other annotators. If we exclude annotator 4 from the averaging process, the Kappa values 196
become 0.72 and 0.54 for BAUM-1a and BAUM-1s, respectively. Moderate agreement is expected for BAUM-1s 197
database since the emotional and mental state expressions are spontaneous and sometimes subtle, hence they 198
are challenging to recognize even for humans. There are also multiple emotions in some sequences (e.g. angrily 199
surprised, happily surprised) and the annotators might have a difÔ¨Åculty in choosing the dominant emotion. 200
3.4 Facial Feature Point Tracking 201
In many facial expression recognition algorithms, the Ô¨Årst step is to track the location of salient points (i.e.landmarks) 202
on the face. We used three different facial landmark tracking methods and compared them experimentally [40], 203
March 16, 2016 DRAFT
1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 10
Fig.
5. The Kappa values for each pair of annotators for BAUM-1a database.
(a)
 (b)
 (c)
Fig.
6. (a) Facial landmarks (cyan stars) and the head pose angle (written in forehead) have been successfully tracked using the
method in [40] for most clips in the database. (b) Facial feature tracking sometimes fails under sudden head movements [40]. (c)
CHEHRA [41] (left) and IntraFace [42] (right) face trackers have been observed to give better results in difÔ¨Åcult cases.
[41], [42], [43]. In [40], a model based on mixtures of trees is used, which shows a good performance over a wide 204
range of head poses from frontal to proÔ¨Åle. CHEHRA tracker [41] is based on a cascade of linear regressors 205
for incremental training of robust discriminative deformable models, which can automatically adapt to person- 206
speciÔ¨Åc properties and imaging conditions. The IntraFace tracker [42] uses a supervised descent method for 207
non-rigid image alignment to track proÔ¨Åle-to-proÔ¨Åle faces. 208
Several landmark tracking examples can be seen in Fig. 6 (a), which shows that the method in [40] works 209
acceptably well for tracking a total of 68 facial landmarks on the face, which are shown with cyan star signs. 210
The head pose angle in the yaw direction is also estimated in the range [-90,90] degrees with an increment of 211
15 degrees, which are shown as written in the forehead in Fig. 6 (a). The face tracker [40] sometimes fails to 212
estimate the location of the facial landmarks and the head pose as can be seen in Fig. 6(b). The facial landmarks 213
are visually inspected, and acceptable tracking results are used in the facial expression recognition experiments 214
(e.g. 1184 of 1222 sequences have acceptable results in BAUM-1s dataset). In such difÔ¨Åcult cases, CHEHRA and 215
IntraFace trackers have been observed to give better tracking results as shown in Fig. 6 (c), which track 49 facial 216
points. 217
March 16, 2016 DRAFT
1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 11
TABLE 3
Basic Properties of BAUM-1 Database.
Property BAUM-1a BAUM-1s
Acted Spontaneous
Number
of clips 273 1222
Number of subjects 31 (13 female, 18 male)
Age Range of Subjects 19-65 19-65
Language of clips Turkish Turkish
Clip Length (min/max/average) in sec. 0.60/16.98/4.07 0.43/9.34/1.82
Video Format AVI MP4/AVI
Number of annotators 5 5
Kappa value 0.72 0.54
Number
of Clips Per Expression
(min/max/average scores over 5)
Happiness 27 (2.00/4.36/3.28) 179 (1.00/4.86/3.30)
Anger 43 (2.43/4.83/3.43) 56 (1.00/5.00/2.86)
Sadness 38 (1.97/3.10/3.08) 139 (1.00/4.52/2.83)
Disgust 35 (2.50/4.45/3.51) 86 (1.00/4.90/3.53)
Fear 36 (1.61/4.49/3.30) 38 (1.00/4.70/2.98)
Surprise - 43 (1.00/4.50/3.38)
Boredom 27 (2.12/4.65/3.16) 22 (1.00/4.40/3.24)
Contempt - 16 (1.50/4.30/3.20)
Unsure 38 (1.78/4.20/2.97) 148 (1.00/5.00/3.02)
Interest 29 (2.07/3.83/3.00) -
Neutral - 187
Thinking - 112 (1.10/4.33/2.99)
Concentrating - 64 (1.00/4.30/2.77)
Bothered - 91 (1.00/4.40/3.05)
3.5
Properties and Novelties of the BAUM1 Database 218
In Fig. 7 and Fig. 8 some example frames from the clips in BAUM-1a and BAUM-1s databases are shown, 219
respectively. The properties of the databases are summarized in Table 3. The BAUM-1 database is a novel 220
contribution to the affective computing area since it contains spontaneous recordings of the six basic emotions 221
and several mental states in Turkish. 222
3.6 Availability 223
The database is publicly available for research purposes only and can be obtained ofÔ¨Åcially upon request via 224
the web site [34]. 225
March 16, 2016 DRAFT
1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 12
(a)
(b) (c) (d) (e) (f) (g) (h)
Fig.
7. Images selected from the video recordings in the BAUM-1a acted database are shown for subject 22 (top) and subject 11
(bottom). The Ô¨Ågures in parenthesis indicate the sequence and frame numbers of (top / bottom) images. (a) Anger (s6-f75 / s7-f83)
(b) Boredom (s9-f118 / s10-f12) (c) Disgust (s7-f10 / s8-f11) (d) Fear (s4-f108 / s6-f53) (e) Interest (s10-f68 / s11-f2) (f) Happiness
(s1-f49 / s3-f74) (g) Sadness (s3-f46 / s4-f74) (h) Unsure (s8-f10 / s11-f11)
(a)
(b) (c) (d) (e) (f) (g) (h)
(i)
(j) (k) (l) (m)
Fig.
8. Example frames from the clips in the BAUM-1s spontaneous database for subject 22 (top) and subject 21 (bottom). The Ô¨Ågures
in parenthesis indicate the sequence and frame numbers of (top / bottom) images. (a) Anger (s32-f3 / s50-f19) (b) Boredom (s67-f44
/ s59-f6) (c) Bothered (s35-f91 / s22-f6) (d) Concentrating (s30-f82 / s21-f15) (e) Contempt (s56-f22 / s36-f130) (f) Disgust (s66-f92 /
s78-f14) (g) Fear (s59-f18 / s63-f15) (h) Happiness (s22-f15 / s10-f19) (i) Neutral (s40-f46 / s48-f50) (j) Sadness (s55-f43 / s52-f51)
(k) Surprise (s47-f7 / s62-f21) (l) Thinking (s31-f42 / s71-f3) (m) Unsure (s43-f102 / s11-f6)
March 16, 2016 DRAFT
1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 13
4 M ULTI-MODAL RECOGNITION OF AFFECTIVE AND MENTAL STATES 226
In order to demonstrate the usefulness and the challenging nature of the collected BAUM-1 database, we 227
conducted multi-modal affective and mental state recognition experiments on the acted BAUM-1a and sponta- 228
neous BAUM-1s databases. In these experiments, we employ a multi-modal affect recognition algorithm based 229
on apex frame selection that was recently developed by the authors [44], [45]. Below, we brieÔ¨Çy explain the 230
visual and audio features used in the experiments, and the fusion process. We also brieÔ¨Çy summarize the apex 231
frame selection method presented in [44], [45] for the sake of completeness. The experimental results are given 232
in Section 5. 233
4.1 Extraction of Visual Features from Video 234
There are many approaches in the literature for facial expression recognition (FER) from images [2], [46]. An 235
affective video contains many frames, where emotions are expressed with varying intensities in each frame. 236
Thus, one of the biggest challenges in emotion recognition from video is determining which frames to use and 237
how to use them in order to attain the maximum possible recognition accuracy. A propitious technique is to 238
use a single frame or a set of selected frames representing the emotional expression with high intensities (i.e. 239
at the apex or peak phase). This approach assumes that there is only a single emotion expressed in the whole 240
video clip. In this work, we use an approach based on apex frame selection [45], which is summarized below. 241
The peak frame selection is preceded by a face detection and alignment process. Then, the procedure followed 242
for computation of visual features is explained. 243
4.1.1 Face Detection and Alignment 244
Before selecting the peak frames and extracting the facial features from the video clip, the face is detected 245
or tracked at each frame of the video and aligned so that global transformations of the head are minimized 246
between frames. There are many approaches for face detection and tracking in the literature [40], [47], [48], [49]. 247
In this work, the locations of eyes in all frames are detected utilizing three facial feature trackers, namely the 248
algorithms by Zhu et. al. [40], CHEHRA [41] and IntraFace [42]. The face region is rescaled and cropped to a 249
size of 168126so that the distance between the eye centers is 64 pixels. The face region is tessellated into 250
sub-blocks of size 86 = 48 and the 18 sub-blocks that are irrelevant to the expression are discarded (e.g 251
around the hair and background) [24], [45]. In the remaining (relevant) blocks, which are numbered from 1 to 252
30, we extract the Local Phase Quantization features as described below. 253
4.1.2 Facial Features 254
We experimented with two different facial features, namely LPQ [50] and POEM [51] features, which are brieÔ¨Çy 255
described below. 256
Local Phase Quantization (LPQ) features were proposed for blur-insensitive image texture classiÔ¨Åcation [50] 257
and have also has been successfully employed for facial expression recognition [52]. Local Phase Quantization is 258
similar to Local Binary Patterns (LBP) [53] in the sense that they both use local histograms to construct feature 259
vectors. LBP features have has also been popular for facial expression recognition [54], [55]. Since LPQ has 260
March 16, 2016 DRAFT
1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 14
demonstrated better performance than LBP for facial expression recognition [22], [24], [50], [52], we adopted 261
the former one to extract texture-based (i.e. appearance) features from facial images. 262
LPQ features use the phase information of the 2D short-term DFT in local neighborhoods. The resulting 263
DFT is sampled at four frequencies. The samples are decorrelated, quantized and represented using integers 264
between 0-255. Finally, 256-bin histograms in sub-blocks of the face region are concatenated and used as feature 265
vectors to represent the face for classiÔ¨Åcation. More details about extraction of LPQ features can be found in 266
[50]. 267
We exploited the LPQ code available from [56] in our experiments. The DFT is computed in a uniform 268
window and the window size is 33. The resulting LPQ feature vector is of length 7680, since there are 30 269
blocks, each of which is represented by a 256-bin LPQ histogram. 270
Patterns of Oriented Edge Magnitudes (POEM) features [51] of a pixel are calculated by replacing the 271
intensity values in the calculation of the traditional LBP features by gradient magnitudes using the accumulated 272
local histogram of gradient directions over a region around that pixel. Since POEM features are calculated at 273
different scales using cells and blocks, it can capture both local and global information. It is also more robust to 274
lighting variations as compared to LBP since gradient magnitudes are used instead of pixel intensities. 275
4.1.3 Peak Frame Selection 276
We used the maximum dissimilarity based peak frame selection (MAXDIST) method [45] in the experiments. It 277
is assumed that the potential peak frames are ‚Äúmaximally dissimilar‚Äù to the rest of the frames in the video clip. 278
Hence, Ô¨Årst the dissimilarity between frames of a video clip are computed using appearance-based features of 279
the face (e.g. the LPQ features). Then, these scores are arranged in a dissimilarity matrix so that the elements in 280
theithrow of the matrix represent the distance scores between the LPQ histogram vectors of frame iand the 281
rest of the frames in the video clip. The scores of each row of the dissimilarity matrix are then averaged and 282
ordered. Finally, the peak frames are determined by selecting rows corresponding to the highest Kscores, since 283
those frames are estimated to be the most ‚Äúdissimilar‚Äù frames in the video clip. The reader is referred to [45] 284
for further details of the MAXDIST method. 285
During the experiments, choosing six peak frames for each video clip have been found to give good results. 286
The Ô¨Ånal visual feature vector, x1, of a video clip is calculated by averaging the LPQ feature vectors of the 287
selected peak frames. 288
4.2 Extraction of Speech Features 289
The Mel-Frequency Cepstral CoefÔ¨Åcients (MFCC) [57] and relative spectral features (RASTA) based on per- 290
ceptual linear prediction (PLP) [58] were utilized to calculate the audio features for emotion recognition. As 291
a pre-processing step, silent intervals of audio including the leading and trailing edges were eliminated by 292
soft-thresholding the energy over short windows. Then, the MFCC and RASTA-PLP features were extracted 293
using 12 and 20 order Ô¨Ålters, respectively, utilizing a window of length 25msec and an overlap ratio of 50%. 294
Finally, the 12 MFCC and 13 RASTA-PLP coefÔ¨Åcients were merged with the Ô¨Årst and second time derivatives 295
March 16, 2016 DRAFT
1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 15
and nine statistical functions (such as min, max etc.) were extracted [45]. The above feature extraction process 296
produces an audio feature vector, x2, of length 759 = 675 , which is used for classiÔ¨Åcation. 297
4.3 ClassiÔ¨Åcation and Fusion of Facial and Speech Features 298
The well-known Support Vector Machine (SVM) [59] classiÔ¨Åer was employed for classiÔ¨Åcation of audio and 299
video features. The SVM classiÔ¨Åer used for the video features utilizes a linear kernel to surpass the curse of 300
dimensionality problem since the dimension of video features is high. An SVM classiÔ¨Åer with a radial basis 301
kernel was used for audio features using one-against-all approach. It is worth noting that audio features were 302
normalized to the interval [0, 1] before classiÔ¨Åcation. 303
A decision level fusion technique was applied to integrate the decision probabilities of each modality and 304
emotion. After a thorough investigation of several probability fusion approaches [60], [61], it was inferred that 305
theweighted product rule was the most successful [62] in our experiments. In particular, in the weighted 306
product rule, the probabilities attained from each modality for a clip under test are multiplied and then the 307
label of the maximum product is chosen [45]. 308
We represent the feature vectors of the audio and visual modalities with x2andx1, respectively. Let 2and 309
1denote the trained classiÔ¨Åers for the audio and visual modalities, where the probability estimated for the kth310
emotion/mental state in each modality is denoted by P(~!kjxi; i); i= 1; 2. These probabilities are merged as 311
given below: 312
P(!kjx1; x2) =2‚àè
i=1[P(~!kjxi; i)]Wi; k= 1; 2; : : : ; 6 (2)
!= max
kP(!kjx1; x2); k= 1; 2; : : : ; 6; (3)
where ~!kand!krepresent the label for the kthemotion without and with fusion, respectively. The parameter 313
!represents the estimated emotion label of the video clip, and Widenotes the weight used for each modality. 314
5 B ASELINE AUDIO-VISUAL AFFECTIVE AND MENTAL STATE RECOGNITION EXPERIMENTS 315
In this section, we present the single modality and multi-modal affective and mental state recognition results on 316
the collected BAUM-1a and BAUM-1s databases. We also carried out experiments on the eNTERFACE database, 317
which is a well-known acted audio-visual database in the literature, which contains six basic emotions. In 318
all the experiments given below, we employed 5-fold subject independent cross-validation to ensure subject 319
independent results. 320
5.1 Results on BAUM-1a database 321
There are 273 acted video clips in the BAUM-1a database representing 8 emotional and mental states. The 322
number of clips in each class are given in Table 3. We conducted two sets of experiments on the BAUM-1a 323
database using LPQ features and IntraFace tracker [42]. In the Ô¨Årst set, we used the Ô¨Åve basic emotions, namely, 324
anger, disgust, fear, happiness, and sadness. In the second set of experiments we also included boredom, interest 325
March 16, 2016 DRAFT
1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 16
TABLE 4
Single and multi-modal affective and mental state recognition accuracies on BAUM-1a database using LPQ features and IntraFace
tracker [42].
5
Basic Emotions 8 Emotions/
Mental States
V
ideo-Based 47.44 % 31.24 %
Audio-Based 71.71 % 63.53 %
Audio-Visual (sum rule) 72.33 % 61.09 %
Audio-Visual (product rule) 71.56 % 61.24 %
Audio-Visual (weighted product rule) 75.32 % 65.84 %
T
ABLE 5
Confusion matrix: BAUM-1a database, 5 emotions, audio modality. Figures represent percentages and the average recognition rate
is 71.71%.
Anger
Disgust Fear Happiness Sadness
Anger 87.88 6.26 1.82
0.0 4.04
Disgust 10.0 73.33 5.0 3.33 8.33
Fear 23.69 8.19 46.62 7.00 14.50
Happiness 10.00 6.19 12.38 60.57 10.86
Sadness 2.00 2.50 5.33 0.00 90.17
and
unsure in addition to the Ô¨Åve basic emotions. As we can see in Table 4, the audio-based recognition accuracy 326
(71.71%) is higher than the video-based accuracy (47.44%) for the 5 emotion case. The audio-visual accuracy is 327
75.32% using the weighted product rule with W1= 1andW2= 3. The confusion matrices are given in Table 5, 328
Table 6 and Table 7 for the 5 emotion experiments. We can observe from the confusion matrices that anger 329
(87.88%) and sadness (90.17%) have the highest two recognition rates from the audio channel (see Table 5), 330
and disgust (59.33%) and sadness (63.56%) have the highest two recognition rates from the video channel (see 331
Table 6). All the emotions except anger beneÔ¨Åt from the fusion process and the average recognition rate after 332
fusion rises to 75.32% (see Table 7). 333
The gap between audio (63.53%) and video accuracies (31.24%) becomes larger when we include boredom, 334
interest and unsure to the experiments. The confusion matrices for the 8 class experiments are given in Table 8, 335
Table 9 and Table 10. Similar to the 5 class case, anger (82.75%) and sadness (76.78%) have the highest two 336
recognition rates from the audio channel (see Table 8). Disgust (74.13%) and anger (52.97%) have the highest 337
two recognition rates from the video channel (see Table 9). Boredom can not be recognized from the video 338
channel at all. The average recognition rate rises to 65.84% after decision level fusion. 339
March 16, 2016 DRAFT
1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 17
TABLE 6
Confusion matrix: BAUM-1a database, 5 emotions, visual modality using LPQ features and IntraFace tracker. Figures represent
percentages and the average recognition rate is 47.44%.
Anger
Disgust Fear Happiness Sadness
Anger 51.77 9.87 12.93
17.37 8.06
Disgust 12.50 59.33 11.50 0.00 16.67
Fear 21.21 10.19 32.90 16.52 19.17
Happiness 13.52 9.52 48.10 22.67 6.19
Sadness 13.11 11.33 6.00 6.00 63.56
T
ABLE 7
Confusion matrix: BAUM-1a database, 5 emotions, audio-visual using LPQ features and IntraFace tracker. Fusion is done with
weighted product rule (W 2= 3). Figures represent percentages and the average recognition rate is 75.32%.
Anger
Disgust Fear Happiness Sadness
Anger 78.71 12.93
4.04 2.5 1.82
Disgust 10.00 81.67 5.00 0.00 3.33
Fear 21.21 2.00 63.29 9.00 4.50
Happiness 6.67 9.52 12.86 64.10 6.86
Sadness 2.00 2.50 4.00 0.00 91.50
T
ABLE 8
Confusion matrix: BAUM-1a database, 8 emotions/mental states, audio modality. Figures represent percentages and the average
recognition rate is 63.53%.
Anger
Boredom Disgust Fear Happiness Interest Sadness Unsure
Anger 82.75 4.72 8.48 0.00
0.00 0.00 4.04 0.00
Boredom 18.00 47.50 12.50 0.00 8.00 0.00 10.00 4.00
Disgust 10.00 2.50 76.67 2.50 5.83 0.00 2.50 0.00
Fear 17.69 4.00 11.19 56.62 2.00 0.00 4.50 4.00
Happiness 10.00 0.00 6.19 6.67 66.95 0.00 6.86 3.33
Interest 10.00 2.86 6.19 5.00 0.00 59.29 10.00 6.67
Sadness 2.00 0.00 11.17 2.00 0.00 3.33 76.78 4.72
Unsure 3.33 2.00 7.67 13.67 4.00 5.00 22.67 41.67
Mar
ch 16, 2016 DRAFT
1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 18
TABLE 9
Confusion matrix: BAUM-1a database, 8 emotions/mental states, visual modality. Figures represent percentages and the average
recognition rate is 31.24%.
Anger
Boredom Disgust Fear Happiness Interest Sadness Unsure
Anger 52.97 0.00 12.30
3.83 8.94 0.00 14.64 7.32
Boredom 21.48 0.00 2.63 16.34 26.35 4.22 13.70 15.28
Disgust 2.50 0.00 74.13 6.15 6.85 2.63 3.51 4.22
Fear 21.15 0.00 3.51 24.24 2.11 13.25 18.47 17.27
Happiness 23.11 0.00 10.04 0.00 42.06 7.03 7.73 10.04
Interest 11.66 0.00 24.59 34.63 6.52 6.02 0.00 16.56
Sadness 26.57 0.00 9.13 21.08 6.15 2.34 25.06 9.66
Unsure 28.33 0.00 7.73 18.27 7.73 4.22 13.70 20.03
T
ABLE 10
Confusion matrix: BAUM-1a database, 8 emotions/mental states, audio-visual . Fusion is done with weighted product rule (W 2= 3).
Figures represent percentages and the average recognition rate is 65.84%.
Anger
Boredom Disgust Fear Happiness Interest Sadness Unsure
Anger 84.29 4.72 4.44 0.00
2.50 0.00 4.04 0.00
Boredom 14.00 45.00 10.00 2.50 8.00 0.00 10.00 10.50
Disgust 7.50 2.50 76.67 2.50 8.33 0.00 2.50 0.00
Fear 14.86 0.00 8.19 61.95 4.50 0.00 4.50 6.00
Happiness 6.67 0.00 3.33 6.67 70.29 3.33 9.71 0.00
Interest 10.00 0.00 5.71 2.86 0.00 64.76 13.33 3.33
Sadness 0.00 0.00 6.94 4.22 0.00 3.33 78.56 6.94
Unsure 3.33 0.00 5.67 13.67 4.00 7.00 16.67 49.67
5.2
Results on BAUM-1s database 340
We carried experiments on the BAUM-1s database, which has a total of 1222 clips from 31 subjects. There are 341
13 emotional and mental states, which are Anger (An), Disgust (Di), Fear (Fe), Happiness (Ha), Sadness(Sa), 342
Surprise (Su), Boredom (Bo), Contempt (Co), Unsure (Un), Neutral (Ne), Thinking (Th), Concentrating (Con), 343
Bothered (Bot). 344
Below, we present the results of two different sets of experiments using LPQ features and IntraFace tracker 345
[42]. In the Ô¨Årst set, we used the six basic emotions (544 clips), and in the second set, we used all the 13 classes 346
listed in the previous paragraph. The single and multi-modal affective and mental state recognition accuracies 347
are given in Table 11. We can see that combining the audio and video based results at the decision level increases 348
the recognition rate by 6%for the six emotion case and by 0:5%for the 13 emotion case. 349
March 16, 2016 DRAFT
1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 19
TABLE 11
Single and multi-modal affective and mental state recognition accuracies on BAUM-1s database using LPQ features and IntraFace
tracker [42].
6
Basic Emotions 13 Emotions/
Mental States
V
ideo-Based 45.04% 25.17%
Audio-Based 29.41% 15.29%
Audio-Visual (sum rule) 45.78% 24.91%
Audio-Visual (product rule) 48.57% 25.19%
Audio-Visual (weighted product rule) 51.29% 25.76%
T
ABLE 12
Single and multi-modal emotion recognition accuracies on eNTERFACE database using LOSO cross-validation (based on LPQ
features and IntraFace tracker [42].)
LOSO
CV
V
ideo-based Results 42.16%
Audio-based Results 72.95%
Audio-Visual Results (Sum Rule) 74.95%
Audio-Visual Results (Product Rule) 76.48%
Audio-Visual Results (Weighted Product Rule, W2= 3) 77.02%
5.3
Results on eNTERFACE database 350
We also carried out audio-visual emotion recognition experiments on the eNTERFACE‚Äô05 dataset [15], which 351
contains clips of 44 subjects from 14 different nationalities. The subjects are asked to act the six basic emotions 352
while uttering selected sentences in English with target emotions. 353
In the following, we report the emotion recognition results not only for each modality but also after decision 354
level fusion. We use the LPQ features together with the IntraFace tracker [42] to extract the facial features. Since 355
the samples are distributed almost uniformly over the emotions for each subject, we used a leave-one-subject- 356
out cross validation method (LOSO). In Table 12, we present the experimental results of subject independent 357
audio-visual emotion recognition rates. It can be observed that for visual and audio modalities, emotion 358
recognition accuracies are 42.16% and 72.95%, respectively. We obtained an accuracy of 77.02% after decision 359
level fusion. The reported results clearly indicate that audio based classiÔ¨Åcation is better as compared to the 360
visual-features based classiÔ¨Åcation. The confusion matrices for the audio, visual and audio-visual experiments 361
are given in Table 13, Table 14 and Table 15, respectively. It is also worth noting that multi-modal fusion is 362
beneÔ¨Åcial for enhancing the recognition accuracy of all emotions except anger. Anger and happiness have 363
highest emotion recognition accuracies, which are 83.62% and 86.13%, respectively, after fusion. Happiness and 364
disgust are the emotions that experience the highest increases in their recognition rates after audio-visual fusion, 365
since they show an increase of 11% and 4% in their accuracy, respectively. 366
March 16, 2016 DRAFT
1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 20
TABLE 13
Confusion matrix: eNTERFACE database, audio modality (LOSO). Figures represent percentages and the average recognition rate
is 72.95%
Estimated
Emotion
Anger
Disgust Fear Happiness Sadness Surprise
Anger 88.4 1.4 3.3
2.3 2.3 2.3
Disgust 5.6 71.2 7.0 4.2 6.0 6.0
Fear 7.4 9.8 64.2 4.7 7.4 6.5
Happiness 7.0 4.7 1.9 75.3 7.0 4.2
Sadness 3.3 6.5 5.1 6.0 72.6 6.5
Surprise 4.2 5.1 7.4 6.5 10.7 66.0
T
ABLE 14
Confusion matrix: eNTERFACE database, video modality (LOSO). Figures represent percentages and the average recognition rate
is 42.16%.
Estimated
Emotion
Anger
Disgust Fear Happiness Sadness Surprise
Anger 23.43 20.13
8.00 14.41 11.56 22.48
Disgust 4.89 59.49 5.78 21.46 5.27 3.11
Fear 7.94 12.32 23.05 14.03 25.40 17.27
Happiness 4.95 14.13 5.52 59.94 4.38 11.08
Sadness 6.79 10.92 13.71 7.17 44.89 16.51
Surprise 8.25 4.13 10.16 21.33 13.71 42.41
T
ABLE 15
Audio-visual confusion matrix for the eNTERFACE database (LOSO). Figures represent percentages and the average recognition
rate is 77.02%.
Estimated
Emotion
Anger
Disgust Fear Happiness Sadness Surprise
Anger 83.62 3.56 4.89
3.37 1.02 3.56
Disgust 3.24 74.92 9.90 6.22 3.68 2.03
Fear 7.81 6.60 65.65 1.46 7.49 10.98
Happiness 1.33 5.94 0.44 86.13 3.33 2.83
Sadness 2.22 4.51 4.89 1.46 79.56 7.37
Surprise 3.11 0.89 9.27 7.68 6.67 72.38
Mar
ch 16, 2016 DRAFT
1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 21
TABLE 16
Single and multi-modal emotion recognition accuracies on eNTERFACE, BAUM-1a and BAUM-1s databases using three different
face trackers (Zhu [40], CHEHRA [41] and IntraFace [42]) and two different facial features (LPQ [50], POEM [51]. Figures are given in
percentages.
eNTERF
ACE BAUM-1a
BAUM-1a BAUM-1s
BAUM-1s
(6 emotions) (5
emotions) (8 classes) (6
emotions) (13 classes)
Audio-only 72.95 71.71 63.53 29.41 15.29
V
ideo-only (Zhu+LPQ) 38.22 46.60 26.30 43.75 24.07
V
ideo-only (Zhu+POEM) 32.95 48.69 29.64 46.32 23.65
V
ideo-only (CHEHRA+LPQ) 40.08 43.65 30.16 43.38 25.08
V
ideo-only (CHEHRA+POEM) 33.26 45.13 31.53 44.30 25.00
V
ideo-only (IntraFace+LPQ) 42.16 47.44 31.24 45.04 25.17
V
ideo-only (IntraFace+POEM) 36.12 46.24 32.39 47.06 23.56
Audio-V
isual (Zhu+LPQ) 76.79 74.42 65.06 50.00 26.18
Audio-V
isual (Zhu+POEM) 75.45 76.38 63.45 50.37 25.51
Audio-V
isual (CHEHRA+LPQ) 77.40 73.94 63.53 50.37 25.42
Audio-V
isual (CHEHRA+POEM) 74.55 75.24 63.83 49.26 25.25
Audio-V
isual (IntraFace+LPQ) 77.02 75.32 65.84 51.29 25.76
Audio-V
isual (IntraFace+POEM) 75.78 76.54 64.13 50.18 25.68
5.4
Summary of the Experimental Results 367
In order to compare the emotion recognition accuracies on the acted eNTERFACE and BAUM-1 databases, 368
we summarize the uni-modal and multi-modal accuracies in Table 16 using three different face trackers (Zhu, 369
CHEHRA and IntraFace) and two different facial features (LPQ and POEM). We can see that eNTERFACE 370
and BAUM-1a databases, which are both acted in different languages show similar characteristics, in the sense 371
that the emotion recognition accuracy from audio is much higher as compared to the emotion recognition 372
accuracy from video. However, the audio-based accuracies are much lower than the video-based accuracies for 373
the BAUM-1s database both for the 6 class and 13 class cases. BAUM-1s accuracies for the 13 class case are much 374
lower as compared to the 6 class case, which implies that recognition of mental states is quite challenging. If we 375
compare the video-based results, we can observe that IntraFace tracker gives slightly higher accuracies in most 376
of the cases except for BAUM-1a (5 emotions). If we compare the audio-visual accuracies in Table 16, we can 377
conclude that the results using three different face trackers and facial features are comparable on all databases 378
with 1-2% differences in accuracies. 379
6 C ONCLUSION 380
We presented a new spontaneous audio-visual Turkish database, BAUM-1 containing expressions of affective 381
as well as mental states. The challenging nature of the database was demonstrated via baseline audio-visual 382
emotion recognition experiments based on a peak frame selection approach. Although current algorithms can 383
March 16, 2016 DRAFT
1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 22
achieve reasonable recognition rates on acted expressions, which are somewhat exaggerated, the accuracies 384
drop dramatically for naturalistic and subtle expressions. 385
The BAUM-1 database can be useful a resource to the affective computing community as it contains multi- 386
modal, close-to-natural, affective and mental state expressions as opposed to the mostly single-modality and 387
posed databases available in the literature. The database can be combined with other databases in other 388
languages to investigate multi-language cross-corpora aspects of emotion recognition from speech. Investigating 389
the correlation between speech-related and face-related features and emotion-independent face recognition 390
could be other directions for future research. 391
ACKNOWLEDGMENTS 392
The authors would like to thank Prof. Metehan Irak from the Department of Physchology, Bahcesehir University, 393
for the useful discussions during the design of the stimuli video. 394
REFERENCES 395
[1] N. Sebe, I. Cohen, and T. S. Huang, Internet Imaging VI,
ser. ISBN / ISSN: 0-8194-5643-8, 2005, vol. 5670, no. 5670, ch. 396
Multimodal Approaches for Emotion Recognition: A Survey, pp. 56 ‚Äì 67. 397
[2] Z. H. Zeng, M. Pantic, G. I. Roisman, and T. S. Huang, ‚ÄúA survey of affect recognition methods: Audio, visual, and spontaneous 398
expressions,‚Äù IEEE T
ransactions onPattern Analysis andMachine Intelligence,
vol. 31, no. 1, pp. 39‚Äì58, 2009. 399
[3] A. Ryan, J. Cohn, S. Lucey, J. Saragih, P. Lucey, F. D. la Torre, and A. Rossi, ‚ÄúAutomated facial expression recognition system,‚Äù 400
inPr
oceedings oftheInternational Carnahan Confer
ence onSecurity T
echnology, 2009, pp. 172‚Äì177. 401
[4] G. C. Littlewort, M. S. Bartlett, and K. Lee, ‚ÄúAutomatic coding of facial expresssions displayed during posed and genuine pain,‚Äù 402
Image andV
ision Computing,
vol. 27, no. 12, pp. 1797‚Äì1803, 2009. 403
[5] A. B. Ashraf, S. Lucey, J. F. Cohn, T. Chen, Z. Ambadar, K. M. Prkachin, and P. E. Solomon, ‚ÄúThe painful face - pain expression 404
recognition using active appearance models,‚Äù Image andV
ision Computing,
vol. 27, no. 12, pp. 1788‚Äì1796, 2009. 405
[6] P. Ekman and W. V. Friesen, Pictur
esofFacial Ef
fect. Palo Alto, CA: Consulting Psychologists Press, 1976. 406
[7] J. Bassili, ‚ÄúEmotion recognition: the role of facial movement and the relative importance of upper and lower areas of the face,‚Äù 407
J.Pers. Soc.Psychol,
vol. 37, pp. 2049‚Äì2058, 1979. 408
[8] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews, ‚ÄúThe extended cohn-kanade dataset (CK+): A 409
complete dataset for action unit and emotion-speciÔ¨Åed expression,‚Äù in Pr
oceedings ofIEEE workshop onCVPR forHuman 410
Communicative Behavior Analysis,
San Francisco, USA, 2010. 411
[9] T. Banziger and K. R. Scherer, Blueprint forAf
fective Computing: ASour
cebook. Oxford University Press, 2010, ch. Introducing 412
the Geneva multimodal emotionportrayal (GEMEP) corpus, pp. 271‚Äì294. 413
[10] A. Savran, H. D. N. Alyuz, O. Celiktutan, B. Gkberk, B. Sankur, and L. Akarun, ‚ÄúBosphorus database for 3D face analysis,‚Äù in 414
First COST 2101 W
orkshop onBiometrics andIdentity Management (BIOID 2008),
2008. 415
[11] E. Sariyanidi, H. Gunes, and A. Cavallaro, ‚ÄúAutomatic analysis of facial affect: A survey of registration, representation and 416
recognition,‚Äù IEEE T
ransactions onPattern Analysis andMachine Intelligence,
2014. 417
[12] M. J. Lyons, S. Akamatsu, M. Kamachi, and J. Gyoba, ‚ÄúCoding facial expressions with gabor wavelets,‚Äù in Pr
oc.Thir
dIEEE 418
International Confer
ence onAutomatic Face andGestur
eRecognition,
Japan, 1998, pp. 200‚Äì205. 419
[13] M. Pantic, M. F. Valstar, R. Rademaker, and L. Maat, ‚ÄúWeb-based database for facial expression analysis,‚Äù in Pr
oc.IEEE Int. 420
Conf. onMultimedia andExpo (ICME‚Äô05),
http://www.mmifacedb.com/, Amsterdam, The Netherlands, 2005. 421
[14] M. F. Valstar, B. Jiang, M. Mehu, M. Pantic, and K. R. Scherer, ‚ÄúThe Ô¨Årst facial expression recognition and anaylsis challenge,‚Äù 422
inIEEE Int.Conf. Face andGestur
eRecognition (FG‚Äô2011),
2011. 423
[15] O. Martin, I. Kotsia, B. Macq, and I. Pitas, ‚ÄúThe eNTERFACE05 audio-visual emotion database,‚Äù in Pr
oceedings oftheFirst 424
IEEE W
orkshop onMultimedia Database Management,
2006. 425
March 16, 2016 DRAFT
1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 23
[16] M. Grimm, K. Kroschel, and S. Narayanan, ‚ÄúThe vera am mittag german audio-visual emotional speech database,‚Äù in Proc.Int. 426
Conf. Multimedia andExpo (ICME),
2008. 427
[17] G. Mckeown, M. F. Valstar, R. Cowie, M. Pantic, and M. Schroeder, ‚ÄúThe SEMAINE database: Annotated multimodal records 428
of emotionally coloured conversations between a person and a limited agent,‚Äù IEEE T
rans. Af
fective Computing,
vol. 3, no. 1, 429
pp. 5‚Äì17, 2012. 430
[18] F. Wallhoff, ‚ÄúFacial expressions and emotion database [online],‚Äù Available: http://www.mmk.ei.tum.de/ waf/fgnet/feedtum.html, 431
2006. 432
[19] S. M. Mavadati, M. H. Mahoor, K. Bartlett, P. Trinh, and J. F. Cohn, ‚ÄúDisfa: A spontaneous facial action intensity database,‚Äù 433
IEEE T
ransactions onAf
fective Computing,
vol. 4, no. 2, pp. 151‚Äì160, 2013. 434
[20] C. Busso, M. Bulut, C. C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, and S. S. .Narayanan, ‚ÄúIEMOCAP: 435
Interactive emotional dyadic motion capture database,‚Äù Journal ofLanguage Resour
cesand Evaluation,
vol. 42, no. 4, pp. 436
335‚Äì359, 2008. 437
[21] E. Douglas-Cowie, R. Cowie, and M. Schoder, ‚ÄúA new emotion database: Considerations, sources and scope,‚Äù in Pr
oc.ISCA 438
ITR
WonSpeech andEmotion,
2000, pp. 39‚Äì44. 439
[22] A. Dhall, R. Goecke, S. Lucey, and T. Gedeon, ‚ÄúCollecting large, richly annotated facial-expression databases from movies,‚Äù 440
IEEE Multimedia,
vol. 19, no. 3, pp. 34‚Äì41, 2012. 441
[23] C. Turan, C. Kansin, S. Zhalehpour, Z. Aydin, and C. E. Erdem, ‚ÄúA method for extraction of audio-visual facial clips from 442
movies,‚Äù in IEEE Signal Pr
ocessing andApplications Confer
ence (SIU),
Girne, Northern Cyprus, April 2013. 443
[24] C. E. Erdem, C. Turan, and Z. Aydin, ‚ÄúBAUM-2: A multilingual audio-visual affective face database,‚Äù Multimedia T
ools and 444
Applications,
2014. 445
[25] D. McDuff, R. E. Kaliouby, and R. W. Picard, ‚ÄúCrowdsourcing facial responses to online videos,‚Äù IEEE T
ransactions onAf
fective 446
Computing,
vol. 3, no. 4, pp. 456‚Äì468, 2012. 447
[26] J.-Y. Zhu, A. Agarwala, A. A. Efros, E. Shechtman, and J. Wang, ‚ÄúMirror mirror: Crowdsourcing better portraits,‚Äù ACM 448
T
ransactions onGraphics (SIGGRAPH Asia 2014),
vol. 33, no. 6, 2014. 449
[27] G. Fanelli, J. Gall, H. Romsdorfer, T. Weise, and L. V. Gool, ‚ÄúA 3-D audio-visual corpus of affective communication,‚Äù IEEE T
rans. 450
onMultimedia,
vol. 12, no. 6, pp. 591‚Äì598, 2010. 451
[28] X. Zhang, L. Yin, J. Cohn, S. Canavan, M. Reale, A. Horowitz, and P. Liu, ‚ÄúA highresolution spontaneous 3d dynamic facial 452
expression database,‚Äù in International confer
ence onautomatic face andgestur
er
ecognition (FG‚Äô13),
Shanghai, China, April 453
2013. 454
[29] J. J. Gross and R. W. Levenson, ‚ÄúEmotion elicitation using Ô¨Ålms,‚Äù Cognition andEmotion,
vol. 9, no. 1, pp. 87‚Äì108, 1995. 455
[30] R. Westermann, K. Spies, G. Stahl, and F. W. Hesse, ‚ÄúRelative effectiveness and validity of mood induction procedures: a 456
meta-analysis,‚Äù Eur
opean Journal ofSocial Psychology,
vol. 26, no. 4, pp. 557 ‚Äì 580, 1996. 457
[31] X. Zhang, L. Yin, J. F. Cohn, S. J. Canavan, M. Reale, A. Horowitz, and P. Liu, ‚ÄúA high-resolution spontaneous 3d dynamic facial 458
expression database,‚Äù in IEEE Int.Conf. onAutomatic Face andGestur
eRecognition,
2013. 459
[32] P. J. Lang, M. M. Bradley, and B. N. Cuthbert, ‚ÄúInternational affectivepicture system (iaps): Technical manual and affective 460
ratings,‚Äù NIMH Center for the Study of Emotion and Attention, Tech. Rep., 1997. 461
[33] M. C. Escher, ‚Äúhttp://www.mcescher.com/.‚Äù 462
[34] O. Onder, S. Zhalehpour, and C. E. Erdem, ‚ÄúBahcesehir university multimodal face database of spontaneous affective ans 463
mental states (BAUM-1),‚Äù http://baum1.bahcesehir.edu.tr/, 2014. 464
[35] R. Cowie, C. Cox, J.-C. Martin, A. Batliner, D. Heylen, and K. Karpouzis, Emotion-Oriented Systems: TheHumaine Handbook. 465
Berlin,
Heidelberg: Springer-Verlag, 2011, ch. Issues in Data Labelling, pp. 215‚Äì244. 466
[36] R. A. Kaliouby, ‚ÄúMind-reading machines: automated inference of complex mental states,‚Äù Cambridge, ISSN 1476-2986 UCAM- 467
CL-TR-636, 2005. 468
[37] ‚ÄúMind reading software,‚Äù essica Kingsley Publishers, 2003. 469
[38] M. W. Watkins and M. Pacheco, ‚ÄúInterobserver agreement in behavioral research,‚Äù Journal ofBehavioral Education,
vol. 10, 470
no. 4, pp. 205‚Äì212, 2000. 471
[39] A. J. Viera and J. M. Garrett, ‚ÄúUnderstanding interobserver agreement: The kappa statistic,‚Äù Family Medicine,
vol. 37, no. 5, 472
2005. 473
March 16, 2016 DRAFT
1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 24
[40] X. Zhu and D. Ramanan, ‚ÄúFace detection, pose estimation and landmark localization in the wild,‚Äù in Computer Vision and 474
Pattern Recognition (CVPR),
Rhode Island, 2012. 475
[41] A. Asthana, S. Zafeiriou, S. Cheng, and M. Pantic, ‚ÄúIncremental face alignment in the wild,‚Äù in InPr
oc.of2013 IEEE Confer
ence 476
onComputer V
ision andPattern Recognition (CVPR),
2014. 477
[42] X. Xuehan and F. D. la Torre, ‚ÄúSupervised descent method and its applications to face alignment,‚Äù in InPr
oc.IEEE Confer
ence 478
onComputer V
ision andPattern Recognition (CVPR 2013),
Portland, Oregon, USA, June 2013, pp. 532 ‚Äì 539. 479
[43] ‚Äî‚Äî, ‚ÄúGlobal supervised descent method,‚Äù in InPr
oc.IEEE Confer
ence onComputer V
ision andPattern Recognition (CVPR 480
2015),
Boston, MA, USA, June 2015, pp. 2664 ‚Äì 2673. 481
[44] S. Zhalehpour, Z. Akhtar, and C. E. Erdem, ‚ÄúMultimodal emotion recognition with automatic peak frame selection,‚Äù in IEEE 482
International Symposioum onInnovations inIntelligent Systems andApplications (INIST
A), 2014, pp. 116‚Äì121. 483
[45] ‚Äî‚Äî, ‚ÄúMultimodal emotion recognition based on peak frame selection from video,‚Äù Signal, Image andV
ideo Pr
ocessing, 2015, 484
DOI: 10.1007/s11760-015-0822-0. 485
[46] S. Ulukaya and C. E. Erdem, ‚ÄúGaussian mixture model based estimation of the neutral face shape for emotion recognition,‚Äù 486
vol. 32, pp. 11‚Äì23, 2014. 487
[47] P. Viola and M. J. Jones, ‚ÄúRobust real-time face detection,‚Äù International Journal ofComputer V
ision, vol. 57, no. 2, pp. 137‚Äì154, 488
2004. 489
[48] C. E. Erdem, S. Ulukaya, A. Karaali, and A. T. Erdem, ‚ÄúCombining haar feature and skin color based classiÔ¨Åers for face 490
detection,‚Äù in IEEE 36th International Confer
ence onAcoustics, Speech andSignal Pr
ocessing (ICASSP 2011),
Prague, 2011. 491
[49] J. M. Saragih, S. Lucey, and J. F. Cohn, ‚ÄúDeformable model Ô¨Åtting by regularized landmark mean-shift,‚Äù International Journal 492
ofComputer V
ision (IJCV),
vol. 91, pp. 200‚Äì215, 2011. 493
[50] V. Ojansivu and J. Heikkil, ‚ÄúBlur insensitive texture classiÔ¨Åcation using local phase quantization,‚Äù Lectur
eNotes inComputer 494
Science,
vol. 5099, pp. 236‚Äì243, 2008. 495
[51] N. Vu and A. Caplier, ‚ÄúFace recognition with patterns of oriented edge magnitudes,‚Äù in Eur
opean Confer
ence onComputer 496
V
ision (ECCV),
ser. LNCS, vol. 6311, 2010, pp. 313‚Äì326. 497
[52] A. Dhall, R. Goecke, and T. Gedeon, ‚ÄúEmotion recognition using PHOG and LPQ features,‚Äù in InPr
oc.oftheW
orkshop on 498
Facial Expr
ession Recognition andAnalysis Challenge FERA2011, IEEE Automatic Face andGestur
eRecognition Confer
ence 499
FG2011,
Santa Barbara (CA), USA, 2011. 500
[53] T. Ojala, M. Pietikainen, and T. Maenpaa, ‚ÄúMultiresolution gray-scale and rotation invariant texture classiÔ¨Åcation with local 501
binary patterns,‚Äù IEEE T
rans. Pattern Anlaysis andMachine Intelligence,
vol. 24, no. 7, pp. 971‚Äì987, 2002. 502
[54] C. Shan, S. Gong, and P. W. McOwan, ‚ÄúFacial expression recognition based on local binary patterns: A comprehensive study,‚Äù 503
Image andV
ision Computing,
vol. 27, pp. 803‚Äì816, 2009. 504
[55] G. Zhao and M. Pietikainen, ‚ÄúDynamic texture recognition using local binary patterns with an application to facial expressions,‚Äù 505
IEEE T
rans. Pattern Anlaysis andMachine Intelligence,
vol. 29, no. 6, pp. 915‚Äì928, 2007. 506
[56] ‚ÄúMachine vision group, matlab codes for local phase quantization,,‚Äù http://www.cse.oulu.Ô¨Å/CMV/Downloads/LPQMatlab, 507
last Accessed: 01/07/2013. 508
[57] X. Huang, A. Acero, and H. Hon, Spoken Language Pr
ocessing: Aguide totheory
,algorithm, and system development. 509
Pr
entice Hall, 2001. 510
[58] H. Hermansky and N. Morgan, ‚ÄúRasta processing of speech,‚Äù IEEE T
ransactions onSpeech andAudio Pr
ocessing, vol. 2, no. 4, 511
pp. 578‚Äì589, 1994. 512
[59] C.-C. Chang and C.-J. Lin, ‚ÄúLIBSVM: A library for support vector machines,‚Äù ACM T
ransactions onIntelligent Systems and 513
T
echnology, vol. 2, pp. 27:1‚Äì27:27, 2011. 514
[60] P. K. Atrey, M. A. Hossain, A. E. Saddik, and M. S. Kankanhalli, ‚ÄúMultimodal fusion for multimedia analysis: a survey,‚Äù 515
Multimedia Systems,
vol. 16, pp. 345‚Äì379, 2010. 516
[61] J. Kittler, M. H. R. P. Duin, and J. Matas, ‚ÄúOn combining classiÔ¨Åers,‚Äù IEEE T
rans. Pattern Analysis andMachine Intelligence, 517
vol.
20, no. 3, p. 226239, 1998. 518
[62] Y. Wang, L. Guan, and A. N. Venetsanopoulos, ‚ÄúKernel cross-modal factor analysis for information fusion with application to 519
bimodal emotion recognition,‚Äù IEEE T
ransactions onMultimedia,
vol. 14, no. 3, pp. 597‚Äì607, 2012. 520
March 16, 2016 DRAFT
1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE
Transactions on Affective Computing
JOURNAL
OF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 25
SaraZhalehpour received both the B.Sc. and M.Sc. degrees in Telecommunications Engineering from the 521
University of Tabriz, Iran in 2009 and 2012, respectively. She received her second M.Sc. degree in Electrical 522
and Electronics Engineering from Bahcesehir University, Turkey in 2014. She joined INRS-EMT, Montreal, 523
Canada as a PhD student in September 2014. Her main research areas of interest are audio-visual emotion 524
recognition and human-computer interaction. 525
526
On
ur Onder received the B.Sc. degree in Electrical and Electronics Engineering from Ege University, Izmir 527
in 2010. He received the M.Sc. degree in Electrical and Electronics Engineering from Bahcesehir Univer- 528
sity,Istanbul in 2014. He is currently a Ph.D. student at Dokuz Eylul University, Izmir. His research interests 529
include digital image and video processing, human-computer interaction, machine learning, and remote sens- 530
ing. 531
532
Zahid
Akhtar received the Ph.D. degree in Electronic and Computer Engineering from the University of 533
Cagliari, Italy in 2012. Currently, he is working as a research associate in the Department of Mathematics 534
and Computer Science at the University of Udine, Italy. His research interests include computer vision, pattern 535
recognition and image processing with applications to biometrics, affective computing and security systems. 536
537
Cigdem
Eroglu Erdem received the B.S. and M.S. degrees in Electrical and Electronics Engineering from 538
Bilkent University, Ankara, Turkey in 1995 and 1997, respectively. She received the Ph.D. degree in Electri- 539
cal and Electronics Engineering from Bogazici University, Istanbul, in 2002. From September 2000 to June 540
2001, she was a visiting researcher in the Department of Electrical and Computer Engineering, University of 541
Rochester, NY , USA. Between 2003-2004, she was a postdoctoral fellow at the Faculty of Electrical Engineer- 542
ing at Delft University of Technology, the Netherlands, where she was also afÔ¨Åliated with the video processing 543
group at Philips Research Laboratories, Eindhoven. She is currently a professor in the Department of Electrical 544
and Electronics Engineering at Bahcesehir University, Istanbul, Turkey. Her research interests are in the areas of digital image and 545
video processing, including affective computing, vision and scene understanding and human computer 546
March 16, 2016 DRAFT
"
https://ieeexplore.ieee.org/abstract/document/4539275/,"498 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY , VOL. 3, NO. 3, SEPTEMBER 2008
Bilinear Models for 3-D Face and
Facial Expression Recognition
Iordanis Mpiperis, Sotiris Malassiotis, and Michael G. Strintzis , Fellow, IEEE
Abstract‚Äî In this paper, we explore bilinear models for jointly
addressing 3-D face and facial expression recognition. An elasti-cally deformable model algorithm that establishes correspondence
among a set of faces is proposed Ô¨Årst and then bilinear models that
decouple the identity and facial expression factors are constructed.Fitting these models to unknown faces enables us to perform facerecognition invariant to facial expressions and facial expressionrecognition with unknown identity. A quantitative evaluation ofthe proposed technique is conducted on the publicly availableBU-3DFE face database in comparison with our previous workon face recognition and other state-of-the-art algorithms forfacial expression recognition. Experimental results demonstratean overall 90.5% facial expression recognition rate and an 86%rank-1 face recognition rate.
Index Terms‚Äî Bilinear model, elastically deformable model, 3-D
face recognition, 3-D facial expression recognition.
I. I NTRODUCTION
THE 3-D geometry of the face conveys valuable informa-
tion on the identity as was recently demonstrated by 3-D
face-recognition research [3], [4]. But in addition, deformationof the facial surface caused by facial expressions may also pro-vide cues that are useful for automatic facial expression classi-
Ô¨Åcation.
Traditionally, researchers have examined these problems
separately (e.g., using neutral images for face recognition andknown identity for facial expressions). In practice, however,
such decoupling does not exist. Thus, an ideal 3-D face analysis
system should be able to recognize simultaneously: 1) facesacross any possible expression and 2) facial expressions andtheir intensity independent of identity, gender, and ethnicity.
Also, in order to be applicable under real-life conditions, this
face analyzer should meet additional requirements, such as full
Manuscript received November 20, 2007; revised March 19, 2008. First pub-
lished June 6, 2008; last published August 13, 2008 (projected). This work
was supported in part by the European Commission under the FP6 IST Project:
‚ÄúPASION-Psychologically Augmented Social Interaction over Networks‚Äù (con-
tract No FP6-027654) and FP6 IST Network of Excellence: ‚Äú3DTV-IntegratedThree-Dimensional Television‚ÄîCapture, Transmission, and Display‚Äù (contract
FP6-511568) and in part by the Greek Ministry of Development-General Sec-
retariat of Research and Technology under Project: PENED 2003 Ontomedia
(03E
/1475). The associate editor coordinating the review of this manuscript and
approving it for publication was Prof. Vijaya Kumar Bhagavatula.
I. Mpiperis and M. G. Strintzis are with the Centre for Research and Tech-
nology Hellas/Informatics and Telematics Institute, Thessaloniki 57001, Greece
and also with the Information Processing Laboratory of Electrical and Computer
Engineering Department, Aristotle University of Thessaloniki, Thermi-Thessa-loniki 541 24, Greece (e-mail: iordanis@iti.gr; strintzi@iti.gr).
S. Malassiotis is with the Centre for Research and Technology Hellas/Infor-
matics and Telematics Institute, Thermi-Thessaloniki 57001, Greece (e-mail:
malasiot@iti.gr).
Digital Object IdentiÔ¨Åer 10.1109/TIFS.2008.924598automation in all stages of processing from 3-D data acquisi-
tion to face and facial expression classiÔ¨Åcation, near-real time
response and robustness to head pose, aging effects, and partialor self occlusion.
In this paper, we present a technique for joint 3-D identity-in-
variant facial expression recognition and expression-invariant
face recognition (Fig. 1). First, we present a novel model-basedframework for establishing correspondence among 3-D pointclouds of different faces. This allows us to subsequently create
bilinear models that decouple expression and identity factors.
A bootstrap set of faces is used to tune an asymmetric bilinearmodel which is incorporated in a probabilistic framework forthe recognition of the six prototypic expressions proposed by
Ekman and Friesen [5]. Face recognition, on the other hand, is
performed by ‚Äúmodulating‚Äù the probe surface to resemble theexpression depicted in the associated gallery image. This allowsexpression-invariant face recognition. Modulation is performed
using the expression and identity bilinear model. Finally, we
present results evaluating our algorithms using the BU‚Äì3DFE
1
face database [6], and demonstrates comparisons with our pre-vious work [2] on 3-D face recognition and Wang et al. ‚Äôs work
[1] on 3-D facial expression recognition.
A. Related Work
Over the past three decades, face and facial expression recog-
nition have received growing interest within the computer vi-
sion community. Most works in the literature assume neutral
expression for 3-D face recognition (e.g., [7]‚Äì[9]), and use 2-Dimagery for facial expression recognition. Extensive surveys on3-D face recognition and facial expression recognition from 2-D
images and video may be found in [3] and [10], respectively.
Here, we focus on research performed recently toward expres-sion-invariant 3-D face recognition and facial expression recog-nition from 3-D surfaces.
To cope with facial expressions, the majority of techniques in
the literature detect regions of the face that are affected by facialexpressions and then try to minimize their contribution to sim-ilarity computation. Other techniques use expression-invariant
features or representations.
In [11] and [12], the authors use an annotated face model that
is deformed elastically to Ô¨Åt each face thus allowing the anno-tation of its different anatomical areas such as the nose, eyes,
mouth, etc. To account for expressions, the authors then clas-
sify faces using wavelet coefÔ¨Åcients representing face areas thatremain unaffected by facial expressions, such as eyes and nose.However, the best recognition rate is achieved when the whole
1The database is available at the http://www.cs.binghamton.edu/~lijun/Re-
search/3DFE/3DFE_Analysis.html
1556-6013/$25.00 ¬© 2008 IEEE
MPIPERIS et al. : BILINEAR MODELS FOR 3-D FACE AND FACIAL EXPRESSION RECOGNITION 499
face is used, which implies that rejection of deformable parts
of the face leads to the loss of valuable discriminative infor-mation. Similarly, Chang et al. [13] follow a multiregion tech-
nique in which multiple overlapping regions around the nose
are matched using the iterative closest point algorithm (ICP).This approach suffers too from the disadvantage that deformableparts of the face that still encompass discriminative information
are rejected during matching.
On the other hand, Li and Zhang [14] classify faces using ex-
pression-invariant descriptors that are based on surface curva-ture, geodesic distance, and attributes of a 3-D mesh Ô¨Åtted to the
face. The descriptors are weighted appropriately to form a Ô¨Ånal
face signature that is used to identify faces. Bronstein et al. [15]
use multidimensional scaling on pair-wise geodesic distances to
embed the surface to a 4-D sphere where classiÔ¨Åcation is per-
formed on the basis of normalized moments. In the same spirit,
we used a geodesic polar parameterization of the facial surfaceto construct expression-invariant attribute images [2] which canbe classiÔ¨Åed following a variety of 2-D classiÔ¨Åcation techniques.
The approaches [2], [14], [15] depend on the assumption that
facial skin deforms isometrically, which is not valid in case ofextreme expressions. In addition, the computation of expres-sion-invariant features using curvature and geodesic distance is
problematic because of their sensitivity to noise, which is also
magniÔ¨Åed by the approximate character of the isometric mod-elling of facial deformations.
Although great effort has been directed towards 3-D face
recognition, not many works have examined 3-D facial ex-
pression recognition. Instead, most studies on automatic facialexpression recognition are based on still images and imagesequences [16]‚Äì[18]. The only work using purely 3-D in-
formation we are aware of is that of Wang et al. [1]. In that
work, expression classiÔ¨Åcation is based on the distribution ofgeometric descriptors deÔ¨Åned on different regions of the facialsurface which are delimited according to the neuroanatomic
knowledge of the conÔ¨Åguration of facial muscles and their
dynamics. In each region, surface points are labeled accordingto their principal curvatures and then histograms of the labelsare used to classify facial expressions. However, this technique
is also limited by the need of computation of curvature features
which may be problematic as we have already described.
All of the aforementioned techniques treat 3-D face recogni-
tion and expression recognition as two separate problems. To the
best of our knowledge, there are no reported studies about joint
expression-invariant facial identity recognition and identity-in-variant facial expression recognition based on 3-D information.A few researchers have addressed the problem using 2-D images
by trying to encode identity and expression variability of facial
appearance in independent control parameters that are then usedfor recognition. Vasilescu and Terzopoulos [19] use the N-modeSVD tensor decomposition on face images to separate the in-
Ô¨Çuence of identity, pose, illumination, and expression, while
Wang and Ahuja [20] use higher-order singular value decompo-sition (SVD) to recognize and synthesize facial expressions. Bi-linear models proposed by Tenenbaum and Freeman [21], [22]
offer an efÔ¨Åcient way for modeling bifactor interactions, since
they combine simplicity in training and implementation with thecapability of capturing subtle interactions between factors. Assuch, bilinear models are used in this work to model the 3-D fa-
cial surface as the interaction of expression and identity compo-nent. After separating the parameters which control expression
and those which control identity, joint expression-invariant face
recognition and identity-invariant expression recognition are ef-Ô¨Åciently achieved.
Apart from presenting a novel uniÔ¨Åed framework for 3-D face
and facial expression recognition, this work introduces several
contributions.
‚Ä¢ We propose using bilinear models for handling joint iden-
tity and expression contribution to facial appearance and
we provide a generic solution for the minimization in-
volved in bilinear model training. The proposed techniqueis applicable even with incomplete data sets in contrastto the conventional SVD approach which is intended for
evenly distributed training data.
‚Ä¢ We present a novel technique for establishing point corre-
spondence among faces which is based on an elasticallydeformable 3-D model and is achieved by solving a simple
linear system of equations. Unlike other relevant tech-
niques, we use surface-to-model and model-to-surfacedistances during model deformation which leads to moreplausible point correspondence.
‚Ä¢ Correspondence is established automatically by building
a low-dimensional face eigenspace, but at the expenseof possible poor correspondence in the mouth region incase of expressions with a widely open mouth. To handle
this problem caused by the rough approximation of the
face manifold, we detect landmarks that deÔ¨Åne the mouthboundary instead of using complex nonlinear models asusual.
The rest of this paper is organized as follows: In Section II,
we present how elastically deformable models can be employedto resolve the problem of point correspondence and how theuse of landmarks can improve model Ô¨Åtting. Bilinear models‚Äô
training and Ô¨Åtting are described in Section III while their ap-
plications on facial expression recognition and face recognitionare demonstrated, respectively, in Sections IV and V. The pro-posed algorithm is evaluated in Section VI where experimental
results are reported and conclusions are drawn.
II. E
STABLISHING POINT CORRESPONDENCE
A. Elastically Deformable Model
Our goal is to establish a point-to-point correspondence
among 3-D facial surfaces guaranteeing alignment amonganatomical facial features. Since faces are represented asclouds of 3-D points acquired by a 3-D sensor, it is more conve-
nient that this correspondence be established instead between
surface models Ô¨Åtted to the point clouds. We achieve this by de-forming a prototypic facial surface model (neutral expression,average identity) so that it resembles the expression/identity
depicted by the point clouds. Once this deformation has been
estimated, Ô¨Ånding correspondences is straightforward.
In this work, the face is modelled as a subdivision surface,
similar to [23]. A triangular 3-D mesh
with
 vertices
is used, where
 are the coordi-
nates of each vertex. At each subdivision step, each edge of the
500 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY , VOL. 3, NO. 3, SEPTEMBER 2008
Fig. 1. Flowchart of the proposed algorithm for joint 3-D face and facial expression recognition.
mesh is split with the introduction of new vertices using linear
subdivision rules.2Thus, each triangle is subdivided into four
subtriangles and the mesh becomes more dense. Each vertex of
the resulting 3-D mesh may be written as a linear combinationof the vertices of the previous level mesh and eventually of theinitial mesh
. That is
(1)
where
 is the set of vertex coordinates at level
 of subdivi-
sion and
 is a matrix which may be easily computed given
the subdivision rule. After an inÔ¨Ånite number of subdivisions,
the mesh converges to a continuous smooth surface which is a
function of the initial mesh and the subdivision rule. In practice,however, we do not have to make inÔ¨Ånite subdivisions, sinceafter a few levels (e.g., three in our experiments), the mesh be-
comes dense enough to approximate the subdivision surface. For
notation simplicity, let the mesh
at the last level of subdivi-
sion that best approximates the subdivision surface be called the
subdivision mesh and let
 denote its vertices.
If
 also denotes the corresponding matrix with entries
 , then
altogether we have
(2a)
(2b)
As is evident from the above, the geometry of the subdivision
surface may be uniquely determined by deÔ¨Åning the base mesh
vertices and topology (e.g., vertex connectivity).
Now let us return to the problem of Ô¨Åtting the subdivision
surface to a 3-D cloud of points. To make the Ô¨Åt anatomicallyvalid, a set of landmarks corresponding to anatomically salient
points of the face has to be deÔ¨Åned on the surface and the cloud
2We have used the Loop subdivision scheme [23].of points. Let
 and
 denote the points of the cloud
and
 ,
 the associated landmarks (e.g., point
corresponds to the left eye leftmost point,
 corresponds to
the left eye rightmost point, and so on). Similarly, we select asubset
of vertices on the base mesh
 that anatomically
correspond to the
 landmarks. According to the subdivision
rule, the coordinates of vertices in
 remain untouched on
the subdivision mesh but are indexed differently. Thus, we caneasily deÔ¨Åne a table
that maps each landmark index
 to the
corresponding vertex index of
 .
Fitting the subdivision surface to the cloud of 3-D points is
formulated as an energy minimization problem. We deÔ¨Åne adeformation energy which consists of terms giving rise to op-posing forces between the subdivision surface and the cloud of
points. The interaction of these forces deforms the subdivision
surface or equivalently displaces the vertices of the base meshwhich control the surface, until equilibrium is established. Theterms comprising the deformation energy are deÔ¨Åned so that cer-
tain intuitively reasonable criteria are met.
The deformation should obey the a priori known correspon-
dences between landmarks and associated mesh vertices. There-fore, the Ô¨Årst term of the deformation energy minimizes the dis-
tance of each landmark from its corresponding vertex in
.
That is
(3)
The contribution of this term is important in the Ô¨Årst stages
of optimization because it drives the minimization close tothe global minimum, preventing it from being trapped in localminima of the objective function.
The Ô¨Ånal form of the subdivision surface should also be as
close as possible to the original surface represented by the cloudof points. Therefore, the distance between the surface and thecloud should be minimized. We formulate two terms for this
MPIPERIS et al. : BILINEAR MODELS FOR 3-D FACE AND FACIAL EXPRESSION RECOGNITION 501
minimization‚Äîone for the distance directed from the subdivi-
sion surface to the point cloud, that is between each vertex of thesubdivision-mesh and the nearest point of the cloud, and one for
the distance with reverse direction. The use of two terms leads to
two forceÔ¨Åelds between the surface and the point cloud, whichare approximately similar in Ô¨Çat regions of the face but quitedifferent in regions of high curvature (see Fig. 3). The resulting
forceÔ¨Åeld is smoother than both of them separately and estab-
lishes anatomically more plausible correspondence.
If we deÔ¨Åne function
, which returns the index
 of the
point
 closest to the
 vertex
 and function
 that returns
inversely the index of the
 vertex nearest to point
 , we can
write the aforementioned described energy terms as
(4)
(5)
If the subdivision-mesh deformation is based solely on the
correspondence between landmarks and the proximity to the
cloud of points, the result will be a rough mesh with folded
triangles. This is because the forces that act on the mesh mayattract its vertices to disparate positions and, thus, fold the tri-angles. This problem can be overcome by posing a smoothness
constraint to make the underconstrained optimization tractable.
The smoothness is deÔ¨Åned as a measure of the elastic energy ofthe base mesh which penalizes nonparallel displacements of theedges and is given by
(6)
where
 is the set of
 ‚Äôs neighbors,
 is its cardinality, and
and
 are the initial positions of the vertices.
The deformation energy, whose minimization is sought, is
deÔ¨Åned as the weighted sum of the aforementioned energy terms
(7)
The coordinates of mesh
 vertices
 that minimize
can be found by differentiating (7) with respect to
 and set-
ting the partial derivatives equal to zero. Differentiation leads toa linear system which can be solved easily. However, we may
dispense with differentiation by writing
in matrix notation
(see the Appendix) and show that
 is minimized by the solu-
tion of the overdetermined linear system (8) solved using SVD.
(8)
Matrices
 and
 show whether a base-mesh vertex cor-
responds to a landmark and which is the neighborhood of each
vertex, respectively.
 is a vector formed by landmarks coordi-
nates while
 is the vector of base-mesh vertex initial positions.
,
 ,
 , and
 are used to deÔ¨Åne which vertices (
 ,
 )
Fig. 2. Fitting base mesh to a surface. (a) Base mesh. (b) Original surface. (c)
Base mesh Ô¨Åtted to the surface.
correspond to which points of the cloud (
 ,
). (More details
are given in the Appendix.)
As we have already described, vertex displacement is gov-
erned by two opposing forceÔ¨Åelds: 1) forces stemming from
,
 , and
 , which attracts the subdivision mesh toward
the cloud of points and 2) forces stemming from
 , which try
to keep vertices to their initial positions. However, the forces
due to
 and
 attract vertices toward the nearest points
of the cloud instead of the anatomically corresponding points.This is not a problem if the mesh is relatively close to the cloud,since nearest points and anatomically corresponding points al-
most coincide. But if the mesh is relatively far from the cloud,
vertices may be displaced so that anatomically erroneous corre-spondence is established. To overcome this problem, we iteratethe minimization process several times, letting vertices move
progressively until they converge to a Ô¨Ånal position. Thereby, at
the
th iteration, vertices are updated according to
(9)
where
 is the step size usually chosen in [0.2, 0.8], and
is the solution of the system (8) at the
 th iteration. We note
that
 ,
 , and
 in (8) vary at each iteration.
Finally, since the most time-consuming part of the minimization
is searching for nearest points according to (4) and (5), a space
partitioning technique, kD-trees [24], is applied to acceleratethe optimization. An illustration of mesh Ô¨Åtting can be seen inFig. 2.
The formulation of the point-to-point correspondence
problem as an energy minimization problem described by (7)and (8) has several advantages over other approaches reportedin the literature. In [25], the authors solve the problem of 3-D
correspondence using the optical Ô¨Çow between the associated
color images. Apart from the need of texture, the color imagesshould also be parameterized in the same cylindrical coordinateframe which is another problem of its own. Therefore, this
technique cannot be applied when the 3-D data are textureless
and in the form of a point cloud as in our case. A possiblealternative for our method is the use of elastically adaptivedeformable models, proposed by Metaxas [26] and used by
Kakadiaris et al. [12] and Passalis et al. [11] for face recog-
nition. However, this method involves the integration of asystem of second-order differential equations, which is moredemanding than the solution of a linear system as we propose.
502 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY , VOL. 3, NO. 3, SEPTEMBER 2008
Fig. 3. Correspondence is based on proximity. The deformable model is de-
picted with the continuous curve while the cloud of points has the dashed one.
Arrows show the point which is closest to their start. Dotted arrows depict the
cloud-to-model correspondences, while continuous arrows depict the model-to-
cloud correspondence. The usefulness of using both sets of directed distances
is apparent in the high curvature region, where there is a discontinuity in themodel-to-cloud set of correspondences (continuous arrows).
Allen et al. [27] present the method that is most similar to
ours, even though it is intended for the establishment of point
correspondence between whole human body scans. They alsouse landmarks and minimize the distance between the modeland the surface in hand using a smoothness term for regulariza-
tion. The main difference, however, is that they use only dis-
tances directed from the model to the cloud of points, while wealso use the distances with the inverse direction. This is impor-tant in the early stages of the optimization when the model is still
far from the cloud (see Fig. 3). In this case, the model-to-cloud
distances (continuous arrows in Fig. 3) may have discontinuitiesin regions of high curvature and, thus, give rise to anatomicallyinaccurate correspondences. Also using the other set of directed
distances (dotted arrows in Fig. 3), the resulting vector Ô¨Åeld be-
comes smoother and helps the optimization from getting trappedin local minima.
B. Subspace-Guided Elastically Deformable Model
In most cases of practical interest, the number of facial land-
marks which can be detected automatically and relatively ac-
curately is limited. Furthermore, the facial surface captured by
depth acquisition devices usually includes extraneous parts ofthe body, such as the neck and the upper head. In such a case,mesh Ô¨Åtting cannot be based on forces stemming from land-
marks‚Äô correspondence and surface-to-mesh distance. Recov-
ering the surface deformation in this case is an ill-posed problemwhich, however, becomes more tractable by assuming that thedeformations lie on a subspace of low dimensionality. This sub-
space may be estimated from training data by means of prin-
cipal component analysis (PCA). Once the faces of the trainingset have been set in correspondence following the procedure de-scribed in the previous section, base-mesh vertices of any novel
face may be written as a linear combination of eigenmeshes
(10)
Fig. 4. Mouth boundary detection. (a) The original face surface. (b) A vertical
proÔ¨Åle used to deÔ¨Åne a mouth bounding box. The tip of the nose, the upper and
lower points of the mouth are denoted by /84, /85, and /76, respectively. (c) The
sign of mean curvature /72of the surface. The sign of /72along with a measure of
cornerness inside the mouth bounding box is used to detect the mouth boundary
and deÔ¨Åne the associated landmarks. (d) The base mesh Ô¨Åtted to the original sur-
face using mouth-associated landmarks. (e) The base mesh Ô¨Åtted to the originalsurface without mouth-associated landmarks.
where
 is the truncated matrix which describes the principal
modes of deformation,
 is the control vector, and
 is the mean
vector of aligned vertex coordinates (i.e., mean face).
Now, by replacing (10) in (8) (the terms
 and
 are ex-
cluded), we may rewrite the deformation energy as a function
of
. Similar to the analysis in the previous section, the con-
trol vector
 which minimizes
 is the solution of the linear
system
(11)
Our experiments showed that this approach converges to the
global minimum of the deformation energy most of the time. Itwas also observed that failure usually occurs if the person dis-
plays an expression with a widely open mouth such as when
displaying surprise. Mesh Ô¨Åtting in this case may result in a de-formed closed mouth (Fig. 4), which implies that the low-di-mensional approximation of the face manifold is so rough in
the region of mouth that optimization is trapped in a local min-
imum. To overcome this problem, one can use a more complexmodel to capture the structure of the face manifold. For instance,in [28], the authors use active appearance models [29] combined
with a multilayer perceptron to establish correspondence among
2-D face images. This, however, makes the problem nonlinear.Thus, in this paper, we have experimented with a simpler tech-
nique that relies on the detection of the mouth boundary.
Mouth boundary is detected using depth and curvature in-
formation following an approach similar to that of [30]. A ver-tical proÔ¨Åle curve, as illustrated in Fig. 4, is used to detect the
MPIPERIS et al. : BILINEAR MODELS FOR 3-D FACE AND FACIAL EXPRESSION RECOGNITION 503
upper and lower points of the mouth contour exploiting the fact
that these points are local extrema of the proÔ¨Åle curve. Then, abounding box of the mouth is deÔ¨Åned where the mean curvature
of the surface
 is calculated. Mean curvature is
used because it obtains opposite signs on the lip ridges and themouth hollow assuming that the
axis is parallel to the gaze
direction. A measure of cornerness
 is also computed to
identify the horizontal outermost points of the mouth contour,
that is, the corners of the lips. Cornerness is deÔ¨Åned by the equa-tion
(12)
which is derived by the Harris corner detector [30]. Mean cur-
vature and cornerness are then fused with a MINI -MAX rule to a
single index that indicates the extent to which points of the sur-
face belong to the boundary. The boundary is Ô¨Ånally formed byÔ¨Åtting a spline curve through points with indices above a certainthreshold set empirically. Once the boundary has been detected,
the associated landmarks are included in the formulation of the
deformation energy, and the linear system in (11) is modiÔ¨Åed to
(13)
Boundary detection relies on the quality of 3-D data since it
involves estimation of second-order surface derivatives whichare sensitive to noise. As expected, higher depth quality re-sults in better detection. However, our experiments showed that
even an approximate detection of the mouth boundary is enough
for the minimization of the deformation energy to avoid localminima. Therefore, the proposed system is insensitive to smallboundary mislocalizations.
In summary, the overall procedure for establishing correspon-
dence among faces is as follows: During the training phase, cor-respondence among a set of annotated training 3-D images isestablished. These faces are rigidly registered with each other
using the ICP algorithm and then each facial point cloud is mod-
eled by a subdivision surface. Modeling consists in Ô¨Ånding thebase-mesh vertices that minimize the deformation energy de-Ô¨Åned by (7) with minimization achieved by iterating (8) upon
convergence. Once correspondence is established, PCA is ap-
plied to learn the principal modes of base-mesh deformation.
In the test phase, for a novel unseen face, its mouth boundary
has to be detected Ô¨Årst. Then (13) is solved to obtain the con-
trol parameters
from which the base-mesh vertices
 may be
recovered (10). Having Ô¨Åtted a subdivision surface to the newface, correspondence with all faces in the gallery is established,since all Ô¨Åtted surfaces originate from the deformation of an av-
erage face surface.
III. M
ODELING FACIAL EXPRESSION AND IDENTITY VARIATION
Once point correspondence is established, each facial surface
can be represented by the vector
 of the base-mesh verticessince it deÔ¨Ånes unambiguously the subdivision surface that
approximates the cloud of facial points. Then, one may classifyfaces or facial expressions using typical pattern classiÔ¨Åcation
techniques that rely on distance metrics of vector spaces.
However, this approach will result in degraded recognitionperformance, because such a representation cannot distinguishwhether a certain shape of the face is attributed to the identity
of the person or the expression he or she displays. For example,
we cannot distinguish whether a puffy cheek comes from anoverweight person or if it is due to a smiling expression. Itbecomes clear, therefore, that we have to encode identity and
expression in independent control parameters in order to be able
to perform joint expression-invariant facial identity recognitionand identity-invariant facial expression recognition.
In this work, we use bilinear models to capture the iden-
tity-expression structure of face appearance. Bilinear models are
linear in either factor when the other is held constant and, assuch, they share almost all of the advantages of linear models:they are simple in structure, computation, and implementation,
they can be trained with well-known algorithms, and their com-
plexity can be easily adjusted by their dimensionality as a com-promise between exact reproduction of training data and gener-alization during testing [22]. Simultaneously and despite their
simplicity, they can model subtle interactions by allowing fac-
tors to modulate each other‚Äôs contribution multiplicatively.
We use two types of bilinear models: 1) the symmetric and 2)
the asymmetric bilinear model, suitable for identity and expres-
sion recognition, respectively. In case of face recognition, we
neutralize the effect of expressions by deforming the matchedfaces so that they display a common expression (e.g., a neutralexpression or the expression displayed by the gallery face). This
is accomplished by modifying their expression control parame-
ters which are extracted by Ô¨Åtting faces to a symmetric bilinearmodel. The asymmetric model on the contrary is used to per-form identity-invariant expression recognition. After training
the model with several subjects depicting different facial expres-
sions, it is incorporated in a maximum-likelihood classiÔ¨Åcationframework which allows facial expression discrimination acrossidentity.
In the following sections, we describe, in detail, the sym-
metric and asymmetric bilinear model and we present a novelgeneral solution of the minimization involved in their training.
A. Symmetric Model
Let
be the stacked column vector of the
 base-mesh
vertices of the facial surface of person
 with expression
 . The
dimension of
 , which is
 , is denoted by
 for simplicity.
Then, each component
 is given by the general bilinear form
[21], [22]
(14)
Here,
 and
 are the control parameters which control expres-
sion and identity, respectively, while
 are the coefÔ¨Åcients
504 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY , VOL. 3, NO. 3, SEPTEMBER 2008
which model the interaction of the factors and we try to esti-
mate from the training set. The aforementioned equation can bewritten equivalently as
(15)
where
 are stacked vectors of
dimension
 . Now, it is clear that
 is the bilinear
combination of basis vectors
 which are mixed by control
coefÔ¨Åcients
 and
 . The aforementioned equation also shows
that control parameters symmetrically weigh the basis vectors
and, therefore, this model is referred to as the symmetric bilinearmodel.
Let us assume that
faces exist in our database belonging
to
 individuals and each one depicting one of
 possible ex-
pressions. Our goal is to Ô¨Ånd the interaction coefÔ¨Åcients
and the control parameters
 and
 for each individual-ex-
pression couple. Using the matrix notation, (14) is simpliÔ¨Åed to
(16)
where
 ,
 , and
. Since, in practice, a facial expression from the
 pos-
sible ones may not be available for some people, we also deÔ¨Åne
the zero-one function
 which is one if the
 th face
belongs to the individual
 with expression
 . Unknown coefÔ¨Å-
cients arise from the minimization of the total squared error [21]
(17)
By differentiating and setting the partial derivatives equal to
zero, we end up with the system of equations
(18)
(19)
(20)
where
 is the number of training faces which
belong to subject
 displaying expression
 and
 is their sum
. Equation(20) is a general Sylvester equation [31] which can be rewritten
as
(21)
where
 is the Kronecker product operator and
 is the
matrix vectorization operator which stacks the columns of thematrix.
Interaction matrices
and control vectors
 and
 may
now be found by iterating (21), (18), and (19), respectively. Toensure the stability of the solution, updating is performed pro-gressively according to the rule
(22)
where
 is the step size which is usually chosen in [0.2, 0.8],
stands for the Ô¨Ånal value of
 ,
or
 in the
 th itera-
tion, and
 stands for the value resulting from (18), (19), or
(21), respectively.
It should be noted that convergence depends on the dimen-
sionalities
 of
 , and
 of
 , which control the exactness of
the training data reproduction. Convergence is guaranteed if
and
 are less than or equal to
 and
 , the number of expres-
sions and the number of individuals, respectively. If
 is equal
to
 , and
 is equal to
 , training data are reproduced exactly,
while more coarse but also more compact representations result
if these dimensionalities are decreased.
The minimization of the total squared error through (18),
(19), and (21) presented here differs from the optimization
scheme proposed by Tenenbaum and Freeman in [22] and their
previous work. There, minimization is achieved by means ofSVD applied to the
mean observation block matrix
......
 (23)
where
 is the mean vertex vector of the facial surfaces of
subject
 with expression
 . This technique relies on evenly dis-
tributed data across expression and identity. In practice, how-
ever, data may not be evenly distributed across expression andidentity or even worse, there may not be available data for aparticular expression-identity combination. Then,
will have
some indeterminate entries and SVD will not be applicable.
One remedy is Ô¨Ålling the missing entries with the mean valuesof the appropriate expression and identity, but this substitutiondoes not guarantee the global minimization of the total squared
error and, thus, the best Ô¨Åt of the bilinear model to the training
data.
3In contrast, the proposed method may be applied directly
3Generalizations of bilinear models using tensors, such as [19] and [32], face
the same problem since the estimation of model parameters is accompished by
applying SVD to matrices resulting from tensor mode- /110Ô¨Çattening.
MPIPERIS et al. : BILINEAR MODELS FOR 3-D FACE AND FACIAL EXPRESSION RECOGNITION 505
Fig. 5. Deformation of facial surface across expression and identity control parameters. Inside boxes are surfaces which are stored in the BU-3DFE da tabase.
SpeciÔ¨Åcally, subjects F0004, M0044 and F0002 display the sad and happy expressions. The rest surfaces have been generated by linear interpolation of expression
and identity control parameters of the former surfaces.
without any assumptions for the data distribution, but at the ex-
pense of computational complexity.
An illustration of facial surface modeling using the sym-
metric bilinear model is shown in Fig. 5, where the surfaceshave been generated by linearly interpolating the coefÔ¨Åcientscorresponding to two expressions and three identities.
B. Asymmetric Model
As already mentioned in the previous section, the symmetric
model symmetrically weighs the coefÔ¨Åcients which controlidentity and expression. This symmetry implies that the model
can generalize on both directions, identity and expression,
which is a desirable property for both face and expressionrecognition. However, there is a difference between theserecognition tasks. In case of face recognition, it is usually
assumed that unseen faces are free to display an unrestricted
number of possible expressions. In case of expression recog-nition instead, the number of possible expressions is usuallyconsidered Ô¨Ånite (most studies classify expressions to the six
prototypic universal expressions proposed by Ekman [5]). In
the latter case, the symmetric model can be modiÔ¨Åed so thata better Ô¨Åt to expressions can be achieved thus improvingrecognition performance. This is accomplished by letting the
interaction coefÔ¨Åcients
vary with the expression control
parameters
 [22], that is
(24)Using the aforementioned deÔ¨Ånition and (14), the vector repre-
sentation of the face is now given by
(25a)
(25b)
where now matrix
 controls expression. The identity of the
face is still controlled by vector
 .
Fitting the asymmetric model to the training data consists in
Ô¨Ånding the expression matrices
 and identity vectors
 that
minimize the total squared error [21]
(26)
and
 are obtained by differentiating the error function and
setting the partial derivatives to be equal to zero. Following thisprocedure,
and
 should satisfy the system of [21]
(27)
(28)
which is Ô¨Ånally solved by iterating the equations according to
rule (22) until the values of
 and
 converge.
506 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY , VOL. 3, NO. 3, SEPTEMBER 2008
The exactness of training data reproduction is determined by
the number of columns of matrix
 or, equivalently, the dimen-
sionality of vector
 . More exact reconstruction is achieved if
more columns are used. Nevertheless, the number of columns
should be restricted in order to avoid overÔ¨Åtting [22] and it mustbe less than the number of subjects in the training set so that thesolution of (27) and (28) is feasible.
IV . F
ACIAL EXPRESSION RECOGNITION
In this section, we show how a facial expression classiÔ¨Åer may
be built on the basis of an asymmetric bilinear model Ô¨Åtted to a
training set of faces.
A. Training
The input is a set of training faces annotated with salient fa-
cial points. This set should contain images of several people de-picting different facial expressions. First, anatomical correspon-
dence among raw data is established by means of the elastically
deformable base mesh
as explained in Section II-A. Then,
the resulting model parameters are used to build the 3-D faceeigenspace so that newly seen faces may be processed without
the need of facial landmarks.
An asymmetric bilinear model is then Ô¨Åtted to the deformable
model parameters
of the training faces. That is, we estimate
the expression control matrices
 and identity control vectors
that minimize the total squared reconstruction error given
by (26). This is done by iterating equations (27) and (28) untilthe relative change in the Frobenius norm of both
and
becomes less than a predeÔ¨Åned constant threshold.
Estimated
 and
 are then used to build a maximum-
likelihood classiÔ¨Åer. The goal is to estimate the likelihood ofthe surface model parameters
for each expression, that is, the
conditional probability density function (pdf)
 , where
is one of the prototypic expressions. To this end, we assumethat the vertex vector
deÔ¨Åning the facial surface of person
with expression
 is a random vector with spherical gaussian
pdf centered at the prediction of the asymmetric bilinear model.
That is
(29)
where
 is the error variance. Using the total probability the-
orem, the conditional pdf of
 assuming expression
 may now
be written as
(30)
where
 stands for the a priori probability of person
 which
may be considered constant for all subjects and equal to
 .
B. ClassiÔ¨Åcation
The facial expression of a novel test face is classiÔ¨Åed simply
by comparing the conditional pdfs of
 (30) for all expressions.
First, the test surface is set into anatomical correspondence withall meshes in the training set. If landmarks are available for
the test face, then the landmark-guided deformation of the basemesh explained in Section II-A is followed; otherwise, the sub-
space-guided deformation described in Section II-B is applied.Once the surface model parameters
are found, the expression
of the face is classiÔ¨Åed to the prototypic expression with the
greatest likelihood, that is, the expression
 for which
(31)
V. F ACE RECOGNITION
Similar to facial expression recognition, face recognition also
involves a bootstrapping phase before classiÔ¨Åcation. However,
classiÔ¨Åcation follows a different approach which consists in al-tering the expression of the probe face. This is accomplished by
modulating the parameters of a symmetric bilinear model Ô¨Åtted
to the probe face in order to force it to display the expressionof every gallery face before matching. In the following text, wepresent this procedure in detail.
A. Bootstrapping
Since subjects to be classiÔ¨Åed are free to display various ex-
pressions, a bootstrap set of faces is used to train a symmetricbilinear model so that we may be able to generalize on any novelunseen identity or expression. The bootstrap set is also used for
learning the mesh deformation eigenspace so that newly seen
faces may be processed without the need for facial landmarks.
Bootstrap faces are Ô¨Årst set into anatomical correspondence
as described in Section II-A and the mesh eigenspace is then
learned by means of PCA. The training of the symmetric model
starts by training an asymmetric bilinear model such as the oneused for facial expression recognition. This is done to obtain ini-tial values for the identity control vectors
. After training the
asymmetric model, vectors
 are used to train an initial sym-
metric model following the SVD approach presented in [22].Analytically, this is done by applying SVD to the mean observa-tion block matrix deÔ¨Åned in (23). By deÔ¨Åning the vector trans-
pose operator
, which transposes a matrix block-wise as
shown in Fig. 6, the mean observation matrix may be written as
(32)
(33)
where matrices
 ,
, and
 are comprised with
 ,
, and
(15), respectively
......
 (34)
Matrix
 is initialized by the identity control vectors es-
timated by the asymmetric model. Then, the SVD of
is computed and matrix
 is up-
dated by the Ô¨Årst
 rows of
 . Similarly, the SVD of
is computed and matrix
 is updated
by the Ô¨Årst
 rows of
 . This constitutes one iteration of the
error minimization algorithm whose convergence is guaranteed.
Upon convergence,
 results in
 .
MPIPERIS et al. : BILINEAR MODELS FOR 3-D FACE AND FACIAL EXPRESSION RECOGNITION 507
Fig. 6. Vector transpose operator /40/91 /1 /93
 /41acting on a matrix which is consid-
ered to consist of 2 /21 blocks. The operator transposes the matrix blockwise.
(For more details, the reader is referred to [22].) By rearranging
, we may compute interaction matrices
 .
Then, Ô¨Ånal optimal parameters are found by iterating equations
(18), (19), and (21) until convergence is achieved.
B. Gallery Image Processing
Using the optimal mixing matrices
 found from the boot-
strap set, we extract the expression and identity control vectors
for each gallery face. This is accomplished by minimizing thereconstruction squared error
(35)
Minimization is achieved similarly to the minimization of the
total squared error in (17) during the training of the symmetricbilinear model. By differentiating (35) and setting partial deriva-tives equal to zero, control vectors are found by iterating the
system of equations
(36)
(37)
Equations are iterated until the change in the norm of
 and
becomes less than a threshold. The estimated expression control
vectors of the gallery faces are subsequently used during classi-Ô¨Åcation to modify the expression displayed by the probe face.
C. ClassiÔ¨Åcation
The classiÔ¨Åcation of a novel probe face starts by setting it into
correspondence with the gallery faces according to Section II.
If landmarks are available, the landmark-guided approach
(Section II-A) is followed; otherwise, the subspace-guidedtechnique (Section II-B) is applied using the mesh eigenspacelearned from the bootstrap set. We note that we have made no
assumptions about the expression of the gallery faces meaning
that they are allowed to display various expressions that arepossibly different from that of the probe faces. Then, to handlethe inÔ¨Çuence of expression on surface matching, we force
the probe face to display the expression of the gallery face
before matching (Fig. 7). Its expression control vector has to
Fig. 7. Expression manipulation. The Ô¨Årst column shows the original 3-D
face scans of the same subject displaying a smiling (Ô¨Årst row) and a surprising
(bottom row) expression. The second column shows the elastically deformable
model Ô¨Åtted to the original surfaces, while the third column shows recon-
structions of the surfaces using bilinear model coefÔ¨Åcients. Neutralization
of expressions shown in the fourth column is achieved by modulating the
expression control parameters.
be extracted by Ô¨Åtting the symmetric bilinear model as in the
case of the gallery faces. Then, by substituting the expression
control vector by the corresponding vector of the gallery face,we may reconstruct a new probe facial surface which depicts anexpression similar to that of the gallery face. If the new probe
face is represented by the vertex vector
while the gallery
face is represented by the vertex vector
 , then expression-free
comparison between them may be established on the inverse oftheir vertex vectors‚Äô squared Euclidean distance
(38)
VI. P ERFORMANCE EV ALUATION
In this section, we evaluate the performance of bilinear
models on the Binghamton University 3-D Facial Expression
(BU-3DFE) database [6] and we provide comparisons withWang et al. ‚Äôs work [1] on facial expression recognition and our
previous work [2] on face recognition. BU-3DFE is preferred
over other popular databases, such as FRGC [33], because
expressions are well deÔ¨Åned and distributed better.
BU-3DFE contains 100 subjects‚Äî56 female and 44
male‚Äîwith a variety of ethnic/racial ancestries, including
white, black, East Asian, Middle East Asian, hispanic latino,
and others. The subjects display the six universal expressionsof anger, fear, disgust, happiness, sadness, and surprise in fourlevels of intensity: low, middle, high, and highest. For each
subject, there is also a 3-D face scan with neutral expression,
thus resulting in a total number of 2500 face scans in the data-base. 3-D facial data are in the form of VRML models whichcontain about 13 000‚Äì21 000 polygons and are associated with
a set of feature points located on the eyes, eyebrows, nose,
mouth, and the boundary of the face. These Ô¨Åducial points havebeen detected manually and are used as landmarks during theestablishment of point correspondence in training stages. It is
emphasized that in the following experiments, landmarks are
used only in training stages and not in testing stages.
508 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY , VOL. 3, NO. 3, SEPTEMBER 2008
A. Facial Expression Recognition
In this series of experiments, we demonstrate the performance
of the asymmetric model in the recognition of the six prototypicexpressions following the ten-fold cross-validation approach.
In each experiment, BU-3DFE subjects are divided randomly
in two sets: 1) a training set consisting of 90 subjects and 2) atest set consisting of the remaining ten subjects, thus ensuringthe independence on a subject identity. First, the base mesh
is built by selecting
 vertices lying on an average facial
surface and then training faces are set into point correspondenceas described in Section II-A. The training set is also used to learnthe mesh deformation subspace (250 principal modes are kept)
and then to train the asymmetric bilinear model. That is, we es-
timate the six matrices
corresponding to the six possible ex-
pressions and the 90 identity control vectors
 corresponding
to the subjects of the training set. The column number of
and the dimension of
 is set to 80 while the number of rows
is equal to the triple of vertices number
 . The entries
of
 and
 are initialized randomly and then they are com-
puted by iterating (27) and (28) until the relative change in their
Frobenius norm goes below a threshold (0.01) or a maximum
number of iterations (150‚Äì180) is reached. Estimated matrices
and vectors
 are then used to build the maximum-likeli-
hood classiÔ¨Åer, letting the error variance be
 .
During testing, each test face is set into correspondence fol-
lowing the subspace-guided approach described in Section II-B(we assume that landmarks are not available for test faces).Once the vertex vector
is computed, the expression of the
face is classiÔ¨Åed to the class with the highest likelihood (see
Section IV).
This procedure (training-testing) is repeated ten times en-
suring that every subject is included in one test set. Results
are averaged and presented as a confusion matrix in Table I.Expressions have been labelled by the subjects who have per-formed them and this labelling is used as ground truth in our
experiments. In order to provide a measure of comparison, we
report that the average recognition rate achieved by human ex-perts [6] varies from 94.1% for low intensities up to 98.1% forthe highest intensities. We also provide Table II which shows
the corresponding confusion matrix obtained according to [1].
From Table I, it can be seen that the highest misclassiÔ¨Åcationoccurs between the expressions of anger and sadness. The de-crease in these recognition rates is attributed to their similarity
especially in low intensities. We note that in our experiments, we
used for testing facial models of all intensities for every expres-sion, instead of the highest two as in [1]. The main differencebetween the angry and sad expression lies mostly on the conÔ¨Åg-
uration of the eyebrows, which cannot be effectively captured
using depth (at least with our point correspondence technique)especially in low intensities, where the difference is so subtleeven for a human eye. Nevertheless, the total average recogni-
tion rate of 90.5% that was achieved proves the overall superior
performance of the proposed algorithm.
B. Face Recognition
To evaluate 3-D face recognition performance, we adopt the
following procedure which simulates a realistic application sce-
nario. We split the database into two parts based on subjectTABLE I
EXPRESSION RECOGNITION BASED ON THE ASYMMETRIC BILINEAR MODEL
Values in parentheses are standard deviations.
TABLE II
EXPRESSION RECOGNITION BASED ON PRIMITIVE SURFACE FEATURE
DISTRIBUTION (WANG et al. [1])
identity. The Ô¨Årst part serves as a bootstrap set and is used for
training the elastically deformable model (subspace learning de-
scribed in Section II-B) and computing the bilinear model coef-
Ô¨Åcients. The rest of the data, the test set, is split into the galleryset that contains a single 3-D image per subject (neutral or non-neutral), and the probe set that contains the images to be classi-
Ô¨Åed (various facial expressions).
The bootstrap set is comprised of 50 subjects chosen ran-
domly from the database while the gallery and probe set con-
sist of the remaining 50 subjects. Faces in the bootstrap set are
Ô¨Årst set into correspondence following the landmark-guided ap-proach of Section II-A. Then, they are used to learn the meshdeformation subspace (250 principal modes are kept again) and
to train a symmetric bilinear model. The training of the model
starts by training an asymmetric bilinear model. such as the oneused for facial expression recognition before. This model is usedto initialize the training of another initial symmetric model fol-
lowing the SVD approach described in Section V. The dimen-
sion of vectors
is set to 45 while the dimension of vectors
is set to 5. The Ô¨Ånal symmetric model results from the op-
timization of the estimated parameters
 ,
, and
 , which
is achieved by iterating equations (18), (19), and (21) until the
relative change in their Frobenius norm goes below a threshold(0.01) or a maximum number of iterations (150‚Äì180) is reached.
Then we proceed with gallery image processing. First, the
deformable model has to be Ô¨Åtted to each gallery image. We as-
sume that we do not have any landmarks and, therefore, the sub-space-guided approach described in Section II-B is used. Then,the bilinear model is Ô¨Åtted to each gallery image to acquire its
expression and identity control parameters.
MPIPERIS et al. : BILINEAR MODELS FOR 3-D FACE AND FACIAL EXPRESSION RECOGNITION 509
Fig. 8. Cumulative match characteristic (CMC) and receiver operating characteristic (ROC) for face recognition based on: (a) the symmetric bilinea r model and
(b) the geodesic polar representation presented in [2]. Error bars indicate a standard deviation in CMC and a 95% conÔ¨Ådence interval in ROC.
This procedure (deformable model and bilinear model Ô¨Åtting)
is also repeated for each probe image during testing. Having ob-tained its expression and identity control parameters, the probe
image is then compared with every image in the gallery to ob-
tain the similarity score (see Section V).
We have repeated the aforementioned experiments on several
randomly chosen subdivisions of bootstrap and test sets, under
the constraint that all subjects are included at least once in the
test set. The recognition results are averaged and presented inFig. 8, which shows the cumulative match characteristic and thereceiver operating characteristic (ROC) of the proposed system
compared with the results obtained by our previous work in
[2]. There, we used an expression-invariant face representation,based on geodesic polar coordinates and an isometric model ofthe facial surface deformation, which proved to be better than
Bronstein et al. ‚Äôs [15] canonical images and a PCA-based al-
gorithm. We also note that some images depicting extreme ex-pressions that violated the isometry assumption and had to beexcluded from experiments in [2] are now included in these ex-
periments. The increase in the rank-1 recognition rate shows that
the proposed algorithm may deal well even with extreme ex-pressions which are one of the main limitations of current 3-Dface-recognition algorithms.
The deformable model and bilinear model training requires a
few hours in a typical Pentium V , 3 GHz, 1-GB RAM worksta-tion running nonspeed-optimized code. On the other hand, depthacquisition is performed in less than 2 ms [6], while processing
of a novel image takes less than 3 s (2 s for point correspon-
dence and 1 s for bilinear model Ô¨Åtting), which means that theproposed algorithm may be used in near-real-time systems.
C. Limitations
In the previous sections, we showed that bilinear models may
effectively capture the bifactor nature of the facial surface geom-
etry and, thus, lead to high facial identity and expression recog-
nition rates. Nevertheless, there are still some issues that limitrecognition performance, especially face recognition, and pro-vide room for further investigation.The main limitation is the need for a large bootstrap set which
should also be annotated with respect to facial expressions. Themore different expressions that are present in the bootstrap set,
the better the estimation of the interaction matrices
is, and
the better the novel face Ô¨Åt is. Training with a few expressionsleads to unbalanced generalization ability in favor of identitywhich, in turn, leads to better surface approximation but with
poorer expression control. However, building and annotating in
practice a bootstrap set may be difÔ¨Åcult considering that a greatnumber of possibly ambiguous expressions have to be classiÔ¨Åedinto a Ô¨Ånite number of expression classes.
Another factor that affects performance is the accuracy of
point correspondence between faces. In our experiments, weobserved that poor correspondence substantially affects bilinearmodel training and eventually recognition performance. The
problem is twofold: During training, the bilinear model cannot
learn the true identity-expression manifold, implying errorsin bilinear parameters‚Äô estimation. During testing, expressionmanipulation is actually applied on a slightly (or quite) dif-
ferent face. This error is further ampliÔ¨Åed by inaccurate bilinear
parameters, leading to a distorted facial surface.
VII. S
UMMARY
In this paper, we proposed a technique for joint 3-D face and
facial expression recognition. We Ô¨Årst presented a novel model-based approach for establishing point correspondence among
faces which involves the solution of a simple linear system. We
also proposed using directed distances both from the mesh to thecloud of facial points and inversely which leads to a smootherforceÔ¨Åeld and, thus, more plausible anatomical correspondence.
Another advantage of this approach is that correspondence may
be performed fully automatically after training the system witha number of facial surfaces annotated with anatomical salientpoints. We also provided a solution for the problem of open
mouth in this case whose conÔ¨Åguration might cause problems
during the establishment of correspondence. Then, we proposedbilinear models for joint face and expression recognition and weprovided the general solution of the error minimization during
510 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY , VOL. 3, NO. 3, SEPTEMBER 2008
the training of the symmetric bilinear model. Our algorithm was
Ô¨Ånally evaluated on the BU-3DFE database and proved its su-periority over expression-invariant face representations for face
recognition [2] and primitive surface features for expression
recognition [1].
A
PPENDIX
Let
 be the
 -dimensional vector of the base-mesh ver-
tices. Also, let
 be a
 -dimensional vector
containing landmark coordinates
 and
 ,
 , 3-D
vectors so that
if vertex
 of
 corresponds
to a landmark
otherwise(39)
if vertex
 is a landmark vertex
otherwise.(40)
4Then, (3) yields
(41)
where
 using incom-
plete Cholesky decomposition.
By deÔ¨Åning the
 and
 vectors
 and
 , respectively,
and the
 and
 block matrices
 and
 made
by 3
 3 blocks
(42)
(43)
(44)
(45)
Equations (4) and (5) yield
(46)
(47)
The elastic energy
 in (6) can be written in matrix notation
using the
 block matric
as follows:
(48)
4 /48
is the vector /91/48 /48 /48 /93
 , /49
is the vector /91/49 /49 /49/93
 and /73
is the 3 /23 identity
matrix.where
 is a truncated triangular matrix resulting from the in-
complete Cholesky decomposition of
 .
By using the above equations and (7), it can be easily shown
that
 is minimized by the solution of the overdetermined
linear system (49), which is solved by using SVD
(49)
REFERENCES
[1] J. Wang, L. Yin, X. Wei, and Y. Sun, ‚Äú3D facial expression recognition
based on primitive surface feature distribution,‚Äù in Proc. Conf. Com-
puter Vision and Pattern Recognition , 2006, vol. 2, pp. 1399‚Äì1406.
[2] I. Mpiperis, S. Malassiotis, and M. G. Strintzis, ‚Äú3D face recognition
with the geodesic polar representation,‚Äù IEEE Trans. Inf. Forensics Se-
curity , vol. 2, no. 3, pp. 537‚Äì547, Sep. 2007.
[3] K. Bowyer, K. Chang, and P. Flynn, ‚ÄúA survey of approaches and chal-
lenges in 3D and multi-modal 3D+2D face recognition,‚Äù Comp. Vision
Image Understanding , vol. 101, pp. 1‚Äì15, 2006.
[4] K. Chang, K. Bowyer, and P. Flynn, ‚ÄúAn evaluation of multi-modal 2D
+ 3D face biometrics,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 27,
no. 4, pp. 619‚Äì624, Apr. 2005.
[5] P. Ekman and W. Friesen , Facial Action Coding System (FACS):
Manual . Palo Alto, CA: Consulting Psychologists Press, 1978.
[6] L. Yin, X. Wei, Y. Sun, J. Wang, and M. J. Rosato, ‚ÄúA 3D facial ex-
pression database for facial behavior research,‚Äù in Proc. 7th Int. Conf.
Automatic Face and Gesture Recognition , Apr. 2007, pp. 211‚Äì216.
[7] X. Lu and A. Jain, ‚ÄúDeformation analysis for 3D face matching,‚Äù in
Proc. 7th IEEE Workshop Applications of Computer Vision , Jan. 2005,
vol. 1, pp. 99‚Äì104.
[8] C. Samir, A. Srivastava, and M. Daoudi, ‚ÄúThree-dimensional face
recognition using shapes of facial curves,‚Äù IEEE Trans. Pattern Anal.
Mach. Intell. , vol. 28, no. 11, pp. 1858‚Äì1863, Nov. 2006.
[9] C.-S. Chua, F. Han, and Y.-K. Ho, ‚Äú3D human face recognition using
point signature,‚Äù in Proc. 4th IEEE Int. Conf. Automatic Face Gesture
Recognition , 2000, pp. 233‚Äì238.
[10] M. Pantic and L. J. M. Rothkrantz, ‚ÄúAutomatic analysis of facial ex-
pressions: The state of the art,‚Äù IEEE Trans. Pattern Anal. Mach. In-
tell., vol. 22, no. 12, pp. 1424‚Äì1445, Dec. 2000.
[11] G. Passalis, I. Kakadiaris, T. Theoharis, G. Toderici, and N. Murtuza,
‚ÄúEvaluation of 3D face recognition in the presence of facial expres-
sions: An annotated deformable model approach,‚Äù presented at the
IEEE Workshop Face Recognition Grand Challenge Experiments, Jun.
2005.
[12] I. Kakadiaris, G. Passalis, G. Toderici, M. Murtuza, Y. Lu, N. Karam-
patziakis, and T. Theoharis, ‚ÄúThree-dimensional face recognition in the
presence of facial expressions: An annotated deformable model ap-
proach,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 29, no. 4, pp.
640‚Äì649, Apr. 2007.
[13] K. Chang, K. Bowyer, and P. Flynn, ‚ÄúMultiple nose region matching
for 3D face recognition under varying facial expression,‚Äù IEEE Trans.
Pattern Anal. Mach. Intell. , vol. 28, no. 10, pp. 1695‚Äì1700, Oct. 2006.
[14] X. Li and H. Zhang, ‚ÄúAdapting geometric attributes for expression-in-
variant 3D face recognition,‚Äù in Proc. IEEE Int. Conf. Shape Modeling
Applications , 2007, pp. 21‚Äì32.
[15] A. M. Bronstein, M. M. Bronstein, and R. Kimmel, ‚ÄúExpression-in-
variant representations of faces,‚Äù IEEE Trans. Image Process. , vol. 16,
no. 1, pp. 188‚Äì195, Jan. 2007.
[16] P. Aleksic and A. Katsaggelos, ‚ÄúAutomatic facial expression recogni-
tion using facial animation parameters and multistream HMMs,‚Äù IEEE
Trans. Inf. Forensics Security , vol. 1, no. 1, pp. 3‚Äì11, Mar. 2006.
[17] S. Ioannou, G. Caridakis, K. Karpouzis, and S. Kollias, ‚ÄúRobust feature
detection for facial expression recognition,‚Äù EURASIP J. Image Video
Process. , 2007.
[18] C. S. Lee and A. Elgammal, ‚ÄúNonlinear shape and appearance models
for facial expression analysis and synthesis,‚Äù in Proc. 18th Int. Conf.
Pattern Recognition , 2006, pp. 497‚Äì502.
[19] M. A. O. Vasilescu and D. Terzopoulos, ‚ÄúMultilinear subspace analysis
of image ensembles,‚Äù in Proc. IEEE Conf. Computer Vision and Pattern
Recognition , Jun. 2003, pp. 93‚Äì99.
[20] H. Wang and N. Ahuja, ‚ÄúFacial expression decomposition,‚Äù in Proc.
Ninth IEEE Int. Conf. Computer Vision , Oct. 2003, vol. 2, pp. 958‚Äì965.
MPIPERIS et al. : BILINEAR MODELS FOR 3-D FACE AND FACIAL EXPRESSION RECOGNITION 511
[21] J. Tenenbaum and W. Freeman, ‚ÄúSeparating style and content,‚Äù Ad-
vances in Neural Information Processing Systems , vol. 9, 1997.
[22] J. Tenenbaum and W. Freeman, ‚ÄúSeparating style and content with bi-
linear models,‚Äù Neural Comput. , vol. 12, pp. 1247‚Äì1283, 2000.
[23] C. Mandal, H. Qin, and B. C. Vemuri, ‚ÄúNovel FEM-based dynamic
framework for subdivision surfaces,‚Äù Comput.-Aided Design , vol. 32,
no. 8, pp. 479‚Äì497, 2000.
[24] J. L. Bentley, ‚ÄúK-d trees for semidynamic point sets,‚Äù in Proc. 6th Annu.
Symp. Computational Geometry , 1990, pp. 187‚Äì197.
[25] V. Blanz and T. Vetter, ‚ÄúFace recognition based on Ô¨Åtting a 3D mor-
phable model,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 25, no. 9,
pp. 1063‚Äì1074, Sep. 2003.
[26] D. Metaxas and I. Kakadiaris, ‚ÄúElastically adaptive deformable
models,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 2, no. 10, pp.
1310‚Äì1321, Oct. 2002.
[27] B. Allen, B. Curless, and Z. Popovic ¬¥, ‚ÄúThe space of human body
shapes: Reconstruction and parameterization from range scans,‚Äù in
Proc. SIGGRAPH: ACM SIGGRAPH Papers , New York, 2003, pp.
587‚Äì594, ACM.
[28] G. Edwards, A. Lanitis, C. Taylor, and T. Cootes, ‚ÄúStatistical models
of face images: Improving speciÔ¨Åcity,‚Äù in Proc. British Machine Vision
Conf. , 1996, vol. 2, pp. 765‚Äì774.
[29] T. F. Cootes, G. J. Edwards, and C. J. Taylor, ‚ÄúActive appearance
models,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 23, no. 6, pp.
681‚Äì685, Jun. 2001.
[30] X. Lu and A. K. Jain, ‚ÄúMultimodal facial feature extraction for auto-
matic 3D face recognition‚Äù Dept. Comput. Sci., Michigan State Univ.,
East Lansing, Tech. Rep. MSU-CSE-05-22, Aug. 2005.
[31] C. V. Loan, ‚ÄúThe ubiquitous kronecker product,‚Äù J. Comput. Appl.
Math. , vol. 123, p. 85100, 2000.
[32] D. Lin, Y. Xu, X. Tang, and S. Yan, ‚ÄúTensor-based factor decomposi-
tion for relighting,‚Äù in Proc. IEEE Int. Conf. Image Processing , Sep.
2005, vol. 2, pp. 386‚Äì389.
[33] P. Phillips, P. Flynn, T. Scruggs, K. Bowyer, J. Chang, K. Hoffman,
J. Marques, J. Min, and W. Worek, ‚ÄúOverview of the face recognition
grand challenge,‚Äù in Proc. IEEE Computer Vision Pattern Recognition ,
Jun. 2005, pp. 947‚Äì954.
Iordanis Mpiperis received the Diploma in elec-
trical and computer engineering from Aristotle
University of Thessaloniki, Thessaloniki, Greece,in 2004, where he is currently pursuing the Ph.D.
degree in electrical and computer engineering.
He holds research and teaching assistantship
positions at the Aristotle University of Thessaloniki.
He is a Postgraduate Research Fellow with the Infor-
matics and Telematics Institute, Centre for Research
and Technology Hellas, Thessaloniki. His research
interests include biometrics, multimodal interfaces
for human-machine communication, computer vision, and pattern recognition.
Sotiris Malassiotis was born in Thessaloniki,
Greece, in 1971. He received the B.S. and Ph.D.
degrees in electrical engineering from the Aristotle
University of Thessaloniki, Thessaloniki, Greece, in
1993 and 1998, respectively.
From 1994 to 1997, he was conducting research
in the Information Processing Laboratory at the
Aristotle University of Thessaloniki. Currently, he
is a Senior Researcher (Grade C) in the Informatics
and Telematics Institute, Thessaloniki. He has
participated in many European and national research
projects. He is the author of more than 20 articles in refereed journals and
more than 50 papers in international conferences. His research interests include
biometrics, computer vision, stereoscopic and range image analysis, pattern
recognition, and computer graphics.
Michael G. Strintzis (S‚Äô68‚ÄìM‚Äô70‚ÄìSM‚Äô80‚ÄìF‚Äô03)
received the Diploma in electrical engineering fromthe National Technical University of Athens, Athens,
Greece, in 1967 and the M.A. and Ph.D. degrees
in electrical engineering from Princeton University,
Princeton, NJ, in 1969 and 1970, respectively.
He joined the Electrical Engineering Department,
University of Pittsburgh, Pittsburgh, PA, where he
served as an Assistant Professor from 1970 to 1976
and an Associate Professor from 1976 to 1980.
During that time, he worked in the area of stability
of multidimensional systems. Since 1980, he has been a Professor of electrical
and computer engineering at the Aristotle University of Thessaloniki, Thessa-
loniki, Greece. He has worked in the areas of multidimensional imaging and
video coding. Over the past ten years, he has authored more than 100 journal
publications and over 200 conference presentations. In 1998, he founded the
Informatics and Telematics Institute, currently part of the Centre for Research
and Technology Hellas, Thessaloniki.
Prof. Strintzis was awarded the Centennial Medal of the IEEE in 1984 and
the Empirikeion Award for Research Excellence in Engineering in 1999.
"
https://ieeexplore.ieee.org/document/9815154,Error
https://ieeexplore.ieee.org/document/6200254,"Collecting
Large, Richly
Annotated
Facial-Expression
Databases from
Movies
Abhinav Dhall
Australian National University
Roland Goecke
University of Canberra, Australia
Simon Lucey
Commonwealth Scientific and Industrial Research
Organization (CSIRO)
Tom Gedeon
Australian National University
Collecting richly annotated, large
datasets representing real-world
conditions is a challenging task.With the progress in computer
vision research, researchers have developed ro-bust human facial-expression analysis solu-tions, but largely only for tightly controlledenvironments. Facial expressions are the visible
facial changes in response to a person‚Äôs inter-
nal affective state, intention, or social commu-nication. Automatic facial-expression analysishas been an active research field for morethan a decade, with applications in affectivecomputing, intelligent environments, lie detec-tion, psychiatry, emotion and paralinguisticcommunication, and multimodal human-
computer interface (HCI).
In the automatic human facial-analysis do-
main, realistic data plays an important role.However, as anyone in the facial-analysis com-munity will attest, such datasets are extremelydifficult to obtain. Much progress has beenmade in the facial- and human-activity-recog-nition fields in the past few years due to the
availability of realistic databases as well as ro-
bust representation and classification tech-niques. However, although several popularfacial-expression databases exist, the majorityhave been recorded in tightly controlled labora-tory environments, where the subjects wereasked to generate certain expressions. Theselab scenarios are in no way a true representa-
tion of the real world. Ideally, we want a dataset
of spontaneous facial expressions in challeng-ing real-world environments.
To address this problem, we have collected
two new facial-expression databases derivedfrom movies via a semiautomatic recommen-der-based method. We extracted a database of
temporal and static facial expressions from
scenes in movies, environments that moreclosely resemble the real world than those ofprevious datasets. The database contains videosshowing natural head poses and movements,close-to-real-world illumination, multiple sub-jects in the same frame, occlusions, and search-able metadata. The datasets also cover a large
age range, including toddler, child, and teen-
ager subjects, which are missing in other cur-rently available temporal facial-expressiondatabases.
Inspired by the Labeled Faces in the Wild
(LFW) database,
1we call our temporal database
Acted Facial Expressions in the Wild (AFEW)
and its static subset Static Facial Expressions in
the Wild (SFEW).2In this context, ‚Äò‚Äòin the
wild‚Äô‚Äô refers to the challenging conditions inwhich the facial expressions occur rather thanspontaneous facial expressions. We believethat these datasets will help advance facial-expression research and act as a benchmarkfor experimental validation of facial-expressionanalysis algorithms in real-world environments.
Constructing Facial-Expression Datasets
Until now, researchers have manually col-lected all facial-expression databases, which is
time consuming and error prone. To address
this limitation, we propose a video clip[3B2-9] mmu2012030034 .3d 12/7/012 10:42 Page 34
Large-Scale Multimedia Data Collections
Two large facial-
expression databasesdepicting
challenging real-
world conditionswere constructedusing a semi-automatic approachvia a recommendersystem based onsubtitles.
1070-986X/12/$31.00 /C14c2012 IEEE Published by the IEEE Computer Society 34
recommender system based on subtitle pars-
ing. Rather than manually scan a full movie,
our labelers reviewed only the video clips sug-
gested by the recommender system, whichsearched for clips with a high probability of asubject showing a meaningful expression.This method lets us collect and annotatelarge amounts of data quickly. Based on theavailability of detailed information regardingthe movies and their content on the Web,the labelers then annotated the video clipswith dense information about the subjects.
We used an XML-based representation for
the database metadata, which makes it search-able and easily accessible using any conven-tional programming language.
Over the past decade, researchers have devel-
oped robust facial-expression analysis methods,which along with their different databases have
followed various experimental protocols. This
severely limits the ability to objectively evalu-ate the different methods. In response, wehave defined clear experimental protocols,which represent different subject dependencyscenarios.
Given the huge amount of video data on the
Web, it is worthwhile to investigate the prob-
lem of facial-expression analysis in tough con-
ditions. For the AFEW dataset, we labeled thevideo clips with one of six basic expressions:anger, disgust, fear, happiness, sadness, sur-prise, or neutral. The database captures facialexpressions, natural head pose movements,occlusions, subjects‚Äô races, gender, diverseages, and multiple subjects in a scene. Our base-line results show that current facial-expression
recognition approaches that have reportedly
achieved high recognition rates on existingdatasets cannot cope with such realistic envi-ronments, underpinning the need for a newdatabase and further research.
Although movies are often shot in some-
what controlled environments, they are signifi-cantly closer to real-world environments than
current lab-recorded datasets. We do not
c l a i mt h a tA F E Wi sas p o n t a n e o u sf a c i a l - e x p r e s -sion database. However, clearly, method actorsattempt to mimic real-world human behaviorto give audiences the illusion that they arebehaving spontaneously, not posing, in mov-ies. The AFEW dataset, in particular, addressesthe issue of temporal facial expressions in diffi-cult conditions that are approximating real-world conditions, which provides for a muchmore difficult test set than currently availabledatasets.
Related Databases
One of the earliest databases published is thewidely used Cohn-Kanade database,
3which
contains 97 subjects who posed in a lab situa-tion for the six universal and neutral expres-sions. Its extension CK+ contains 123subjects, but the new videos were shot in asimilar environment.
3The Multi-PIE database
is another popular database that containsboth temporal and static samples recorded inthe lab over five sessions.
4It contains 337 sub-
jects covering different pose and illuminationscenes. Each of these databases were con-structed manually, with the subjects posingin sequential scenes. The MMI database is asearchable temporal database with 75 sub-jects.
6All of these are posed, lab-controlled en-
vironment databases. The subjects display
various acted (not spontaneous) expressions.
The recording environment is nowhere nearreal-world conditions.
The RU-FACS (Rutgers and University of Cal-
ifornia, San Diego, Facial Action Coding System[FACS]) database is a FACS -coded temporal
database containing spontaneous facial expres-sions,
6but it is proprietary and unavailable to
other researchers. The Belfast database consistsof a combination of studio recordings and TVprogram grabs labeled with particular expres-sions.
7The number of TV clips in this database
is sparse. Compared to the manual method
used to construct and annotate these data-
bases, our recommender system method isfaster and more easily accessible. The metadataschema is in XML and, hence, easily search-able and accessible from a variety of languagesand platforms. In contrast, CK, CK+, Multi-PIE, RU-FACS, and Bel fast must be searched
manually.
The Japanese Female Facial Expression
(JAFFE) database is one of the earliest static fa-cial-expression datasets.
8It contains 219 images
of 10 Japanese females. However, it has a lim-ited number of samples and subjects and was
also created in a lab-controlled environment.
In one of the first experiments on close-to-
real data, Marco Paleari, Ryad Chellali, and
Benoit Huet proposed a bimodal, audio-videofeatures-based system.
9The database was con-
structed from TV programs, but it is fairly
small, with only 107 clips.[3B2-9] mmu2012030034 .3d 12/7/012 10:42 Page 35July/C151September 2012
35
Table 1 compares thes e and other facial-
expression databases.
Our AFEW database is similar in spirit to the
LFW database1and the Hollywood Human
Actions (HOHA) dataset.15These contain varied
pose, illumination, age, gender, and occlusion.
However, LFW is a static facial-recognitiondatabase created from single face imagesfound on the Web specifically for face recogni-tion, and HOHA is an action-recognition data-base created from movies.
Database Contributions
The AFEW and SFEW databases offer severalnovel contributions to the state of the art.AFEW is a dynamic, temporal facial-expressiondata corpus consisting of short video clips offacial expressions in close-to-real-world envi-ronments. To the best of our knowledge,SFEW is also the only static, tough conditionsdatabase covering the seven facial-expressionclasses.
Our subjects ranged from 1 to 70 years old,
which makes the resulting datasets generic interms of age, unlike other facial-expressiondatabases. The databases have many clipsdepicting children and teenagers, which canbe used to study facial expressions in youngersubjects. The datasets can also be used forboth static and temporal facial age research.
To the best of our knowledge, AFEW is cur-
rently the only facial-expression database withmultiple labeled subjects in the same frame.This will enable interesting studies on variousthemes (expressions) involving scenes withmultiple subjects, who might or might nothave the same expression at a given time.
The databases also exhibit close-to-real illu-
mination conditions. The clips include sceneswith indoor, nighttime, and outdoor naturalillumination. Although movie studios use
controlled illumination conditions, even in
outdoor settings, these are closer to naturalconditions than lab-controlled environments
and, therefore, are valuable for facial-expression
research. The diverse nature of the illuminationconditions in the dataset makes it useful for not
just facial-expression analysis but potentially
also for facial recognition, facial alignment,age analysis, and action recognition.
T h em o v i e sw ec h o s ec o v e ral a r g es e to f
actors. Many actors appear in multiple moviesin the dataset, which will enable researchersto study how their expressions have evolved
over time, whether they differ for different gen-
res, and so forth.
The design of the database schema is based
on XML. This enables further informationabout the data and its subjects to be added eas-ily at any stage without changing the videoclips. This means that detailed annotations
with attributes about the subjects and the
scene are possible.
The database download website will also
contain information regarding the experimentprotocols and training and test splits for bothtemporal and static facial-expression recogni-
tion (FER) experiments.[3B2-9] mmu2012030034 .3d 12/7/012 10:42 Page 36
Table 1. Comparison of temporal facial expression databases.*
DatabaseConstruction
process EnvironmentAge
range Illumination Occlusion Subjects SearchableSubject
detailsMultiple
subjects
AFEW Assisted CTR 1 /C15170 CTN Yes 330 Yes Yes Yes
Belfast7Manual TV & Lab ? C Yes 100 No No No
CK3Manual Lab 18 /C15150 C No 97 No No No
CK+3Manual Lab 18 /C15150 C No 123 No No No
F.TUM9Manual Lab ? C No 18 No No No
GEMEP10Manual Lab ? C Yes 10 No No No
M-PIE4Manual Lab 27.9 C Yes 337 No No No
MMI5Manual Lab 19 /C15162 C Yes 29 Yes No No
Paleari11Manual CTR /C151 CTN Yes /C151 No No No
RU-FACS6Manual Lab 18 /C15130 C Yes 100 No No No
Semaine12Manual Lab ? C Yes 75 Yes No No
UT-Dallas13Manual Lab 18 /C15125 C Yes 284 No No No
VAM14Manual CTR ? C Yes 20 No No No
* C stands for controlled, CTN for close to natural, and CTR for close to real.IEEE MultiMedia
36
Database Creation
To construct the database, we followed a semi-
automatic approach and divided the processinto two parts (see Figure 1). First, the subtitlesare extracted and parsed in the recommendersystem. Second, a human labeler annotates
the recommended clips based on information
available on the Internet.
Subtitle Extraction
We purchased and analyzed 54 movie DVDs.We extracted subtitles for the deaf and hearingimpaired (SDH) and closed caption (CC) subti-
tles from the DVDs because they contain infor-
mation about the audio and nonaudio contextsuch as emotions and information about theactors and scene (for example, [CHEERING],[SHOUTS], and [SURPRISED]). We extractedthe subtitles from the movies using the Vob-Sub Rip (VSRip) tool (www.videohelp.com/
tools/VSRip). For the movies that VSRip
could not extract subtitles, we downloadedthe SDH from the Web. The extracted subtitleimages were parsed using optical character rec-ognition (OCR) and converted into the .srtsubtitle format using the Subtitle Edit tool(www.nikse.dk/se). The .srt format contains
the start time, end time, and textual content
with millisecond accuracy.
Video Recommender System
Once the subtitles have been extracted, weparse the subtitles and search for expression-related keywords‚Äîfor example, happy, sad,surprised, shouts, cries, groans, cheers, laughs,
s o b s ,s i l e n c e ,a n g r y ,w e e p i n g ,s o r r o w ,d i s a p -
point, and amazed. If fo und, the system rec-
ommends video clips to the labeler. Theclip‚Äôs start and end time is extracted from thesubtitle information. The system plays the
video clips sequentially, and the labeler enters
information about the clip and its charactersand actors from the Web. If clips contain mul-tiple actors, the labeling sequence is based ontwo criteria. For actors appearing in the same
frame, the order of annotation is left to right.
If the actors appear at different timestamps,then it is in the order of appearance. The dom-inating expression in the video is labeled asthe theme expression . The labeling is then
stored in an XML metadata schema. Finally,the labeler enters the character‚Äôs age or his orher estimated age if this information isunavailable.
In total, the subtitles from the 54 DVDs con-
tained 77,666 individual subtitles. Out of these,the recommender system suggested 10,327clips corresponding to subtitles containing ex-pressive keywords. The labelers chose 1,426
clips from these on the basis of criteria such
as the visible presence of subjects, at leastsome part of the face being visible, and the dis-play of meaningful expressions.
Because subtitles are manually created by
humans, they can contain errors. This mightlead to a situation where the recommender sys-tem suggests an erroneous clip. However, the
labelers can reject a recommendation. When[3B2-9] mmu2012030034 .3d 12/7/012 10:42 Page 37
Figure 1. Database creation process. A subtitle is extracted from a DVD and then parsed by the
recommender system. In this example, from the 2009 movie The Hangover, when the subtitle contains the
keyword ‚Äò‚Äòlaugh,‚Äô‚Äô the tool plays the corresponding clip. The human labeler then annotates the subjects inthe scene, using a GUI tool, based on the information about the subjects in the clip available on the Web.The resulting annotation, which in this case contains the information about a scene containing multiplesubjects, is stored in the XML schema shown at the bottom of the diagram.
Recommender system
Recommended clip
Labeler
Extract
closed-caption
 subtitlesJuly/C151September 2012
37
annotating the clips, the labelers use the clips‚Äô
video, audio, and subtitle information to
make informed decisions. We can use the pro-
posed recommender system to easily addmore clips to the database and scale it up inthe future.
Database Annotations
Our database contains metadata about thev i d e oc l i p si na nX M L - b a s e ds c h e m a ,w h i c h
enables efficient data handling and updating.
The human labelers densely annotated thevideo clips with the expression and subjectinformation.
The subject information contains various
attributes describing the actor and/or characterin the scene:
/C138Pose. This denotes the head pose based onthe labeler‚Äôs observation. In the current ver-sion, we manually classify the head pose asfrontal or nonfrontal.
/C138Character age . Frequently, only the age of the
lead actors‚Äô characters are available on theWeb. The labeler estimated any other ages.
/C138Actor name . Here we provide the actor‚Äôs real
name.
/C138Actor age . The labelers extracted the actor‚Äôs
real ages from www.imdb.com. In a few
cases, the age information was missing, so
the labeler estimated it.
/C138Expression of person .T h i sd e n o t e st h ee x p r e s -
sion class of the character as labeled by the
human observer. This could differ from thehigher-level expression tag because theremight be multiple people in the frame show-
ing different expressions with respect toeach other and the scene/theme.
/C138Gender . Here we provide the actor‚Äôs gender.
Expression tag specifies the theme expression
conveyed by the scene. The expressions weredivided into the six expression classes, plus neu-tral. The default value is based on the search
keyword found in the subtitle text‚Äîfor exam-
ple, we use happiness for ‚Äò‚Äòsmile‚Äô‚Äô and ‚Äò‚Äòcheer.‚Äô‚ÄôHuman observer can change it based on theirobservation of the audio and scene in the clip.
This XML-based metadata schema has two
major advantages. First, it is easy to use andsearch using any standard programming lan-guage on any platform that supports XML. Sec-ond, the structure makes it simple to add new
attributes about the video clips in the future,
such as the pose of the person in degrees andscene information, while keeping the existingdata and ensuring that pre-existing tools canexploit this information with minimal changes.
Currently, the database metadata indexes
1,426 video clips. Table 2 gives the databasedetails. Additional information on how to ob-tain the database and its experimental proto-
cols are available at http://cs.anu.edu.au/few.
SFEW
Static facial-expression analysis databases suchas Multi-PIE and JAFFE are lab-recorded data-bases in tightly controlled environments. Weextracted frames from AFEW to create a static
image database that more closely represents the
real world. Later, we describe the three versionsof SFEW, which are based on the level of subjectdependency for evaluating facial-expressionrecognition performance of systems in different
scenarios. The strictly person-independent ver-
sion of SFEW is described in an earlier work
4
and is posted as a challenge on the Benchmark-ing Facial Image Analysis Technologies (BEFIT)website (http://fipa.cs.kit.edu/511.php).
Comparison with Other Databases
To evaluate our datasets, we compared the per-
formance of state-of-the-art descriptors on
AFEW and SFEW with that on existing, widelyused datasets. Specifically, we compared AFEWto the CK+ database, which is an extension of
the Cohn-Kanade database.
3Ab a s i cf a c i a l[3B2-9] mmu2012030034 .3d 12/7/012 10:42 Page 38
Table 2. AFEW database attributes.
Attribute Description
Length of sequences 300 /C1515,400 ms
Number of sequences 1,426
Total number of expressions
(including multiple subjects)1,747
Video format AVI
Maximum number of clips of a subject 134Minimum number of clips of a subject 1Number of labelers 2
Number of subjects 330
Number of clips per expression Anger (194), disgust (123), fear (156),
sadness (165) happiness (387),
neutral (257), surprise (144)IEEE MultiMedia
38
expression consists of various temporal dy-
namic stages: onset, apex, and offset stage. In
CK+, all videos follow the temporal dynamic
sequence: neutral !onset!apex,w h i c hi s
not a true reflection of how expressions aredisplayed in real-world situations because thedata about the offset phase is missing.
We also argue that all data containing the
complete temporal sequence might not alwaysbe available. For example, a person entering ascene might already be happy and close to the
highest intensity of happiness (onset). Earlier
systems trained on existing databases like CK+have learned on such stages. However, theavailability of full temporal dynamic stages isnot guaranteed in real-world settings. In ourd a t a b a s e ,t h i si sn o tf i x e dd u et oi t sc l o s e - t o -natural settings. To extract a face, we computedthe Viola-Jones detector
16over the CK+ sequen-
ces. In our comparison experiments, we usedsix common classes from both the AFEW and
CK+ databases (anger, fear, disgust, happiness,
sadness, and surprise).
We compared SFEW with the JAFFE and
Multi-PIE databases in two experiments:
/C138a comparison of SFEW, JAFFE, and Multi-PIE
on the basis of four common expression
classes (disgust, neutral, happiness, and sur-prise) and
/C138a comparison of SFEW and JAFFE on allseven expression classes.
We computed feature descriptors on the
cropped faces from all the databases. The
cropped faces were divided into 4 /C24 blocks
for local binary pattern (LBP),
17local phase
quantization (LPQ),17and pyramid histogram
of gradients (PHOG).18For LBP and LPQ, we
set the neighborhood size to eight. For PHOG,
bin length was eight, pyramid levels Lwere 2,
and angle range equaled [0, 360]. We appliedprincipal component analysis (PCA) on theextracted features and kept 98 percent of thevariance. For classification, we used a support
vector machine (SVM) learned model. The ker-
nel was C-support vector classification (C-SVC),with a radial basis function (RBF) kernel. Weused five-fold cross validation to select theparameters. For AFEW, the static descriptorswere concatenated.
LBP-TOP performed the best out of all the
methods. The overall expression classification
accuracy is much higher for CK+ (see Figure 2).For the SFEW four-expression class experi-
ment, the classification accuracy on the Multi-
PIE subset was 86.25 and 88.25 percent forLPQ and PHOG, respectively. For JAFFE, it was83.33 percent for LPQ and 90.83 percent for
PHOG. For SFEW, it was 53.07 percent for
LPQ and 57.18 percent PHOG. For the seven-expression class experiment, the classificationaccuracy for JAFFE was 69.01 percent for LPQ
and 86.38 percent for PHOG. For SFEW, it was
43.71 percent for LPQ and 46.28 percent forPHOG. Thus, LPQ and PHOG achieve a high ac-curacy on JAFFE and Multi-PIE, but a signifi-
cantly lower accuracy for SFEW.
In our opinion, the primary reason for the
poor performance of state-of-the-art descriptorson AFEW and SFEW is that the databases on
which these state-of-the-art methods havebeen experimented on were recorded in lab-based environments. Expression analysis in
close-to-real-world sit u a t i o n si san o n t r i v i a l
task and requires more sophisticated methods
a ta l ls t a g e so ft h ea p p r o a c h ,s u c ha sr o b u s tface localization and tracking, illumination,
and pose invariance.
Experimentation Protocols
Over the years, researchers have proposedmany facial-expression recognition methods
based on experiments on various databases fol-lowing different protocols, making it difficultto compare the results fairly. Therefore, we cre-
ated strict experimentation protocols for both
databases. The different protocols are based onthe level of person dependency present in thesets (see Table 3).[3B2-9] mmu2012030034 .3d 12/7/012 10:42 Page 39
Figure 2. Performance of LBP, PHOG, LPQ, and LBP-TOP on the CK+ and
AFEW databases. The descriptors performed poorly on the AFEW dataset.80
010203040506070
LBP PHOG LPQ LBP-TOPCK + AFEWJuly/C151September 2012
39
The BEFIT workshop challenge2falls under
Strictly Person Indepe ndent (SPI) for SFEW.
Data, labels, and other protocols will be
made available on the database website.
AFEW Partial Person Independent (PPI) con-
tains 745 videos and AFEW SPI contains 741videos in two sets. AFEW Strictly Person Inde-pendent (SPI) contains 40 videos of the actorDaniel Radcliffe for four expression categories
(fear, happiness, neutral, and surprise). For
SFEW, SFEW SPS contains 76 images of DanielRadcliffe for five expression classes (anger,fear, happiness, neutral, and surprise). SFEWPPI contains 700 images and SFEW SPI con-tains 700 images in two sets.
Baseline
For all the protocols for SFEW, we computedthe baselines based on the method defined in
our earlier work.
2(These results are an average
of training and testing on the sets.) PHOG and
LPQ features were computed on the croppedface. The features were concatenated togetherto form a feature vector. For dimensionality re-duction, we computed PCA and kept 98 per-
cent of the variance. Furthermore, we used a
nonlinear SVM to learn and classify expres-sions. (Again, see our earlier work for the pa-rameter selection details.
2) To encode the
temporal data, we computed LBP-TOP fea-
tures, as in the previous section.
Table 4 shows the classification accuracy for
both databases and their protocols. The lowclassification accuracy results demonstrate
that the current methods are inappropriate for
real-world scenarios.
Conclusions
Facial-expression analysis is a well-researched
field. However, progress in the field has been
hampered due to the unavailability of data-bases depicting real-world conditions. State-
of-art FER methods that have performed well
on existing datasets do not work well on thedatasets we propose here. This is due to alack of robust ‚Äò‚Äòin the wild‚Äô‚Äô face-alignment
methods and efficient temporal descriptors.
As part of future work, we will adapt current
methods and extend algorithms for FER in
tough conditions. AFEW contains group-level-expression video clips, which in the future
can be used to develop systems that analyze
theme expressions in scenes containing groupsof people. We believe that these datasets will
enable novel contributions to facial-expression
research and act as a benchmark for experimen-tal validation of facial-expression analysis algo-rithms in real-world environments. MM
References
1. G.B. Huang et al., Labeled Faces in the Wild: A
Database for Studying Face Recognition in Uncon-
strained Environments, tech. report 07-49, Univ.
of Massachusetts, Amherst, 2007.
2. A. Dhall et al., ‚Äò‚ÄòStatic Facial Expression Analysis
in Tough Conditions: Data, Evaluation Protocoland Benchmark,‚Äô‚Äô Proc. IEEE Int‚Äôl Conf. Computer
Vision Workshop (BeFIT), IEEE Press, 2011,
pp. 2106 /C1512112.
3. P. Lucey et al., ‚Äò‚ÄòThe Extended Cohn-Kanade
Dataset (CK+): A Complete Dataset for Action
Unit and Emotion-Specified Expression,‚Äô‚Äô Proc.
IEEE Conf. Computer Vision and Pattern Recognition
Workshops (CVPR4HB), IEEE CS Press, 2010,
pp. 94 /C151101.[3B2-9] mmu2012030034 .3d 12/7/012 10:42 Page 40
Table 4. Average classification accuracies of different protocols.
Protocol Anger (%) Disgust (%) Fear (%) Happiness (%) Neutral (%) Sadness (%) Surprise (%) Average (%)
AFEW PPI 32.5 12.3 14.1 44.2 33.8 25.2 21.8 26.3
AFEW SPI 40.1 7.9 14.5 37.0 40.1 23.5 8.9 24.5
AFEW SPS /C151/C151 0.0 50.0 0.0 /C151 50.0 25.0
SFEW PPI 29.5 43.5 48.5 35.5 33.0 12.0 35.0 33.8
SFEW SPI 23.0 13.0 13.9 29.0 23.0 17.0 13.5 18.9
SFEW SPS 35.0 /C151 45.8 0.0 7.1 /C151 0.0 17.5Table 3. Experimentation protocol scenarios for SFEW and AFEW.
Protocol AFEW/SFEW training-test content
Strictly Person Specific (SPS) Same single subject
Partial Person Independent (PPI) A mix of common and different subjects
Strictly Person Independent (SPI) Different subjects2
40
4. R. Gross et al., ‚Äò‚ÄòMulti-PIE,‚Äô‚Äô Proc. 8th IEEE Int‚Äôl
Conf. Automatic Face and Gesture Recognition (FG),
2008, pp. 1 /C1518, doi:10.1109/AFGR.2008.4813399.
5. M. Pantic et al., ‚Äò‚ÄòWeb-Based Database for Facial
Expression Analysis,‚Äô‚Äô Proc. IEEE Int‚Äôl Conf. Multi-
media and Expo (ICME), IEEE CS Press, 2005,
pp. 317 /C151321.
6. M.S. Bartlett et al., ‚Äò‚ÄòAutomatic Recognition of Fa-
cial Actions in Spontaneous Expressions,‚Äô‚Äô J. Multi-
media, vol. 1, no. 6, 2006, pp. 22 /C15135.
7. E. Douglas-Cowie, R. Cowie, and M. Schro ¬®der,
‚Äò‚ÄòA New Emotion Database: Considerations,
Sources and Scope,‚Äô‚Äô Proc. ISCA ITRW on Speech
and Emotion, 2000, pp. 39 /C15144.
8. M.J. Lyons et al., ‚Äò‚ÄòCoding Facial Expressions with
Gabor Wavelets,‚Äô‚Äô Proc. IEEE Int‚Äôl Conf. Automatic
Face Gesture Recognition and Workshops (FG), IEEE
CS Press, 1998, p. 200.
9. F. Wallhoff, ‚Äò‚ÄòFacial Expressions and Emotion Data-
base,‚Äô‚Äô 2006, www.mmk.ei.tum.de/~waf/fgnet/
feedtum.html.
10. T. Ba ¬®nziger and K. Scherer, ‚Äò‚ÄòIntroducing the Geneva
Multimodal Emotion Portrayal (GEMEP) Corpus,‚Äô‚Äô
Blueprint for Affective Computing: A Sourcebook,
K. Scherer, T. Ba ¬®nziger, and E. Roesch, eds.,
Oxford Univ. Press, 2010.
11. M. Paleari, R. Chellali, and B. Huet, ‚Äò‚ÄòBimodal
Emotion Recognition,‚Äô‚Äô Proc. 2nd Int‚Äôl Conf. Social
Robotics (ICSR), Springer, 2010, pp. 305 /C151314.
12. G. McKeown et al., ‚Äò‚ÄòThe SEMAINE Corpus of
Emotionally Coloured Character Interactions,‚Äô‚ÄôProc. IEEE Int‚Äôl Conf. Multimedia and Expo (ICME),
IEEE CS Press, 2010, pp. 1079 /C1511084.
13. A.J. O‚ÄôToole et al., ‚Äò‚ÄòA Video Database of Moving
Faces and People,‚Äô‚Äô IEEE Trans. Pattern Analysis
and Machine Intelligence, vol. 27, no. 5, 2005,
pp. 812 /C151816.
14. M. Grimm, K. Kroschel, and S. Narayanan, ‚Äò‚ÄòThe
Vera am Mittag German Audio-Visual EmotionalSpeech Database,‚Äô‚Äô Proc. IEEE Int‚Äôl Conf. Multimedia
and Expo (ICME), IEEE CS Press, 2008, pp. 865 /C151868.
15. I. Laptev et al., ‚Äò‚ÄòLearning Realistic Human Actions
from Movies,‚Äô‚Äô Proc. IEEE Conf. Computer Vision
and Pattern Recognition (CVPR), IEEE CS Press,
2008, pp. 1 /C1518.
16. P .A. Viola and M.J. Jones, ‚Äò‚ÄòRapid Object Detection
Using a Boosted Cascade of Simple Features,‚Äô‚Äô
Proc. IEEE Conf. Computer Vision and Pattern Recog-
nition (CVPR), IEEE CS Press, 2001, pp. 511 /C151518.
17. D. Huang et al., ‚Äò‚ÄòLocal Binary Patterns and its Appli-
cation to Facial Image Analysis: A Survey,‚Äô‚Äô IEEE
Trans. Systems, Man, and Cybernetics, Part C: Applica-
tions and Reviews, vol. 41, no. 6, 2011, pp. 1 /C15117.18. A. Bosch, A. Zisserman, and X. Munoz, ‚Äò‚ÄòRepre-
senting Shape with a Spatial Pyramid Kernel,‚Äô‚Äô
Proc. 6th ACM Int‚Äôl Conf. Image and Video Retrieval(CIVR), ACM Press, 2007, pp. 401 /C151408.
Abhinav Dhall isa d
 octoral candidate in the Re-
search School of Computer Science at the AustralianNational University. His research interests include
affective computing, computer vision, pattern recog-
nition, and human-computer interaction. Dhall has aBS in computer science from the DAV Institute ofEngineering and Technology, India. He is a student
member of IEEE and was awarded the 2010 Australian
Leadership Award Scholarship. Contact him atabhinav.dhall@anu.edu.au.
Roland Goecke leads the Vision and Sensing Group
in the Faculty of Information Sciences and Engineer-
ing at the University of Canberra, Australia. His re-
search interests include affective computing,computer vision, human-computer interaction, and
multimodal signal processing. Goecke has a PhD in
computer science from the Australian National Uni-versity. He is a member of IEEE. Contact him atroland.goecke@ieee.org.
Simon Lucey is a Senior Research Scientist in the
CSIRO and a ""Futures Fellow Award"" recipient from
the Australian Research Council. He holds adjunctProfessorial positions at the University of Queenslandand, Queensland University of Technology. His re-
search interests include computer vision and ma-
chine learning and their application to humanbehavior. He is a member of IEEE and was awardedthe 2009 ARC Future Fellowship by the Australian Re-
search Council. Lucey has a PhD in computer science
from the Queensland University of Technology, Bris-bane. Contact him at simon.lucey@csiro.au.
Tom Gedeon is the chair professor of computer
science at the Australian National University and pres-
ident of the Computing Research and Education Asso-ciation of Australasia. His research interests includethe development of automated systems for informa-
tion extraction and synthesis into humanly useful in-
formation resources, mostly using fuzzy systems andneural networks. Gedeon has a PhD from the Univer-sity of Western Australia. He is a member of IEEE and
a former president of the Asia-Pacific Neural Network
Assembly. Contact him at tom@cs.anu.edu.au.[3B2-9] mmu2012030034 .3d 12/7/012 10:42 Page 41July/C151September 2012
41
"
https://ieeexplore.ieee.org/document/4804691,"IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS‚ÄîPART B: CYBERNETICS, VOL. 39, NO. 5, OCTOBER 2009 1217
Color Face Recognition for Degraded Face Images
Jae Young Choi, Yong Man Ro, Senior Member, IEEE , and Konstantinos N. (Kostas) Plataniotis, Senior Member, IEEE
Abstract ‚ÄîIn many current face-recognition (FR) applications,
such as video surveillance security and content annotation in aweb environment, low-resolution faces are commonly encounteredand negatively impact on reliable recognition performance. Inparticular, the recognition accuracy of current intensity-based FRsystems can signiÔ¨Åcantly drop off if the resolution of facial imagesis smaller than a certain level (e.g., less than 20 √ó20 pixels).
To cope with low-resolution faces, we demonstrate that facial colorcue can signiÔ¨Åcantly improve recognition performance comparedwith intensity-based features. The contribution of this paper istwofold. First, a new metric called ‚Äúvariation ratio gain‚Äù (VRG) isproposed to prove theoretically the signiÔ¨Åcance of color effect on
low-resolution faces within well-known subspace FR frameworks;
VRG quantitatively characterizes how color features affect therecognition performance with respect to changes in face resolu-tion. Second, we conduct extensive performance evaluation studiesto show the effectiveness of color on low-resolution faces. In partic-ular, more than 3000 color facial images of 341 subjects, which arecollected from three standard face databases, are used to performthe comparative studies of color effect on face resolutions to bepossibly confronted in real-world FR systems. The effectiveness ofcolor on low-resolution faces has successfully been tested on threerepresentative subspace FR methods, including the eigenfaces, theÔ¨Åsherfaces, and the Bayesian. Experimental results show that colorfeatures decrease the recognition error rate by at least an orderof magnitude over intensity-driven features when low-resolutionfaces (25 √ó25 pixels or less) are applied to three FR methods.
Index Terms ‚ÄîColor face recognition (FR), face resolution, iden-
tiÔ¨Åcation, variation ratio gain (VRG), veriÔ¨Åcation (VER), videosurveillance, web-based FR.
I. INTRODUCTION
FACE recognition (FR) is becoming popular in research and
is being revisited to satisfy increasing demands for video
surveillance security [1]‚Äì[3], annotation of faces on multimedia
contents [4]‚Äì[7] (e.g., personal photos and video clips) in webenvironments, and biometric-based authentication [58]. Despite
the recent growth, precise FR is still a challenging task due to
ill-conditioned face capturing conditions, such as illumination,
Manuscript received September 14, 2008; revised December 28, 2008. First
published March 24, 2009; current version published September 16, 2009. The
work of J. Y . Choi and Y . M. Ro was supported by the Korean Government
under Korea Research Foundation Grant KRF-2008-313-D01004. The work ofK. N. Plataniotis was supported in part by the Natural Science and Engineering
Research Council of Canada under the Strategic Grant BUSNet. This paper was
recommended by Associate Editor J. Su.
J. Y . Choi and Y . M. Ro are with the Image and Video System Laboratory,
Korea Advanced Institute of Science and Technology (KAIST), Daejeon305-732, Korea (e-mail: jygchoi@kaist.ac.kr; ymro@ee.kaist.ac.kr).
K. N. Plataniotis is with the Edward S. Rogers, Sr. Department of Electrical
and Computer Engineering, University of Toronto, Toronto, ON M5S 3G4,
Canada, and also with the School of Computer Science, Ryerson University,Toronto, ON M5B 2K3, Canada (e-mail: kostas@comm.toronto.edu).
Color versions of one or more of the Ô¨Ågures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object IdentiÔ¨Åer 10.1109/TSMCB.2009.2014245pose, aging, and resolution variations between facial images
being of the same subject [8]‚Äì[10]. In particular, many current
FR-based applications (e.g., video-based FR) are commonly
confronted with much-lower-resolution faces (20√ó20 pixels
or less) and suffer largely from them [2], [11], [12]. Fig. 1
shows practical cases in which the faces to be identiÔ¨Åed or
annotated have very small resolutions due to limited acquisitionconditions, e.g., faces captured from long distance closed-
circuit television (CCTV) cameras or camera phones. As can
be seen in Fig. 1(a) and (b), the faces enclosed in red boxeshave much lower resolution and additional blurring, which
often lead to unacceptable performance in the current grayscale
(or intensity)-based FR frameworks [13]‚Äì[18].
In the practical FR applications, which frequently encounter
low-resolution faces , it is of utmost importance to select face
features that are robust against severe variations in face resolu-
tion and to make efÔ¨Åcient use of these features. In contrast to the
intensity-driven features, color-based features are known to beless susceptible to resolution changes for objection recognition
[20]. In particular, the psychophysical results of the FR test in
human visual systems showed that the contribution of facialcolor becomes evident when the shapes of faces are getting
degraded [21]. Recently, considerable research effort has been
devoted to the efÔ¨Åcient utilization of facial color information toimprove the recognition performance [22]‚Äì[29]. For the color
FR reported so far, questions could be categorized as follows:
1) Was color information helpful in improving the recogni-tion accuracy compared with using grayscale only [22]‚Äì[29];
2) how were three different spectral channels of face images
incorporated to take advantages of face color characteristics
[22], [24], [25], [28], [29]; and 3) which color space was the
best for providing discriminate power needed to perform thereliable classiÔ¨Åcation tasks [22], [25], [26]? To our knowledge,
however, the color effect on face resolution has not yet been
rigorously investigated in the current color-based FR works,and no systematic work suggests the effective color FR frame-
work robust against much-lower-resolution faces in terms of
recognition performance.
In this paper, we carry out extensive and systematic studies
to explore the facial color effect on the recognition performance
as the face resolution is signiÔ¨Åcantly changed. In particular, wedemonstrate the signiÔ¨Åcant impact of color on low-resolution
faces by comparing the performance between grayscale and
color features. The novelty of this paper comes from thefollowing.
1) The derivation of a new metric, which is the so-called
variation ratio gain (VRG), for providing the theoreti-
cal foundation to prove the signiÔ¨Åcance of color effecton low-resolution faces. Theoretical analysis was made
within subspace-based FR methods, which is currently
1083-4419/$25.00 ¬© 2009 IEEE
1218 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS‚ÄîPART B: CYBERNETICS, VOL. 39, NO. 5, OCTOBER 2009
Fig. 1. Practical illustrations of extremely small-sized faces in FR-based applications. (a) Surveillance video frame from ‚ÄúWashington Dulles Int ernational
Airport.‚Äù The two face regions occupy approximately 18 √ó18 pixels of the video frame shown, having an original resolution of 410 √ó258 pixels. (b) Personal
photo from a ‚ÄúFlickr‚Äù [19] web site. The face region occupies about 14 √ó14 pixels in the picture shown, having an original resolution of 500 √ó333 pixels.
one of the most popular FR techniques [30], [31] due to
reliability in performance and simplicity in implementa-tion. VRG quantitatively characterizes how color features
affect recognition performance with respect to changes in
face resolution.
2) Extensive and comparative recognition performance eval-
uation experiments to show the effectiveness of color on
low-resolution faces. In particular, 3192 frontal facial im-ages corresponding to 341 subjects collected from three
public data sets of the Carnegie Mellon University Pose,
Illumination, and Expression (CMU PIE) [32], Facial
Recognition Technology (FERET) [33], and the Extended
Multimodal VeriÔ¨Åcation for Teleservices and SecurityApplications Database (XM2VTSDB) [34] were used to
demonstrate the contribution of color to improved recog-
nition accuracy over various face resolutions commonlyencountered from still-image- to video-based real-world
FR systems. In addition, the effectiveness of color has
successfully been tested on three representative subspaceFR methods‚Äîprincipal component analysis [35] (PCA or
‚Äúeigenfaces‚Äù), linear discriminant analysis [8], [36] (LDA
or ‚ÄúÔ¨Åsherfaces‚Äù), and Bayesian [37] (or ‚Äúprobabilisticeigenspace‚Äù). According to experimental results, the ef-
fective use of color features drastically reduces the lower
bound of face resolution to be reliably recognizable inthe computer FR beyond what is possible with intensity-
based features.
The rest of this paper is organized as follows. The next sec-
tion provides background about the low-resolution-face prob-
lem in the current FR works. Section III introduces the proposed
color FR framework. In Section IV, we Ô¨Årst deÔ¨Åne variationratio and then make a theoretical analysis to explain the effect
of color on variation ratio. In Section V, based on an analysis
made in Section IV, VRG is proposed to provide a theoreticalinsight on the relationship between color effect and face resolu-
tions. Section VI presents the results of extensive experiments
performed to demonstrate the effectiveness of color on low-resolution faces. The conclusion is drawn in Section VI.
II. R
ELATED WORKS
In the state-of-the-art FR research, a few works dealt with
face-resolution issues. The main concern in these works would
be summarized as follows: 1) what is the minimum face reso-lution to be potentially encountered with the practical applica-
tions and to be detectable and recognizable in the computer FRsystems [2], [13], [14], [38]‚Äì[40] and 2) how do low-resolution
faces affect the detection or recognition performances
[15]‚Äì[17], [40]. In the cutting-edge FR survey literature [2],15√ó15 pixels is considered to be the minimum face resolution
for supporting reliable detection and recognition. The CHIL
project [14] reported that normal face resolution in video-basedFR is from 10 to 20 pixels in the eye distance. Furthermore, they
indicated that the face region is usually 1/16th of commonly
used TV recording video frames of resolutions of 320 √ó
240 pixels. Furthermore, the FR vendor test (FRVT) 2000
[12] studied the effect of face resolution on the recognitionperformance until the eye distance on the face is as low as 5 √ó
5 pixels. In the research Ô¨Åelds of face detection, 6 √ó6p i x e l so f
faces has been reported so far to be the lowest resolution that isfeasible for automatic detection [40]. Furthermore, the authors
of [39] proposed the face detection algorithm that supports
acceptable detection accuracy, even until 11 √ó11 pixels.
Several previous works also examined how low-resolution
faces impact on recognition performance [15]‚Äì[17]. Their
works were carried out through intensity-based FR frameworks.They reported that much-lower-resolution faces signiÔ¨Åcantly
degrade recognition performance in comparison with higher-
resolution ones. In [15], face registration and recognitionperformances were investigated with various face resolutions
ranging from 128 √ó128 to 8 √ó8 pixels. They revealed that
face resolutions below 32 √ó32 pixels show a considerable
decreased recognition performance in PCA and LDA. In [16],
face resolutions of 20 √ó20 and 10 √ó10 pixels dramatically
deteriorated recognition performance compared with 40 √ó
40 pixels in video-based FR systems. Furthermore, the author of
[17] reported that the accuracy for face expression recognitionis dropped off below 36 √ó48 pixels in the neural-network-
based recognizer.
Obviously, low-resolution faces impose a signiÔ¨Åcant restric-
tion on current intensity-based FR applications to accomplish
reliability and feasibility . To handle low-resolution-face prob-
lems, resolution-enhancement techniques such as ‚Äúsuperresolu-tion‚Äù [18], [41], [42] are traditional solutions. These techniques
usually estimate high-resolution facial images from several
low-resolution ones. One critical disadvantage, however, is thatthe applicability of these techniques is limited to restricted FR
domain. This is because they require a sufÔ¨Åcient number of
CHOI et al. : COLOR FACE RECOGNITION FOR DEGRADED FACE IMAGES 1219
multiple low-resolution images captured from the same identity
for the reliable estimation of high-resolution faces. In practice,
it is difÔ¨Åcult to always support such requirement in practical
applications (e.g., the annotation of low-resolution faces on
personal photos or snapshot Web images). Another drawbackto these approaches is the requirement of a complex framework
for the estimation of an image degradation model. It is also
computationally demanding for the reconstruction of a high-resolution face image. In this paper, we propose an effective
and simple method of using face color features to overcome the
low-resolution-face problem. The proposed color FR methodimproves degraded recognition accuracy, which is caused by
low-resolution faces, by a signiÔ¨Åcant margin compared to con-
ventional intensity-based FR frameworks. In addition, contraryto previous resolution-enhancement algorithms, our approach is
not only simple in implementation but also guarantees extended
applicability to FR applications where only a single colorimage with a low resolution is available during actual testing
operations.
III. C
OLOR FR F RAMEWORK
In this section, we formulate the baseline color FR frame-
work [20] that can make efÔ¨Åcient use of facial color features toovercome low-resolution faces. Red‚Äìgreen‚Äìblue (RGB) color
face images are Ô¨Årst converted into another different color
space (e.g., YC
bCrcolor space). Let Ibe a color face image
generated in the color space conversion process. Then, let sm
be anmth spectral component vector of I(in the form of a col-
umn vector by lexicographic ordering of the pixel elements of2-D spectral images), where s
m‚ààRNmandRNmdenotes an
Nm-dimensional real space. Then, the face vector is deÔ¨Åned
as the augmentation (or combination) of each spectral compo-nents
msuch that x=[sT
1sT2¬∑¬∑¬∑sT
K]T, where x‚ààRN,
N=/summationtextK
m=1Nm, andTrepresents the transpose operator of
the matrix. Note that each smshould be normalized to zero
mean and unit variance prior to their augmentation. Face vector
xcan be generalized in that, for K=1, the face vector could be
deÔ¨Åned by grayscale only, while for K=3, it could be deÔ¨Åned
by a spectral component conÔ¨Åguration like YCbCrorYQ C r
by column order from YCbCrandYIQ color spaces.
Most subspace FR methods are separately divided into the
training and testing stages. Given a set {Ii}M
i=1ofMcolor
face images, Iishould be Ô¨Årst rescaled into the prototype
template size to be used for the creation of a corresponding facevector x
i. With a formed training set {xi}M
i=1ofMface vector
samples, the feature subspace is trained and constructed. The
rationale behind the feature subspace construction is to Ô¨Ånd aprojection matrix Œ¶=[ e
1e2¬∑¬∑¬∑eF]by optimizing crite-
ria to get a lower dimensional feature representation f=Œ¶Tx,
where each column vector eiis a basis vector spanning the
feature subspace Œ¶‚ààRN√óF, andf‚ààRF. It should be noted
thatF/lessmuchN. For the testing phase, let {gi}G
i=1b eag a l l e r y
(or target) set consisting of Gprototype enrolled face vectors of
known individuals, where gi‚ààRN. In addition, let pbe an un-
known face vector to be identiÔ¨Åed or veriÔ¨Åed, which is denoted
as a probe (or query), where p‚ààRN. To perform FR tasks
on the probe, gi(i=1,...,G )andpare projected onto thefeature subspace to get corresponding feature representations
such that
fgi=Œ¶Tgi,f p=Œ¶Tp (1)
where fgi‚ààRFandfp‚ààRF. A nearest-neighbor classiÔ¨Åer
is then applied to determine the identity of pby Ô¨Ånding the
smallest distance between fgi(i=1,...,G )andfpin the
feature subspace as follows:
/lscript(p)=/lscript(gi‚àó),i‚àó=a r gG
min
i=1/bardblfgi‚àífp/bardbl (2)
where /lscript(¬∑)returns a class label of face vectors, and /bardbl¬∑/bardbl denotes
the distance metric. To exploit why the role of color is getting
signiÔ¨Åcant as face resolution is decreased within our baseline
color FR framework, a theoretical analysis will be given in thefollowing sections.
IV . A
NALYSIS OF COLOR EFFECT AND FACE RESOLUTION
Wang and Tang [43] proposed a face difference model that
establishes a uniÔ¨Åed framework of PCA, LDA, and Bayesian
FR methods. Based on this model, intra- and extrapersonal
variations of feature subspace are critical factors in determiningthe recognition performance in the three methods. These two
parameters are quantitatively well represented by the variation
ratio proposed in [44]. Before exploiting the color effect onthe recognition performance with respect to changes in face
resolution, we begin by introducing the variation ratio and
explore how chromaticity components affect the variation ratio
within our color FR framework.
1
A. Variation Ratio
In PCA, covariance matrix Ccan be computed by using the
differences between all possible pairs of two face vectors [43]
included in {xi}M
i=1such that
C=M/summationdisplay
i=1M/summationdisplay
j=1(xi‚àíxj)(xi‚àíxj)T. (3)
Then, Cis decomposed into intra- (or within) and extrapersonal
(or between class) covariance matrices [43], which are denoted
as IC and EC, respectively. IC and EC are deÔ¨Åned as
IC=/summationdisplay
l(xi)=l(xj)(xi‚àíxj)(xi‚àíxj)T
EC=/summationdisplay
l(xi)/negationslash=l(xj)(xi‚àíxj)(xi‚àíxj)T(4)
where l(¬∑)is a function that returns a class label of xias input.
As pointed out in [43], the total variation that resides in
the feature subspace is divided into intra- and extrapersonal
1In this paper, a theoretical analysis in only the PCA-based color FR
framework is given. Our analysis, however, is readily applied to LDA andBayesian due to the same intrinsic connection of intra- and extrapersonal
variations described in [43].
1220 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS‚ÄîPART B: CYBERNETICS, VOL. 39, NO. 5, OCTOBER 2009
variations related to IC and EC, respectively. From a classiÔ¨Åca-
tion point of view, it is evident that the recognition performance
is enhanced as the constructed feature subspace learns and
contains a larger variation of EC than that of IC. From this
principle, the ratio of extra- to intrapersonal variations can beadopted as an important parameter that reÔ¨Çects the discrimina-
tive power of feature space [45]. To deÔ¨Åne variation ratio, Ô¨Årst,
letŒ¶be an eigenvector matrix of C, and then, let Var
Œ¶(IC) and
VarŒ¶(EC) be intra- and extrapersonal variations of the feature
subspace spanned by Œ¶, which are computed as [44]
VarŒ¶(IC)=tr(Œ¶TICŒ¶),VarŒ¶(EC)=tr(Œ¶TECŒ¶) (5)
where tr(¬∑)is a trace operator of the matrix. Using (5), the
variation ratio (J)is deÔ¨Åned as
J=VarŒ¶(EC)
VarŒ¶(IC). (6)
AsJincreases, a trained feature subspace relatively includes a
larger variation of EC in comparison to that of IC. Therefore,
Jrepresents a well-discriminative capability of the feature
subspace for classiÔ¨Åcation tasks. In (6), the formulation of
VarŒ¶(IC) and VarŒ¶(EC) is similar to that of the J-statistic
[55] used in the Ô¨Åeld of economics. However, it should be
pointed out that the metric is used in a novel and quite different
way. In particular, the J-statistic has been used as a criterion
function to determine the optimal unknown parameter vectors
[55], while VarŒ¶(IC) and VarŒ¶(EC) are used to represent
quantitatively the discriminative ‚Äúeffectiveness‚Äù of the featuresubspace spanned by Œ¶.
B. Intra- and Extrapersonal Variations in Color FR
In the following section, without loss of generality, we
assume that the ith face vector x
iis a conÔ¨Åguration of one
luminance (si1)and two different chromaticity components
(si2andsi3) so that xi=[sT
i1sT
i2sT
i3]T. By substituting
[sT
i1sT
i2sT
i3]Tintoxiin (3), Cis written as
C=‚é°
‚é£C11C12C13
C21C22C23
C31C32C33‚é§
‚é¶ (7)
where Cmn=/summationtextM
i=1/summationtextM
j=1(sim‚àísjm)(sin‚àísjn)T, andm,
n=1,2,3. As shown in (7), Cis a block covariance
matrix whose entries are partitioned into covariance or cross-
covariance submatrices Cmn.F o rm=n,Cmnis a covariance
submatrix computed from a set {sim}M
i=1; otherwise, for m/negationslash=n,
Cmn is a cross-covariance submatrix computed between
{sim}M
i=1and{sin}M
i=1, where Cmn=CT
nm.F r o m( 4 ) ,t h eI C
and EC decompositions of Cshown in (7) are represented as
IC=‚é°
‚é£IC11IC12IC13
IC21IC22IC23
IC31IC32IC33‚é§
‚é¶
EC=‚é°
‚é£EC11EC12EC13
EC21EC22EC23
EC31EC32EC33‚é§
‚é¶ (8)where IC mnand EC mnare
ICmn=/summationdisplay
l(xi)=l(xj)(sim‚àísjm)(sin‚àísjn)T
ECmn=/summationdisplay
l(xi)/negationslash=l(xj)(sim‚àísjm)(sin‚àísjn)T. (9)
LikeC, IC and EC are also block covariance matrices.
To explore the color effect on variation ratio, we analyze
how IC mnand EC mn, which are computed from two different
chromaticity components of smandsn(m,n=2,3), impact
on the construction of variations of IC and EC in (8). By the
proof given in the Appendix, trace values of IC and EC can bewritten as
tr(IC)= 3/summationdisplay
m=1tr(IŒõmm),tr(EC)=3/summationdisplay
m=1tr(EŒõmm)(10)
where IŒõmm andEŒõmm are diagonal eigenvalue matrices
of IC mm and EC mm, respectively. Using (5) and the cyclic
property of the trace operator, the variations of IC and EC arecomputed as
Var
Œ¶(IC) = tr(Œ¶Œ¶TIC)=t r ( IC)
VarŒ¶(EC) = tr(Œ¶Œ¶TEC)=t r ( EC) (11)
where Œ¶is an eigenvector matrix of CdeÔ¨Åned in (7).
Furthermore, using (5) and the diagonalization of a matrix, the
variations of IC mmand EC mmare computed as
VarŒ¶mm(ICmm)=t r ( IŒõmm)
VarŒ¶mm(ECmm)=t r ( EŒõmm) (12)
where Œ¶mmis an eigenvector matrix of Cmm, andm=1,2,3.
It should be noted that, in case of m=1,VarŒ¶11(IC11)and
VarŒ¶11(EC11)denote intra- and extrapersonal variations cal-
culated from a luminance component of the face vector, while
others (m=2,3)are corresponding variations computed from
two different chromaticity components.
Substituting (11) and (12) into (10), intra- and extraper-
sonal variations of the feature subspace spanned by Œ¶can be
represented as
VarŒ¶(IC)=3/summationdisplay
m=1VarŒ¶mm(ICmm)
VarŒ¶(EC)=3/summationdisplay
m=1VarŒ¶mm(ECmm). (13)
From (13), we can see that the variation of IC and EC is
equal to the summation of the variations of the respective
diagonal submatrices of IC mmand EC mm, respectively. This
means that VarŒ¶(IC) and VarŒ¶(EC) are partially decom-
posed into three independent portions of VarŒ¶mm(ICmm)and
VarŒ¶mm(ECmm), where m=1,2,3. This confers an impor-
tant implication about the effect of color on the variation ratio in
the color-based FR. Two different chromaticity components can
make an independent contribution to construct the intra- and
extrapersonal variations in a separate manner with luminance.
CHOI et al. : COLOR FACE RECOGNITION FOR DEGRADED FACE IMAGES 1221
Aside from the independent contribution, since each spectral
component of skin-tone color has its own inherent character-
istics [38], [46], [47], VarŒ¶mm(ICmm)andVarŒ¶mm(ECmm)
may differently be changed by practical facial imaging con-
ditions, e.g., illumination and spatial-resolution variations. Asa result, intra- and extrapersonal variations in the color-based
FR are formed by the composition of variations computed
from each spectral component along with different imagingconditions. On the contrary, in the traditional grayscale-based
subspace FR, the distribution of the intra- and extrapersonal
variations (denoted as Var
Œ¶11(IC11)andVarŒ¶11(EC11))i nt h e
feature subspace spanned by Œ¶11is entirely governed by the
statistical characteristic of only the luminance component.
C. Color Boosting Effect on Variation Ratio Along With
Face Resolution
Now, we make an analysis of the color effect on variation
ratio with respect to changes in face resolutions. Our analysisis based on the following two observations: 1) As proven in
Section IV-B, each spectral component can contribute in an
independent way to construct the intra- and extrapersonal vari-ations of the feature subspace in color-based FR; as described
in [54] and [58], such independent impact on evidence fusion
usually facilitates a complementary effect between different
components for recognition purposes, and 2) the robustness of
the color features against variation in terms of face resolution;previous research [20], [48], [49] revealed that chromatic con-
trast sensitivity is mostly concentrated on low-spatial frequency
regions compared to luminance; this means that intrinsic fea-tures of face color are even less susceptible to a decrease
or variation of the spatial resolution. Considering these two
observations, it is reasonable to infer that two chromaticitycomponents can play a supplement role in boosting the de-
creased variation ratio caused by the loss in the discriminative
power of the luminance component arising from low-resolutionface images.
To quantize the color boosting effect on variation ratio over
changes in the face resolution, we will now derive a simplemetric, which is called variation ratio grain (VRG). Using
(6) and (12), the variation ratio, which is parameterized by
face resolution (Œ≥), for an intensity-based feature subspace is
deÔ¨Åned as
J
lum(Œ≥)=VarŒ¶11(Œ≥)(EC11(Œ≥))
VarŒ¶11(Œ≥)(IC11(Œ≥)). (14)
It should be noted that all terms in (14) are obtained from
a training set of intensity facial images having resolution Œ≥.
On the other hand, using (13), the variation ratio for a color-augmentation-based feature subspace is deÔ¨Åned as
J
lum+chrom(Œ≥)=VarŒ¶(Œ≥)(EC(Œ≥))
VarŒ¶(Œ≥)(IC(Œ≥))
=3/summationtext
m=1VarŒ¶mm(Œ≥)(ECmm(Œ≥))
3/summationtext
m=1VarŒ¶mm(Œ≥)(ICmm(Œ≥)).(15)
Fig. 2. Average variation ratios and the corresponding standard deviations
with respect to six different face-resolution parameters Œ≥. Note that the margin
between curves of Jlum(Œ≥)andJlum+chrom(Œ≥)represents the numerator of
VRG deÔ¨Åned as in (16).
Finally, a VRG having input argument Œ≥is deÔ¨Åned as
VR G(Œ≥)=Jlum+chrom(Œ≥)‚àíJlum(Œ≥)
Jlum(Œ≥)√ó100. (16)
VR G(Œ≥)measures the relative amount of variation ratio in-
creased by chromaticity components compared to that from
only luminance at face resolution Œ≥. Therefore, it reÔ¨Çects well
the degree of the effect of color information on the improved
recognition performance with respect to changes in Œ≥.
To validate the effectiveness of VRG as a relevant metric
for the purpose of quantization of the color effect along with
variations in face resolutions, we conducted an experiment
using three standard color face DBs of CMU PIE, FERET,and XM2VTSDB. A total of 5000 facial images were collected
from three data sets and were manually cropped using the eye
position provided by ground truth. Each cropped facial image
was Ô¨Årst rescaled to a relatively high resolution of 112 √ó
112 pixels. To simulate the effect of lowering the face resolutionfrom different distances to the camera, the 5000 facial images
with 112 √ó112 pixels were Ô¨Årst blurred and then subsequently
downsampled by Ô¨Åve different factors to produce Ô¨Åve differentlower-resolution facial images [18]. For blurring, we used a
point spread function, which was set to a 5 √ó5 normalized
Gaussian kernel with zero mean and a standard deviation ofone pixel. After the blurring and downsampling processing,
we obtained six sets, each of which consisted of 5000 facial
images with six different face resolutions: 112 √ó112, 86 √ó86,
44√ó44, 25 √ó25, 20 √ó20, and 15 √ó15 pixels (see Fig. 2).
We calculated J
lum(Œ≥)andJlum+chrom(Œ≥)in (16) over six dif-
ferent face resolution Œ≥parameters. For this, 500 facial images
were randomly selected from each set and then used to compute
variation ratios by using (14) and (15). The selection process
was repeated 20 times so that the variation ratios computed herewere the averages of 20 random selections. For luminance and
chromaticity components, the YC
bCrcolor space was adopted
since it has been widely used in image (JPEG) and video
(MPEG) compression standards.
1222 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS‚ÄîPART B: CYBERNETICS, VOL. 39, NO. 5, OCTOBER 2009
Experimental results are shown in Fig. 2. In Fig. 2, Jlum(Œ≥)
denotes the average variation ratio calculated from luminance
face images with resolution Œ≥, i.e., the Yplane from the YCbCr
color space. Furthermore, Jlum+chrom(Œ≥)denotes the average
variation ratio computed from YCbCrcomponent conÔ¨Ågura-
tion samples. To guarantee the stability of measured variation
ratios, the standard deviations for all cases of Jlum(Œ≥)and
Jlum+chrom(Œ≥)are shown in Fig. 2 as well. As can be seen in
Fig. 2, at a high resolution (above 44 √ó44 pixels), the margin
between Jlum(Œ≥)andJlum+chrom(Œ≥)is relatively small. This is
because the luminance component is even more dominant thantwo chromaticity components in determining J
lum+chrom(Œ≥).
However, we can observe that Jlum(Œ≥)noticeably falls off at
low resolution (25 √ó25 pixels or less) compared to those com-
puted from high-resolution faces (above 44 √ó44 pixels). On the
other hand, Jlum+chrom(Œ≥)has a slower decay compared with
Jlum(Œ≥)even as the face resolution becomes much lower. In
particular, when the face resolution Œ≥is below 25 √ó25 pixels,
the difference between Jlum(Œ≥)andJlum+chrom(Œ≥)is much
larger compared to cases of face resolutions above 44 √ó
44 pixels. This result is mostly due to the fact that lumi-
nance contrast sensitivity drops off at low spatial frequenciesmuch faster than chromatic contrast sensitivity. Hence, two
chromaticity components in (15) can compensate a decreased
extrapersonal variation caused by luminance faces with lowresolution.
V. E
XPERIMENTS
In the practical FR systems, there are two possible FR
approaches to perform FR tasks over lower-resolution probeimages [13]. The Ô¨Årst method is to prepare multiple training
sets of multiresolution facial images and then construct multiple
feature subspaces, each of which is charged with a particularface resolution of a probe. An alternative method is that a
lower-resolution probe is reconstructed to be matched with the
prototype resolution of training and gallery facial images byadopting resolution-enhancement or interpolation techniques.
The second method would be appropriate in typical surveillance
FR applications in which high-quality training and galleryimages are usually employed, but probe images transmitted
from surveillance cameras (e.g., CCTV) are often at a low
resolution. To demonstrate the effect of color on low-resolutionfaces in both FR scenarios, two sets of experiments have been
carried out in our experimentation. The Ô¨Årst experiment is to
assess the impact of color on recognition performance with
varying face resolutions of probe-given multiresolution trained
feature subspaces. On the other hand, the second experimentis to conduct the same assessment when a single-resolution
feature subspace trained with high-resolution facial images is
only available to the actual testing operation.
A. Face DB for the Experiment and FR Evaluation Protocol
Three de facto standard data sets of CMU PIE, Color FERET,
and XM2VTSDB have been used to perform the experiments.
The CMU PIE [32] includes 41 368 color images of 68 sub-
jects (21 samples/subject). Among them, 3805 images have
the coordinate information of facial feature points. From these
Fig. 3. (a) Examples of facial images from CMU PIE. These images have
illumination variations with ‚Äúroom lighting on‚Äù conditions. (b) Examples of
facial images from FERET. The Ô¨Årst and second rows show image examples offaandfbsets. (c) Examples of facial images from XM2VTSDB. Note that the
facial images in each column belong to the same subject, and all facial images
are manually cropped using eye coordinate information. Each cropped facial
image is rescaled to the size of 112 √ó112 pixels.
3805 images, 1428 frontal-view facial images with neutral ex-
pression and illumination variations were selected in our exper-
imentation. For one subject, 21 facial images have 21 differentillumination variations with ‚Äúroom lighting on‚Äù conditions. The
Color FERET [33] consists of 11 388 facial images correspond-
ing to 994 subjects. Since the facial images are captured overthe course of 15 sessions, there are pose, expression, illumina-
tion, and resolution variations for one subject. To support the
evaluation of recognition performance in various FR scenarios,the Color FERET is to be divided into Ô¨Åve sets: ‚Äú fa,‚Äù ‚Äúfb,‚Äù ‚Äúfc,‚Äù
‚Äúdup1 ,‚Äù and ‚Äú dup2‚Äù partitions [33]. The XM2VTSDB [34] is
designed to test realistic and challenging FR with four sessionsrecorded with no control on severe illumination variations. It
is composed of facial images taken on digital video recordings
from 295 subjects over a period of one month. Fig. 3 showsexamples of facial images selected from three DBs. All facial
images shown in Fig. 3 were manually cropped from original
images using the eye position provided by a ground truth set.
To construct the training and probe (or test) sets in both sets
of experiments, a total of 3192 facial images from 341 subjects
were collected from three public data sets. During the collection
phase, 1428 frontal-view images of 68 subjects were selected
CHOI et al. : COLOR FACE RECOGNITION FOR DEGRADED FACE IMAGES 1223
Fig. 4. Examples of facial images from color FERET according to six different face resolutions. A low-resolution observation below the original 112 √ó112
pixels is interpolated using nearest-neighbor interpolation.
from CMU PIE; for one subject, facial images had 21 different
lighting variations. From the Color FERET, the 700 frontal-view images of 140 subjects (5 samples/subject) were cho-
sen from the fa,fb,fc, and dup1 sets. From XM2VTSDB,
1064 frontal-view images of 133 subjects were obtained fromtwo different sessions; each subject included eight facial images
that contained illumination and resolution variations. Further-
more, we constructed a gallery set composed of 341 differentsamples corresponding to 341 different subjects to be identiÔ¨Åed
or veriÔ¨Åed. Note that, here, gallery images had neutral illumi-
nation and expression according to the standard regulation forgallery registration described in [59].
To acquire facial images with varying face resolutions, we
carried out resizing over the original collected DB sets. Fig. 4shows examples of facial images containing face-resolution
variations used in our experiments. We took original high-
resolution images of faces (shown in the leftmost image of
Fig. 4), synthetically blurred them with a Gaussian kernel
[41], and then downsampled them so as to simulate a lowerresolution effect as closely as possible to practical camera
lens. As a result, six different face resolutions of 112 √ó112,
86√ó86, 44 √ó44, 25 √ó25, 20 √ó20, and 15 √ó15 (pixels)
were generated to cover face resolutions that are commonly
encountered from practical still-image- to video-based FR
applications previously reported in [14]‚Äì[16], and [18].
Table I shows the grayscale features, different kinds of color
spaces and chromatic features, and spectral component conÔ¨Åg-
urations used for our experiments. As shown in Table I, for thegrayscale face features, the ‚ÄúR‚Äù channel from the RGB color
space and the grayscale conversion method proposed in [56]
were adopted in our experiments. The R channel of skin-tonecolor is known to be the best monochrome channel for FR [28],
[29]. Moreover, in [56], the 0.85¬∑R+0.10¬∑G+0.05¬∑Bis
reported to be an optimal grayscale conversion method for facedetection. For the spectral component conÔ¨Åguration features,
theYC
bCr,YIQ , andL‚àóa‚àób‚àócolor spaces were used in our
experimentation. The YIQ color space deÔ¨Åned in the National
Television System Committee video standard was adopted. The
YCbCrcolor space is scaled and is the offset version of the
YUV color space [57]. Moreover, the L‚àóa‚àób‚àócolor space
deÔ¨Åned in the CIE perceptually uniform color space was used.
The detailed description of the used color spaces is given in[57]. As described in [57], the YC
bCrand YIQ color spaces
separate RGB into ‚Äúluminance‚Äù (e.g., Yfrom the YC bCrcolor
space) and ‚Äúchrominance‚Äù (or chromaticity) information (e.g.,C
borCrfrom the YCbCrcolor space). In addition, since
theL‚àóa‚àób‚àócolor space is based on the CIE XYZ color space
[57], it is separated into ‚Äúluminance‚Äù (L‚àó)and ‚Äúchromaticity‚Äù
(a‚àóandb‚àó) components. To generate the spectral componentTABLE I
GRAYSCALE FEATURES AND DIFFERENT KINDS OF COLOR SPACES
AND SPECTRAL COMPONENT CONFIGURATIONS USED IN OUR
EXPERIMENTATION .NOTE THAT THE GRAYSCALE FEATURE IS
COMBINED WITH THE CHROMATIC FEATURES TO GENERATE
THE SPECTRAL COMPONENT CONFIGURATIONS
conÔ¨Ågurations depicted in Table I, two different chromaticity
components from the used color spaces are combined with a
selected grayscale component.
For FR experiments, all facial images were preprocessed
according to the recommendation of the FERET protocol [33]
as follows: 1) Color facial images were rotated and scaledso that the centers of eye were placed on the speciÔ¨Åc pixels;
2) color facial images were rescaled into one of Ô¨Åxed template
size among six different spatial resolutions; 3) a standard
mask was applied to remove nonface portions; 4) each spectral
component of color facial images was separately normalized tohave zero mean and unit standard deviations; 5) each spectral
image was transformed to a corresponding column vector; and
6) each column vector was used to form a face vector deÔ¨Ånedin Section III, which covers both grayscale only and spectral
component conÔ¨Ågurations shown in Table I.
To show the stability of the signiÔ¨Åcance of color effect on
low-resolution faces regardless of FR algorithms, three repre-
sentative FR methods, which are PCA, Fisher‚Äôs LDA (FLDA),
and Bayesian, were employed. In subspace FR methods, therecognition performance heavily relies on the number of linear
subspace dimensions (feature dimension) [50]. Thus, the sub-
space dimension was carefully chosen and then Ô¨Åxed over sixdifferent face resolutions to make a fair comparison of perfor-
mance. For PCA, the PCA process in FLDA, and Bayesian,
a well-known 95% energy capturing rule [50] was adoptedto determine subspace dimension. In these experiments, the
number of training samples was 1023 facial images so that
the subspace dimension was experimentally determined as 200
to satisfy the 95% energy capturing rule. Mahalanobis [51],
1224 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS‚ÄîPART B: CYBERNETICS, VOL. 39, NO. 5, OCTOBER 2009
Euclidean distances, and ‚Äúmaximum a posteriori probability‚Äù
were used for similarity metrics in PCA, FLDA, and Bayesian,
respectively.
In FR tasks, the recognition performance results can be
reported for identiÔ¨Åcation and veriÔ¨Åcation (VER). IdentiÔ¨Åcationperformance is usually plotted on a cumulative match char-
acteristic (CMC) curve [33]. The horizontal axis of the CMC
curves is the rank, while the vertical axis is the identiÔ¨Åcationrate. The best found correct recognition rate (BstCRR) [50]
was adopted as the identiÔ¨Åcation rate for fair comparison.
For the VER performance, the receiver operating characteristic(ROC) [52] curve is popular. The ROC curve plots the face
VER rate (FVR) versus the false accept rate (FAR). For an
experimental protocol, the collected set of 3192 facial imageswas randomly partitioned into two sets: training and probe
(or test) sets. The training set consisted of (3 samples √ó
341 subjects) facial images, with the remaining 2169 facialimages for the probe set. There was no overlapping between
the two sets for an evaluation of the used FR algorithms‚Äô
generalization performance with regard to the color effect on
face resolution. To guarantee the reliability of the evaluation,
20 runs of random partitions were executed, and all of theexperimental results reported here were averaged over 20 runs.
B. Experiment 1: To Assess the Impact of Color in
Multiresolution Trained Feature Subspace FR Scenario
In experiment 1, it should be noted that the face resolution of
each pair of training, gallery, and probe sets were all the same.
Since six different face resolutions were used, each feature
subspace was trained with a respective set of facial imageswhose spatial resolution was one of six different kinds. We
performed the comparative experiment to compare the recogni-
tion performances between the two different grayscale featuresdepicted in Table I. Our experimentation indicates that the R
grayscale [28], [29] shows a better performance for most of
the face resolutions, as shown in Fig. 4, in the PCA, FLDA,
and Bayesian methods. However, the performance difference
between the two grayscale conÔ¨Ågurations is marginal. Thus, Rwas selected as the grayscale feature of choice for the exper-
iments aiming to the effect of color on low-resolution faces.
In addition, ‚ÄúRQC
r‚Äù shows the best BstCRR performance of
all kinds of spectral component conÔ¨Ågurations represented in
Table I in all face resolutions and the three FR algorithms. This
result is consistent with a previous one [26] that reported that‚ÄúQC
r‚Äù is the best chromaticity component in the FR grand
challenge DB and evaluation framework [33]. Hence, RQCr
was chosen as a color feature in the following experiments.
Fig. 5 shows the CMC curves for the identiÔ¨Åcation rate (or
BstCRR) comparisons between the grayscale and color features
with respect to six different face resolutions in the PCA, FLDA,and Bayesian FR methods. As can be seen in CMC curves
obtained from the grayscale R feature (in the left side of Fig. 5),
the differences in BstCRR between face resolutions of 112 √ó
112, 86 √ó86, and 44 √ó44 pixels are relatively marginal in
all three FR methods. However, the BstCRRs obtained from
a low resolution of 25 √ó25 pixels and below tend to be
signiÔ¨Åcantly deteriorated in all three FR methods. For example,for PCA, FLDA, and Bayesian methods, the rank-one BstCRRs
(identiÔ¨Åcation rate of top response being correct) decline from
77.20%, 83.69%, and 82.46% to 56.03%, 37.29%, and 62.32%,
respectively, as face resolution is reduced from 112 √ó112 to
15√ó15 pixels.
In case of CMC curves from the RQC
rcolor feature (on the
right side of Fig. 5), we can Ô¨Årst observe that color information
improves the BstCRR compared with grayscale features overall face resolutions in all three FR algorithms. In particular, it is
evident that color features make a substantial enhancement of
the identiÔ¨Åcation rate as face resolutions are 25 √ó25 pixels
and below. In PCA, 56.03%, 59.81%, and 60.97% of rank-
one BstCRRs for 15 √ó15, 20 √ó20, and 25 √ó25 grayscale
faces increase to 69.70%, 62.16%, and 75.14%, respectively,by incorporating color feature QC
r. In FLDA, the color feature
raises rank-one BstCRRs from 37.29%, 49.72%, and 56.48% to
62.16%, 74.64%, and 77.45% for 15 √ó15, 20 √ó20, and 25 √ó
25 face resolutions, respectively. Furthermore, in Bayesian,
rank-one BstCRRs increase from 62.23%, 69.17%, and 71.05%
to 75.14%, 82.46%, and 84.07% for 15 √ó15, 20 √ó20, and
25√ó25 face resolutions, respectively.
To demonstrate the color effect on the VER performance
according to face-resolution variations, the ROC curves are
shown in Fig. 6. We followed the protocol of FRVT [52] to
compute the FVR to the corresponding FAR ranging from 0.1%to 100%, and the z-score normalization [54] technique was
used. Similar to the identiÔ¨Åcation performance in Fig. 6, face
color information signiÔ¨Åcantly improves the VER performanceat low-resolution faces (25 √ó25 pixels and below) compared
with high-resolution ones. For example, when facial images
with a high resolution of 112 √ó112 pixels are applied to
PCA, FLDA, and Bayesian, 5.84%, 4.04%, and 2.18% VER
enhancements at a FAR of 0.1% are attained from the color
feature in PCA, FLDA, and Bayesian methods, respectively. Onthe other hand, in case of a low resolution of 15 √ó15 pixels,
the color feature achieves 19.46%, 38.58%, 15.90% VER
improvement at the same FAR for the respective method.
Table II shows the comparison results of VRGs deÔ¨Åned in
(16) with respect to six different face resolutions in PCA.TheVR G(Œ≥)for each face resolution Œ≥has been averaged
over 20 random selections of 1023 training samples generated
from 3192 collected facial images. The corresponding standarddeviation for each VR G(Œ≥)is also given to guarantee the
stability of the VR G(Œ≥)metric. From Table II, we can see
thatVR G(Œ≥)computed from high-resolution facial images
(higher than 44 √ó44 pixels) are relatively small compared with
those from low-resolution images (25 √ó25 pixels or lower).
This result is largely attributed to the dominance of grayscaleinformation at high-resolution facial images to build intra-
and extrapersonal variations in the feature subspace, so that
the contribution of color is comparatively small. Meanwhile,in low-resolution color faces, VR G(Œ≥)becomes much larger,
since color information can boost the decreased extrapersonal
variation, thanks to its resolution-invariant contrast characteris-tic and independent impact on constructing variations of feature
subspace [20]. The results in Table II verify that face color
features play a supplement role in maintaining an extrapersonal
variation of feature subspace against face-resolution reduction.
CHOI et al. : COLOR FACE RECOGNITION FOR DEGRADED FACE IMAGES 1225
Fig. 5. IdentiÔ¨Åcation rate (or BstCRR) comparison between grayscale and color features with respect to six different face resolutions of each pair of training,
gallery, and probe facial images in the three FR methods. The graphs on the left side resulted from grayscale feature R, while those on the right side wer e generated
from color feature RQCrfor each face resolution. (a) PCA. (b) FLDA. (c) Bayesian.
C. Experiment 2: To Assess the Impact of Color in a
Single-Resolution Trained Feature Subspace FR Scenario
In the practical subspace-based FR applications with face-
resolution constraints (e.g., video surveillance), a single fea-ture subspace is usually provided to perform identiÔ¨Åcation or
VER tasks on probes. It is reasonable to assume that the fea-
ture subspace is pretrained with relatively high-resolution faceimages [13]. On the other hand, the probes to be tested may
have lower and various face resolutions due to heterogeneous
acquisition conditions. Therefore, the objective of Experiment 2is to evaluate the color effect on recognition performance in the
FR scenario where high-resolution training images are used to
construct a single feature subspace, while probe images havevarious face resolutions. In Experiment 2, the face resolution
of training images was Ô¨Åxed as 112 √ó112 pixels, while the
resolution of probe was varied as six different resolutions, as
shown in Fig. 4. Since the high-quality gallery images areusually preregistered in FR systems before testing probes [33],
we assume that the resolution of gallery is the same as thetraining facial images, i.e., 112 √ó112 pixels. In Experiment 2,
R from the RGB color space was used as a grayscale feature.
Due to the best performance from Experiment 1, RQC
rwas
adopted as a color feature.
Fig. 7 shows the CMC curves with respect to six different
probe resolutions in both cases of grayscale (in the left side) and
color features (in the right side) in PCA, FLDA, and Bayesian.
To obtain a low-dimensional feature representation for a lowerface-resolution probe, the probe has been upsampled to have the
same resolution of training faces by using a cubic interpolation
technique in Fig. 7. From Fig. 7, in case of a grayscale feature,we can see a considerable identiÔ¨Åcation rate degradation in all
three FR methods, considering low-resolution (25 √ó25 pixels
and below) probes compared with relatively high-resolutioncounterparts (above 44 √ó44 pixels). In particular, similar to the
1226 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS‚ÄîPART B: CYBERNETICS, VOL. 39, NO. 5, OCTOBER 2009
Fig. 6. FVR comparison at FAR ranging from 0.1% to 100% between grayscale and color features with respect to six different face resolutions in the three
FR algorithms. The graphs on the left side came from grayscale feature R, while those on the right side were obtained from color feature RQCrfor each face
resolution. Note that the z-score normalization technique was used to compute FVR and FAR. (a) PCA. (b) FLDA. (c) Bayesian.
TABLE II
COMPARATIVE EVA L UAT I O N O F VRGs D EFINED IN (16) W ITHRESPECT TO SIXDIFFERENT FACE RESOLUTIONS OF TRAINING
IMAGES IN PCA. G RAYSCALE AND COLOR FEATURES USED FOR COMPUTATION OF VRGs A RERAND RQCr
(SEETABLE I), R ESPECTIVELY .NOTE THAT THE UNIT OF VRGs I SPERCENT
results from Experiment 1, the identiÔ¨Åcation rate resulting from
FLDA is signiÔ¨Åcantly deteriorated at low-resolution probes.The margins of a rank-one identiÔ¨Åcation rate between 112 √ó
112 and each 25 √ó25, 20 √ó20, and 15 √ó15 pixel grayscale
probe in FLDA are 25.66%, 43.77%, and 62.41%, respectively.In case of a color feature, the BstCRR improvement is made
at all probe face resolutions in all three FR algorithms. As
expected, face color information greatly improves the identiÔ¨Å-cation performance obtained from low-resolution probes (25 √ó
25 pixels and below) compared with grayscale feature. In PCA,by incorporating a color feature, the BstCRR margins between
agrayscale probe of the 112 √ó112 resolution and a color
probe of the 25 √ó25, 20 √ó20, and 15 √ó15 resolutions are
reduced to 3.33%, 4.77%, and 8.02%, respectively. In FLDA,
these differences are decreased to 6.65%, 7.28%, and 11.60%
at 25 √ó25, 20 √ó20, and 15 √ó15 resolutions, respectively.
CHOI et al. : COLOR FACE RECOGNITION FOR DEGRADED FACE IMAGES 1227
Fig. 7. IdentiÔ¨Åcation rate comparison between grayscale and color features with respect to six different face resolutions of probe images. The graph s on the left
side resulted from R as a grayscale feature from the RGB color space, while those on the right side were generated from RQCras a color feature for each face
resolution. Note that a single feature subspace trained with face images having a resolution of 112 √ó112 pixels was given to test probe images with varying face
resolutions. (a) PCA. (b) FLDA. (c) Bayesian.
In addition, in Bayesian, 1.47%, 2.61%, and 5.64% perfor-
mance margin decreases are achieved with the aforementioned
three different probe resolutions, thanks to the color feature.
Table III presents the FVRs at a FAR of 0.1% obtained from
the R grayscale and RQCrcolor features with respect to six dif-
ferent face resolutions of probes in three FR methods. Similar
to the identiÔ¨Åcation rates shown in Fig. 7, the color feature hasa great impact on the FVR improvement at low-resolution faces
(25√ó25 pixels and below) in all three FR algorithms. In case
of 15 √ó15 probe resolutions in PCA, FLDA, and Bayesian, the
color feature makes FVR improvements of 15.67%, 54.05%,
and 15.62% at a FAR of 0.1%, respectively, in comparison with
corresponding FVRs from grayscale probes.
VI. D
ISCUSSION AND CONCLUSION
According to the results from Experiments 1 and 2, there
was a commonly harsh drop-off of identiÔ¨Åcation and VER ratescaused by a low-resolution grayscale image (25 √ó25 pixels
or less) in PCA, FLDA, and Bayesian methods. Considering
the performance sensitivity depending on variations in face
resolution, FLDA is found to be the weakest to low-resolutiongrayscale faces (25 √ó25 pixels and below) of all three methods.
As shown in the CMC curves on the left side of Figs. 5(b)
and 7(b), the margins of identiÔ¨Åcation rates between 112 √ó
112 and 15 √ó15 pixels were even 46.40% and 62.41%,
respectively. The underlying reason behind such weakness is
that optimal criteria used to form the feature subspace in FLDAtakes strategy with emphasis on the extrapersonal variation by
attempting to maximize it. Therefore, the recognition perfor-
mance in FLDA is even more sensitive to the portion of ex-
trapersonal variation in the feature subspace compared with the
other two methods. Since grayscale features from much-lower-resolution images have a difÔ¨Åculty in providing a sufÔ¨Åcient
amount of extrapersonal variation to the construction of the fea-
ture subspace, the recognition performance could signiÔ¨Åcantly
1228 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS‚ÄîPART B: CYBERNETICS, VOL. 39, NO. 5, OCTOBER 2009
TABLE III
FVR C OMPARISONS AT A FAR OF0.1% B ETWEEN GRAYSCALE AND COLOR FEATURES WITHRESPECT TO SIXDIFFERENT FACE RESOLUTIONS OF
PROBE IMAGES IN THE THREE FR A LGORITHMS .RF ROM THE RGB C OLOR SPACE WASUSED AS A GRAYSCALE FEATURE ,WHILE THE RQCr
CONFIGURATION WASEMPLOYED AS A COLOR FEATURE .NOTE THAT THE z-SCORE NORMALIZATION WASUSED TO COMPUTE FVR V ERSUS FAR
be decreased. On the contrary, thanks to the color‚Äôs boosting
characteristic of the extrapersonal variation, color features in
FLDA outperformed by 24.86% and 50.81% margins in case
of 15 √ó15 pixels, compared with corresponding grayscale
images, as shown in Figs. 5(b) and 7(b), respectively. Asanother interesting Ô¨Ånding, Bayesian is more robust to face-
resolution variations than PCA and FLDA. For example, from
the CMC curves in the left side of Fig. 7, the performancedifference between 112 √ó112 and 25 √ó25 pixels was not so
even with 8.54% compared with 15.40% and 25.66% obtained
from PCA and FLDA, respectively. A plausible reason undersuch robustness lies in the fact that Bayesian depends more
on the statistical distribution of the intrapersonal variation
rather than the extrapersonal variation [30], [37] so that therecognition performance is less likely affected by the reduc-
tion of the extrapersonal variation caused by low-resolution
images.
Traditionally, low-resolution FR modules have extensively
been used in video-surveillance-like applications. Recently, FR
applications in the web environment are getting increasing
attention due to the popularity of online social networks (e.g.,
Myspace and Facebook) and their high commercialization po-tentials [4]‚Äì[7]. Under a web-based FR paradigm, many devices
such as cellular phone cameras and web cameras often produce
low-resolution or low-quality face images which, however,can be used for recognition purposes [4], [5]. As shown in
our experimentation, color-based FR outperforms grayscale-
based FR over all face resolutions. In particular, thanks tocolor information, both identiÔ¨Åcation and VER rates obtained
by using low-resolution 25 √ó25 or 20 √ó20 templates are
comparable to rates obtained by using much larger grayscaleimages such as 86 √ó86 pixels. Moreover, as shown in Fig. 3,
the face DB, which is used in our experimentation, contains
images obtained under varying illumination conditions. Hence,the robustness of color in low-resolution FR appears to be stable
with respect to the variation in illumination, at least, in our
experimentation. These results demonstrate that facial color can
reliably and effectively be utilized in real-world FR systems
of practical interest, such as video surveillance and promisingweb applications, which frequently have to deal with low-
resolution face images taken under uncontrolled illumination
conditions.A
PPENDIX
LetIŒ¶mmandIŒõmmbe eigenvector and corresponding di-
agonal eigenvalue matrices of IC mmin (9), where m=1,2,3.
That is
IŒ¶T
mmICmmIŒ¶mm=IŒõmm. (A.1)
Using IŒ¶mm(m=1,2,3), we deÔ¨Åne a block diagonal matrix
Qgiven by
Q= diag( IŒ¶11,IŒ¶22,IŒ¶33). (A.2)
Note that Qis an orthogonal matrix. Using (8) and (A.2), we
now deÔ¨Åne matrix IS as
IS=QTIC Q
=‚é°
‚é£IŒõ11 IŒ¶T
11IC12IŒ¶22IŒ¶T
11IC13IŒ¶33
IŒ¶T
22IC21IŒ¶11 IŒõ22 IŒ¶T
22IC23IŒ¶33
IŒ¶T
33IC31IŒ¶11IŒ¶T
33IC32IŒ¶22 IŒõ33‚é§
‚é¶.
(A.3)
IS in (A.3) is similar to IC since there exists an invertible matrix
Qsatisfying IS =Q‚àí1ICQ=QTICQ, where Q‚àí1=QT.D u e
to their similarity , IS and IC have the same eigenvalues and
trace value, so that tr(IS)=t r ( IC). Note that tr(IŒõmm)is the
sum of all the eigenvalues of IC mm.U s i n g tr(IS)=t r ( IC),
tr(IC)can be expressed as
tr(IC)=3/summationdisplay
m=1tr(IŒõmm). (A.4)
A similar derivation to (A.1)‚Äì(A.3) is also readily applied to EC
shown in (8). That is, tr(EC)can represented as
tr(EC)=3/summationdisplay
m=1tr(EŒõmm) (A.5)
where EŒõmm(m=1,2,3)is a diagonal eigenvalue matrix of
ECmm.
CHOI et al. : COLOR FACE RECOGNITION FOR DEGRADED FACE IMAGES 1229
ACKNOWLEDGMENT
The authors would like to thank the anonymous reviewers
for their constructive comments and suggestions. The authors
would also like to thank the FERET Technical Agent, the U.S.
National Institute of Standards and Technology (NIST) forproviding the FERET database.
R
EFERENCES
[1] R. Chellappa, C. L. Wilson, and S. Sirohey, ‚ÄúHuman and machine
recognition of faces: A survey,‚Äù in Proc. IEEE , May 1995, vol. 83,
pp. 705‚Äì740.
[2] W. Zhao, R. Chellappa, P. J. Phillips, and A. Rosenfeld, ‚ÄúFace recognition:
A literature survey,‚Äù ACM Comput. Surv. , vol. 35, no. 4, pp. 399‚Äì458,
Dec. 2003.
[3] K. W. Bowyer, ‚ÄúFace recognition technology: Security versus privacy,‚Äù
IEEE Technol. Soc. Mag. , vol. 23, no. 1, pp. 9‚Äì19, Jun. 2004.
[4] Z. Zhu, S. C. H. Hoi, and M. R. Lyu, ‚ÄúFace annotation using transduc-
tive kernel Ô¨Åsher discriminant,‚Äù IEEE Trans. Multimedia , vol. 10, no. 1,
pp. 86‚Äì96, Jan. 2008.
[5] L. Chen, B. Hu, L. Zhang, M. Li, and H. J. Zhang, ‚ÄúFace annotation for
family photo album management,‚Äù Int. J. Image Graph. , vol. 3, no. 1,
pp. 1‚Äì14, 2003.
[6] S. Satoh, Y . Nakamura, and T. Kanade, ‚ÄúName-it: Naming and detecting
faces in news videos,‚Äù IEEE Trans. Multimedia , vol. 6, no. 1, pp. 22‚Äì35,
Jan.‚ÄìMar. 1999.
[7] J. Y . Choi, S. Yang, Y . M. Ro, and K. N. Plataniotis, ‚ÄúFace annotation for
personal photos using context-assisted face recognition,‚Äù in Proc. ACM
Int. Conf. MIR , 2008, pp. 44‚Äì51.
[8] Z. Wangmeng, D. Zhang, Y . Jian, and W. Kuanquan, ‚ÄúBDPCA plus
LDA: A novel fast feature extraction technique for face recognition,‚Äù
IEEE Trans. Syst., Man, Cybern. B, Cybern. , vol. 36, no. 4, pp. 946‚Äì953,
Aug. 2006.
[9] Q. Li, J. Ye, and C. Kambhamettu, ‚ÄúLinear projection methods in face
recognition under unconstrained illumination: A comparative study,‚Äù in
Proc. IEEE Int. Conf. CVPR , 2004, pp. II-474‚ÄìII-481.
[10] R. Singh, M. Vatsa, A. Ross, and A. Noore, ‚ÄúA mosaicing scheme for
pose-invariant face recognition,‚Äù IEEE Trans. Syst., Man, Cybern. B,
Cybern. , vol. 37, no. 5, pp. 1212‚Äì1225, Oct. 2007.
[11] J. H. Lim and J. S. Jin, ‚ÄúSemantic indexing and retrieval of home photos,‚Äù
inProc. IEEE Int. Conf. ICARCV , 2007, pp. 186‚Äì191.
[12] D. M. Blackburn, J. M. Bone, and P. J. Phillips, ‚ÄúFace recognition ven-
dor test 2000: Evaluation report,‚Äù Defense Adv. Res. Projects Agency,
Arlington, V A, 2001.
[13] J. Y . Choi, Y . M. Ro, and K. N. Plataniotis, ‚ÄúFeature subspace determi-
nation in video-based mismatched face recognition,‚Äù in Proc. IEEE Int.
Conf. AFGR , 2008, pp. 14‚Äì20.
[14] H. K. Ekenel and A. Pnevmatikakis, ‚ÄúVideo-based face recognition evalu-
ation in the CHIL project‚ÄîRun1,‚Äù in Proc. IEEE Int. Conf. AFGR , 2006,
pp. 85‚Äì90.
[15] B. J. Boom, G. M. Beumer, L. J. Spreeuwers, and R. N. J. Veldhuis,
‚ÄúThe effect of image resolution on the performance of a face recognitionsystem,‚Äù in Proc. IEEE. Int. Conf. CARV , 2006, pp. 1‚Äì6.
[16] A. Hadid and M. Pietikainen, ‚ÄúFrom still image to video-based face
recognition: An experimental analysis,‚Äù in Proc. IEEE Int. Conf. AFGR ,
2004, pp. 813‚Äì818.
[17] L. Tian, ‚ÄúEvaluation of face resolution for expression analysis,‚Äù in Proc.
IEEE Int. Conf. CVPR , 2004, p. 82.
[18] B. K. Gunturk, A. U. Batur, Y . Altunbasak, M. H. Hayes, III, and
R. M. Mersereau, ‚ÄúEigenface-domain super-resolution for face recogni-tion,‚Äù IEEE Trans. Image Process. , vol. 12, no. 5, pp. 597‚Äì606, May 2003.
[19] [Online]. Available: http://www.Ô¨Çickr.com
[20] L. H. Wurm, G. E. Legge, L. M. Isenberg, and A. Lubeker, ‚ÄúColor
improves object recognition in normal and low vision,‚Äù J. Exp. Psychol.
Hum. Percept. Perform. , vol. 19, no. 4, pp. 899‚Äì911, Aug. 1993.
[21] A. Yip and P. Sinha, ‚ÄúRole of color in face recognition,‚Äù J. Vis. ,v o l .2 ,
no. 7, p. 596, 2002.
[22] L. Torres, J. Y . Reutter, and L. Lorente, ‚ÄúThe importance of the color
information in face recognition,‚Äù in Proc. IEEE Int. Conf. ICIP , 1999,
pp. 627‚Äì631.
[23] M. Rajapakse, J. Tan, and J. Rajapakse, ‚ÄúColor channel encoding with
NMF for face recognition,‚Äù in Proc. IEEE Int. Conf. ICIP , 2004, vol. 3,
pp. 2007‚Äì2010.[24] C. F. Jones, III and A. L. Abbott, ‚ÄúOptimization of color conversion for
face recognition,‚Äù EURASIP J. Appl. Signal Process. , vol. 2004, no. 4,
pp. 522‚Äì529, 2004.
[25] P. Shih and C. Liu, ‚ÄúComparative assessment of content-based face image
retrieval in different color spaces,‚Äù Int. J. Pattern Recogn. Artif. Intell. ,
vol. 19, no. 7, pp. 873‚Äì893, 2005.
[26] P. Shih and C. Liu, ‚ÄúImproving the face recognition grand challenge
baseline performance using color conÔ¨Ågurations across color spaces,‚Äù in
Proc. IEEE Int. Conf. Image Process. , 2006, pp. 1001‚Äì1004.
[27] B. Karimi, ‚ÄúComparative analysis of face recognition algorithms and
investigation on the signiÔ¨Åcance of color,‚Äù M.S. thesis, Concordia Univ.,Montreal, QC, Canada, 2006.
[28] M. T. Sadeghi, S. Khoushrou, and J. Kittler, ‚ÄúConÔ¨Ådence based gating
of colour features for face authentication,‚Äù in Proc. Int. Workshop MCS ,
2007, vol. 4472, pp. 121‚Äì130.
[29] J. Wang and C. Liu, ‚ÄúA general discriminant model for color face recog-
nition,‚Äù in Proc. IEEE Int. Conf. ICCV , 2007, pp. 1‚Äì6.
[30] B. Moghaddam, ‚ÄúPrincipal manifolds and probabilistic subspaces for vi-
sual recognition,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 24, no. 6,
pp. 780‚Äì788, Jun. 2002.
[31] J. Lu, K. N. Plataniotis, A. N. Venetsanopoulos, and S. Z. Li, ‚ÄúEnsemble-
based discriminant learning with boosting for face recognition,‚Äù IEEE
Trans. Neural Netw. , vol. 17, no. 1, pp. 166‚Äì178, Jan. 2006.
[32] T. Sim, S. Baker, and M. Bsat, ‚ÄúThe CMU pose, illumination, and expres-
sion database,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 25, no. 12,
pp. 1615‚Äì1618, Dec. 2003.
[33] P. J. Phillips, H. Moon, S. A. Rizvi, and P. J. Rauss, ‚ÄúThe FERET evalu-
ation methodology for face-recognition algorithms,‚Äù IEEE Trans. Pattern
Anal. Mach. Intell. , vol. 22, no. 10, pp. 1090‚Äì1104, Oct. 2000.
[34] K. Messer, J. Mastas, J. Kittler, J. Luettin, and G. Maitre, ‚ÄúXM2VTSDB:
The extended M2VTS database,‚Äù in Proc. IEEE Int. Conf. AVBPA , 1999,
pp. 72‚Äì77.
[35] M. A. Turk and A. P. Pentland, ‚ÄúEigenfaces for recognition,‚Äù J. Cogn.
Neurosci. , vol. 3, no. 1, pp. 71‚Äì86, 1991.
[36] P. N. Belhumeur, J. P. Hesphanha, and D. J. Kriegman, ‚ÄúEigenfaces vs.
Fisherfaces: Recognition using class speciÔ¨Åc linear projection,‚Äù IEEE
Trans. Pattern. Anal. Mach. Intell. , vol. 9, no. 7, pp. 711‚Äì720, Jul. 1997.
[37] B. Moghaddam, T. Jebara, and A. Pentland, ‚ÄúBayesian face recognition,‚Äù
Pattern Recognit. , vol. 33, no. 11, pp. 1771‚Äì1782, 2000.
[38] R. Hsu, M. A. Monttaleb, and A. Jain, ‚ÄúFace detection in color images,‚Äù
IEEE Trans. Pattern Anal. Mach. Intell. , vol. 24, no. 5, pp. 696‚Äì706,
May 2002.
[39] A. J. Colmenarez and T. S. Huang, ‚ÄúFace detection and tracking of faces
and facial features,‚Äù in Proc. IEEE Int. Conf. CVPR , 1997, pp. 657‚Äì661.
[40] S. Hayashi and O. Hasegawa, ‚ÄúA detection technique for degraded face
images,‚Äù in Proc. IEEE Int. Conf. CVPR , 2006, pp. 1506‚Äì1512.
[41] S. Baker and T. Kanade, ‚ÄúLimits on super-resolution and how to break
them,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 24, no. 9, pp. 1167‚Äì
1183, Sep. 2002.
[42] F. W. Wheeler, X. Liu, and P. H. Tu, ‚ÄúMulti-frame super-resolution for
face recognition,‚Äù in Proc. IEEE Int. Conf. BTAS , 2007, pp. 1‚Äì6.
[43] X. Wang and X. Tang, ‚ÄúA uniÔ¨Åed framework for subspace face recogni-
tion,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 26, no. 9, pp. 1222‚Äì
1228, Sep. 2004.
[44] J. Wang, K. N. Plataniotis, and A. N. Venetasanopoulos, ‚ÄúSelecting
discriminant eigenfaces for face recognition,‚Äù Pattern Recognit. Lett. ,
vol. 26, no. 10, pp. 1470‚Äì1482, Jul. 2005.
[45] J. Xiao-Yuan and D. Zhang, ‚ÄúA face and palmprint recognition approach
based on discriminant DCT feature extraction,‚Äù IEEE Trans. Syst., Man,
Cybern. B, Cybern. , vol. 34, no. 6, pp. 2405‚Äì2415, Dec. 2004.
[46] H. Stokman and T. Gevers, ‚ÄúSelection and fusion of color models for
image feature detection,‚Äù
IEEE Trans. Pattern Anal. Mach. Intell. , vol. 29,
no. 3, pp. 371‚Äì381, Mar. 2007.
[47] Y . Ohta, T. Kanade, and T. Sakai, ‚ÄúColor information for region segmen-
tation,‚Äù Comput. Graph. Image Process. , vol. 13, no. 3, pp. 222‚Äì241,
Jul. 1980.
[48] D. H. Kelly, ‚ÄúSpatiotemporal variation of chromatic and achromatic
contrast thresholds,‚Äù J. Opt. Soc. Amer. , vol. 73, no. 6, pp. 742‚Äì749,
Jun. 1983.
[49] J. B. Derrico and G. Buchsbaum, ‚ÄúA computational model of spatiochro-
matic image coding in early vision,‚Äù J. Vis. Commun. Image Represent. ,
vol. 2, no. 1, pp. 31‚Äì38, Mar. 1991.
[50] J. Wang, K. N. Plataniotis, J. Lu, and A. N. Venetsanopoulos, ‚ÄúOn solv-
ing the face recognition problem with one training sample per subject,‚Äù
Pattern Recognit. , vol. 39, no. 6, pp. 1746‚Äì1762, Sep. 2006.
[51] V . Perlibakas, ‚ÄúDistance measures for PCA-based face recognition,‚Äù
Pattern Recognit. Lett. , vol. 25, no. 12, pp. 1421‚Äì1430, Apr. 2004.
1230 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS‚ÄîPART B: CYBERNETICS, VOL. 39, NO. 5, OCTOBER 2009
[52] P. J. Grother, R. J. Micheals, and P. J. Phillips, ‚ÄúFace recognition vendor
test 2002 performance metrics,‚Äù in Proc. Int. Conf. Audio- Video-Based
Biometric Person Authentication , 2003, vol. 2688, pp. 937‚Äì945.
[53] P. J. Phillips, P. J. Flynn, T. Scruggs, K. W. Bowyer, C. Jin, K. Hoffman,
J. Marques, M. Jaesik, and W. Worek, ‚ÄúOverview of the face recognition
grand challenge,‚Äù in Proc. IEEE Int. Conf. CVPR , 2005, pp. 947‚Äì954.
[54] A. Jain, K. Nandakumar, and A. Ross, ‚ÄúScore normalization in multi-
modal biometric systems,‚Äù Pattern Recognit. , vol. 38, no. 12, pp. 2270‚Äì
2285, Dec. 2005.
[55] L. P. Hansen, ‚ÄúLarge sample properties of generalized method of moments
estimators,‚Äù Econometrica , vol. 50, no. 4, pp. 1029‚Äì1054, 1982.
[56] J. Lu, M. Thiyagarajah, and H. Zhou, ‚ÄúConverting a digital image from
color to gray-scale,‚Äù U.S. Patent 20 080 144 892, Jun. 19, 2008.
[57] R. Lukac and K. N. Plataniotis, Color Image Processing: Methods and
Application . New York: CRC, 2007.
[58] A. K. Jain, A. Ross, and S. Prabhaker, ‚ÄúAn introduction to biometric
recognition,‚Äù IEEE Trans. Circuits Syst. Video Technol. , vol. 14, no. 1,
pp. 4‚Äì20, Jan. 2004.
[59] Proposed Draft Amendment to ISO/IEC 19794-5 Face Image Data on
Conditions for Taking Pictures , Mar. 1, 2006.
Jae Young Choi received the B.S. degree from
Kwangwoon University, Seoul, Korea, in 2004 andthe M.S. degree from the Korea Advanced Instituteof Science and Technology (KAIST), Daejeon, Ko-
rea, in 2008, where he is currently working toward
the Ph.D. degree with the Image and Video SystemLaboratory.
He was an Intern Researcher for the Electronic
Telecommunications Research Institute, Daejon, in2007. In 2008, he was a Visiting Student Researcherat the University of Toronto, Toronto, ON, Canada.
His research interests include face recognition/detection, image/video indexing,
pattern recognition, machine learning, MPEG-7, and personalized broadcastingtechnologies.
Yong Man Ro (M‚Äô92‚ÄìSM‚Äô98) received the B.S.
degree from Yonsei University, Seoul, Korea, and the
M.S. and Ph.D. degrees from the Korea AdvancedInstitute in Science and Technology (KAIST),Daejon, Korea.
In 1987, he was a Researcher with Columbia
University, New York, NY , and from 1992 to 1995,he was a Visiting Researcher with the University ofCalifornia, Irvine, and with KAIST. In 1996, he was
a Research Fellow with the University of California,
Berkeley. He is currently a Professor and the Director
of the Image and Video System Laboratory, Korea Advanced Institute of Sci-ence and Technology (KAIST), Daejeon. He participated in international stan-
dardizations including MPEG-7 and MPEG-21, where he contributed several
MPEG-7 and MPEG-21 standardization works, including the MPEG-7 texturedescriptor and MPEG-21 DIA visual impairment descriptors and modality
conversion. His research interests include image/video processing, multimedia
adaptation, visual data mining, image/video indexing, and multimedia security.
Dr. Ro was the recipient of the Young Investigator Finalist Award of the
International Society for Magnetic Resonance in Medicine in 1992 and the
Scientist Award (Korea), in 2003. He has served as a Technical Program Com-
mittee member for many international conferences, including the InternationalWorkshop on Digital Watermaking (IWDW), Workshop on Image Analysisfor Multimedia Interactive Services (WIAMI), Asia Information Retrieval
Symposium (AIRS), Consumer Communications and Networking Conference,
etc., and as the Co-Program Chair of the 2004 IWDW.
Konstantinos N. (Kostas) Plataniotis (S‚Äô90‚ÄìM‚Äô92‚Äì
SM‚Äô03) received the B.Eng. degree in computer
engineering from the University of Patras, Patras,Greece, in 1988 and the M.S. and Ph.D. degrees
in electrical engineering from the Florida Insti-
tute of Technology, Melbourne, in 1992 and 1994,respectively.
He is currently a Professor with the Edward S.
Rogers, Sr. Department of Electrical and Computer
Engineering, University of Toronto, Toronto, ON,Canada, where he is a member of the Knowledge
Media Design Institute and the Director of Research for the Identity, Privacy,
and Security Initiative and is an Adjunct Professor with the School of Com-
puter Science, Ryerson University, Toronto. His research interests include bio-metrics, communications systems, multimedia systems, and signal and imageprocessing.
Dr. Plataniotis is the Editor-in-Chief for the IEEE S
IGNAL PROCESSING
LETTERS for 2009‚Äì2011. He is a Registered Professional Engineer in the
province of Ontario and a member of the Technical Chamber of Greece. He
was the 2005 recipient of IEEE Canada‚Äôs Outstanding Engineering Educator
Award ‚Äúfor contributions to engineering education and inspirational guidanceof graduate students‚Äù and is the corecipient of the 2006 IEEE T
RANSACTIONS
ONNEURAL NETWORKS Outstanding Paper Award for the paper entitled ‚ÄúFace
Recognition Using Kernel Direct Discriminant Analysis Algorithms,‚Äù which
was published in 2003.
"
https://ieeexplore.ieee.org/iel5/83/4358840/06020798.pdf,"1366 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 21, NO. 3, MARCH 2012
Color Local Texture Features for
Color Face Recognition
Jae Young Choi, Yong Man Ro , Senior Member, IEEE , and Konstantinos N. Plataniotis , Senior Member, IEEE
Abstract‚Äî This paper proposes new color local texture features,
i.e., color local Gabor wavelets (CLGWs) and color local binarypattern (CLBP), for the purpose of face recognition (FR). Theproposed color local texture features are able to exploit the dis-criminative information derived from spatiochromatic texturepatterns of different spectral channels within a certain local faceregion. Furthermore, in order to maximize a complementary effecttaken by using both color and texture information, the opponent
color texture features that capture the texture patterns of spatial
interactions between spectral channels are also incorporated intothe generation of CLGW and CLBP. In addition, to performthe Ô¨Ånal classiÔ¨Åcation, multiple color local texture features (eachcorresponding to the associated color band) are combined withina feature-level fusion framework. Extensive and comparative ex-periments have been conducted to evaluate our color local texturefeatures for FR on Ô¨Åve public face databases, i.e., CMU-PIE, ColorFERET, XM2VTSDB, SCface, and FRGC 2.0. Experimentalresults show that FR approaches using color local texture featuresimpressively yield better recognition rates than FR approachesusing only color or texture information. Particularly, comparedwith grayscale texture features, the proposed color local texturefeatures are able to provide excellent recognition rates for faceimages taken under severe variation in illumination, as well as forsmall- (low-) resolution face images. In addition, the feasibility
of our color local texture features has been successfully demon-
strated by making comparisons with other state-of-the-art colorFR methods.
Index Terms‚Äî Color face recognition (FR), color local texture
features, color spaces, combination, Gabor wavelets, local binarypattern (LBP).
I. I NTRODUCTION
FACE recognition (FR) has received a signiÔ¨Åcant interest
in pattern recognition and computer vision due to the wide
range of applications including video surveillance [1], biometricidentiÔ¨Åcation [2], and face indexing in multimedia contents [3].
As in any classiÔ¨Åcation task, feature extraction is of great impor-
tance in the FR process. Recently, local texture features [4]‚Äì[6]
Manuscript received October 14, 2010; revised January 24, 2011 and April
28, 2011; accepted May 16, 2011. Date of publication September 15, 2011; dateof current version February 17, 2012. This work was supported by Brain Korea
21 Project, BK Electronics and Communications Technology Division, KAIST
in 2011. The associate editor coordinating the review of this manuscript and
approving it for publication was Dr. Arun A. Ross.
J. Y. Choi and Y. M. Ro are with the Image and Video Systems Lab, De-
partment of Electrical Engineering, Korea Advanced Institute of Science and
Technology (KAIST), Daejeon 305-732, Korea (e-mail: jygchoi@kaist.ac.kr;
ymro@ee.kaist.ac.kr).
K. N. Plataniotis is with the Multimedia Lab, Department of Electrical and
Computer Engineering, University of Toronto, ON M5S3G4, Canada (e-mail:
kostas@comm.utoronto.ca).
Color versions of one or more of the Ô¨Ågures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object IdentiÔ¨Åer 10.1109/TIP.2011.2168413have gained reputation as powerful face descriptors because
they are believed to be more robust to variations of facial pose,
expression, occlusion, etc. In particular, Gabor wavelets [7] and
local binary pattern (LBP) [8] texture features have proven to behighly discriminative for FR due to different levels of locality.
There has been a limited but increasing amount of work on
the color aspects of textured image analysis [9]‚Äì[11]. Results in
these works indicate that color information can play a comple-mentary role in texture analysis and classiÔ¨Åcation/recognition,and consequently, it can be used to enhance classiÔ¨Åcation/recog-
nition performance. In [9], the authors performed an empirical
evaluation study that compares color indexing, grayscale tex-ture, and color texture methods for classiÔ¨Åcation tasks on textureimages data set taken under either constant (static) or varying
illumination conditions. Experimental result shows that, for the
case of static illumination condition, color texture descriptorsgenerally perform better than their grayscale counterparts. In[10], three grayscale texture techniques including local linear
transform, Gabor Ô¨Åltering, and co-occurrence methods are ex-
tended to color images. This paper reports that the use of colorinformation can improve classiÔ¨Åcation performance obtainedusing only grayscale texture analysis techniques. In [11], incor-
porating color into a texture analysis can be beneÔ¨Åcial for classi-
Ô¨Åcation/recognition schemes. In particular, the authors showedthat perceptually uniform color spaces
and hue, satura-
tion, and value
 perform better than red, green, and blue
for color texture analysis.
Following the aforementioned studies, it is natural to expect
better FR performance by combining color and texture informa-tion than by using only color or texture information. However,
at the moment, how to effectively make use of both color and
texture information for the purpose of FR still remains an openproblem. The aim of this paper is to suggest a new color FRframework, which effectively combines color and texture infor-
mation, aiming to improve FR performance. The main contri-
bution of our paper is threefold.
1) This paper proposes the Ô¨Årst so-called color local texture
features. SpeciÔ¨Åcally, we develop two effective color localtexture features, i.e., color local Gabor wavelets (CLGWs)
and color LBP (CLBP), both of which are able to encode
the discriminative features derived from spatiochromatic
texture patterns of different spectral channels (or bands)
within a certain local region. In addition, to make full use
of both color and texture information, the opponent color
texture features that capture the texture patterns of spatialinteractions between spectral bands are incorporated into
the generation of CLGW and CLBP. This allows for ac-
quiring more discriminative color local texture features, as
1057-7149/$26.00 ¬© 2011 IEEE
CHOI et al. : COLOR LOCAL TEXTURE FEATURES FOR COLOR FACE RECOGNITION 1367
compared with conventional grayscale texture features, for
improving FR performance.
2) The effective way of combining color local texture features
has not been explored in the current FR works. This papersuggests the feature-level fusion approach in order to inte-grate multiple color local texture features [each extracted
from an associated color component (or spectral) image]
for the Ô¨Ånal classiÔ¨Åcation. As demonstrated by our exper-imental results, the feature-level fusion approach worksbetter than the decision-level (or score-level) fusion ap-
proach in terms of maximizing complementary effect that
lies on both color and texture information.
3) Comparative and extensive experiments have been
conducted to investigate the effectiveness of proposed
color local texture features. For this, Ô¨Åve public face
databases (DB), i.e., CMU-PIE [12], Color FERET [13],XM2VTSDB [14], SCface [48], and FRGC 2.0 [15], areused. Our experimental results show that the FR approach
using our color local texture features achieves even better
FR performance than the FR approach relying only oncolor or texture information. In particular, the importantresults exploited by our experimental study are that color
local texture features are highly effective for low-reso-
lution images and are robust against severe variation inillumination (caused by the interruption of backgroundcolored light and cast shadow), as compared with conven-
tional grayscale texture features. Furthermore, we clearly
validate the feasibility of our color local texture featuresby comparing the best results of our method with theresults of other state-of-the-art color FR methods.
The rest of this paper is organized as follows: Section II re-
views the existing FR methods using texture features and FRmethods using color information. In Section III, the proposedcolor FR framework using color local texture features is out-
lined. Section IV details the proposed color local texture feature
extraction approach. In Section V, we present our fusion ap-proach to combining multiple color local texture features to per-form FR. In Section VI, we present comparative experimental
results that demonstrate the effectiveness of color local texture
features. Conclusions constitute Section VII.
II. R
ELATED WORK
So far, numerous approaches using texture features have been
developed and successfully applied to FR. These works roughlyfall into four categories: 1) methods using Gabor wavelets [7]or LBP features [8]; 2) methods using the fusion of global and
local face features [5]; 3) methods using the fusion of magnitude
and phase information of Gabor wavelets [6]; 4) methods usingfusion of Gabor and LBP [16]. However, most of these workshave been limited to grayscale texture analysis.
FR using color information is a relatively new research topic
in the area of automatic FR. Initial works on color FR focused ondetermining Ô¨Åxed color-component conÔ¨Ågurations (from var-ious color spaces) suitable for FR through empirical compar-isons. In [17], the
,
 , and
 color spaces were
examined in the context of FR. The results show that facialcolor contains complementary information and that the accu-
racy of FR is affected by the color space chosen. Shih and Liu[18] also studied the use of different color spaces and colorcomponent conÔ¨Ågurations for improving the FR performance.They showed that the
(luminance, chrominance blue,
and chrominance red),
 (luminance, in-phase, and quadra-
ture), and
 color spaces are better suited for the
purpose of FR than several other color spaces. In [19] and [20],combining spectral components across different color spaces isfound to be useful for enhancing the FR accuracy. In particular,in [20], a new hybrid color space ‚Äú
‚Äù is proposed, where
the
 channel is taken from the
 color space and the
 and
channels are taken from the
 and
 color spaces,
respectively. The authors show that the
 color represen-
tation can achieve a better FR performance.
Recent works proposed the use of alternative grayscale or
color space conversions in order to further obtain an enhancedFR performance. In [21], the authors proposed an optimalconversion of color images in the
color space into a
monochromatic form. They developed a color image discrimi-
nant model to Ô¨Ånd a set of optimal combination coefÔ¨Åcients and
demonstrated the usefulness of the proposed monochromaticrepresentation. Liu [22] proposed three new color repre-sentations, i.e., the so-called uncorrelated color space, theindependent color space, and the discriminating color space.The author shows that the later three-color representations areeffective for enhancing the FR performance, as compared withthe use of color images represented in the
color space. In
[23], the authors found out a common characteristic of a pow-erful color space for FR by analyzing the transformation matrixof the different color spaces from the
color space. In
addition, based on the characteristic of powerful color spaces,they proposed color space normalization techniques, which areable to convert weak color spaces into powerful ones, so thatbetter FR performance can be obtained by making use of these
normalized color spaces.
More recently, a little research effort has been dedicated to
apply a multiple-feature encoding scheme to multiple and dif-ferent color-component images. In [45], the authors proposeda hybrid color-and-frequency-feature (CFF) method for colorFR. A new hybrid color space
, which combines the
component image of the
 color space and the chromatic
components
 and
 of the
 color space, is proposed. The
CFF method is devised to extract the complementary featuresfrom the real part, the imaginary part, and the magnitude of the
,
, and
 color-component images, respectively, in the dis-
crete-cosine-transform (DCT) frequency domain. Multiple fre-quency features each extracted from the corresponding compo-nent image are combined using a weighted sum rule for the
purpose of FR. In [24], the authors propose the FR method
that fuses multiple global and local features derived from a hy-brid color space
. SpeciÔ¨Åcally, three different image en-
coding schemes are suggested, i.e., a patch-based Gabor imagerepresentation for the
component image, a multiresolution
LBP feature fusion scheme for the
 component image, and a
component-based DCT multiple encoding for the
 component
image. This color FR method achieves considerably better FRperformance, as compared with the reported results on FRGCversion 2 Experiment 4 [15], by means of fusing three similarity
1368 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 21, NO. 3, MARCH 2012
Fig. 1. Proposed color FR framework based on color local texture features. /89/67
 /67
is used for an illustration of color space conversion.
matrices (using a weighted sum rule) that are generated from
three component images in the
 color space. In [52], mul-
tispectral LBP histograms are extracted from the
 color
face image and concatenated into a regional feature for FR.For three channels in the
color space, multispectral LBP
histograms are computed using three monochrome and six op-ponent LBP operators. In [53], the authors suggest six multi-
scale color LBP operators for the visual-object-class recogni-
tion tasks. In particular, in this work, the opponent-LBP andnOpponent-LBP operators are proposed. The opponent-LBP isobtained by computing the LBP over all three channels (in the
color space) of the opponent color space, whereas the
nOpponent-LBP is generated by performing the LBP over twochannels of the normalized opponent color space.
Although previous works in color-based FR have success-
fully demonstrated the importance of color information in orderto obtain improved FR performance, a large part of color FR re-search [17]‚Äì[23], [45] has been limited to developing color FRmethods using only global-based feature extraction techniquessuch as DCT and principal component analysis (PCA). As such,the complementary effect taken by combining color and localtexture information on the FR problem (e.g., illumination or
pose constraints) has not been systematically explored in the
current color FR work. The aim of this paper is to Ô¨Åll this blankby presenting the effective FR framework based on integratingcolor and local texture information in an effective way.
III. F
RAMEWORK OF FR U SING COLOR LOCAL
TEXTURE FEATURES
As shown in Fig. 1, the proposed color FR framework using
color local texture features consists of three major steps: colorspace conversion and partition, feature extraction, and combi-nation and classiÔ¨Åcation.
A face image represented in the
color space is Ô¨Årst
translated, rotated, and rescaled to a Ô¨Åxed template [13],yielding the corresponding aligned face image. Subsequently,the aligned
color image is converted into an image repre-
sented in another color space. Note that not only conventionallinear or nonlinear color spaces (e.g.,
 or
 )b u t
also new color spaces devised for the purpose of FR (e.g.,normalized color space proposed in [23]) can be used for colorspace conversion. Each of the color-component images ofcurrent color model is then partitioned into local regions assuggested by [4].
In the next step, texture feature extraction is independently
and separately performed on each of these local regions. Since
texture features are extracted from the local face regions ob-
tained from different color channels , they are referred to as
‚Äúcolor local texture features.‚Äù Note that the key to FR usingcolor information is to extract the so-called opponent texture
features [25] between each pair of two spectral images, as wellas unichrome (or channelwise) texture features. This allows forobtaining much more complementary texture features for im-proving the FR performance, as compared with grayscale tex-ture feature extraction, where only the luminance of an image istaken into account.
Since
color local texture features (each obtained from the
associated local region and spectral channel) are available, wehave to combine them to reach the Ô¨Ånal classiÔ¨Åcation. To thisend, multimodal fusion techniques [26] are employed for inte-
grating multiple color local texture features for improving the
FR performance. In the following subsections, the detailed ex-planation of color local texture feature extraction and combi-nation and classiÔ¨Åcation steps are provided (For further detailsregarding color space conversion and partition methods, refer to[27] and [4], respectively).
IV . E
XTRACTION OF COLOR LOCAL TEXTURE FEATURES
Here, we present the methods of extracting the proposed
color local texture features from a color image. Two commonlyused texture feature representations are considered, i.e., Gaborwavelet and LBP. Here, these grayscale texture features areextended to the multispectral texture features using color infor-mation. SpeciÔ¨Åcally, given a color image, the texture operatoris applied on each separate color channel. In addition, we can
further extend the texture operator to make use of opponent
CHOI et al. : COLOR LOCAL TEXTURE FEATURES FOR COLOR FACE RECOGNITION 1369
Fig. 2. Illustration of the Gabor wavelet representations with Ô¨Åve scales and eight orientations of sample local regions. Three local region images f or the same
facial component (e.g., eye or nose) are separately obtained from three color channels in the /89/67
 /67
color space. The text enclosed in brackets represents the values
of orientation /117and scale /118(which are used to obtain the below Gabor wavelet representations placed in the same column) in the following format: ( orientation ,
scale ).
colors. The term ‚Äúopponent color‚Äù here follows the deÔ¨Ånition
suggested by [25], i.e., all pairs of different color channels arenamed ‚Äúopponent colors.‚Äù
Before describing the method of extracting the color local tex-
ture features, we introduce notation commonly used throughoutthe following subsections. Let us assume that
different kinds
of spectral images are generated from an
 color face image
via color space conversions prespeciÔ¨Åed. Then, let
 be the
th spectral image (e.g., chrominance component ‚Äú
 ‚Äù from the
color space), where
 . Moreover, assuming
that
 is divided into
 local face regions (as described in
Fig. 1), we denote its
 th local region by
 .
A. Extraction of Color Local Gabor Wavelets
Gabor wavelets can be obtained based on Gabor Ô¨Ålters [6]
that detect amplitude-invariant spatial frequencies of pixel grayvalues. Gabor wavelet features have been widely adopted in FRdue to the robustness against illumination changes. The 2-DGabor Ô¨Ålter can be deÔ¨Åned as follows [7]:
(1)
where
 and
 deÔ¨Åne the orientation and the scale of the Gabor
Ô¨Ålters,
 ,
 denotes the norm operator,
,
 ,
 ,
 is the maximum
frequency, and
 is the spacing factor between Ô¨Ålters in the fre-
quency domain [7]. Note that the Gabor Ô¨Ålters in (1) can take avariety of different forms, along with different
scales and
orientations.
To reÔ¨Çect and encode the local properties of the
 th spectral
image
 when computing its associated Gabor wavelet, the
following operation can be performed [7]:
(2)where
 denotes the convolution operator and
 is a
Gabor wavelet representation (with orientation
 and scale
 )
of the
 th local region
 of the
 th spectral image. Hence,
set
forms a set of Gabor wavelet representations corresponding to
local regions of
 . Fig. 2 shows the Gabor wavelet repre-
sentations (the magnitude) of sample local region images. Notethat, in Fig. 2, three local region images (one for each colorband) corresponding to the same facial component (e.g., eye ornose) differ in the pattern of Gabor wavelet representations. Thisindicates that they can provide different complementary infor-mation for the purpose of FR.
In addition, in order to exploit the additional information
contained between spectral bands, we now deÔ¨Åne an opponent
local Gabor representation. Let
and
 be
the Gabor wavelet representations (with a particular orienta-
tion
 ) of the
 th local regions
 and
 of two
different spectral bands
 and
 , respectively. Using
and
 , the opponent Gabor representation between
and
 are then computed as follows:
for all
 (3)
where
 (
 can be calcu-
lated in a similar way) and
 and
 denote the different scales
of the Gabor Ô¨Ålters used.
In order to make an effective use of different complementary
information, we have to reserve locality information and to
encompass color texture information. To this end, all Gabor
1370 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 21, NO. 3, MARCH 2012
wavelet representations
 and
 with a
particular
 are concatenated, resulting in an augmented feature
vector named the CLGW feature. For concatenation, we form
a column vector, which we write as
 , from
 by
stacking its rows (or columns).
 can be calculated
(from
 ) in a similar way.
The CLGW feature for the
 th spectral image
 is then de-
Ô¨Åned as follows:
for
 (4)
where
 is the transpose operator,
 denotes
the unichrome Gabor feature such that
, and
 denotes
the opponent Gabor feature deÔ¨Åned as
. Recall that
 and
 denote the number of orientations and
scales, respectively, and
 is the number of local regions.
Note that
 consists of a total of
 unichrome local
Gabor features
 , whereas
 is represented by a
total of
 opponent local Gabor features
conditioned on
 .
B. Extraction of Color LBP
Given
 different color-component images
, the unichrome (or channelwise) LBP feature is
separately and independently computed from each
 . Note
that, in the computation of the unichrome LBP feature, theuniform LBP operator [8] is adopted because a typical faceimage contains only a small number of LBP values (called the
uniform pattern), as reported in [8]. Let us denote that
is the
center pixel position of
 and
 are
equally spaced pixels (or sampling points) on a circle of radius
that form a circular neighborhood of the center pixel
. The unichrome LBP operation for the center pixel position
of
 is then deÔ¨Åned as follows [8], [28]:
LBP
if
otherwise(5)
where
(6)
and
 ,
 denotes the
pixel values of
 at
 , and
 denotes the pixel value at
 of
a circular neighborhood. Now, let
 denote an LBP image cor-
responding to
 . Note that each of the pixel values of
is Ô¨Ålled with LBP
 at its given pixel location
 .
SpeciÔ¨Åcally, if
 (i.e., for the case of uniform pattern), each
pixel of
 is labeled as a value of
 .
Otherwise, it will be labeled as constant
 . The
Fig. 3. Illustration of the LBP operation process performed on each color-com-
ponent image corresponding to respective channel in the /89/73 /81 color space. Note
that, for the sake of clear visualization, pixel values of LBP images are scaled
to the range of [0, 255].
aforementioned LBP operation performed on each color com-
ponent image from the
 color space is illustrated in Fig. 3.
To reÔ¨Çect and encode the local properties of a color compo-
nent image
 , we compute a regional LBP pattern histogram
for each local region
 . Note that every re-
gional histogram consists of
 bins, i.e.,
bins for the patterns with two transitions, two bins for the pat-terns with zero transitions, and one bin for all nonuniform pat-
terns. The LBP histogram for the
th local region
 is com-
puted as follows:
for
 (7)
where
if
 is true
if
 is false(8)
and
 denotes the
 th LBP value in the range of
and, therefore,
 values denote the number of pixels with
the LBP value
 within the local region
 .
Using (7), the regional LBP descriptor for
 can be de-
Ô¨Åned as follows:
(9)
Note that, in (9),
 provides regional LBP histogram in-
formation for
 . In order to keep the information about the
spatial relation of facial local regions, all of the
values are concatenated into a single column vector
termed as the unichrome LBP feature for
 , i.e.,
(10)
To obtain the opponent LBP features, each pair of different
color channels is used. For this, the center coordinate for aneighborhood and the neighbor itself are taken from different
CHOI et al. : COLOR LOCAL TEXTURE FEATURES FOR COLOR FACE RECOGNITION 1371
Fig. 4. (a) Three LBP pattern histograms each computed by performing unichrome LBP operation [deÔ¨Åned in (5)] on (enclosed by white boxes) the correspo nding
three local regions that are obtained from each color channel in the /89/73 /81 color space. (b) Three LBP pattern histograms each computed by performing the opponent
LBP operation [deÔ¨Åned in (11)] on a pair of two corresponding local regions that are obtained from two different color channels in the /89/73 /81 color space.
color channels. Based on (5), given a pair of
 and
 , the
opponent LBP operation is deÔ¨Åned as follows:
LBP
if
otherwise
for
 (11)
where
(12)
and
 denotes a pixel value of
 at its coordinate
(i.e., the center pixel of a circular neighborhood) and
denotes the pixel values of
that form a circular neighborhood of the center pixel
 of
 .
Using (7), (9), and (10), the opponent LBP feature denoted by
for the pair of
 and
 can be readily computed from
LBP
 shown in (11).Finally, the proposed color LBP (CLBP) feature for
 can
be deÔ¨Åned as follows:
for
 (13)
Note that, in (13),
 consists of one unichrome LBP
feature (extracted from
 ) and
 opponent LBP features
computed between
 (with a particular
 ) and
 different
(
 and
 ).
Fig. 4 shows six different LBP pattern histograms deÔ¨Åned
by (7). The Ô¨Årst three histograms are derived from performingunichrome LBP operations on three corresponding local re-gions, one for each channel in the
color space, whereas
the other three histograms are derived from performing op-ponent LBP operations on a pair of two corresponding localregions of two different color channels. It is easy to see thatthese histograms have different patterns of distribution ofLBP values so that they are complementary to one another forclassiÔ¨Åcation purposes. Indeed, the proposed
is likely
to contain much more discriminating information than what a
single grayscale LBP operation can provide.
1372 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 21, NO. 3, MARCH 2012
V. C OMBINING COLOR LOCAL TEXTURE FEATURES FOR FR
This section suggests the way of combining color local tex-
ture features for achieving the best FR performance. For sim-
plicity of notation, we denote a color local texture feature (ob-
tained from a color-component image
 )b y
 . Hence,
 can
represent either
 or
 deÔ¨Åned in Section IV. In ad-
dition, let
 be an unknown
 color face image (to be
identiÔ¨Åed or veriÔ¨Åed), which is denoted as a probe. In addition,let
be a gallery set consisting of prototype enrolled
color face images (to be recognized), each of which denoted by
, of known individuals (i.e.,
 ). Furthermore, without
any loss of generality, we denote the individual color local tex-ture features of
and
 by
 and
 , respectively, where
.
Given the
 color local texture features for recognizing
 ,
our goal is to attain the best FR performance by combining thembased on the information fusion theory [29] for pattern classiÔ¨Å-cation. Techniques for fusing multiple evidences (i.e., multiple
classiÔ¨Åcation results or multiple features) can be generally clas-
siÔ¨Åed into two classes, i.e., fusion at the ‚Äúfeature level‚Äù and fu-sion at the ‚Äúdecision level.‚Äù In the proposed method, we makeuse of the feature-level fusion strategy for two reasons: 1) Asreported in [26], ‚Äúfeature-level fusion‚Äù can generally achieve abetter classiÔ¨Åcation result compared with performance obtainedusing the ‚Äúdecision-level‚Äù fusion. 2) Our experimentation usingthe most popular FR DBs, such as the FRGC 2.0 data set, in-dicates that ‚Äúfeature-level‚Äù fusion methods achieve better FRperformance than the competing ‚Äúdecision-level‚Äù fusion frame-work (for more details, please refer to the experimental assess-ment in Section VI-B).
Following the aforementioned observation, we adopt infor-
mation fusion techniques performed at the level of features.Using feature-level information fusion techniques, color local
texture features of
(or
 ) could be simply concatenated into
a longer global feature vector. However, it should be noted thatdirectly applying a nearest-neighbor (NN) classiÔ¨Åer to such aconcatenated feature vector could suffer from the degradation inthe FR performance caused by the high dimensionality and theredundant information. To overcome the aforementioned lim-itation, low-dimensional feature extraction techniques are em-ployed. Let us denote the
th face feature extractor by
 (e.g.,
PCA) in order to extract a low-dimensional feature of the colorlocal texture feature
. Note that
 can be formed with a
training set of color local texture features
 , all of which are
computed from the
 th color-component training images
 .
Then, the low-dimensional features of
 and
 are obtained
as follows (using the corresponding
 ):
(14)
where
 ,
 ,
 denotes
 -dimensional real space,
and
 . Then,
 complementary low-dimensional
features, given by (14), are combined at the level of the fea-tures (by concatenating low-dimensional features in the columnorder), i.e.,
(15)
where
 ,
 , and
 .It is important to note that, in (15), each low-dimensional fea-
ture
 (or
 ) should be independently normalized in order to
have zero mean and unit variance prior to their concatenation.By doing so, we may avoid the negative effect of the magni-tude dominance of one low-dimensional feature over the others.
Consequently, the fusion method described in (15) allows for
effectively facilitating a complementary effect between its dif-ferent components, leading to positively affecting the classiÔ¨Åca-tion performance.
To perform FR tasks (identiÔ¨Åcation or veriÔ¨Åcation) on
,a n
NN classiÔ¨Åer is then applied to determine the identity of
 by
matching the corresponding
 with the closest
 [20].
VI. E XPERIMENTS
An extensive experimental study was carried out to investi-
gate the effectiveness of the proposed color local texture fea-
tures for FR. The following subsection describes in detail theface DBs used in our experiments, as well as our evaluationmethodology.
A. Experimental Setup and Condition
Four publicly available face DBs, i.e., CMU-PIE [12], Color
FERET [13], XM2VTSDB [14], SCface [48], and FRGC 2.0[15], were used to evaluate the proposed color local texture fea-tures. All facial images used in our experiments were manually
cropped from original images based on the locations of the two
eyes. The eye coordinates are those supplied with the originaldata set. Each cropped facial image was rescaled to the size of120
120 pixels (see Fig. 5). After alignment, each of the facial
images with a size of 120
 120 is divided into the 64 different
face local regions to compute the proposed color local texturefeatures. Thus, the size of each local region is 15
15 pixels.
To construct a face feature extractor
 (described in
Section V), Ô¨Åve popular low-dimensional feature extraction
techniques were used, i.e., PCA [30], Fisher‚Äôs linear discrimi-nant analysis (FLDA) [31], Enhanced Fisher linear discriminantModel (EFM) [7], eigenfeature regularization and extraction
(ERE) [32], and kernel direct discriminant analysis (KDDA)
[33]. Note that a radial basis function was adopted as the kernelfunction for implementing the KDDA [33]. The PCA and theFLDA are commonly used as benchmarks for the evaluation
of the performance of FR algorithms [13]. The EFM is one of
the popular face feature extraction techniques based on lineardiscriminant analysis (LDA). In particular, it has been reportedin [7] that the EFM guarantees a better generalization perfor-
mance than FLDA by avoiding the overÔ¨Åtting problem. The
ERE outperforms all other LDA-based FR methods discussedin [32]. In [33], the KDDA shows considerably a better FRperformance compared with other popular kernel-based FR
methods (such as kernel PCA and kernel LDA). As for the
NN classiÔ¨Åers, the Euclidean distance was used for the FLDAand the KDDA, whereas the cosine distance measure was usedfor the EFM and the ERE (as recommended in [7] and [32],
respectively), and the Mahalanobis distance [34] was employed
for the PCA.
In our experiments, Ô¨Åve different scales and eight different
orientations, which are most widely used for FR [6], [7], are em-
ployed to construct a set of Gabor Ô¨Ålter banks deÔ¨Åned in (1). In
CHOI et al. : COLOR LOCAL TEXTURE FEATURES FOR COLOR FACE RECOGNITION 1373
addition, we downsampled each Gabor wavelet representation
by a factor of 36 for the purpose of reducing the dimensionalityand the computational complexity [7]. The downsampled ver-
sion of each Gabor representation is then used to produce the
corresponding column vector. Note that, as described in [7], theFR performance difference is quite marginal, as compared withthe case where downsampling is not performed. In the case of
LBP operations shown in (5) and (11), following the recommen-
dation of [8], we adopted the LBP
operator (i.e.,
and
 ).
In order to generate color-component images, two effective
color representations (devised for the purpose of FR) were used
in our experiments, i.e., the normalized hybrid ‚Äú
 ‚Äù [23] and
(
taken from
 ,
taken from
 , and
 taken
from
 ) color representations [20]. Note that, to derive the
normalized hybrid ‚Äú
 ‚Äù color representation, we employed
the across-color-component normalization technique suggestedby [23] as this method achieves the best FR performance of allnormalized color spaces evaluated in [23]. In addition,
color representation shows the best FR performance of all pos-sible color-component image conÔ¨Ågurations discussed in [20].
In our experiments, the cumulative match curve [13] and the
face veriÔ¨Åcation rate (FVR) at the false accept rate (FAR) [35]
were used for measuring the identiÔ¨Åcation and veriÔ¨Åcation FR
performance, respectively. Note that, in general, the FR perfor-mance relies on the number of low-dimensional features used[32], [36]. Therefore, in order to realize a fair comparison, the
best found correct recognition rate (BstCRR) proposed in [36]
was selected as the identiÔ¨Åcation rate. In addition, in all exper-
iments, the frontal-view images with neutral illumination andexpression were used to build the gallery set.
B. Effectiveness of Color Local Texture Features for FR
Here, we compare the FR using the proposed color local tex-
ture features with the FR using grayscale texture features and the
FR using only color information. For FR using grayscale tex-ture features, only luminance information is applied to extractgrayscale Gabor wavelet or LBP features. In our experiments,
the ‚Äú
‚Äù channel [20] from
 color space was adopted for
extracting grayscale texture features. For FR using only colorinformation, given
different color component images, indi-
vidual color-component vectors were Ô¨Årst generated in the form
of a column vector by lexicographic ordering of the pixel ele-
ments of corresponding color-component images [20]. In orderto guarantee fair and stable comparisons with the method usingour color local texture features, the low-dimensional features of
these
color-component vectors were then combined at the
level of features in the same way as described in (15) (but notusing Gabor or LBP). The resulting concatenated features areapplied to FR.
In order to validate the advantage of making use of the fea-
ture-level fusion approach to combining color local texture fea-tures, we report the experimental results obtained using the pro-posed feature-level fusion, as well as using the decision-level
fusion. For the case of using the decision-level fusion, a sig-
moid function [37] followed by a sum normalization method[37] was used to normalize the matching scores (e.g., distance
Fig. 5. (a) Examples of the facial images with Ô¨Çash illumination from the
CMU-PIE DB. (b) Examples of facial images with uncontrolled illumination
condition from the XM2VTSDB. (c) Examples of facial images with pose
variations from the Color FERET DB. (d) Examples of facial images with
variation in face resolutions from the Color FERET DB.
scores). Furthermore, the sum rule [29] was adopted for com-
bining these multiple matching scores at the decision level. It
has been shown in [29] that the sum rule achieves the best clas-siÔ¨Åcation performance in comparison with other decision-levelfusion strategies, such as product and median rules.
In the following three subsections, we present comparative
experimental results to validate the effectiveness of color localtexture features for FR under variations in illumination, pose,and resolution, respectively.
Results on Illumination Variation: In order to validate the ro-
bustness of the proposed color local texture features (namely,CLGW and CLBP) against extensive variations in illumination,the CMU-PIE and XM2VTSDB face DBs were used. For this,
1428 frontal images of 68 subjects (21 images per subject) were
collected from the CMU-PIE; the facial images for each sub-ject have 21 different illumination variations (using the ‚Äúroomlighting off‚Äù condition). From the XM2VTSDB, 900 frontal
images of 100 subjects were obtained; each subject included
nine facial images captured with no control on severe illumi-nation variations. As a result, through using random partition,the training set consisted of 6 images
168 subjects, whereas
the remaining 1320 images were used to create a probe set. In
order to guarantee stable experimental results, 40 independent
runs of aforementioned random partitions were executed. Thus,the all the following results reported in Section VI-B were av-
eraged over 40 runs.
Fig. 5(a) and (b) shows examples of facial images used in this
experiment. We can see that the face images shown in Fig. 5 aresubject to severe illumination changes caused by the interrup-
tion of background colored light and cast shadow.
Table I shows the average rank-1 identiÔ¨Åcation rates of the FR
using the two proposed CLGW and CLBP features and the FR
1374 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 21, NO. 3, MARCH 2012
TABLE I
AVERAGE RANK-1 I DENTIFICATION RATES (INPERCENT )AND CORRESPONDING STANDARD DEVIATIONS FOR EV ALUATING
THE EFFECTIVENESS OF COLOR LOCAL TEXTURE FEATURES ON ILLUMINATION VARIATION .NOTE THAT /90/82 /71 AND /82/81 /67
COLOR SPACES WEREEMPLOYED . (a) CLBP. (b) CLGW
Note that FL corresponds to the feature-level fusion method, whereas DL corresponds to the decision-level fusion
method. In addition, the bold value denotes the best result of FR approaches in each low-dimensional feature extractiontechnique, and the similar notations are also used in the following tables.
methods using grayscale Gabor and LBP features, and the FR
methods using only color information. From Table I, we have
the following observations:
1) Compared with both the FR using grayscale texture fea-
tures and the FR using only color information, the FR
performances of two proposed CLGW and CLBP are
clearly better for all low-dimensional feature extractiontechniques. This result can validate the complementaryeffect, supplemented with additional information by the
virtue of fusing chromaticity and texture features.
2) The color local texture features perform clearly better than
their grayscale counterparts. In particular, the grayscaleLBP severely suffers from the nonmonotonic illumination
changes, as compared with the grayscale Gabor. How-
ever, the proposed CLBP is much more robust againstillumination changes than its grayscale counterpart. Theperformance increases, as compared with the grayscale
LBP, can be increased up to around 15%, 22%, 18%, and
17% for the PCA, the FLDA, the KDDA, and the ERE,respectively.
3) For the approach of combining color local texture features,
the feature-level fusion method yields a considerably better
FR performance than the decision-level fusion method.
Results on Pose Variation: We further assess the usefulness
of the color local texture features under moderate pose varia-
tions. A total of 1378 face images of 107 subjects were col-
lected from the Color FERET face DB. It should be noted thatonly the rotated face images that both eyes can be reliably iden-tiÔ¨Åed for normalization were collected. The facial images used
include Ô¨Åve different pose angles ranging from
to
[see Fig. 5(c)]. Moreover, it should be noted that all the images
have neutral expression and illumination. By using random par-tition, the training set consisted of 535 images (5 images
107subjects), whereas the probe set contained the remaining 843
images of the same 107 subjects.
The comparison results are described in Table II. It is shown
that the proposed CLFG and CLBP outperform both their oppo-nent grayscale texture features and the associated FR methods
using only color information for all feature extraction methods
used. In particular, grayscale Gabor is found to be much weakerthan grayscale LBP in terms of reliably recognizing face im-ages with pose changes. However, with CLGW (for the case of
using
color space and FL), we can attain improvement
in the identiÔ¨Åcation rate of about 9%, 12%, 10%, and 8%, inthe order of the PCA, the FLDA, the KDDA, and the ERE, re-spectively. Furthermore, for the case of using CLBP, as com-
pared with grayscale LBP, identiÔ¨Åcation rates can be improved
by 12%, 8%, 7%, and 6% for the PCA, the FLDA, the KDDA,and the ERE, respectively.
The underlying reason for this is that our color local texture
features are designed to exploit the discriminative information
derived from spatiochromatic texture patterns residing within
local face regions . Hence, when recognizing face images taken
under moderate pose variations, discriminating color texture
patterns arising from local face regions are more likely to be
preserved for a FR purpose, as compared with the texture pat-terns globally extracted from the entire face region. In addition,it has been reported in [49] and [50] that color information is
helpful for improving recognition/classiÔ¨Åcation rates for a va-
riety of objects under modest changes in the orientation. Con-sequently, based on the results in Table II, combining the colorand texture feature information can lead to the improved recog-
nition of face images captured with moderate pose changes.
Results on Face Resolution Variation: Some real-life FR ap-
plications (particularly FR on video surveillance or mobile de-vices) are likely to be confronted with low-resolution and low-
CHOI et al. : COLOR LOCAL TEXTURE FEATURES FOR COLOR FACE RECOGNITION 1375
TABLE II
AVERAGE RANK-1 I DENTIFICATION RATES (INPERCENT )AND CORRESPONDING STANDARD DEVIATIONS FOR EV ALUATING
THE EFFECTIVENESS OF COLOR LOCAL TEXTURE FEATURES ON POSE CHANGE .NOTE THAT /90/82 /71 AND /82/81 /67
COLOR SPACES WEREEMPLOYED . (a) CLBP. (b) CLGW
quality face images [38], [39]. The goal of this experiment is
to measure the degree to which the proposed color local texturefeatures are robust against small-resolution face images, rela-
tive to grayscale texture features. To this end, 1428 frontal-view
images of 68 subjects were selected from CMU-PIE; for onesubject, facial images have 21 different lighting variations. Fur-thermore, from the Color FERET, the 1120 frontal-view images
of 140 subjects (eight samples per subject) were chosen from
thefa,fb,fc, and dup1 sets. The training set consisted of 1040
images (5 samples
208 subjects), whereas the remaining 1508
facial images were used for the probe set. To obtain facial im-
ages with varying face resolutions, we performed resizing over
the aforementioned original collected data set. Fig. 5(d) showsexamples of facial images containing face resolution variations.We took original high-resolution face images of size of 120
120 pixels, synthetically blurred them with a Gaussian kernel[40], and then downsampled them in order to simulate a lowerresolution effect as closely as possible to practical camera lens.As a result, Ô¨Åve different face resolutions of 120
120, 86
86, 44
 44, 32
 32, 20
 20 pixels were generated.
In real-life surveillance-like FR applications, it is reason-
able to assume that high-resolution face images are chosen astraining and gallery images. On the other hand, the probe to be
tested may have lower and various face resolutions [20], [38].
Hence, in this experiment, the face resolution of training and
gallery images was Ô¨Åxed as 120
120 pixels, whereas the reso-
lution of probe was varied to Ô¨Åve different resolutions, as shown
in Fig. 5(d). In addition, in order to match a low-resolution probe
to a high-resolution gallery face, the probe has been upsampledby using a cubic interpolation technique before recognition.
Fig. 6 shows the results obtained for color local texture and
grayscale texture features that are extracted from face images
of different face resolutions. It can be seen that, with the de-crease in the face resolution, the FR performances of grayscaleGabor and grayscale LBP features drop much more quickly thanthose of their opponent color local texture features for both the
KDDA and the ERE. Note that local texture features generallyencode detailed variations within some local areas in the face.
Thus, some facial details contained in the grayscale face image
may be lost at small resolution, yielding the severe degradationin the FR performance. On the contrary, the chromatic contrastsensitivity information can be generally concentrated on low
spatial frequencies [41], [42]. Hence, the texture patterns ex-
tracted from chromaticity components are able to be reservedat small resolution. For this reason, the texture features of chro-maticity components can compensate the decreased FR perfor-
mance caused by a low-resolution face image. The observation
that color local texture features are much more robust to varia-tion in face resolution than grayscale texture features is a veryencouraging result, considering that extracting reliable features
from low-quality face images is a critical problem in emerging
FR applications such as FR from video and FR on mobile de-vices [43], [44].
C. Comparisons With Other Color FR Methods
Here, to compare the best results of our method with the re-
sults obtained by other state-of the-art color FR methods, thefollowing two testbeds have been constructed using four dif-
ferent face DBs:
‚Ä¢ Data set 1: A total of 3612 facial images from 341 subjects
were collected from three public face DBs to constructthe training and probe sets. During collection phase,
1428 frontal-view images of 68 subjects were selected
from CMU-PIE; for one subject, facial images had 21different lighting variations. From Color FERET, the 1120frontal-view images of 140 subjects (eight samples per
subject) were chosen from ‚Äúfa,‚Äù ‚Äúfb,‚Äù ‚Äúfc,‚Äù and ‚Äúdup1‚Äù
sets. From XM2VTSDB, 1064 frontal-view images of133 subjects were obtained from two different sessions;each subject included eight facial images that contained
1376 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 21, NO. 3, MARCH 2012
Fig. 6. Rank-1 identiÔ¨Åcation rates for demonstrating the robustness of color local texture features on face resolution variations. Note that the /90/82 /71 color space
and the feature-level fusion approach are used for CLBP and CLGW. (a) KDDA. (b) ERE.
Fig. 7. Comparisons with other state-of-the-art color FR methods using ‚ÄúData set 1.‚Äù Note that the /90/82 /71 color space and the feature-level fusion approach are
used for the proposed methods using CLBP (or CLGW). (a) EFM. (b) ERE.
illumination and resolution variations. The training set is
randomly constructed to include 1705 images (5 samples
341 subjects), whereas the remaining 1907 images are
for the probe set.
‚Ä¢ Data set 2: The objective of constructing this data set is
to justify the effectiveness of the proposed method whentraining and testing images have been captured usingdifferent cameras. For this purpose, 1950 facial images of
130 different subjects (15 images
130 subjects) were
collected from the SCface DB [48] to construct the trainingand probe sets. Note that all of the collected facial imagesfor each subject are taken with Ô¨Åve different surveillance
cameras, each labeled ‚Äúcam1‚Äù through ‚Äúcam5‚Äù [48].
SpeciÔ¨Åcally, there are three images per subject for eachcamera, captured at three different distances (4.20, 2.60,and 1.00 m). Using random partition, the training set
consisted of 910 images (7 images
130 subjects), and
the remaining 1040 facial images were used for the probeset. In addition, we constructed a gallery set consistingof 130 frontal facial mug shots (taken with a consumer
digital camera) of 130 different subjects.
In our experiments, we compare the proposed methods with
six state-of-the-art color FR methods [21]‚Äì[24], [45], [46]. Notethat, when implementing other color FR methods, we have
strictly followed the implementation guidelines (and parameter
settings) listed in the respective published paper for the purposeof stable comparisons. SpeciÔ¨Åcally, for the method in [45],the hybrid ‚Äú
‚Äù color space (
 from
 and
 and
from
 color spaces, respectively) was used as proposed. In
addition, as recommended by [45], the same size of masks usedto select frequency sets in the frequency domain was used. Forthe method in [23], the normalized hybrid ‚Äú
‚Äù color space
using the across-color-component normalization technique
[23] was used as this method achieves the best FR performanceof all normalized color spaces evaluated. For the methodin [21], we implemented its extended version based on the
color space. Furthermore, following the same parameter
values as used in [21], the initial value of the CID algorithmand the convergence threshold were set to ‚Äú[1/3,1/3,1/3]‚Äùand ‚Äú0.1,‚Äù respectively. For the method in [22], the Comon
independent-component-analysis algorithm [46] was used in
our experiment to compute mutual information between colorcomponents and high-order statistics, as suggested in [22]. Forthe method in [24], the hybrid ‚Äú
‚Äù color space was used
as proposed. Moreover, strictly following [24], we used the
same method for partitioning the Gabor image representationinto an ensemble of patches, as well as for determining DCTmask sizes used to select various DCT feature sets.
Fig. 7 shows the comparison results. Note that the experi-
mental results shown in Fig. 7 are obtained using ‚ÄúData set1.‚Äù As shown in Fig. 7, the proposed methods using CLBP (orCLGW) achieve better or comparable FR performance com-
pared with the other six color FR methods, for both the EFM
and the ERE. In particular, the proposed method (for the caseof using the CLBP and the feature-level fusion) attains the bestrank-1 identiÔ¨Åcation rates of up to 94.15% and 97.82% for the
CHOI et al. : COLOR LOCAL TEXTURE FEATURES FOR COLOR FACE RECOGNITION 1377
TABLE III
COMPARISONS OF AVERAGE RANK-1 I DENTIFICATION RATES (INPERCENT )TOSHOW THE EFFECTIVENESS OF THE PROPOSED METHODS
WHEN COLOR FACE IMAGES CAPTURED WITHDIFFERENT CAMERAS AREUSED DURING THE TRAINING AND
TESTING STAGES .NOTETHAT EXPERIMENTAL RESULTS AREOBTAINED USING ‚ÄúDATA SET 2‚Äù
EFM and the ERE, respectively, of all color FR methods under
consideration. These comparison results validate the feasibility
of color FR methods using the proposed CLBP and CLGWfeatures.
Table III presents comparative results on the ‚ÄúData set 2.‚Äù
As shown in Table III, both CLBP and CLGW considerably
outperform their grayscale counterparts (i.e., grayscale LBPand grayscale Gabor), respectively, for the EFM and the ERE.For instance, in the case of the ERE, 15.44% and 12.98%
improvements in the rank-1 identiÔ¨Åcation rate can be attained
from the proposed CLBP and CLGW, respectively. In addition,from the comparison, we can see that the results of the proposedmethods are better than those obtained from the other six color
FR methods. Consequently, the results shown in Table III con-
Ô¨Årm the feasibility of our methods in terms of improving the FRperformance when color face images captured with differentcameras are used during the training and testing stages.
D. Evaluation of Proposed Methods on the FRGC 2.0 Data Set
In addition, we conducted comparative experiments on the
FRGC 2.0 data set to further evaluate our color local texture
features. Here, ‚ÄúFRGC Experiment 4‚Äù is chosen to assess theproposed method because FRGC Experiment 4 has been re-ported to be the most challenging FRGC experiment [22]. Note
that direct comparisons are made with other state-of-the-art re-
sults recently reported by other researchers on these FRGC 2.0data set. As such, all the results for the comparison are directlycited from papers recently published. It should be also noted
that we report the veriÔ¨Åcation rates at FAR
(which is
corresponding to ROC III curve [15]) for the purpose of directcomparison with other color FR methods, since recent publishedstudies have reported the veriÔ¨Åcation performance of competing
state-of-the-art solutions using FAR
. This guarantees
fair and reliable comparisons of our methods with other colorFR methods. In addition, note that the EFM was adopted for aface feature extractor in our method because all other color FR
methods make use of the EFM for extracting low-dimensional
features.
Table IV tabulates the comparative results on the FRGC 2.0
data set. The results show that color FR methods using ourTABLE IV
COMPARISONS WITHOTHER STATE -OF-THE-ARTCOLOR FR M ETHODS ON
‚ÄúFRGC 2.0 E XPERIMENT 4.‚Äù N OTETHAT /122-SCORE NORMALIZATION [26] I S
USED TO COMPUTE FVR AND FAR
CLBP and CLGW work comparatively well with the published
results on the FRGC 2.0 data set. In particular, the veriÔ¨Åcation
performance of our method using CLBP, i.e., 91.01%, is quitecomparable with the most recent best performance, i.e., 92.43%,reported in [24]. Note that, differing from the results reported in
Fig. 7 and Table III, the color FR solution proposed in [24] per-
forms the best, followed in order by the proposed CLBP andCLGW, as shown in Table IV. This is mainly due to the factthat implementation parameters recommended in [24] may be
optimally tuned up for the purpose of recognizing face images
within the FRGC 2.0 data set but not tuned up for other FR DBsused to obtain the results in Fig. 7 and Table III.
E. Comparisons of Unichrome and Opponent Texture Parts
Recall from Section IV that the proposed color local texture
features consist of two parts, i.e., unichrome (or channelwise)
and opponent texture features. Here, we have performed addi-tional experiments to investigate which part (i.e., unichromeversus opponent feature representations) contributes the most
to the FR performance. In addition, we have investigated the
use of ‚Äúcombined (or fused)‚Äù unichrome and opponent tex-ture features and compared against a framework separately
utilizing unichrome and opponent texture features. To this
end, the face image data set collected from CMU-PIE, Color
FERET, and XM2VTSDB were used in our experimentation. Adetailed description of the experimental data set used is givenin Section VI-C.
1378 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 21, NO. 3, MARCH 2012
Fig. 8. Comparisons of rank-1 identiÔ¨Åcation rates obtained for ‚Äúcombined‚Äù (or fused) unichrome and opponent texture features with those obtained us ing sepa-
rately unichrome and opponent texture features. Note that the /90/82 /71 color space and the feature-level fusion approach are used. (a) EFM. (b) ERE.
The results are given in Fig. 8. Note that, for comparison
purposes, the identiÔ¨Åcation rates obtained for grayscale tex-ture features are provided as the baseline performance. From
Fig. 8(a) and (b), the following three observations can be made:
1) the unichrome and opponent texture parts contribute simi-larly for FR; 2) by combining unichrome and opponent textureparts, the FR performances can be impressively improved, in-
dicating that the unichrome and opponent texture parts are able
to provide different information and to be mutually compensa-tional in terms of enhancing the FR performance; and 3) bothunichrome and opponent texture parts considerably outperform
the baseline grayscale texture (although separately used).
VII. C
ONCLUSION
This paper has investigated the contribution of color to ex-
isting texture features for improving the FR performance. This
paper has examined how to effectively exploit the discrimi-
nating information by combining color and texture information,as well as its fusion approach. To this end, under the frameworkof local pattern encoding, we have proposed two effective color
local texture features, i.e., CLBP and CLGW. Furthermore, in
order to combine multiple color local texture features, we havesuggested the feature-level fusion approach, which maximizestheir complementary effect in the context of FR. We exper-
imentally reveal that color FR methods based on CLBP and
CLGW signiÔ¨Åcantly outperform the methods relying only ontexture or color information. One particularly important resultis that our color local texture features allows for a signiÔ¨Åcant
improvement in the FR accuracy when recognizing face images
taken under a severe change in illumination (at least, in the datasets used in our experimentation), as well as for low-resolutionface images, as compared with their grayscale counterparts.
In addition, comparative experimental results show that color
FR methods using color local texture features yield better orcomparable FR performance compared with those obtainedusing other recent advanced color FR methods.
The experimental study in this paper has been limited to eval-
uating the effectiveness of color local texture features that areextracted from Ô¨Åxed color-component conÔ¨Åguration consistingof three components (such as
). It has been observed in
[47] that different color spaces possess distinct characteristics
and effectiveness in terms of discriminating power for the vi-sual-classiÔ¨Åcation task. Hence, for the future work, we will de-velop the method of selecting an optimal subset of color compo-nents (from a number of different color spaces), aiming to ob-
tain more discriminating color local texture features for FR. Tothis end, we plan to make use of multiclass boosting color fea-
ture selection algorithm proposed in [54] to Ô¨Ånd the best color
local texture features, each of which corresponding to a partic-ular local face region (such as eye or mouth).
In addition, in this paper, to perform color FR, multiple color
local texture features have been combined at the feature level
with uniform (equal) weights. For the future work, we willdevise a more effective weighted feature-level fusion schemewhere color local texture features are fused with different
weights. The weight of each color local texture feature could
be determined based on the degree to which color local texturefeatures contribute to the improvement in the FR performance.
R
EFERENCES
[1] K. W. Bowyer, ‚ÄúFace recognition technology: Security versus privacy,‚Äù
IEEE Technol. Soc. Mag. , vol. 23, no. 1, pp. 9‚Äì19, Spring 2004.
[2] A. K. Jain, A. Ross, and S. Prabhaker, ‚ÄúAn introduction to biometric
recognition,‚Äù IEEE Trans. Circuits Syst. Video Technol. , vol. 14, no. 1,
pp. 4‚Äì20, Jan. 2004.
[3] J. Y. Choi, W. De Neve, Y. M. Ro, and K. N. Plataniotis, ‚ÄúAutomatic
face annotation in photo collections using context-based unsupervisedclustering and face information fusion,‚Äù IEEE Trans. Circuits Syst.
Video Technol. , vol. 20, no. 10, pp. 1292‚Äì1309, Oct. 2010.
[4] J. Zou, Q. Ji, and G. Nagy, ‚ÄúA comparative study of local matching
approach for face recognition,‚Äù IEEE Trans. Image Process. , vol. 16,
no. 10, pp. 2617‚Äì2628, Oct. 2007.
[5] Y. Su, S. Shan, X. Chen, and W. Gao, ‚ÄúHierarchical ensemble of global
and local classiÔ¨Åers for face recognition,‚Äù IEEE Trans. Image Process. ,
vol. 18, no. 8, pp. 1885‚Äì1896, Aug. 2009.
[6] S. Xie, S. Shan, X. Chen, and J. Chen, ‚ÄúFusing local patterns of
Gabor magnitude and phase for face recognition,‚Äù IEEE Trans. Image
Process. , vol. 19, no. 5, pp. 1349‚Äì1361, May 2010.
[7] C. Liu and H. Wechsler, ‚ÄúGabor feature based classiÔ¨Åcation using the
enhanced Ô¨Åsher linear discriminant model for face recognition,‚Äù IEEE
Trans. Image Process. , vol. 11, no. 4, pp. 467‚Äì476, Apr. 2002.
[8] T. Ahonen, A. Hadid, and M. Pietikainen, ‚ÄúFace description with local
binary pattern: Application to face recognition,‚Äù IEEE Trans. Pattern
Anal. Mach. Intell. , vol. 28, no. 12, pp. 2037‚Äì2041, Dec. 2006.
[9] T. Maenpaa and M. Pietikainen, ‚ÄúClassiÔ¨Åcation with color and texture:
Jointly or separately,‚Äù Pattern Recognit. , vol. 37, no. 8, pp. 1629‚Äì1640,
Aug. 2004.
[10] A. Drimbarean and P. F. Whelan, ‚ÄúExperiments in colour texture anal-
ysis,‚Äù Pattern Recognit. Lett. , vol. 22, no. 10, pp. 1161‚Äì1167, Aug.
2001.
[11] G. Paschos, ‚ÄúPerceptually uniform color spaces for color texture anal-
ysis: An empirical evaluation,‚Äù IEEE Trans. Image Process. , vol. 10,
no. 6, pp. 932‚Äì937, Jun. 2001.
[12] T. Sim, S. Baker, and M. Bsat, ‚ÄúThe CMU pose, illumination, and ex-
pression database,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 25,
no. 12, pp. 1615‚Äì1618, Dec. 2003.
CHOI et al. : COLOR LOCAL TEXTURE FEATURES FOR COLOR FACE RECOGNITION 1379
[13] P. J. Phillips, H. Moon, S. A. Rizvi, and P. J. Rauss, ‚ÄúThe FERET
evaluation methodology for face recognition algorithms,‚Äù IEEE Trans.
Pattern Anal. Mach. Intell. , vol. 22, no. 10, pp. 1090‚Äì1104, Oct. 2000.
[14] K. Messer, J. Mastas, J. Kittler, J. Luettin, and G. Maitre,
‚ÄúXM2VTSDB: The extended M2VTS database,‚Äù in Proc. IEEE
Int. Conf. AVBPA , 1999, pp. 72‚Äì77.
[15] P. J. Phillips, P. J. Flynn, T. Scruggs, K. W. Bowyer, J. Chang, K.
Hoffman, J. Marques, J. Min, and W. Worek, ‚ÄúOverview of the facerecognition grand challenges,‚Äù in Proc. IEEE Int. Conf. Comput. Vis.
Pattern Recog. , 2005, pp. 947‚Äì954.
[16] W. Zhang, S. Shan, W. Gao, X. Chen, and H. Zhang, ‚ÄúLocal Gabor
binary pattern histogram sequence (LGBPHS): A novel non-statisticalmodel for face representation and recognition,‚Äù in Proc. IEEE ICCV ,
2005, pp. 786‚Äì791.
[17] L. Torres, J. Y. Reutter, and L. Lorente, ‚ÄúThe importance of the
color information in face recognition,‚Äù in Proc. IEEE ICIP , 1999, pp.
627‚Äì631.
[18] P. Shih and C. Liu, ‚ÄúComparative assessment of content-based face
image retrieval in different color spaces,‚Äù Int. J. Pattern Recog. Artif.
Intell. , vol. 19, no. 7, pp. 873‚Äì893, Nov. 2005.
[19] P. Shih and C. Liu, ‚ÄúImproving the face recognition grand challenge
baseline performance using color conÔ¨Ågurations across color spaces,‚ÄùinProc. IEEE ICIP , 2006, pp. 1001‚Äì1004.
[20] J. Y. Choi, Y. M. Ro, and K. N. Plataniotis, ‚ÄúColor face recognition for
degraded face images,‚Äù IEEE Trans. Syst., Man, Cybern. B, Cybern. ,
vol. 39, no. 5, pp. 1217‚Äì1230, Oct. 2009.
[21] J. Wang and C. Liu, ‚ÄúColor image discriminant models and algorithms
for face recognition,‚Äù IEEE Trans. Neural Netw. , vol. 19, no. 12, pp.
2088‚Äì2098, Dec. 2008.
[22] C. Liu, ‚ÄúLearning the uncorrelated, independent, and discriminating
color spaces for face recognition,‚Äù IEEE Trans. Inf. Forensics Security ,
vol. 3, no. 2, pp. 213‚Äì222, Jun. 2008.
[23] J. Yang, C. Liu, and L. Zhang, ‚ÄúColor space normalization: Enhancing
the discriminating power of color spaces for face recognition,‚Äù Pattern
Recognit. , vol. 35, no. 1, pp. 615‚Äì625, 2002.
[24] Z. Liu and C. Liu, ‚ÄúFusion of color, local spatial and global frequency
information for face recognition,‚Äù Pattern Recognit. , vol. 43, no. 8, pp.
2882‚Äì2890, Aug. 2010.
[25] A. Jain and G. Healey, ‚ÄúA multiscale representation including op-
ponent color features for texture recognition,‚Äù IEEE Trans. Image
Process. , vol. 7, no. 1, pp. 124‚Äì128, Jan. 1998.
[26] A. Jain, K. Nandakumar, and A. Ross, ‚ÄúScore normalization in mul-
timodal biometric systems,‚Äù Pattern Recognit. , vol. 38, no. 12, pp.
2270‚Äì2285, Dec. 2005.
[27] R. Lukac and K. N. Plataniotis , Color Image Processing: Methods and
Application . New York: CRC Press, 2007.
[28] T. Ojala, M. Pietikainen, and T. Maenpaa, ‚ÄúMultiresolution gray-scale
and rotation invariant texture classiÔ¨Åcation with local binary patterns,‚ÄùIEEE Trans. Pattern Anal. Mach. Intell. , vol. 24, no. 7, pp. 971‚Äì987,
Jul. 2002.
[29] J. Kittler, ‚ÄúOn combining classiÔ¨Åers,‚Äù IEEE Trans. Pattern Anal. Mach.
Intell. , vol. 20, no. 3, pp. 226‚Äì239, Mar. 1998.
[30] M. A. Turk and A. P. Pentland, ‚ÄúEigenfaces for recognition,‚Äù J. Cogn.
Neurosci. , vol. 3, no. 1, pp. 71‚Äì86, 1991.
[31] P. N. Belhumeur, J. P. Hesphanha, and D. J. Kriegman, ‚ÄúEigenfaces vs.
Fisherfaces: Recognition using class speciÔ¨Åc linear projection,‚Äù IEEE
Trans. Pattern Anal. Mach. Intell. , vol. 19, no. 7, pp. 711‚Äì720, Jul.
1997.
[32] X. Jiang, B. Mandal, and A. Kot, ‚ÄúEigenfeature regularization and ex-
traction in face recognition,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. ,
vol. 30, no. 3, pp. 383‚Äì394, Mar. 2008.
[33] J. Lu, K. N. Plataniotis, and A. N. Venetsanopoulos, ‚ÄúFace recogni-
tion using kernel direct discriminant analysis algorithms,‚Äù IEEE Trans.
Neural Netw. , vol. 14, no. 1, pp. 117‚Äì126, Jan. 2003.
[34] V. Perlibakas, ‚ÄúDistance measures for PCA-based face recognition,‚Äù
Pattern Recognit. Lett. , vol. 25, no. 6, pp. 711‚Äì724, Apr. 2004.
[35] P. J. Grother, R. J. Micheals, and P. J. Phillips, ‚ÄúFace recognition
vendor test 2002 performance metrices,‚Äù in Proc. Int. Conf. Audio
Video-Based Biometric Person Authentication , 2003, vol. 2688, LNCS,
pp. 937‚Äì945.
[36] J. Lu, K. N. Plataniotis, and A. N. Venetsanopoulos, ‚ÄúRegularized dis-
criminant analysis for the small sample size problem in face recog-nition,‚Äù Pattern Recognit. Lett. , vol. 24, no. 16, pp. 3079‚Äì3087, Dec.
2003.[37] C. L. Liu, ‚ÄúClassiÔ¨Åer combination based on conÔ¨Ådence transforma-
tion,‚Äù Pattern Recognit. , vol. 38, no. 1, pp. 11‚Äì28, Jan. 2005.
[38] P. H. Hennings-Yeomans, B. V. K. Vijaya Kumar, and S. Baker, ‚ÄúRo-
bust low-resolution face identiÔ¨Åcation and veriÔ¨Åcation using high-res-
olution features,‚Äù in Proc. IEEE ICIP , 2009, pp. 33‚Äì36.
[39] W. Zhao, R. Chellappa, P. J. Phillips, and A. Rosenfeld, ‚ÄúFace recog-
nition: A literature survey,‚Äù ACM Comput. Surv. , vol. 35, no. 4, pp.
399‚Äì458, Dec. 2003.
[40] S. Baker and T. Kanade, ‚ÄúLimits on super-resolution and how to break
them,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 24, no. 9, pp.
1167‚Äì1183, Sep. 2002.
[41] D. H. Kelly, ‚ÄúSpatiotemporal variation of chromatic and achromatic
contrast thresholds,‚Äù J. Opt. Soc. Amer. , vol. 73, no. 6, pp. 742‚Äì749,
Jun. 1983.
[42] J. B. Derrico and G. Buchsbaum, ‚ÄúA computational model of spati-
ochromatic image coding in early vision,‚Äù J. Vis. Commun. Image Rep-
resent. , vol. 2, pp. 31‚Äì38, 1991.
[43] M. Kim, S. Kumar, V. Pavlovic, and H. Rowley, ‚ÄúFace tracking and
recognition with visual constraints in real-world videos,‚Äù in Proc. IEEE
Int. Conf. CVPR , 2008, pp. 1‚Äì8.
[44] S. Mukherjee, Z. Chen, A. Gangopadhyay, and S. Russell, ‚ÄúA secure
face recognition system for mobile-devices without the need of decryp-tion,‚Äù in Proc. Workshop SKM , 2008, pp. 11‚Äì16.
[45] Z. Liu and C. Liu, ‚ÄúA hybrid color and frequency features method for
face recognition,‚Äù IEEE Trans. Image Process. , vol. 17, no. 10, pp.
1975‚Äì1980, Oct. 2008.
[46] P. Comon, ‚ÄúIndependent component analysis, a new concept?,‚Äù Signal
Process. , vol. 36, no. 3, pp. 287‚Äì314, Apr. 1994.
[47] H. Stokman and T. Gevers, ‚ÄúSelection and fusion of color models for
image feature detection,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol.
29, no. 3, pp. 371‚Äì381, Mar. 2007.
[48] M. Grgic, K. Delac, and S. Grgic, ‚ÄúSCface‚ÄîSurveillance cameras face
database,‚Äù Multimedia Tools Appl. , vol. 51, no. 3, pp. 863‚Äì879, Feb.
2011.
[49] G. D. Finlayson, S. S. Chatterjee, and B. V. Funt, ‚ÄúColor angular in-
dexing,‚Äù in Proc. ECCV , 1996, pp. 16‚Äì27.
[50] S. S. Chatterjee, ‚ÄúColor invariant object and texture recognition,‚Äù M.S.
thesis, Simon Fraser Univ., Burnaby, BC, Canada, 1995.
[51] G. Healey and L. Wang, ‚ÄúIllumination-invariant recognition of texture
in color images,‚Äù J. Opt. Soc. Amer. A, Opt. Image Sci. Vis. , vol. 12, no.
9, pp. 1877‚Äì1883, 1995.
[52] C. H. Chan, J. Kittler, and K. Messer, ‚ÄúMultispectral local binary pat-
tern histogram for component-based color face veriÔ¨Åcation,‚Äù in Proc.
IEEE Int. Conf. BTAS , 2007, pp. 1‚Äì7.
[53] C. Zhu, C. E. Bichot, and L. Chen, ‚ÄúMulti-scale color local binary pat-
terns for visual object classes recognition,‚Äù in Proc. IEEE ICPR , 2010,
pp. 3065‚Äì3068.
[54] J. Y. Choi, Y. M. Ro, and K. N. Plataniotis, ‚ÄúBoosting color feature
selection for color face recognition,‚Äù IEEE Trans. Image Process. , vol.
20, no. 5, pp. 1‚Äì10, May 2011.
Jae Young Choi received the B.S. degree from
Kwangwoon University, Seoul, South Korea, in
2004 and the M.S. and Ph.D. degrees from the Korea
Advanced Institute of Science and Technology
(KAIST), Daejeon, South Korea, in 2008 and 2011,
respectively.
In 2008, he was a visiting researcher at the
University of Toronto, Toronto, ON, Canada. He
is currently a Postdoctoral Researcher with theUniversity of Toronto and KAIST. His research
interests include face recognition/detection/tracking,
medical image processing, pattern recognition, machine learning, computer
vision, and the Social Web. He is the author or co-author of over 30 refereed
research publications in the aforementioned research areas.
Dr. Choi was the recipient of the Samsung HumanTech Thesis Prize in 2010.
1380 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 21, NO. 3, MARCH 2012
Yong Man Ro (S‚Äô85‚ÄìM‚Äô92‚ÄìSM‚Äô98) received
the B.S. degree from Yonsei University, Seoul,
Korea, and the M.S. and Ph.D. degrees from Korea
Advanced Institute of Science and Technology
(KAIST), Daejeon, Korea.
In 1987, he was a Visiting Researcher with Co-
lumbia University, NY , and from 1992 to 1995, he
was a Visiting Researcher with the University of Cali-
fornia, Irvine, and KAIST. He was a Research Fellow
with the University of California, Berkeley, and a Vis-
iting Professor with the University of Toronto in 1996
and 2007, respectively. He is currently a Full Professor with KAIST, where he
is directing the Image and Video Systems Laboratory. He participated in the
MPEG-7 and MPEG-21 international standardization efforts, contributing to the
deÔ¨Ånition of the MPEG-7 texture descriptor, the MPEG-21 Digital Item Adapta-tion visual impairment descriptors, and modality conversion. His research inter-
ests include image/video processing, multimedia adaptation, visual data mining,
image/video indexing, and multimedia security.
Dr. Ro was a recipient of the Young Investigator Finalist Award of the In-
ternational Society for Magnetic Resonance in Medicine in 1992 and the Sci-
entist Award in Korea in 2003. He served as a TPC member of international
conferences such as the International Workshop on Digital forensics and Wa-termarking (IWDW), the International Workshop on Image Analysis for Mul-
timedia Interactive Services, AIRS, and the IEEE Consumer Communications
and Networking Conference, and he was the co-program chair of IWDW 2004.
Konstantinos N. (Kostas) Plataniotis(S‚Äô90‚ÄìM‚Äô92‚ÄìSM‚Äô03) received the B.Eng. degree
in computer engineering from the University of
Patras, Patras, Greece, in 1988 and the M.S and
Ph.D. degrees in electrical engineering from Florida
Institute of Technology, Melbourne, in 1992 and1994, respectively.
He is a Professor with The Edward S. Rogers Sr.
Department of Electrical and Computer Engineering,
University of Toronto in Toronto, ON, Canada, and an
Adjunct Professor with the School of Computer Sci-
ence, Ryerson University, Canada. He is the Director of the Knowledge Media
Design Institute with the University of Toronto, where he is also the Director of
Research for the Identity, Privacy and Security Institute. His research interests
include biometrics, communications systems, multimedia systems, and signaland image processing.
Dr. Plataniotis is the Editor-in-Chief (2009‚Äì2011) for the IEEE S
IGNAL
PROCESSING LETTERS , a registered professional engineer in the province of
Ontario, and a member of the Technical Chamber of Greece. He is the 2005
recipient of the IEEE Canada‚Äôs Outstanding Engineering Educator Award ‚Äúfor
contributions to engineering education and inspirational guidance of graduate
students‚Äù and the corecipient of the 2006 IEEE T RANSACTIONS ON NEURAL
NETWORKS Outstanding Paper Award for the published paper in 2003 entitled
""Face Recognition Using Kernel Direct Discriminant Analysis Algorithms.
"
https://ieeexplore.ieee.org/document/7284873,"Deep Learning based FACS Action Unit Occurrence and Intensity
Estimation
Amogh Gudi, H. Emrah Tasli, Tim M. den Uyl, Andreas Maroulis
Vicarious Perception Technologies, Amsterdam, The Netherlands
Abstract ‚Äî Ground truth annotation of the occurrence and
intensity of FACS Action Unit (AU) activation requires great
amount of attention. The efforts towards achieving a common
platform for AU evaluation have been addressed in the FG 2015
Facial Expression Recognition and Analysis challenge (FERA
2015). Participants are invited to estimate AU occurrence
and intensity on a common benchmark dataset. Conventional
approaches towards achieving automated methods are to train
multiclass classiÔ¨Åers or to use regression models. In this paper,
we propose a novel application of a deep convolutional neural
network (CNN) to recognize AUs as part of FERA 2015
challenge. The 7 layer network is composed of 3 convolutional
layers and a max-pooling layer. The Ô¨Ånal fully connected layers
provide the classiÔ¨Åcation output. For the selected tasks of the
challenge, we have trained two different networks for the two
different datasets, where one focuses on the AU occurrences
and the other on both occurrences and intensities of the AUs.
The occurrence and intensity of AU activation are estimated
using speciÔ¨Åc neuron activations of the output layer. This way,
we are able to create a single network architecture that could
simultaneously be trained to produce binary and continuous
classiÔ¨Åcation output.
I. INTRODUCTION
A picture is worth a thousand words, but how many
words is the picture of a face worth? As humans, we make
a number of conscious and subconscious evaluations of a
person just by looking at their face. Identifying a person
can have a deÔ¨Åning inÔ¨Çuence on our conversation with them
based on past experiences; estimating a person‚Äôs age, and
making a judgement on their ethnicity, gender, etc. makes
us sensitive to their culture and habits. We also often form
opinions about that person (that are often highly prejudiced
and wrong); we analyse his or her facial expressions to
gauge their emotional state (e.g. happy, sad), and try to
identify non-verbal communication messages that they intent
to convey (e.g. love, threat). We use all of this information
when interacting with each other. In fact, it has been argued
that neonates, only 36 hours old, are able to interpret some
very basic emotions from faces and form preferences [1]. In
older humans, this ability is highly developed and forms one
of the most important skills for social and professional inter-
actions. Indeed, it is hard to imagine expression of humour,
love, appreciation, grief, enjoyment or regret without facial
expressions.
Human face constantly conveys information, both con-
sciously and subconsciously. However, as basic as it is for
humans to visually interpret this information, it is quite a
big challenge for machines. Such studies have been proven
important in many different Ô¨Åelds like psychology [2], hu-mancomputer interaction [3], visual expression monitoring
[4]- [5] and market research [6]. Conventional semantic facial
feature recognition and analysis techniques mostly suffer
from lack of robustness in real life scenarios.
This paper proposes a method to interpret semantic infor-
mation available in faces in an automated manner without
requiring manual design of feature detectors, using the
approach of Deep Learning. FG 2015 Facial Expression
Recognition and Analysis challenge invites researchers to
develop methods to estimate facial AU occurrence and inten-
sity. The proposed network architecture in this study has been
previously developed for detecting facial semantic features
(like emotions, age, gender, ethnicity etc.) present in faces
[7]. The same architecture is trained for the given tasks using
the ground truth datasets.
In the network architecture development, effects of various
hyper-parameters of deep neural networks have been inves-
tigated towards achieving an optimal conÔ¨Åguration. Further-
more, the relation between the effect of high-level concepts
on low level features is explored through an analysis of
the similarities in low-level descriptors of different semantic
features.
II. RELATED WORK
There has been previous efforts on detecting the facial AU
occurrence as well as the intensity. The study in [8] utilizes
the facial landmark displacement as a measure of intensity
dynamics. Regression based models [9] and multiclass clas-
siÔ¨Åers are also [10] utilized for such purposes.
Latest developments in computational hardware have re-
directed attention to deep neural networks for many computer
vision tasks. In the light of such studies deep learning
methods have been proposed as a highly promising approach
in solving such facial semantic feature recognition tasks. A
recent work for facial recognition by Taigman et al. [11]
has shown near-human performance using deep networks.
Thanks to the use of pre-processing steps like face alignment
and frontalization, and the use of a very large dataset, a
robust and invariant classiÔ¨Åer is produced that sets the state-
of-the-art in the Labeled Faces in the Wild dataset [12].
In the task of emotion recognition from faces, Tang [13]
sets the state-of-the-art on the Facial Expression Recog-
nition Challenge-2013 (FERC-2013). This is achieved by
implementing a two stage network: a convolutional network
trained in a supervised way on the Ô¨Årst stage, and a Support
Vector Machine as the second stage, which is trained on
the output of the Ô¨Årst stage. The initial global contrast
normalization has proven to be beneÔ¨Åcial and hence has also
been adopted as a pre-processing step in this paper.
III. DATASETS
FERA 2015 challenge provides two ground truth datasets:
the BP4D, and the SEMAINE Dataset. The BP4D Dataset
contains occurrence annotations for Action Units 1, 2, 4, 6,
7, 10, 12, 14, 15, 17, 23, as well as intensity annotations for
Action Units 6, 10, 12, 14, 17. The SEMAINE data contains
occurrence annotations for Action Units 2, 12, 17, 25, 28,
45. Details of the dataset are explained in the FERA 2015
baseline paper [14].
IV. METHOD
In this section, we describe the proposed method used
to detect the occurrences and intensities of Facial AUs
from face images. The algorithmic pipeline depicted in 2 is
primarily based on the master‚Äôs thesis work [7]. The whole
pipeline is composed of the initial pre-processing step and
followed by the classiÔ¨Åcation step using the deep neural
network.
A. Pre-Processing
The AU classiÔ¨Åer (the deep network) is designed to accept
frontal images of cropped faces as input. Therefore, we apply
an initial pre-processing step to detect the face and handle
pose variations. In an effort to normalize the input intensity
differences due to lighting conditions, we also apply a global
contrast normalization in this initial step. The basic pipeline
of these pre-processing steps are as follows:
Face Location Normalization :
We Ô¨Ånd faces in the images using a face detection
algorithm (using the facial points based on the method
described in [15], and as provided by the challenge)
and extract the crop of the face such that the image is
centered around the face.
We perform in-plane rotation in order to remove tilt in
the X-Y plane. This is achieved by enforcing the line
connecting the two eyes to be horizontal.
We resize the image such that the approximate scale
of the face is constant. This is done by ensuring the
distance between the two eyes to be constant. The Ô¨Ånal
resolution of the face-crop is set to 4848pixels.
Global Contrast Normalization :
We convert the face-crop to 32-bit grayscale images
to minimize the computational complexity of using
multiple color channels.
We normalize the pixel values by the standard deviation
of pixel values (face-crop) in the whole video session,
as done for single images in [16].
This pre-processing pipeline is illustrated in Figure 1.
In addition, in order to increase the variance of training
data (with the intention of making the network more robust),
we add a horizontally mirrored copy of all training images.
Input Image Face LocalizationFace Alignment
Face Location Normalization
Global Contrast NormalizationMean Pixel 
RemovalMean 
PixelStandard 
Deviation
Standard Deviation 
DivisionGrayscale 
Conversion
Fig. 1: The Pre-processing pipeline.
B. The Deep Neural Network
The estimation of AUs from the pre-processed face-crops
is performed by a Deep Convolutional Neural Network.
In our framework, the input image size of the network is set
to4848grayscale pixels arranged in a 2D matrix. This
pre-processed image is fed to the Ô¨Årst hidden layer of the
network: A convolutional layer with a kernel size of 55
having a stride of 1 both dimensions. The number of parallel
feature-maps in this layer is 64. The 4444output image
produced by this layer is then passed to a max-pooling layer
[17] of kernel size 33with a stride of 2 in each dimension.
This results in a sub-sampling factor of 1=2, and hence the
resulting image is of size 2222. The second hidden layer is
also a 64 feature-map convolutional layer with a kernel size
of55(and stride 1). The output of this layer is a 1818
pixel image, and this feeds directly into the third hidden
layer of the network, which is again a convolutional layer
of 128 feature maps with a kernel size of 44(and stride
1). Finally, the output of this layer, which is of dimension
1515, is fed into the last hidden layer of the network,
a fully connected linear layer with 3072 neurons. Dropout
technique [18], [19] is applied to this fully connected layer,
with a dropout probability of 0.2. The output of this layer is
connected to the output layer, which is composed of either
11 or 6 neurons (11 for the BP4D database, and 6 for the
SEMAINE dataset), each representing one AU class label.
All layers in the network are made up of ReLu units/neurons
[20]. This architecture is illustrated in Figure 2.
Training of the deep neural network is done using stochas-
tic gradient descent with momentum in mini-batch mode,
with batches of 100 data samples. Negative log-likelihood is
used as the objective function.
It should be noted that for the BP4D dataset, while the
network was trained on occurrence of AUs 1, 2, 4, 7, 15, 23,
it was also trained on the intensities of AUs 6, 10, 12, 14,
17. This way, the same network is able to participate in both
the occurrence sub-challenge (for the BP4D dataset), as well
as the two intensity sub-challenges.
[48 x 48]
Input 
LayerGlobal Contrast 
Normalization
[3 x 3]
Max Pooling
(Stride of 2)
[5x5] x 64
Convolutional 
Layer
[5x5] x 64
Convolutional 
Layer[4x4] x 128
Convolutional 
Layer
[3072]
Fully Connected 
Layer[11/6]
Output Layer[3072] [15x15] [18x18] [22x22] [44x44] [48x48] [48x48]
Face Location 
Normalization
Input 
ImageFig. 2: The architecture of the deep convolutional neural network.
The training of the deep network involved over 140,000
images of the BP4D dataset and 90,000 images of the
SEMAINE Dataset (excluding mirrored copies). Initially,
50% of these were used in the training sets, while the rest
(development sets) was used as the validation sets. However,
in order to make full use of the annotated data given, 100% of
the data was included in the training sets in the Ô¨Ånal versions
of the networks, which were trained for a Ô¨Åxed number of
epochs (determined by validation set performance in previous
training experiments).
C. Post-Processing
The raw output of the deep network (the activations of the
Ô¨Ånal layer) essentially denote the conÔ¨Ådence scores for the
presence/absence of each AU, where an extremely low value
represents a strong absence of an AU, while a high value
represents its strong presence. In an ideal case, the decision
threshold of the deep network must lie on the mid-point of
values representing the ground-truth presence and absence
of an AU in the training set. However, many conditions
can result in this threshold being skewed to one of the
two extremes (e.g., uneven distribution of occurrences in the
training set).
As a work-around to this problem, we optimize the de-
cision threshold of the AUs on a validation set, i.e., we set
the decision thresholds to a value that gives be highest F1
score, when tested on a validation set. In our case, we Ô¨Årst
separated the annotated dataset into two partitions: a training
set and a validation set (same as the development set by the
challenge). Next, we train the network only on the training
set, and test all possible values of the decision threshold on
the development validation set. Finally, we apply these best-
performing thresholds as the decision thresholds for our Ô¨Ånal
network, which is trained on the complete set of provided
data (including the development set).
V. RESULTS
The training of these networks took roughly 15 hours each
on a 1000+ core GPU. The learning curve and the weights
of the trained network (on the BP4D Dataset) can be seen
in Figure 3.
0.010.020.020.030.030.04
0 5 10 15 20 25Mean Squared Error
EpochsTrain Set
Valid Set(a) Learning curve of the net-
work w.r.t. mean squared error on
the training and validation set.
(b) Visualization of the Ô¨Årst-
layer convolutional feature-map
weights.
Fig. 3: Deep neural network learning curve and weights
trained on the BP4D Dataset.
BP4D Dataset SEMAINE Dataset
Action Unit F1 Score Action Unit F1 Score
AU01 0.399 AU02 0.372
AU02 0.346 AU12 0.707
AU04 0.317 AU17 0.067
AU06 0.718 AU25 0.602
AU07 0.776 AU28 0.040
AU10 0.797 AU45 0.257
AU12  0.793
AU14 0.681
AU15 0.235
AU17 0.368
AU23 0.309
Mean 0.522 Mean 0.341
TABLE I: Occurance sub-challenge results for BP4D and
SEMAINE datasets.
Action Unit MSE PCC ICC
AU06 1.287 0.664 0.663
AU10 1.249 0.735 0.733
AU12 0.928 0.788 0.788
AU14 1.686 0.591 0.549
AU17 0.757 0.329 0.329
Mean 1.181 0.621 0.613TABLE II: Fully automated intensity estimation sub-
challenge results for the BP4D dataset.
Action Unit MSE PCC ICC
AU06 1.125 0.423 0.423
AU10 0.963 0.555 0.543
AU12 0.803 0.632 0.613
AU14 1.554 0.533 0.495
AU17 1.198 0.244 0.219
Mean 1.129 0.478 0.459
TABLE III: Pre-segmented intensity estimation sub-
challenge results for the BP4D dataset.
The performance of the network on the test set on the
three sub-challenges of FERA-2015 are presented in Tables
I,II and III. The evaluation methods used in these results
are the F1 Score, the Mean Squared Error (MSE), Pearsons
Correlation CoefÔ¨Åcient (PCC) and the Intra-class Correla-
tion CoefÔ¨Åcient (ICC). The sub-challenges, test set and the
evaluation measure are explained in [14].
As can be seen, our deep network performs with a mean
F1 score of 0.522 on the BP4D test set, and 0.341 on the SE-
MAINE dataset in the occurrence sub-challenge. The method
gives an average mean squared error of 1.181 and 1.129
on the fully automated and pre-segmented intensity sub-
challenges respectively. Additionally, on the BP4D dataset,
it was observed that video-segment global contrast normal-
ization pre-processing step contributes an improvement of
0.051 to the Ô¨Ånal F1 score, while the post-processing step
improves the Ô¨Ånal F1 score by 0.113.
It can also be seen that the performance of our method
on the SEMAINE dataset was much lower than on the
BP4D dataset. One of the main contributing factors to this
observation is the low number of individual faces included
in the SEMAINE training data (31 people), as compared
to the BP4D training data (41 individuals). In fact, we have
been able to attain much higher performance for emotion and
facial characteristics estimation in previous experiments [7],
where the same network was trained on datasets containing
more than 200 individuals. This suggests that the deep
network ends up over-Ô¨Åtting by learning the identities of the
individuals in the training set.
Some classiÔ¨Åcation examples (from the validation set) by
the network can be viewed in Figure 4.
AU1 AU2 AU4 AU6 AU7 AU10 AU12 AU14 AU15 AU17 AU23
Ab Ab Ab Ab Ab Ab Ab Ab Ab Ab Ab
Ab Ab Ab Ab Ab Ab Ab Ab Ab Ab Ab
AU1 AU2 AU4 AU6 AU7 AU10 AU12 AU14 AU15 AU17 AU23
Ab Pr Ab C Pr D Ab Ab Ab Ab Ab
Ab Ab Ab D Pr D D C Ab Ab Ab
AU1 AU2 AU4 AU6 AU7 AU10 AU12 AU14 AU15 AU17 AU23
Ab Ab Pr Ab Pr C Ab Ab Pr C Ab
Ab Ab Pr B Pr C Ab C Pr A Pr
AU1 AU2 AU4 AU6 AU7 AU10 AU12 AU14 AU15 AU17 AU23
Ab Ab Ab B Pr D B B Pr Ab Ab
Pr Ab Pr C Pr D C C Ab Ab Pr
AU1 AU2 AU4 AU6 AU7 AU10 AU12 AU14 AU15 AU17 AU23
Ab Ab Ab B Ab C B Ab Ab B Pr
Ab Ab Pr C Pr C C C Pr A PrFig. 4: Example AU estimation of images from the BP4D
Dataset. The Ô¨Årst row corresponds to the ground truth, and
the second row corresponds to the AU estimation by the
proposed method.
VI. CONCLUSIONS AND FUTURE WORKS
A. Conclusions
In this paper, a deep learning approach has been demon-
strated for detecting the occurrence and estimating the inten-
sity of facial AUs. This approach is primarily based on the
use of convolutional neural networks on two dimensional
pre-processed and aligned images of faces. Deep convolu-
tional networks have been recently very successful in very
different visual tasks. That has been the main motivation of
utilizing it in this speciÔ¨Åc challenge. However, the results
have not been as promising as expected. We believe that this
is due to the fact that the deep learning techniques depend
on being trained on big datasets with high variation in the
data. In the datasets supplied in the challenge, there is little
variation in terms of the number of individuals and this can
cause over Ô¨Åtting of the network on the individual faces in
the dataset.
B. Future Works
We have observed the following items as potential future
work:
This framework proposed in this paper does not take
into account the temporal dimension of the input data
(frames of videos). Assuming time-domain information
could improve the results, 3-D convolutional networks
as proposed in [21], could readily Ô¨Åt into our frame-
work.
More extensive experimentation with alternative pre-
processing techniques could be carried out. Methods
like whitening are used in a wide range of machine
learning tasks to reduce redundancy, and could be used
to aid deep networks as well.
A limiting factor in the conducted experiments is the
available computational resources. This calls for more
experimentation on larger networks, as the true optimal
performance of these networks can only be achieved
after extending the upper bound restrictions on the
network size.
REFERENCES
[1] Teresa Farroni, Enrica Menon, Silvia Rigato, and Mark H Johnson,
‚ÄúThe perception of facial expressions in newborns,‚Äù European Journal
of Developmental Psychology , vol. 4, no. 1, pp. 2‚Äì13, 2007.
[2] Paul Ekman, Wallace V Freisen, and Sonia Ancoli, ‚ÄúFacial signs of
emotional experience.,‚Äù Journal of personality and social psychology ,
vol. 39, no. 6, pp. 1125, 1980.
[3] Maja Pantic and Leon JM Rothkrantz, ‚ÄúToward an affect-sensitive
multimodal human-computer interaction,‚Äù Proceedings of the IEEE ,
vol. 91, no. 9, pp. 1370‚Äì1390, 2003.
[4] H. Emrah Tasli and Paul Ivan, ‚ÄúTurkish presidential elections
trt publicity speech facial expression analysis,‚Äù in arXiv preprint
arXiv:1408.3573 , 2014.
[5] H Emrah Tasli, Amogh Gudi, and Marten Den Uyl, ‚ÄúIntegrating
remote ppg in facial expression analysis framework,‚Äù in ACM
International Conference on Multimodal Interaction (ICMI) . ACM,
2014, pp. 74‚Äì75.
[6] Thales Teixeira, Michel Wedel, and Rik Pieters, ‚ÄúEmotion-induced
engagement in internet video advertisements,‚Äù Journal of Marketing
Research , vol. 49, no. 2, pp. 144‚Äì159, 2012.
[7] Amogh Gudi, ‚ÄúRecognizing semantic features in faces using deep
learning,‚Äù M.S. thesis, Faculty of Science, University of Amsterdam,
2014.
[8] Michel Valstar and Maja Pantic, ‚ÄúFully automatic facial action unit
detection and temporal analysis,‚Äù in Computer Vision and Pattern
Recognition Workshop, 2006. CVPRW‚Äô06. Conference on . IEEE, 2006,
pp. 149‚Äì149.
[9] Arman Savran, Bulent Sankur, and M Taha Bilge, ‚ÄúRegression-
based intensity estimation of facial action units,‚Äù Image and Vision
Computing , vol. 30, no. 10, pp. 774‚Äì784, 2012.
[10] Mohammad H Mahoor, Steven Cadavid, Daniel S Messinger, and
Jeffrey F Cohn, ‚ÄúA framework for automated measurement of the
intensity of non-posed facial action units,‚Äù in Computer Vision and
Pattern Recognition Workshops, 2009. CVPR Workshops 2009. IEEE
Computer Society Conference on . IEEE, 2009, pp. 74‚Äì80.
[11] Yaniv Taigman, Ming Yang, Marc‚ÄôAurelio Ranzato, and Lior Wolf,
‚ÄúDeepface: Closing the gap to human-level performance in face
veriÔ¨Åcation,‚Äù in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , 2013, pp. 1701‚Äì1708.
[12] Gary B Huang and Erik Learned-Miller, ‚ÄúLabeled faces in the wild:
Updates and new reporting procedures,‚Äù .
[13] Yichuan Tang, ‚ÄúDeep learning using linear support vector machines,‚Äù
inWorkshop on Challenges in Representation Learning, ICML , 2013.
[14] Michel Franc ¬∏ois Valstar, ‚ÄúFera 2015 - second facial expression
recognition and analysis challenge,‚Äù in Automatic Face & Gesture
Recognition and Workshops (FG 2015), 2015 IEEE International
Conference on . IEEE, 2015.
[15] Xuehan Xiong and Fernando De la Torre, ‚ÄúSupervised descent method
and its applications to face alignment,‚Äù in Computer Vision and Pattern
Recognition (CVPR), 2013 IEEE Conference on . IEEE, 2013, pp. 532‚Äì
539.
[16] Adam Coates, Andrew Y Ng, and Honglak Lee, ‚ÄúAn analysis of single-
layer networks in unsupervised feature learning,‚Äù in International
Conference on ArtiÔ¨Åcial Intelligence and Statistics , 2011, pp. 215‚Äì
223.
[17] Yann LeCun, Koray Kavukcuoglu, and Cl ¬¥ement Farabet, ‚ÄúConvolu-
tional networks and applications in vision,‚Äù in Circuits and Systems
(ISCAS), Proceedings of 2010 IEEE International Symposium on .
IEEE, 2010, pp. 253‚Äì256.
[18] Nitish Srivastava, Improving neural networks with dropout , Ph.D.
thesis, University of Toronto, 2013.
[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton, ‚ÄúImagenet
classiÔ¨Åcation with deep convolutional neural networks,‚Äù in Advances
in neural information processing systems , 2012, pp. 1097‚Äì1105.
[20] Vinod Nair and Geoffrey E Hinton, ‚ÄúRectiÔ¨Åed linear units improve re-
stricted boltzmann machines,‚Äù in Proceedings of the 27th International
Conference on Machine Learning (ICML-10) , 2010, pp. 807‚Äì814.
[21] Moez Baccouche, Franck Mamalet, Christian Wolf, Christophe Garcia,
and Atilla Baskurt, ‚ÄúSequential deep learning for human action
recognition,‚Äù in Human Behavior Understanding , pp. 29‚Äì39. Springer,
2011.
"
https://ieeexplore.ieee.org/document/7574736,"1 
Deep Neural Networks with Relativity Learning 
for Facial Expression Recognition 
! ! 2 ‚Ä¢ 3 Y L'! D h T 3 Yanan Guo , Dapeng Tao, Jun Yu ,Hao XIOng , aotang 1, ac eng ao 
'Yunnan University, China, 2Hangzhou Dianzi University, China, 3University of Tchnology Sydney, 
Australia 
Y ananguo. YNU@qq.com, dapeng.tao@gmail.com, yujun@hdu.edu.cn, hao.xiong@student.uts.edu.au, 
Iiyaotang@ynu.edu.cn, dacheng.tao@uts.edu.au. 
ABSTRACT 
Facial expression recogmtlOn aims to classify facial 
expression as one of seven basic emotions including ""neutral "". 
This is a difficult problem due to the complexity and subtlety 
of human facial expressions, but the technique is needed in 
important applications such as social interaction research. 
Deep learning methods have achieved state-of-the -art 
performance in many tasks including face recognition and 
person re- identification. Here we present a deep learning 
method termed Deep Neural Networks with Relativity 
Learning (DNNRL) , which directly learns a mapping from 
original images to a Euclidean space, where relative distances 
correspond to a measure of facial expression similarity. 
DNNRL updates the model parameters according to sample 
importance. This strategy results in an adjustable and robust 
model. Experiments on two representative facial expression 
datasets (FER-2013 and SFEW 2.0) are performed to 
demonstrate the robustness and effectiveness ofDNNRL. 
Index Terms- Facial expression, deep feature learning, 
convolutional neural network, social interaction. 
1 INTRODUCTION 
Human emotion recognition is attracting attention due to its 
many practical applications in, for example, social interactions, 
human-robot interactions, and call center systems. Facial 
expression recognition (FER), a basic part of motion analysis, 
can be used to recognize internal human emotions. FER 
methods attempt to classify facial expression in a given image 
or sequence of images as one of six basic emotions (anger, 
disgust, fear, happiness, sadness, and surprise) or as ""neutral "" 
[4]. Although much effort and some progress has been made 
in the field, accurately recognizing facial expression remains 
difficult due to the complexity and subtlety of facial 
expressions and how they relate to emotion. 
Most FER systems involve three main steps: face image 
pre-processing, feature extraction, and classification. Deriving an effective and robust facial representation from raw face 
images is extremely important for FER. In general, methods 
used to extract facial representations can be grouped into two 
categories: action unit (AU)-based methods and 
appearance- based methods. AU-based methods, originally 
proposed by Ekman et al. [4], detect the presence of individual 
AUs which are then extracted to form a feature based on their 
combinations. However, each AU detector usually requires 
careful hand engineering (such as through reliable facial 
detection and tracking) to ensure good performance, which is 
hard to accommodate in many situations. Appearance- based 
methods [15] represent an individual's emotion from their 
facial shape and texture. Appearance-based methods apply 
traditional handcrafted features such as Gabor wavelets [10], 
Local Binary Pattern (LBP) features [12], and Motion History 
Images (MHIs) [17] to either specific face regions or the 
whole face to extract the facial appearance changes; these 
methods have achieved impressive performance on several 
accepted FER benchmarks. However, most handcrafted 
features are limited by not being directly applicable to 
practical problems and lack generalizability when applied to 
novel data. 
A number of well-established problems in computer vision 
have recently benefited from the rise in deep learning as 
appearance- based feature representations or classifiers. Deep 
learning methods have boosted performance in a variety of 
tasks including face recognition, speech recognition, and 
object detection. However, most deep learning methods 
minimize cross-entropy loss and employ the softmax 
activation function for prediction. Thus, when updating the 
model, they treat all samples equally and do not consider the 
fact that giving difficult samples more weight could learn a 
more robust model. 
Here we present a scheme termed Deep Neural Networks 
with Relativity Learning (DNNRL) for FER. DNNRL directly 
learns a mapping from original images to a Euclidean space, 
where relative distances correspond to a measure of facial 
expression similarity. Furthermore, DNNRL updates the 
model according to sample importance, leading to a more 
adjustable and robust model. We conduct extensive 
experiments on two well-known facial expression datasets 
(FER-2013 and SFEW 2.0) and obtain results that are 
significantly better than traditional deep learning or other 
state-of-the-art methods. 
The remainder of the paper is organized as follows: in 
Section 2, we briefly review related works about feature 
representation using appearance-based methods for FER 
problems. We detail the newly proposed DNNRL in Section 3. 
The experimental results on the two representative datasets are 
presented in Section 4, and we conclude in Section 5. 
2 RELATED WORK 
In general, feature representation using appearance -based 
methods can be group into two main categories: handcrafted 
features and deep features. 
Gabor-wavelet representations have been extensively used 
in face image analysis and show promising performance. 
However, Gabor-wavelet representations are computationally 
costly. Bartlett et al. [2] showed that Gabor-wavelet 
representations derived from every 48 x 48 face image have 
the high dimensionality of 0(105). LBP describes local 
texture variation and is often used with a holistic 
representation. Shan et al. [14] observed that LBP features 
perform robustly and stably over a range of low-resolution 
images, and the method showed promising performance in 
low-resolution video sequences captured in practice. 
Deep learning has also recently obtained state-of-the -art 
performance for FER problems as an appearance-based feature 
representation or classifier. Khorrami et al. [7] showed that 
convolutional neural networks (CNNs) are effective, and they 
introduced a method to decipher which part of the face image 
influences the CNN's predictions. Mollahosseini et al. [11] 
proposed a deep learning architecture consisting of two 
convolutional layers, each followed by max pooling layers and 
four Inception layers, to address the FER problem across 
mUltiple representative face datasets. By minimizing a 
margin-based loss rather than the cross-entropy loss, Tang et 
al. [16] demonstrated that switching from softmax to SVM is 
simple and beneficial for classification. 
3. DEEP NEURAL NETWORKS WITH RELATIVITY 
LEARNING 
Deep learning methods have, therefore, achieved excellent 
performance in many applications including face recognition 
and person re- identification. However, when updating the 
deep learning model, they treat all samples equally and do not 
consider that giving difficult samples more weight learns a 
more robust model. In this section, we present a novel deep 
feature learning method, DNNRL, which can update the 
model according to sample importance. 
3.1 The Network Architecture 
The overview of the network architecture is shown in 
Figure 1. The network consists of three elements. First, the 
network contains three convolutional layers followed by one 2 
max pooling layer. Following these layers, three inception 
modules are added consisting of 1 xl, 3 x 3, and 5 x 5 
convolution layers in parallel. Each inception module is 
followed by a pooling layer. The final layer is a fully 
connected layer with normalization [8] and dropout [6]. The 
Inception layer allows for improved recognition of local facial 
features, since smaller convolutions are applied to local 
features while larger convolutions approximate global features; 
looking at local features including the mouth and eyes allows 
humans to easily distinguish most emotions [1]. Normalization 
ensures that the relative distance derived from two images has 
an upper bound. Dropout is a statistical randomness that 
reduces the risk of network overfitting. The non-linear 
activation functions for all convolutional layers are set as 
rectified linear units (ReLUs) [8], and using the ReLU 
activation function avoids the vanishing gradient problem 
caused by some other activation functions. Finally, Batch 
normalizations [9] are used for all convolutional layers to be 
less careful about initialization and use much higher learning 
rates. The network configuration is shown in Table 1. 
To train a more robust model, data are pre- processed as 
follows. The images are first resized to 96x96. Next, we 
randomly scale the resized images, where the range of scaling 
is [-lO, lO] . We then pad 0 round scaled images to the size of 
120x 120. Finally, we rotate the obtained images randomly in 
the range of [-15¬∞,15¬∞] and crop these images randomly to 
108 xl 08. The random rotation generates additional unseen 
samples and, therefore, makes the network even more robust 
to deviated and rotated faces. The number of images is 
enlarged by a factor of lO. Thus, the images are normalized to 
a standard size of 108 xl 08, and this manipulation causes 
shape distortion that can train a more accurate model. The top 
and the third rows in Figure 2 give 14 examples of raw faces, 
while the second and the fourth rows show their corresponding 
pre-processed faces. Each column represents the same 
expression: from left to right, angry, disgust, fear, happy, sad, 
surprise, and neutral. 
Figure 2. Examples of pre-processed faces. The first and the 
third rows show the original faces, while the second and the 
fourth rows show their corresponding pre-processed faces. 
Each column represents the same expression. From left to 
right: angry, disgust, fear, happy, sad, surprise, and neutral. 
, 
, 
\ 
, 
, 
, 
, 
"" 
, 
, 
Figure 1. The network architecture. 
Table 1. Network configuration 
Layer type Patch Output Size/Stride 
convolution 3x3/2 54x54x32 
convolution 3x311 54x54x32 
convolution 3x311 54x54x64 
max pooling 3x3/2 27x27x64 
inception(3a) - 27x27x256 
max pooling 3x3/2 13x13x256 
inception( 4a) - 13x13x288 
max pooling 3x3/2 6x6x288 
inception(5a) - 6x6x288 
ave pooling 3x3/3 2x2x288 
dropout(20% ) - 2x2x288 , 
\ 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
, 
, Inception 
, 
\ 
Inception 
, 
, 
, 
\ 
I 
I 
Inception 
I 
I 
I 
I 
I 
, 
, 
Params 
288 
9K 
18K 
-
203K 
-
823K 
-
926K 
-
-Ops 
839K 
26M 
53M 
-
148M 
-
139M 
-
33M 
-
-3 
linear lxlx512 589K 589K 
3.2 Loss Function and Optimization 
Given a network and treating it as an end-to-end system, 
triplet loss [13] can be directly applied to the practical 
problem of expression recognition; that is, triplet loss training 
aims to learn an optimal feature representation that maximizes 
the relative distance. However, when updating the model, all 
samples are treated equally and giving difficult samples more 
weight to learn a more robust model is not considered. Here 
we utilize the exponential function to address this problem. 
Take a set of triplets P = {Pi}' Pi = {(Pi' P;, p;)} , 
where Pj and P; are images with the same expression and 
Pi and P; are images with different expressions. Let 
f ( x) denote the network output of image x; that is, 
f ( x) is the embedded feature representation of image x. 
We want to learn effective features f (Pi)' f (p;), and 
f (p;) to ensure that f (Pi) is closer to all other features 
f (p;) of the same expression than to any feature f (p;) 
of any other expression; we are, therefore, prone to giving 
difficult samples more weight when updating the model. Thus, 
the loss function is 
L = I L. 
1 
(pj,p; ,pi )EP 
(1) 
where the fixed scalar y 2: 0 is a learning margin to prevent 
learning very difficult samples (the relative distance between 
Ilf (Pi) -f (p; )112 and Ilf (Pi) -f (p; )112 is smaller than 
r ), and a is a tradeoff parameter. 
Figure 3 shows (. The smaller the relative distance, the 
larger the gradient of Li; that is, we are biased to updating the 
model from difficult samples, and when the relative distance is 
large (the relative distance between Ilf (Pi) -f (p; )112 and 
Ilf (Pi) -f (p; )112 is large), the gradient of Li is close to 
o and the model is only updated slightly when learning simple 
samples. 
loss 
relative distance 
Figure 3. The exponential triplet-based loss function. The x 
coordinate is the relative distance and the y coordinate is the 
corresponding loss. The absolute value of the gradient of loss 
decreases as the relative distance increases. 
The gradients with respect to f (Pi)' f (p;), or f (p;) 
are 
where ÔøΩ -2aÔøΩ [(f (Pi) -f (p;)) -(f (Pi) -f (p; ))] 
,llf (Pi) -f (p; )112 -Ilf (Pi) -f (p;)112 > r 
0, Ilf (Pi) -f (p;)W -Ilf (pJ -f (p;)112 ::; r 
-2aÔøΩ(f(p;) -f(P;))' 
,llf(Pi) -f(P;)W -llf(Pi) -f(p;)112 > r 
0, Ilf (Pi) -f (p;)W -Ilf (Pi) -f (p; )112 ::; r 
(2) 
-2aÔøΩ(f(pJ -f(P;))' 
,llf(Pi) -f(p;)112 -llf(Pi) -f(P;)W > r 
o,llf(Pi)-f(p;)W -llf(Pi)-f(p;)112::; r 
(3) 
-2aÔøΩ(f(p;) -f(pJ), 
,llf(Pi) -f(P;)W -llf(Pi) -f(p;)112 > r 
0, Ilf (Pi) -f (p; )112 -Ilf (pJ -f (p; )112 ::; r 
(4) 
denotes e-allf(pt)-f(Pill! ' -llf(Pt)-f(P;lI!' -y. Hence, the 
loss function in (1) can be easily integrated in back 
propagation in neural networks. 
4. EXPERIMENT AL RESULTS 
We conducted facial expression recognition experiments on 
two widely used datasets, the FER-2013 dataset [5] and the 
SFEW 2.0 dataset [3], to demonstrate the effectiveness of the 
proposed method. The FER-2013 dataset contains 27,809 
training images, 3,589 validation images, and 3,589 test 4 
images. Faces are labeled with any of the six basic expressions 
or neutral: the number of images representing the six basic 
expressions (anger, disgust, fear, happiness, sadness, and 
surprise) and neutral are 4953, 547, 5121, 8989, 6077, 4002, 
and 6189, respectively. This dataset was created using the 
Google image search API and contains large variations 
reflecting real-world conditions. The images are 48x48 
pixels. 
The SFEW 2.0 dataset was created from Acted Facial 
Expressions in the Wild (AFEW) [3] using the key-frame 
extraction method. It contains 880 training samples, 383 
validation samples, and 372 test samples. Since SFEW 2.0 is a 
competition dataset in the Wild Challenge (EmotiW) 2015, 
test sample labeling is private and, therefore, unavailable. 
Furthermore, SFEW 2.0 is relatively small, rendering it prone 
to overfitting when trained on the DNNRL. Hence, we 
pre-trained the model on the FER-2013 training set and fine 
tuned on the SFEW 2.0 and FER-2013 training sets; the test 
samples are FER-2013 test images. The SFEW 2.0 dataset 
includes different head poses, occlusions, backgrounds, and 
close to real- world illuminations; thus, face alignment is 
necessary to extract face-related information and for unifying 
all face images to the same domain. We used SDM [18] for 
face alignment, transformed the aligned faces to grayscale, 
and resized to 48x48, which is the same as FER-2013 data. 
Figure 4 shows examples of face alignment by SDM, where 
the top row shows original images and the bottom row shows 
their corresponding aligned faces. 
Figure 4. Examples of face alignment by SDM, where the top 
row shows original images and the bottom row shows their 
corresponding aligned faces. 
We first normalized the images in the above two datasets to 
96 x 96 and then pre-processed these images using the 
manipulation described in Section 3.1. After obtaining deep 
features from the DNNRL model, to make a class choice, we 
used a k-NN classifier to obtain the recognition accuracy, 
where any test sample was classified by a majority vote of its 
k training samples, with the test sample being assigned to the 
class most common among its k nearest training samples. Use 
of the k-NN classifier is reasonable, because the DNNRL loss 
is also based on Euclidean distances. We determined the value 
of k for sample testing when the validation samples obtained 
the highest accuracy on the value of k. 
The initial learning rate was set to 0.01, while the minimum 
learning rate was set to 0.000l. Each training epoch had 
[N / 128] batches, with the training samples randomly 
selected from the training set. The learning margin y was set 
to 0, and the tradeoff parameter a was set to 0.2. The trained 
network parameters and loss at each epoch were recorded. The 
validation loss was assessed after each round of training; if the 
validation loss increased by more than 10%, the learning rate 
was reduced by one-tenth and the previous network with the 
best validation loss was reloaded. The termination criterion for 
the training model was judged by the new lowest value for the 
validation loss in 20 iterations. 
4.1 Performance on FER-2013 
The perfonnance of DNNRL and baselines on FER-2013 are 
shown in Table 2. ""DNN"" refers to the accuracy of the 
proposed network described in Section 3.1 with softmax, and 
""T-DNN"" refers to the accuracy of the proposed DNNRL with 
the k-NN classifier, where the loss function of DNNRL is 
replaced by the triplet-based loss. ""DNNRL "" refers to the 
accuracy of the proposed DNNRL with the k-NN classifier. 
AlexNet [8] and FER-2013 Champion [16] results are also 
listed. From Table 2, it can be seen that DNNRL obtains the 
highest recognition rate of the tested methods, demonstrating 
its effectiveness and robustness for FER. DNN outperfonns 
AlexNet because the Inception layer improves local feature 
recognition. T-DNN outperforms DNN because it can directly 
learn a mapping from original images to a Euclidean space, 
where relative distances correspond to a measure of facial 
expression similarity. DNNRL outperforms T-DNN because it 
updates the model more or less according to the sample 
difficulty. 
0.11 0.02 0.12 0.01 0.10 
0.02 0.00 0.00 0.04 
0.20 0.07 0.06 
0.03 
0.01 
0.00 0.09 
0.00 0.05 0.04 
ÔøΩ'"" % 1> .rÔøΩ .r¬´ 
""ÔøΩIl ÔøΩ& .... ÔøΩ"".rt ÔøΩ .... 0:"" ÔøΩ"". v .... ÔøΩ/ &.r.r ;s'& 
Figure 5. DNNRL classification confusion matrices on 
FER-2013 test set (trained on FER-2013 training set). 
The confusion matrix of DNNRL on the FER-2013 dataset 
(trained on FER-2013 training set) is shown in Figure 5. The 
confusion matrix between the ground-truth class label and the 
most likely inferred class label information provide a better 
understanding of DNNRL's limitations. As expected, 
confusion frequently occurs between ""anger"", ""fear"", and 
""sadness "" because they create similar motions. The confusion 
matrices also show that ""natural "" is easily misclassified as 
""sadness "". 5 
Table 2. Classification accuracy of the proposed DNNRL with 
other methods on the FER-2013 test set (trained on FER-2013 
training set). 
Accu DNNRL T-DNN DNN AlexNet FERwin 
[16] 
Test 0.7060 0.7013 0.6922 0.6482 0.693 
4.2 Performance on SFEW 2.0 and FER-2013 datasets 
The performance of DNNRL and baselines on FER-2013 
(trained on SFEW 2.0 and FER-2013 training sets) are shown 
in Table 3. The baselines include AlexNet [8], DNN, and 
T-DNN. DNNRL obtains the highest recognition rate of the 
tested methods and outperforms the highest recognition rate of 
Table 2, demonstrating its robustness for FER. 
Figure 6 shows the confusion matrices of DNNRL on the 
FER-2013 (trained on SFEW 2.0 and FER-2013 training sets). 
Confusion also frequently occurs between ""anger"", ""fear"", and 
""sadness"", and ""natural "" is easily misclassified as ""sadness "". 
0.08 0.03 0.12 0.02 0.08 
0.00 0.00 0.02 
0.21 0.09 0.06 
0.02 0.04 
0.01 0.14 
0.01 0.07 
0.00 0.04 0.04 
ÔøΩ'"" ÔøΩ& 1> .rÔøΩ .r¬´ 
""ÔøΩIl ÔøΩ& .... ÔøΩ"".rt ÔøΩ .... 0:"" ÔøΩ"". v .... ÔøΩ/ &.r.r ;s'& 
Figure 6. DNNRL classification confusion matrices on 
FER-2013 validation and test sets. 
Table 3. Classification accuracy of the proposed DNNRL with 
other methods on FER-2013 test set (trained on SFEW 2.0 and 
FER-2013 training sets). 
Accu DNNRL 
Test 
5. CONCLUSION 
This work introduces DNNRL, a new deep learning method 
for FER. DNNRL consists of three convolutional layers, four 
pooling layers, three Inception layers, and one fully connected 
layer. The Inception layers increase the width and depth of the 
network while not increasing computational cost, furthennore, 
they improves local feature recognition. By using the 
exponential triplet loss, DNNRL can directly learn a mapping 
from original images to a Euclidean space, where relative 
distances correspond to a measure of facial expression 
similarity. Furthermore, the model is updated to a greater or 
lesser degree according to the sample difficulty, which leads 
to a more adjustable and robust model. 
Compared to traditional deep neural networks such as 
AlexNet, DNNRL is competitive and achieves excellent 
performance for FER. 
REFERENCES 
[1] E. Bal, E. Harden, D. Lamb, A. Van Hecke, J. Denver, and 
S. Porges, ""Emotion recognition in children with autism 
spectrum disorders: Relations to eye gaze and autonomic 
state,"" Journal of Autism and Developmental Disorders, 
vol. 40, no. 3, pp. 358-370,2010. 
[2] M. S. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. 
Fasel, and J. Movellan, ""Recognizing facial expression: 
machine learning and application to spontaneous 
behavior, "" in Proceedings of the Computer Society 
Conference on Computer Vision and Pattern Recognition 
(CVPR), 2005, pp. 568-573. 
[3] A. Dhall, O. V. R Murthy, R. Goecke, 1. Joshi, and T. 
Gedeon, ""Video and image based emotion recognition 
challenges in the wild: Emotiw 2015,"" in Proceedings of 
the 20i5 ACM on international Conference on 
Multimodal interaction, 2015 ,pp. 423-426. 
[4] P. Ekman and W. V. Friesen, ""Constants across cultures in 
the face and emotion, "" Journal of personality and social 
psychology, vol. 17, no. 2, pp. 124-129, 1971. 
[5] I. J. Goodfellow, D. Erhan, P. L. Carrier, A. Courville, M. 
Mirza, B. Hamner, W. Cukierski, Y. Tang, D. Thaler, D. H. 
Lee, Y. Zhou, C. Ramaiah, F. Feng, R. Li, X. Wang, D. 
Athanasakis, 1. Shawe-Taylor, M. Milakov, 1. Park, R. 
Lonescu, M. Popescu, C. Grozea, 1. Bergstra, 1. Xie, L. 
Rpmaszko, B. Xu, Z. Chuang, and Y. Bengio, ""Challenges 
in representation learning: A report on three machine 
learning contests, "" Neural Networks, vol. 64, pp. 59-63, 
2015. 
[6] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, 
and R. R. Salakhutdinov, ""Improving neural networks by 
preventing coadaptation of feature detectors, "" arXiv 
pre print, arXiv:1207.0580, 2012. 
[7] P. Khorrami, T. Paine, and T. Huang, ""Do Deep Neural 
Networks Learn Facial Action Units When Doing 
Expression Recognition? "" in Proceedings of the iEEE 
international Conference on Computer Vision Workshops 
(iCCV), 2015, pp. 19-27. 
[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ""Imagenet 
classification with deep convolutional neural networks, "" 
in Proceedings of the Conference on Neural information 
Processing Systems (NiPS) , 2012, pp.1097-1105. 
[9] S. Ioffe and C. Szegedy, ""Batch normalization: 
Accelerating deep network training by reducing internal 
covariate shift, "" arXiv pre print arXiv: 1502.03167, 2015. 
[10] M. J. Lyons, J. Budynek, and S. Akamatsu, ""Automatic 
classification of single facial images, "" iEEE Transactions 
on Pattern Analysis and Machine intelligence vol. 21, no. 
12, pp. 1357-1362, 1999. 
[11] A. Mollahosseini, D. Chan, and M. H. Mahoor, ""Going 
Deeper in Facial Expression Recognition using Deep 6 
Neural Networks, "" arXiv preprint, arXiv: 1511.04110, 
2015. 
[12] T. Ojala, M. Pietikanien, and T. Maenpa, ""Multiresolution 
gray-scale and rotation invariant texture classification 
with local binary patterns, "" iEEE Transactions on Pattern 
Analysis and Machine intelligence, vol. 24, no. 7, pp. 
971-987, 2002. 
[13] F. Schroff, K. Dmitry, and P. James, ""Facenet: A unified 
embedding for face recognition and clustering, "" in 
Proceedings of the iEEE Conference on Computer Vision 
and Pattern Recognition (CVPR), 2015, pp: 815-823. 
[14] C. Shan, S. Gong, and P. W. McOwan, ""Facial expression 
recognition based on local binary patterns: A 
comprehensive study,"" image and Vision Computing, vol. 
27,no. 6,pp. 803-816,2009. 
[15]M. Soleymani, S. Asghari-Esfeden, M. Pantic, and Y. Fu, 
""Continuous emotion detection using eeg signals and 
facial expressions "" in Proceedings of the iEEE 
international Conference on Multimedia and Expo 
(iCME), 2014, pp. 1-6. 
[16] Y. Tang, ""Deep learning using linear support vector 
machines, "" arXiv preprint, arXiv:1306.0239, 2013. 
[17] M. Valstar, M. Pantic, and I. Patras, ""Motion history for 
facial action detection in video,"" in Proceedings of the 
iEEE international Conference on Systems, Man and 
Cybernetics, 2004, pp. 635-640. 
[18] X. Xiong and F. Torre, ""Supervised descent method and 
its applications to face alignment, "" in Proceedings of the 
Computer Society Conference on Computer Vision and 
Pattern Recognition (CVPR), 2013, pp. 532-539. 
"
https://ieeexplore.ieee.org/document/5674019,"Emotion Recognition of Affective Speech Based
on Multiple Classifiers Using Acoustic-Prosodic
Information and Semantic Labels
Chung-Hsien Wu, Wei-Bin Liang
Abstract‚Äî This work presents an approach to emotion recognition of affective speech based on multiple classifiers using acoustic-
prosodic information (AP) and semantic labels (SLs). For AP-based recognition, acoustic and prosodic features including
spectrum, formant, and pitch-related features are extracted from the detected emotional salient segments of the input speech.
Three types of models, GMMs, SVMs, and MLPs, are adopted as the base-level classifiers. A Meta Decision Tree (MDT) is then
employed for classifier fusion to obtain the AP-based emotion recognition confidence. For SL-based recognition, semantic labels
derived from an existing Chinese knowledge base called HowNet are used to automatically extract Emotion Association Rules
(EARs) from the recognized word sequence of the affective speech. The maximum entropy model (MaxEnt) is thereafter utilized to
characterize the relationship between emotional states and EARs for emotion recognition. Finally, a weighted product fusion
method is used to integrate the AP-based and SL-based recognition results for the final emotion decision. For evaluation, 2,033
utterances for four emotional states (Neutral, Happy, Angry, and Sad) are collected. The speaker-independent experimental results
reveal that the emotion recognition performance based on MDT can achieve 80.00 percent, which is better than each individual
classifier. On the other hand, an average recognition accuracy of 80.92 percent can be obtained for SL-based recognition. Finally,
combining acoustic-prosodic information and semantic labels can achieve 83.55 percent, which is superior to either AP-based or
SL-Based approaches. Moreover, considering the individual personality trait for personalized application, the recognition
accuracy of the proposed approach can be further improved to 85.79 percent.
Axles Receivers Asynchronous circuits DMTF Web servers IEEE catalogs Powders.  Snow Ambient intelligence Electromagnetic scattering
Small satellites Multichip modules Timbre Geoscience Grippers Optical metrology.  Unsupervised learning Sea measurements Web pages DC
machines Spaceborne radar Power MOSFET Pairwise error probability Sequential circuits Atrophy.  Electromagnetic reflection Virtual private
networks Blanking Smart TV Harmonic filters Autonomous vehicles Colloidal nanocrystals Asymptotic stability Communication cables
Prefabricated construction Land mobile radio.  Research and development management Linear accelerators Program management Electrical
accidents Bladder Remaining life assessment Prognostics and health management DMTF Standards Orthopedic procedures Bovine Signal
design.  Spectroradiometers Programming profession Radar antennas Piezoresistance Distributed information systems Ferrite films FDDI
Chemical hazards Virtual private networks Optical superlattices Single electron memory.  Computational neuroscience Autonomic nervous
system Relaxation methods Software radio Paper making.  Constellation diagram Data storage systems Progenitor cells Cataracts
Multifrequency antennas Doppler measurement Linear accelerators Functional point analysis Computer languages.  Video surveillance Wind
tunnels X3D Network resource management Vector quantization 3GPP Standards.
(1)

Context modeling Ground support Graphical user interfaces Zirconium Electronic countermeasures Fuzzing Hafnium oxide Coprocessors
Insertion loss Video sharing Blind equalizers.  Middleboxes Stellar dynamics Pressure effects Functional point analysis Epoxy resins Channel
models.  Semiconductor device measurement Synthetic aperture radar interferometry Economics Interface states Global Positioning System
Block codes Peptides Autonomous robots Leg Arsenic compounds Current supplies.  Cavity perturbation methods Paints Application specific
processors FDDI Video compression Animal structures Resource virtualization Wet etching Femtocell networks Diffraction gratings
Photothyristors.  Tire pressure Tunable circuits and devices Document handling Ground support Wind tunnels Graphene Optical fiber theory
Bromine compounds Animatronics Magnetoresistance.  Genetic expression Lead acid batteries Single electron devices Acoustic scattering
Costing System implementation.  Neurophysiology Thumb Motion compensation Stray light Biological tissues Scalp Thermoresistivity.
Prefabricated construction Hydrocarbons Olfactory Virtual artifact Integrated circuit yield Fuzzy control Kinetic energy Air cleaners
Expectation-maximization algorithms Lubricants Disk recording Human-robot interaction.  Upper bound International Atomic Time
Hydrocarbons CMOS analog integrated circuits Nerve tissues Constellation diagram Sea measurements Receptor (biochemistry)
Electromagnetic analysis Dynamic equilibrium Effluents Social intelligence Notch filters.
Pistons Armature Solid-state physics Macroeconomics Web TV Desktop publishing Through-silicon vias Plasma transport processes.  Spread
spectrum radar Associative memory Underwater equipment Python Femtocell networks.  Buttocks Universal motors Mechanical bearings
Middleboxes Optical microscopy Power system modeling.  Application specific processors Filament lamps Support vector machines Fusion
power generation Costing Pulse transformers IEEE magazines Respiratory system Data conversion.  Hydrogen Proteins Lung Capacity planning
Zero knowledge proof Space vector pulse width modulation Embedded systems.
Information analysis Web page design IEEE staff Geography Silicon carbide Laser feedback Synchronous generators.  Solid-state physics
Quantum well lasers Bulk storage Text mining Data science Muscles Thick film circuits Electron microscopy Rectennas Rough sets Heart
valves Task analysis Web pages Delay lines.  MMICs Simulated annealing Programming profession Open Access Aluminum Ferrimagnetic
materials Ion emission Induction motor drives Mathematics computing Dinosaurs Cardiovascular system Government policies.  Call admission
control Cognitive neuroscience Electromagnetic fields Soil pollution Passive RFID tags Leaching Pressure effects Nanosensors Windows.  Web
services Computational fluid dynamics Brillouin scattering Lung Microcavities Well logging X-ray lasers.  Computational fluid dynamics
Brushless DC motors Neuromorphic engineering Eyelids Log-periodic dipole antennas Social computing.  Cognitive neuroscience Heat engines
Bismuth Handover Volcanic ash.  Dendrites (neurons) Google Magnetic sensors Shape control Machine tool spindles Mashups Atrophy.
Source coding Presence network agents Cerebral cortex Data breach Lead isotopes Research and development Materials requirements planning.
Telecommunication network reliability Junctionless nanowire transistors Plastic products Heat treatment Radiography NOMA Levee.  Robots
Plasma transport processes Foot Aneurysm Germanium silicon alloys Spinal cord Nanobioscience Breast biopsy.  Geophysics Optical films
Turbomachinery Brain Voltage measurement Optical fiber amplifiers Potassium Global Positioning System GSM.  Ventilation Time of arrival
estimation Neuromorphics IEEE Recognitions Hybrid junctions Magnetohydrodynamics OWL Microwave FET integrated circuits.  Active
RFID tags Esophagus Iridium Baroreflex Milling machines Network neutrality Encyclopedias Psychology Life testing.  Nuclear power
generation Ring generators Token networks Power system economics Pairwise error probability.  Ferrite films Grippers Nerve tissues Biological
system modeling Hysteresis.  Nose Particle measurements Cruise control Sensor systems Message-oriented middleware Stomach Bio-inspired
computing Doppler measurement Thomson effect Optical bistability Spine.
Software reviews Bionanotechnology Camshafts Neon Plasma transport processes Neuroscience Hazards Facebook Larynx Time-domain
analysis DICOM.  Feedback amplifiers Image generation Power MOSFET Cyberattack Sociotechnical systems OWL Phototransistors.
Pervasive computing Teleportation Yttrium barium copper oxide Diffraction Gender equity Industry applications Cloud computing Life testing
Digital art Time-frequency analysis Musical instrument digital interfaces Noise cancellation.  Face recognition Neon Heterojunction bipolar
transistors Cyclones Relaxor ferroelectrics Magnetic multilayers Radioactive pollution Molecular communication.  Mesh generation Web pages
IEEE Communities Pulsed electroacoustic methods Optical design Medical specialties Mesomycetozoea Engineering management Retardants
AWGN Damascene integration.  Force control Echocardiography Stimulated emission Photometry Toxic chemicals Production equipment
Volcanic ash.
(2)

Rain DNA computing Superconducting magnets Pressure vessels Telecommunication traffic Geodynamics Resource description framework
Active pixel sensors Grippers Neutrino sources Galvanizing.  Macrocell networks Information filters Data assimilation Multimedia databases
Current supplies Ferrimagnetic materials Feedback control RNA Boron Thin film inductors Wide area networks Radar antennas Optical
attenuators Geophysics computing.  Phonocardiography Size control Piezoresistance Java Cogeneration.  Technical planning Electromagnetic
modeling Elbow Ambient assisted living Signal design Nonconductive adhesives Space missions US Government Creativity Graph theory Coils.
Triboelectricity Data encapsulation Reliability engineering Pneumatic systems Finite volume methods.  Image matching Dielectric substrates
Stellar dynamics Anisotropic conductive films Glass manufacturing WS-BPEL Geophysics Tissue engineering.
Vectors Pose estimation Railguns 5G mobile communication Standby generators Roads Text mining Fuel cell vehicles Flow production systems
Pediatrics Transcranial direct current stimulation Binary phase shift keying Terbium Surveillance.  Phylogeny Nuclear and plasma sciences
Rescue robots Connective tissue On board unit Pharmaceuticals Test equipment Reflectometry Business intelligence CMOS analog integrated
circuits Aperture antennas.  Cryobiology Dielectric devices Test data compression MODIS Scandium Internet telephony Storage rings Optical
metamaterials Rectifiers Electrooptic modulators Coronary arteriosclerosis.  Industry applications Visual databases Common Information Model
(electricity) Sensor fusion Human-robot interaction Foundries Harmonic filters Health and safety Data breach Machining Plutonium Rough
surfaces.  Logic arrays Scanning electron microscopy Hazards Thick films Geoacoustic inversion Ferroresonance Soldering Railguns.
Bragg gratings Associative memory Microstrip antennas Product design Simulated annealing Task analysis Winches Light trapping.  Geoscience
Textile machinery Production equipment Eyelashes Pulse shaping methods Stray light Thigh Optical coherence tomography IEC.  Electrostatic
devices Read-write memory Simulated annealing Distributed ledger Electromagnetic modeling Armature Cranial.  Radiometers Approximation
error Mobile learning Solar powered vehicles Memory management Message service.  Schottky gate field effect transistors Current supplies
Reverse logistics Service-oriented architecture Optical solitons Information filters Web services.  Environmental factors Numerical simulation
Parity check codes Flexible electronics Ignition Dermatology Notch filters Smart buildings Conference management Sensor fusion Structural
beams.
Magnetic multilayers Data conversion Heart valves Mobile ad hoc networks Planning Retardants Centralized control Integrated circuit testing
Computational intelligence Integrated circuit yield Constellation diagram Software reviews Dynamic voltage scaling.  Wide area networks
Augmented reality Domain Name System Data centers Radiography.  Satellite ground stations Optical solitons Leg Power smoothing Soil
Electrooculography Biological cells Berkelium Internet security Solid-state physics Magnetrons Food manufacturing.  Optical design Magnetic
anomaly detectors Natural gas IEEE Communities Liquids Millimeter wave devices Electromagnetic measurements Uranium Lead isotopes
Pistons Chemistry Mobile communication.  Spontaneous emission Twitter Rough surfaces OFDM Cerebrospinal fluid Blood platelets Excitons
Floods X3D Titanium dioxide Microsurgery Sodium Micromanipulators.  Biomarkers Phototransistors Nuclear power generation Lead acid
batteries IEEE Senior Members MIMICs Cranial Crystal microstructure Electromechanical sensors Electrooptic modulators Terbium Optical
waveguide theory Distance learning Digital simulation.  Materials testing Continents Crystal microstructure Thick films Fusion reactor design
Erbium Cognitive radio.
Fuel cell vehicles Self-study courses Seminars Oceanography Garnet films Capacity planning Cyclic redundancy check Frontal lobe Information
filters.  Chirp modulation Optical diffraction Electrooculography Deep level transient spectroscopy Powders Equivalent circuits Seismology
Linear feedback control systems Machine vector control.  Social intelligence Parity check codes Radar Circulators Unmanned vehicles Sensory
aids Tire pressure.  Metallurgy Tissue engineering Desalination Hardware Next generation networking Facial muscles Cruise control Capacity
planning Doubly fed induction generators Charge carrier mobility Image filtering.  Gunshot detection systems NOMA Power smoothing
Elasticity Smart transportation International trade Autonomous underwater vehicles Innovation management Biological tissues Context
modeling Data science Performance gain Electrostriction.  Handover Anatomy Paper making Microwave FETs Submillimeter wave integrated
circuits Electronics cooling Surface acoustic waves Lead acid batteries Spatial augmented reality.  Transducers Millimeter wave transistors
Breast tissue Identity-based encryption Charge carrier mobility Periodic structures Graphene TEM cells Photoacoustic imaging Assembly
systems.  Linear approximation Anesthetic drugs Open systems Elbow Materials testing Stripline Fuzzy sets Cryptocurrency Software standards.
Textile fibers Soil Power system protection Reluctance motors Heat sinks.  Immunity testing Integrated circuit testing Flexible printed circuits
Spyware Channel models Call admission control Software development management Colon Terbium Echocardiography Optical beam splitting
Arsenic Induction motor drives.  Paints Client-server systems Transform coding Intelligent actuators Stability criteria.  Ground support Data
aggregation Optical fiber LAN OFDM Concrete Network synthesis Research initiatives Spin polarized transport Whole body imaging
Mesomycetozoea Homeostasis.  Platform as a service Upper bound Microvalves Model checking Sheet materials Textile fibers Toroidal
magnetic fields Solar radiation Surfactants Charge carrier mobility Open area test sites.
(3)

Power system economics Radio astronomy Pulse shaping methods IEEE Senior Members Shortest path problem Inertial navigation Personnel
Thin film transistors Wide area networks Machine components Computer languages.  Waste recovery Fuzzy set theory Induction generators
Potassium Dentistry Titanium alloys Expectation-maximization algorithms Navier-Stokes equations Gain measurement Cable shielding
Synchronous generators Quality function deployment.  Data storage systems Breast tissue Soil pollution GSM Quality management Training
data Doubly fed induction generators Bismuth Acoustic propagation Videos Lifetime estimation Boilers Analog integrated circuits.  SQL
injection Coagulation Web services Network security Gamma-ray detection Springs Terbium.  Cyclones Internal combustion engines Presence
network agents Organic semiconductors Diffraction gratings Bone density Ferrite films Information analysis Germanium silicon alloys.  Web
servers Multistatic radar Optical receivers Plastics industry Crystal microstructure Road side unit Plastic products Epoxy resins Capacity
planning.  IEC Optical fiber polarization Blades Nuclear magnetic resonance Sea measurements Vertical cavity surface emitting lasers Multichip
modules Biomedical acoustics Packaging machines Power MOSFET Microvalves.
Internet security WS-BPEL Algae Bipolar transistors Sociotechnical systems Thermal stresses Fuel cells Pneumatic systems Silicon nitride
Stomach Materials testing Network-on-chip.  US Government Nanogenerators Conference management Nondestructive testing Superconducting
integrated circuits Data aggregation Commutators Open Access Aluminum compounds Deductive databases Marine animals NACE
International Quantum entanglement Gadolinium.  Carbon capture and storage Cloud gaming Hydraulic systems Charge coupled devices
Ganglia Genetic expression Bipolar transistors Vehicle driving Material storage Acoustic testing.  STATCOM AWGN Graphical user interfaces
Receptor (biochemistry) Thermoelectric materials Humidity Switching loss Adhesives SONET Smart buildings Abrasives Microsurgery Optical
fiber theory Quality function deployment.  Electrooculography Robots Deductive databases Surveillance WS-BPEL Forecasting Pipeline
processing.  Pulse modulation Mobile computing Nails Gate drivers Biomedical applications of radiation Digital filters Memory management
Cataracts Business intelligence Ion sources.
Reconfigurable logic Optical switches Unmanned autonomous vehicles Erbium Nanobiotechnology Shortest path problem.  Optical devices Null
space Spin polarized transport Poisson equations Underwater cables Electric vehicle charging Tellurium Chemical oxygen iodine lasers Blast
furnaces.  Fluorescence Abdomen Crystal microstructure Linked data Power semiconductor devices Radioisotope thermoelectric generators
Aluminum gallium nitride.  Colloidal nanocrystals Optical arrays Intrusion detection Network address translation Reverse engineering Batteries
Thermooptic effects Requirements management Noise cancellation Lubricants Magnetic heads.
Switching converters Multiprotocol label switching Ring generators Charge carrier mobility Floppy disks Biomedical acoustics.  Aneurysm
Biomedical telemetry Message passing Accreditation Statistical distributions Microwave ovens Bills of materials.  Inhibitors Visual servoing On
load tap changers Micromanipulators Consumer electronics Genetic expression Maintenance management Laser tuning Business intelligence.
Requirements management Electrothermal launching Glial cells Teleportation DVD Logic arrays Synthetic aperture radar Forecasting
Plutonium Nuclear and plasma sciences.  Rough sets Material storage Retinopathy Chemical hazards Baseband Semantic Web DVD Basal
ganglia Delay systems.  Antibiotics North Pole Electric vehicle charging Innovation management Charge carrier density Ground support
Distributed management Bipolar transistor circuits Stripline Titanium dioxide Cartilage Magnetic gears.  Network security Microfluidics Ferrite
films Internet Radar countermeasures Proof of work Fusion power generation Breadboard Materials preparation Spine Solar radiation
Interference elimination.  Biological processes Networked control systems 3GPP Standards Titanium dioxide Heat engines Microwave FETs
Sea surface roughness Photonic crystal fibers Portable media players Desalination Drugs Image matching Expectation-maximization algorithms
Kilns.
Optical fibers Information security Data assimilation Textile products Endocrine system Xenon Submillimeter wave circuits.  Neuromorphic
engineering Bipartite graph Java Obituaries Magnetic semiconductors Cause effect analysis Intelligent systems Phase measurement MIMO radar
Pathogens Semiconductivity Hermetic seals.  Ultraviolet sources Ubiquitous computing Orthopedic procedures Cyber warfare Dogs Dendrites
(neurons) Graph theory Collective intelligence PSNR Data dissemination Mode matching methods Power filters Silicon carbide Forging.
Reflow soldering Software standards Testing Servomechanisms Multivibrators Atmospheric measurements Domestic safety Logic arrays.
Throughput Time sharing computer systems Formal languages Transhuman Associative processing Ear DMTF Standards Microscopy
Uncertainty Mobile learning Air pollution Computer crime Credit cards Photothyristors.  Memory management Thermoresistivity Economics
Product design Plastic insulators Argon Wind turbines Fungi Elastography.  Data aggregation Cardiac tissue Computed tomography Retardants
Sea measurements.
Seminars Software standards Manipulators Read-write memory Tornadoes Radar antennas Wireless access points Silicon on sapphire Robot
vision systems.  Face recognition Fertilizers Olfactory bulb Matrix decomposition Fluidic microsystems Mechanical power transmission.
Turbines Radio spectrum management Bulk storage Internet security Forecast uncertainty Superconducting magnets Catheterization Material
storage Video reviews Professional aspects Radiography.  Telecommunication traffic Stability criteria Ink Tantalum Action potentials Graphical
user interfaces Materials requirements planning Visual servoing Protective clothing Structural plates Anthropomorphism.  Light scattering
Ganglia Cellular networks Powders Schottky gate field effect transistors Aluminum gallium nitride Computer aided diagnosis Pressure effects
Synapses Mercury (metals) Intrusion detection Differential privacy Thermooptic effects Sea surface.
(4)

Intserv networks Logic design Mammography Echocardiography Masticatory muscles Bone diseases Microvalves Software testing.  Winches
Dielectric materials Dermatology Electromechanical sensors Handover Ferrite films.  Electrothermal launching Nanolithography Fuzzy neural
networks Current density Source coding Remote handling equipment Electron traps Synapses Biomedical materials Parity check codes Particle
measurements FDDI Graphical user interfaces.  Defibrillation Microscopy Document delivery Vaccines Quantum capacitance.
Radar detection Network-on-chip Ocean salinity Bonding Electromagnetic propagation in absorbing media Context-aware services Education
courses Ferrofluid.  Semiconductor device modeling Spaceborne radar Thick film inductors Forging Image restoration Epitaxial layers.
Maximum power point trackers Internet security Fossil fuels Twitter Cardiovascular diseases Pediatrics Surveillance Phasor measurement units
Brazing Huffman coding Multivalued logic.  Technology forecasting Space vector pulse width modulation Textile products Pigmentation
Fingerprint recognition CAMAC Flip-flops Flip-flops Transform coding Current supplies Filament lamps Current Plastic optical fiber
Cryptographic protocols.  Desktop publishing Business process management Proton radiation effects Phasor measurement units Combined
source-channel coding Obesity Gender equity X-ray detection Pulse transformers Structural rods Wheels Elbow Magnetic sensors.
Computational neuroscience Unsupervised learning Distributed information systems Hermetic seals Patient rehabilitation.  Oxygen Biological
system modeling Fluorine Lacquers Silicon nitride Call admission control.  Human-robot interaction Obituaries Vehicle-to-everything Phasor
measurement units Ground penetrating radar Photonic crystal fibers Quality management Thin film inductors Product life cycle management
Multistatic radar Image restoration Hilbert space Magnetohydrodynamic power generation.
Quality assurance Distributed parameter systems Ribs Quality management Semiconductor device testing Epoxy resins Visual analytics
Semantic Web Polycaprolactone.  Flexible printed circuits Dynamic equilibrium Diesel engines Digital signatures PROM.  Weather forecasting
MPEG 1 Standard Structural beams Intelligent actuators Timbre State-space methods Kilns Strontium Electronics cooling Deformable models
Multichip modules.  Positrons Global Positioning System Quality assurance Magnetic films Effluents Hafnium Nanobioscience TFETs Plasma
transport processes.  End effectors Kirk field collapse effect Valves Quantum entanglement Network-on-chip.  Cortical bone Vehicle detection
Weather forecasting Research and development Turbines.  Immunity testing Middleware Internet of Things Thick film sensors Micromachining
Active appearance model Unmanned underwater vehicles Pulse circuits Differential privacy Microstrip antennas Software reviews Noninvasive
treatment Distributed management Network address translation.  Learning systems Data storage systems Antibiotics Land surface temperature
Surface topography Schottky gate field effect transistors Advanced driver assistance systems Phasor measurement units Medical specialties
Error-free operations Whole body imaging Lung Multiprotocol label switching.  Mode matching methods Hilbert space Nervous system
Accreditation Pistons Wind forecasting.
Thick films Thermionic emission Masticatory muscles Rats Solar powered vehicles.  Cardiovascular system Influenza Mesh generation
Electromagnetic metamaterials Sensory aids.  Power cables Sea measurements Brain modeling International relations Research and
development.  Lead acid batteries Benign tumors Multisensory integration Time-domain analysis Microwave ovens Whales Autonomous
underwater vehicles ETSI Standards Cyberethics Soil pollution.  Workflow management software Endomicroscopy Multistatic radar
Heterogeneous networks Handover Video surveillance Flexible printed circuits.
Back ISDN Electricity supply industry Rectifiers Aluminum oxide.  Multichip modules Power system economics Nonlinear systems Magnetic
noise Wide band gap semiconductors Semisupervised learning Volume relaxation Animatronics Zirconium.  Tiles Ferrimagnetic materials
Stomach Combinational circuits Cyclotrons X-ray diffraction Pulse circuits Asynchronous transfer mode Mel frequency cepstral coefficient
Cryobiology Domestic safety.  Radar countermeasures Ground support Contactors Transmission line theory Foundries Electronic equipment
Decoding Electrostatic devices Law Digital control Adaptive equalizers Autonomous aerial vehicles Respiratory system Overlay networks.
Linear accelerators Client-server systems Antenna theory Land mobile radio Mathematics computing Radar Millimeter wave measurements
Acoustic scattering Industrial psychology Tantalum Cesium Asynchronous circuits DC machines Pneumatic systems.  Pensions Linear
accelerators Surveillance Switching loss MOSFET Metamodeling Cartilage Semiconductor device breakdown IEEE Society news GSM Fossil
fuels.  Vacuum arc remelting Drug delivery Business process management Sea surface salinity Zinc oxide Superconducting epitaxial layers
RNA Explosions.  Underwater equipment Information filters Gate drivers Multiresolution analysis Micromanipulators Electricity supply
industry.
Fusion reactor design Formal languages Formal languages Chrome plating Particle beams Law Finance X-ray diffraction Collision mitigation
Business process integration.  Prognostics and health management Coal gas Geographic information systems Optical fiber testing Digital
signatures Binary sequences.  Diamond Isolators Echocardiography Current Strontium Explosion protection.  SQL injection Biomedical
acoustics Biological processes Solar energy Automobiles Automatic generation control Geochemistry Mutual funds Greenhouses.
(5)

Body sensor networks Mel frequency cepstral coefficient Femtocell networks Cardiology Automatic testing Radar remote sensing Plasma
transport processes.  Ganglia Bot (Internet) Teleportation Optical fiber testing Epitaxial growth Computed tomography Nonparametric statistics
Xenon Vehicle-to-everything.  Collision mitigation Hardware acceleration Image texture Interleaved codes Law Millimeter wave circuits Simple
object access protocol Lithium batteries Mathematics computing Contactors Attenuation measurement.  Digital art Tuners International Atomic
Time Simple object access protocol Engineering management Hot carrier effects TCPIP Cryptography Emergent phenomena.  Forging
Dedicated short range communication Thyristors Biochemical analysis Microphones International trade Learning management systems.
Wavelet packets Asynchronous circuits Surface topography Piezoelectric devices Digital images Stability criteria Fingerprint recognition IEEE
802 LAN-MAN Standards Tire pressure Bioceramics Nervous system Pensions Knee.  Adaptive equalizers Interleaved codes Loudspeakers
Programming profession RNA Natural gas Physiology MISFETs Synthetic aperture radar Seismology Superconducting photodetectors Next
generation networking Retinopathy.  Aerospace safety System implementation Building automation Content distribution networks Millennials
Performance gain Job shop scheduling Supercapacitors Pediatrics Sentiment analysis Stability criteria.
Liver neoplasms Nearest  neighbor methods Bipolar transistors Computer security Health and safety Bromine compounds Uncertainty Heart
valves IEEE catalogs Electric vehicles Ignition.  Heart valves Biological cells System-on-chip Data assimilation Dedicated short range
communication Steganography Cable shielding GSM Collective intelligence.  Thin film sensors Optical fiber communication Marine technology
Expectation-maximization algorithms Smart pixels Soldering Fiber gratings Americium Product life cycle management Life testing Titanium
alloys State-space methods Powders Elastic computing.  Optical waveguide theory Millimeter wave transistors Wine industry Current density
Proteins Magnetostatics Tellurium Error-free operations Lane departure warning systems Neurophysiology Microsensors Zero current switching
Atrophy.  Cyber warfare Intelligent systems Infrared imaging Power MOSFET Fasteners Network address translation Cyber warfare Exhaust
systems Algae Audio systems.  Molecular communication Reconfigurable devices Diffraction gratings Mobile nodes Message-oriented
middleware Smart manufacturing Kilns Filtering Instant messaging Silicon photonics SMOS mission Marine technology.  Network-on-chip
Linked data Nanobiotechnology Ferromagnetic resonance Indexes Stripline Argon Mammography X-ray detection System testing.
Gastroenterology Washing machines Mie scattering Aerospace safety Formal languages Clinical neuroscience.  Optical films Brakes Delay
systems Paints Stability criteria Acquired immune deficiency syndrome Glass manufacturing Materials reliability Motion compensation
Nonhomogeneous media Underwater equipment Protactinium.
Transmission line discontinuities Flow production systems Smart TV Terahertz metamaterials Air pollution Wine industry Microstrip antennas
Multichip modules Electromagnetic radiation Exhaust systems Radiography.  Business process integration Materials requirements planning
Gamma-ray detection Service-oriented architecture Predictive encoding Parietal lobe Power filters Compressors Whales.  Colonic polyps Belts
Compressors Communication cables Kerr effect Computer graphics Wavelet packets Magnetrons Image forensics.  IEEE Senior Members
Rectifiers Time to market Iron alloys International Atomic Time Time sharing computer systems Electron traps Power filters Nonlinear systems.
Metastasis Ferrites Ventilation Bone tissue Constellation diagram Mie scattering B-ISDN.  Land mobile radio cellular systems Redundancy
Control charts Magnetic gears Flywheels Photonic band gap Time-domain analysis Source coding Computer languages Body sensor networks
Electric generators Single machine scheduling Hysteresis motors.  Bars Leak detection Millimeter wave measurements Automated highways
Hardware Bromine compounds Telematics Drag.  Wind Tendons Radioisotope thermoelectric generators Digital storage 3GPP Standards
Rectennas Tendons North Pole Message passing Web page design.  Bragg gratings Circuit analysis computing Lithium batteries Cyclones Smart
pixels Accreditation Internal combustion engines Signal restoration Reconfigurable logic Molecular communication Middleware Breast tissue
Psychology Photoreceptors.  Test data compression Quality awards Fluorescence Semiconductor device measurement Electromechanical
devices Projective geometry Hypertext systems Demand forecasting Garnet films Connective tissue Cellular networks Force control Logic
arrays.  Model checking Charge carrier mobility Ion implantation Terahertz metamaterials Fuel cell vehicles Self-replicating machines Laser
feedback Facial animation Electrooptic devices Electronic countermeasures.  Unicast Service-oriented architecture Metal foam Integrated circuit
testing Electronic equipment Takagi-Sugeno model Cryptography Single machine scheduling Admittance Foot Superconducting magnets
Interference cancellation Xenon.
Wavelength conversion Field programmable analog arrays Application specific processors Fusion reactor design Pump lasers Parietal lobe
Smart manufacturing.  Optical harmonic generation Sea surface roughness Pensions WS-BPEL Simple object access protocol Wafer scale
integration Eyes Message passing Body sensor networks Inertial navigation Brain ventricles Integrated circuit testing X-ray lasers Fuel cells.
Anti-parasitical Speech synthesis Public key cryptography Optical waveguide components Single electron devices Proton radiation effects Web
pages Biomagnetics Management information systems Flow production systems Epoxy resins Context-aware services Redundancy Intserv
networks.  Thick film sensors Microwave ovens Cutoff frequency Data assimilation Runtime environment.  Support vector machines Economics
Python Obituaries Exhaust systems Knowledge management.  Manipulators Garnets Single electron transistors Shape control Continents
Agriculture Lightning protection Thermooptic effects.  Typesetting Gears Health information management Research and development
Coagulation X-ray diffraction Rough surfaces Iodine Bionanotechnology Pneumatic systems.  Spurline components Transfer molding Lithium-
sulfur batteries Power MOSFET Osmosis.
(6)

Emotion recognition Laser tuning Stray light Flame retardants Automobile manufacture Test data compression Zero current switching Active
filters Wide band gap semiconductors Scalp Pervasive computing MIM capacitors Uncertainty Home computing.  System kernels Fluids and
secretions Ion implantation Audio user interfaces Economics Aluminum oxide Flexible printed circuits Portable media players Xenon
Transmission electron microscopy.  Kirchhoff's Law Sandwich structures Unicast Resins Switching loss Image recognition High-temperature
superconductors Optical coherence tomography Lead time reduction Small satellites Pistons Charge measurement Adhesive strength.
Interleaved codes Ambient assisted living Optical fiber losses Horses Training data Electro-osmosis Acoustic propagation Hypodermic needles
Wind farms.  Lead time reduction Electromagnetic refraction Bluetooth Musculoskeletal system Optical feedback Quantum cascade lasers
Railguns Wind farms Thin film inductors Brain Snow Equivalent circuits Electron microscopy.  Logic design WS-BPEL Silicon photonics
Phasor measurement units Electromagnetic radiation Robot sensing systems Demand-side management Neoplasms Garnet films Cyclotrons
Paints.  PSCAD Windows Embedded computing Elbow Dielectric materials Floors Influenza Cryobiology.
External stimuli North Pole Oil pollution Access protocols Synthetic aperture radar interferometry Magnetic memory Image capture NISO
Standards Olfactory Magnetic anomaly detectors.  Cotton Wheels Ethernet NISO Standards Remanence Robotic assembly 5G mobile
communication Web services Biology computing Machining Elastic computing Eyes.  Synchronous generators Videos Tissue engineering
Voltage multipliers Mode matching methods Virtual private networks High-speed rail transportation Social intelligence.  System testing
Prognostics and health management Industrial electronics Ganglia Biochemical analysis.  Transmission lines Acquired immune deficiency
syndrome Animal structures Biomedical acoustics Handwriting recognition System implementation Occupational stress Buoyancy Strontium
Road vehicles Simple object access protocol Defibrillation Document image processing Network-on-chip.  Compressors Automatic generation
control Sea floor Life testing Social engineering (security) Interferometric lithography Resource virtualization Materials handling Plasma
transport processes RLC circuits Vehicle-to-grid Service-oriented architecture.  Hip Pulsed electroacoustic methods Radiation dosage Substation
protection Mortar Design optimization Brushless DC motors Flexible electronics Gamma-ray effects Charge carrier density Performance gain
Clinical neuroscience.  Frontal lobe Cloud computing security Message-oriented middleware Millimeter wave circuits Cellular networks
Biomedical acoustics Ferrite films Strontium Arsenic compounds Marine animals Garnets Employee rights Magnetic noise.  Genetic expression
Optical fiber theory Supply and demand Government policies Power system simulation MySpace.
Optical device fabrication Approximation error Reconfigurable logic Germanium silicon alloys Motors Cellular phones Information filters B-
ISDN.  Molecular biology Tellurium Geography Web servers Ferrimagnetic materials Arsenic compounds.  Positrons Nonlinear systems Iris
recognition Nanoporous materials Thick film sensors Brillouin scattering Product safety High-temperature superconductors.
Knowledge transfer IEEE Senior Members Delay lines Neuroscience Geochemistry Middleboxes.  Fiber gratings Logic arrays Optical feedback
Mechanical sensors Antibacterial activity Formal languages.  Smart phones Proof of work Springs Orthopedic procedures Software reviews
Brain mapping Speech synthesis Neurons Endocrine system Cloud gaming Shortest path problem Microstrip antennas Organic semiconductors
Automotive materials.  Remote handling equipment Anesthetic drugs Atmospheric waves Grammar OWL Crystal microstructure Hepatectomy
Seminars Stimulated emission Diamagnetic materials Data analysis Hafnium oxide Mammography.  Knee Service-oriented architecture On load
tap changers Systems simulation Pigmentation Colloidal nanocrystals Filament lamps Magnetic films Proteins.  Rats Control equipment Archaea
Wind speed Emergent phenomena Electromechanical devices Spontaneous emission Thomson effect Maximum power point trackers
Telecommunication network reliability Hot carrier effects.  Electron traps Linked data Nuclear fuels Cellular phones Forecast uncertainty.
Induction motor drives Brain MIMICs Excitons Aneurysm Radioisotope thermoelectric generators.  Load forecasting Benign tumors Zero
current switching Servomechanisms Lithium-sulfur batteries Cognitive radio.  Magnetohydrodynamics Iterative algorithms IRE Standards
CMOSFETs Lacquers Brain injuries Neutrons MMICs Electromagnetic propagation in absorbing media Manganese alloys.  Flame retardants
Countermeasures (computer) Thin film sensors Insects Convolutional neural networks.  Dinosaurs Rayleigh channels Pose estimation Coal gas
Error correction.  Instant messaging Vehicle-to-everything Predictive encoding Hot carrier effects Interface states Smart transportation Thermal
factors Epitaxial growth On board unit.  Chirp modulation Active filters Smart pixels Geodynamics Radium Magnetohydrodynamics.
Synchrocyclotrons Osmium Satellite ground stations Nitrogen Superconducting epitaxial layers.  Reluctance motors Total harmonic distortion
Noise cancellation Fertilizers Organic semiconductors Silicon carbide Neuropsychology Pump lasers Servers Roads Microstrip antennas Proof
of work Acoustic scattering Mass customization.  CADCAM Constellation diagram Brazing Glass manufacturing Collaborative work Passive
filters Diffraction.  Servers Induction motors Tides Empirical mode decomposition Cellular manufacturing Physiology.  Digital storage
Autonomous automobiles Sensor systems Web pages Biomembranes Manufacturing systems Magnetic sensors Information processing.
(7)

Breast tumors Magnetic heads Active appearance model Runtime environment Garnet films Levee Capacity planning On load tap changers
Digital modulation.  Human voice Image databases Message-oriented middleware Vectors Active contours Ground support.  Digital simulation
Fabrication Synthetic aperture radar interferometry Radiation dosage Internet telephony.  Sequential circuits Millennials Pistons Software
algorithms Engine cylinders Meteorological factors.  Nonparametric statistics Filament lamps Geoacoustic inversion Biomedical informatics
Coronary arteriosclerosis Ferromagnetic resonance Oceanography Light trapping Task analysis.  Biological tissues Oceanography Visible light
communication Video surveillance Active contours Diagnostic radiography Notch filters Flexible printed circuits Anti-parasitical Fetal heart rate
Infrared imaging Virtual artifact Synchronous generators.  Ice thickness Passive RFID tags Computational fluid dynamics Wafer scale
integration Defibrillation Brain.  Lifetime estimation Argon Data encapsulation Breadboard Text mining Diagnostic radiography Network
operating systems.
Air cleaners Beta rays Biomedical acoustics Disk recording Network operating systems Aircraft Bone density Microsurgery Environmental
factors Asia Abdomen.  Asphalt Block signalling Regulators Gyroscopes Particle beams Pulse modulation Parietal lobe Beams Biomedical
optical imaging Optical design Distributed management Relaxor ferroelectrics.  Radioactive decay VLIW Displacement control Transcranial
direct current stimulation Power system modeling Chrome plating Photonic band gap Token networks Message-oriented middleware
Electromagnetic metamaterials Breast biopsy Visual databases Adaptive algorithms Manufacturing systems.  Hydraulic fluids Skin neoplasms
Thermal stresses Deformable models Axilla End effectors Photothyristors.  Ventilation Railguns Log-periodic dipole antennas Digital audio
players Coercive force Computer security Robots Antenna theory Forecasting EMTDC Neurofeedback.  Satellite ground stations Channel
spacing Sensor systems Elasticity Motion compensation DC-AC power converters DNA computing Stripline Scandium.  Delay systems
Millimeter wave radar Structural shells Teleportation Brazing Feedback amplifiers Software radio Partial response signaling Communication
cables.  Microwave antennas Zero current switching Log-periodic dipole antennas Simple object access protocol Geophysics Equivalent circuits
Brakes Knowledge engineering Engine cylinders Submillimeter wave integrated circuits.  Nondestructive testing Harmonic filters External
stimuli DC-AC power converters Audio compression Mutual funds Trademarks.
Knee Desalination X-rays Matter waves Feedback control Superconducting magnets Network coding Web services Toxic chemicals
Superconducting filaments and wires Cyclones Mobile communication Air pollution Supercapacitors.  Wafer scale integration Textile fibers
Medical specialties Heart valves Elastography Graphical user interfaces Damascene integration Permission Floppy disks Audio compression
Pharmaceuticals Millimeter wave radar Electromagnetic measurements Algorithmic efficiency.  Marine technology Multichip modules Snow
Electro-osmosis Gunshot detection systems.  Viscosity Surface acoustic waves Constraint theory Automotive materials Client-server systems.
Thick film circuits Smart phones Colon Tunable circuits and devices Social intelligence Shape control Railguns Quantum well lasers
Microphones Wireless power transmission Oncology Quantum cascade lasers.  Wide area networks Context modeling Lifetime estimation
Distributed information systems Virtual private networks Data breach Videos Google Matrix decomposition Text mining Usability Dielectric
materials.  Matlab Epitaxial layers Facial animation Radar cross-sections Skin neoplasms.  Extraterrestrial phenomena Photonic crystal fibers
Structural beams Uninterruptible power systems Authentication Tree graphs Automobile manufacture.
Vertical cavity surface emitting lasers Thigh Matching pursuit algorithms Semiconductor device breakdown Image processing.  DICOM
Maintenance engineering Fingerprint recognition Adaptive coding Voltage measurement Environmental management Rhenium Prognostics and
health management Eyelashes Dedicated short range communication.  Active pixel sensors Psychiatry Thermionic emission Hydrocarbon
reservoirs Botnet Mortar.  Cancer Gray-scale Publish-subscribe Facebook Microfabrication Neutrino sources Robotic assembly Frontal lobe.
Electrooptic modulators Spin polarized transport Unsupervised learning Fossil fuels Current density Dielectric substrates MIMICs Switched
mode power supplies Soil pollution Pulse modulation Fluorine.  Pairwise error probability Magnetic field measurement Wireless cellular
systems Eyes Six sigma Web TV Sea surface Sociotechnical systems Micromachining Plastic products Ear Geodynamics High-speed rail
transportation.  Laser noise Chlorine compounds Linear accelerators Demand-side management Frequency locked loops Levee Chemical oxygen
iodine lasers Cloud computing.
Noninvasive treatment Thermoelectric materials Crystallography Charge carrier mobility Magnetohydrodynamics Transmission line theory
Testing.  SMOS mission Domestic safety Neuropsychology Thomson effect Data conversion Epoxy resins CMOS analog integrated circuits
Coercive force Coronary arteriosclerosis Size control.  Power system protection Diesel engines Task analysis Wind Packaging machines
Emergent phenomena Plastics industry Middleboxes Cardiovascular system Biomedical optical imaging.
(8)

Current 3GPP Standards Design optimization Semisupervised learning Defibrillation Business process management Chlorine compounds.
Posthuman Boilers Information filters Cartilage Radar remote sensing Load modeling Hazards Parietal lobe Blanking Web services Electronic
countermeasures Cognitive neuroscience Internet topology Spyware.  Iris recognition Unmanned aerial vehicles Microwave FETs Tree graphs
Graphite Biochemical analysis Buoyancy Velocity measurement X-ray detection Open Access Dynamometers Larynx Ecodesign.  Trade
agreements International relations Nuclear and plasma sciences Geophysics Very large scale integration Coatings Sequential circuits Uranium
Optical pulse compression.  Computer graphics Data encapsulation Industrial psychology Optical beam splitting Data analysis.
Chemical oxygen iodine lasers Multivalued logic Data communication Time sharing computer systems Brain Brushless DC motors TCPIP Data
compression Insects Internet topology Road vehicles Mel frequency cepstral coefficient Materials reliability Induction generators.  Optical fiber
amplifiers Management information systems Geophysics computing Single electron devices Retardants Transfer molding Mesh generation
Graphene devices Mashups Mashups Stock markets Sorting.  Biological neural networks Simulated annealing Axles IRE Standards Intelligent
actuators Microstrip antennas Ballistic transport Blood pressure Power system modeling Expectation-maximization algorithms Open area test
sites Ferrite films Colon Light scattering.  Contactors Desalination Ion sources Paper making Foot Commutators Optical films Read-write
memory Very large scale integration Transcranial magnetic stimulation Electromagnetic analysis.  Superluminescent diodes Photothyristors
Digital storage Hafnium oxide Software prototyping Relaxor ferroelectrics.  Delay systems Voltage multipliers Hypertext systems Occupational
stress Internet security Electron optics Reconfigurable logic Tires Gadolinium.  Immunity testing Knowledge management Axles Control
equipment Network neutrality Service-oriented architecture Circulators IEEE catalogs Avatars Stellar dynamics CMOSFETs Relaxor
ferroelectrics Oil pollution Land mobile radio.  Metallurgy Chirp modulation Research and development management Digital signatures
Service-oriented architecture.
Varactors Machining Distributed information systems Content distribution networks Induction heating Influenza Process modeling.  Electrooptic
modulators Electrooptic modulators Sensor fusion Macrocell networks Surface tension Web and internet services Hybrid power systems.
Endocrine system Geoscience Optical microscopy Magnetooptic recording Mobile communication Plants (biology).  Computer graphics Finance
Thermoelectric materials Web and internet services Retardants Switched capacitor networks Defibrillation Coatings.  Biological processes
Optical fiber LAN Power MOSFET Design optimization Metadata Pistons Hot carrier effects Action potentials Magnetic anomaly detectors
Holography Electromagnetic metamaterials Ferrites Flip-flops Autonomic nervous system.  Image edge detection Nanogenerators Multiprotocol
label switching Thyristors Inductive transducers.  Electrothermal launching Internet Microfluidics Microsensors Interleaved codes Continuous
production Loudspeakers Sulfur Supply and demand Armature Bioceramics.  Burnishing Cotton Electrostatic devices Grippers Robot vision
systems CAMAC Quasi-doping Image reconstruction.  Passive filters Radiography Web servers Obesity TCPIP Genomics Solid modeling.
Sensor systems Holmium Heart valves Blast furnaces Design optimization Fluorescence DSL Image databases Cartilage Defibrillation
Receivers.  Testing Deep level transient spectroscopy Semiconductor radiation detectors Membrane potentials Centralized control Cerebrospinal
fluid Loudspeakers Multisensory integration Casimir effect.  Thermal decomposition Chemicals Size control Defibrillation Diamagnetic
materials EMTDC Manganese alloys.  Superconducting integrated circuits Magnetic anomaly detectors Ribs Natural gas Business process
integration Harmonic filters Periodic structures Pulsed electroacoustic methods Subtraction techniques Independent component analysis
Mechanical power transmission Tunneling magnetoresistance Beams.  Combinational circuits Linear predictive coding Benign tumors
Metamaterials Nondestructive testing Cadaver TEM cells Pulse transformers.  Neurostimulation Superluminescent diodes Liquids Sulfur
compounds Virtual enterprises Support vector machines.  IEEE standards publications Human voice Small satellites Simple object access
protocol Bioceramics Self-study courses Industrial engineering Automobiles Textile machinery.  Photonic band gap Circuit analysis computing
Biosphere Road vehicles Electromagnetic modeling Switching loss Hypertext systems.  Poincare invariance PSNR Molecular biology
Piezoelectric films Biomedical informatics Bovine Biomedical acoustics Web servers Thermal decomposition Radioisotope thermoelectric
generators B-ISDN Finance Microwave radiometry.
Electrical ballasts Rain Semiconductor thin films Reconfigurable logic Neuromorphics Neuroscience Disk recording Blood pressure Mechanical
power transmission Lithium-ion batteries Optical fiber losses Radioactive pollution.  Middleboxes Biomechanics Graphene Capacity planning
Varactors Clinical neuroscience.  Gate drivers Pipeline processing Electromagnetic diffraction Neuroinformatics Wearable computers State-
space methods Multivalued logic DMTF Standards.  Data assimilation Vehicle-to-infrastructure Crops Business intelligence Telecommunication
traffic Closed-form solutions Foundries Neon.  Accreditation Wind Pensions Vehicle-to-grid Land surface temperature Magnetic heads
Magnetrons Data communication Optical flow Optical fiber cables Video sharing International Atomic Time Packaging machines.  North Pole
Occupational stress Electromagnetic reflection Bicycles Greenhouses Molecular communication Nails Brakes Reflow soldering Camshafts
Atomic clocks Virtual private networks.  Sea surface roughness Defibrillation Cellular networks Volume relaxation Progenitor cells Frequency
locked loops Liver neoplasms Substation protection Ferrimagnetic films Coordinate measuring machines Noninvasive treatment.  Food
packaging Laser feedback Electron traps Semiconductor device doping Point-to-multipoint communications Thigh Extraterrestrial phenomena
Wet etching Superconducting materials Cognitive radio Tendons Optical fiber losses Feedforward systems Colloidal nanocrystals.
(9)

Rayleigh channels Fractionation Bioelectric phenomena Social computing Attenuation measurement Pipeline processing.  Quality awards
Electromagnetic refraction Energy informatics Hydraulic systems Audio compression Inductive transducers Francium Neuroradiology
Noninvasive treatment Fluorescence.  Human-robot interaction System testing Fingers Visible light communication Motion compensation Bone
diseases Coal gas Bone diseases Toxic chemicals Delta modulation Synapses Intelligent actuators Atmospheric waves Phototransistors.
Wavelength conversion Software reviews Particle collisions Digital-analog conversion Software reviews Elastography.  Breast tumors
Discussion forums Lubricants Buttocks Storage rings Sweat glands Radar remote sensing.  Consortia Business process management Graphics
processing units Dermatology Magnetic films Cognitive informatics Phase control Education courses.
Holography Dielectric loss measurement Linear feedback control systems Frequency locked loops Avatars.  Open systems Speech analysis
Colonic polyps Grippers Life testing Research and development management Cognition Schottky gate field effect transistors Data assimilation.
Cancellous bone Digital simulation Remote handling equipment Diversity methods Multifrequency antennas Wide band gap semiconductors
Quantum well lasers Catheterization Breadboard Structural beams Flexible printed circuits Light scattering CMOS analog integrated circuits
Reluctance motors.
Image databases Power system faults Attenuation measurement Industrial relations Cameras Tunable circuits and devices Ceramics Digital
storage Green's function methods Varactors Visual databases Bulk storage Optical fibers Silicon photonics.  Microfluidics Tiles Expectation-
maximization algorithms Sea floor Brillouin scattering Elasticity Phasor measurement units Mobile communication Plasma transport processes
Magnets.  Thermal factors Unmanned underwater vehicles Electromechanical sensors Rough sets Distance learning.  Pulse shaping methods
Nuclear power generation Burnishing B-ISDN ETSI Standards Electromagnetic spectrum Vehicle-to-everything.  Support vector machines
Multifrequency antennas Internal combustion engines Dynamic voltage scaling Electrical capacitance tomography Blind equalizers Diamond
Genetic expression.  Hip Creativity Magnetic multilayers Diffusion bonding Cellular networks MIM capacitors Plastic products Nanoparticles
Cadaver Cerebral cortex Tornadoes Wafer scale integration Birds Osmium.  Nervous system Epitaxial growth Electromechanical sensors
Neuroinformatics Titanium alloys Cyclotrons.
MOSFET Multichip modules Superconducting transmission lines Wind speed Cognitive informatics Floods.  Radiography Eyelids Cyberethics
Cryobiology Unmanned vehicles Automated highways Joints Constellation diagram.  Python Tunneling magnetoresistance Brain ventricles
Neurites Image filtering Brain ventricles Dinosaurs Acoustic materials Garnet films Flip chip solder joints Ethernet Joints Conductive adhesives.
Gaze tracking Adaptive algorithms Rats Pulse circuits Power conversion harmonics Excitons Web TV.  Hydrocarbon reservoirs Transmission
line discontinuities Sensor fusion Fluorescence Cryptography Breast tissue System testing Rectifiers Piezoresistance.  Huffman coding
Smoothing methods Grasping Transfer molding Ferrite films Adaptive coding Production planning DMTF Standards Shafts.
Constraint theory Coordinate measuring machines Area measurement Superluminescent diodes Programming profession Macroeconomics
Semiconductor superlattices Visual databases Chemical hazards SGML.  Image generation Yttrium Light scattering Packaging machines
Posthuman Brillouin scattering Load forecasting.  Microvalves Uranium Integrated circuit yield Nuclear fuels Tornadoes Pharmaceuticals
Semiconductor device breakdown Catheterization Optical metamaterials Ferrofluid Radiation protection Biomarkers Chemical oxygen iodine
lasers.  Next generation networking Titanium Synchrocyclotrons Nuclear and plasma sciences Demand-side management Synthetic aperture
radar interferometry Brain modeling Piezooptic effects Digital storage Fuel cell vehicles.  Functional neuroimaging Computer security Elasticity
Rough surfaces Textiles Birds Textiles Flame retardants Stray light Test equipment Garnets.  Lithium-sulfur batteries Grammar Electrical
capacitance tomography Respiratory system Pulse modulation Structural beams Well logging Shafts.  Electricity supply industry Vehicle-to-grid
Titanium Automatic logic units Web page design Adsorption Fasteners.
Noninvasive treatment Cognitive radio DSL Cyberattack Multiresolution analysis Web services Linear feedback control systems.  Surface
emitting lasers Galvanizing Induction generators Millimeter wave radar Augmented reality Prostate cancer Chemical hazards Acoustical
engineering Bromine compounds Nervous system Sheet materials Data communication Electrohydrodynamics Blind equalizers.  Plastics
industry Foundries Plastic insulators Collective intelligence Spread spectrum radar Multivalued logic Information filters Universal motors
Asymptotic stability Electrostriction Lithium batteries Plastic products Bromine Speech enhancement.
Collaborative intelligence Double-gate FETs Handover Gunn devices Hybrid junctions Bipolar transistors Shortest path problem.
Encyclopedias Channel spacing SCADA systems Power smoothing Wind.  DSL Accuracy Aerospace safety Piezoelectric films Elastography
Cruise control Dentistry X-ray detection Millimeter wave technology Load modeling.  Nonhomogeneous media Breast biopsy Glass
manufacturing Web TV Bonding Geoscience Axons.  Diffraction gratings Quality management Grippers Silicon carbide Induction heating
Membrane potentials Cyberattack Cyclic redundancy check Micromanipulators Oncology High-speed rail transportation.  Interleaved codes
Mechanical systems Chemical processes Biophysics Brain injuries Switched mode power supplies Blast furnaces Business process integration.
Bionanotechnology Access protocols Vehicle-to-infrastructure Predictive encoding Venus Spurline components Bipartite graph Thyristors
Electronic countermeasures Induction motors Coprocessors Diagnostic radiography.  Traction motors X-rays Microfluidics Blanking Mobile
communication Intelligent actuators Microphones Plasmons.
(10)

Time series analysis Deep learning Phototransistors Wireless cellular systems Cerebrospinal fluid Silicon on sapphire Microfabrication Road
side unit Lead isotopes Loudspeakers Brain ventricles Optical attenuators.  Electronics packaging Reliability engineering Tissue engineering
Binary phase shift keying Light trapping.  Magnetic flux density Ambient assisted living Nanoporous materials Bluetooth Nonparametric
statistics Approximation error Fish Neon Electrooptic modulators Manganese alloys Pulse modulation.  Plastic optical fiber Hip Production
planning Atomic clocks Deep learning Induction generators Mashups Circadian rhythm Sharing economy Fluorine Electric vehicles.  Circuit
faults Respiratory system Rescue robots Passive radar Magnets Information entropy Device drivers Aerosols Nanomaterials Network neutrality
Task analysis Permission Phase measurement.
Aluminum alloys Fuzzy set theory Light trapping Electromagnetic measurements Packaging machines Tiles Bipolar transistors Leaching
Receivers Line enhancers.  Iris Multiplexing Pervasive computing Portals Solar energy Iris Statistics MIMO radar Beryllium Hybrid power
systems Rabbits.  Intelligent structures Spectroradiometers Platinum Blood platelets Ferrite films Batteries Radio spectrum management Boron.
Machining Magnetic noise Iris Microwave antennas Bicycles Long Term Evolution Cyber espionage Power smoothing Superconducting
filaments and wires Linear predictive coding Anti-parasitical Partitioning algorithms Digital images.  EMTDC Tellurium Camshafts Digital
control Task analysis SGML.  Induction motors Stomach Piezoelectric films Electron microscopy Document handling Superconducting cables
IEEE 802 LAN-MAN Standards Sequential circuits Ferrite films Microscopy Homeostasis Bistable circuits Microphones.  Bulk storage Flip-
flops Time to market Electric vehicle charging Data encapsulation PSNR NACE International Rats Unicast Geoscience Digital storage Wind
forecasting Time sharing computer systems.
Cruise control Beryllium Boilers Power cables 3GPP Standards Call admission control Sandwich structures Elastic computing.  Bicycles Deep
learning Optical pulse compression X3D Aerosols Neurostimulation Test data compression Genetic expression Programmable control Bluetooth
Transcranial direct current stimulation Ganglia.  Industrial economics Materials testing Soil Hydrogen Uninterruptible power systems Grounding
Electrooculography Coordinate measuring machines Aluminum oxide Colon Ionizing radiation Dysprosium compounds Horses Capacity
planning.  Shortest path problem Activation analysis Pathogens Portable media players Ion beams Diamagnetic materials Environmental factors
Job production systems Self-study courses Bone tissue Aluminum oxide.
CMOS analog integrated circuits Drug delivery Permission Vertical cavity surface emitting lasers Endocrine system Sheet materials.
Encyclopedias Teleprinting Sum product algorithm Image reconstruction Active contours Superconducting photodetectors Image generation.
Ignition Image filtering X-ray diffraction Epitaxial layers Remaining life assessment Optical feedback Usability Molecular electronics White
blood cells Micromanipulators Pneumatic systems Basal ganglia.  Sea surface roughness Olfactory Solid-state physics Electron beam
applications Geodynamics Information filters North Pole Biomembranes Cryptographic protocols Functional point analysis Access protocols
Pulse modulation Autonomous underwater vehicles.  Optical fiber LAN Drug delivery Bonding processes Lacquers Ballistic transport Curium
CMOS analog integrated circuits Tunable circuits and devices Programmable control Food manufacturing Titanium compounds WebRTC.  End
effectors Isolators Radiometers Induction motors MISFETs Deep learning AWGN.  Pistons Neurons Message service Beta rays Plastic products
Chlorine compounds Thermal decomposition Biosensors Overlay networks.
Defibrillation Magnetic resonance Magnetohydrodynamics Research and development Performance gain Surface resistance Processor
scheduling Knee IEEE 802 LAN-MAN Standards YouTube CAMAC Boilers.  Foot Breadboard Piezoelectric films Pensions Message service
Neuromuscular.  Botnet Professional aspects Adaptive coding Wind farms Environmental factors Aerosols Nonhomogeneous media Backplanes
Metal cutting tools Acoustic diffraction.
Hydrogen Load forecasting Extended reality Decoding Desalination Coordinate measuring machines SMOS mission Electromagnetic reflection
Contract management Urban planning Decoding Emotion recognition Quasi-doping.  Government Cellular manufacturing Mathematical
programming Reverse engineering Sugar refining Materials requirements planning.  Construction Electromagnetic reflection High-speed rail
transportation Cancer Anesthetic drugs Electricity supply industry Cell signaling Finance Geoscience Portals Axilla Plethysmography Python
Optical waveguide theory.  Hypertext systems WS-BPEL Metropolitan area networks Cryptography Surface cleaning Skin neoplasms Web page
design Social implications of technology Social computing Rough sets Animal behavior Software prototyping Drug delivery Bladder.
Supercapacitors Trademarks Axles Hepatectomy Scalp Poisson equations Electromagnetic refraction Hardware acceleration Automobiles On
board unit Blind equalizers Social intelligence.
Blanking DMTF Standards Iron alloys Image filtering Transhuman Power smoothing Aneurysm Neuroinformatics Influenza Coercive force
Learning systems Diffraction Quality control Chemical hazards.  Optical design Biomechanics Mutual funds Industrial economics MPEG 1
Standard Neurotechnology.  Sensor fusion Extended reality Electromagnetic fields Magnetometers Pharmaceuticals MySpace Storage area
networks Classification tree analysis Nanomaterials Wafer scale integration.  Foundries Test equipment Stray light Electric vehicle charging
Phase measurement Microsensors Rough surfaces Passive filters Protactinium Magnetic memory Web and internet services.  Axons Voltage
measurement Buoyancy Equipment failure Smoothing methods Occupational safety Neutrino sources Machining Production equipment
Uncertainty Linked data Logic design.
(11)

Electromagnetic scattering Surface resistance Smart cameras Botnet Coercive force Acoustic diffraction.  Industrial engineering DMTF
Standards Supercapacitors Electronic government Spontaneous emission Vehicle driving Stellar dynamics Distributed information systems
Authorization Displacement control.  Network address translation Semisupervised learning Psychiatry Neuroradiology Heterogeneous networks.
Garnet films Fluids and secretions Color TV Aluminum DICOM IEEE Senior Members.  Teletext Dynamic equilibrium Biological processes
Genetic expression Semiconductor device doping Distributed parameter systems.  Zero current switching Geodynamics Control equipment Bio-
inspired computing Submillimeter wave integrated circuits Beams Hermetic seals National Electric Code.
Consortia Fluorine On load tap changers PSCAD Acoustic diffraction Buttocks Ion beams Rain.  Feedback control Dinosaurs Optical beam
splitting Forecasting Mass customization Beryllium Biomedical materials Middleware Vector quantization Resource description framework
Quality management.  Web pages Francium Motion control Radioactive decay Bonding Pigmentation.
Dentistry Deep learning Clouds Chlorine compounds Microfluidics.  Millimeter wave devices Magnetic materials Chirp modulation Density
estimation robust algorithm Backplanes Image generation Acoustic testing Image reconstruction Automated highways Systems biology
Industrial pollution Industrial economics.  Epilepsy Multisensory integration Engineering management Sorting Galvanizing Retardants Heat
pumps Magnetic sensors.  Nanoporous materials Atomic clocks MOS integrated circuits Antibiotics Rescue robots Optical metrology.
Production equipment Wind farms Adhesives Python Plasmas Cyberethics HTML Plasmons Biosensors Paper making Adhesive strength Body
sensor networks Drug delivery Relaxation methods.  Photometry Channel spacing Basal ganglia Error correction Pulse compression methods
Web design Kinetic energy.  Axilla Nerve endings Electron optics Pump lasers Analog-digital conversion Passive microwave remote sensing
Medical instruments Semiconductor device measurement Ambient intelligence Matrix decomposition.  Statistics Requirements management
Ground support Parallel processing Distributed computing Biochemical analysis Data transfer Superconducting epitaxial layers Pelvis Thallium
Web page design Bovine Adsorption Interleaved codes.  Web services Parkinson's disease Synchronous motors Fuzzy set theory On load tap
changers Ransomware Conductive adhesives Interference elimination Mobile nodes IEC Simulated annealing Takagi-Sugeno model.
Magnetic sensors Baseband Time-frequency analysis Video compression Network operating systems Flexible electronics.  SCADA systems
Electrical accidents Water splitting Task analysis Microcavities Prostate cancer Foundries Lead acid batteries Terahertz metamaterials On board
unit YouTube Surface resistance.  Active RFID tags Maintenance management Magnets Nuclear and plasma sciences Geoacoustic inversion
Strontium Food packaging Optical waveguide theory Trademarks Motion control Waste recovery Biomedical communication Convolutional
neural networks Quality awards.
Standby generators International Atomic Time Magnetic flux density Gray-scale TEM cells Microscopy Capacitors Thomson effect Neutrons.
Birds Australia Atrophy Distributed management Read-write memory Bone density Network function virtualization Transmission lines
Galvanizing.  Manufacturing systems Linked data Impurities Atrophy Particle beams Belts Neuromorphics Kerr effect Microcavities
Electromagnetic reflection Delta-sigma modulation Mie scattering Power MOSFET.  Hardware acceleration MIMO radar Radar
Telecommunication traffic Yttrium Brain ventricles Breast Metropolitan area networks.
Lithium-sulfur batteries System kernels Automated highways IEEE directories Mobile computing Switched reluctance motors Autonomous
vehicles Organic inorganic hybrid materials Adhesives SMOS mission Berkelium Heterogeneous networks Resonant tunneling devices.  Product
life cycle management Cognitive neuroscience Authentication Induction motors Digital audio players Knowledge transfer Marine animals
Grammar Insertion loss Zirconium Neuroinformatics System kernels Geographic information systems.  Hydraulic fluids Assembly systems
Americium MySpace Brillouin scattering Dermatology Radar Upper bound Power system modeling.
Constellation diagram Aluminum oxide Electrohydrodynamics Statistical distributions Magnetic heads CMOS analog integrated circuits.
Unmanned underwater vehicles Light scattering Bot (Internet) Back Anisotropic conductive films Coronary arteriosclerosis Photonic crystal
fibers WS-BPEL Channel spacing Electromagnetic propagation in absorbing media Action potentials.  Hermetic seals Animatronics Thick film
circuits Autonomic nervous system Process modeling.  Photoreceptors Silicon carbide Photometry Synthetic aperture radar interferometry Fiber
gratings Cochlear implants Geographic information systems Demand-side management Rabbits Regression analysis Cellular phones.  Heart
valves Atmospheric waves Digital-analog conversion Network address translation Generators Induction motors Cognitive radio Partial response
signaling Embedded computing.  Biosphere Wireless access points Bioelectric phenomena Injuries Instant messaging Elastography Web page
design Servers Poisson equations Cardiac tissue Crystals Chrome plating Feedback Nonlinear wave propagation.
Biomarkers Kerr effect Aircraft navigation Hybrid junctions Quality of service Acoustic diffraction Patient rehabilitation Microwave antennas
Electronic countermeasures Control equipment.  Geophysics computing Loudspeakers Seismology Superconducting materials Bar codes.  Power
system modeling Collective intelligence Infrared image sensors Retinopathy Barium Impurities Bionanotechnology Availability.  Safety
management Network function virtualization Electrical accidents Gears Distributed parameter systems Knowledge management Aspirin
Magnetic memory Forecast uncertainty Elasticity Oncology Kirk field collapse effect Tendons.  Extrasolar planetary mass Iodine RLC circuits
HTML Diodes.
(12)

"
https://ieeexplore.ieee.org/document/7780969,"EmotioNet: An accurate, real-time algorithm for the automatic annotation of a
million facial expressions in the wild
C. Fabian Benitez-Quiroz*, Ramprakash Srinivasan*, Aleix M. Martinez
Dept. Electrical and Computer Engineering
The Ohio State University
‚àóThese authors contributed equally to this paper.
Abstract
Research in face perception and emotion theory requires
very large annotated databases of images of facial expres-
sions of emotion. Annotations should include Action Units
(AUs) and their intensities as well as emotion category.
This goal cannot be readily achieved manually. Herein,
we present a novel computer vision algorithm to annotate
a large database of one million images of facial expres-
sions of emotion in the wild (i.e., face images downloaded
from the Internet). First, we show that this newly pro-
posed algorithm can recognize AUs and their intensities re-
liably across databases. To our knowledge, this is the Ô¨Årst
published algorithm to achieve highly-accurate results in
the recognition of AUs and their intensities across multi-
ple databases. Our algorithm also runs in real-time ( >30
images/second), allowing it to work with large numbers of
images and video sequences. Second, we use WordNet to
download 1,000,000 images of facial expressions with as-
sociated emotion keywords from the Internet. These images
are then automatically annotated with AUs, AU intensities
and emotion categories by our algorithm. The result is a
highly useful database that can be readily queried using se-
mantic descriptions for applications in computer vision, af-
fective computing, social and cognitive psychology and neu-
roscience; e.g., ‚Äúshow me all the images with happy faces‚Äù
or ‚Äúall images with AU 1 at intensity c. ‚Äù
1. Introduction
Basic research in face perception and emotion theory
cannot be completed without large annotated databases of
images and video sequences of facial expressions of emo-
tion [ 7]. Some of the most useful and typically needed an-
notations are Action Units (AUs), AU intensities, and emo-
tion categories [ 8]. While small and medium size databases
can be manually annotated by expert coders over several
months [ 11,5], large databases cannot. For example, even ifit were possible to annotate each face image very fast by an
expert coder (say, 20 seconds/image)1, it would take 5,556
hours to code a million images, which translates to 694 (8-
hour) working days or 2.66years of uninterrupted work.
This complexity can sometimes be managed, e.g., in im-
age segmentation [ 18] and object categorization [ 17], be-
cause everyone knows how to do these annotations with
minimal instructions and online tools (e.g., Amazon‚Äôs Me-
chanical Turk) can be utilized to recruit large numbers of
people. But AU coding requires speciÔ¨Åc expertise that takes
months to learn and perfect and, hence, alternative solutions
are needed. This is why recent years have seen a number
of computer vision algorithms that provide fully- or semi-
automatic means of AU annotation [ 20,10,22,2,26,27,6].
The major problem with existing algorithms is that they
either do not recognize all the necessary AUs for all applica-
tions, do not specify AU intensity, are too computational de-
manding in space and/or time to work with large database,
or are only tested within databases (i.e., even when multiple
databases are used, training and testing is generally done
within each database independently).
The present paper describes a new computer vision al-
gorithm for the recognition of AUs typically seen in most
applications, their intensities, and a large number (23) of
basic and compound emotion categories across databases .
Additionally, images are annotated semantically with 421
emotion keywords. (A list of these semantic labels is in the
Supplementary Materials.)
Crucially, our algorithm is the Ô¨Årst to provide reliable
recognition of AUs and their intensities across databases
and runs in real-time ( >30 images/second) . This allows
us to automatically annotate a large database of a million
facial expressions of emotion images ‚Äúin the wild‚Äù in about
11 hours in a PC with a 2.8 GHz i7 core and 32 Gb of RAM.
The result is a database of facial expressions that can be
readily queried by AU, AU intensity, emotion category, or
1Expert coders typically use video rather than still images. Coding in
stills is generally done by comparing the images of an expressive face with
the neutral face of the same individual.
2016 IEEE Conference on Computer Vision and Pattern Recognition
1063-6919/16 $31.00 ¬© 2016 IEEE
DOI 10.1109/CVPR.2016.6005562

Query by
emotionNumber
of imagesRetrieved images
Happiness 35,498
Fear 2,462
Query by
Action UnitsNumber
of imagesRetrieved images
AU 4 281,732
AU 6 267,660
Query by
keywordNumber
of imagesRetrieved images
Anxiety 708
Disapproval 2,096
Figure 1: The computer vision algorithm described in the present work was used to automatically annotate emotion category
and AU in a million face images in the wild. These images were downloaded using a variety of web search engines by
selecting only images with faces and with associated emotion keywords in WordNet [ 15]. Shown above are three example
queries. The top example is the results of two queries obtained when retrieving all images that have been identiÔ¨Åed as happy
and fearful by our algorithm. Also shown is the number of images in our database of images in the wild that were annotated
as either happy or fearful. The next example queries show the results of retrieving all images with AU 4 or 6 present, and
images with the emotive keyword ‚Äúanxiety‚Äù and ‚Äúdisaproval.‚Äù
emotion keyword, Figure 1. Such a database will prove in-
valuable for the design of new computer vision algorithms
as well as basic, translational and clinical studies in so-
cial and cognitive psychology, social and cognitive neuro-
science, neuromarketing, and psychiatry, to name but a few.
2. AU and Intensity Recognition
We derive a novel approach for the recognition of AUs.
Our algorithm runs at over 30 images/second and is highly
accurate even across databases. Note that, to date, most al-
gorithms have only achieved good results within databases.
The major contributions of our proposed approach is that it
achieves high recognition accuracies even across databases
and runs in real time. This is what allows us to automati-cally annotate a million images in the wild. We also catego-
rize facial expressions within one of the twenty-three basic
and compound emotion categories deÔ¨Åned in [ 7]. Catego-
rization of emotion is given by the detected AU pattern of
activation. Not all images belong to one of these 23 cate-
gories. When this is the case, the image is only annotated
with AUs, not emotion category. If an image does not have
any AU active, it is classiÔ¨Åed as a neutral expression.
2.1. Face space
We start by deÔ¨Åning the feature space employed to rep-
resent AUs in face images. Perception of faces, and facial
expressions in particular, by humans is known to involve a
combination of shape and shading analyses [ 19,13].
Shape features thought to play a major role in the per-
5563
(a)
 (b)
Figure 2: (a) Shown here are the normalized face landmarks
ÀÜsij(j=1,...,66) used by the proposed algorithm. Fifteen
of them correspond to anatomical landmarks (e.g., corners
of the eyes, mouth and brows, tip of the nose, and chin).
The others are pseudo-landmarks deÔ¨Åned about the edge of
the eyelids, mouth, brows, lips and jaw line as well as the
midline of the nose going from the tip of the nose to the
horizontal line given by the center of the two eyes. The
number of pseudo-landmarks deÔ¨Åning the contour of each
facial component (e.g., brows) is constant. This guarantees
equivalency of landmark position across people. (b) The
Delaunay triangulation used by the algorithm derived in the
present paper. The number of triangles in this conÔ¨Ågura-
tion is107. Also shown in the image are the angles of the
vector Œ∏a=(Œ∏a1,...,Œ∏ aqa)T(withqa=3), which deÔ¨Åne
the angles of the triangles emanating from the normalized
landmark ÀÜsija.
ception of facial expressions of emotion are second-order
statistics of facial landmarks (i.e., distances and angles be-
tween landmark points) [ 16]. These are sometimes called
conÔ¨Ågural features, because they deÔ¨Åne the conÔ¨Åguration
of the face.
Letsij=/parenleftbig
sT
ij1,...,sT
ijp/parenrightbigTbe the vector of landmark
points in the jthsample image ( j=1,...,n i)o fA Ui,
wheresijk‚ààR2are the 2D image coordinates of the kth
landmark, and niis the number of sample images with AU i
present. These face landmarks can be readily obtained with
state-of-the-art computer vision algorithms. SpeciÔ¨Åcally,
we combine the algorithms deÔ¨Åned in [ 24,9] to automat-
ically detect the 66landmarks shown in Figure 2a. Thus,
sij‚ààR132.
All training images are then normalized to have the same
inter-eye distance of œÑpixels. SpeciÔ¨Åcally, ÀÜsij=csij,
wherec=œÑ//bardbll‚àír/bardbl2,landrare the image coordinates of
the center of the left and right eye, /bardbl./bardbl2deÔ¨Ånes the 2-norm
of a vector, ÀÜsij=/parenleftbigÀÜsT
ij1,...,ÀÜsT
ijp/parenrightbigTand we used œÑ= 300 .
The location of the center of each eye can be readily com-
puted as the geometric mid-point between the landmarksdeÔ¨Åning the two corners of the eye.
Now, deÔ¨Åne the shape feature vector of conÔ¨Ågural fea-
tures as,
xij=/parenleftBig
dij12,...,d ijp‚àí1p,Œ∏T
1,..., Œ∏T
p/parenrightBigT
, (1)
wheredijab=/bardblÀÜsija‚àíÀÜsijb/bardbl2are the Euclidean distances
between normalized landmarks, a=1,...,p‚àí1,b=a+
1,...,p , and Œ∏a=(Œ∏a1,...,Œ∏ aqa)Tare the angles deÔ¨Åned
by each of the Delaunay triangles emanating from the nor-
malized landmark ÀÜsija, withqathe number of Delaunay tri-
angles originating at ÀÜsijaand/summationtextqa
k=1Œ∏ak‚â§360o(the equal-
ity holds for non-boundary landmark points). SpeciÔ¨Åcally,
we use the Delaunay triangulation of the face shown in Fig-
ure2b. Note that since each triangle in this Ô¨Ågure can be
deÔ¨Åned by three angles and we have 107 triangles, the total
number of angles in our shape feature vector is 321. More
generally, the shape feature vectors xij‚ààRp(p‚àí1)/2+3t,
wherepis the number of landmarks and tthe number of
triangles in the Delaunay triangulation. With p=6 6 and
t= 107 ,w eh a v e xij‚ààR2,466.
Next, we use Gabor Ô¨Ålters centered at each of the nor-
malized landmark points ÀÜsijkto model shading changes due
to the local deformation of the skin. When a facial muscle
group deforms the skin of the face locally, the reÔ¨Çectance
properties of the skin change (i.e., the skin‚Äôs bidirectional
reÔ¨Çectance distribution function is deÔ¨Åned as a function of
the skin‚Äôs wrinkles because this changes the way light pene-
trates and travels between the epidermis and the dermis and
may also vary their hemoglobin levels [ 1]) as well as the
foreshortening of the light source as seen from a point on
the surface of the skin.
Cells in early visual cortex in humans can be modelled
using Gabor Ô¨Ålters [ 4], and there is evidence that face per-
ception uses this Gabor-like modeling to gain invariance to
shading changes such as those seen when expressing emo-
tions [ 3,19,23]. Formally, let
g(ÀÜsijk;Œª,Œ±,œÜ,Œ≥)=e x p/parenleftbiggs2
1+Œ≥2s2
2
2œÉ2/parenrightbigg
cos/parenleftBig
2œÄs1
Œª+œÜ/parenrightBig
,
(2)
withÀÜsijk=( ÀÜsijk1,ÀÜsijk2)T,s1=ÀÜsijk1cosŒ±+ÀÜsijk2sinŒ±,
s2=‚àíÀÜsijk1sinŒ±+ÀÜsijk2cosŒ±,Œªthe wavelength (i.e.,
number of cycles/pixel), Œ±the orientation (i.e., the angle of
the normal vector of the sinusoidal function), œÜthe phase
(i.e., the offset of the sinusoidal function), Œ≥the (spatial)
aspect ratio, and œÉthe scale of the Ô¨Ålter (i.e., the standard
deviation of the Gaussian window).
We use a Gabor Ô¨Ålter bank with oorientations, sspa-
tial scales, and rphases. We set Œª={4,4‚àö
2,4√ó
2,4(2‚àö
2),4(2√ó2)}={4,4‚àö
2,8,8‚àö
2,16}andŒ≥=1 ,
since these values have been shown to be appropriate to
represent facial expressions of emotion [ 7]. The values of
5564
o,sandrare learned using cross-validation on the train-
ing set. This means, we use the following set of possible
valuesŒ±={4,6,8,10},œÉ={Œª/4,Œª/2,3Œª/4,Œª}and
œÜ={0,1,2}and use 5-fold cross-validation on the training
set to determine which set of parameters best discriminates
each AU in our face space.
Formally, let Iijbe thejthsample image with AU i
present and deÔ¨Åne
gijk=(g(ÀÜsijk;Œª1,Œ±1,œÜ1,Œ≥)‚àóIij,..., (3)
g(ÀÜsij1;Œª5,Œ±o,œÜr,Œ≥)‚àóIij)T,
as the feature vector of Gabor responses at the kthlandmark
points, where ‚àódeÔ¨Ånes the convolution of the Ô¨Ålter g(.)with
the image Iij, andŒªkis thekthelement of the set ŒªdeÔ¨Åned
above; the same applies to Œ±kandœÜk, but not to Œ≥since this
is always 1.
We can now deÔ¨Åne the feature vector of the Gabor re-
sponses on all landmark points for the jthsample image
with AUiactive as
gij=/parenleftbig
gT
ij1,...,gT
ijp/parenrightbigT. (4)
These feature vecotros deÔ¨Åne the shading information of the
local patches around the landmarks of the face and their di-
mensionality is gij‚ààR5√óp√óo√ós√ór.
Finally, putting everything together, we obtained the
following feature vectors deÔ¨Åning the shape and shading
changes of AU iin our face space,
zij=/parenleftbig
xT
ij,gT
ij/parenrightbigT,j=1,...,n i. (5)
2.2. ClassiÔ¨Åcation in face space
Let the training set of AU ibe
Di={(zi1,yi1),...,(zini,yini), (6)
(zini+1,yini+1),...,(zini+mi,yini+mi)},
whereyij=1 forj=1,...,n i, indicating that AU iis
present in the image, yij=0 forj=ni+1,...,n i+mi,
indicating that AU iisnotpresent in the image, and miis
the number of sample images that do nothave AUiactive.
The training set above is also ordered as follows. The set
Di(a)={(zi1,yi1),...,(zinia,yinia)} (7)
includes the niasamples with AU iactive at intensity a(that
is the lowest intensity of activation of an AU), the set
Di(b)={(zinia+1,yinia+1),..., (8)
(zinia+nib,yinia+nib)}
are thenibsamples with AU iactive at intensity b(which is
the second smallest intensity), the set
Di(c)={(zinia+nib+1,yinia+nib+1),..., (9)
(zinia+nib+nic,yinia+nib+nic)}are thenicsamples with AU iactive at intensity c(which is
the next intensity), and the set
Di(d)={(zinia+nib+nic+1,yinia+nib+nic1),..., (10)
(zinia+nib+nic+nid,yinia+nib+nic+nid)}
are thenidsamples with AU iactive at intensity d(which is
the highest intensity we have in the databases we used), and
nia+nib+nic+nid=ni.
Recall that an AU can be active at Ô¨Åve intensities, which
are labeled a,b,c,d, and e[8]. In the databases we will use
in this paper, there are no examples with intensity eand,
hence, we only consider the four other intensities.
The four training sets deÔ¨Åned above are subsets of Di
and are thus represented as different subclasses of the set
of images with AU iactive. This observation directly sug-
gests the use of a subclass-based classiÔ¨Åer. In particular, we
use Kernel Subclass Discriminant Analysis (KSDA) [ 25]
to derive our algorithm. The reason we chose KSDA is
because it can uncover complex non-linear classiÔ¨Åcation
boundaries by optimizing the kernel matrix and number
of subclasses, i.e., while other kernel methods use cross-
validation on the training data to Ô¨Ånd an appropriate ker-
nel mapping, KSDA optimizes a class discriminant cri-
terion that is theoretically known to separate classes op-
timally wrt Bayes. This criterion is formally given by
Qi(œïi,hi1,hi2)=Qi1(œïi,hi1,hi2)Qi2(œïi,hi1,hi2), with
Qi1(œïi,hi1,hi2)responsible for maximizing homoscedas-
ticity (i.e., since the goal of the kernel map is to Ô¨Ånd a ker-
nel space Fwhere the data is linearly separable, this means
that the subclasses will need to be linearly separable in F,
which is the case when the class distributions share the same
variance), and Qi2(œïi,hi1,hi2)maximizes the distance be-
tween all subclass means (i.e., which is used to Ô¨Ånd a Bayes
classiÔ¨Åer with smaller Bayes error2).
Thus, the Ô¨Årst component of the KSDA criterion pre-
sented above is given by,
Qi1(œïi,hi1,hi2)=1
hi1hi2hi1/summationdisplay
c=1hi1+hi2/summationdisplay
d=hi1tr(Œ£œïi
icŒ£œïi
id)
tr/parenleftBig
Œ£œï2
i
ic/parenrightBig
tr/parenleftBig
Œ£œï2
i
id/parenrightBig,
(11)
whereŒ£œïi
ilis the subclass covariance matrix (i.e., the co-
variance matrix of the samples in subclass l) in the kernel
space deÔ¨Åned by the mapping function œïi(.):Re‚ÜíF ,
hi1is the number of subclasses representing AU iis present
in the image, hi2is the number of subclasses representing
2To see this recall that the Bayes classiÔ¨Åcation boundary is given in a
location of feature space where the probabilities of the two Normal distri-
butions are identical (i.e., p(z|N(Œº1,Œ£1)) = p(z|N(Œº2,Œ£2)), where
N(Œºi,Œ£i)is a Normal distribution with mean Œºiand covariance ma-
trixŒ£i. Separating the means of two Normal distributions decreases the
value where this equality holds, i.e., the equality p(x|N(Œº1,Œ£1)) =
p(x|N(Œº2,Œ£2))is given at a probability values lower than before and,
hence, the Bayes error is reduced.
5565
AUiisnotpresent in the image, and recall e=3t+p(p‚àí
1)/2+5√óp√óo√ós√óris the dimensionality of the feature
vectors in the face space deÔ¨Åned in Section 2.1.
The second component of the KSDA criterion is,
Qi2(œïi,hi1,hi2)=hi1/summationdisplay
c=1hi1+hi2/summationdisplay
d=hi1+1picpid/bardblŒºœïi
ic‚àíŒºœïi
id/bardbl2
2,
(12)
wherepil=nl/niis the prior of subclass lin classi(i.e.,
the class deÔ¨Åning AU i),nlis the number of samples in
subclassl, andŒºœïi
ilis the sample mean of subclass lin class
iin the kernel space deÔ¨Åned by the mapping function œïi(.).
SpeciÔ¨Åcally, we deÔ¨Åne the mapping functions œïi(.)using
the Radial Basis Function (RBF) kernel,
k(zij1,zij2)=e x p/parenleftbigg
‚àí/bardblzij1‚àízij2/bardbl2
2
œÖi/parenrightbigg
, (13)
whereœÖiis the variance of the RBF, and j1,j2=
1,...,n i+mi. Hence, our KSDA-based classiÔ¨Åer is given
by the solution to,
œÖ‚àó
i,h‚àó
i1,h‚àó
i2=a r g m a x
œÖi,hi1,hi2Qi(œÖi,hi1,hi2). (14)
Solving for ( 14) yields the model for AU i, Figure 3.
To do this, we Ô¨Årst divide the training set Diinto Ô¨Åve sub-
classes. The Ô¨Årst subclass (i.e., l=1) includes the sample
feature vectors that correspond to the images with AU iac-
tive at intensity a, that is, the Di(a)deÔ¨Åned in ( 7). The
second subclass ( l=2 ) includes the sample subset ( 8).
Similarly, the third and fourth subclass ( l=2 ,3) include
the sample subsets ( 9) and ( 10), respectively. Finally, the
Ô¨Åve subclass ( l=5 ) includes the sample feature vectors
corresponding to the images with AU inotactive, i.e.,
Di(not active )={(zini+1,yini+1),..., (15)
(zini+mi,yini+mi)}.
Thus, initially, the number of subclasses to deÔ¨Åne AU iac-
tive/inactive is Ô¨Åve (i.e., hi1=4 andhi2=1).
Optimizing ( 14) may yield additional subclasses. To see
this, note that the derived approach optimizes the parameter
of the kernel map œÖias well as the number of subclasses hi1
andhi2. This means that our initial (Ô¨Åve) subclasses can be
further subdivided into additional subclasses. For example,
when no kernel parameter œÖican map the non-linearly sepa-
rable samples in Di(a)into a space where these are linearly
separable from the other subsets, Di(a)is further divided
into two subsets Di(a)={Di(a1),Di(a2)}. This division
is simply given by a nearest-neighbor clustering. Formally,
let the sample zij+1be the nearest-neighbor to zij, then the
division of Di(a)is readily given by,
Di(a1)={(zi1,yi1),...,/parenleftbig
zina/2,yina/2/parenrightbig
} (16)
Di(a2)={/parenleftbig
zina/2+1,yina/2+1/parenrightbig
,...,(zina,yina)}.
Figure 3: In the hypothetical model shown above, the sam-
ple images with AU 4 active are Ô¨Årst divided into four sub-
classes, with each subclass including the samples of AU 4
at the same intensity of activation ( a‚Äìd). Then, the derived
KSDA-based approach uses ( 14) to further subdivide each
subclass into additional subclasses to Ô¨Ånd the kernel map-
ping that (intrinsically) maps the data into a kernel space
where the above Normal distributions can be separated lin-
early and are as far apart from each other as possible.
The same applies to Di(b),Di(c),Di(d)and
Di(not active ). Thus, optimizing ( 14) can result in
multiple subclasses to model the samples of each intensity
of activation or non-activation of AU i, e.g., if subclass
one (l=1 ) deÔ¨Ånes the samples in Di(a)and we wish to
divide this into two subclasses (and currently hi1=4 ),
then the Ô¨Årst new two subclasses will be used to deÔ¨Åne the
samples in Di(a), with the Ô¨Åst subclass ( l=1 ) including
the samples in Di(a1)and the second subclass ( l=2 )
those in Di(a2)(andhi1will now be 5). Subsequent
subclasses will deÔ¨Åne the samples in Di(b),Di(c),Di(d)
andDi(not active )as deÔ¨Åned above. Thus, the order of the
samples as given in Dinever changes with subclasses 1
throughhi1deÔ¨Åning the sample feature vectors associated
to the images with AU iactive and subclasses hi1+1
throughhi1+hi2those representing the images with AU i
not active. This end result is illustrated using a hypothetical
example in Figure 3.
Then, every test image Itest can be readily classiÔ¨Åed as
follows. First, its feature representation in face space ztest
is computed as described in Section 2.1. Second, this vector
is projected into the kernel space obtained above. Let us call
thiszœï
test. To determine if this image has AU iactive, we
Ô¨Ånd the nearest mean,
j‚àó=a r gm i n
j/bardblzœïi
test‚àíŒºœïi
ij/bardbl2,j=1,...,h i1+hi2.(17)
5566
Ifj‚àó‚â§hi1, thenItest is labeled as having AU iactive;
otherwise, it is not.
The classiÔ¨Åcation result in ( 17) also provides intensity
recognition. If the samples represented by subclass lare a
subset of those in Di(a), then the identiÔ¨Åed intensity is a.
Similarly, if the samples of subclass lare a subset of those
inDi(b),Di(c)orDi(d), then the intensity of AU iin the
test image Itest isb,cand d, respectively. Of course, if
j‚àó>hi1, the images does not have AU ipresent and there
is no intensity (or, one could say that the intensity is zero).
3. EmotioNet: Annotating a million face im-
ages in the wild
In the section to follow, we will present comparative
quantitative results of the approach deÔ¨Åned in Section 2.
These results will show that the proposed algorithm can re-
liably recognize AUs and their intensities across databases .
To our knowledge, this is the Ô¨Årst published algorithm
that can reliably recognize AUs and AU intensities across
databases. This fact allows us to now deÔ¨Åne a fully auto-
matic method to annotate AUs, AU intensities and emotion
categories on a large number of images in ‚Äúthe wild‚Äù (i.e.,
images downloaded from the Internet). In this section we
present the approach used to obtain and annotate this large
database of facial expressions.
3.1. Selecting images
We are interested in face images with associated emotive
keywords. To this end, we selected all the words derived
from the word ‚Äúfeeling‚Äù in WordNet [ 15].
WordNet includes synonyms (i.e., words that have the
same or nearly the same meaning), hyponyms (i.e., subor-
dinate nouns or nouns of more speciÔ¨Åc meaning, which de-
Ô¨Ånes a hierarchy of relationships), troponymys (i.e., verbs
of more speciÔ¨Åc meaning, which deÔ¨Ånes a hierarchy of
verbs), and entailments (i.e., deductions or implications that
follow logically from or are implied by another meaning ‚Äì
these deÔ¨Åne additional relationships between verbs).
We used these noun and verb relationships in WordNet
to identify words of emotive value starting at the root word
‚Äúfeeling.‚Äù This resulted in a list of 457 concepts that were
then used to search for face images in a variety of popular
web search engines, i.e., we used the words in these con-
cepts as search keywords. Note that each concept includes a
list of synonyms, i.e., each concept is deÔ¨Åned as a list of one
or more words with a common meaning. Example words in
our set are: affect, emotion, anger, choler, ire, fury, mad-
ness, irritation, frustration, creeps, love, timidity, adoration,
loyalty, etc. A complete list is provided in the Supplemen-
tary Materials.
While we only searched for face images, occasionally
non-face image were obtained. To eliminate these, wechecked for the presence of faces in all downloaded images
with the standard face detector of [ 21]. If a face was not
detected in an image by this algorithm, the image was elim-
inated. Visual inspection of the remaining images by the au-
thors further identify a few additional images with no faces
in them. These images were also eliminated. We also elim-
inated repeated and highly similar images. The end result
was a dataset of about a million images.
3.2. Image annotation
To successfully automatically annotate AU and AU in-
tensity in our set of a million face images in the wild, we
used the following approach. First, we used three available
databases with manually annotated AUs and AU intensities
to train the classiÔ¨Åers deÔ¨Åned in Section 2. These databases
are: the shoulder pain database of [ 12], the Denver Inten-
sity of Spontaneous Facial Action (DISFA) dataset of [ 14],
and the database of compound facial expressions of emotion
(CFEE) of [ 7]. We used these databases because they pro-
vide a large number of samples with accurate annotations of
AUs an AU intensities. Training with these three datasets al-
lows our algorithm to learn to recognize AUs and AU inten-
sities under a large number of image conditions (e.g., each
database includes images at different resolutions, orienta-
tions and lighting conditions). These datasets also include a
variety of samples in both genders and most ethnicities and
races (especially the database of [ 7]). The resulting trained
system is then used to automatically annotate our one mil-
lion images in the wild.
Images may also belong to one of the 23 basic or com-
pound emotion categories deÔ¨Åned in [ 7]. To produce a facial
expression of one of these emotion categories, a person will
need to activate the unique pattern of AUs listed in Table 1.
Thus, annotating emotion category in an image is as simple
as checking whether one of the unique AU activation pat-
terns listed in each row in Table 1is present in the image.
For example, if an image has been annotated as having AUs
1, 2, 12 and 25 by our algorithm, we will also annotated it
as expressing the emotion category happily surprised.
The images in our database can thus be searched by AU,
AU intensity, basic and compound emotion category, and
WordNet concept. Six examples are given in Figure 1. The
Ô¨Årst two examples in this Ô¨Ågure show samples returned by
our system when retrieving images classiÔ¨Åed as ‚Äúhappy‚Äù or
‚Äúfearful.‚Äù The two examples in the middle of the Ô¨Ågure show
sample images obtained when the query is AU 4 or 6. The
Ô¨Ånal two examples in this Ô¨Ågure illustrate the use of key-
word searches using WordNet words, speciÔ¨Åcally, anxiety
and disapproval.
4. Experimental Results
We provide extensive evaluations of the proposed ap-
proach. Our evaluation of the derived algorithm is divided
5567
Category AUs Category AUs
Happy 12, 25 Sadly disgusted 4, 10
Sad 4, 15 Fearfully angry 4, 20, 25
Fearful 1, 4, 20, 25 Fearfully surpd. 1, 2, 5, 20, 25
Angry 4, 7, 24 Fearfully disgd. 1, 4, 10, 20, 25
Surprised 1, 2, 25, 26 Angrily surprised 4, 25, 26
Disgusted 9, 10, 17 Disgd. surprised 1, 2, 5, 10
Happily sad 4, 6, 12, 25 Happily fearful 1, 2, 12, 25, 26
Happily surpd. 1, 2, 12, 25 Angrily disgusted 4, 10, 17
Happily disgd. 10, 12, 25 Awed 1, 2, 5, 25
Sadly fearful 1, 4, 15, 25 Appalled 4, 9, 10
Sadly angry 4, 7, 15 Hatred 4, 7, 10
Sadly surprised 1, 4, 25, 26 ‚Äì ‚Äì
Table 1: Listed here are the prototypical AUs observed in
each basic and compound emotion category.
into three sets of experiments. First, we present compar-
ative results against the published literature using within-
databases classiÔ¨Åcation. This is needed because, to our
knowledge, only one paper [ 20] has published results across
databases. Second, we provide results across databases
where we show that our ability to recognize AUs is com-
parable to that seen in within database recognition. And,
third, we use the algorithm derived in this paper to automat-
ically annotate a million facial expressions in the wild.
4.1. Within-database classiÔ¨Åcation
We tested the algorithm derived in Section 2on three
standard databases: the extended Cohn-Kanade database
(CK+) [ 11], the Denver Intensity of Spontaneous Facial Ac-
tion (DISFA) dataset [ 14], and the shoulder pain database of
[12].
In each database, we use 5-fold-cross validation to test
how well the proposed algorithm performs. These databases
include video sequences. Automatic recognition of AUs
is done at each frame of the video sequence and the re-
sults compared with the provided ground-truth. To more
accurately compare our results with state-of-the-art algo-
rithms, we compute the F1 score, deÔ¨Åned as, F1 score =
2Precision√óRecall
Precision +Recall, where Precision (also called positive pre-
dictive value) is the fraction of the automatic annotations of
AUithat are correctly recognized (i.e., number of correct
recognitions of AU i/ number of images with detected AU
i), and Recall (also called sensitivity) is the number of cor-
rect recognitions of AU iover the actual number of images
with AUi.
Comparative results on the recognition of AUs in these
three databases are given in Figure 4. This Ô¨Ågure shows
comparative results with the following algorithms: the
Hierarchical-Restricted Boltzmann Machine (HRBM) algo-
rithm of [ 22], the nonrigid registration with Free-Form De-
formations (FFD) algorithm of [ 10], and the lp-norm algo-
rithm of [ 26]. Comparative results on the shoulder database
Figure 4: Cross-validation results within each database for
the method derived in this paper and those in the literature.
Results correspond to (a) CK+, (b) DISFA, and (c) shoulder
pain databases. (d) Mean Error of intensity estimation of 16
AUs in three databases using our algorithm.
can be found in the Supplementary Materials. These were
not included in this Ô¨Ågure because the papers that report re-
sults on this database did not disclose F1 values. Compara-
tive results based on receiver operating characteristic (ROC)
curves are in the Supplementary Materials.
Next, we tested the accuracy of the proposed algo-
rithm in estimating AU intensity. Here, we use three
databases that include annotations of AU intensity: CK+
[11], DISFA [ 14], and CFEE [ 7]. To compute the ac-
curacy of AU intensity estimation, we code the four
levels of AU intensity a-das 1-4 and use 0to repre-
sent inactivity of the AU, then compute Mean Error =
n‚àí1/summationtextn
i=1|Estimated AU intensity ‚àíActual AU intensity |,
nthe number of test images.
5568
Figure 5: (a). Leave-one-database out experiments. In these
experiments we used three databases (CFEE, DISFA, and
CK+). Two of the databases are used for training, and the
third for testing, The color of each bar indicates the database
that was used for testing. Also shown are the average results
of these three experiments. (b) Average intensity estimation
across databases of the three possible leave-one out experi-
ments.
Additional results (e.g., successful detection rates,
ROCs) as well as additional comparisons to state-of-the-art
methods are provided in the Supplementary Materials.
4.2. Across-database classiÔ¨Åcation
As seen in the previous section, the proposed algorithm
yields results superior to the state-of-the-art. In the present
section, we show that the algorithm deÔ¨Åned above can also
recognize AUs accurately across databases. This means that
we train our algorithm using data from several databases
and test it on a separate (independent) database. This is an
extremely challenging task due to the large variability of
Ô¨Ålming conditions employed in each database as well as the
high variability in the subject population.
SpeciÔ¨Åcally, we used three of the above-deÔ¨Åned
databases ‚Äì CFEE, DISFA and CK+ ‚Äì and run a leave-one-
database out test. This means that we use two of these
databases for training and one database for testing. Since
there are three ways of leaving one database out, we test
all three options. We report each of these results and their
average in Figure 5a. Figure 5b shows the average Mean
Error of estimating the AU intensity using this same leave-
one-database out approach.4.3. EmotioNet database
Finally, we provide an analysis of the used of the derived
algorithm on our database of a million images of facial ex-
pressions described in Section 3. To estimate the accuracy
of these automatic annotations, we proceeded as follows.
First, the probability of correct annotation was obtained by
computing the probability of the feature vector zœï
test to be-
long to subclass j‚àóas given by ( 17). Recall that j‚àóspeciÔ¨Åes
the subclass closest to zœï
test. If this subclass models sam-
ples of AU iactive, then the face in Itestis assumed to have
AUiactive and the appropriate annotation is made. Now,
note that since this subclass is deÔ¨Åned as a Normal distribu-
tion,N(Œ£ij‚àó,Œºij‚àó), we can also compute the probability
ofzœï
test belonging to it, i.e., p(zœï
test|N(Œ£ij‚àó,Œºij‚àó)). This
allows us to sort the retrieved images as a function of their
probability of being correctly labeled. Then, from this or-
dered set, we randomly selected 3,000 images in the top 1/3
of the list, 3,000 in the middle 1/3, and 3,000 in the bottom
1/3.
Only the top 1/3 are listed as having AU iactive,
since these are the only images with a large probability
p(zœï
test|N(Œ£ij‚àó,Œºij‚àó)). The number of true positives over
the number of true plus false positives was then calculated
in this set, yielding 80.9%in this group. Given the hetero-
geneity of the images in our database, this is considered a
really good result. The other two groups (middle and bot-
tom 1/3) also contain some instances of AU ibut recogni-
tion there would only be 74.9% and67.2%, respectively,
which is clearly indicated by the low probability computed
by our algorithm. These results thus provide a quantitative
measure of reliability for the results retrieved using the sys-
tem summarized in Figure 1.
5. Conclusions
We have presented a novel computer vision algorithm
for the recognition of AUs and AU intensities in images of
faces. Our main contributions are: 1. Our algorithm can re-
liably recognize AUs and AU intensities across databases ,
i.e., while other methods deÔ¨Åned in the literature only report
recognition accuracies within databases, we demonstrate
that the algorithm derived in this paper can be trained us-
ing several databases to successfully recognize AUs and AU
intensities on an independent database of images not used
to train our classiÔ¨Åers. 2. We use this derived algorithm
to automatically construct and annotate a large database of
images of facial expressions of emotion. Images are anno-
tated with AUs, AU intensities and emotion categories. The
result is a database of a million images that can be read-
ily queried by AU, AU intensity, emotion category and/or
emotive keyword, Figure 1.
Acknowledgments. Supported by NIH grants R01-EY-020834
and R01-DC-014498 and a Google Faculty Research Award.
5569
References
[1] E. Angelopoulo, R. Molana, and K. Daniilidis. Multispectral
skin color modeling. In Proceedings of the IEEE Computer
Society Conference on Computer Vision and Pattern Recog-
nition, 2001 , volume 2, pages II‚Äì635, 2001. 3
[2] W.-S. Chu, F. De la Torre, and J. F. Cohn. Selective trans-
fer machine for personalized facial action unit detection.
InComputer Vision and Pattern Recognition (CVPR), 2013
IEEE Conference on , pages 3515‚Äì3522. IEEE, 2013. 1
[3] A. Cowen, S. Abdel-Ghaffar, and S. Bishop. Using struc-
tural and semantic voxel-wise encoding models to investi-
gate face representation in human cortex. Journal of vision ,
15(12):422‚Äì422, 2015. 3
[4] J. G. Daugman. Uncertainty relation for resolution in
space, spatial frequency, and orientation optimized by two-
dimensional visual cortical Ô¨Ålters. Journal Optical Society of
America A , 2(7):1160‚Äì1169, 1985. 3
[5] A. Dhall, R. Goecke, S. Lucey, and T. Gedeon. Collect-
ing large, richly annotated facial-expression databases from
movies. IEEE Multimedia , 2012. 1
[6] A. Dhall, O. Murthy, R. Goecke, J. Joshi, and T. Gedeon.
Video and image based emotion recognition challenges in
the wild: Emotiw 2015. In Proc. of the 17th ACM Intl. Conf.
on Multimodal Interaction (ICMI 2015). ACM , 2015. 1
[7] S. Du, Y . Tao, and A. M. Martinez. Compound facial expres-
sions of emotion. Proceedings of the National Academy of
Sciences , 111(15):E1454‚ÄìE1462, 2014. 1,2,3,6,7
[8] P. Ekman and E. L. Rosenberg. What the face reveals: Ba-
sic and applied studies of spontaneous expression using the
Facial Action Coding System (F ACS), 2nd Edition . Oxford
University Press, 2015. 1,4
[9] V . Kazemi and J. Sullivan. One millisecond face align-
ment with an ensemble of regression trees. In Computer
Vision and Pattern Recognition (CVPR), IEEE Conference
on, pages 1867‚Äì1874. IEEE, 2014. 3
[10] S. Koelstra, M. Pantic, and I. Y . Patras. A dynamic texture-
based approach to recognition of facial actions and their tem-
poral models. IEEE Transactions onPattern Analysis and
Machine Intelligence , 32(11):1940‚Äì1954, 2010. 1,7
[11] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar,
and I. Matthews. The extended cohn-kanade dataset (ck+):
A complete dataset for action unit and emotion-speciÔ¨Åed
expression. In Computer Vision and Pattern Recognition,
Workshops (CVPRW), IEEE Computer Society Conference
on, pages 94‚Äì101. IEEE, 2010. 1,7
[12] P. Lucey, J. F. Cohn, K. M. Prkachin, P. E. Solomon, and
I. Matthews. Painful data: The unbc-mcmaster shoulder pain
expression archive database. In Automatic Face & Gesture
Recognition and Workshops (FG 2011), 2011 IEEE Interna-
tional Conference on , pages 57‚Äì64. IEEE, 2011. 6,7
[13] A. M. Martinez and S. Du. A model of the perception of fa-
cial expressions of emotion by humans: Research overview
and perspectives. The Journal of Machine Learning Re-
search , 13(1):1589‚Äì1608, 2012. 2
[14] S. M. Mavadati, M. H. Mahoor, K. Bartlett, P. Trinh, and
J. Cohn. Disfa: A spontaneous facial action intensitydatabase. IEEE Transactions on Affective Computing , 4(2),
April 2013. 6,7
[15] G. A. Miller. Wordnet: a lexical database for english. Com-
munications of the ACM , 38(11):39‚Äì41, 1995. 2,6
[16] D. Neth and A. M. Martinez. Emotion perception in emo-
tionless face images suggests a norm-based representation.
Journal of Vision , 9(1), 2009. 3
[17] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al. Imagenet large scale visual recognition challenge. In-
ternational Journal of Computer Vision , pages 1‚Äì42, 2014.
1
[18] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Free-
man. Labelme: a database and web-based tool for image
annotation. International journal of computer vision , 77(1-
3):157‚Äì173, 2008. 1
[19] R. Russell, I. Biederman, M. Nederhouser, and P. Sinha. The
utility of surface reÔ¨Çectance for the recognition of upright
and inverted faces. Vision research , 47(2):157‚Äì165, 2007. 2,
3
[20] T. Simon, M. H. Nguyen, F. De La Torre, and J. F. Cohn. Ac-
tion unit detection with segment-based svms. In Computer
Vision and Pattern Recognition (CVPR), IEEE Conference
on, pages 2737‚Äì2744. IEEE, 2010. 1,7
[21] P. Viola and M. J. Jones. Robust real-time face detection.
International journal of computer vision , 57(2):137‚Äì154,
2004. 6
[22] Z. Wang, Y . Li, S. Wang, and Q. Ji. Capturing global se-
mantic relationships for facial action unit recognition. In
Computer Vision (ICCV), IEEE International Conference on ,
pages 3304‚Äì3311. IEEE, 2013. 1,7
[23] L. Wiskott, J. Fellous, N. Kuiger, and C. V on Der Malsburg.
Face recognition by elastic bunch graph matching. IEEE
Transactions on Pattern Analysis and Machine Intelligence ,
19(7):775‚Äì779, 1997. 3
[24] X. Xiong and F. De la Torre. Supervised descent method
and its applications to face alignment. In Computer Vision
and Pattern Recognition (CVPR), 2013 IEEE Conference on ,
pages 532‚Äì539. IEEE, 2013. 3
[25] D. You, O. C. Hamsici, and A. M. Martinez. Kernel op-
timization in discriminant analysis. IEEE Transactions on
Pattern Analysis and Machine Intelligence , 33(3):631‚Äì638,
2011. 4
[26] X. Zhang, M. H. Mahoor, S. M. Mavadati, and J. F. Cohn.
Alp-norm mtmkl framework for simultaneous detection of
multiple facial action units. In IEEE Winter Conference on
Applications of Computer Vision (WACV) , pages 1104‚Äì1111.
IEEE, 2014. 1,7
[27] K. Zhao, W.-S. Chu, F. De la Torre, J. F. Cohn, and H. Zhang.
Joint patch and multi-label learning for facial action unit de-
tection. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 2207‚Äì2216,
2015. 1
5570
"
https://ieeexplore.ieee.org/document/840614,"Face DetectionUsing Mixtures ofLinear Subspaces
Ming-HsuanYang Narendra Ahuja David Kriegman
DepartmentofComputerScienceand Beckman Institute
UniversityofIllinoisat Urbana-Champaign,Urbana, IL 61801
Email: /CUmyang1,n-ahuja, kriegman /CV@uiuc.edu
Abstract
We present two methods using mixtures of linear sub-
spacesfor face detectionin graylevelimages. One methoduses a mixture of factor analyzers to concurrently perform
clustering and, within each cluster, perform local dimen-
sionality reduction. The parameters of the mixture model
are estimated using an EM algorithm. A face is detected
if the probability of an input sample is above a predeÔ¨Ånedthreshold. The other mixture of subspaces method uses
Kohonen‚Äôs self-organizing map for clustering and Fisher
Linear Discriminant to Ô¨Ånd the optimal projection for pat-ternclassiÔ¨Åcation,andaGaussiandistributiontomodelthe
class-conditionaldensity function of the projected samples
foreachclass. Theparametersoftheclass-conditionalden-
sityfunctionsaremaximumlikelihoodestimatesandthede-
cision rule is also based on maximum likelihood. A widerangeoffaceimagesincludingonesindifferentposes,with
differentexpressionsandunderdifferentlightingconditions
are used as the training set to capturethe variationsof hu-man faces. Our methods have been tested on three sets of
225 images which contain 871 faces. Experimental results
on the Ô¨Årst two datasets show that our methods perform as
well as the best methods in the literature, yet have fewer
falsedetects.
1 Introduction
Images of human faces are central to intelligent human
computerinteraction. Much research is being done involv-
ing face images, including face recognition, face tracking,pose estimation, expression recognition and gesture recog-
nition. However, most existing methods on these topics
assume human faces in an image or an image sequence
have been identiÔ¨Åed and localized. To build a fully auto-
mated system that extracts informationfrom images of hu-man faces, it is essential to develop robust and efÔ¨Åcient al-
gorithms to detect human faces. Given a single image or a
sequence of images, the goal of face detection is to iden-tifyandlocateallofthehumanfacesregardlessoftheirpo-
sitions, scales, orientations, poses and lighting conditions.
This is a challenging problem because human faces are
highlynon-rigidobjectswitha highdegreeofvariabilityin
size,shape,colorandtexture. Mostrecentmethodsforface
detection can only detect upright, frontal faces under cer-tain lighting conditions. In this paper, we present two face
detection methods that use mixtures of linear subspaces to
detect faces with different features and expressions, in dif-ferentposes,andunderdifferentlightingconditions.
Since the images of a human face lie in a complex sub-
set of the image space that is unlikely to be modeled by
a single linear subspace, we use a mixture of linear sub-
spaces to model the distribution of face and nonface pat-
terns. The Ô¨Årst detection method is an extension of factoranalysis. Factoranalysis(FA),astatisticalmethodformod-
eling the covariance structure of high dimensional data us-
ing a small number of latent variables, has analogue withprincipalcomponentanalysis(PCA).HoweverPCA,unlike
FA,doesnotdeÔ¨Åneaproperdensitymodelforthedatasince
the cost of coding a data pointis equal anywherealong the
principalcomponentsubspace(i.e.,thedensityisunnormal-
ized along these directions). Further, PCA is not robust toindependentnoise in the featuresof the data since the prin-
cipalcomponentsmaximizethevariancesoftheinputdata,
thereby retaining unwanted variations. Hinton et al. haveapplied FA to digit recognition and they compare the per-
formance of PCA and FA models [10]. A mixture model
of factor analyzers has recently been extended [7] and ap-
pliedtofacerecognition[6]. BothstudiesshowthatFAper-
forms better than PCA in digit and face recognition. Sincepose,orientation,expression,andlightingaffecttheappear-
anceof a humanface,the distributionof facesin the image
space can be better represented by a mixture of subspaceswhereeachsubspacecapturescertaincharacteristicsofcer-
tain face appearances. We present a probabilistic method
thatusesamixtureoffactoranalyzers(MFA)todetectfaces
with wide variations. The parametersin the mixture model
areestimatedusinganEMalgorithm.
The second method that we present uses Fisher Linear
Discriminant(FLD) to projectsamples from a high dimen-
sional image space to a lower dimensional feature space.
Recently, the Fisherface method has been shown to out-
perform the widely used Eigenface method in face recog-nition [2]. The reason for this is that FLD provides a bet-
ter projection than PCA for pattern classiÔ¨Åcation. In the
second proposed method, we decompose the training face
and nonface samples into several classes using Kohonen‚Äôs
Self Organizing Map (SOM). From these labeled classes,thewithin-classandbetween-classscattermatricesarecom-
puted, thereby generating the optimal projection based on
FLD. For each subspace, we use a Gaussian to modeleachclass-conditionaldensityfunctionwheretheparametersare
estimated based on maximum likelihood [5]. To detect
faces, each input image is scanned with a rectangular win-
dow in which the class-dependentprobability is computed.
Themaximumlikelihooddecisionruleisusedtodeterminewhetheraface isdetectedornot.
Tocapturethevariationsinfacepatterns,weuseasetof
1,681 face images from Olivetti [20], UMIST [8], Harvard
[9],Yale[2]andFERET[15]databases. Bothmethodshave
beentestedusingthedatabasesin[18][22]tocomparetheir
performanceswithothermethods. Ourexperimentalresults
onthe data sets used in [18] [22] (whichconsistof 225im-ageswith619faces)showthatourmethodsperformaswell
asthereportedmethodsintheliterature,yetwithfewerfalse
detects. To further test our methods, we collect a set of 80images containing 252 faces. This data set is rather chal-
lenging since it contains proÔ¨Åle faces, faces with expres-
sions and faces with heavy shadows. Our methodsare able
to detect most of these faces regardless of their poses, fa-
cial expressions and lighting conditions. Furthermore, ourmethodshavefewerfalse detectsthanothermethods.
2 RelatedWork
Numerous intensity-based methods have been proposed
recently to detect human faces in a single image or a se-
quence of images. In this section, we give a brief review
of intensity-based face detection methods. See [23] for a
comprehensive survey on face detection. Sung and Pog-
gio [22] report an example-basedlearning approachfor lo-cating vertical frontal views of human faces. They use a
number of Gaussian clusters to model the distributions of
face and nonface patterns. For computational efÔ¨Åciency,a subspace spanned by each cluster‚Äôs eigenvectors is then
used to compute the evidence of a face. A small window
is moved over all portions of an image to determine, based
on distance metrics measured in the subspaces, whether a
face exists in each window. In [16], a detection algorithmis proposed that combines template matching and feature-
based detection method using hierarchical Markov random
Ô¨Åelds(MRF)andmaximum aposteriori probability(MAP)estimation. The watershed algorithm is used to segmentan
image at some Ô¨Åxed scales and to generate an image pyra-
mid. To reduce the search, a heuristic is used to select ar-
eas where faces may appear. Layered processes are usedin a MRF to reÔ¨Çect ap r i o r iknowledge about the spatial
relationships between facial features (eye, mouth and the
whole face) which are identiÔ¨Åed by template matching and
gradient of intensity. Detection decision is based on MAP
estimation. ColmenarezandHuang[3] applyKullbackrel-ative informationfor maximaldiscriminationbetween pos-
itive and negative examples of faces. They use a family
of discrete Markov processes to model the face and back-ground patterns and estimate the density functions. De-
tection of a face is based on the likelihood ratio computed
during training. Moghaddam and Pentland [12] propose a
probabilistic method that is based on density estimation in
a high dimensional space using an eigenspace decomposi-tion. In [18], Rowley et al. use an ensemble of neural net-
worksto learnface and nonfacepatternsforface detection.
Schneiderman et al. describe a probabilistic method basedonlocalappearanceandprincipalcomponentanalysis[21].
TheirmethodgivessomepreliminaryresultsonproÔ¨Åleface
detection. Finally, hidden Markov models [17], higher or-
derstatistics[17],andsupportvectormachines(SVM)[13]
[14] have also been applied to face detection and demon-stratedsomesuccessindetectinguprightfrontalfacesunder
certainlightingconditions.
3 Mixture ofFactorAnalyzers
IntheÔ¨Årstmethod,weÔ¨Åtthemixturemodeloffactoran-
alyzers to the training samples using an EM algorithm and
obtain a distribution of face patterns. To detect faces, each
inputimageisscannedwitharectangularwindowinwhichthe probability of the current input being a face pattern is
calculated. A face is detected if the probability is above
a predeÔ¨Åned threshold. We brieÔ¨Çy describe factor analysisandamixtureoffactoranalyzersinthissection. Thedetails
ofthesemodelscanbefoundin [1][7]./BF/BA/BD /BY /CP/CR/D8/D3/D6 /BT/D2/CP/D0/DD/D7/CX/D7
Factor analysis is a statistical model in which the ob-
served vector is partitioned into an unobserved systematic
part and an unobserved error part. The systematic part istaken as a linear combination of a relatively small number
ofunobservedfactorvariableswhile thecomponentsofthe
error vector are considered as uncorrelatedor independent.
Fromanotherpointofview,factoranalysisgivesa descrip-
tion of the interdependenceof a set of variablesin termsofthefactorswithoutregardtotheobservedvariability. Inthis
model, a/CS-dimensional real-valued observable data vector/DCis modeled using a /D4-dimensional vector of real-valued
factors /DEwhere /D4is generally much smaller than /CS.T h e
generativemodelisgivenby:/DC /BP/A3 /DE /B7 /D9 (1)
where /A3isknownasthe factorloadingmatrix . Thefactors/DEa r ea s s u m e dt ob e /C6 /B4/BC /BN/C1 /B5distributed (zero-mean inde-
pendent normals with unit variance). The /CS-dimensional
random variable /D9is distributed /C6 /B4/BC /BN /A9/B5where /A9is a di-
agonalmatrix,duetotheassumptionthattheobservedvari-ables are independent given the factors. According to this
model,/DCisthereforedistributedwithzeromeanandcovari-
ance /A6/BP/A3 /A3
/CC/B7/A9. ThegoaloffactoranalysisistoÔ¨Åndthe/A3and /A9thatbest modelthe covariancestructureof /DC.T h e
factor variables /DEmodel correlationsbetween the elements
of /DC, while the /D9variables account for independent noise
in each element /DC.T h e /D4factors play the same role as the
principalcomponentsinPCA,i.e.,theyareinformativepro-jections of the data. Given/A3and /A9, the expected value of
thefactorscanbecomputedthroughthelinearprojections:/BX /CJ /DE /CY /DC /CL/BP /AC/DC (2)/BX /CJ /DE/DE
/CC/CY /DC /CL/BP /C1 /A0 /AC /A3/B7 /AC/DC /DC
/CC/AC
/CC(3)
where /AC /BP/A3
/CC/A6
/A0 /BD./BF/BA/BE /C5/CX/DC/D8/D9/D6/CT /C5/D3 /CS/CT/D0
In this section, we consider a mixture of /D1factor an-
alyzers (indexed by /CU/CY
/BN/CY /BP /BD /BN/BM/BM/BM/BN/D1) where each factor
analyzer has the same number of /D4factors and each fac-
toranalyzerhasa differentmean /AM/CY. The generativemodel
obeysthemixturedistribution:/C8 /B4 /DC /B5/BP
/D1/CG/CY /BP/BD
/CI/C8 /B4 /DC /CY /DE/BN /CU/CY
/B5 /C8 /B4 /DE /CY /CU/CY
/B5 /C8 /B4 /CU/CY
/B5 /CS/DE(4)
where/C8 /B4 /DE /CY /CU/CY
/B5/BP /C8 /B4 /DE /B5/BP /C6 /B4/BC /BN/C1 /B5 (5)/C8 /B4 /DC /CY /DE/BN /CU/CY
/B5/BP /C6 /B4 /AM/CY
/B7/A3/CY
/DE/BN /A9/B5 (6)
The parametersof this mixture modelare /CU /B4 /AM/CY, /A3/CY
/B5
/D1/CY /BP/BD, /AP,/A9 /CVwhere /APis the vector of adaptable mixing proportions,/AP/CY
/BP /C8 /B4 /CU/CY
/B5. The latent variablesin this modelare the fac-
tors /DEand the mixture indicator variable /CU/CY,w h e r e /CU/CY
/BP/BD
whenthedatapointisgeneratedbytheÔ¨Årstfactoranalyzer.
Given a set of training images, the EM algorithm [4] is
usedtoestimate /CU /B4 /AM/CY
/BN /A3/CY
/B5
/D1/CY /BP/BD, /AP, /A9 /CV. FortheE-stepofthe
EM algorithm, we need to compute expectations of all the
interactions of the hidden variables that appear in the log
likelihood,/BX /CJ /CU/CY
/DE /CY /DC/CX
/CL/BP /BX /CJ /CU/CY
/CY /DC/CX
/CL /BX /CJ /DE /CY /CU/CY
/BN/DC/CX
/CL (7)
/BX /CJ /CU/CY
/DE/DE
/CC/CY /DC/CX
/CL/BP /BX /CJ /CU/CY
/CY /DC/CX
/CL /BX /CJ /DE/DE
/CC/CY /CU/CY
/BN/DC/CX
/CL(8)
DeÔ¨Åning/CW/CX/CY
/BP /BX /CJ /CU/CY
/CY /DC/CX
/CL /BB /C8 /B4 /DC/CX
/BN/CU/CY
/B5/BP /AP/CY
/C6 /B4 /DC/CX
/A0 /AM/CY
/BN /A3/CY
/A3
/CC/CY
/B7/A9 /B5
(9)
andusingequations(2)and(6),weobtain/BX /CJ /CU/CY
/DE /CY /DC/CX
/CL/BP /CW/CX/CY
/AC/CY
/B4 /DC/CX
/A0 /AM/CY
/B5 (10)
where /AC/CY
/AH /A3
/CC/CY
/B4/A3/CY
/A3
/CC/CY
/B5
/A0 /BD. Similarly, using equations (3)
and(8),weobtain/BX /CJ /CU/CY
/DE/DE
/CC/CY /DC/CX
/CL/BP /CW/CX/CY
/B4 /C1 /A0 /AC/CY
/A3/CY
/B7 /AC/CY
/B4 /DC/CX
/A0 /AM/CY
/B5/B4 /DC/CX
/A0 /AM/CY
/B5
/CC/AC
/CC/CY
/B5
(11)
The EM algorithm for mixture of factor analyzers can be
statedasfollows:/AFE-step: Compute /BX /CJ /CU/CY
/CY /DC/CX
/CL, /BX /CJ /DE /CY /CU/CY
/BN/DC/CX
/CLand /BX /CJ /DE/DE
/CC/CY/CU/CY
/BN/DC/CX
/CLfor all data points /CXand mixture components/CY./AFM-step: Solveasetoflinearequationsfor /AP/CY, /A3/CY, /AM/CY
and /A9.
The mixture of factor analyzersis essentially a reduced di-
mensionality mixture of Gaussians. Each factor analyzer
Ô¨Åts a Gaussian to a portion of the data, weighted by the
posteriorprobabilities, /CW/CX/CY. Since thecovariancematrixfor
each Gaussian is speciÔ¨Åed through the lower dimensionalfactorloadingmatrices,themodelhas/D1/D4/CS /B7 /CS,ratherthan/D1/CS /B4 /CS /B7/BD /B5 /BP /BEparametersdedicatedtomodelingcovariance
structureinhighdimensions./BF/BA/BF /BW/CT/D8/CT/CR/D8/CX/D2/CV /BY /CP/CR/CT /C8 /CP/D8/D8/CT/D6/D2/D7
To detectfaces, eachinputimageis scannedwith a rect-
angular window in which the probability of there being a
face pattern is estimated as given in equation(4). A face is
detected if the probability is above a predeÔ¨Åned threshold.
In order to detect faces of different scales, each input im-
ageisrepeatedlysubsampledbyafactorof1.2andscannedthroughfor10iterations.
4 MixtureofLinearSpacesUsingFisherLin-
earDiscriminant
In the second mixture model, we Ô¨Årst use Kohonen‚Äôs
self-organizing map [11] to divide the face and nonface
samplesinto /CR/BDfaceclassesand /CR/BEnonfaceclasses,thereby
generating labels for the samples. Next, Fisher projection
is computed based on all /CR/BD
/B7 /CR/BEclasses to maximize the
ratioofthe between-classscatter (variance)andthe within-classscatter(variance). Thenowlabeledtrainingsetispro-
jected from a high dimensional image space to a lower di-
mensionalfeaturespace,andaGaussiandistributionisused
to model the class-conditional density function for each
class where the parameters are estimated using the maxi-
mum likelihood principle. For detection, the conditional
probability of each sample given each class is computedand the maximum likelihood principle is used to decide to
which class the sample belongs. In our experiments, the
reasonthatwechoose25faceand25nonfaceclassesisbe-
cause of the size of training set. If the number of classes
is too small, the clustering results may be poor. On theother hand, we may not have enough samples to estimate
the class-conditional density function well if we choose a
largenumberofclasses./BG/BA/BD /C4/CP/CQ /CT/D0/CX/D2/CV /CB/CP/D1/D4/D0/CT/D7 /CD/D7/CX/D2/CV /CB/C7/C5
In applyingFisher Linear Discriminant to Ô¨Ånd a projec-
tion, we need to know the class label of each training sam-
ple. However,suchinformationisnotavailableinthetrain-
ingsamples. Therefore,we useKohonen‚ÄôsSelf-OrganizingMap [11] to divide face samples into a Ô¨Ånite number of
classes. In our experiments, we divide the face sample im-
ages into 25 classes. After training, the Ô¨Ånal weight vectorforeachnodeis thecentroidofthe class, i.e.,the prototype
vector, which corresponds to the prototype of each class.
Thesameprocedureisappliedtononfacesamples. Figure1
showstheprototypicalfaceofeachclass. Itisclearthatthe
samplefaceimageswithdifferentposesandunderdifferentlightingconditions(intensityincreasesfromthelowerright
cornertotheupperleftcorner)havebeenclassiÔ¨Åedintodif-
ferentclasses. NotethattheSOM algorithmalso placestheprototypes in the two dimensional feature map, shown in
1, in accordance with their topological relationships in the
imagespace. Inotherwords,prototypevectorscorrespond-
ing to nearby points on the feature map grid have nearby
locationsin thehighdimensionalimagespace(e.g.,nearbyprototypeshavesimilar intensityandpose)./BG/BA/BE /BY/CX/D7/CW/CT/D6 /C4/CX/D2/CT/CP/D6 /BW/CX/D7/CR/D6/CX/D1/CX/D2/CP/D2 /D8
While PCA is commonly used to project face patterns
from a high dimensional image space to a lower dimen-
sional feature space, a drawback of this approach is thatit deÔ¨Ånes a subspace such that it has the greatest variance
of the projected sample vectors among all the subspaces.
However, such projection is not suitable for classiÔ¨Åcationsinceitmaycontainprincipalcomponentswhichretainun-
wanted large variations. Therefore, the classes in the pro-
jectedspacemaynotbewellclusteredandinsteadsmeared
together [2] [6] [10]. Fisher Linear Discriminant is an ex-
ampleof a class speciÔ¨Åc methodthat Ô¨Åndsthe optimalpro-jection for classiÔ¨Åcation. Rather than Ô¨Ånding a projection
that maximizes the projected variance, FLD determines a
projection,/DE /BP /CF
/CC/BY/C4 /BW
/DC, that maximizes the ratio be-
/BY/CX/CV/D9/D6/CT /BD/BA /C8/D6/D3/D8/D3/D8 /DD/D4 /CT /D3/CU /CT/CP/CR/CW /CU/CP/CR/CT /CR/D0/CP/D7/D7/BA
tween the between-class scatter (variance) and the within-
classscatter(variance). Consequently,classiÔ¨Åcationissim-pliÔ¨Åedin the projectedspace. Recently,it hasbeendemon-
strated that the Fisherface method outperforms the Eigen-
facemethodinfacerecognition[2].
Considera/CR-classproblem,let thebetween-classscatter
matrixbedeÔ¨Ånedas/CB/BU
/BP
/CR/CG/CX /BP/BD
/C6/CX
/B4 /AM/CX
/A0 /AM /B5/B4 /AM/CX
/A0 /AM /B5
/CC(12)
andthe within-classscattermatrixbedeÔ¨Ånedas/CB/CF
/BP
/CR/CG/CX /BP/BD
/CG/DC/CZ
/BE /CG/CX
/B4 /DC/CZ
/A0 /AM/CX
/B5/B4 /DC/CZ
/A0 /AM/CX
/B5
/CC(13)
where /AMis the mean of all samples, /AM/CXis the mean of class/CG/CX,a n d /C6/CXis the number of samples in class /CG/CX.T h e
optimal projection /CF/BY/C4 /BWis chosen as the matrix with or-
thonormalcolumnswhichmaximizestheratioofthe deter-
minant of the between-class scatter matrix of the projected
samplestothedeterminantofthewithin-classscattermatrixoftheprojectedsampled,i.e.,/CF/BY/C4 /BW
/BP /CP/D6/CV /D1/CP/DC/DB
/CY /CF
/CC/CB/BU
/CF /CY/CY /CF
/CC/CB/CF
/CF /CY
/BP/CJ /DB/BD
/DB/BE
/BM/BM/BM /DB/D1
/CL(14)
where /CU /DB/CX
/CY /CX /BP/BD /BN /BE /BN/BM/BM/BM /BN/D1 /CVis thesetofgeneralizedeigen-
vectorsof /CB/BUand /CB/CF,correspondingto the /D1largestgen-
eralized eigenvalues /CU /AL/CX
/CY /CX /BP /BD /BN /BE /BN/BM/BM/BM /BN/D1 /CV.H o w e v e r , t h e
rank of /CB/BUis /CR /A0 /BDor less because it is the sum of /CRma-
trices of rank one or less. Thus, the upper bound on /D1is
/CR /A0 /BD[5]. See [2] for details about a method to overcome
singularityproblemsin computing /CF/BY/C4 /BW./BG/BA/BF /BV/D0/CP/D7/D7/B9/BV/D3/D2/CS/CX/D8/CX/D3/D2/CP/D0 /BW/CT/D2/D7/CX/D8 /DD /BY /D9/D2/CR/D8/CX/D3/D2
Once /CF/BY/C4 /BWis computed, the now labeled training set
is projected to the /CR /A0 /BDdimensional feature space, i.e.,/DE /BP /CF
/CC/BY/C4 /BW
/DC, and a Gaussian distribution is used to
model each class-conditional density (CCD) function, i.e.,/C8 /B4 /DE /CY /CG/CX
/B5/BP /C6 /B4 /AM/CG/CX
/BN /A6/CG/CX
/B5where /CX /BP/BD /BN/BM/BM/BM /BN/CR. The param-
eters, /AI/CG/CX
/BP/B4 /AM/CG/CX
/BN /A6/CG/CX
/B5 /CVof each CCD are the maximum
likelihoodestimates, i.e.,/CM /AM/CG/CX
/BP
/BD/CY /CG/CX
/CY
/CG/DE/CZ
/BE /CG/CX
/DE/CZ (15)
and/CM/A6/CG/CX
/BP
/BD/CY /CG/CX
/CY
/CG/DE/CZ
/BE /CG/CX
/B4 /DE/CZ
/A0 /CM /AM/CG/CX
/B5/B4 /DE/CZ
/A0 /CM /AM/CG/CX
/B5
/CC(16)/BG/BA/BG /BW/CT/D8/CT/CR/D8/CX/D2/CV /BY /CP/CR/CT /C8 /CP/D8/D8/CT/D6/D2/D7
Each input image is scanned with a rectangularwindow
to determine whether a face exists in the window or not.
The decision rule for deciding whether an input windowcontainsa faceornotisbasedonmaximumlikelihood,/CG
/A3/BP /CP/D6/CV /D1/CP/DC/CG/CX
/C8 /B4 /DE /CY /CG/CX
/B5 (17)
To detect faces of different scales, each input image is re-
peatedlysubsampledbyafactorof1.2andscannedthrough
for10iterations.
5 Experiments
Fortraining,weuseasetof1,681faceimages(collected
from Olivetti [20], UMIST [8], Harvard [9], Yale [2] andFERET[15]databases)whichhavewidevariationsinpose,
facialexpressionandlightingcondition. Inthesecondmix-
ture method, we start with 8,422 nonface examples from
400imagesoflandscapes,trees, buildings,etc. Althoughit
is extremely difÔ¨Åcult to collect a representative set of non-face examples,the bootstrapmethodsimilar to [22] is used
to include more nonface examples during training. Each
face sample is manually cropped and normalized such thatit is aligned vertically and its size is/BE/BC /A2 /BE/BCpixels. To
make the detection method less sensitive to scale and ro-
tation variation, 10 face examples are generated from each
originalsample. The imagesare producedbyrandomlyro-
tatingthe imagesbyup to /BD/BHdegreeswith scaling between/BK/BC/B1and /BD/BE/BC/B1. Thisproduces16,810facesamples.
We test both methods on the three sets of images col-
lected by Rowley [18], Sung [22] and ourselves. In ourexperiments, a detected face is a successful detect is if the
subimage contains eyes and mouth. Otherwise, it is a false
detect. The detection rate is the ratio between the number
ofsuccessfuldetectsandthenumberoffacesinthetestset.Table 1 shows the detection rates of our methods and the
reportedresults of severaldetectionmethodson the test set
in[18]. Experimentalresultsontestset1,whichconsistsof
125 images (483 faces) excluding 5 images of hand drawn
faces, show that our methods have comparable detectionperformance with other methods, yet with fewer false de-
tects. Table 1 also shows the our experimental results on
the test set of Sung and Poggio [22] which consists of 20imagesexcluding3imagesofline drawnfaces(136faces).
Bothofourmethodsconsistentlyperformwellandhavefew
false detects.
Test set 3 consists of 80 images (252 faces), collected
from the World Wide Web, with different poses, expres-
sions and faces with heavy shadows. The detection ratesare/BK/BI /BM /BJ/B1and /BK/BK /BM /BE/B1for MFA and FLD-based methods.
The number of false detects are /BG/BHand /BG/BC, respectively.
Bothmethodsperformequallywell indetectingthesefacesthoughtheFLD-basedmethodperformsslightlybetterthan
the Ô¨Årst one. Figures 2 and 3 show the results of our meth-
ods on some test images. See the web page mentionedabovefor moreresults. Notice that there is a false detect in
theupperleftcorneroftheimageinFigure2sinceonewin-
dowresemblesaface. Alsonoticethatourmethodscande-
tect,uptocertaindegree,proÔ¨Ålefacesandfaceswithheavy
shadows. However occluded, rotated faces or faces withsunglasses cannot be detected effectively by both methods
due to lack of such examples in the training sets. None
of the existing detection methods cannot effectively detectthese types of faces except one recent method [19] seems
to able to detect rotated faces. Nevertheless, this method
cannotdetectoccludedfacesorfacewithheavyshadows.
6 DiscussionandConclusion
We havedescribedmethodsusingmixtureof linearsub-
spaces methods to detect human faces regardless of their
poses, facial expressions and lighting conditions. Bothmethods Ô¨Ånd better projection than PCA for pattern clas-
siÔ¨Åcation,therebyfacilitatingdetectionoffaceandnonface
patterns. The Ô¨Årst methodÔ¨Åts a mixtureof factor analyzerstoestimatethedensityfunctionoffaceimages,andthesec-
ondmethodusesSelf-OrganizingMaptopartitionthetrain-
ing set into classes and Fisher Linear Discriminant to Ô¨Ånd
the optimal projection for classiÔ¨Åcation. Experimental re-
sultsonthreesetsofimagesdemonstratethatbothmethodsperform as well as the best algorithms in detecting upright
frontalfaces,yetwith fewerfalse detects.
The contributions of this paper can be summarized as
follows. First, we introduce projection methods that per-
/CC /CP/CQ/D0/CT /BD/BA /BX/DC/D4 /CT/D6/CX/D1/CT/D2/D8/CP/D0 /D6/CT/D7/D9/D0/D8/D7 /D3/D2 /CX/D1/CP/CV/CT/D7 /CU/D6/D3/D1 /D8/CT/D7/D8 /D7/CT/D8 /BD /B4/BD/BE/BH /CX/D1/CP/CV/CT/D7 /DB/CX/D8/CW /BG/BK/BF /CU/CP/CR/CT/D7/B5 /CX/D2 /CJ/BD/BK/CL /CP/D2/CS /D8/CT/D7/D8/D7/CT/D8 /BE /B4/BE/BC /CX/D1/CP/CV/CT/D7 /DB/CX/D8/CW /BD/BF/BI /CU/CP/CR/CT/D7/B5 /CX/D2 /CJ/BE/BE/CL /B4/D7/CT/CT /D8/CT/DC/D8 /CU/D3 /D6 /CS/CT/D8/CP/CX/D0/D7/B5/BA
Test Set 1 Test Set 2
Method DetectRate False Detects Detect Rate FalseDetects
Mixtureoffactoranalyzers 92.3% 82 89.4% 3
Fisherlineardiscriminant 93.6% 74 91.5% 1
Distribution-based[22] N/A N/A 81.9% 13
Neuralnetwork[18] 92.5% 862 90.3% 42
NaiveBayes[21] 93.0% 88 91.2% 12
Kullbackrelativeinformation[3] 98.0% 12758 N/A N/A
Supportvectormachine[13] N/A N/A 74.2% 20
/BY/CX/CV/D9/D6/CT /BE/BA /CB/CP/D1/D4/D0/CT /CT/DC/D4 /CT/D6/CX/D1/CT/D2/D8/CP/D0 /D6/CT/D7/D9/D0/D8/D7 /D9/D7/CX/D2/CV/D1/CX/DC/D8/D9/D6/CT /D3/CU /CU/CP/CR/D8/D3 /D6 /CP/D2/CP/D0/DD/DE/CT/D6/D7 /D3/D2 /CX/D1/CP/CV/CT/D7 /CU/D6/D3/D1/D8/CW/D6/CT/CT /D8/CT/D7/D8 /D7/CT/D8/D7/BA /BX/DA/CT/D6/DD /CS/CT/D8/CT/CR/D8/CT/CS /CU/CP/CR/CT /CX/D7 /D7/CW/D3 /DB/D2/DB/CX/D8/CW /CP/D2 /CT/D2/CR/D0/D3/D7/CX/D2/CV /DB/CX/D2/CS/D3 /DB/BA
/BY/CX/CV/D9/D6/CT /BF/BA /CB/CP/D1/D4/D0/CT /CT/DC/D4 /CT/D6/CX/D1/CT/D2/D8/CP/D0 /D6/CT/D7/D9/D0/D8/D7 /D9/D7/CX/D2/CV/D1/CX/DC/D8/D9/D6/CT /D3/CU /D7/D9/CQ/D7/D4/CP/CR/CT/D7 /DB/CX/D8/CW /BY/CX/D7/CW/CT/D6 /C4/CX/D2/CT/CP /D6 /BW/CX/D7/B9/CR/D6/CX/D1/CX/D2/CP/D2/D8 /D3/D2 /CX/D1/CP/CV/CT/D7 /CU/D6/D3/D1 /D8/CW/D6/CT/CT /D8/CT/D7/D8 /D7/CT/D8/D7/BA /BX/DA/B9/CT/D6/DD /CS/CT/D8/CT/CR/D8/CT/CS /CU/CP/CR/CT /CX/D7 /D7/CW/D3 /DB/D2 /DB/CX/D8/CW /CP/D2 /CT/D2/CR/D0/D3/D7/CX/D2/CV/DB/CX/D2/CS/D3 /DB/BA
form better than PCA. Consequently, the classiÔ¨Åcation re-
sult in the linear subspace is better. Second, we apply mix-
ture models such that the linear subspaces can better cap-
turethevariationsoffacepatterns. Althoughsomemethods[12][22]haveappliedmixturemodel,theyusePCAforpro-
jection which is suboptimal for classiÔ¨Åcation in subspaces.
On the other hand, it is not clear how SVM performs in
face detection since the study in [13] has applied SVM on
a rather small test set with 136 faces. It will be of great in-teresttocompareourmethodswithSVM onalargetestset
since SVM aims to Ô¨Ånd the optimal hyperplane that min-
imizes the generalization error under the theoretical upperbounds.
Acknowledgments
D. Kriegman was supported in part by the Army Re-
seearchOfÔ¨ÅceunderAROY-99-0006andtheNationalEye
Institute.
References
[1] T. W. Anderson. An Introduction to Multivariate Statistical
Analysis. John Wiley, New York, 1984.
[2] P.Belhumeur,J.Hespanha,andD.Kriegman.Eigenfacesvs.
Ô¨Åsherfaces: Recognition using class speciÔ¨Åc linear projec-
tion.IEEE Transactions on Pattern Analysis and Machine
Intelligence , 19(7):711‚Äì720, 1997.
[3] A. J. Colmenarez and T. S. Huang. Face detection with
information-based maximum discrimination. In Proceed-
ingsoftheIEEEComputerSocietyConferenceonComputer
Vision andPatternRecognition , pages 782‚Äì787, 1997.
[4] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum
likelihood fromincompletedataviatheemalgorithm. Joru-
anl ofthe Royal Statistical Society , 39(1):1‚Äì38, 1977.
[5 ] R.O.Du d aan dP .E.Hart. Pattern ClassiÔ¨Åcation and Scene
Analysis. John Wiely, New York, 1973.
[6] B. J. Frey, A. Colmenarez, and T. S. Huang. Mixtures of
local subspaces for face recognition. In Proceedings of the
IEEEComputerSocietyConferenceonComputerVisionand
PatternRecognition , pages 32‚Äì37, 1998.
[7] Z. Ghahramani and G. E. Hinton. The em algorithm for
mixturesoffactoranalyzers. TechnicalReportCRG-TR-96-
1, Department of Computer Science, University of Toronto,
1996. Available at ftp://ftp.cs.toronto.edu/pub/zoubin/tr-96-1.ps.gz.
[8] D. B. Graham and N. M. Allinson. Characterizing virtual
eigensignatures for general purpose face recognition. InH. Wechsler, P. J. Phillips, V. Bruce, F. Fogelman-Soulie,
and T.S.Huang, editors, Face Recognition: FromTheoryto
Applications , volume 163 of NATO ASI Series F, Computer
and Systems Sciences , pages 446‚Äì456. Springer,1998.
[9] P.Hallinan. ADeformable Model for Face Recognition Un-
derArbitraryLightingConditions . PhDthesis,HarvardUni-
versity, 1995.[10] G. E. Hinton, P. Dayan, and M. Revow. Modeling the man-
ifolds of images of handwritten digits. IEEE Trans. Neural
Networks , 8(1):65‚Äì74, 1997.
[11] T. Kohonen. Self Organizing Map . Springer, 1996.
[12] B. Moghaddam and A. Pentland. Probabilistic visual learn-
ing for object recognition. IEEE Transactions on Pattern
Analysis andMachine Intelligence , 19(7):696‚Äì710, 1997.
[13] E.Osuna, R. Freund, and F. Girosi. Training support vector
machines: an application to face detection. In Proceedings
of the IEEE Computer Society Conference on Computer Vi-
sionand PatternRecognition , pages 130‚Äì136, 1997.
[14] C.Papageorgiou, M. Oren,and T.Poggio. Ageneralframe-
work for object detection. In Proceedings of the FifthInter-
national Conference on Computer Vision , pages 555‚Äì562,
1998.
[15] P. J. Phillips, H. Moon, S. Rizvi, and P. Rauss. The
feret evaluation. In H. Wechsler, P. J. Phillips, V. Bruce,
F. Fogelman-Soulie, and T. S. Huang, editors, Face Recog-
nition: From Theory to Applications , volume 163 of NATO
ASI Series F, Computer and Systems Sciences , pages 244‚Äì
261. Springer, 1998.
[16] R. J. Qian and T. S. Huang. Object detection using hierar-
chical mrf and map estimation. In Proceedings of the IEEE
Computer Society Conference on Computer Vision and Pat-
ternRecognition , pages 186‚Äì192, 1997.
[17] A. N. Rajagopalan, K. S. Kumar, J. Karlekar, R. Mani-
vasakan, and M. M. Patil. Finding faces in photographs. In
Proceedings of the Sixth International Conference on Com-
puter Vision , pages 640‚Äì645, 1998.
[18] H.Rowley,S.Baluja,andT.Kanade. Neuralnetwork-based
face detection. IEEE Transactions on Pattern Analysis and
Machine Intelligence , 20(1):23‚Äì38, 1998.
[19] H. Rowley, S. Baluja, and T. Kanade. Rotation invariant
neural network-based face detection. In Proceedings of the
IEEEComputerSocietyConferenceonComputerVisionand
PatternRecognition , pages 38‚Äì44, 1998.
[20] F. S. Samaria. Face Recognition Using Hidden Markov
Models. PhD thesis,University of Cambridge, 1994.
[21] H. Schneiderman and T. Kanade. Probabilistic modeling of
local appearance and spatial relationships for object recog-nition. In Proceedings of the IEEE Computer Society Con-
ference on Computer Vision and PatternRecognition , pages
45‚Äì51, 1998.
[22] K.-K. Sung and T. Poggio. Example-based learning for
view-based human face detection. IEEE Transactions on
Pattern Analysis and Machine Intelligence , 20(1):39‚Äì51,
1998.
[23] M.-H. Yang, N. Ahuja, and D. Kriegman. A survey on face
detection methods. 2000. To be submitted.
"
https://ieeexplore.ieee.org/document/7280539,"Face Expression Recognition with a 2-Channel 
Convolutional Neural Network 
Dennis Hamester, Pablo Barros, Stefan Wermter 
University of Hamburg -Department of Informatics 
Vogt-Kolln-StraBe 30, 22527 Hamburg, Germany 
http://www.informatik.uni-hamburg.de/wTMI 
{hamester, barros, wermter} @informatik.uni-hamburg.de 
Abstract-A new architecture based on the Multi-channel 
Convolutional Neural Network (MCCNN) is proposed for rec¬≠
ognizing facial expressions. Two hard-coded feature extractors 
are replaced by a single channel which is partially trained in 
an unsupervised fashion as a Convolutional Autoencoder (CAE). 
One additional channel that contains a standard CNN is left 
unchanged. Information from both channels converges in a fully 
connected layer and is then used for classification. We perform 
two distinct experiments on the JAFFE dataset (Ieave-one-out 
and ten-fold cross validation) to evaluate our architecture. Our 
comparison with the previous model that uses hard-coded Sobel 
features shows that an additional channel of information with 
unsupervised learning can significantly boost accuracy and re¬≠
duce the overall training time. Furthermore, experimental results 
are compared with benchmarks from the literature showing 
that our method provides state-of-the-art recognition rates for 
facial expressions. Our method outperforms previously published 
methods that used hand-crafted features by a large margin. 
I. INTRODUCTION 
Recognizing human emotions has been the focus of at¬≠
tention in several areas from psychology and sociology to 
cognitive and computer science. Emotions have been studied 
for many years, and are still one of the most challenging topics 
in human psychological interactions. Through emotional state 
determination, it is possible to enhance, highlight and under¬≠
stand better human interactions, actions and even feelings. 
The Artificial Intelligence community has studied and pro¬≠
posed approaches for automatic emotion recognition for the 
past two decades [1]-[3]. Emotional states can be conveyed 
by facial expressions, body posture, motion, language structure 
or even from the environment [4]. Based on facial expression, 
in one of the most comprehensive studies on understanding 
human emotions, Ekman and Friesen [5] established six uni¬≠
versal emotions: disgust, fear, happiness, surprise, sadness 
and anger. 
One of the most relevant characteristics of the presented 
systems is the use of a set of specific features, which constrains 
the solution to some rules. When used for facial expression 
recognition, the solutions based on hard-coded features show 
very good results, but lack the capability to be applied in a real¬≠
world scenario because of the many restrictions based on en¬≠
vironment illumination, position of the subject, and skin color 
among others [6]. To overcome this problem in the computer 
vision area, the use of implicit features for image classification 
led to some success in the past years [7]-[9]. Among implicit 
978-1-4799-1959-8/15/$31.00 @2015 IEEE feature models, deep neural networks [10] were proposed be¬≠
cause they use the data themselves to learn the most significant 
aspects of the image. One of the advantages of deep neural 
networks is their power of generalization, and once they are 
trained, the low computational cost to extract features and 
classify them. Among deep neural architectures, deep belief 
networks [11] and Convolutional Neural Networks (CNN) [12] 
are the ones which present the most promising results for a 
wide range of applications [13]-[15]. 
In the past years, some approaches using deep neural 
networks for facial expression recognition were proposed and 
evaluated, for example by Kahou et al. [16]. In their work they 
evaluate two experiments using CNNs. In the first experiment, 
they implement a standard CNN and train it with the Acted 
Facial Expression in the Wild (AFEW) dataset [17]. In the sec¬≠
ond experiment, they pre-train the model with the Toronto Face 
Dataset. After that, a softmax layer is trained with the AFEW 
dataset. The networks presented in that approach implement 
the usual CNN model, adapting the number of layers and filters 
for each experiment executed. Both experiments were used for 
the recognition of six facial expressions, based on the universal 
facial expressions and one neutral expression. The idea of pre¬≠
training was shown to be successful, but a problem remains: 
the network is trained to learn specific features from the data 
it is shown. If a fair amount of data is shown to the network, 
it can have a significant capacity for generalization. If not, 
the network will not be able to learn proper features and will 
not be able to recognize images that were not shown during 
training. 
Our proposed model uses a Multi-channel Convolutional 
Neural Network (MCCNN) architecture, proposed in previous 
work [18]. This network is able to learn with less data than the 
usual deep learning models, and needs less effort for training. 
One of the most sensitive restrictions of the MCCNN is the use 
of a pair of hard-coded Sobel-based layers which show good 
results when applied in a 3-channel topology. The proposed 
model replaces both Sobel channels with a single channel 
that is trained separately and unsupervised as a Convolutional 
Autoencoder (CAE). 
Unsupervised feature extraction has become an attractive 
alternative to hand-crafted features and can yield highly com¬≠
petitive results [19], [20]. The second channel in our proposed 
model contains fixed filters that are learned by a CAE in a 
separate step. We train this layer on the Kyoto natural images 
dataset [21] so that the filters are not specific to our facial 
expression recognition task. These filter weights are kept fixed 
in all further task-specific training processes. 
To evaluate the proposed model on a face recognition task, 
a set of experiments is described using the JAFFE dataset [22]. 
First, to find the most suitable model parameters, several 
parameter exploration experiments are designed, executed and 
evaluated. Using the selected parameters, two experiments 
with the JAFFE dataset are established. The results are col¬≠
lected, evaluated and discussed in this paper. Comparisons 
with benchmark results are also shown. 
The paper is structured as follows: section II introduces 
our 2-channel architecture and describes preprocessing as well 
as how both channels are trained. The methodology for our 
experiments is given in section III. Preliminary experiments 
for parameter optimization and the results are shown as well. 
Section IV shows results from our main experiments and 
compares them to related work. Finally, a conclusion is given 
in section V. 
II. ARCHITECTURE 
The MCCNN architecture uses multiple independent Con¬≠
volutional Neural Networks in parallel, but connects them in 
the last layer to extract and recognize patterns from images. In 
previous approaches, the use of three channels was explored, 
and presented good results for hand posture [18] and motion 
recognition [23]. The concept behind this architecture is to 
make use of existing knowledge, here represented by a dif¬≠
ferent channel, to diversify the input. In our previous work, 
each channel received different information, usually Sobel¬≠
based filters were applied to the image. The channels receiving 
the image after the application of the Sobel operators acted 
as tuning for the layer receiving the raw image. At the end, 
all channels were connected and able to extract different and 
specific information of the image. 
The Sobel-based layers were a solution to explore the edges 
in two directions, horizontal and vertical. To extend this princi¬≠
ple, our new proposed model (see Figure 1) replaces the hard¬≠
coded Sobel-based layers by a layer containing Gabor-like 
filters. This increases the kind of information extracted from 
the image, specializing one channel with a strong multiple¬≠
orientation edge detector. Following the multi-channel archi¬≠
tecture, the proposed model uses two channels, both of them 
receiving the same image. What differs in the channels is 
the implementation and training of the filters in the first 
layer of each channel. The first channel acts like a common 
CNN training all the parameters in a supervised fashion. The 
second channel uses pre-trained parameters, obtained by a 
CAE, which learn Gabor-like filters. After pre-training, the 
weights of this layer are fixed and not trained in the supervised 
phase. The two channels are connected with a fully-connected 
hidden layer that produces the output for a logistic regression 
classifier. All parameters of network are optimized at the same 
time. The final error is obtained with the output of the two 
channels, so that both second layers act like a bias for each other. Thus, the final classification layer in our proposed model 
combines information from both channels, a standard CNN 
and a second channel whose weights are trained as CAE. 
Figure 1 shows the architecture of the model which was used 
in the experiments. 
When training a CAE on natural images, the resulting filters 
usually look like Gabor filters, which respond strongly to local 
edge elements. They are thus similar to hard-coded Sobel 
filters used in our previous work. However, the Gabor-like 
filters here are much more diverse in the sense that there 
are filters for many more specific orientations. Subsequent 
layers in our architecture receive explicit information about 
the existence of specific edges making classification later on 
easier and more robust. 
This approach provides an important advantage over pure 
supervised methods besides a potential boost in recognition 
performance. After training a CAE once on a generic dataset 
such as the Kyoto database [21], the network can be transferred 
to several different tasks. Reusing a pre-trained network for the 
first layer in a potentially deep architecture is a viable method 
to speed up overall training time. 
A. Channel 1 -Convolutional Neural Network 
A Convolutional Neural Network is a set of pairs of 
convolution and max-pooling layers that enable the model 
to extract and enhance implicit features of an image. When 
stacked together, the first layers act like an edge enhancement 
and allow to extract local features, which are passed to deeper 
layers that act as complex feature extractors. 
Each convolutional layer contains a set of feature maps, 
or filters, that extract features from a region of units using a 
convolution operation. Then, an additive bias is applied and 
the result is passed through a non-linear activation function. 
The value of a unit v;ÔøΩ in the position (x,y) at the n-th feature 
map in the c-th layer is given by 
vXY = max (b + """"""""' ÔøΩ"" ÔøΩ"" whw v(x+h)(y+w) 0) (1) ne en ÔøΩ ÔøΩ ÔøΩ 'Jm (,-l)m ' 
m h=O w=O 
where max(-, 0) represents the rectified linear function, which 
was shown to be more suitable for training deep neural 
architectures [24], ben is the bias for the n-th feature map of 
the c-th layer, and m indexes over the set of features maps in 
the (i-I) layer connected to the current layer c. In the equation, 
Wnek is the weight of the connection between the unit (h,w) 
within a region, or kernel, connected to the previous layer. Hi 
and Wi are the height and width of the kernel. 
In the max-pooling layers, a region of the previous layer is 
connected to a unit in the current layer, reducing the dimen¬≠
sionality of the feature maps. Each max-pooling layer retains 
only the maximum value within its receptive field and passes 
it to the next layer. This enhances invariance to scale and 
distortions of the input [9]. The parameters of a CNN could 
be learned either by a supervised approach tuning the filters in 
a training database [11], or by an unsupervised approach [19]. 
The proposed model uses the supervised approach. 
CNN Channel 
Gabor-like Max-pooling 
kernel 
Layer 1 Conv-Kernel Max-pooling 
Layer n connected regression 
hidden classifier 
layer 
Figure 1. Proposed architecture for a Multi-channel Convolutional Neural Network using 2 channels. The upper channel is a standard CNN. consisting of 
convolutional layers and max-pooling layers. The lower channel has the same topology. but Its first layer weights are tramed as a Convolutional Autoencod er. 
B. Channel 2 -Convolutional Autoencoder 
The second channel in our proposed architecture consists of 
two convolutional layers with max pooling in between. The 
input to this channel is the same as for the first channel. How¬≠
ever, the first layer is trained as a Convolutional Autoencoder 
in an extra step, prior to supervised training of the whole 
network. This step is similar to pre-training found in other 
deep-learning methods [11]. However, we keep the weights 
of the first layer fixed during the supervised learning phase, 
whereas pre-training is usually followed by supervised fine¬≠
tuning. 
The hidden layer uses rectified linear units as well and 
activations in the i-th feature map are given by: 
Yi = max (x * Wi + bi, 0) (2) 
Reconstructions are computed as linear combinations of Y i and 
use tied weights (W denotes the vertically and horizontally 
flipped version of W): 
(3) 
A CAE with k feature maps is over-complete by a factor 
of roughly kl. In order to prevent the network from learning 
trivial solutions like identity functions, the capacity of the 
hidden layer must be regularized during training. We impose 
a very strong sparsity prior by regularizing the network using 
a Winner-Take-All (WTA) method, as described by Makhzani 
and Frey [25]. For each feature map, the maximum value is 
determined after applying the rectified linear function. This 
value is retained, but all other values in the feature map are 
set to zero. This can be thought of as multiplying each feature 
map with a different mask that leaves only one value intact 
before computing the reconstructions. 
This approach is very efficient, especially in comparison 
with methods like contractive regularization [26]. Since the 
WTA approach avoids computing additional gradients (or 
1 The exact size of a feature maps depends on the border treatment of the 
convolution. Figure 2. Typical filters learned by our Convolutional Autoencoder with 
Winner- Take-All regularization. 
Jacobian matrices) and relies only on determining the max¬≠
imum in each feature map. We also found this method to be 
highly effective at learning sparse codes while not introducing 
more hyper-parameters like trade-off weights in the case of 
contractive regularization or many other regularization tech¬≠
niques. WTA regularization is only applied during training 
and dropped after the network is finalized and the weights 
are frozen. Otherwise, the hidden layer would contain only 
a single non-zero value per feature map and be incapable 
of properly transferring information about the whole image 
towards the next layer. 
Makhzani and Frey [25] state that using the same weights 
for the hidden layer as well as for the reconstruction (tied 
weights) hurts generalization. However, we use tied weights 
as shown in eqs. (2) and (3) and observe no such problems 
in any of our experiments. On the other hand, having two 
distinct weight matrices (or tensors in the case of convolutional 
networks) doubles the number of model parameters and makes 
training more difficult and more time consuming, which is why 
we decided to use tied weights in the first place. 
The CAE is trained on the Kyoto natural images dataset [21] 
using a fixed kernel of 11 x 11 pixels. Each image is pre¬≠
processed by convolving it with a Difference of Gaussian 
(DoG) filter. DoG filters are effectively similar to ZCA whiten¬≠
ing [27], without the need to learn the filter kernels first. 
Furthermore, the shape of DoG filters is a good approximation 
of ideal decorrelation filters for grayscale images [28]. Whiten¬≠
ing enables our CAE to learn Gabor-like filters, otherwise 
filters would look more like principle components [29]. In 
Neutral (NE) Happiness (HA) Sadness (SA) Surprise (SU) 
Anger (AN) Disgust (01) Fear (FE) 
Figure 3. Samples of all seven classes from the JAFFE dataset [22]. The 
two-letter codes in parentheses are used throughout this paper to refer to the 
class. Each sample shows a different subject as well, out of the ten subjects 
in the dataset. 
comparison to ZCA whitening, using DoG filters in the context 
of convolutional networks also has the advantage that they can 
be applied directly to arbitrarily-sized images. The number 
of features is not set a-priori but determined experimentally. 
Details are given in section III. Figure 2 shows 60 typical 
Gabor-like filters learned by our CAE. 
III. EXPERIMENTS 
We evaluate our architecture for facial expression recogni¬≠
tion on the well established JAFFE dataset [22]. The dataset 
consists of 213 images, showing ten Japanese women per¬≠
forming seven different facial expressions (neutral, happiness, 
sadness, surprise, anger, disgust, and fear). For each com¬≠
bination of subject and facial expression, there are between 
two and four examples present in the dataset. All images have 
an equal resolution of 256 x 256 pixels and are in grayscale. 
We usually refer to the seven classes using common two-letter 
codes (i.e. NE for neutral) as shown in fig. 3. 
Even though the JAFFE dataset was first published in 
1998 [22], it is still relevant today and used as a benchmark for 
facial expression recognition throughout the literature [30]¬≠
[32]. It is a challenging dataset because there are only few 
examples per subject / expression, even more so if a separate 
testing set is excluded from training. Furthermore, the fact 
that every subject performs every expression allows for inter¬≠
esting experiments like for example the leave-one-out scheme 
described later in this section. 
The performance is analyzed with two different experiments 
following the methodology of Subramanian et al. [32]. In the 
first experiment, a leave-one-out scheme is used to split the 
dataset into training and testing subsets. One sample from each 
subject / expression combination is randomly taken out and 
used for testing. In total, there are 143 training samples and 
70 samples for testing [32]. This process is repeated 15 times 
and performance measures are averaged. 
In the second experiment, we use ten-fold cross validation 
to determine the training and testing sets. The whole dataset 
is randomly split into ten almost equally sized subsets of 
about 21 samples each. Each subset is then used once for 
testing while the remaining nine subsets compose the training Table I 
CLASSIFICATION FI-SCORES, STANDARD DEVIATIONS AND TRAINING 
TIME FOR FIVE DIFFERENT NUMBERS OF FEATURE MAPS IN THE CAE 
LAYER. THE TIME MEASURES A COMPLETE RUN OF TEN-FOLD CROSS 
VALIDATION. 
Feature maps 20 40 60 80 100 
Mean FI -score 89.8% 91.2% 91.8% 93.7% 90.4% 
Std. dev. 4.3% 9% 6.3% 2.6% 3.9% 
Time (Minutes) 16.4 25 41.1 58.4 85.4 
data. Finally, the perfonnance measures of all ten splits are 
averaged. As with the first experiment, this process is repeated 
15 times. 
As mentioned in section II, we optimize the number of 
feature maps in the CAE and both first layers in the 3-channel 
Sobel-based network. In both cases, we explore a range from 
20 to 100 feature maps, in steps of 20. For this parameter 
exploration we use the same ten-fold cross validation as it is 
used in the second main experiment. For each parameter, the 
average Fl -score over all classes is detennined, of which in 
turn the mean is computed after running all ten partitions in 
the cross validation. 
Table I shows average Fl -scores for the explored range as 
well as standard deviations. The general trend is an increase 
in performance as more feature maps are used, which is not 
surprising. However, the Fl -score drops to 90.4% on 100 
feature maps, which might indicate that the large amount of 
model parameters causes training to be more difficult. Even as 
low as 20 feature maps result in a classifier that achieves just 
short of 90% Fl -score. This is a very good result, considering 
that training a corresponding network takes less than two 
minutes2. 
For our Sobel-based network, the results are shown in 
table II. Interestingly, a network that contains only 20 feature 
maps in its first layers performs better than all other parameters 
we tested. It is also noteworthy, that this particular network 
performs better than our equivalent CAE-based network (com¬≠
pare table I), which is not true for any of the other parameter 
values. In general, it seems the Sobel-based architecture has 
difficulties during training when too many feature maps and 
consequently too many model parameters are present. 
In order to determine the network parameters for our final 
experiments, we are not just taking into account the Fl -scores 
of the presented ten-fold cross validation, but also the time 
it takes to train a network. The goal is to find a suitable 
compromise between both criteria. Considering the training 
time is important here, because both main experiments are 
repeated 15 times for each network architecture (CAE-and 
Sobel-based). Tables I and II show that the training time 
depends roughly linear on the number of features and ranges 
from as low as 16.4 minutes to more than 2 hours. Based on 
these considerations as well as the F1-scores discussed before, 
2This value corresponds to training a single network, not ten-fold cross 
validation, and excludes the time it took to train the CAE. 
Table II 
CLASSIFICATION FI-SCORES, STANDARD DEVIATIONS AND TRAINING 
TIME FOR FIVE DIFFERENT NUMBERS OF FEATURE MAPS IN BOTH SOBEL 
LAYERS. THE TIME MEASURES A COMPLETE RUN OF TEN-FOLD CROSS 
VALIDATION. 
Feature maps 20 40 60 80 100 
Mean FI-score 93.4% 89.5% 88% 90.8% 88% 
Std. dey. 2% 5.8% 6% 5.8% 5.7% 
Time (Minutes) 2l.5 34.6 60.1 88.1 130.1 
we choose 40 feature maps for both architectures and all 
following experiments. Choosing a value on the lower half of 
the range allows us repeat our main experiments often, because 
they can be trained quickly. On the other hand, both models 
achieve good Fl -scores (about 90% and 91 %), which are well 
within the average of the considered range here. Choosing the 
same number of feature maps for both models also has the 
advantage of better comparability. 
Of course, our method requires determining several hyper¬≠
parameters. These include the number of layers in each 
channel, their number of feature maps and kernel sizes, as 
well as the size of the fully-connected layer that merges 
both channels. We concentrate our efforts here on the layer 
that is trained in an unsupervised fashion, mainly because 
the MCCNN architecture itself has already been explored in 
previous work [18]. The same parameter is optimized in our 
Sobel-based network in order to provide an equal ground for 
all following experiments. 
All experiments as well as the CAE training were performed 
on a desktop machine with an Intel Xeon E5630 processor, 
two Nvidia GeForce GTX 590 graphics cards and 24 GiB of 
RAM. Theano was used to accelerate computations on both 
GPUs [33], [34]. Training the CAE with 40 feature maps took 
about 15 minutes and comprised 20000 parameter updates. 
Training times for the whole network are given in Tables I 
and II. 
IV. RESULTS 
A. Experiment 1 
We first report the results of the leave-one-out experiment, 
averaged over 15 runs. Table III shows the summary of our 
results, as well as the results reported by Subramanian et al. 
[32]. For our experiment, an average accuracy of 95.8% was 
obtained with a standard deviation of 1.6. These results present 
an improvement of almost 8% when compared to the McFIS 
method. When compared with the architecture using Sobel¬≠
based filters, the results are almost 3% higher. While the CAE¬≠
based network consists of 2 channels, the Sobel-based one is 
implemented with 3 channels. The training and recognition 
time are improved as less parameters need to be updated, while 
at the same time, more diverse information is present. 
A combined confusion matrix is shown in table IV. Each 
cell in the matrix is computed as the average of the corre¬≠
sponding values of all confusion matrices. The low standard Table 1lI 
RESULTS AND COMPARISON FOR THE LEAVE-ONE-OUT EXPERIMENT. 
REPORTED ARE AVERAGE ACCURACIES AND STANDARD DEVIATIONS. 
Method Performance 
CAE-based 95.8 ¬± 1.6 
Sobel-based 93.1 ¬± l.6 
McFIS [32] 87.6 ¬± 5.79 
Table IV 
COMBINED CONFUSION MATRIX OF 15 LEAVE-ON E-OUT EXPERIMENTS. 
VALUES ARE GIVEN IN PERCENT AND DO NOT NECESSARILY ADD UP TO 
100% DUE TO ROUNDING. 
Actual class 
AN DI FE HA NE SA SU 
AN 94 0 2.7 0 0.7 0 0 
"" DI 0 97.3 2.7 0 4.7 0 0 
.S: FE 0 0.7 90.7 0 0 0 0 t) 
'6 HA 0 0 0 99.3 1.3 0 0 e 
0-NE 0 2 0 0 93.3 0 4 
SA 6 0 0 0 0 100 0 
SU 0 0 4 0.7 0 0 96 
deviations are visible here as well, as most values are either 
zero or quite low. Especially noticeable are the results for the 
class fear (FE). The JAFFE dataset authors describe fear as 
being difficult to express for Japanese people and consequently 
exclude it from several experiments. This is consistent with our 
results here, as fear has the lowest accuracy (90.7%) among 
all classes. 
In fig. 4 we show the best and worst confusion matrices 
based on the accuracy. In the best case we achieved an 
accuracy of 98.6%. In this particular experiment, only a single 
sample of the training set was misclassified (neutral instead of 
disgust). Subramanian et al. [32] report a best result of 94.5% 
accuracy which is about 4% lower than ours. The right image 
in fig. 4 shows the worst single experiment in which we still 
achieved 92.9% accuracy. Here we can see that three classes 
(happiness, sadness and surprise) were classified perfectly. All 
other classes have only minor misclassification (at most 2 out 
of 10). 
B. Experiment 2 
The second experiment uses ten-fold cross validation and 
is repeated for 15 runs as well. A summary of the results is 
shown in table V. The same table also shows results from the 
literature for comparison. We achieve an accuracy of 94.1 % 
with a standard deviation of 4.3. Our CAE-based MCCNN 
architecture achieves more than 5% better accuracy than the 
McFIS method [32] and is on average slightly better the 
Linear Programming method presented by Feng et al. [35]. 
The improvement over the Sobel-based network is not as 
high (2.1 %) but still consistent. It should be noted, that the 
CAE-based network has one channel less than the Sobel-based 
Actual class Actual class 
AN DI FE HA NE SA SU AN DI FE HA NE SA SU 
10 10 
SU SU 
SA 8 SA 8 
:: NE :: NE 
.S: 6 .S: 6 
U HA U HA 'i5 'i5 
¬£ FE 4 ¬£ FE 4 
DI 2 DI 2 
AN AN 
0 0 
(a) Best confusion matrix (b) Worst confusion matrix 
Figure 4. Best (a) and worst (b) confusion matrices based on accuracies of a single leave-one-out experiment. 
Table V 
RESULTS AND COMPARISON FOR THE TEN-FOLD CROSS VALIDATION 
EXPERIMENT. REPORTED ARE AVERAGE ACCURACIES AND STANDARD 
DEVIATIONS. 
Method 
CAE-based 
Sobel-based 
McFIS [32] 
Linear Programming [35] Performance 
94.1 ¬± 4.3 
92 ¬± 6.1 
89.05 ¬± 3.214 
93.8 
network, whereas the remaining topology is identical. It might 
thus be concluded that the proposed method is less expressive. 
However as the experiments show, the Gabor-like filters in our 
network support classification very well. 
Interestingly, the accuracy here is lower than for the leave¬≠
one-out experiment reported earlier. At the same time, the 
standard deviation is much higher (4.3% vs. 1.6%). We believe 
this is a result of ambiguities in the dataset [22] and the 
small (10%) testing set used here, compared to the previous 
experiment (about 33%). There is a higher chance that a certain 
class is represented by only ambiguous samples, which might 
account for the higher standard deviation as well as a slightly 
lower average accuracy. 
In table VI, we show a combined confusion matrix of all 
ten-fold cross validation experiments. Compared to the first 
experiment (table IV), the higher standard deviation is visible 
as many more cells are now greater than zero. However, many 
cells outside the main diagonal are just slightly above zero 
which means that most of the time very few sample were rnis¬≠
classified. The results are consistent with the previous ones re¬≠
gardingf ear, which shows the lowest accuracy again (85.9%). 
Judging by both experiments (tables IV and VI),jear is mostly 
confused with anger, disgust and surprise. 
V. CONCLUSI ON 
In this paper, we presented a variant of the Multi-Channel 
Convolutional Neural Network. The architecture is character-Table VI 
COMBINED CONFUSION MATRIX OF THE TEN-FOLD CROSS VALIDATION 
EXPERIMENTS. VALUES ARE GIVEN IN PERCENT AND DO NOT 
NECESSARILY ADD UP TO 100% DUE TO ROUNDING. 
Actual class 
AN DI FE HA NE SA SU 
AN 90.9 0.8 3 0 1.4 2.5 0 
:: DI 0 89.6 4.3 0.8 4.9 0 0 
0 
'B FE 1.1 4.8 85.9 0 0 0 1.7 
'i5 HA 0 2.4 0.5 96.3 3.7 0 0.9 ÔøΩ 
c... NE 0 2.1 0.8 0 88 0 4.3 
SA 8.1 0.3 0.5 0.5 0 97.5 0 
SU 0 0 5 2.4 2 0 93.1 
ized by multiple CNNs (channels), that first process infor¬≠
mation independently. All streams merge in a fully-connected 
layer, which is used for classification. Compared to previous 
work [18], hard-coded Sobel filters are replaced by a single 
channel, making the proposed model a 2-channel architecture. 
The weights of this new channel are trained as Convolutional 
Autoencoder, which usually results in Gabor-like filters when 
trained on whitened natural images. This model is motivated 
by two observations: First, using additional channels with 
input from edge detectors (Sobel) proved useful in previous 
work. Our Gabor-like filters provide qualitatively the same 
information, but in a much more diverse scope, because 
the CAE has 40 different feature maps. Second, the effort 
it takes to fully train a CAE-based network is lower than 
the previous Sobel-based network. Having one channel less 
certainly contributes to this, but also the fact that the CAE 
layer is reusable. The time it takes to train a CAE can easily 
be amortized by using it multiple times. 
Evaluation of our model was done using the JAFFE dataset 
and we followed the methodology of two experiments done 
by Subramanian et al. [32]. Both experiments were performed 
on the proposed model as well as the Sobel-based model 
Figure 5. Visualizations of four first-layer filters of the CNN channel. 
for comparison. The experiments and results show, that the 
proposed model is a very viable method for facial expression 
recognition. It does not depend on any hand-crafted or task¬≠
specific feature extraction but exploits unsupervised learning. 
We easily reach state-of-the-art recognition rates with minimal 
effort. In the leave-one-out experiment, we achieve an average 
of 95.8% accuracy, which is 8.2% higher than what Subrama¬≠
nian et al. [32] reported. The ten-fold cross validation leads 
to an average accuracy of 94.1 %. 
The improvements over our previous Sobel-based model 
range between two and three percent. Thus, our new model 
does not only perform better in terms of classification perfor¬≠
mance but is also faster to train, because it uses one channel 
less. However, the first layer in the CAE channel has to be 
taken into account as well, when considering training time, 
because it is trained in a separate step and the CAE training 
effort pays off the more often it is reused. 
For future work, we would like to investigate what exact 
role each channel plays during the implicit feature extraction 
task. The CAE channel for example already contains diverse 
information about the existence of edges. It seems intuitive 
to assume, that the first channel (which is trained fully super¬≠
vised) would not take on the same filter shapes. In this sense, it 
is still unclear in what way both channels influence each other 
and whether they extract complementary information. Another 
possibility might be that both channels learn qualitatively the 
same information and act as weak classifiers. In this case, the 
fully-connected layer at the end might act as a boosting layer. 
In some very preliminary experiments to understand better 
what our architecture learns, we visualized the first layer filters 
of the channel, which is trained fully supervised (see fig. 5). 
On first sight and especially in comparison to the CAE filters 
(compare Figure 2), the filter do not seem to have meaningful 
structure after learning. On the other hand, the filter do not 
just contain white noise. A possibly better approach, that we 
want to pursue in the future, might be to visualize not only 
first layer filters but also those in other layers. For example, 
the deconvolutional networks used by Zeiler and Fergus [36] 
could give valuable insights into our networks. However, 
even without having analyzed the relationship between both 
channels yet, our model achieves state-of-the-art recognition 
rates for face expressions and is very fast to train. 
ACKNOW LEDGEMENTS 
This work was partially supported by the State Graduate 
Funding Program at the University of Hamburg and by CAPES 
Brazilian Federal Agency for the Support and Evaluation of 
Graduate Education (p.n.5951-13-5). 
The authors thank Dr. Cornelius Weber and Katja Kosters 
for their valuable suggestions when writing this paper. REFERENCES 
[1] S. Morishima and H. Harashima. ""Facial expression synthesis based 
on natural voice for virtual face-to- face communication with machine"". 
in, 1993 IEEE Virtual Reality Annual International Symposium, 1993. 
Sep. 1993. pp. 486-491. 
[2] A. Colmenarez. B. Frey. and T. Huang. ""A probabilistic framework for 
embedded face and facial expression recognition"", in Computer Vision 
and Pattern Recognition, 1999. IEEE Computer Society Conference 
on .‚Ä¢ vol. 1. 1999. 
[3] B. Chu. S. Romdhani. and L. Chen. ""3D-Aided Face Recognition 
Robust to Expression and Pose Variations"". in 2014 IEEE Conference 
on Computer Vision and Pattern Recognition (CVPR). Jun. 2014. 
pp. 1907-1914. 
[4] M. Cabanac. ""What is emotion?"". Behavioural Processes. vol. 60. no. 
2. pp. 69-83. Nov. 2002. 
[5] P. Ekman and W. V. Friesen, ""Constants across cultures in the face and 
emotion"". 1. Pers. Soc. Psycho!., vol. 17. no. 2, pp. 124-129. 1971. 
[6] Z. Guo-Feng, W. Ke-Jun, Y Lei, and F. Gui-Xia. ""New research 
advances in facial expression recognition"", in Control and Decision 
Conference (CCDC), 2013 25th Chinese. May 2013, pp. 3403-3409. 
[7] 1. Nagi, F. Ducatelle. G. Di Caro, D. Ciresan, U. Meier, A. Giusti. 
F. Nagi. J. Schmidhuber. and L. Gambardella. ""Max-pooling convolu¬≠
tional neural networks for vision-based hand gesture recognition"", in 
20ll IEEE International Conference on Signal and Image Processing 
Applications (ICSIPA). Nov. 2011, pp. 342-347. 
[8] S. Ji. W. Xu, M. Yang, and K. Yu. ""3D Convolutional Neural Networks 
for Human Action Recognition"", IEEE Trans. Pattern Anal. Mach. 
Intell., vol. 35, no. 1. pp. 221-231, Jan. 2013. 
[9] D. Ciresan, U. Meier, and J. Schmidhuber. ""Multi-column deep 
neural networks for image classification"", in 2012 IEEE Conference 
on Computer Vision and Pattern Recognition (CVPR). Jun. 2012. 
pp. 3642-3649. 
[10] Y Bengio, A. Courville, and P. Vincent, ""Representation Learning: 
A Review and New Perspectives"". IEEE Trans. Pattern Anal. Mach. 
Intell., vol. 35. no. 8, pp. 1798-1828. Aug. 2013. 
[11] G. Hinton, S. Osindero, and Y Teh, ""A Fast Learning Algorithm for 
Deep Belief Nets"", Neural Comput .‚Ä¢ vol. 18. no. 7, pp. 1527-1554. 
Jul. 2006. 
[12] Y LeCun, L. Bottou, Y Bengio, and P. Haffner, ""Gradient-based 
learning applied to document recognition"". Proc. IEEE. vol. 86, no. 
11. pp. 2278-2324, Nov. 1998. 
[l3] S. Lawrence. C. Giles. A. C. Tsoi. and A. Back. ""Face recognition: 
A convolutional neural-network approach"". IEEE Trans. Neural Netw., 
vol. 8, no. 1, pp. 98-113, Jan. 1997. 
[l4] M. Khalil-Hani and L. S. Sung. ""A convolutional neural network ap¬≠
proach for face verification"". in 2014 International Conference on High 
Petformance Computing Simulation (HPCS), Jul. 2014, pp. 707-714. 
[l5] T. P. Kamowski. l. Are\, and D. Rose. ""Deep Spatiotemporal Fea¬≠
ture Learning with Application to Image Classification"". in 2010 
Ninth International Conference on Machine Learning and Applications 
(ICMLA). Dec. 2010, pp. 883-888. 
[16] S. E. Kahou, C. Pal. X. Bouthillier. P. Froumenty. <;. Gtil<;:ehre, R. 
Memisevic. P. Vincent, A. Courville. Y Bengio. R. C. Ferrari. M. 
Mirza, S. Jean. P.-L. Carrier. Y Dauphin. N. Boulanger-Lewandowski. 
A. Aggarwal, J. Zumer, P. Lamblin, J.-P. Raymond, G. Desjardins. R. 
Pascanu, D. Warde-Farley. A. Torabi. A. Sharma. E. Bengio. M. Cote. 
K. R. Konda, and Z. Wu, ""Combining Modality Specific Deep Neural 
Networks for Emotion Recognition in Video"", in Proceedings of the 
15th ACM on International Conference on Multimodal Interaction. ser. 
ICMI '13, New York, NY, USA: ACM. 2013. pp. 543-550. 
[17] A. Dhall. R. Goecke, S. Lucey. and T. Gedeon, ""Collecting Large, 
Richly Annotated Facial-Expression Databases from Movies"", IEEE 
Multimedia. vol. 19, no. 3, pp. 34-41, 2012. 
[18] P. Barros, S. Magg, C. Weber. and S. Wermter. ""A Multichannel 
Convolutional Neural Network for Hand Posture Recognition"". in 
Artificial Neural Networks and Machine Learning -ICANN 2014, 
S. Wermter, C. Weber, W. Duch, T. Honkela, P. Koprinkova-Hristova. 
S. Magg, G. Palm, and A. E. P. Villa. Eds., ser. Lecture Notes in 
Computer Science 8681. Springer International Publishing. Sep. 15, 
2014, pp. 403-410. 
[l9] M. Ranzato. F. J. Huang, Y-L. Boureau, and Y LeCun, ""Unsupervised 
Learning of Invariant Feature Hierarchies with Applications to Object 
Recognition"", in IEEE Conference on Computer Vision and Pattern 
Recognition, 2007. CVPR '07, Jun. 2007, pp. 1-8. 
[20] M. Ranzato, R. Monga, M. Devin, K. Chen, G. Corrado, J. Dean, 
Q. Y. Le, and A. Y. Ng, ""Building high-level features using large 
scale unsupervised learning"", presented at the Proceedings of the 
29th International Conference on Machine Learning (ICML-12), 2012, 
pp. 81-88. 
[21] E. Doi, T Inui, T-W. Lee, T Wachtler, and T J. Sejnowski, ""Spa¬≠
tiochromatic Receptive Field Properties Derived from Information¬≠
Theoretic Analyses of Cone Mosaic Responses to Natural Scenes"", 
Neural Computation, vol. 15, no. 2, pp. 397-417, Feb. 1,2003. 
[22] M. Lyons, S. Akamatsu, M. Kamachi, and 1. Gyoba, ""Coding facial 
expressions with Gabor wavelets"", in Third IEEE International Confer¬≠
ence on Automatic Face and Gesture Recognition, 1998. Proceedings, 
Apr. 1998, pp. 200-205. 
[23] P. Barros, G. I. Parisi, D. Jirak, and S. Wermter, ""Real-time Gesture 
Recognition Using a Humanoid Robot with a Deep Neural Archi¬≠
tecture"", in Humanoid Robots (Humanoids), 2014 14th IEEE-RAS 
International Coriference on, Nov. 2014, In Press. 
[24] X. Glorot, A. Bordes, and Y. Bengio, ""Deep Sparse Rectifier Neural 
Networks"", presented at the International Conference on Artificial 
Intelligence and Statistics, 2011, pp. 315-323. 
[25] A. Makhzani and B. Frey, ""A Winner-Take-All Method for Training 
Sparse Convolutional Autoencoders"", ARXlVJ4092752 Cs, Sep. 9, 
2014, arXiv: 1409.2752. 
[26] S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio, ""Contrac¬≠
tive Auto-Encoders: Explicit Invariance During Feature Extraction"", 
presented at the Proceedings of the 28th International Conference on 
Machine Learning (lCML-II), 2011, pp. 833-840. 
[27] A. J. Bell and T J. Sejnowski, ""The ""independent components"" of 
natural scenes are edge filters"", Vision Research, vol. 37, no. 23, 
pp. 3327-3338, Dec. 1997. 
[28] M. Brown, S. Susstrunk, and P. Fua, ""Spatio-chromatic decorrela¬≠
tion by shift-invariant filtering"", in 2011 IEEE Computer Society 
Conference on Computer Vision and Pattern Recognition Workshops 
(CVPRW), Jun. 2011, pp. 27-34. [29] A. Coates, A. Y. Ng, and H. Lee, ""An Analysis of Single-Layer 
Networks in Unsupervised Feature Learning"", presented at the In¬≠
ternational Conference on Artificial Intelligence and Statistics, 2011, 
pp. 215-223. 
[30] F. Cheng, J. Yu, and H. Xiong, ""Facial Expression Recognition in 
JAFFE Dataset Based on Gaussian Process Classification"", IEEE 
Trans. Neural Netw., vol. 21, no. 10, pp. 1685-1690, Oct. 2010. 
[31] S. Liu, Y. Zhang, and K. Liu, ""Facial expression recognition under 
random block occlusion based on maximum likelihood estimation 
sparse representation"", in 2014 International foint Conference on 
Neural Networks (]JCNN), Jul. 2014, pp. 1285-1290. 
[32] K. Subramanian, S. Suresh, and R. Venkatesh Babu, ""Meta-Cognitive 
Neuro-Fuzzy Inference System for human emotion recognition"", in The 
20I2International foint Conference on Neural Networks (]JCNN), Jun. 
2012, pp. 1-7. 
[33] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. 
Desjardins, J. Turian, D. Warde-Farley, and Y. Bengio, ""Theano: A 
CPU and GPU Math Compiler in Python"", presented at the Proceedings 
of the 9th Python in Science Conference, 2010, pp. 3-10. 
[34] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, l. Goodfellow, A. 
Bergeron, N. Bouchard, D. Warde-Farley, and Y. Bengio, ""Theano: 
New features and speed improvements"", ARXIVI2JJ5590 Cs, Nov. 23, 
2012, arXiv: 1211.5590. 
[35] X. Feng, M. Pietikiiinen, and A. Hadid, ""Facial expression recognition 
with local binary patterns and linear programming"", Pattern Recognit. 
Image Anal., vol. IS, no. 2, pp. 546-548, 2005. 
[36] M. D. Zeiler and R. Fergus, ""Visualizing and Understanding Con¬≠
volutional Networks"", in Computer Vision -ECCV 2014, D. Reet, 
T Pajdla, B. Schiele, and T Tuytelaars, Eds., ser. Lecture Notes in 
Computer Science 8689. Springer International Publishing, Jan. 1, 
2014, pp. 818-833. 
"
https://ieeexplore.ieee.org/document/1176132,"IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 14, NO. 1, JANUARY 2003 117
Face Recognition Using Kernel Direct Discriminant
Analysis Algorithms
Juwei Lu , Student Member, IEEE , Konstantinos N. Plataniotis , Member, IEEE , and
Anastasios N. Venetsanopoulos , Fellow, IEEE
Abstract‚Äî Techniques that can introduce low-dimensional
feature representation with enhanced discriminatory power is ofparamount importance in face recognition (FR) systems. It is wellknown that the distribution of face images, under a perceivablevariation in viewpoint, illumination or facial expression, is highlynonlinear and complex. It is, therefore, not surprising that lineartechniques, such as those based on principle component analysis(PCA) or linear discriminant analysis (LDA), cannot providereliable and robust solutions to those FR problems with complexface variations. In this paper, we propose a kernel machine-based
discriminant analysis method, which deals with the nonlinearity
of the face patterns‚Äô distribution. The proposed method alsoeffectively solves the so-called ‚Äúsmall sample size‚Äù (SSS) problem,which exists in most FR tasks. The new algorithm has beentested, in terms of classification error rate performance, onthe multiview UMIST face database. Results indicate that theproposed methodology is able to achieve excellent performancewithonlyaverysmallsetoffeaturesbeingused,anditserrorrateis approximately 34% and 48% of those of two other commonlyused kernel FR approaches, the kernel-PCA (KPCA) and thegeneralized discriminant analysis (GDA), respectively.
IndexTerms‚Äî Facerecognition(FR),kerneldirectdiscriminant
analysis (KDDA), linear discriminant analysis (LDA), principle
component analysis (PCA), small sample size problem (SSS),
kernel methods.
I. INTRODUCTION
WITHIN the last decade, face recognition (FR) has found
a wide range of applications, from identity authentica-
tion, access control, and face-based video indexing/browsing,
tohuman-computerinteraction/communication.Asaresult,nu-
merous FR algorithms have been proposed, and surveys in thisarea can be found in [1]‚Äì[5]. Two issues are central to all thesealgorithms: 1) feature selection for face representation and 2)
classification of a new face image based on the chosen feature
representation[6].Thisworkfocusesontheissueoffeaturese-lection. The main objective is to find techniques that can in-
troduce low-dimensional feature representation of face objects
with enhanced discriminatory power. Among various solutionstotheproblem,themostsuccessfularethoseappearance-basedapproaches, which generally operate directly on images or ap-
pearancesoffaceobjectsandprocesstheimagesastwo-dimen-
sional (2-D) holistic patterns, to avoid difficulties associatedwiththree-dimensional(3-D)modeling,andshapeorlandmarkdetection [5].
Manuscript received December 12, 2001; revised July 1, 2002.
The authors are with the Multimedia Laboratory, Edward S. Rogers, Sr.
Department of Electrical and Computer Engineering, University of Toronto,
Toronto, ON M5S 3G4, Canada (e-mail: kostas@dsp.toronto.edu).
Digital Object Identifier 10.1109/TNN.2002.806629Principle component analysis (PCA) and linear discrimi-
nant analysis (LDA) are two classic tools widely used in the
appearance-based approaches for data reduction and feature
extraction. Many state-of-the-art FR methods, such as Eigen-faces [7] and Fisherfaces [8], are built on these two techniquesor their variants. It is generally believed that when it comes
to solving problems of pattern classification, LDA-based
algorithms outperform PCA-based ones, since the formeroptimizes the low-dimensional representation of the objectswith focus on the most discriminant feature extraction while
the latter achieves simply object reconstruction. However,
many LDA-based algorithms suffer from the so-called ‚Äú small
sample size problem ‚Äù (SSS) which exists in high-dimensional
pattern recognition tasks, where the number of available
samples is smaller than the dimensionality of the samples.
The traditional solution to the SSS problem is to utilize PCAconcepts in conjunction with LDA (PCA
LDA), as it was
done for example in Fisherfaces [8]. Recently, more effective
solutions, called direct LDA (D-LDA) methods, have been
presented [9], [10]. Although successful in many cases, linearmethods fail to deliver good performance when face patternsare subject to large variations in viewpoints, which results
in a highly nonconvex and complex distribution. The limited
success of these methods should be attributed to their linearnature [11]. As a result, it is reasonable to assume that a bettersolution to this inherent nonlinear problem could be achieved
using nonlinear methods, such as the so-called kernel machine
techniques [12]‚Äì[15].
Inthispaper,motivatedbythesuccessthatsupportvectorma-
chines (SVMs) [16]‚Äì[18], kernel PCA (KPCA) [19] and gen-
eralized discriminant analysis (GDA) [20] have in pattern re-
gression and classification tasks, we propose a new kernel dis-criminantanalysisalgorithmforfacerecognition.Thealgorithmgeneralizes the strengths of the recently presented D-LDA and
the kernel techniques while at the same time overcomes many
of their shortcomings and limitations. Therefore, the proposedalgorithm can be seen as an enhanced kernel D-LDA method(hereafterKDDA).FollowingtheSVMparadigm,wefirstnon-
linearlymaptheoriginalinputspacetoanimplicithigh-dimen-
sional feature space, where the distribution of face patterns ishoped to be linearized and simplified. Then, a new variant ofthe D-LDA method is introduced to effectively solve the SSS
problem and derive a set of optimal discriminant basis vectors
in the feature space.
The rest of this paper is organized as follows. Since KDDA
is built on D-LDA and GDA, in Section II, we start the anal-
ysisbybrieflyreviewingthetwolattermethods.Followingthat,
1045-9227/03$17.00 ¬© 2003 IEEE
118 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 14, NO. 1, JANUARY 2003
KDDA is introduced and analyzed. The relationship of KDDA
to D-LDA and GDA is also discussed. In Section III, two setsofexperimentsarepresentedtodemonstratetheeffectivenessof
the KDDA algorithm on highly nonlinear highly complex face
patterndistributions.KDDAiscompared,intermsoftheclassi-ficationerrorrateperformance,toKPCAandGDAonthemul-tiview UMIST face database. Conclusions are summarized in
Section IV.
II. M
ETHODS
Theproblemtobesolvedisformallystatedasfollows:Aset
oftraining face images is available. Each image is
definedasavectoroflength ,i.e., ,where
isthefaceimagesizeand denotesa -dimensional
realspace.Itisfurtherassumedthateachimagebelongstooneof
classes . The objective is to find a transformation
, based on optimization of certain separability criteria, which
producesamapping ,with thatleadstoan
enhanced separability of different face objects.
A. GDA
For solving nonlinear problems, the classic LDA has been
generalizedtoitskernelversion,namelyGDA[20].Let
be a nonlinear mapping from the input space
to a high-dimensional feature space , where different classes
ofobjectsaresupposedtobelinearlyseparable.Theideabehind
GDAistoperformaclassicLDAinthefeaturespace instead
of the input space .
Let and be the between- and within-class
scatter matrices in the feature space , respectively, expressed
as follows:
(1)
(2)
where
 is the mean
of class
 is the average of the
ensemble, and is the element number in , which leads
to
 . LDA determines a set of optimal discrim-
inant basis vectors, denoted by , so that the ratio of
the between- and within-class scatters is maximized [21]. As-suming
, the maximization can be achieved
by solving the following eigenvalue problem:
(3)
The feature space could be considered as a ‚Äúlinearization
space‚Äù [22], however, its dimensionality could be arbitrarilylarge, and possibly infinite. Fortunately, the exact
is not
needed and the feature space can become implicit by using
kernel methods, where dot products in are replaced with a
kernel function in the input space so that the nonlinear
mapping is performed implicitly in [23], [24].In FR tasks, the number of training samples, , is in most
casesmuchsmallerthanthedimensionalityof (forLDA)or
(forGDA)leadingtoadegeneratedscattermatrix .Tra-
ditionalmethods,forexampleGDAandFisherfaces[8],attempt
tosolvetheso-calledSSSproblembyusingtechniquessuchaspseudoinverseorPCAtoremovethenullspaceof
.How-
ever,ithasbeenrecentlyshownthatthenullspacemaycontain
the most significant discriminant information [9], [10].
B. Direct LDA (D-LDA)
Recently, Chen et al.[9] and Yang et al.[10] proposed the
so-calleddirectLDA(D-LDA)algorithmthatattemptstoavoidthe shortcomings existing in traditional solutions to the SSS
problem. The basic idea behind the algorithm is that the null
space of
may contain significant discriminant informa-
tion if the projection of is not zero in that direction, and
that no significant information will be lost if the null space of
isdiscarded.Assuming,forexample,that andrepre-
sentthenullspacesof and ,respectively,thecom-
plement spaces of andcan be written as
and . Therefore, the optimal discriminant sub-
space sought by the D-LDA algorithm is the intersection space
.
The difference between Chen‚Äôs method [9] and Yang‚Äôs
method [10] is that Yang‚Äôs method first diagonalizes
to find when seek solution of (3), while Chen‚Äôs method
first diagonalizes to find . Although there is no
significant difference between the two approaches, it may beintractable to calculate
when the size of is large,
which is the case in most FR applications. For example,
the size of and amounts to 10304 10304
for face images of size 112 92 such as those used in our
experiments. Fortunately, the rank of is determined by
, withthe number of image
classes,usuallyasmallvalueinmostofFRtasks,e.g.,
in our experiments, resulting in .can be
easily foundby solving eigenvectorsof a19 19 matrix rather
than the original 10304 10304 matrix through the algebraic
transformationproposedin[7].Theintersectionspace
can be obtained by solving the null space of projection of
into, with the projection being a small matrix of
size 19 19. For the reasons explained above, we proceed by
first diagonalizing the matrix instead of in the
derivation of the proposed here algorithm.
C. KDDA
1) Eigen-Analysis of in the Feature
Space:Following the general D-LDA framework, we start
by solving the eigenvalue problem of , which can be
rewritten here as follows:
(4)
LUet al.: FACE RECOGNITION USING KERNEL DIRECT DISCRIMINANT ANALYSIS ALGORITHMS 119
where , and . Since
thedimensionalityofthefeaturespace ,denotedas ,could
bearbitrarilylargeorpossiblyinfinite,itisintractabletodirectlycomputetheeigenvectorsofthe
matrix .For-
tunately, the first most significanteigenvectors of
, which correspond to nonzero eigenvalues, can be indi-
rectly derived from the eigenvectors of the matrix (with
size ) [7].
Computing , requires dot product evaluation in .
This can be done in a manner similar to the one used in
SVM, KPCA, and GDA by utilizing kernel methods. For any
,weassumethatthereexistsakernelfunction
such that . The introduction of
the kernel function allows us to avoid the explicit evaluation of
the mapping. Any function satisfying Mercer‚Äôs condition canbe used as a kernel, and typical kernel functions include poly-nomial function, radial basis function (RBF) and multilayer
perceptrons [17].
Using the kernel function, for two arbitrary classes
and
,a dot product matrix can be defined as
where
(5)
For all of classes , we then define a kernel
matrix
(6)
which allows us to express as follows:
(7)
where
 ,isa matrixwith
terms all equal to: one, is a
blockdiagonalmatrix,and isa vectorwithallterms
equal to: (see Appendix I for a detailed derivation of
(7).).
Letand , be the th eigenvalue and cor-
responding eigenvector of , sorted in decreasing order of
eigenvalues. Since is
the eigenvector of . In order to remove the null space
of , we only use its first eigenvectors:
where ,whose
corresponding eigenvalues are greater than 0. It is not difficult
to see that , with ,a
diagonal matrix.
2) Eigen-Analysis of in the Feature Space: Let
. Projecting and into the subspace
spannedby ,itcaneasilybeseenthat while
can be expanded as
(8)Using the kernel matrix , a closed-form expression of
can be obtained as follows:
(9)
withanddefined in Appendix II along with the detailed
derivation of the expression in (9).
We proceed by diagonalizing , a tractable
matrix with size . Let be theth eigenvector of
, where , sorted in increasing order
of the corresponding eigenvalue . In the set of ordered
eigenvectors, those that correspond to the smallest eigenvalues
maximize the ratio in (3), and should be considered the mostdiscriminative features. Discarding the eigenvectors with the
largest eigenvalues, the
selected eigenvectors are
denoted as . Defining a matrix ,w e
can obtain , with ,a
diagonal matrix.
Based on the calculations presented above, a set of op-
timal discriminant feature vectors can be derived through
. The features form a low-dimensional subspace
in,wheretheratioin(3)ismaximized.SimilartotheD-LDA
framework, the subspace obtained contains the intersection
space shown in Section II-B. However, it is possible
that there exist eigenvalues with in. To alleviate
the problem, threshold values were introduced in [10], whereany value below the threshold
is promoted to (a very
small value). Obviously, performance heavily depends on the
heuristic evaluation of the parameter .
To robustify the approach, we propose a modified Fisher‚Äôs
criterion to be used instead of the conventional definition in
(3) when is singular. The newcriterion can be ex-
pressed as
(10)
The modified Fisher‚Äôs criterion of (10) has been proved to be
equivalent to the conventional one (3) in [25]. The expression
which is used in (10) instead of the
can be shown to be nonsingular by the following
lemma.
Lemma 1: Suppose is a real matrix of size
 , and
can be represented by where is a real matrix of
size
 .Then, ispositivedefinite,i.e., ,
whereis a
 identity matrix.
Proof:Since isarealsymmetricmatrix.
For any
 nonzero real vector:
. According to [26], the
matrix that satisfies the above conditions is positive
definite.
Following a procedure similar to can
be expressed as , with
.Since and
satisfies the conditions on discussed in Lemma 1,
is positive definite. As a result,
is nonsingular.
120 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 14, NO. 1, JANUARY 2003
3) Dimensionality Reduction and Feature Extraction: For
anyinputpattern ,itsprojectionintothesetoffeaturevectors,
, derived in Section II-C2, can be calculated by
(11)
where . Since
(12)
we have
(13)
where isa
kernel vector.
Combining (11) and (13), we obtain
(14)
where
is a matrix which can be calculated of-
fline. Thus, through (14), a low-dimensional representation
onwith enhanced discriminant power, suitable for classifica-
tion tasks, has been introduced.
4) Comments: TheKDDAmethodimplementsanimproved
D-LDA in a high-dimensional feature space using a kernel ap-proach. Its main advantages can be summarized as follows.
1) KDDA introduces a nonlinear mapping from the input
space to an implicit high-dimensional feature space,
where the nonlinear and complex distribution of patternsintheinputspaceis‚Äúlinearized‚Äùand‚Äúsimplified‚Äùsothatconventional LDA can be applied. It is not difficult to
see that KDDA reduces to D-LDA for
. Thus,
D-LDA can be viewed as a special case of the proposedKDDA framework.
2) KDDAeffectivelysolvestheSSSprobleminthehigh-di-
mensional feature space by employing an improved
D-LDAalgorithm.UnliketheoriginalD-LDAmethodof[10], zero eigenvalues of the within-class scatter matrix
are never used as divisors in the improved one. In this
way, the optimal discriminant features can be exactlyextractedfrombothofinsideandoutsideof
‚Äôsnull
space.
3) In GDA, to remove the null space of , it is re-
quired to compute the pseudo inverse of the kernel ma-trix
, which could be extremely ill-conditioned when
certain kernels or kernel parameters are used. Pseudoin-
versionisbasedoninversionofthenonzeroeigenvalues.
Fig. 1. Some face samplesof one subject from the UMIST facedatabase.
Due to round-off errors, it is not easy to identify the truenull eigenvalues. As a result, numerical stability prob-
lems often occur [14]. However, it can been seen fromthe derivation of KDDA that such problems are avoidedin KDDA. The improvement can be observed also in ex-
perimental results reported in Figs. 4(a) and 5(a).
The detailed steps for implementing the KDDA method are
summarized in Fig. 6.
III. E
XPERIMENTAL RESULTS
Two sets of experiments are included in this paper to illus-
trate the effectiveness of the KDDA algorithm. In all experi-ments reported here, we utilize the UMIST face database [27],
[28],amultiviewdatabase,consistingof575gray-scaleimages
of 20 subjects, each covering a wide range of poses from pro-filetofrontalviewsaswellasrace,genderandappearance.Allinput images are resized into 112
92, a standardized image
size commonly used in FR tasks. The resulting standardized
input vectors are of dimensionality . Fig. 1 depicts
somesampleimagesofatypicalsubsetintheUMISTdatabase.
A. Distribution of Multiview Face Patterns
The distribution of face patterns is highly nonconvex and
complex,especiallywhenthepatternsaresubjecttolargevaria-tionsinviewpointsasisthecasewiththeUMISTdatabase.Thefirstexperimentaimstoprovideinsightson howtheKDDA al-
gorithmlinearizes and simplifiesthe face patterndistribution.
For the sake of simplicity in visualization, we only use a
subset of the database, which contains 170 images of five ran-domly selected subjects (classes). Four types of feature bases
are generalized from the subset by utilizing the PCA, KPCA,
D-LDA, and KDDA algorithms, respectively. In the four sub-spacesproduced,twoarelinear,producedbyPCAandD-LDA,and two are nonlinear, produced by KPCA and KDDA. In the
sequence, all of images are projected onto the four subspaces.
For each image, its projections in the first two most significantfeaturebases of each subspace arevisualized in Figs. 2and 3.
In Fig. 2, the visualized projections are the first two most
significant principal components extracted by PCA andKPCA, and they provide a low-dimensional representationfor the samples, which can be used to capture the structure
of data. Thus, we can roughly learn the original distribution
of face samples from Fig. 2(a), which is nonconvex andcomplex as we expected based on the analysis presented inthe previous sections. In Fig. 2(b), KPCA generalizes PCA
to its nonlinear counterpart using a RBF kernel function:
LUet al.: FACE RECOGNITION USING KERNEL DIRECT DISCRIMINANT ANALYSIS ALGORITHMS 121
Fig.2. Distributionof170samplesoffivesubjectsinPCA-andKPCA-based
subspaces. (A) PCA-based subspace /40 /26 /41. (B) KPCA-based subspace /40 /26
/41.
Fig. 3. Distribution of 170 samples of five subjects in D-LDA- and
KDDA-based subspaces. (A) D-LDA-based subspace /40 /26 /41. (B)
KDDA-based subspace /40 /26 /41.
with .H o w -
ever, it is hard to find any useful improvement for the purposeof pattern classification from Fig. 2(b). It can be concluded,therefore, that the low-dimensional representation obtained by
PCA like techniques, achieve simply object reconstruction,
and they are not necessarily useful for discrimination andclassification tasks [8], [29].
Unlike PCA approaches, LDA optimizes the low-dimen-
sional representation of the objects based on separability
criteria. Fig. 3 depicts the first two most discriminant featuresextracted by utilizing D-LDA and KDDA, respectively. Simpleinspection of Figs. 2 and 3 indicates that these features out-
perform, in terms of discriminant power, those obtained using
PCA like methods. However, subject to limitation of linearity,some classes are still nonseparable in the D-LDA-based
subspace as shown in Fig. 3(a). In contrast to this, we can
see the linearization property of the KDDA-based subspace,as depicted in Fig. 3(b), where all of classes are well linearlyseparable when a RBF kernel with
is used.
B. Comparison With KPCA and GDA
Thesecondexperimentcomparestheclassificationerrorrate
performance of the KDDA algorithm to two other commonlyusedkernelFRalgorithms,KPCAandGDA.TheFRprocedureis completed in two stages:
1) Featureextraction.Theoveralldatabaseisrandomlypar-
titionedintotwosubsets:thetrainingsetandtestset.Thetraining set is composed of 120 images: Six images per
person are randomly chosen. The remaining 455 imagesare used to form the test set. There is no overlapping be-
tween the two. After training is over, both sets are pro-jected into the feature spaces derived from the KPCA,
GDA and KDDA methods.
2) Classification. This is implemented by feeding feature
vectors obtained in Step 1) into a nearest neighbor clas-sifier.Itshouldbenotedatthispointthat,sincethefocus
inthispaperisonfeatureextraction,asimpleclassifieris
alwayspreferedsothattheFRperformanceisnotmainlycontributed by the classifier but the feature selection al-gorithms. We anticipate that the classification accuracy
of all the three methods compared here will improve if a
moresophisticatedclassifiersuchasSVMisusedinsteadof the nearest neighbor. However, such an experiment isbeyond the scope of this paper. To enhance the accuracy
of performance evaluation, the classification error rates
reported in this work are averaged over eight runs. Eachrun is based on a random partition of the database intothetrainingandtestsets.Followingtheframeworkintro-
ducedin[30],[6],[31],theaverageerrorrate,denotedas
, is given as follows:
(15)
whereisthenumberofruns, isthenumberofmis-
classificationsforthe thrun,and isthenumberoftotal
test samples of each run.
Toevaluatetheoverallperformanceofthethreemethods,two
typical kernel functions: namely the RBF and the polynomial
function, and a wide range of parameter values are tested. Sen-
sitivity analysis is performed with respect to the kernel param-eters and the number of used feature vectors
. Figs. 4 and 5
depicttheaverageerrorrates ofthethreemethodscom-
pared when the RBF and polynomial kernels are used.
The only kernel parameter for RBF is the scale value .
Fig.4(a)showstheerrorratesasfunctionsof withintherange
from to, when the optimal number of feature vec-
tors, ,isused.Theoptimalfeaturenumberisaresult
oftheexistenceofthepeakingeffectinthefeatureselectionpro-cedure.Itiswellknownthattheclassificationerrorinitiallyde-clineswiththeadditionofnewfeatures,attainsaminimum,and
thenstartstoincrease[32].Theoptimalnumbercanbefoundby
searching the number of used feature vectors that results in theminimalsummationoftheerrorratesoverthevariationrangeof
. In Fig. 4(a), is the value used for KPCA, while
is used for GDA and KDDA. Fig. 4(b) depicts the
errorratesasfunctionsof withintherangefrom5to19,when
optimal is used. Similar to is defined as
the scale parameter that results in the minimal summation of
theerrorratesoverthevariationrangeof fortheexperiment
discussed here. In Fig. 4(b), a value is found for
KPCA, for GDA and for
KDDA.
As such, the average error rates of the three methods with
polynomial kernel areshown
in Fig. 5. For the sake of simplicity, we only test the influence
of, while and are fixed. Fig. 5(a) depicts the
122 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 14, NO. 1, JANUARY 2003
(a) (b)
Fig. 4. Comparison of error rates based on RBF kernel function. (a) Error rates as functions of /27. (b) Error rate as functions of /77.
(a) (b)
Fig. 5. Comparison of error rates based on Polynomial kernel function. (a) error rates as functions of /97. (b) Error rate as functions of /77.
TABLE I
AVERAGEPERCENTAGES OF THE ERROR
RATE OFKDDA O VERTHOSE OFOTHERS
error rates as functions of within the range from to
, where for KPCA, for GDA
and KDDA. Fig. 5(b) shows the error rates as functions of
within the range from 5 to 19 with for KPCA,
for GDA and for KDDA,
determined similarly to and .
LetandbetheaverageerrorratesofKDDAandany
oneofothertwomethodsrespectively,where .
From Figs. 4(b) and 5(b), we can obtain an interesting quan-
tity comparison: the average percentages of the error rate ofKDDA overthose of other methods by
. The
resultsaretabulatedinTableI.TheaverageerrorrateofKDDA
to KPCA and GDA are only about 34.375% and 47.765% re-
spectively.ItshouldbealsonotedthatFigs.4(a)and5(a)revealthenumericalstabilityproblemsexistinginpracticalimplemen-tations of GDA. Comparing the GDA performance to that of
KDDA we can easily see that the later is more stable and pre-dictable,resultinginacosteffectivedeterminationofparameter
values during the training phase.
IV. C
ONCLUSION
AnewFRmethodhasbeenintroducedinthispaper.Thepro-
posed method combines kernel-based methodologies with dis-
criminant analysis techniques. The kernel function is utilizedto map the original face patterns to a high-dimensional featurespace,wherethehighlynonconvexandcomplexdistributionof
facepatternsislinearizedandsimplified,sothatlineardiscrim-
inant techniques can be used for feature extraction. The smallsample size problem caused by high dimensionality of mappedpatterns, is addressed by animproved D-LDA techniquewhich
exactly finds the optimal discriminant subspace of the feature
space without any loss of significant discriminant information.ExperimentalresultsindicatethattheperformanceoftheKDDAalgorithm is overall superior to those obtained by the KPCA
or GDA approaches. In conclusion, the KDDA algorithm is a
general pattern recognition method for nonlinearly feature ex-tractionfromhigh-dimensionalinputpatternswithoutsufferingfromtheSSSproblem.Weexpectthatinadditiontofacerecog-
nition, KDDA will provide excellent performance in applica-
tions where classification tasks are routinely performed, suchas content-based image indexing and retrieval as well as videoand audio classification.
LUet al.: FACE RECOGNITION USING KERNEL DIRECT DISCRIMINANT ANALYSIS ALGORITHMS 123
Fig. 6. KDDA pseudocode implementation.
APPENDIX I
COMPUTATION OF
Expanding ,w eh a v e
(16)
where
(17)
Wedevelopeachtermof(17)accordingtothekernelmatrix
as follows:
‚Ä¢
‚Ä¢
‚Ä¢
Applying theabove derivationsinto (17), weobtain the(7).
APPENDIX II
COMPUTATION OF
Expanding ,w eh a v e
(18)
124 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 14, NO. 1, JANUARY 2003
where
(19)
First, expand the term
 in (19), and
have
(20)
Wedevelopeachtermof(20)accordingtothekernelmatrix
as follows:
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
Defining
 ,wecon-
clude
(21)
Expanding the term
 in (19),we obtain
(22)
Using the kernel matrix , the terms in (22) can be developed
as follows:
‚Ä¢
‚Ä¢

LUet al.: FACE RECOGNITION USING KERNEL DIRECT DISCRIMINANT ANALYSIS ALGORITHMS 125
‚Ä¢
‚Ä¢
where
 is a block diagonal
matrix,and isa matrixwithtermsallequalto: .
Defining
 , and
using the above derivations, we conclude that
(23)
Thus
(24)
ACKNOWLEDGMENT
The authors would like to thank Dr. D. Graham and Dr. N.
Allinson for providing the UMIST face database.
REFERENCES
[1] A. Samal and P. A. Iyengar, ‚ÄúAutomatic recognition and analysis of
human faces and facial expressions: A survey,‚Äù Pattern Recognit. , vol.
25, pp. 65‚Äì77, 1992.
[2] D.Valentin,J.O.TooleHerveAbdiAlice,andG.W.Cottrell,‚ÄúConnec-
tionistmodelsoffaceprocessing:Asurvey,‚Äù PatternRecognit. ,vol.27,
no. 9, pp. 1209‚Äì1230, 1994.
[3] R.Chellappa,C.L.Wilson,andS.Sirohey,‚ÄúHumanandmachinerecog-
nition of faces: A survey,‚Äù Proc. IEEE , vol. 83, pp.705‚Äì740, 1995.
[4] S. Gong, S. J. McKenna,and A. Psarrou, Dynamic Vision From Images
toFaceRecognition . Singapore:ImperialCollegePress,WorldScien-
tific, May 2000.
[5] M. Turk, ‚ÄúA random walk through eigenspace,‚Äù IEICE Trans. Inform.
Syst., vol. E84-D, no. 12, pp. 1586‚Äì1695, Dec. 2001.
[6] S. Z. Li and J. Lu, ‚ÄúFace recognition using the nearest feature line
method,‚Äù IEEE Trans. Neural Networks , vol. 10, pp. 439‚Äì443, Mar.
1999.
[7] M. A. Turk and A. P. Pentland, ‚ÄúEigenfaces for recognition,‚Äù J. Cogn.
Neurosci. , vol. 3, no. 1, pp. 71‚Äì86, 1991.
[8] P. N. Belhumeur, J. P. Hespanha, and D. J. Kriegman, ‚ÄúEigenfaces vs.
Fisherfaces: Recognition using class specific linear projection,‚Äù IEEE
Trans.Pattern Anal. Machine Intell. , vol. 19, pp.711‚Äì720,July 1997.
[9] L.-F. Chen, H.-Y. M. Liao, M.-T. Ko, J.-C. Lin, and G.-J. Yu, ‚ÄúA new
LDA-based face recognition system which can solve the small sample
size problem,‚Äù Pattern Recognit. , vol. 33, pp. 1713‚Äì1726, 2000.
[10] H. Yu and J. Yang, ‚ÄúA direct lda algorithm for high-dimensional data
with application to face recognition,‚Äù Pattern Recognit. , vol. 34, pp.
2067‚Äì2070, 2001.[11] M. Bichsel and A. P. Pentland, ‚ÄúHuman face recognition and the
face image set‚Äôs topology,‚Äù CVGIP: Image Understand. , vol. 59, pp.
254‚Äì261, 1994.
[12] B. Sch√∂lkopf, C. Burges, and A. J. Smola, Advances in Kernel
Methods‚ÄîSupport Vector Learning . Cambridge, MA: MIT Press,
1999.
[13] (2000). [Online]. Available: http://www.kernel-machines.org
[14] A. Ruiz and P. E. L√≥pez de Teruel, ‚ÄúNonlinear kernel-based statistical
patternanalysis,‚Äù IEEETrans.NeuralNetworks ,vol.12,pp.16‚Äì32,Jan.
2001.
[15] K.-R. M√ºller, S. Mika, G. R√§tsch, K. Tsuda, and B. Sch√∂lkopf, ‚ÄúAn
introduction to kernel-based learning algorithms,‚Äù IEEE Trans. Neural
Networks , vol. 12, pp. 181‚Äì201, Mar. 2001.
[16] C.CortesandV.N.Vapnik,‚ÄúSupportvectornetworks,‚Äù MachineLearn. ,
vol. 20, pp. 273‚Äì297, 1995.
[17] V. N. Vapnik, The Nature of Statistical Learning Theory . New York:
Springer-Verlag, 1995.
[18] B. Sch√∂lkopf, Support Vector Learning , Munich, Germany: Olden-
bourg-Verlag, 1997.
[19] B. Sch√∂lkopf, A. Smola, and K. R. M√ºller, ‚ÄúNonlinear component
analysis as a kernel eigenvalue problem,‚Äù Neural Comput. , vol. 10, pp.
1299‚Äì1319, 1999.
[20] G. Baudat and F. Anouar, ‚ÄúGeneralized discriminant analysis using a
kernel approach,‚Äù Neural Comput. , vol. 12, pp.2385‚Äì2404, 2000.
[21] R. A. Fisher, ‚ÄúThe use of multiple measures in taxonomic problems,‚Äù
Ann. Eugenics , vol. 7, pp. 179‚Äì188, 1936.
[22] M. A. Aizerman, E. M. Braverman, and L. I. Rozono√©r, ‚ÄúTheoretical
foundations of the potential function method in pattern recognition
learning,‚Äù Automat. Remote Contr. , vol. 25, pp. 821‚Äì837, 1964.
[23] V. N.Vapnik, Statistical Learning Theory . New York: Wiley,1998.
[24] B. Scholkopf, S. Mika, C. J. C. Burges, P. Knirsch, K.-R. Muller, G.
Ratsch, and A. J. Smola, ‚ÄúInput space versus feature space in kernel-
basedmethods,‚Äù IEEETrans.NeuralNetworks ,vol.10,pp.1000‚Äì1017,
Sept. 1999.
[25] K.Liu,Y.Q.Cheng,J.Y.Yang,andX.Liu,‚ÄúAnefficientalgorithmfor
foley-sammonoptimalsetofdiscriminantvectorsbyalgebraicmethod,‚Äù
Int. J. Pattern Recog. Artif. Intell. , vol. 6, pp. 817‚Äì829, 1992.
[26] R. A. Horn and C. R. Johnson, Matrix Analysis . Cambridge, MA:
Cambridge Univ. Press, 1992.
[27] , D. Graham and N. Allinson. (1998). [Online]. Available: http://im-
ages.ee.umist.ac.uk/danny/database.html
[28] D. B. Graham and N. M. Allinson, ‚ÄúCharacterizing virtual eigensigna-
tures for general purpose face recognition,‚Äù in Face Recognition: From
TheorytoApplications,NATOASISeriesF,ComputerandSystemsSci-
ences, H. Wechsler, P. J. Phillips, V. Bruce, F. Fogelman-Soulie, and T.
S. Huang, Eds., 1998, vol. 163, pp. 446‚Äì456.
[29] D. L. Swets and J. Weng, ‚ÄúUsing discriminant eigenfeatures for image
retrieval,‚Äù IEEE Trans. Pattern Anal. Machine Intell. , vol. 18, pp.
831‚Äì836, Aug. 1996.
[30] S. Lawrence, C. Lee Giles, A. C. Tsoi, and A. D. Back, ‚ÄúFace recog-
nition: A convolutional neural network approach,‚Äù IEEE Trans. Neural
Networks , vol. 8, pp. 98‚Äì113, Jan. 1997.
[31] M. Joo Er, S. Wu, J. Lu, and H. L. Toh, ‚ÄúFace recognition with radial
basis function (RBF) neural networks,‚Äù IEEE Trans. Neural Networks ,
vol. 13, pp. 697‚Äì710, May 2002.
[32] S. J. Raudys and A. K. Jain, ‚ÄúSmall sample size effects in statistical
pattern recognition: Recommendations for practitioners,‚Äù IEEE Trans.
Pattern Anal. Machine Intell. , vol. 13, pp. 252‚Äì264, May 1991.
Juwei Lu (S‚Äô00) received the B.Eng. degree in
electrical engineering from Nanjing University of
Aeronautics and Astronautics, China, in 1994 and
the M.Eng. degree from the School of Electrical
and Electronic Engineering, Nanyang Technological
University, Singapore, in 1999. Currently, he is pur-suing the Ph.D. degree in the Edward S. Rogers, Sr.
DepartmentofElectricalandComputerEngineering,
Universityof Toronto, Toronto, ON, Canada.
From July 1999 to January 2001, he was with
the Center for Signal Processing, Singapore, as a
ResearchEngineer.Hisresearchinterestsincludemultimediasignalprocessing,
face detection and recognition, kernel methods, support vector machines,neural networks, and boosting technologies.
126 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 14, NO. 1, JANUARY 2003
Konstantinos N. Plataniotis (S‚Äô88‚ÄìM‚Äô95) received
theB.Eng.degreeincomputerengineeringandinfor-
matics from University of Patras, Patras, Greece, in
1988andtheM.SandPh.Ddegreesinelectricalengi-
neeringfromFloridaInstituteofTechnology(Florida
Tech),Melbourne, in1992 and 1994,respectively.
He was with the Computer Technology Institute
(CTI), Patras, Greece from 1989 to 1991. He was a
Postdoctoral Fellow at the Digital Signal and Image
Processing Laboratory, Department of Electrical
and Computer Engineering University of Toronto,
Toronto, ON, Canada from November 1994 to May 1997. From September
1997toJune1999,hewasanAssistantProfessorwiththeSchoolofComputer
Science at Ryerson University, Toronto. He is now an Assistant Professor with
the Edward S. Rogers Sr. Department of Electrical and Computer Engineeringat the University of Toronto, a Nortel Institute for Telecommunications
Associate, and an Adjunct Professor in the Department of Mathematics,
Physics and Computer Science at Ryerson University. His research interests
include multimedia signal processing, intelligent and adaptive systems, and
wireless communication systems.
Dr. Plataniotis is a member of the Technical Chamber of Greece.
Anastasios N. Venetsanopoulos (S‚Äô66‚ÄìM‚Äô69‚Äì
SM‚Äô79‚ÄìF‚Äô88) received the diploma degree in
engineering from the National Technical University
of Athens (NTU), Athens, Greece, in 1965, and
the M.S., M.Phil., and Ph.D. degrees in electrical
engineering from Yale University, New Haven, CT,in 1966, 1968, and 1969 respectively.
He joined the Department of Electrical and
Computer Engineering of the University of Toronto,
Toronto, ON, Canada, in September 1968, as a
LecturerandhewaspromotedtoAssistantProfessor
in 1970, Associate Professor in 1973, and Professor in 1981. He has served as
ChairoftheCommunicationsGroupandAssociateChairoftheDepartmentof
Electrical Engineering. Between July 1997 and June 2001, he was Associate
Chair: Graduate Studies of the Department of Electrical and ComputerEngineering, and was Acting Chair during spring term 1998‚Äì1999. In 1999, a
Chair in Multimedia was established in the ECE Department, made possible
by a donation of $1.25 M from Bell Canada, matched by $1.0 M of university
funds.HeassumedthepositionasInauguralChairholderinJuly1999,andtwo
additional Assistant Professor positions became available in the same area.
Since July 2001, he has served as the 12th Dean of the Faculty of Applied
ScienceandEngineeringoftheUniversityofToronto.Hewasonresearchleaveat the Imperial College of Science and Technology, the National Technical
UniversityofAthens,theSwissFederalInstituteofTechnology,theUniversity
of Florence and the Federal University of Rio de Janeiro, and has also served
as Adjunct Professor at Concordia University. He has served as lecturer in 138
shortcoursestoindustryandcontinuingeducationprogramsandasConsultant
to numerous organizations; he is a contributor to 29 books, a coauthor of
Nonlinear Filters in Image Processing: Principles Applications (Boston,
MA: Kluwer, 1990), and Artificial Neural Networks: Learning Algorithms,
Performance Evaluation and Applications , (Boston, MA: Kluwer, 1993),
Fuzzy Reasoning in Information Decision and Control Systems , (Boston, MA:
Kluwer, 1994), and Color Image Processing and Applications (New York:
Springer-Verlag, 2000),and haspublished over700papers inrefereedjournals
and conference proceedings on digital signal and image processing and digital
communications.
Prof. Venetsanopoulos has served as Chair on numerous boards, councils,
and technical conference committees of the IEEE, such as the Toronto Section
(1977‚Äì1979)andtheIEEECentralCanadaCouncil(1980‚Äì1982);hewasPres-
ident of the Canadian Society for Electrical Engineering and Vice President ofthe Engineering Institute of Canada (EIC) (1983‚Äì1986). He was a Guest Ed-
itor or Associate Editor for several IEEE journals and the Editor of the Cana-
dianElectricalEngineeringJournal (1981‚Äì1983).HeisamemberoftheIEEE
Communications, Circuits and Systems, Computer, and Signal Processing So-
cieties,aswellasamemberofSigmaXi,theTechnicalChamberofGreece,the
EuropeanAssociationofSignalProcessing,theAssociationofProfessionalEn-
gineersofOntario(APEO)andGreece.HewaselectedasaFellowoftheIEEE‚Äúfor contributions to digital signal and image processing.‚Äù He is also a Fellow
of the EIC, and was awarded an Honorary Doctorate from the National Tech-
nical Universityof Athens, in October 1994.In October 1996,he was awarded
the‚ÄúExcellenceinInnovationAward‚ÄùoftheInformationTechnologyResearch
Centre of Ontario and Royal Bank of Canada, ‚Äúfor innovative work in color
image processing and its industrial applications‚Äù. In November 2000, he be-
cameRecipientofthe‚ÄúMillenniumMedalofIEEE.‚ÄùInApril2001,hebecame
aFellowoftheCanadianAcademyofEngineering,andonJuly1,2001,hewas
appointedasthetwelthDeanoftheFacultyofAppliedScienceandEngineering,University of Toronto.
"
https://ieeexplore.ieee.org/document/1000134,"IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 13, NO. 3, MAY 2002 697
Face Recognition With Radial Basis Function (RBF)
Neural Networks
Meng Joo Er , Member, IEEE , Shiqian Wu , Member, IEEE , Juwei Lu , Student Member, IEEE , and
Hock Lye Toh , Member, IEEE
Abstract‚Äî Ageneralandefficientdesignapproachusingaradial
basis function (RBF) neural classifier to cope with small trainingsetsofhighdimension,whichisaproblemfrequentlyencounteredin face recognition, is presented in this paper. In order to avoidoverfittingandreducethecomputationalburden,facefeaturesarefirstextractedbytheprincipalcomponentanalysis(PCA)method.Then, the resulting features are further processed by the Fisher‚Äôslineardiscriminant(FLD)techniquetoacquirelower-dimensionaldiscriminantpatterns.Anovelparadigmisproposedwherebydatainformation is encapsulated in determining the structure and ini-tial parameters of the RBF neural classifier before learning takesplace.AhybridlearningalgorithmisusedtotraintheRBFneuralnetworkssothatthedimensionofthesearchspaceisdrasticallyre-duced in the gradient paradigm. Simulation results conducted onthe ORL database show that the system achieves excellent perfor-mance both in terms of error rates of classification and learning
efficiency.
Index Terms‚Äî Face recognition, Fisher‚Äôs linear discriminant,
ORLdatabase,principalcomponentanalysis,radialbasisfunction(RBF) neural networks,small training sets of high dimension.
I. INTRODUCTION
MACHINErecognitionofhumanfacefromstillandvideo
images has become an active research area in the com-
munities of image processing, pattern recognition, neural net-
works and computer vision. This interest is motivated by wide
applications ranging from static matching of controlled format
photographs such as passports, credit cards, driving licenses,and mug shots to real-time matching of surveillance video im-
ages presenting different constraints in terms of processing re-
quirements[1].Althoughresearchersinpsychology,neuralsci-ences and engineering, image processing and computer vision
haveinvestigatedanumberofissuesrelatedtofacerecognition
by human beings and machines, it is still difficult to design an
automaticsystemforthistask,especiallywhenreal-timeidenti-
ficationisrequired.Thereasonsforthisdifficultyaretwo-fold:1)Faceimagesarehighlyvariableand2)Sourcesofvariability
include individual appearance, three-dimensional (3-D) pose,
facial expression, facial hair, makeup, and so on and these fac-
tors change from time to time. Furthermore,the lighting, back-
ground, scale, and parameters of the acquisition are all vari-
Manuscript received March 29, 1999; revised March 5, 2001 and December
5, 2001.
M.J.EriswiththeSchoolofElectricalandElectronicEngineering,Nanyang
Technological University, Singapore 639798, Singapore.
S. Wu and H. L. Toh are with the Centre for Signal Processing, Innovation
Centre, Singapore 637722, Singapore.
J.LuiswithDepartmentofElectricalandComputerEngineering,University
of Toronto, Toronto, ON M5S 3G4, Canada.
Publisher Item Identifier S 1045-9227(02)03984-X.ables in facial images acquired under real-world scenarios [1].
As stated by Moses et al.[2], ‚ÄúThe variations between the im-
agesofthesamefaceduetoilluminationandviewingdirectionare almost always larger than image variations due to changes
in the face identity.‚Äù This makes face recognition a great chal-
lenging problem. In our opinion, two issues are central to face
recognition:
1) Whatfeaturescanbeusedtorepresentafaceunderenvi-
ronmental changes?
2) How to classify a new face image based on the chosen
representation?
For 1), many successful face detection and feature extrac-
tion paradigms have been developed [3]‚Äì[12]. The frequently
used approaches are to use geometrical features, where the rel-ative positions and shapes of different features are measured
[3], [4]. At the same time, several paradigms have been pro-
posed to use global representation of a face, where all features
ofafaceareautomaticallyextractedfromaninputfacialimage
[5]‚Äì[12]. It has been indicated in [4] that these algorithms withglobal encoding of a face are fast in face recognition. In [5],
singular value decomposition (SVD) of a matrix was used to
extract features from the patterns. It has been illustrated that
singular values of an image are stable and represent the alge-
braic attributes of an image, being intrinsic but not necessarilyvisible. The eigenface approach of describing the features of a
face was presented in [6]. The key idea is to calculate the best
coordinatesystemforimagecompression,inwhicheachcoordi-nateisactuallyanimagethatiscalledaneigenpicture.However,
the eigenface paradigm, which uses principal component anal-
ysis(PCA),yieldsprojectiondirectionsthatmaximizethetotal
scatteracrossallclasses,i.e.,acrossallfaceimages.Inchoosing
the projection which maximizes the total scatter, the PCA re-tainsunwantedvariationscausedbylighting,facialexpression,
andotherfactors[7].Accordingly,thefeaturesproducedarenot
necessarily good for discrimination among classes. In [7], [8],
thefacefeaturesareacquiredbyusingthefisherfaceordiscrim-
inanteigenfeatureparadigm.ThisparadigmaimsatovercomingthedrawbackoftheeigenfaceparadigmbyintegratingFisher‚Äôs
lineardiscriminant(FLD)criteria,whileretainingtheideaofthe
eigenface paradigm in projecting faces from a high-dimension
imagespacetoasignificantlylower-dimensionalfeaturespace.
Instead of using statistical theory, neural-networks-based fea-tureextractionhasbeenreportedrecently[9]‚Äì[12].Thegoalof
face processing using neural networks is to develop a compact
internal representation of faces, which is equivalent to featureextraction.Therefore,thenumberofhiddenneuronsislessthan
thatineitherinputoroutputlayers,whichresultsinthenetwork
encoding inputs in a smaller dimension that retains most of the
importantinformation.Then,thehiddenunitsoftheneuralnet-
1045-9227/02$17.00 ¬© 2002 IEEE
698 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 13, NO. 3, MAY 2002
work can serve as the input layer of another neural network to
classify face images.
In many pattern recognition systems, the methodology fre-
quentlyusedisthestatisticalapproach,wherebydecisiontheoryderivedfromstatisticsofinputpatternsisusedtodesignaclas-
sifier [13]. Although this paradigm has been successfully ap-
plied to solve various problems in pattern classification, it hasdifficulty in expressing structural information unless an appro-priatechoiceoffeaturesismadepossible.Furthermore,thisap-
proach requires much heuristic information to design a classi-
fier [14]. Neural-networks-based paradigms, as new means ofimplementing various classifiers based on statistical and struc-tural approach, have been proven to possess many advantages
forclassificationbecauseoftheirlearningabilityandgoodgen-
eralization [9]‚Äì[12], [14]‚Äì[16]. Generally speaking, multilay-ered networks (MLNs), usually coupled with the backpropaga-
tion (BP) algorithm, are most widely used in face recognition
[9]. Yet, two major criticisms are commonly raised against theBP algorithm: 1) It is computationally intensive because of itsslow convergence speed and 2) there is no guarantee at all that
the absolute minima can be achieved. On the other hand, RBF
neuralnetworkshaverecentlyattractedextensiveinterestsinthecommunity ofneuralnetworksforawiderangeofapplications[17]‚Äì[29]. The salient features of RBF neural networks are as
follows.
‚Ä¢ They are universal approximators [17].
‚Ä¢ They possess the best approximation property [18].
‚Ä¢ Their learning speed is fast because of locally tuned neu-
rons [19].
‚Ä¢ They have more compact topology than other neural net-
works [20].
Normally,RBFneuralnetworksarewidelyusedforfunction
approximation and pattern recognition wherein the pattern di-mensionintheseapplicationsisusuallysmall.Aspointedoutby
Moody and Darken [19], ‚ÄúRBF neural networks are best suited
forlearningtoapproximatecontinuousorpiecewisecontinuous,real-valued mapping where the input dimension is sufficientlysmall.‚Äù When RBF neural networks are implemented in face
recognition,suchsystemspossessthefollowingcharacteristics:
‚Ä¢ High dimension. For example, a 128
128 image will
have 16384 features.
‚Ä¢ Small sample sets. The sample patterns are very few for
each class, say, only one‚Äìten images per person so that
(is the number of training patterns, is the
number of features), which is more severe than the case
shown in [16].
Therefore, face recognition is substantially different from clas-
sicalpatternrecognitionproblem,forinstance,characterrecog-nition [14], in which there are a limitednumber of classes with
a large number of training patterns in each class. This situation
leads to the following challenges in designing an RBF neuralclassifier:
1)Overfitting problem . It has been indicated that if the di-
mension of the network input is comparable to the sizeof the training set, the system is liable to overfitting and
result in poor generalization [16].
2)Overtraining problem . High dimension of the network
inputresultsincomplexoptimalprocessingandslowcon-vergence. Hence, it is likely to cause overtraining.
Fig. 1. Schematic diagram of RBF neural classifier for small training sets of
high dimension.
3)Small-sample effect . It has been indicated that small
sample can easily contaminate the design and evaluation
of a proposed system [30]. For applications with a large
number of features and a complex classification rule, thetraining sample size must be quite large [30]. It has beenfurtherpointed out that thesample sizeneeds to increase
exponentially in order to have an effective estimate of
multivariate densities as thedimension increases [31].
4)Singularproblem .If
islessthan ,thesamplecovari-
ancematrixissingular,andthereforeunusableregardless
of the true value of the covariance matrix [32].
To circumvent the aforementioned problems, a systematic
methodologyforRBFneuralclassifierdesigntodealwithsmalltraining sets of high-dimensional feature vectors is presented,
as shown in Fig. 1. The proposed methodology comprises the
following parts: 1) The number of input variables is reduced
through feature selection, i.e., a set of the most expressive fea-
tures is first generated by the PCA and the FLD is then imple-mented to generate a set of the most discriminant features so
that different classes of training data can be separated as far
as possible and the same classes of patterns are compacted asclose as possible; 2) A new clustering algorithm concerning
category information of training samples is proposed so that
homogeneous data could be clustered and a compact structure
of an RBF neural classifier with limited mixed data could be
achieved;3)Twoimportantcriteriaareproposedtoestimatetheinitial widths of RBF units which control the generalization of
RBFneuralclassifier;and4)Ahybridlearningalgorithmispre-
sented to train the RBF neural networks so that the dimension
of the search space is significantly reduced in the gradient par-
adigm.
The rest of this paper is organized as follows. Section II
presents the architecture of RBF neural networks and the
related design problems when they are used as a classifier.
Section III provides the procedure of extracting face features.
In Section IV, we propose a systemic approach for structuredetermination and initialization of RBF neural networks. A
hybrid learning algorithm is developed in Section V. Experi-
mental results are demonstrated in Section VI. In Section VII,we discuss some important issues concerning performances of
the proposed approach and provide more insights into several
paradigms,whicharecloselyrelatedtoourproposedparadigm.
Finally, conclusions are drawn in Section VIII.
ERet al.: FACE RECOGNITION WITH RBF NEURAL NETWORKS 699
Fig. 2. RBF neural networks.
II. RBF N EURALNETWORKS
An RBF neural network, shown in Fig. 2, can be considered
as a mapping: .
Let bethe input vectorand be
theprototypeoftheinputvectors.TheoutputofeachRBFunitis as follows:
(1)
where indicates the Euclidean norm on the input space.
Usually, the Gaussian function is preferred among all possibleradialbasisfunctionsduetothefactthatitisfactorizable.Hence
(2)
where isthewidthofthe thRBFunit.The thoutput
of an RBF neural network is
(3)
where ,is the weight or strength of the th re-
ceptive field to the th output and is the bias of the th
output. In order to reduce the network complexity, the bias isnot considered in the following analysis.
Wecanseefrom(2)and(3)thattheoutputsofanRBFneural
classifier are characterized by a linear discriminant function.
They generate linear decision boundaries (hyperplanes) in theoutput space. Consequently, the performance of an RBF neuralclassifier strongly depends on the separability of classes in the
-dimensionalspacegeneratedbythenonlineartransformation
carried out by the RBF units.
AccordingtoCover‚Äôstheoremontheseparabilityofpatterns
wherein a complex pattern classification problem cast in a
high-dimensionalspacenonlinearlyismorelikelytobelinearly
separable than in a low-dimensional space [33], the numberof Gaussian nodes
, where is the dimension of input
space. On the other hand, the increase of Gaussian units may
result in poor generalization because of overfitting, especially,
inthecaseofsmalltrainingsets[16].Itisimportanttoanalyzethe training patterns for the appropriate choice of RBF hiddennodes.
Geometrically, the key idea of an RBF neural network is to
partition the input space into a number of subspaces which areintheformofhyperspheres.Accordingly,clusteringalgorithms(
-meansclustering,fuzzy -meansclusteringandhierarchical
clustering)whicharewidelyusedinRBFneuralnetworks[19],
(a)
(b)
Fig.3. Two-dimensionpatternsandclustering:(a)conventionalclustering,(b)
clustering with homogeneous analysis.
Fig. 4. Effect of Gaussian widths in clustering.
[21], are a logical approach to solve the problems [19], [22].
However, it should be noted that these clustering approachesare inherently unsupervised learning algorithms as no category
information about patterns is used. As an illustrative example,
consider a simple training set
illustrated in Fig. 3. The
blackandwhitedatapointsreflectthecorrespondingvaluesas-sumed bythedependent variable
. Ifwesimply use -means
clustering approach without considering , two evident clus-
tersasshowninFig.3(a)areachieved.Thisbringsaboutsignif-icantmisclassificationinitially. Althoughtheclusteringbound-aries are modified in the subsequent learning phase, this could
easilyleadtoanundesiredandhighlydominantaveragingphe-
nomenon as well as to make the learning less effective [21].Topreservehomogeneousclusters,threeclustersasdepictedinFig. 3(b) should be created. In other words, a supervised clus-
teringprocedurewhichtakesintoconsiderationthecategoryin-
formation of training data should be considered.
While considering the category information of training pat-
terns, it should be emphasized that the class memberships are
notonlydependedonthedistanceofpatterns,butalsodepended
on the Gaussian widths. As illustrated in Fig. 4,
is near to
the center of class in Euclidean distance, but we can select
different Gaussian widths for each cluster so that the point
has greater class membership to class than that to class .
Therefore, the use of class membership implies that we shouldpropose a supervised procedure to cluster the training patternsanddeterminetheinitialGaussianwidths,andthisworkwillbe
elaborated in Section IV.
700 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 13, NO. 3, MAY 2002
III. EXTRACTION OF FACEFEATURES
A. Principal Component Analysis (PCA)
Let a face image be a two-dimensional array of
intensity values. An image may also be considered as a vectorof dimension
. Denote the training set of face images by
, and we assume that each
imagebelongstooneof classes.Definethecovariancematrix
as follows [6], [13]:
(4)
where and
. Then, the eigenvalues and eigenvectors of the
covariance are calculated. Let
be the eigenvectors corresponding to the
largest eigenvalues. Thus, for a set of original face images
, their corresponding eigenface-based feature
can be obtained by projecting into the eigenface
space as follows:
(5)
B. Fisher‚Äôs Linear Discriminant (FLD)
Actually, the PCA paradigm does not provide any informa-
tion for class discrimination but dimension reduction. Accord-
ingly, the FLD is applied to the projection of the set of training
samples in the eigenface space
. The paradigm finds an optimal subspace for classifi-
cation in which the ratio of the between-class scatter and the
within-classscatterismaximized[7],[8],[13].Letthebetween-class scatter matrix be defined as
(6)
and the within-class scatter matrix be defined as
(7)
where isthemeanimageoftheensemble,
and is the mean image of the th class,
isthenumberofsamplesinthe thclass,and isthenumber
ofclasses.Theoptimalsubspace, bytheFLDisdeter-
mined as follows [7], [8], [13]:
(8)
where is the set of generalized eigenvectors
ofandcorresponding to the largest generalized
eigenvalues , i.e.,
(9)Thus, the feature vectors for any query face images in the
most discriminant sense can be calculated as follows:
(10)
Remarks:
1) From (7), we see .I n
order to prevent from becoming singular, the value
ofshould be no more than .
2) Alsowecanseefrom(6)that .
Accordingly,thereareatmost nonzerogeneralized
eigenvectors.Inotherwords,theFLDtransformsthe -di-
mensionspaceinto -dimensionspacetoclassify
classes of objects.
3) It should be noted that the FLD is a linear transforma-
tion which maximizes the ratio of the determinant of the
between-class scatter matrix of the projected samples to
the determinant of the within-class scatter matrix of theprojected samples. The results are globally optimal only
forlinearseparabledata.Thelinearsubspaceassumption
is violated for the face data that have great overlappings[34]. Moreover, the separability criterion is not directlyrelated to the classification accuracy in the output space
[34].
4) Several researchers have indicated that the FLD method
achieved the best performance on the training data, butgeneralized poorly to new individuals [35], [36].
Therefore, RBF neural networks, as a nonlinear alternative
with good generalization, have been proposed for face classifi-cation. In the sequel, we will use the feature vectors
instead
of their corresponding original data in Sections IV‚ÄìVIII.
IV. STRUCTURE DETERMINATION AND INITIALIZATION OF RBF
NEURALNETWORKS
A. Structure Determination and Choice of Prototypes
From the point of view of face recognition, a set of optimal
boundaries between different classes should be estimated byRBF neural networks. Conversely, from the point of view ofRBF neural networks, the neural networks are regarded as a
mapping from the feature hyperspace to the classes. Each pat-
ternisrepresentedbyarealvectorandeachclassisassignedfora suitable code. Therefore, we set:
‚Ä¢ the number of inputs to be equal to that of features (i.e.,
the dimension of the input space);
‚Ä¢ the number of outputs to be equal to that of classes (see
Fig. 2).
It is cumbersome to select the hidden nodes. Different ap-
proaches revolving around increasing or decreasing the com-plexityofthearchitecturehavebeenproposed[19]‚Äì[28].Manyresearchers have illustrated that the number of hidden units de-
pends on the geometrical properties of the training patterns as
wellasthetypeofactivationfunction[24].Nevertheless,thisisstill an open issue in implementing RBF neural networks. Ourproposed approach is as follows.
1) Initially, we set the number of RBF units to be equal to
that of the output,
, i.e., we assume that each class
has only one cluster.
ERet al.: FACE RECOGNITION WITH RBF NEURAL NETWORKS 701
2) For each RBF unit , , the center is se-
lectedasthemeanvalueofthesamplepatternsbelongingto class
, i.e.,
(11)
where isthethsamplebelongingtoclass andis
the total number of training patterns in class .
3) Foranyclass , computetheEuclidean distance from
the mean to the furthest point belonging to
class, i.e.,
(12)
4) For any class :
‚Ä¢ Calculatethedistance betweenthemeanof
classand the mean of other classes as follows:
(13)
‚Ä¢ Find
(14)
‚Ä¢ Check the relationship between and,
.
Case1)Nooverlapping. If ,
classhas no overlapping with other
classes [see Fig. 5(a)].
Case2)Overlapping. If ,
class has overlapping with other
classes and misclassification may occurin this case. Fig. 5(b) represents thecase that
and
, while Fig. 5(c)
depictsthecasethat
and .
5) Splitting Criteria:
i)Embody Criterion : If class is embodied in class
completely, i.e., and
, classwill besplit into two clus-
ters, see Fig. 6.
ii)Misclassified Criterion : If class contains many
data of other classes (in the following experiment,thisimpliesthatifthenumberofmisclassifieddatain class
is more than one), then class will be
split into two clusters.
If class satisfies one of the above conditions, class
will be split into two clusters in which the centers are
calculated based on their corresponding sample patterns
according to (11).
6) Repeat(2)‚Äì(5),untilallthetrainingsamplepatternsmeet
the above two criteria.
B. Estimation of Widths
Essentially, RBF neural networks overlap localized regions
formed by simple kernel functions to create complex decision
(a)
(b)
(c)
Fig.5. Clustersanddistributionofsamplepatterns:(a) /100 /43 /100 /20 /100 /40 /107/59 /108 /41.
(b) /100 /43 /100 /62/100 /40 /107/59 /108 /41and /106 /100 /0 /100 /106 /60/100 /40 /107/59 /108 /41.(c) /100 /43 /100 /62/100 /40 /107/59 /108 /41
and /106 /100 /43 /100 /106/21 /100 /40 /107/59 /108 /41.
Fig. 6. Splitting of one class into two clusters.
regions while the amount of overlapping is controlled by the
widths of RBF units [22], [24]. If no overlapping occurs, the
systemwill not givemeaningful outputsforinputs betweentheinputs for which the system is designed, i.e., the RBF units donot generalize well. However, if the widths are too large, the
interaction of different classes will be great and the output be-
longing to the class will not be so significant [25], while theoutput of other classes may be large so that it will lead to mis-classification greatly. Hence, our goal is to select the widths in
such a way that they would minimize overlapping of nearest
neighbors of different classes to preserve local properties, aswellasmaximizethegeneralizationabilityofthenetwork[25].Here, we present a general approach to select the widths of an
RBF neural classifier according to two criteria.
1)Majority Criterion : The majority criterion can be de-
scribed as follows: In any class, each datum should havemore than 50% confidence level for the class it belongs
to. The detailed calculations are presented as follows:
First,
, the distance from the mean to the furthest
pointbelongingtoclass ,iscalculatedaccordingto(11)
and (12).
702 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 13, NO. 3, MAY 2002
Next, define the width of class considering the
confidence level as
(15)
whereiscalledtheconfidencecoefficient,whichliesin
the range .
2)Overlapping Criterion : The overlappingcriterion can be
described as follows: For any class , the choice of
consideringtheoverlappingofthenearestclass isdeter-
mined by
(16)
whereisanoverlappingfactorthatcontrolstheoverlap
of different classes, is the minimum distance
betweenthecenterofclass andcentersofotherclasses.
Then, the width of class is finally determined by
(17)
Thekeyideaofthisapproachistoconsidernotonlythe
intra-data distribution butalso theinter-data variations.
In order to efficiently determine the width , the pa-
rameter could beapproximately estimated as follows:
(18)
The choice of is determined by the distribution
ofsamplepatterns.Ifthedataarescattered
largely, but the centers are close, a small should be
selected as demonstrated in Table IV. Normally, lies
in the range . The best values of and
are selected when the best performance is achieved fortraining patterns.
V. H
YBRIDLEARNING ALGORITHM
TheadjustmentofRBFunitparametersisanonlinearprocess
whiletheidentificationofweight isalinearone.Though
we can apply the gradient paradigm to find the entire set ofoptimal parameters, the paradigm is generally slow and likelyto become trapped in local minima. Here, a hybrid learning al-
gorithm, which combines the gradient paradigm and the linear
least square (LLS) paradigm to adjust the parameters, is pre-sented.
A. Weight Adjustment
Let
andbe the number of inputs and outputs respec-
tively, and suppose that RBF units are generated according
to the above clustering algorithm for all training patterns. For
any input , theth output of the system is
(19)
or
(20)Given and , where
is the total number of sample patterns, is the target matrix
consisting of ‚Äú1‚Äôs‚Äù and ‚Äú0‚Äôs‚Äù with exactly one per column that
identifiestheprocessingunittowhichagivenexemplarbelongs,
find an optimal coefficient matrix such that the
error energy is minimized. This
problem can be solved by the LLS method [15]
(21)
where isthetransposeof ,and isthe
pseudoinverse of .
B. Modification of Parameters of RBF Units
Here, the parameters (centers and widths) of the prototypes
areadjustedbytakingthenegativegradientoftheerrorfunction
(22)
where andrepresent the th real output and the target
output at the th pattern, respectively. The error rate for each
output can be calculated readily from (22)
(23)
For the internal nodes (center and width ), the error rate
can be derived by the chain rule as follows [15]:
(24)
(25)
where isthecentralerrorrateofthe thinputvariable
ofthethprototypeatthe thtrainingpattern, isthewidth
error rate of the th prototype at the th pattern, is the
thinputvariableatthe thtrainingpatternand isthelearning
rate.
ERet al.: FACE RECOGNITION WITH RBF NEURAL NETWORKS 703
C. Learning Procedure
In theforward pass, wesupplyinputdataandfunctional sig-
nals to calculate the output of theth RBF unit. Then, the
weight is modified according to (21). After identifying the
weight, the functional signals continue going forward till the
error measure is calculated. In the backward pass, the errors
propagatefromtheoutputendtowardtheinputend.Keepingtheweightfixed,thecentersandwidthsofRBFnodesaremodifiedaccordingto(24)and(25).Thelearningprocedureisillustrated
in Table I.
Remarks:
1) IfwefixtheparametersoftheRBFunits,theweightsfound
by the LLS are guaranteed to be global optimum. Accord-
ingly, the dimension of the search space is drastically re-duced in the gradient paradigm so that this hybrid learning
algorithm converges much faster than the gradient descent
paradigm.
2) It is well known that the learning rate
is sensitive to the
learning procedure. If is small, the BP algorithm will
closely approximate the gradient path, but the convergence
speed will be slow since the gradient must be calculatedmany times. On the other hand, if
is large, convergence
speed will be very fast initially, but the algorithm will
oscillate around the optimum value. Here, we propose an
approach sothat will bereduced gradually. Wecompute
(26)
where ,are maximum and minimum learning
rates, respectively, is the number of epochs, and is a
descent coefficient which lies in the range .
3) As the widths are sensitive to the generalization of an RBF
neural classifier, a larger learning rate is adopted for width
adjustment than for center modification (twice as that for
center modification).
4) Asthesystemwithhighdimensionisliabletoovertraining,
the early stop strategy in [24] is adopted.
VI. EXPERIMENTAL RESULTS
A. ORL Database
Our experiments were performed on the face database
which contains a set of face images taken between April 1992
and April 1994 at the Olivetti Research Laboratory (ORL)in Cambridge University, U.K. There are 400 images of 40individuals. For some subjects, the images were taken at
differenttimes,whichcontainquiteahighdegreeofvariability
in lighting, facial expression (open/closed eyes, smiling/non-smiling etc), pose (upright, frontal position etc), and facialdetails(glasses/noglasses).Alltheimagesweretakenagainsta
dark homogeneous background with the subjects in an upright,
frontal position, with tolerance for some tilting and rotation ofup to 20
. The variation in scale is up to about 10%. All the
images in the database are shown in Fig. 7.1
Inthefollowingexperiments,atotalof200imageswereran-
domly selected as the training set and another 200 images asthe testing set, in which each person has five images. Next, the
1The ORL database is available from http//www.cam-orl.co.uk/face-
database.html.TABLE I
TWOPASSES IN THE HYBRIDLEARNING PROCEDURE
Fig. 7. The ORL face database.
trainingandtestingpatternswereexchangedandtheexperiment
was repeated one more time. Such procedure was carried outseveral times.
B. Clustering Error Before Learning
ThestructureofRBFneuralnetworksandparametersofpro-
totypes are obtained according to the algorithm shown in Sec-tionIV.Inordertotesthowtheclusteringalgorithmworks,the
data distributions on six simulations are illustrated in Table II
andthe misclusteringnumber based on differentdimensions offeature patterns in six runs of simulations before learning arelisted in Table III. We see from Table II that there are a total of
fiveclasseswhichareinwell-separateddistributionasdepicted
inFig.5(a)and235classesaredistributedasshowninFig.5(b)whenthedimensionofthefeaturevectorsis39.Theseparationof data becomes better and better as the dimension decreases.
Correspondingly,theclusteringperformance isbetterwhenthe
numberoffeaturevectorsreduces,asshowninTableIII.How-ever, as we will see later, it does not imply that the recognitionperformance will improve along with reduction in the dimen-
sion. We also find from Table III that the maximum misclus-
704 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 13, NO. 3, MAY 2002
TABLE II
DATADISTRIBUTION BASED ON THE PROPOSED APPROACH BEFORELEARNING
(THERESULTSAREOBTAINEDBASED ON SIXSIMULATIONS )
tering number within a class for each simulation is one except
forone casewhich occurredwhenthe dimension is39. Wecanconclude that the clustering paradigm presented is good for allcases.
C. Error of Classification After Learning
AfterthestructureofRBFneuralnetworksandparametersof
prototypesareselected,thehybridlearningalgorithmpresentedin Section V is employed to train the network. One run of therecognition results is shown in Table IV.
From Table IV, we can see that:
1) If the information is sufficient (feature dimension is
larger than 20), the results are stable in each case for
different choice of initial parameters
andin terms
of the number of misclassifications. Otherwise, the errorrate will increase drastically.
2) On the other hand, it does not mean that more informa-
tion(dimensionislargerthan30)willresultinhigherper-
formance. The reason may be that high dimension willlead to complexity in structure and increase difficultyin learning. Moreover, the addition of some unimportant
information may become noise and degrade the perfor-
mance.Thebestresultsareachievedwhenthedimensionis 25‚Äì30.
3) Along with the increase in the feature dimension, the
training patterns have more overlapping, and a small
should be selected.2
The total results based on six simulations are summarized in
Tables V and VI.
D. Comparisons With Other Approaches
Recently, a number of researchers use the ORL database to
verify their algorithms [11], [12], [29], [37]‚Äì[45]. Here, weadopt the same definition of average error rate,
used in
[11], [37], which is given by
(27)
2It should be noted that /12lies in the range /48 /58 /53 /20 /12/60 /49. Even the RMSE
is smaller in the case of /12/60 /48 /58 /53when the dimension is 39, the generalization
will be very bad.TABLE III
CLUSTERING ERRORS FOR TRAININGPATTERNS BEFORELEARNING (THE
RESULTIS THESUM OFSIXSIMULATIONS )
TABLE IV
SPECIFIED PARAMETERS AND CLASSIFIED PERFORMANCE
* RMSE‚ÄîRoot Mean Squared Error
** NOM‚ÄîNumber of Misclassifications
TABLE V
CLASSIFIED PERFORMANCES ON SIXSIMULATIONS
TABLE VI
BESTPERFORMANCES ON 6SIMULATIONS (DIMENSION /114 /61/50 /53OR30)
whereis the number of experimental runs, each one being
performedonrandompartitioningofthedatabaseintotwosets,
isthenumberofmisclassificationsforthe thrun,and
is the number of total testing patterns of each run. Using the
criterion of , comparisons with CNN [11], NFL [37] and
M-PCA [38] performed on the same ORL database are shown
in Table VII.
Here, the best value of for the CNN is based on three
runs of experiments, and the SOM size is 8 and 9. For NFL,
the best error rate is obtained when the number of feature vec-
tors is 40, and the average error rate is evaluated based on fourrunsofexperiments.ForM-PCA,itwasreportedthattheoverall
performance is the average of ten runs of experiments. For our
proposedparadigm,thebesterrorrateisbasedonsixruns,andthe feature dimension is 25 and 30, respectively. The face fea-tures are the same as [37], and the way to partition the training
set and query set isthe same as the methods in [11]and [37].
ERet al.: FACE RECOGNITION WITH RBF NEURAL NETWORKS 705
TABLE VII
ERRORRATES OFDIFFERENT APPROACHES
Some other results recently performed on the ORL database
arelistedinTableVIIIasreferences(theseresultsaretabulated
separately from Table VII because we are not aware of how
theirexperimentsareexactlyperformed).Itshouldbenotedthatsomeapproachesuseddifferentnumberoftrainingdata(forex-ample,onlyonetrainingpatternperpersonisusedin[39],[40],
and eight patterns per person in [29]); some results were eval-
uated based on the best performance of one run, such as [41],[42]; some experiments were performed based on part of thedatabase [40]. It is not clear how the experiments were carried
outandhowtheperformanceswereevaluatedin[12],[43]‚Äì[45].
Itisnotfairtocomparetheperformancesunderdifferentexper-imental conditions.
VII. D
ISCUSSION
Inthispaper,ageneralandefficientapproachfordesigningan
RBF neural classifier to cope with high-dimensional problemsinfacerecognitionispresented.Forthetimebeing,manyalgo-rithms have been proposed to configure RBF neural networks
forvariousapplicationsincludingfacerecognition,asshownin
[19]‚Äì[29]. Here, we would like to provide more insights intothesealgorithmsandcomparetheirperformanceswithourpro-posed method.
A. Face Features, Classifiers, and Performances
Here, the face features are first extracted by the PCA par-
adigm so that the resulting data are compressed significantly.
Then, the information is further condensed via the FLD ap-proach. Corresponding to Tables II and III in which the pat-terns are obtained from the PCA
FLD, the data distribution
resultingfromthePCAandtheclusteringerrorsfortrainingpat-
ternsbasedonourproposedapproacharetabulatedinTablesIXand X. Comparing Tables II and III with Tables IX and X, wehavethefollowingobservations:1)Classoverlappinggradually
reduces along with decrease in the number of feature vectors
for the data resulting from both the PCA and the PCA
FLD
methods; 2) For the data from the PCA, the clustering errorsincrease along with decrease in the feature dimension, but the
clusteringerrorsdecreaseforthedatafromthePCA
FLD;and
3) The data from the PCA FLD are still overlapping without
completeseparationunlessthefeaturedimensionislessthan20.However,theFLDindeedalleviatestheclassoverlappingasev-
idenced in comparing Tables IX and II.
Different face features are then used for testing by different
classifiers. Figs. 8 and 9 illustrate the effect of data dimensionresultedfromthePCAandthePCA
FLDmethodsonperfor-
mance classified by the nearest neighbor method. We see thatTABLE VIII
OTHERRESULTSRECENTLY PERFORMED ON THE ORL DATABASE
TABLE IX
DATADISTRIBUTION RESULTED FROM THE PCA BASED ON THE PROPOSED
APPROACH (THERESULTSAREOBTAINEDBASED ONSIXSIMULATIONS )
TABLE X
CLUSTERING ERRORS FOR TRAININGPATTERNS RESULTED FROM THE PCA
(THERESULTIS THESUM OFSIXSIMULATIONS )
more information (more dimensions) result in higher perfor-
manceinthePCA.However,theperformanceresultingfromthe
PCAFLDisnotmonotonicallyimprovedalongwithincrease
inthefeaturedimension,andthebestperformanceisalittlede-crease in PCA
FLD because of information loss.3. Table XI
illustratestheperformancesbyusingdifferentfacefeaturesand
classifiers.
As foreshadowed earlier, the FLD is a linear transformation
and the data resulting from this criterion are still heavily over-
lapping.Itisalsoindicatedin[34]thatthisseparabilitycriterion
3This is also indicated in several papers, for example, [35] and [36]
706 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 13, NO. 3, MAY 2002
Fig.8. ErrorrateasafunctionofpatterndimensioninPCA(thisistheaverage
result of two runs).
Fig. 9. Error rate as a function of pattern dimension in PCA /43FLD (this is
the average result of two runs).
isnotdirectlyrelatedtotheclassificationaccuracyintheoutput
space. Accordingly, nonlinear discriminant analysis is neces-sary for classification among which neural networks are one ofthe popular approaches [15].
The advantage of neural classifiers over linear classifiers is
thattheycanreducemisclassificationsamongtheneighborhoodclasses as shown in Fig. 10. However, this ability will gradu-ally decrease along with increase in the feature dimension. We
can seefromTableXI thatthe performancegainedbythe PCA
FLD is better than that obtained from the PCA. This is be-
cause the FLD can alleviate data overlapping, and reductionin the number of feature dimension moderates the architecture
complexity of the RBF neural classifier and reduces the com-
putational burden significantly in order to avoid overtrainingand overfitting. However, for those data falling into nonnearestclassesasshowninFig.10,theneuralclassifierstillcannotclas-
sify correctly.
It has been reported in [11] that error rates of the multilayer
networks (MLNs) classifier are 41.2% and 39.6% respectivelyontheORLdatabasewhenthefeaturesareextractedbythePCA
andtheSOMrespectively.Ourproposedapproach,whichisdif-
ferent from the MLN, wherein a particular supervised learningparadigm is employed, is a tremendous improvement over theresultsofMLN,CNN[11]andtheRBFmethodshownin[29].TABLE XI
PERFORMANCE COMPARISONS OF VARIOUSFACEFEATURES AND CLASSIFIERS
Fig. 10. An RBF neural classifier versus a linear classifier.
B. Training Samples versus Performances
Due to the fact that there are very small sample patterns for
each face in the ORL database, and further, as mentioned inSectionIthatsimilaritiesbetweenthedifferentfaceimageswiththe same pose are almost always larger than those between the
samefaceimagewithdifferentposes,thechoiceoftrainingdata
is consequently very crucial for generalization of RBF neuralnetworks. If thetraining dataare representativeof face images,thegeneralizationofRBFneuralclassifierimpliestointerpolate
thetesting data.Otherwise, it meansto predict thetesting data.
From the viewpoint of images, it is shown that the proposed
approach is not as sensitive to illumination (see Fig. 11), asother paradigms do [2], [6]. Usually, the proposed method also
discounts the variations of facial pose, expression and scale
when such variations are presented in the training patterns. Ifthe training patterns are not representative of image variationswhich appear during the testing phase, say, upward, then the
face turning up in the testing phase will not be recognized
correctly, as shown in Fig. 11. According to this principle,another database consisting of 600 face images of 30 individ-uals, which comprise different poses (frontal shots, upward,
downward, up-right, down-left and so on), and high degree
of variability in facial expression, has been set up by us. Allthe images were taken under the same background with theresolution of 160
120 pixels. A total of 300 face images, in
which each person has ten images, were selected to represent
differentposesandexpressionsasthetrainingset.Another300images were used as the testing set. Our results demonstratedthat the success rate of recognition is 100%.
ERet al.: FACE RECOGNITION WITH RBF NEURAL NETWORKS 707
(a)
(b)
Fig.11. Anexampleofincorrectclassification:(a)trainingimagesand(b)test
images.
C. Initialization versus Performances
1) Selection of Gaussian Centers : Several paradigms have
been proposed for kernel location estimation. The simplest ap-proachistoselectfromthetrainingdataasshownin[20],[23],[25],[28].Othertypicaltechniquescanbefoundbyusingclus-
teringalgorithms[19],[21]ormedianoperation[26].Ifwealso
selectthesamesixgroupsoffacedataresultingfromthePCA
FLDwithafeaturedimensionof40,theinitialclusteringerrorsfor training data by other clustering algorithms are tabulated in
Table XII.
WecanseefromTableXIIthatmanydataaremisclassifiedby
theunsupervised
-meansclusteringmethodandtheregression
clusteringmethod[23].Italsoimpliesthatthesedataaresignifi-
cantlyoverlapped.However,theclusteringerrorwillberemark-
ablyreducedifthecategoryinformationaboutpatternsisused,forexample,theMRBFparadigm[26]showninTableXII,andour proposed method achieves the best clustering performance
as shown in Table III.
2) Determination of Gaussian Widths: The appropriate es-
timationofwidthsofGaussianunitsisverysignificantforgen-eralization [20], [22], [24]‚Äì[29]. Frequently, the widths are se-
lectedbyheuristics[19],[21],[23],[29].Alsomanyresearchers
choosethewidthsasthecommonvariance(CV)(i.e.,calculatedoverallsamplepatterns)[20],[33]ortheclass-relatedvariance
(CRV)(i.e.,calculatedoverallthepatternsbelongingtothecor-
respondingclass)[24],[27].Recently,somenewmethodshavebeen proposed to estimate the widths, for example, the samplecovariance (i.e., the class-related variance) plus common co-
variance (SCCC) [32], the minimum distance between cluster
centers (i.e., using
) [25], the median operation (MO) [26],
or the evolutionary optimization [28]. If we use the same sixgroups of data resulting from the PCA
FLD, where the cen-
tersaredeterminedbyourproposedclusteringmethod,andthe
clusternumberisstill40,theinitialclusteringerrorsindifferentwidthschosenbydifferentmethodsaretabulatedinTableXIII.TableXIVillustratesthegeneralizationperformancefortesting
patterns performed on the ORL database.
ItisshownfromTableXIIIthattheSCCCmethodisthebest
methodtodescribethetrainingpatterns.However,amethodfor
good description of training patterns does not imply that it has
good generalization, as we see from Table XIV. On the otherhand, the testing errors before learning for the MO and CRVapproaches are very high (the total NOM‚Äôs are 95 and 117, re-TABLE XII
CLUSTERING ERRORS FOR TRAININGPATTERNS BY OTHERCLUSTERING
ALGORITHMS (THERESULTIS THESUM OFSIXSIMULATIONS )
TABLE XIII
CLUSTERING ERRORS FOR TRAININGPATTERNS CONSIDERING WIDTHS
CHOSEN BY DIFFERENT METHODS(THERESULTIS THESUM OFSIX
SIMULATIONS )
* The widths are best chosen for each case
TABLE XIV
GENERALIZATION ERRORS FOR TESTINGDATA BYDIFFERENT INITIALWIDTHS
(THERESULTIS THESUM OFSIXSIMULATIONS )
* Theresults areobtained whenthefeature dimension is30
spectively).Buttheirfinalperformancesafterlearningarecom-
parabletootherparadigms(CV,SCCCand ).Theoretically,
the final results should be the same regardless of initial param-
etersif thelearningalgorithmisgoodenoughforoptimization.The discrepancies are mainly caused by overfitting and over-training due to small sets of patterns with high dimension.
Two unsupervised algorithms with growing structure, i.e.,
ORBF [23] and D-FNN [25] have been employed to test theORLdatabase.TablesXVandXVIillustratethegeneralizationresults for the first group data with different clusters. We see
that the loss of category information will be at the cost of
more clusters for the comparable performance. However, itshouldbenotedthattheincreaseofclustersislikelytoresultinoverfitting, as shown in Table XVI.
708 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 13, NO. 3, MAY 2002
TABLE XV
GENERALIZATION ERRORS FOR TESTINGDATA BY THE ORBF M ETHOD
TABLE XVI
GENERALIZATION ERRORS FOR TESTINGDATA BY THE D-FNN M ETHOD
D. The Problem of Small Sample Sets
One of the difficulties for neural networks to train in face
recognition is the small sample data. This severe limitation al-
ways results in poor generalization. An alternative paradigm to
improve generalization is to adopt the technique of regulariza-tion [15], [33], i.e., to encourage smoother network mappingsby adding a penalty term
to the error function. But, the ap-
propriatechoiceofregularizationparameteristimeconsuming.
Another direct method to improve generalization is to use
more data patterns, i.e., by adding some patterns with noise[46].Accordingtotheprocedureproposedin[46],anothersetof
training patterns with noise randomly chosen from the uniform
distribution is replenished. The learning algorithm is executedwith and without adding noise to the inputs. Our experimentsshow that if the variance of noise is small, there is no effect ongeneralization, whereas large variance of noise will deteriorate
the performance.
High dimension may be one of the reasons that lead to poor
generalization.Asthevaluesineachdimensionvarygreatlyanddifferentfeatureshavedifferentinfluencesforfacerecognition,
auniformlydistributednoisemayaffectsomefeaturessubstan-
tiallyandhasnoinfluenceonsomeotherfeatures.Ontheotherhand,differentfeaturesmayhavedifferentweightsfordifferentface features. Therefore, normalization of the inputs should be
taken when the noise is injected into the inputs.
Anotherreasonmaybeduetothepresuppositionthattheac-
quisitionofgeneralizationcapabilitybynoiseinjectionintotheinputsreliesontheassumptionthatthemappingfromtheinput
space to the output space should be smooth [46]. For high-di-
mensional classifications, it is not easy to determine whetherthe assumption could be satisfied in advance.
VIII. C
ONCLUSION
It is well known that if the dimension of the network input
is comparable to the size of the training set, which is the usual
caseinfacerecognition,thesystemwilleasilybringaboutover-
fitting and result in poor generalization. In this paper, a gen-eral design approach using an RBF neural classifier for facerecognitiontocopewithsmalltrainingsetsofhigh-dimensional
problem is presented. Firstly, face features are first extracted
by the PCA. Then, the resulting features are further projectedinto the Fisher‚Äôs optimal subspace in which the ratio of the be-tween-classscatterandthewithin-classscatterismaximized.Anovelparadigm,wherebytrainingdatainformationisusedinthechoice of structure and parameters of RBF neural networks be-
forelearningtakesplace,ispresented.Finally,ahybridlearningalgorithm is proposed to train the RBF neural networks. Simu-
lation results show that the system achieves excellent perfor-
mancebothintermsoferrorratesofclassificationandlearningefficiency.
In this paper, the feature vectors are only extracted from
grayscale information. More features extracted from both
grayscale and spatial texture information and a real-time
face detection and recognition system are currently underdevelopment.
A
CKNOWLEDGMENT
TheauthorswishtogratefullyacknowledgeDr.S.Lawrence
and Dr. C. L. Giles for providing details of their experimentson their CNN approach and Dr. A. Bors for his discussionsand comments. Also, the authors would like to express sincere
thankstotheanonymousreviewersfortheirvaluablecomments
that greatly improved the quality of the paper.
R
EFERENCES
[1] R.Chellappa,C.L.Wilson,andS.Sirohey,‚ÄúHumanandmachinerecog-
nition of faces: A survey,‚Äù Proc. IEEE ,vol. 83, pp. 705‚Äì740, 1995.
[2] Y. Moses, Y. Adini, and S. Ullman, ‚ÄúFace recognition: The problem of
compensating for changes in illumination direction,‚Äù in Proc. EuroP.
Conf. Comput. Vision , vol. A, 1994, pp. 286‚Äì296.
[3] R. Brunelli and T. Poggio, ‚ÄúFace recognition: Features versus tem-
plates,‚ÄùIEEE Trans. Pattern Anal. Machine Intell. , vol. 15, pp.
1042‚Äì1053, 1993.
[4] P. J. Phillips, ‚ÄúMatching pursuit filters applied to face identification,‚Äù
IEEE Trans. ImageProcessing , vol. 7, pp. 1150‚Äì1164, 1998.
[5] Z. Hong, ‚ÄúAlgebraic feature extraction of image for recognition,‚Äù Pat-
tern Recognition , vol. 24, pp. 211‚Äì219, 1991.
[6] M.A.TurkandA.P.Pentland,‚ÄúEigenfacesforrecognition,‚Äù J.Cognitive
Neurosci. , vol. 3, pp. 71‚Äì86, 1991.
[7] P.N.Belhumeur,J.P.Hespanha,andD.J.Kriegman,‚ÄúEigenfacesversus
fisherfaces: Recognition using class specific linear projection,‚Äù IEEE
Trans. Pattern Anal. Machine Intell. , vol. 19, pp. 711‚Äì720, 1997.
[8] D. L. Swets and J. Weng, ‚ÄúUsing discriminant eigenfeatures for image
retrieval,‚Äù IEEE Trans. Pattern Anal. Machine Intell. , vol. 18, pp.
831‚Äì836, 1996.
[9] D.Valentin, H.Abdi,A.J.O‚ÄôToole, andG. W.Cottrell,‚ÄúConnectionist
modelsoffaceprocessing:ASurvey,‚Äù PatternRecognition ,vol.27,pp.
1209‚Äì1230, 1994.
[10] J.MaoandA.K.Jain,‚ÄúArtificialneuralnetworksforfeatureextraction
andmultivariatedataprojection,‚Äù IEEETrans.NeuralNetworks ,vol.6,
pp. 296‚Äì317, Mar. 1995.
[11] S. Lawrence, C. L. Giles, A. C. Tsoi, and A. D. Back, ‚ÄúFace recogni-
tion: A convolutional neural-network approach,‚Äù IEEE Trans. Neural
Networks , vol. 8, pp. 98‚Äì113, Jan. 1997.
[12] S.-H. Lin, S.-Y. Kung, and L.-J. Lin, ‚ÄúFace recognition/detection by
probabilistic decision-based neural network,‚Äù IEEE Trans. Neural Net-
works, vol. 8, pp. 114‚Äì132, Jan. 1997.
[13] K. Fukunaga, Introduction to Statistical Pattern Recognition , 2nd
ed. San Diego, CA: Academic Press, 1990.
[14] H. H. Song and S. W. Lee, ‚ÄúA self-organizing neural tree for large-set
pattern classification,‚Äù IEEE Trans. Neural Networks , vol. 9, pp.
369‚Äì380, Mar. 1998.
[15] C.M.Bishop, NeuralNetworksforPatternRecognition ,NewYork:Ox-
ford Univ. Press.
[16] J.L.YuanandT.L.Fine,‚ÄúNeural-Networkdesignforsmalltrainingsets
ofhighdimension,‚Äù IEEETrans.NeuralNetworks ,vol.9,pp.266‚Äì280,
Jan. 1998.
[17] J. Park and J. Wsandberg, ‚ÄúUniversal approximation using radial basis
functions network,‚Äù Neural Comput. , vol. 3, pp. 246‚Äì257, 1991.
[18] F. Girosi and T. Poggio, ‚ÄúNetworks and the best approximation prop-
erty,‚ÄùBiol. Cybern. , vol. 63, pp. 169‚Äì176, 1990.
ERet al.: FACE RECOGNITION WITH RBF NEURAL NETWORKS 709
[19] J. Moody and C. J. Darken, ‚ÄúFast learning in network of locally-tuned
processing units,‚Äù Neural Comput. , vol. 1, pp. 281‚Äì294, 1989.
[20] S. Lee and R. M. Kil, ‚ÄúA Gaussian potential function network with
hierarchically self-organizing learning,‚Äù Neural Networks , vol. 4, pp.
207‚Äì224, 1991.
[21] W. Pedrycz, ‚ÄúConditional fuzzy clustering in the design of radial basis
function neural networks,‚Äù IEEE Trans. Neural Networks , vol. 9, pp.
601‚Äì612, July 1998.
[22] M.T.Musavi,W.Ahmed,K.H.Chan,K.B.Faris,andD.M.Hummels,
‚ÄúOn the training of radial basis function classifiers,‚Äù Neural Networks ,
vol. 5, pp. 595‚Äì603, 1992.
[23] S. Chen, C. F. N. Cowan, and P. M. Grant, ‚ÄúOrthogonal least squares
learning algorithm for radial basis function network,‚Äù IEEE Trans
Neural Networks , vol. 2, pp. 302‚Äì309, 1991.
[24] G.BorsandM.Gabbouj,‚ÄúMinimaltopologyforaradialbasisfunctions
neural networks for pattern classification,‚Äù Digital Processing , vol. 4,
pp. 173‚Äì188, 1994.
[25] S.WuandM.J.Er,‚ÄúDynamicfuzzyneuralnetworks:Anovelapproach
to function approximation,‚Äù IEEE Trans. Syst, Man, Cybern , pt. B: Cy-
bern, vol. 30, pp. 358‚Äì364, 2000.
[26] A. G. Bors and I. Pitas, ‚ÄúMedian radial basis function neural network,‚Äù
IEEE Trans. Neural Networks , vol. 7, pp.1351‚Äì1364, Sept. 1996.
[27] N.B.KarayiannisandG.W.Mi,‚ÄúGrowingradialbasisneuralnetworks:
Merging supervised and unsupervised learning with network growth
techniques,‚Äù IEEE Trans. Neural Networks , vol. 8, pp. 1492‚Äì1506,
Nov. 1997.
[28] A. Esposito, M. Marinaro, D. Oricchio, and S. Scarpetta, ‚ÄúApproxima-
tion of continuous and discontinuous mappings by a growing neural
RBF-based algorithm,‚Äù Neural Networks ,vol. 12,pp. 651‚Äì665,2000.
[29] E.-D.Virginia,‚ÄúBiometricidentificationsystemusingaradialbasisnet-
work,‚Äùin Proc34thAnnu.IEEEInt.CarnahanConf.SecurityTechnol. ,
2000, pp. 47‚Äì51.
[30] S. J. Raudys and A. K. Jain, ‚ÄúSmall sample size effects in statistical
pattern recognition: Recommendations for practitioners,‚Äù IEEE Trans.
Pattern Anal. Machine Intell. , vol. 13, pp. 252‚Äì264, 1991.
[31] L.O.JimenezandD.A.Landgrebe,‚ÄúSupervisedclassificationinhigh-
dimensionalspace:Geometrical,statistical,andasymptoticalpropertiesof multivariate data,‚Äù IEEE Trans. Syst, Man, Cybern. C , vol. 28, pp.
39‚Äì54, 1998.
[32] J.P.HoffbeckandD.A.Landgrebe,‚ÄúCovariancematrixestimationand
classificationwithlimitedtrainingdata,‚Äù IEEETrans.PatternAnal.Ma-
chine Intell. , vol. 18, pp. 763‚Äì767, 1996.
[33] S.Haykin, Neural Networks,A Comprehensive Foundation ,New York:
Macmillan, 1994.
[34] R. Lotlikar and R. Kothari, ‚ÄúFractional-step dimensionality reduction,‚Äù
IEEE Trans.Pattern Anal.Machine Intell. , vol.22,pp. 623‚Äì627,2000.
[35] G. Donato, M. S. Bartlett, J. C. Hager, P. Ekman, and T. J. Sejnowski,
‚ÄúClassifyingfacialactions,‚Äù IEEETrans.PatternAnal.MachineIntell. ,
vol. 21, pp. 974‚Äì989, 1999.
[36] C. Liu and H.Wechsler, ‚ÄúLearning the facespace‚ÄîRepresentation and
recognition,‚Äù in Proc. 15th Int. Conf. Pattern Recognition , Spanish,
2000, pp. 249‚Äì256.
[37] S. Z. Li and J. Lu, ‚ÄúFace recognition using the nearest feature line
method,‚Äù IEEE Trans. Neural Networks , vol. 10, pp. 439‚Äì443, Mar.
1999.
[38] V. Brennan and J. Principe, ‚ÄúFace classification using a multiresolution
principal component analysis,‚Äù in Proc. IEEE Workshop Neural Net-
works Signal Processing , 1998, pp. 506‚Äì515.
[39] K.-M.LamandH.Yan,‚ÄúAnanalytic-to-holisticapproachforfacerecog-
nition based on a single frontal view,‚Äù IEEE Trans. Pattern Anal. Ma-
chine Intell. , vol. 20, pp. 673‚Äì686, 1998.
[40] T.Phiasai,S.Arunrungrusmi,andK.Chamnongthai,‚ÄúFacerecognition
system with PCA and moment invariant method,‚Äù in Proc. IEEE Int.
Symp. Circuits Syst. , 2001, pp. II165‚ÄìII168.
[41] Z.Jin,J.-Y.Yang,Z.-S.Hu,andZ.Lou,‚ÄúFacerecognitionbasedonthe
uncorrelateddiscriminanttransformation,‚Äù PatternRecognition ,vol.34,
pp. 1405‚Äì1416, 2001.
[42] A.S.TolbaandA.N.Abu-Rezq,‚ÄúCombinedclassifiersforinvariantface
recognition,‚Äùin Proc.Int.Conf.Inform.Intell.Syst. ,1999,pp.350‚Äì359.
[43] S.Eickeler,S.Mueller,andG.Rigoll,‚ÄúHighqualityfacerecognitionin
JPEGcompressedimages,‚Äùin Proc.IEEEInt. Conf.ImageProcessing ,
1999, pp. 672‚Äì676.
[44] T. Tan and H. Yan, ‚ÄúObject recognitionusingfractal neighbordistance:
Eventual convergence and recognition rates,‚Äù in Proc. 15th Int. Conf.
Pattern Recognition , 2000, pp. 781‚Äì784.[45] B.-L. Zhang and Y. Guo, ‚ÄúFace recognition by wavelet domain asso-
ciative memory,‚Äù in Proc. Int. Symp. Intell. Multimedia, Video, Speech
Processing , 2001, pp. 481‚Äì485.
[46] K. Matsuoka, ‚ÄúNoise injection into inputs in back-propagation
learning,‚Äù IEEETrans.Syst.,Man,Cybern. ,vol.22,pp.436‚Äì440,1992.
Meng Joo Er (S‚Äô82‚ÄìM‚Äô85) received the B.Eng. and
M.Eng. degrees in electrical engineering from the
National University of Singapore, Singapore, and
the Ph.D. degree in systems engineering from the
Australian National University, Canberra, in 1985,1988, and 1992, respectively.
From1987to1989,hewasasaResearchandDe-
velopment Engineer in Chartered Electronics Indus-
tries Pte Ltd, Singapore, and a Software Engineer in
Telerate Research and Development Pte Ltd, Singa-
pore,respectively.HeiscurrentlyanAssociateProfessorintheSchoolofElec-
trical and Electronic Engineering (EEE), Nanyang Technological University(NTU).FromFebruarytoApril1998,hewasinvitedasaGuestResearcheratthe
PrecisionInstrumentDevelopmentCenterofNationalScienceCouncil,Taiwan.
He served as a consultant to Growth Venture Pte Ltd., Singapore, from Feb-
ruary1999toMay,2000.HewasinvitedasapanelisttotheIEEE-INNS-ENNS
International Conference on Neural Networks held from 24 to 27 July, 2000,Como, Italy. He has been Editor of IES Journal on Electronics and Computer
Engineering since 1995. He is currently a member of the editorial board of the
InternationalJournalof ComputerResearch and theguest editorof thespecial
issue on intelligent control of mobile robots. He has authored numerous pub-
lished works in his research areas of interest, which include control theory and
applications, robotics and automation, fuzzy logic and neural networks, artifi-
cialintelligence,biomedicalengineering,parallelcomputing,powerelectronicsand drives and digital signal processors applications.
Dr.ErwastherecipientoftheInstitutionofEngineers,Singapore(IES)Pres-
tigiousPublication(Application)Awardin1996andtheIESPrestigiousPubli-
cation (Theory) Awardin 2001. Hereceivedthe Teacher of the Year Awardfor
the School of EEE for his excellent performance in teaching in 1999. He was
also a recipient of a Commonwealth Fellowship tenable at the University of
Strathclyde, U.K., from February to October 2000. He is a member of IES. Hehas served in these professional organizations in various capacities, including
Chairman of Continuing Education Subcommittee of IEEE Control Chapter in
1993, Honorary Secretary of the IEEE Singapore Section from 1994 to 1996,
Vice-Chairman of IEEE Singapore Section and Chairman of Continuing Edu-
cationSubcommittee,IEEESingaporeSectionfromMarch,1998toDecember1999. From February 1997 to February, 1999, he also served as a member of
the Singapore Science Centre Board. Currently, he is serving as the First Vice-
Chair of the Action Group on Educational Institutions, Uniform Groups and
Youth of Publication Education Committee on Family. He has also been very
active in organizing international conferences. He was a member of the main
organizing committees of International Conference on Control, Automation,
Robotics and Vision (ICARCV) for four consecutive years, 1994, 1996, 1998,and 2000,and theAsianConference on ComputerVision(ACCV) in1995.He
wastheCochairmanofTechnicalProgrammeCommitteeandperson-in-charge
of invited sessions for ICARCV‚Äô96 and ICARCV‚Äô98. He was in charged of
International Liaison for ICARCV‚Äô2000. He is currently serving as the Tech-
nical Program Chair for ICARCV‚Äô2002. He has also served as a member of
the International Scientific Committee of the International Conference on Cir-
cuits, Systems, Communications and Computers (CSC) since 1998. He was amember of the International Scientific Committee of a unique three confer-
ences series on Soft Computing consisting of 2001 World Scientific and En-
gineering Society (WSES) International Conference on Neural Networks and
Applications, 2001 WSES International Conference on Fuzzy Sets and Fuzzy
Systems,and2001WSESInternationalConferenceonEvolutionaryComputa-tion.HehasalsobeeninvitedtoserveasGeneralChairofthe2001WSESCon-
ference on Robotics, Distance Learning and Intelligent Communications Sys-
tems (RODLICS), International Conference on Speech, Signal and Image Pro-
cessing 2001 (SSIP‚Äô2001), International Conference on Multimedia, Internet,
Video Technologies (MIV‚Äô2001) and International Conference on Simulation
(SIM‚Äô2001)organizedbyWSES.HehasbeenlistedinWho‚ÄôsWhointheWorld
since1998andwasnominatedManoftheYear1999bytheAmericanBiograph-ical Institute Board of International Research.
710 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 13, NO. 3, MAY 2002
Shiqian Wu (M‚Äô02) received the B.S. and M.Eng.
degrees, both from Huazhong University of Science
and Technology, Wuhan, China, in 1985 and 1988,
respectively. He received the Ph.D. degree in elec-
tricalandelectronicengineeringfromNanyangTech-
nological University, Singapore, Singapore, in June2001.
From 1988 to 1997, he was a Lecturer and then
Associate Professor in Huazhong University of
Science and Technology. Since August 2000, he
has been with the Centre for Signal Processing,
Singapore. Currently, his research interests include neural networks, fuzzy
systems, computer vision, face detection and recognition, and infrared image
analysis. He has published more than 20 papers.
Juwei Lu (M‚Äô99‚ÄìS‚Äô01) received the Bachelor of Electronic Engineering de-
greefromNanjingUniversityofAeronauticsandAstronautics,China,in1994,
and the Master of Engineering degree from the School of Electrical and Elec-
tronic Engineering, Nanyang Technological University, Singapore, Singapore,
in 1999.Currently, he is pursuing the Ph.D. degree in the Department of Elec-
tricalandComputerEngineering,UniversityofToronto,Toronto,ON,Canada.
From July 1999 to January 2000, he was with the Centre for Signal Pro-
cessing, Singapore, Singapore, as a Research Engineer.
HockLyeToh (S‚Äô85‚ÄìM‚Äô88)receivedtheB.Eng.de-
greefromtheSchoolofElectricalandElectronicEn-
gineering,NanyangTechnologicalUniversity,Singa-
pore, Singapore, in 1988.
HejoinedtheSignalProcessingLaboratoryofDe-
fence Science Organization in 1988. Prior to joiningCentre for Signal Processing as a Senior Research
Engineer in 1997, he was a Senior Engineer holding
GroupHeadappointment.From1988to1997,hehas
workedonalgorithmdevelopmentprojectsonimage
enhancement and restoration, design, and develop-
ment of real-time computation engine using high-performance digital signal
processors,andprogrammablelogicdevices.HewasProjectLeaderforthede-
velopment of a few real-time embedded systems for defense applications. He
wasGroupHead,ImagingElectronicsGroupfrom1993andGroupHead,ImageProcessing Group from 1995. He was appointed Program Manager, Centre for
Signal Processing from 1998. He is spearheading a few research and develop-
ment projects on embedded video processing and human thermogram analysis
for multimedia, biometrics, and biomedical applications. He has led numerous
in-house and industry projects, filed a patent and submitted a few invention
disclosures. His research interests are embedded processing core development,
thermogram video signal processing, and 3-D signal reconstruction.
Mr. Toh is a member of the Institution of Engineers, Singapore.
"
https://ieeexplore.ieee.org/document/554195,"98 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 8, NO. 1, JANUARY 1997
Face Recognition: A Convolutional
Neural-Network Approach
Steve Lawrence, Member, IEEE, C. Lee Giles, Senior Member, IEEE, Ah Chung Tsoi, Senior Member, IEEE,
and Andrew D. Back, Member, IEEE
Abstract‚Äî Faces represent complex multidimensional mean-
ingful visual stimuli and developing a computational model forface recognition is difÔ¨Åcult. We present a hybrid neural-networksolution which compares favorably with other methods. Thesystem combines local image sampling, a self-organizing map
(SOM) neural network, and a convolutional neural network.
The SOM provides a quantization of the image samples into atopological space where inputs that are nearby in the originalspace are also nearby in the output space, thereby providingdimensionality reduction and invariance to minor changes in theimage sample, and the convolutional neural network provides forpartialinvariance to translation,rotation, scale, anddeformation.The convolutional network extracts successively larger featuresin a hierarchical set of layers. We present results using theKarhunen‚ÄìLo `eve (KL) transform in place of the SOM, and
a multilayer perceptron (MLP) in place of the convolutionalnetwork. The KL transform performs almost as well (5.3% errorversus 3.8%). The MLP performs very poorly (40% error versus3.8%). The method is capable of rapid classiÔ¨Åcation, requiresonly fast approximate normalization and preprocessing, andconsistently exhibits better classiÔ¨Åcation performance than theeigenfaces approach on the database considered as the numberof images per person in the training database is varied fromone to Ô¨Åve. With Ô¨Åve images per person the proposed methodand eigenfaces result in 3.8% and 10.5% error, respectively. Therecognizer provides a measure of conÔ¨Ådence in its output andclassiÔ¨Åcation error approaches zero when rejecting as few as 10%oftheexamples.Weuseadatabaseof400imagesof40individuals
which contains quite a high degree of variability in expression,
pose,andfacialdetails.Weanalyzecomputationalcomplexityanddiscuss how new classes could be added to the trained recognizer.
Index Terms‚Äî Facerecognition,convolutionalneuralnetworks,
self-organizing feature maps, Karhunen‚ÄìLo `eve transforms,
hybrid systems, access control, pattern recognition, imageclassiÔ¨Åcation.
I. INTRODUCTION
THE REQUIREMENT for reliable personal identiÔ¨Åcation
in computerized access control has resulted in an in-
creased interest in biometrics.1Biometrics being investigated
Manuscript received January 1, 1996; revised June 13, 1996. This work
was supported in part by the Australian Research Council (ACT) and the
Australian Telecommunications and Electronics Research Board (SL).
S. Lawrence is with the NEC Research Institute, Princeton, NJ 08540 USA.
He is also with the Department of Electrical and Computer Engineering,
University of Queensland, St. Lucia, Australia.
C. L. Giles is with the NEC Research Institute, Princeton, NJ 08540 USA.
He is also with the Institute for Advanced Computer Studies, University of
Maryland, College Park, MD 20742 USA.
A. C. Tsoi and A. D. Back are with the Department of Electrical and
Computer Engineering, University of Queensland, St. Lucia, Australia.
Publisher Item IdentiÔ¨Åer S 1045-9227(97)00234-8.
1Physiological or behavioral characteristics which uniquely identify us.include Ô¨Ångerprints [4], speech [7], signature dynamics [36],
and face recognition [8]. Sales of identity veriÔ¨Åcation products
exceed $100 million [29]. Face recognition has the beneÔ¨Åt ofbeing a passive, nonintrusive system for verifying personalidentity. The techniques used in the best face recognition
systems may depend on the application of the system. We
can identify at least two broad categories of face recognitionsystems.
1) We want to Ô¨Ånd a person within a large database of
faces(e.g.,inapolicedatabase).Thesesystemstypicallyreturn a list of the most likely people in the database[34]. Often only one image is available per person. It
is usually not necessary for recognition to be done in
real-time.
2) We want to identify particular people in real-time (e.g.,
in a security monitoring system, location tracking sys-
tem, etc.), or we want to allow access to a group of
people and deny access to all others (e.g., access to abuilding, computer, etc.) [8]. Multiple images per personare often available for training and real-time recognition
is required.
In this paper, we are primarily interested in the second
case.
2We are interested in recognition with varying facial
detail, expression, pose, etc. We do not consider invariance to
high degrees of rotation or scaling‚Äîwe assume that a minimalpreprocessing stage is available if required. We are interestedin rapid classiÔ¨Åcation and hence we do not assume that time is
available for extensive preprocessing and normalization. Good
algorithms for locating faces in images can be found in [37],[40], and [43].
The remainder of this paper is organized as follows. The
data we used is presented in Section II and related work
with this and other databases is discussed in Section III.The components and details of our system are described inSections IV and V, respectively. We present and discuss our
results in Sections VI and VII. Computational complexity is
considered in Section VIII, Section IX listsavenues forfurtherresearch, and we draw conclusions in Section X.
II. D
ATA
We have used the ORL database, which contains a set of
faces taken between April 1992 and April 1994 at the Olivetti
2However,wehavenotperformedanyexperimentswherewehaverequired
the system to reject people that are not in a select group (important, for
example, when allowing access to a building).
1045‚Äì9227/97$10.00 Ô£©1997 IEEE
LAWRENCE et al.: FACE RECOGNITION 99
Fig. 1. The ORL face database. There are ten images each of the 40 subjects.
Research Laboratoryin Cambridge, U.K.3There are ten differ-
entimagesof40distinctsubjects.Forsomeofthesubjects,theimages were taken at different times. There are variations infacial expression (open/closed eyes, smiling/nonsmiling), and
facial details (glasses/no glasses). All the images were taken
against a dark homogeneous background with the subjects inan upright frontal position, with tolerance for some tilting androtation of up to about 20 degrees. There is some variation in
scale of up to about 10%. Thumbnails of all of the images are
shown in Fig. 1 and a larger set of images for one subject isshown in Fig. 2. The images are greyscale with a resolutionof 92
112.
3The ORL database is available free of charge, see http://www.cam-
orl.co.uk/facedatabase.html.III. RELATEDWORK
A. Geometrical Features
Manypeoplehaveexploredgeometricalfeaturebasedmeth-
ods for face recognition. Kanade [17] presented an automaticfeature extraction method based on ratios of distances andreportedarecognitionrateofbetween45‚Äì75%withadatabase
of 20 people. Brunelli and Poggio [6] compute a set of
geometrical features such as nose width and length, mouthposition, and chin shape. They report a 90% recognition rateon a database of 47 people. However, they show that a simple
template matching scheme provides 100% recognition for the
same database. Cox et al.[9] have recently introduced a
mixture-distance technique which achieves a recognition rate
100 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 8, NO. 1, JANUARY 1997
Fig. 2. The set of ten images for one subject. Considerable variation can be seen.
of 95% using a query database of 95 images from a total
of 685 individuals. Each face is represented by 30 manually
extracted distances.
Systems which employ precisely measured distances be-
tweenfeaturesmaybemostusefulforÔ¨Åndingpossiblematches
in a large mugshot database.4For other applications, automatic
identiÔ¨Åcation of these points would be required and the result-ing system would be dependent on the accuracy of the feature
location algorithm. Current algorithms for automatic location
of feature points do not provide a high degree of accuracy andrequire considerable computational capacity [41].
B. Eigenfaces
High-level recognition tasks are typically modeled with
manystagesofprocessingasintheMarrparadigmofprogress-
ing fromimagestosurfaces tothree-dimensional(3-D)models
to matched models [28]. However, Turk and Pentland [43]argue that it is likely that there is also a recognition processbased on low-level two-dimensional (2-D) image processing.
Their argument is based on the early development and extreme
rapidity of face recognition in humans and on physiologicalexperiments in a monkey cortex which claim to have isolated
neurons that respond selectively to faces [35]. However, it is
not clear that these experiments exclude the sole operation ofthe Marr paradigm.
Turk and Pentland [43] present a face recognition scheme in
whichfaceimagesareprojectedontotheprincipalcomponents
of the original set of training images. The resulting eigenfaces
are classiÔ¨Åed by comparison with known individuals.
Turk and Pentland present results on a database of 16
subjects with various head orientation, scaling, and lighting.
Their images appear identical otherwise with little variationin facial expression, facial details, pose, etc. For lighting,orientation, and scale variation their system achieves 96%,
85%, and 64% correct classiÔ¨Åcation, respectively. Scale is
renormalized to the eigenface size based on an estimate of thehead size. The middle of the faces is accentuated, reducingany negative affect of changing hairstyle and backgrounds.
In Pentland et al.[33], [34] good results are reported on
a large database (95% recognition of 200 people from a
4A mugshot database typically contains side views where the performance
of feature point methods is known to improve [8].database of 3000). It is difÔ¨Åcult to draw broad conclusions
as many of the images of the same people look very similar,and the database has accurate registration and alignment [30].In Moghaddam and Pentland [30], very good results are
reported with the FERET database‚Äîonly one mistake was
made in classifying 150 frontal view images. The system usedextensive preprocessing for head location, feature detection,and normalization for the geometry of the face, translation,
lighting, contrast, rotation, and scale.
Swets and Weng [42] present a method of selecting discrim-
inant eigenfeatures using multidimensional linear discriminantanalysis. They present methods for determining the most ex-
pressive features (MEF) and the most discriminatory features
(MDF).Wearenotcurrentlyawareoftheavailabilityofresultswhich are comparable with those of eigenfaces (e.g., on the
FERET database as in Moghaddam and Pentland [30]).
In summary, it appears that eigenfaces is a fast, simple,
and practical algorithm. However, it may be limited becauseoptimal performance requires a high degree of correlation be-
tween the pixel intensities of the training and test images. This
limitationhasbeenaddressedbyusingextensivepreprocessingto normalize the images.
C. Template Matching
Template matching methods such as [6] operate by perform-
ing direct correlation of image segments. Template matching
is only effective when the query images have the same scale,orientation, and illumination as the training images [9].
D. Graph Matching
Another approach to face recognition is the well known
method of graph matching. In [21], Lades et al.present
a dynamic link architecture for distortion invariant objectrecognition which employs elastic graph matching to Ô¨Åndthe closest stored graph. Objects are represented with sparse
graphs whose vertices are labeled with a multiresolution
description in terms of a local power spectrum, and whoseedges are labeled with geometrical distances. They presentgood results with a database of 87 people and test images
composed of different expressions and faces turned 15
. The
matchingprocessiscomputationallyexpensive,takingroughly25 s to compare an image with 87 stored objects when using
LAWRENCE et al.: FACE RECOGNITION 101
Fig. 3. A depiction of the local image sampling process. A window is stepped over the image and a vector is created at each location.
a parallel machine with 23 transputers. Wiskott et al.[45] use
an updated version of the technique and compare 300 faces
against 300 different faces of the same people taken from theFERET database. They report a recognition rate of 97.3%. Therecognition time for this system was not given.
E. Neural-Network Approaches
Much of the present literature on face recognition with
neural networks presents results with only a small number
of classes (often below 20). We brieÔ¨Çy describe a couple of
approaches.
In [10] the Ô¨Årst 50 principal components of the images are
extracted and reduced to Ô¨Åve dimensions using an autoassocia-
tive neural network. The resulting representation is classiÔ¨Åed
using a standard multilayer perceptron (MLP). Good resultsare reported but the database is quite simple: the pictures aremanually aligned and there is no lighting variation, rotation,
or tilting. There are 20 people in the database.
A hierarchical neural network which is grown automatically
and not trained with gradient descent was used for facerecognition by WengandHuang [44].They report good results
for discrimination of ten distinctive subjects.
F. The ORL Database
In [39] a hidden Markov model (HMM)-based approach is
used for classiÔ¨Åcation of the ORL database images. The best
model resulted in a 13% error rate. Samaria also performedextensive tests using the popular eigenfaces algorithm [43] onthe ORL database and reported a best error rate of around 10%
when the number of eigenfaces was between 175 and 199.
We implemented the eigenfaces algorithm and also observedaround 10% error. In [38] Samaria extends the top‚ÄìdownHMM of [39]with pseudo 2-DHMM‚Äôs. The errorrate reduces
to5%attheexpenseofhighcomputationalcomplexity‚Äîasin-
gle classiÔ¨Åcation takes 4 min on a Sun Sparc II. Samaria notesthat although an increased recognition rate was achieved thesegmentation obtained with the pseudo 2-D HMM‚Äôs appeared
quite erratic. Samaria uses the same training and test set sizes
as we do (200 training images and 200 test images with nooverlap between the two sets). The 5% error rate is the besterror rate previously reported for the ORL database that we
are aware of.
IV. S
YSTEMCOMPONENTS
A. Overview
In the following sections we introduce the techniques which
form the components of our system and describe our motiva-tion for using them. BrieÔ¨Çy, we explore the use of local image
sampling and a technique forpartial lighting invariance, a self-
organizing map (SOM) for projection of the image samplerepresentation into a quantized lower dimensional space, theKarhunen-Lo `eve (KL) transform for comparison with the
SOM, a convolutional network (CN) for partial translation and
deformation invariance, and an MLP for comparison with theconvolutional network.
B. Local Image Sampling
We have evaluated two different methods of representing
local image samples. In each method a window is scannedover the image as shown in Fig. 3.
1) The Ô¨Årst method simply creates a vector from a
local window on the image using the intensityvalues at each point in the window. Let
be
the intensity at the th column and the th row of
the given image. If the local window is a squareof sides
long, centered on , then
the vector associated with this window is simply
2) The second method creates a representation of the local
sample by forming a vector out of 1) the intensity of
the center pixel and 2) the difference in intensity
between the center pixel and all other pixels withinthe square window. The vector is given by
The resulting rep-
resentation becomes partially invariant to variationsin intensity of the complete sample. The degree of
102 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 8, NO. 1, JANUARY 1997
invariance can be modiÔ¨Åed by adjusting the weight
connected to the central intensity component.
C. The Self-Organizing Map
1) Introduction: Maps are an important part of both natural
andartiÔ¨Åcialneuralinformationprocessingsystems[2].Exam-
ples of maps in the nervous system are retinotopic maps in thevisual cortex [31], tonotopic maps in the auditory cortex [18],and maps from the skin onto the somatosensoric cortex [32].
The SOM, introduced by Kohonen [19], [20], is an unsuper-
vised learning process which learns the distribution of a set ofpatterns without any class information. A pattern is projectedfrom an input space to a position in the map‚Äîinformation is
coded as the location of an activated node. The SOM is unlike
most classiÔ¨Åcation or clusteringtechniques in thatit provides atopological ordering of the classes. Similarity in input patternsis preserved in the output of the process. The topological
preservation of the SOM process makes it especially useful
in the classiÔ¨Åcation of data which includes a large number ofclasses. In the local image sample classiÔ¨Åcation, for example,there may be a very large number of classes in which the
transition from one class to the next is practically continuous
(making it difÔ¨Åcult to deÔ¨Åne hard class boundaries).
2) Algorithm: We give a brief description of the SOM
algorithm, for more details see [20]. The SOM deÔ¨Ånes a
mapping from an input space
onto a topologically ordered
setofnodes,usuallyinalowerdimensionalspace.Anexampleof a 2-D SOM is shown in Fig. 4. A reference vector in the
input space,
is assigned to
each node in the SOM. During training, each input vector,
is compared to all of the obtaining the location of the
closest match (given by where
denotes the norm of vector The input point is mapped
to this location in the SOM. Nodes in the SOM are updatedaccording to
(1)
whereis the time during learning and is theneighbor-
hood function , a smoothing kernel which is maximum at
Usually, , where andrepresent
the location of the nodes in the SOM output space. is the
node with the closest weight vector to the input sample and
rangesoverallnodes. approaches0as increases
and also as approaches A widely applied neighborhood
function is
(2)
where is a scalar valued learning rate and deÔ¨Ånes
the width of the kernel. They are generally both monotonically
decreasing with time [20]. The use of the neighborhood
function means that nodes which are topographically close inthe SOM structure are moved toward the input pattern alongwith the winning node. This creates a smoothing effect which
leadstoaglobalorderingofthemap.Notethat
shouldnot
be reduced too far as the map will lose its topographical orderif neighboring nodes are not updated along with the closest
Fig. 4. A 2-D SOM showing a square neighborhood function which starts
as /104 /99/105 /40 /116 /49 /41and reduces in size to /104 /99/105 /40 /116 /51 /41over time.
node. The SOM can be considered a nonlinear projection of
the probability density, [20].
3) Improving the Basic SOM: The original SOM is com-
putationally expensive due to the following.
1) In the early stages of learning, many nodes are adjusted
in a correlated manner. Luttrel [27] proposed a method,
which is used here, that starts by learning in a smallnetwork,anddoublesthesizeofthenetworkperiodicallyduring training. When doubling, new nodes are inserted
between the current nodes. The weights of the new
nodes are set equal to the average of the weights ofthe immediately neighboring nodes.
2) Each learning pass requires computation of the distance
of the current sample to all nodes in the network, which
is
. However, this may be reduced to
using a hierarchy of networks which is created from theabove node doubling strategy.
5
D. KL Transform
The optimal linear method6for reducing redundancy in
a dataset is the KL transform or eigenvector expansion viaprinciple components analysis (PCA) [12]. PCA generates aset of orthogonal axes of projections known as the principal
components, or the eigenvectors, of the input data distribution
in the order of decreasing variance. The KL transform isa well-known statistical method for feature extraction andmultivariate data projection and has been used widely in
pattern recognition, signal processing, image processing, and
data analysis. Points in an
-dimensional input space are
projected into an -dimensional space, . The KL
transform is used here for comparison with the SOM in the
dimensionality reduction of the local image samples. The KL
transform is also used in eigenfaces, however in that case it
5This assumes that the topological order is optimal prior to each doubling
step.
6In the least mean squared error sense.
LAWRENCE et al.: FACE RECOGNITION 103
Fig. 5. A typical convolutional network.
Fig. 6. A high-level block diagram of the system we have used for face recognition.
is used on the entire images, whereas it is only used on small
local image samples in this work.
E. Convolutional Networks
The problem of face recognition from 2-D images is typ-
ically very ill-posed, i.e., there are many models which Ô¨Åt
the training points well but do not generalize well to unseen
images. In other words, there are not enough training points inthespacecreatedbytheinputimagesinordertoallowaccurateestimation of class probabilities throughout the input space.
Additionally, for MLP networks with 2-D images as input,
there is no invariance to translation or local deformation ofthe images [23].
Convolutional networks (CN) incorporate constraints and
achieve some degree of shift and deformation invariance using
three ideas: local receptive Ô¨Åelds, shared weights, and spatialsubsampling. The use of shared weights also reduces thenumber of parameters in the system aiding generalization.
Convolutional networks have been successfully applied to
character recognition [3], [5], [22]‚Äì[24].
A typical convolutional network is shown in Fig. 5 [24].
The network consists of a set of layers each of which contains
one or more planes. Approximately centered and normalized
images enter at the input layer. Each unit in a plane receivesinput from a small neighborhood in the planes of the previouslayer. The idea of connecting units to local receptive Ô¨Åelds
dates back to the 1960‚Äôs with the perceptron and Hubel
and Wiesel‚Äôs [15] discovery of locally sensitive orientation-selective neurons in the cat‚Äôs visual system [23]. The weightsforming the receptive Ô¨Åeld for a plane are forced to be equal
at all points in the plane. Each plane can be considered
as a feature map which has a Ô¨Åxed feature detector that isconvolved with a local window which is scanned over theplanes in the previous layer. Multiple planes are usually used
in each layer so that multiple features can be detected. These
layers are called convolutional layers. Once a feature hasbeen detected, its exact location is less important. Hence,the convolutional layers are typically followed by another
layer which does a local averaging and subsampling operation
(e.g., for a subsampling factor of two:
where is the output of
a subsampling plane at position andis the output of the
same plane in the previous layer). The network is trained with
the usual backpropagation gradient-descent procedure [13].A connection strategy can be used to reduce the number ofweights in the network. For example, with reference to Fig. 5,
Le Cunet al.[24] connect the feature maps in the second
convolutional layer only to one or two of the maps in theÔ¨Årst subsampling layer (the connection strategy was chosenmanually).
V. S
YSTEMDETAILS
The system we have used for face recognition is a com-
bination of the preceding parts‚Äîa high-level block diagramis shown in Figs. 6 and 7 shows a breakdown of the varioussubsystems that we experimented with or discuss.
Our system works as follows (we give complete details of
dimensions etc. later).
1) For the images in the training set, a Ô¨Åxed size window
(e.g., 5
5) is stepped over the entire image as shown
in Fig. 3 and local image samples are extracted at eachstep. At each step the window is moved by four pixels.
2) An SOM (e.g., with three dimensions and Ô¨Åve nodes
per dimension,
total nodes) is trained on the
vectors from the previous stage. The SOM quantizesthe 25-dimensional input vectors into 125 topologically
104 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 8, NO. 1, JANUARY 1997
Fig. 7. A diagram of the system we have used for face recognition showing alternative methods which we consider in this paper. The top ‚ÄúMLP style
classiÔ¨Åer‚Äù represents the Ô¨Ånal MLP style fully connected layer of the convolutional network. We have shown this decomposition of the convolutional
network in order to highlight the possibility of replacing the Ô¨Ånal layer (or layers) with a different type of classiÔ¨Åer. The nearest-neighbor style c lassiÔ¨Åer
is potentially interesting because it may make it possible to add new classes with minimal extra training time. The bottom MLP shows that the entireconvolutional network can be replaced with an MLP. We present results with either a SOM or the KL transform used for dimensionality reduction, and
either a convolutional neural network or an MLP for classiÔ¨Åcation.
ordered values. The three dimensions of the SOM can
be thought of as three features. We also experimented
with replacing the SOM with the KL transform. In thiscase, the KL transform projects the vectors in the 25-dimensional space into a 3-D space.
3) The same window as in the Ô¨Årst step is stepped over
all of the images in the training and test sets. The localimage samples are passed through the SOM at each step,thereby creating new training and test sets in the output
space created by the SOM. (Each input image is now
represented by three maps, each of which correspondsto a dimension in the SOM. The size of these maps isequal to the size of the input image (92
112) divided
by the step size (for a step size of four, the maps are
2328).
4) A convolutional neural network is trained on the newly
created training set. We also experimented with training
a standard MLP for comparison.
A. Simulation Details
In this section we give the details of one of the best
performing systems.
For the SOM, training is split into two phases as recom-
mended by Kohonen [20]‚Äîan ordering phase and a Ô¨Åne-
adjustment phase. In the Ô¨Årst phase 100000 updates are
performed, and in the second 50000 are performed. In the Ô¨Årstphase, the neighborhood radius starts at two-thirds of the sizeofthemapandreduceslinearlytoone.Thelearningrateduring
this phase is:
whereis the current update
number, and is the total number of updates. In the second
phase, the neighborhood radius starts at two and is reduced toone. The learning rate during this phase is .
The convolutional network contained Ô¨Åve layers excluding
the input layer. A conÔ¨Ådence measure was calculated for eachclassiÔ¨Åcation:
where isthemaximumoutput
and is the second maximum output (for outputs which
have been transformed using the softmaxtransformation
where are the original outputs, are the transformed
outputs, and is the number of outputs). The number of
planes in each layer, the dimensions of the planes, and thedimensions of the receptive Ô¨Åelds are shown in Table I. Thenetwork was trained with backpropagation [13] for a total of
20000 updates. Weights in the network were updated after
each pattern presentation, as opposed to batch update whereweights are only updated once per pass through the trainingset. All inputs were normalized to lie in the range
1t o
1. All nodes included a bias input which was part of the
optimization process. The best of ten random weight sets waschosen for the initial parameters of the network by evaluatingthe performance on the training set. Weights were initialized
on a node by node basis as uniformly distributed random
numbers in the range
where is the fan-in
of neuron [13]. Target outputs were 0.8 and 0.8 using the
output activation function.7The quadratic cost function
7This helps avoid saturating the sigmoid function. If targets were set to the
asymptotes of the sigmoid this would tend to: 1) drive the weights to inÔ¨Ånity;
2) cause outlier data to produce very large gradients due to the large weights;
and 3) produce binary outputs even when incorrect‚Äîleading to decreased
reliability of the conÔ¨Ådence measure.
LAWRENCE et al.: FACE RECOGNITION 105
Fig. 8. The learning rate as a function of the epoch number.
TABLE I
DIMENSIONS FOR THE CONVOLUTIONAL NETWORK.THECONNECTION
PERCENTAGE REFERS TO THE PERCENTAGE OF NODES IN THE PREVIOUSLAYER
TOWHICHEACHNODE IN THE CURRENTLAYERISCONNECTED ‚ÄîA VALUE
LESSTHAN100% R EDUCES THE TOTALNUMBER OF WEIGHTS IN THE NETWORK
ANDMAYIMPROVEGENERALIZATION .THECONNECTION STRATEGY USEDHERE
ISSIMILAR TO THAT USED BYLECUNET AL.[24]FORCHARACTER
RECOGNITION .HOWEVER,ASOPPOSED TO THE MANUALCONNECTION STRATEGY
USED BYLECUNET AL.,THECONNECTIONS BETWEENLAYERS2AND3ARE
CHOSENRANDOMLY .ASA NEXAMPLE OF HOW THEPRECISECONNECTIONS CAN
BEDETERMINED FROM THE TABLE‚ÄîTHESIZE OF THE FIRST-LAYERPLANES(21
/226) ISEQUAL TO THE TOTALNUMBER OF WAYS OFPOSITIONING A
3 /23RECEPTIVE FIELD ON THE INPUT-LAYERPLANES(23 /228)
was used. A search then converge learning rate schedule was
used8
where learning rate, initial learning rate 0.1,
total training epochs, current training epoch,
and The schedule is shown in Fig. 8.
Total training time was around four hours on an SGI Indy100Mhz MIPS R4400 system.
VI. E
XPERIMENTAL RESULTS
We performed various experiments and present the results
here. Except when stated otherwise, all experiments were
performed with Ô¨Åve training images and Ô¨Åve test images perperson for a total of 200 training images and 200 test images.There was no overlap between the training and test sets. We
note that a system which guesses the correct answer would be
rightoneoutof40times,givinganerrorrateof97.5%.Forthe
8Relatively high learning rates are typically used in order to help avoid
slow convergence and local minima. However, a constant learning rate results
in signiÔ¨Åcant parameter and performance Ô¨Çuctuation during the entire training
cycle such that the performance of the network can alter signiÔ¨Åcantly from
the beginning to the end of the Ô¨Ånal epoch. Moody and Darkin have proposed‚Äúsearch then converge‚Äù learning rate schedules. We have found that these
schedules still result in considerable parameter Ô¨Çuctuation and hence we have
added another term to further reduce the learning rate over the Ô¨Ånal epochs (a
simpler linear schedule also works well). We have found the use of learning
rate schedules to improve performance considerably.following sets of experiments, we vary only one parameter in
each case. The error barsshown in the graphs represent plus or
minusonestandarddeviationofthedistributionofresultsfromanumberofsimulations.
9Wenotethatideallywewouldliketo
haveperformedmoresimulationsperreportedresult,however,
we were limited in terms of computational capacity available
to us. The constants used in each set of experiments were:numberofclasses:40,dimensionalityreductionmethod:SOM,dimensions in the SOM: three, number of nodes per SOM
dimension: Ô¨Åve, image sample extraction: original intensity
values, training images per class: Ô¨Åve. Note that the constantsin each set of experiments may not give the best possibleperformance as the current best performing system was only
obtained as a result of these experiments. The experiments are
as follows.
1)Variation of the number of output classes ‚ÄîTable II and
Fig. 9 show the error rate of the system as the number
of classes is varied from ten to 20 to 40. We made noattempt to optimize the system for the smaller numbersof classes. As we expect, performance improves with
fewer classes to discriminate between.
2)Variation of the dimensionality of the SOM ‚ÄîTable III
and Fig. 10 show the error rate of the system as thedimension of the SOM is varied from one to four. The
best performing value is three dimensions.
3)Variation of the quantization level of the SOM ‚ÄîTable
IV and Fig. 11 show the error rate of the system asthe size of the SOM is varied from four to ten nodes
per dimension. The SOM has three dimensions in each
case. The best average error rate occurs for eight or ninenodes per dimension. This is also the best average errorrate of all experiments.
4)Variation of the image sample extraction
algorithm ‚ÄîTable V shows the result of using the
two local image sample representations described
earlier. We found that using the original intensity values
gave the best performance. We investigated altering theweight assigned to the central intensity value in thealternative representation but were unable to improve
the results.
5)Substituting the SOM with the KL transform ‚ÄîTable VI
shows the results of replacing the SOM with the KL
9We ran multiple simulations in each experiment where we varied the
selection of the training and test images (out of a total of /49/48/33 /61 /53 /33/61/51 /48 /50 /52 /48
possibilities) and the random seed used to initialize the weights in the
convolutional neural network.
106 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 8, NO. 1, JANUARY 1997
Fig. 9. The error rate as a function of the number of classes. We did not modify the network from that used for the 40 class case.
transform. We investigated using the Ô¨Årst one, two,
or three eigenvectors for projection. Surprisingly, thesystem performed best with only one eigenvector. The
best SOM parameters we tried produced slightly better
performance. The quantization inherent in the SOM
could provide a degree of invariance to minor image
sample differences and quantization of the PCA projec-tions may improve performance.
6)Replacing the CN with an MLP ‚ÄîTable VII shows the
results of replacing the convolutional network with anMLP. Performance is very poor. This result was ex-
pected because the MLP does not have the inbuilt
invariance to minor translation and local deformationwhich is created in the convolutional network using
the local receptive Ô¨Åelds, shared weights, and spatial
subsampling. As an example, consider when a feature isshifted in a test image in comparison with the training
image(s) for the individual. We expect the MLP to have
difÔ¨Åculty recognizing afeature whichhas been shiftedincomparison to the training images because the weights
connected to the new location were not trained for the
feature.
TheMLPcontainedonehiddenlayer.Weinvestigated
the following hidden layer sizes for the MLP: 20, 50,
100, 200, and 500. The best performance was obtainedwith 200 hidden nodes and a training time of two days.
The learning rate schedule and initial learning rate were
the same as for the original network. Note that thebest performing KL parameters were used while the
best performing SOM parameters were not. We note
that it may be considered fairer to compare against anMLPwithmultiplehiddenlayers[14],howeverselection
of the appropriate number of nodes in each layer is
difÔ¨Åcult (e.g., we have tried a network with two hiddenlayers containing 100 and 50 nodes, respectively, which
resulted in an error rate of 90%).
7)The tradeoffbetweenrejectionthreshold andrecognition
accuracy‚ÄîFig. 12 shows a histogram of the recog-
nizer‚Äôs conÔ¨Ådence for the cases when the classiÔ¨Åeris correct and when it is wrong for one of the best
performing systems. From this graph we expect that
classiÔ¨Åcation performance will increase signiÔ¨Åcantly ifwe reject cases below a certain conÔ¨Ådence threshold.Fig. 13 shows the system performance as the rejection
threshold is increased. We can see that by rejectingexamples with low conÔ¨Ådence we can signiÔ¨Åcantly in-
crease the classiÔ¨Åcation performance of the system. If
we consider a system which used a video camera totake a number of pictures over a short period, we couldexpect that a high performance would be attainable with
an appropriate rejection threshold.
8)Comparison with other known results on the same
database‚ÄîTable VIII shows a summary of the per-
formance of the systems for which we have results
using the ORL database. In this case, we used a
SOM quantization level of eight. Our system is thebest performing system
10and performs recognition
roughly500timesfasterthanthesecondbestperforming
system‚Äîthe pseudo 2-D HMM‚Äôs of Samaria. Fig. 14
shows the images which were incorrectly classiÔ¨Åed forone of the best performing systems.
9)Variation of the number of training images per person.
Table IX shows the results of varying the number of
images per class used in the training set from one toÔ¨Åve for
and also for the eigen-
faces algorithm. We implemented two versions of the
eigenfaces algorithm‚Äîthe Ô¨Årst version creates vectors
for each class in the training set by averaging theresults of the eigenface representation over all imagesfor the same person. This corresponds to the algorithm
as described by Turk and Pentland [43]. However,
we found that using separate training vectors for eachtraining image resulted in better performance. We foundthat using between 40 to 100 eigenfaces resulted in
similar performance. We can see that the
and
methods are both superior to the eigenfaces
technique even when there is only one training imageper person. The SOM+CN method consistently performs
better than the
method.
VII. DISCUSSION
The results indicate that a convolutional network can be
more suitable in the given situation when compared with a
10The 4% error rate reported is an average of multiple simula-
tions‚Äîindividual simulations have given error rates as low as 1.5%.
LAWRENCE et al.: FACE RECOGNITION 107
Fig. 10. The error rate as a function of the number of dimensions in the SOM.
Fig. 11. The error rate as a function of the number of nodes per dimension in the SOM.
Fig. 12. A histogram depicting the conÔ¨Ådence of the classiÔ¨Åer when it turns out to be correct, and the conÔ¨Ådence when it is wrong. The graph suggests that
we can improve classiÔ¨Åcation performance considerably by rejecting cases where the classiÔ¨Åer has a low conÔ¨Ådence.
TABLE II
ERRORRA T EO FT H E FACERECOGNITION SYSTEM WITH VARYINGNUMBER OF
CLASSES(SUBJECTS). EACHRESULTIS THEAVERAGE OF THREESIMULATIONS
standard MLP. This correlates with the common belief that the
incorporation of prior knowledge is desirable for MLP style
networks (the CN incorporates domain knowledge regarding
therelationshipofthepixelsanddesiredinvariancetoadegreeof translation, scaling, and local deformation).TABLE III
ERRORRATE OF THE FACERECOGNITION SYSTEM WITH VARYING
NUMBER OF DIMENSIONS IN THE SELF-ORGANIZING MAP.EACH
RESULTGIVENIS THEAVERAGE OF THREESIMULATIONS
Convolutional networks have traditionally been used on raw
images without any preprocessing. Without the preprocessing
we have used, the resulting convolutional networks are larger,
more computationally intensive, and have not performed aswell in our experiments [e.g., using no preprocessing and the
108 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 8, NO. 1, JANUARY 1997
Fig. 13. The test set classiÔ¨Åcation performance as a function of the percentage of samples rejected. ClassiÔ¨Åcation performance can be improved signi Ô¨Åcantly
by rejecting cases with low conÔ¨Ådence.
Fig. 14. Test images. The images with a thick white border were incorrectly classiÔ¨Åed by one of the best performing systems.
TABLE IV
ERRORRATE OF THE FACERECOGNITION SYSTEM WITH VARYING
NUMBER OF NODES PER DIMENSION IN THE SOM. E ACH
RESULTGIVENIS THEAVERAGE OF THREESIMULATIONS
TABLE V
ERRORRA T EO FT H E FACERECOGNITION SYSTEM WITH VARYINGIMAGESAMPLE
REPRESENTATION .EACHRESULTIS THEAVERAGE OF THREESIMULATIONS
TABLE VI
ERRORRA T EO FT H E FACERECOGNITION SYSTEM WITH LINEAR
PCAANDSOM F EATUREEXTRACTION MECHANISMS .
EACHRESULTIS THEAVERAGE OF THREESIMULATIONS
TABLE VII
ERRORRATECOMPARISON OF THE VARIOUSFEATURE
EXTRACTION AND CLASSIFICATION METHODS.EACH
RESULTIS THEAVERAGE OF THREESIMULATIONS
same CN architecture except initial receptive Ô¨Åelds of 8 8
resulted in approximately two times greater error (for the case
of Ô¨Åve images per person)].
Fig. 15 shows the randomly chosen initial local image
samples corresponding to each node in a 2-D SOM, and
the Ô¨Ånal samples to which the SOM converges. Scanning
across the rows and columns we can see that the quantizedsamples represent smoothly changing shading patterns. This isthe initial representation from which successively higher level
features are extracted using the convolutional network. Fig. 16
shows the activation of the nodes in a sample convolutionalnetwork for a particular test image.
LAWRENCE et al.: FACE RECOGNITION 109
TABLE VIII
ERRORRATE OF THE VARIOUSSYSTEMS./49On a Sun Sparc II./50ONA NSGI
INDY MIPS R4400 100MHZ S YSTEM.ACCORDING TO THE SPECINT92
ANDSPECFP92 R ATINGS AT http://hpwww.epÔ¨Ç.ch/bench/spec.html
THESGI M ACHINEISAPPROXIMATELY THREETIMESFASTERTHAN
THESUN SPARC II, M AKING THE SOM /43CN APPROXIMATELY 160
TIMESFASTER THAN THE PSEUDO2-D HMM‚ÄôS FORCLASSIFICATION
TABLE IX
ERRORRATE FOR THE EIGENFACES ALGORITHM AND THE /83/79 /77/43/67/78 AS THE
SIZE OF THE TRAININGSETISVARIED FROM ONE TOFIVEIMAGES PER PERSON.
AVERAGED OVERTWODIFFERENT SELECTIONS OF THE TRAINING AND TESTSETS
Fig. 15. SOM image samples before training (a random set of imagesamples) and after training.
Fig. 17 shows the results of sensitivity analysis in order to
determine which parts of the input image are most importantfor classiÔ¨Åcation. Using the method of Baluja and Pomerleau
as described in [37], each of the input planes to the convo-
lutional network was divided into 2
2 segments (the input
planes are 23 28). Each of 168 (12 14) segments was
replaced with random noise, one segment at a time. The test
performance was calculated at each step. The error of the
network when replacing parts of the input with random noisegives an indication of how important each part of the image isfor the classiÔ¨Åcation task. From the Ô¨Ågure it can be observed
that, as expected, the eyes, nose, mouth, chin, and hair regions
are all important to the classiÔ¨Åcation task.
Can the convolutional network feature extraction form the
optimal set of features? The answer is negative‚Äîit is unlikely
that the network could extract an optimal set of features for all
images. Although the exact process of human face recognitionis unknown, there are many features which humans may usebut our system is unlikely to discover optimally‚Äîe.g., 1)
knowledge of the 3-D structure of the face; 2) knowledge
of the nose, eyes, mouth, etc.; 3) generalization to glasses/noglasses, different hair growth, etc.; and 4) knowledge of facialexpressions.VIII. C
OMPUTATIONAL COMPLEXITY
The SOM takes considerable time to train. This is not a
drawback of the approach however, as the system can beextended to cover new classes without retraining the SOM.All that is required is that the image samples originally usedto train the SOM are sufÔ¨Åciently representative of the imagesamples used in new images. For the experiments we havereported here, the quantized output of the SOM is very similarif we train it with only 20 classes instead of 40. In addition,the KL transform can be used in place of the SOM with aminimal impact on system performance.
It also takes a considerable amount of time to train a con-
volutional network; how signiÔ¨Åcant is this? The convolutionalnetwork extracts features from the image. It is possible to useÔ¨Åxed feature extraction. Consider if we separate the convo-
lutional network into two parts: the initial feature extraction
layers and the Ô¨Ånal feature extraction and classiÔ¨Åcation layers.Given a well-chosen sample of the complete distribution offaces which we want to recognize, the features extracted fromthe Ô¨Årst section could be expected to also be useful for theclassiÔ¨Åcation of new classes. These features could then beconsidered Ô¨Åxed features and the Ô¨Årst part of the network maynot need to be retrained when adding new classes. The pointat which the convolutional network is broken into two woulddepend on how well the features at each stage are useful forthe classiÔ¨Åcation of new classes (the larger features in theÔ¨Ånal layers are less likely to be a good basis for classiÔ¨Åcationof new examples). We note that it may be possible to replacethe second part with another type of classiÔ¨Åer‚Äîe.g., a nearest-neighborsclassiÔ¨Åer.Inthiscasethetimerequiredforretrainingthe system when adding new classes is minimal (the extractedfeature vectors are simply stored for the training images).
To give an idea of the computational complexity of each
part of the system we deÔ¨Åne:
the number of classes;
the number of nodes in the SOM;
the number of weights in the convolutional net-work;
the number of weights in the classiÔ¨Åer;
the number of training examples;
the number of nodes in the neighborhood function;
the total number of next nodes used to backpropa-gate the error in the CN;
the total number of next nodes used to backpropa-gate the error in the MLP classiÔ¨Åer;
the output dimension of the KL projection;
the input dimension of the KL projection;
the number of training samples for the SOM or theKL projection;
the number of local image samples per image.
Tables X and XI show the approximate complexity of the
various parts of the system during training and classiÔ¨Åcation.WeshowthecomplexityforboththeSOMandKLalternativesfor dimensionality reduction and for both the neural network(MLP) and a nearest-neighbors classiÔ¨Åer (as the lastpart of theconvolutional network‚Äînot as a complete replacement, i.e.,this is not the same as the earlier MLP experiments). We note
110 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 8, NO. 1, JANUARY 1997
Fig. 16. A depiction of the node maps in a sample convolutional network showing the activation values for a particular test image. The input image
is shown on the left. In this case the image is correctly classiÔ¨Åed with only one activated output node (the top node). From left to right after the inputimage, the layers are: the input layer, convolutional layer 1, subsampling layer 1, convolutional layer 2, subsampling layer 2, and the output layer. The
three planes in the input layer correspond to the three dimensions of the SOM.
Fig. 17. Sensitivity to various parts of the input image. It can be observedthat the eyes, mouth, nose, chin, and hair regions are all important for the
classiÔ¨Åcation. The
/122axis corresponds to the mean squared error rather than
theclassiÔ¨Åcationerror(themeansquarederrorispreferablebecauseitvariesin
a smoother fashion as the input images are perturbed). The image orientation
corresponds to upright face images.
that the constant associated with the log factors may increase
exponentiallyintheworstcase(cf.neighborsearchinginhigh-dimensional spaces [1]). We have aimed to show how thecomputational complexity scales according to the number of
classes, e.g., for the training complexity of the MLP classiÔ¨Åer:
although
may be larger than both and
scale roughly according to .TABLE X
TRAININGCOMPLEXITY . /107 /49AND /107 /51REPRESENT THE
NUMBER OF TIMES THE TRAININGSETISPRESENTED TO
THENETWORK FOR THE SOMAND THECN, RESPECTIVELY
TABLE XI
CLASSIFICATION COMPLEXITY . /107 /50REPRESENTS
THEDEGREE OF SHAREDWEIGHTREPLICATION
With reference to Table XI, consider, for example, the main
architecture in recognition mode. The complexity
of the SOM module is independent of the number of classes.The complexity of the CN scales according to the number of
weights in the network. When the number of feature maps in
the internal layers is constant, the number of weights scalesroughly according to the number of output classes (the number
LAWRENCE et al.: FACE RECOGNITION 111
of weights in the output layer dominates the weights in the
initial layers).
In terms of computation time, the requirements of real-time
tasks varies. The system we have presented should be suitable
for a number of real-time applications. The system is capable
of performing a classiÔ¨Åcation in less than half a second for 40classes. This speed is sufÔ¨Åcient fortasks such as access controland room monitoring whenusing40 classes.It is expected that
an optimized version could be signiÔ¨Åcantly faster.
IX. F
URTHERRESEARCH
We can identify the following avenues for improving per-
formance.
1) More careful selection of the convolutional network
architecture, e.g., by using the optimal brain damagealgorithm [25] as used by Le Cun et al.[24] to improve
generalization and speedup handwritten digit recogni-
tion.
2) More precise normalization of the images to account
for translation, rotation, and scale changes. Any nor-malization would be limited by the desired recognition
speed.
3) The various facial features could be ranked according
to their importance in recognizing faces and separate
modules could be introduced for various parts of the
face, e.g., the eye region, the nose region, and themouth region (Brunelli and Poggio [6] obtain very goodperformance using a simple template matching strategy
on precisely these regions).
4) An ensemble of recognizers could be used. These could
be combined via simple methods such as a linear com-bination based on the performance of each network, or
via a gating network and the expectation-maximization
algorithm [11], [16]. Examination of the errors madeby networks trained with different random seeds and bynetworks trained with the SOM data versus networks
trained with the KL data shows that a combination
of networks should improve performance (the set ofcommon errors between the recognizers is often muchsmaller than the total number of errors).
5) Invariancetoagroupofdesiredtransformationscouldbe
enhancedwiththeadditionofpseudo-datatothetrainingdatabase‚Äîi.e., the addition of new examples createdfrom the current examples using translation, etc. Leen
[26] shows that adding pseudodata can be equivalent
to adding a regularizer to the cost function where theregularizer penalizes changes in the output when theinput goes under a transformation for which invariance
is desired.
X. C
ONCLUSIONS
We have presented a fast, automatic system for face recog-
nition which is a combination of a local image sample rep-resentation, an SOM network, and a convolutional network
for face recognition. The SOM provides a quantization of the
image samples into a topological space where inputs that arenearby in the original space are also nearby in the outputspace, which results in invariance to minor changes in the
image samples, and the convolutional neural network pro-vides for partial invariance to translation, rotation, scale, anddeformation. Substitution of the KL transform for the SOM
produced similar but slightly worse results. The method is
capable of rapid classiÔ¨Åcation, requires only fast approximatenormalization and preprocessing, and consistently exhibitsbetter classiÔ¨Åcation performance than the eigenfaces approach
[43] on the database considered as the number of images
per person in the training database is varied from one toÔ¨Åve. With Ô¨Åve images per person the proposed method andeigenfaces result in 3.8% and 10.5% error, respectively. The
recognizer provides a measure of conÔ¨Ådence in its output and
classiÔ¨Åcation error approaches zero when rejecting as few as10% of the examples. We have presented avenues for furtherimprovement.
There are no explicit 3-D models in our system, however
we have found that the quantized local image samples usedas input to the convolutional network represent smoothlychanging shading patterns. Higher level features are con-
structed from these building blocks in successive layers of
the convolutional network. In comparison with the eigenfacesapproach, we believe that the system presented here is able tolearn more appropriate features in order to provide improved
generalization. The system is partially invariant to changes in
the local image samples, scaling, translation and deformationby design.
A
CKNOWLEDGMENT
The authors would like to thank I. Cox, S. Haykin, and the
anonymous reviewers for helpful comments, and the Olivetti
Research Laboratory and Ferdinando Samaria for compiling
and maintaining the ORL database.
REFERENCES
[1] S. Arya and D. M. Mount, ‚ÄúAlgorithms for fast vector quantization,‚Äù
inProc. DCC 93: Data Compression Conf., J. A. Storer and M. Cohn,
Eds. Piscataway, NJ: IEEE Press, pp. 381‚Äì390.
[2] H.-U. Bauer and K. R. Pawelzik, ‚ÄúQuantifying the neighborhood preser-
vation of self-organizing feature maps,‚Äù IEEE Trans. Neural Networks,
vol. 3, pp. 570‚Äì579, 1992.
[3] Y. Bengio, Y. Le Cun, and D. Henderson, ‚ÄúGlobally trained handwritten
word recognizer using spatial representation, space displacement neural
networks, and hidden Markov models,‚Äù in Advances in Neutral Infor-
mation Processing Systems 6 . San Mateo, CA: Morgan Kaufmann,
1994.
[4] J. L. Blue, G. T. Candela, P. J. Grother, R. Chellappa, and C. L. Wilson,
‚ÄúEvaluation of pattern classiÔ¨Åers for Ô¨Ångerprint and OCR applications,‚Äù
Pattern Recognition, vol. 27, no. 4, pp. 485‚Äì501, Apr. 1994.
[5] L. Bottou, C. Cortes, J. S. Denker, H. Drucker, I. Guyon, L. Jackel,
Y. Le Cun, U. Muller, E. Sackinger, P. Simard, and V. N. Vapnik,
‚ÄúComparison of classiÔ¨Åer methods: A case study in handwritten digit
recognition,‚Äù in Proc. Int. Conf. Pattern Recognition . Los Alamitos,
CA: IEEE Comput. Soc. Press, 1994.
[6] R. Brunelli and T. Poggio, ‚ÄúFace recognition: Features versus
templates,‚Äù IEEE Trans. Pattern Anal. Machine Intell., vol. 15, pp.
1042‚Äì1052, Oct. 1993.
[7] D. K. Burton, ‚ÄúText-dependent speaker veriÔ¨Åcation using vector quan-
tization source coding,‚Äù IEEE Trans. Acoust., Speech, Signal Process.,
vol. ASSP-35, pp. 133, 1987.
[8] R. Chellappa, C. L. Wilson, and S. Sirohey, ‚ÄúHuman and machine
recognition offaces: A survey,‚Äù Proc.IEEE, vol.83, pp.5, pp.705‚Äì740,
1995.
[9] I. J. Cox, J. Ghosn, and P. N. Yianilos, ‚ÄúFeature-based face recognition
using mixture-distance,‚Äù in Computer Vision and Pattern Recognition .
Piscataway, NJ: IEEE Press, 1996.
112 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 8, NO. 1, JANUARY 1997
[10] D. DeMers and G. W. Cottrell, ‚ÄúNonlinear dimensionality reduction,‚Äù in
Advances in Neural Information Processing Systems 5 , S. J. Hanson, J.
D.Cowan, and C. Lee Giles, Eds. San Mateo,CA: Morgan Kaufmann,1993, pp. 580‚Äì587.
[11] H. Drucker, C. Cortes, L. Jackel, Y. Le Cun, and V. N. Vapnik,
‚ÄúBoosting and other ensemble methods,‚Äù Neural Computa., vol. 6, pp.
1289‚Äì1301, 1994.
[12] K. Fukunaga, Introduction to Statistical Pattern Recognition , 2nd ed.
Boston, MA: Academic, 1990.
[13] S.Haykin, NeuralNetworks,AComprehensiveFoundation . NewYork:
Macmillan, 1994.
[14]
, private communication, 1996.
[15] D. H. Hubel and T. N. Wiesel, ‚ÄúReceptive Ô¨Åelds, binocular interaction,
and functional architecture in the cat‚Äôs visual cortex,‚Äù J. Physiol., vol.
160, pp. 106‚Äì154, 1962.
[16] R.A.Jacobs,‚ÄúMethodsforcombiningexperts‚Äôprobabilityassessments,‚Äù
Neural Computa., vol. 7, pp. 867‚Äì888, 1995.
[17] T. Kanade, ‚ÄúPicture processing by computer complex and recognition
of human faces,‚Äù Ph.D. dissertation, Kyoto Univ., Japan, 1973.
[18] H. Kita and Y. Nishikawa, ‚ÄúNeural-network model of tonotopic map
formation based on the temporal theory of auditory sensation,‚Äù inProc. World Congr. Neural Networks, WCNN 93, vol. II. Hillsdale,
NJ: Lawrence Erlbaum, pp. 413‚Äì418.
[19] T. Kohonen, ‚ÄúThe self-organizing map,‚Äù Proc. IEEE, vol. 78, pp.
1464‚Äì1480, 1990.
[20]
,Self-Organizing Maps . Berlin: Springer-Verlag, 1995.
[21] M.Lades,J.C.Vorbr¬® uggen,J.Buhmann,J.Lange,C.vonderMalsburg,
Rolf P. W¬® urtz, and W. Konen, ‚ÄúDistortion invariant object recognition
in the dynamic link architecture,‚Äù IEEE Trans. Comput., vol. 42, pp.
300‚Äì311, 1993.
[22] Y. Le Cun, ‚ÄúGeneralization and network design strategies,‚Äù Dep. Com-
put. Sci., Univ. Toronto, Canada, Tech. Rep. CRG-TR-89-4, 1989.
[23] Y. Le Cun and Y. Bengio, ‚ÄúConvolutional networks for images, speech,
andtimeseries,‚Äùin TheHandbookofBrainTheoryandNeuralNetworks,
M. A. Arbib, Ed. Cambridge, MA: MIT Press, 1995, pp. 255‚Äì258.
[24] Y. Le Cun, B. Boser, J. S. Denker, D. Henderson, R. Howard, W.
Hubbard,and L.Jackel,‚ÄúHandwrittendigitrecognitionwithabackprop-
agation neural network,‚Äù in Advances in Neural Information Processing
Systems 2 , D. S. Touretzky, Ed. San Mateo, CA: Morgan Kaufmann,
1990, pp. 396‚Äì404.
[25] Y. Le Cun, J. S. Denker, and S. A. Solla, ‚ÄúOptimal brain damage,‚Äù
inNeural Information Processing Systems , vol. 2, D. S. Touretzky, Ed.
San Mateo, CA: Morgan Kaufmann, 1990, pp. 598‚Äì605.
[26] T. K. Leen, ‚ÄúFrom data distributions to regularization in invariant
learning,‚Äù Neural Computa., vol. 3, no. 1, pp. 135‚Äì143, 1991.
[27] S. P. Luttrell, ‚ÄúHierarchical self-organizing networks,‚Äù in Proc. 1st
IEEConf.ArtiÔ¨ÅcialNeuralNetworks . London: British Neural Network
Soc., 1989, pp. 2‚Äì6.
[28] D. Marr, Vision. San Francisco, CA: W. H. Freeman, 1982.
[29] B. Miller, ‚ÄúVital signs of identity,‚Äù IEEE Spectrum, pp. 22‚Äì30, Feb.
1994.
[30] B. Moghaddam and A. Pentland, ‚ÄúFace recognition using view-based
and modular eigenspaces,‚Äù in Automat. Syst. IdentiÔ¨Åcation Inspection of
Humans, vol. 2257, 1994.
[31] K. Obermayer, G. G. Blasdel, and K. Schulten, ‚ÄúA neural-network
model for the formation and for the spatial structure of retinotopicmaps, orientation, and ocular dominance columns,‚Äù in ArtiÔ¨Åcial Neural
Networks, T. Kohonen, K. M ¬®akisara, O. Simula, and J. Kangas, Eds.
Amsterdam, Netherlands: Elsevier, 1991, pp. 505‚Äì511.
[32] K. Obermayer, H. Ritter, and K. Schulten, ‚ÄúLarge-scale simulation of
a self-organizing neural network: Formation of a somatotopic map,‚Äù
inParallel Processing in Neural Systems and Computers , R. Eck-
miller, G. Hartmann, and G. Hauske, Eds. Amsterdam, Netherlands:North‚ÄìHolland, 1990, pp. 71‚Äì74.
[33] A. Pentland, B. Moghaddam, and T. Starner, ‚ÄúView-based and modular
eigenspaces for face recognition,‚Äù in Proc. IEEE Conf. Comput. Vision
Pattern Recognition, 1994.
[34] A. Pentland, T. Starner, N. Etcoff, A. Masoiu, O. Oliyide, and M. Turk,
‚ÄúExperiments with eigenfaces,‚Äù in Proc. Looking at People Wkshp, Int.
Joint Conf. ArtiÔ¨Åcial Intell. , Chamberry, France, 1993.
[35] D. J. Perret, E. T. Rolls, and W. Caan, ‚ÄúVisual neurones responsive
to faces in the monkey temporal cortex,‚Äù Experimental Brain Res., vol.
47, pp. 329‚Äì342, 1982.
[36] Y. Y. Qi and B. R. Hunt, ‚ÄúSignature veriÔ¨Åcation using global and grid
features,‚Äù Pattern Recognition, vol. 27, no. 12, pp. 1621‚Äì1629, Dec.
1994.
[37] H. A. Rowley, S. Baluja, and T. Kanade, ‚ÄúHuman face detection in
visual scenes,‚Äù School Comput. Sci., Carnegie Mellon Univ., Pittsburgh,
PA, Tech. Rep. CMU-CS-95-158, July 1995.
[38] F. S. Samaria, ‚ÄúFace recognition using hidden Markov models,‚Äù Ph.D.
dissertation, Trinity College, Univ. Cambridge, Cambridge, U.K., 1994.[39] F. S. Samaria and A. C. Harter, ‚ÄúParameterization of a stochastic model
for human face identiÔ¨Åcation,‚Äù in Proc. 2nd IEEE Wkshp. Applicat.
Comput. Vision, Sarasota, FL, 1994.
[40] K.-K. Sung and T. Poggio, ‚ÄúLearning human face detection in cluttered
scenes,‚Äù in Computer Analysis of Images and Patterns , G. Goos, J.
Hartmonis, and J. van Leeuwen, Eds. New York: Springer‚ÄìVerlag,
1995, pp. 432‚Äì439.
[41] K. Sutherland, D. Renshaw, and P. B. Denyer, ‚ÄúAutomatic face recog-
nition,‚Äù in Proc. 1st Int. Conf. Intell. Syst. Eng . Piscataway, NJ: IEEE
Press, 1992, pp. 29‚Äì34.
[42] D. L. Swets and J. J. Weng, ‚ÄúUsing discriminant eigenfeatures for image
retrieval,‚Äù IEEE Trans. Pattern Anal. Machine Intell. , to appear, 1996.
[43] M. Turk and A. Pentland, ‚ÄúEigenfaces for recognition,‚Äù J. Cognitive
Neurosci. , vol. 3, pp. 71‚Äì86, 1991.
[44] J. Weng, N. Ahuja, and T. S. Huang, ‚ÄúLearning recognition and
segmentation of 3-D objects from 2-D images,‚Äù in Proc. Int. Conf.
Comput. Vision, ICCV 93 , 1993, pp. 121‚Äì128.
[45] L. Wiskott, J.-M. Fellous, N. Kr¬® uger, and C. von der Malsburg, ‚ÄúFace
recognition and gender determination,‚Äù in Proc. Int. Wkshp. Automat.
Face Gesture Recognition, Z¬®urich, Switzerland, 1995.
Steve Lawrence (M‚Äô96) received the B.Sc. and
B.Eng. degrees summa cum laude in 1993 from
the Queensland University of Technology, (QUT)
Australia. He submitted his Ph.D. thesis in August
1996 at the University of Queensland, Australia.
He is presently working as a Scientist at the NEC
Research Institute in Princeton, NJ. His research in-terests include neural networks, pattern recognition,
intelligent agents, and information retrieval.
His awards include a university medal, a QUT
award for excellence, ATERB and APA priority
scholarships, QEC and Telecom Australia Engineering prizes, and three prizes
in successive years of the Australian Mathematics Competition.
C. Lee Giles (S‚Äô80‚ÄìM‚Äô80‚ÄìSM‚Äô95) received the
B.A. degree from Rhodes College, Memphis, TN,
the B.S. degree in engineering physics from the
University of Tennessee, Knoxville, in 1969, theM.S. degree in physics from the University of
Michigan,AnnArbor,in1974,andthePh.D.degree
in optical sciences from the University of Arizona,
Tucson, in 1980.
Previously, he was a Program Manager at the
Air Force OfÔ¨Åce of ScientiÔ¨Åc Research in Washing-
ton, D.C. where he initiated and managed research
programs in neural networks and in optics in computing and processing.
Before that, he was a Research Scientist at the Naval Research Laboratory,Washington, D.C., and an Assistant Professor of Electrical and Computer
Engineering at Clarkson University. During part of his graduate education
he was a Research Engineer at Ford Motor ScientiÔ¨Åc Research Laboratory.
He is a Senior Research Scientist in Computer Sciences at NEC Research
Institute, Princeton, NJ, and an Adjunct Associate Professor at the Institute for
AdvancedComputerStudiesattheUniversityofMaryland,CollegePark,MD.
His research interests include neural networks, machine learning, and artiÔ¨Åcial
intelligence; hybrid systems and integrating neural networks with intelligent
systems and agents; applications of intelligent methods to signal processing,communications, computing, networks, time series, pattern recognition, and
optical computing and processing. He has published more than 80 journal and
conference papers and book chapters in these areas.
Dr. Giles serves on many related conference program committees and has
helpedorganizemanyrelatedmeetingsandworkshops.Hehasbeenanadvisor
and reviewer to government and university programs in both neural networks
and in optical computing and processing. He has served or is currently servingon the editorial boards of IEEE T
RANSACTIONS ON KNOWLEDGE AND DATA
ENGINEERING , IEEE T RANSACTIONS ON NEURALNETWORKS,JournalofParallel
and Distributed Computing, Neural Networks, Neural Computation, Optical
Computing and Processing, Applied Optics, and Academic Press. He is a
member of AAAI, ACM, INNS, and OSA.
LAWRENCE et al.: FACE RECOGNITION 113
Ah Chung Tsoi (S‚Äô70‚ÄìM‚Äô72‚ÄìM‚Äô84‚ÄìSM‚Äô90), was
born in Hong Kong. He received the Higher
Diploma in electronic engineering, Hong Kong
Technical College, in 1969, and the M.Sc. degree
in electronic control engineering and the Ph.D.
degree in control engineering from the Universityof Salford in 1970 and 1972, respectively.
From 1972 to 1974 he worked as Senior
Research Fellow at the Inter-University Institute of
Engineering Control, University College of North
Wales, Bangor, Wales. From 1974 to 1977, he
worked as a Lecturer at the Paisley College of Technology, Renfrewshire,
Scotland. From 1977 to 1984, he worked as a Senior Lecturer at theUniversity of Auckland, New Zealand. From 1985 to 1990, he worked
as a Senior Lecturer at the University College, University of New South
Wales. He is presently a Professor in Electrical Engineering at the University
of Queensland, Australia. His research interests include aspects of neural
networks and their application to practical problems, adaptive signal
processing, and adaptive control.
Andrew D. Back (M‚Äô96) received the B.Eng. de-
gree with distinction from Darling Downs Institute
of Advanced Edutcation (now the University of
Southern Queensland), Australia, in 1985, and the
Ph.D. degree from the University of Queensland,
Australia, in 1992.
He was a Research Fellow with the Defence
Science and Technology Organization (DSTO) from
1988 to 1992, and a Research Scientist with the
DSTO in 1993. In 1995 he was a Visiting Scientist
at the NEC Research Institute, Princeton, NJ. He
is currently a Frontier Researcher with the Brain Information Processing
Group in the Frontier Research Program at the Institute of Physical andChemical Research (RIKEN) in Japan. He has published about 30 papers.
His research interests include nonlinear models for system identiÔ¨Åcation, time
series prediction, and blind source separation.
Dr. Back was awarded an Australian Research Council (ARC) Postdoctoral
ResearchFellowshipintheDepartmentofElectricalandComputerEnginering
at the University of Queensland for the period from 1993‚Äì1996. He has orga-
nized workshops for the Neural Information Processing Systems Conference
in 1994, 1995, and 1996 in area of neural networks for signal processing.
"
https://ieeexplore.ieee.org/document/9154121,"Facial Emotion Detection Using Deep Learning
Akriti Jaiswal, A. Krishnama Raju, Suman Deb
Department of Electronics Engineering
SVNIT Surat, India
{nainajaiswal96, krishnamraju995 }@gmail.com, sumandeb@eced.svnit.ac.in
Abstract ‚ÄîHuman Emotion detection from image is one of
the most powerful and challenging research task in social
communication. Deep learning (DL) based emotion detectiongives performance better than traditional methods with image
processing. This paper presents the design of an artiÔ¨Åcial
intelligence (AI) system capable of emotion detection through
facial expressions. It discusses about the procedure of emotiondetection, which includes basically three main steps: face detec-
tion, features extraction, and emotion classiÔ¨Åcation. This paper
proposed a convolutional neural networks (CNN) based deeplearning architecture for emotion detection from images. The
performance of the proposed method is evaluated using two
datasets Facial emotion recognition challenge (FERC-2013) andJapaness female facial emotion (JAFFE). The accuracies achieved
with proposed model are 70.14 and 98.65 percentage for FERC-
2013 and JAFFE datasets respectively.
Index T erms ‚ÄîArtiÔ¨Åcially intelligence (AI), Facial emotion
recognition (FER), Convolutional neural networks (CNN), Rec-
tiÔ¨Åed linear units (ReLu), Deep learning (DL).
I. I NTRODUCTION
Emotion is a mental state associated with the nervous sys-
tem associated with feeling, perceptions, behavioral reactions,and a degree of gratiÔ¨Åcation or displeasure [1]. One of thecurrent application of artiÔ¨Åcial intelligence (AI) using neuralnetworks is the recognition of faces in images and videos for
various applications. Most techniques process visual data and
search for general pattern present in human faces in images orvideos. Face detection can be used for surveillance purposes bylaw enforcers as well as in crowd management. In this paper,we present a method for identifying seven emotions such as
anger, disgust, neutral fear, happy, sad, and surprise using fa-
cial images. Previous research used deep-learning technologyto create models of facial expressions based on emotions toidentify emotions [2]. The typical human computer interaction(HCI) lacks users emotional state and loses a great deal ofinformation during the process of interaction. Comparatively,
users are more efÔ¨Åcient and desired by emotion-sensitive HCI
systems [3]. Now a days, interest in emotional computing hasincreased with the increasing demands of leisure, commerce,physical and psychological well being, and education relatedapplications. Because of this, several products of emotionallysensitive HCI systems have been developed over the past
several years, although the ultimate solution for this researchÔ¨Åeld has not been suggested [4].II. R
ELATED WORK
A. Human Facial Expressions
The universality of facial expressions and the body language
is a key feature of human interaction. Charles Darwin al-ready published on globally common facial expressions in thenineteenth century, which play an important role in nonverbalcommunication [4]. In 1971, Ekman Friesen declared facialbehaviors to be correlated uniformly with speciÔ¨Åc emotions[5]. Apparently humans but also animals, produce speciÔ¨Åcmuscle movements that belong to a certain mental state. Peopleinterested in research on emotion classiÔ¨Åcation via speechrecognition are referred to Nicholson et al. [6].
B. Image ClassiÔ¨Åcation T echniques
Image classiÔ¨Åcation system generally consists of feature
extraction followed by classiÔ¨Åcation stage. Fasel and Luet-tin provided an extensive overview of the analytical featureextractors and neural network approaches for recognition offacial expression [7]. It may be concluded that both approacheswork approximately equally well by the time of writing, atthe beginning of the twenty-Ô¨Årst century. However, given thecurrent availability of training data and computational power,the expectation is that the performance of models based onneural networks can be substantially improved. Several recentmilestones are set out below.
‚Ä¢Krizhevsky and Hinton give a landmark publication onthe automatic image classiÔ¨Åcation in general [8]. Thiswork shows a deep neural network that resembles the hu-man visual cortex‚Äôs functionality. A model to categorizeobjects from pictures is obtained using a self-developedlabeled array of 60,000 images over 10 classes, usingthe CIFAR-10 dataset. Another important outcome of theresearch is the visualization of the Ô¨Ålters in the network,so that how the model breaks down the images can beassessed.
‚Ä¢In 2010, the launch of the annual Imagenet challenges [9]boosted work on the classiÔ¨Åcation of images, and sincethen the belonging gigantic collection of labeled data isoften used in publications . In a later work by Krizhevskyet al. [10], the ImageNet LSVRC-2010 contest trains anetwork of 5 convolutional, 3 max pooling, and 3 fullyconnected layers with 1.2 million high-resolution images.
‚Ä¢In particular, with regard to facial expression recogni-tion, Lv et al. [11] present a network of deep beliefs2020 International Conference for Emerging Technology (INCET) 
Belgaum, India. Jun 5-7, 2020
978-1-7281-6221-8/20/$31.00 ¬©2020 IEEE 1
Authorized licensed use limited to: University of Wollongong. Downloaded on August 13,2020 at 17:55:09 UTC from IEEE Xplore.  Restrictions apply. 
primarily for use with the JAFFE and expanded Cohn-
Kanade (CK+) databases. The results are comparable tothe accuracy obtained 95 percentage on the same databaseby other methods, such as support vector machine (SVM)and learning vector quantization (LVQ).
‚Ä¢The dataset used now is the Facial Expression Recog-nition Challenge (FERC-2013) [2], organized deep net-work, obtained an average accuracy of 67.02 percentageon emotion classiÔ¨Åcation.
III. P
ROPOSED MODEL
A. Emotion Detection Using Deep Learning
In this paper we use the deep learning (DL) open library
‚ÄúKeras‚Äù provided by Google for facial emotion detection, byapplying robust CNN to image recognition [12]. We usedtwo different datasets and trained with our proposed networkand evaluate its validation accuracy and loss accuracy. Imagesextracted from given dataset which have facial expressions forseven emotions, and we detected expressions by means of anemotion model created by a CNN using deep learning. Wehave changed a few steps in CNN as compared to previousmethod using a keras library given by Google and alsomodiÔ¨Åed CNN architectutre which give better accuracy . Weimplemented emotion detection using keras with the proposednetwork.
B. CNN Architecture
The networks are program on top of keras, operating
on Python, using the keras learn library. This environmentreduces the code‚Äôs complexity, since only the neuron layers
need to be formed, rather than any neuron. The software
also provides real-time feedback on training progress andperformance, and makes the model after training easy tosave and reuse. In CNN architecture initially we have toextract input image of 48*48*1 from dataset FERC-2013.The network begins with an input layer of 48 by 48 which
matches the input data size parallelly processed through two
similar models that is functionality in deep learning, andthen concatenated for better accuracy and getting features ofimages perfectly as shown in Fig.1 which is our proposedmodel, Model-A . There are two submodels for the extraction
of CNN features which share this input and both have
same kernel size. The outputs from these feature extraction
sub-models are Ô¨Çattened into vectors and concatenated intoone long vector matrix and transmitted to a fully connectedlayer for analysis before a Ô¨Ånal output layer allows forclassiÔ¨Åcation.This models contains convolutional layer with 64 Ô¨Ålters each
with size of [3*3], followed by a local contrast normalization
layer, maxpooling layer, followed by one more convolutionallayer, max pooling, Ô¨Çatten respectively. After that weconcatenate two similar models and linked to a softmaxoutput layer which can classify seven emotions. We use
dropout of 0.2 for reducing over-Ô¨Åtting. It has been appliedto the fully connected layer and all layers contain units ofrectiÔ¨Åed linear units (ReLu) activation function.First we are passing our input image to convolutional layer
which consists of 64 Ô¨Ålters each of size 3 by 3, after thatit passes through local contrast normalization can removeaverage from neighbourhood pixels leads to get quality offeature maps, followed by ReLu activation function. Maximumpooling is used to reduce spatial dimension reduction soprocessing speed will increase. We are using concatenation forgetting features of images (eyes, eyebrows, lips, mouth etc)perfectly so that prediction accuracy improved as comparedto previous model. Furthermore, it is followed by fullyconnected layer and softmax for classifying seven emotions.A second layer of maxpooling is added to reduce the numberof dimensionality. Here, we use batch normalization, dropout,ReLu activation function, categorical cross entropy loss,adam optimizer, softmax activation function in ouput layerfor seven emotion classiÔ¨Åcation.
In JAFEE dataset, input image size is adjusted to that
128*128*3. The network starts with an input layer of 128by 128 which matches the input data size parallely processedthrough two similar models as shown in Fig.1. Furthermore,it is concatenated and pass through one more softmax layerfor emotion classiÔ¨Åcation and all procedure is same as above.
InModel-B , previously proposed by Correa et al. [2], the
network starts with a 48 by 48 input layer, which matchesthe size of the input data. This layer is preceded by oneconvolutional layer, a local contrast normalization layer, andone layer of maxpooling, respectively. Two more convolutionallayers and one fully connected layer, connected to a softmaxoutput layer, complete the network. Dropout has been appliedto the fully connected layer and all layers contain units of
ReLu.
IV . E
XPERIMENT DETAILS
We develop a network based on the concepts from [12],
[13] and [14] to assess the two models ( Model-A and Model-
B) mentioned above on their emotion detection capability.
This section describes the data used for training and testing,explains the details of the used data sets and evaluates theresults obtained using two different datasets with two models.
A. Datasets
Neural networks, and particularly deep networks, needs
large amounts of training data. In addition, the choice ofimages used for the training is responsible for a large part ofthe eventual model‚Äôs performance. It means the need for a dataset that is both high quality and quantitative. Several datasetsare available for research to recognize emotions, ranging froma few hundred high resolution photos to tens of thousandsof smaller images. The two, we will be debating in thiswork, are the Japanese Female Face Expression (JAFFE) [15],Facial Expression Recognition Challenge (FERC-2013) [16]which contains seven emotions like anger, surprise, happy, sad,disgust, fear, neutral.
2
Authorized licensed use limited to: University of Wollongong. Downloaded on August 13,2020 at 17:55:09 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 1: Network Architecture
The datasets primarily vary in the amount, consistency,
and cleanness of the images. For example, the FERC-2013collection has about 32,000 low-resolution images. It can alsobe noted that the facial expressions in the JAFFE (i.e. further
extended as CK+ ) are posed (i.e. clean), while the FERC-2013
set displays ‚Äúin the wild‚Äù emotions. This makes it harder tointerpret the images from the FERC 2013 set, but given thelarge size of the dataset, a model‚Äôs robustness can be beneÔ¨Åcialfor the diversity.
B. Training Details
We train the network using GPU for 100 epochs to ensure
that the precision converges to the optimum. The network willbe trained on a larger set than the one previously describedin an attempt to improve the model even more. Training willtake place with 20,000 pictures from the FERC-2013 datasetinstead of 9,000 pictures. The FERC-2013 database also usesnewly designed veriÔ¨Åcation (2000 images) and sample sets(1000 images). It shows number of emotions in the Ô¨Ånal testingand validation set after training and testing our model. Theaccuracy will be higher on all validation and test sets than inprevious runs, emphasizing that emotion detection using deepconvolutional neural networks can improve the performanceof a network with more information.C. Results using Proposed Model
In emotion detection we are using three steps, i.e., face
detection, features extraction and emotion classiÔ¨Åcation usingdeep learning with our proposed model which gives betterresult than previous model. In the proposed method, computa-tion time reduces, validation accuracy increases and loss alsodecreases, and further performance evaluation achieved which
compares our model with previous existing model. We testedour neural network architectures on FERC-2013 and JAFFE
database which contains seven primary emotions like sad, fear,happiness, angry, neutral, surprised, disgust.
Fig.2 shows the proportions of detected emotions in a single
image of FER dataset. Fig.2(a) shows the image, whereasthe detected emotion proportions are shown in Fig.2(b). It
is clearly observable that neutral has higher proportion than
other emotions. That means, the emotion detected for thisimage (in Fig.2(a)) is neutral. Similarly, Fig.3 show anotherimage and corresponding emotion proportions. From Fig.3(b),it is observable that happy emotion has higher proportion thanothers. That suggests that image of Fig.3(a) detects happy
emotions.
Similarly, performance is evaluated for all the test images
of the dataset. We have achieved 95 percentage for happy, 75
3
Authorized licensed use limited to: University of Wollongong. Downloaded on August 13,2020 at 17:55:09 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 2: (a) Image, (b) Proportion of emotions.
Fig. 3: (a) Image, (b) Proportion of emotions.
percentage for neutral, 69 percentage for sad, 68 percentage
for surprise, 63 percentage for disgust, 65 percentage for fearand 56 percentage for angry. On an average we are gettingaverage accuracy of 70.14 percentage using our proposedmodel.
The confusion matrix of classiÔ¨Åcation accuracy is shown in
TABLE I. We get an average validation accuracy of 70.14percentage using our proposed model in facial emotiondetection using FER dataset.
Fig.4 shows the result of test sample related to surprise
emotion from JAFFE dataset, and our proposed model alsopredicted the same emotion with reduced computation timeas compared to previous existing model B. Similarly, perfor-
mance is evaluated for all the test samples of JAFFE dataset.When we are using JAFFE dataset we are getting validationaccuracy of 98.65 percentage which is better than previous
result and it takes less computational time per step.
Fig. 4: Prediction of emotion.
V . PERFORMANCE EV ALUATION
In FER dataset we train on 32,298 samples which is validate
on 3589 samples, and in JAFFE dataset we train 833 samples,which is validate on 148 samples for calculation of validationaccuracy, validation loss, computational time per step upto to
4
Authorized licensed use limited to: University of Wollongong. Downloaded on August 13,2020 at 17:55:09 UTC from IEEE Xplore.  Restrictions apply. 
TABLE I: Confusion Matrix ( %) for emotion detection using proposed model
Emotions Angry Sad Happy Disgust Fear Neutral Surprise
Angry 56 12 3 9 8 11 1
Sad 10 69 2 6 9 2 2
Happy 0 0 95 0 0 3 2
Disgust 7 13 0 63 8 5 4
Fear 9 8 3 2 65 10 3
Neutral 2 1 8 1 7 75 6
Surprise 7 3 11 0 3 8 68
Average accuracy = 70.14 ( %)
TABLE II: Qualitative assessment of our proposed model for emotion detection
Model (Dataset) Validation accuracy ( %) Validation loss Computation time per step (msec.)
Proposed Model A (FERC-2013) 70.14 1.7577 16 msec.
Model B (FERC-2013) 67.02 2.0389 45 msec.
Proposed Model A (JAFFE) 98.65 0.1694 284 msec.
Model B (JAFFE) 97.97 0.1426 462 msec.
100 and 50 epochs respectively shown in TABLE II. The aim
of the training step is to determine the correct conÔ¨Ågurationparameters for the neural network which are: number of nodesin the hidden layer (HL), rate of learning (LR), momentum
(Mom), and epoch (Ep). Different combinations of theseparameters have been tested to Ô¨Ånd out how to achieve thebetter recognition rate.From Table II, it is observed that our proposed model shows
70.14%average accuracy compared to the 67.02 %average
accuracy reported in model B FOR FER dataset. In this case ofJAFFE database, we achieved average accuracy 98.65 %which
is also higher than model B.
VI. C
ONCLUSION
In this paper, we have proposed a deep learning based
facial emotion detection method from image. We discussour proposed model using two different datasets, JAFFE andFERC-2013. The performance evaluation of the proposed
facial emotion detection model is carried out in terms ofvalidation accuracy, computational complexity, detection rate,learning rate, validation loss, computational time per step. We
analyzed our proposed model using trained and test sampleimages, and evaluate their performance compare to previous
existing model. Results of the experiment show that the modelproposed is better in terms of the results of emotion detectionto previous models reported in the literature. The experimentsshow that the proposed model is producing state-of-the-art
effects on both two datasets.
R
EFERENCES
[1] S. Li and W. Deng, ‚ÄúDeep facial expression recognition: A survey,‚Äù
arXiv preprint arXiv:1804.08348, 2018.[2] E. Correa, A. Jonker,M.Ozo, andR.Stolk, ‚ÄúEmotion recognition using
deep convolutional neural networks,‚Äù Tech. Report IN4015, 2016.
[3] Y . I. Tian, T. Kanade, and J.F.Cohn, ‚ÄúRecognizing action units for facial
expression analysis,‚Äù IEEE Transactions on pattern analysis and machine
intelligence, vol. 23, no. 2, pp. 97‚Äì115, 2001.
[4] C. R. Darwin. The expression of the emotions in man and animals. John
Murray, London, 1872.
[5] P. Ekman and W. V . Friesen. Constants across cultures in the face and
emotion. Journal of personality and social psychology, 17(2):124, 1971.
[6] J. Nicholson, K. Takahashi, and R. Nakatsu. Emotion recognition in
speech using neural networks. Neural computing applications, 9(4):
290‚Äì296, 2000.
[7] B. Fasel and J. Luettin. Automatic facial expression analysis: a survey.
Pattern recognition, 36(1):259‚Äì275, 2003.
[8] A. Krizhevsky and G. Hinton. Learning multiple layers of features from
tiny images, 2009.
[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A
large-scale hierarchical image database. In Computer Vision and Pattern
Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248‚Äì255.
IEEE, 2009.
[10] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiÔ¨Åcation
with deep convolutional neural networks. In Advances in neural infor-
mation processing systems, pages 1097‚Äì1105, 2012.
[11] Y . Lv, Z. Feng, and C. Xu. Facial expression recognition via deep
learning. In Smart Computing (SMARTCOMP), 2014 InternationalConference on, pages 303‚Äì308. IEEE, 2014.
[12] TFlearn. TÔ¨Çearn: Deep learning library featuring a higher-level api for
tensorÔ¨Çow. URL http://tÔ¨Çearn.org/.
[13] Open Source Computer Vision Face detection using haar cascades. URL
http://docs.opencv.org/master/d7/d8b/tutorialpyfacedetection.html.
[14] P. J. Werbos et al., ‚ÄúBackpropagation through time: what it does and
how to do it,‚Äù Proceedings of the IEEE, vol. 78, no. 10, pp. 1550‚Äì1560,
1990.
[15] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I.
Matthews. The extended cohn-kanade dataset (ck+): A complete dataset
for action unit and emotion-speciÔ¨Åed expression. In Computer Visionand Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer
Society Conference on, pages 94‚Äì101. IEEE, 2010.
[16] Kaggle. Challenges in representation learning: Facial expression recog-
nition challenge, 2013.
5
Authorized licensed use limited to: University of Wollongong. Downloaded on August 13,2020 at 17:55:09 UTC from IEEE Xplore.  Restrictions apply. 
"
http://ieeexplore.ieee.org/document/8723728/,"2019 International Conference on Advanced Science and Engineering (ICOASE),  
University of Zak ho, Duhok Polytechnic University, Kurdistan Region, Iraq  
70 
 978-1-5386 -9343 -8/19/$31.00 ¬©2019 IEEE  Facial Expression C lassification Based on  SVM, 
KNN and MLP  Classifiers  
Hivi Ismat Dino  
Dept. of Computer Science  
University of Zakho  
Duhok, Iraq  
hivi.dino@gmail.com  Maiwan Bahjat Abdulrazzaq  
Dept. of Computer Science  
University Of Zakho  
Duhok,Iraq  
maiwan. abdulrazzaq@uoz.edu.krd
 
Abstract‚ÄîFacial Expression Recognition (FER) has been an 
active topic of papers that were researched during 1990s till now, 
according to its importance, FER has achieved an extremely role 
in image processing area. FER typically pe rformed in three 
stages include, face detection, feature extraction and 
classification. This paper presents an automatic system of face 
expression recognition  which is able to  recognize all eight  basic 
facial expressions which are (normal, h appy, angry,  contempt,  
surprise, sad, fear and disgust) while many FER systems were 
proposed for recognizing only some of face expressions. F or 
validating the method, the Extended Cohn -Kanade (CK+) 
dataset is used. The presented  method uses Viola -Jones 
algorithm fo r face  detection. Histogram of Oriented G radients 
(HOG) is used as a descriptor for feature extraction from the 
images of expressive faces. Principal Component A nalysis (PCA) 
applied to reduce dimensionality of the Features, to obtaining the 
most significant fea tures. Finally, the presented method used  
three different classifiers which are Support Vector Machine 
(SVM), K -Nearest N eighbor (KNN) and Multilayer Perceptron 
Neural N etwork (MLPNN) for classifying the facial expressions 
and the results of them are compa red. The experimental  results 
show that the presented method provides  the recognition rate 
with 93.53 % when using SVM classifier, 82.97% when using 
MLP classifier and 79.97% when using KNN classifier which 
refers that the presented method provides better r esults while 
using SVM as a classifier . 
 Keywor ds‚Äîface expression recognition, histogram of 
oriented gradients , principle component analysis, support 
vector machine, k -nearest neighbor, multi -layer perceptron  
I. INTRODUCTION  
Facial recognition has been one o f the significant and 
interest areas in the field of secu rity and biometric since 
1990s, naturally humans communicate their opinions with 
others, also can see reactions through emotions and intentions, 
therefore it is important in natural communication  [1]. Several 
advance d research  produced in terms of face tracking and 
detection, the mechanisms of feature extraction and the 
techniques of expression classification  [2]. Human have an 
exceptionally face recognition system, the brain of human is 
so sophisticated in such a way that it is able to recognize 
different faces with very f ast changes in appearance. 
Researchers have dependa bly been stunned by the capacity of 
human brain to recognize face in  multiple different situations, 
many efforts were made to reproduce the human brain system. 
Based on this idea, many algorithms were produced for face 
recognition  and Face detection  [3].  The most pop ular approache s doing facial expression 
analysis are Support Vector M achine  (SVM ) [4], Artificial 
Neural N etwork ( ANN ) [5], Active Appeara nce Model 
(AAM) [6] and etc . In this paper  the utilized  classifiers are  
SVM, Multilayer Perceptron (MLP) [7] and KNN  [8]. SVM is 
a good algorithm that used to create a model for Facial 
Expre ssion classification, it tries  to find a hyper plane in an N -
dimensional space that separate the classes of data points , with 
aim to find a plane that has the maximum margin  separation, 
the productivity of recognition relies on th e me thod of feature 
extraction [9]. MLP is one of the most common technique of 
neural network which is based on backpropagation algorithm 
[10]. KNN is a non-parametric  method in pattern recognition, 
it is the simplest algorithm among all machine learning 
algorithms [11]. 
There are three approaches for recognizing faces with 
different face expressions  [12] which are : feature based -
techniques techniques, model -based techniques  [13] and 
correlat ion-based techn iques.  Feature -based technique is 
classified into geometric feature -based, appearance fea ture-
based and movement feature -based method . The most 
common used approaches of feature -based techniques are 
geometric based and appearance -based metho ds. In the 
geometric al feature -based technique, geometric parameters 
such as curves, pointers ‚Ä¶etc., which are used to represent the 
image locating different face parts like eyes, ears, nose, 
mouth ‚Ä¶ etc. Also, by estimating their relative width, position , 
the geometric shapes construct ed for representing these 
parameters. According to the method of appearance based , the 
importance is disposed to pixel values instead of the shape of 
feature components or relative distance. In this method there 
are many differ ent parameters of pixels used such as 
histogram, intensity  and etc. The corresponding values are 
performed in a template as a 2D array. The combination of 
these two approaches is performed in some algorithms for 
getting a good rate of recognition . 
There ar e three main steps for Face expression recognition: 
(1) face detection, (2) feature extraction, (3)  expression 
recognition. Different alg orithms are used for each step.  For 
validating the Face Expression Recognition (FER) many 
dataset were used such as Coh n Kanade (CK), Extended Cohn 
Kanade (CK+), Yale, Jaffe and many other.  In this paper CK+ 
dataset is used to validate the presented method due to its been 
one of the most common dataset for FER system, it released in 
2000 for the purpose to promote the rese arch in to an 
automatic system of expression detection [14]. In FER system, 
2019 International Conference on Advanced Science and Engineering (ICOASE),  
University of Zakho, Duhok Polytechnic University, Kurdistan Region, Iraq  
71 
 Face detection  [15] is considered as a first step for any 
techniques of  image processing. This step is required to 
normalize our data , in other words the image we get has been 
taken under different requirements and conditions using 
different equipment. The image is varying in size and also has 
been affected by many disruptions such as backgrounds, noise, 
illumination variations ‚Ä¶etc. thus; the first step is image 
standardization in FER whic h includes enhancement, resizing, 
noise removal of the input image. These steps are done in 
preprocessing stage. In addition, many techniques of FER 
utilize the gray scale images instead of color images, this 
conversion also counts as a preprocessing step.  Precisely, in 
preprocessing the images are standardized. The most prevalent 
techniques utilized for face detection are Viola -Jones  [16], 
Edge detection, Kirsch, Laplaci an, Sobel, Canny, prewitt etc.  
In this work we used Viola -Jones algorithm for face 
detection, which was proposed by Paul V iola and  Michael 
Jones in 2001 [17]. Generally, dimension reduction loses 
information, PCA tends to reduce that  information loss. PCA 
is a standard method utilized in signal processing and 
statistical pattern recognition for data reduction. Most of times 
the pattern contains much redundant information, PCA used to 
map it in to a feature vector in order to get rid f rom redundant 
information and obtain only important information. The role 
of those extracted features is to identify the input patterns.  
Histogram of Oriented Gradients (HOG) is a n image shape 
descriptor that can counts how many time the occurrence of 
gradient orientations in special portions of an image and 
which used for th e principle of object detection , also by 
instinct useful to form the shape of the facial muscles by edge 
analysis  [18].  Principle Component Analysis ( PCA ) method 
used widely to reduce the number of features in  image in order 
to obtain an available covering image that is coverin g about 
99% of the varia tion [19]. PCA technique is applied  in this 
paper to reduce the dimensionality of HOG features. Finally,  
SVM, MLP and KNN methods are utilized as a classifier for 
expression r ecognition.  
The main goal of the presented method is to recognize all 
eight  facial expressions which are (normal, happy, sad, angry, 
contempt, surprised, fear and disgust) while many modern 
FER system were produced for recognizing only s ome face 
expression s. The presented research focused on the 
investigation techniques that are able in  increasing  both the 
computational efficiency and recognition accuracy by 
applying va rious classification techniques and comparing their 
results.  
Section II of the paper give s an overview of the related 
work. Section III describes the steps of proposed system. 
Section IV presents the experimental results and discussion. 
Finally, section V shows the conclusion of the paper.  
II. RELATE WORKS  
Reference  [20] has proposed a novel me thod of FER using 
equable Principle Component Analysis (EPCA) as a 
representation of expression features and Linear Regression 
Classification (LRC) [21] is implement ed as a classifiers for 
expressions. EPCA deals with the original image by 
preserving the useful information and reducing the feature vector‚Äôs dimension of data. LRC compacts as a linear 
regression problem with the face recognition problem. In this 
paper t he reproduction experiment was commanded on the 
Yale and JAFFE Database of face. The authors proposed the 
method of PCA  for extracting the quantity of the face feature 
that has a great ability in improving the generalization 
performance and the accuracy of  the feature vector. According 
to this paper the proposed method is the improved method of 
PCA that works through changing the feature vectors for 
producing  the transformation of high dimensional data in to 
the low dimensional data. Each class has an avera ge value that 
is an image of linear increase. The authors found that the 
classifier of LRC method is very effective for identifying the 
other expressions . The recognition rate gained 89.1% on Yale 
database which is applied on five expressions while t he 
recognition rate gained 91.1 % on JAFFE database which 
applied on eight expressions.  
Reference [22] presented a FER system to recognize six 
face emotions. It utilized th e method of cascade regression 
tree for feature extraction and used three machine learning 
algorithms for classification which are SVM, NN and logistic 
regression and compared their results. The presented method 
applied on CK+ dataset. The recognition accu racy gained 
77.06% while using logistic regression classifier, 80% while 
using NN classifier and 89% on SVM.  
Reference [23] described both PCA and kernel -based PCA 
approach in FER system for three facial expressions of 3D 
face images.  KNN cla ssifier is used to recognize the face 
expressions.  Imperial College London dataset utilized for 
validating the system. The experimental results show the 
demonstration of kernel PCA being outperforming PCA by 
obtaining 77.29% of rate while PCA obtained on ly 52.69% of 
recognition rate.  
Reference  [24] proposed a new recognition method of face 
expression recognition using the combination of Local 
Directional Number Pattern (LDN) and Loca l Tetra Pattern 
(LTrP). This method uses Modified Decision Based 
Unsymmetrical Median Filter (MDBUTMF) for noise 
removal. LDN descriptor is organized based on the descriptor 
of eight Gaussian edge. The authors used the Japanese female 
facial expression dat abase , the used expressions are (disgust, 
smile, sad and surprise). The proposed method encrypts the 
relationship between the indicated pixel and its neighbors. 
LDPN utilizes the stable directional information against noise 
rather than intensity for coding  the various patterns from the 
texture of face. LTrP technique uses the center pixel direction 
to characterize the spatial frame of the local texture. LTrP 
method encrypts the image based on the pixel direction which  
is calculated by vertical and horizonta l derivatives. To display 
the proposed face expressions, the proposed technique goes 
through mask generation and edge detection of Gaussian; also 
after processing LDN and LTrP process, MLDN histogram 
and LTrP histogram are calculate for training up the fac e 
expressions by using the SVM classifier . In experimental 
results, it shows that the accuracy of FER was higher than 
90%.  
In [25] the authors suggested a dynamic feature -based 
method of facial expression for video of face sequences  based 
2019 International Conference on Advanced Science and Engineering (ICOASE),  
University of Zakho, Duhok Polytechnic University, Kurdistan Region, Iraq  
72 
 on a low dimensional feature space through extracting a 
Pyramid representation of uniform Temporal Local binary 
Pattern (PTLBPu2)using orthogonal planes (XT and YT). The 
proposed method is then used for selecting the most 
discriminating sub regions.  Feature space is reduced that is 
about to be intended on low -dimensional feature space 
thorough applying the PCA method. C4.5 algorithm and SVM 
classifier is used for facial expression classification. 
Experiments proceeded on MMI and CK+ databases which 
are from famous databases of facial expression. The 
experimental result shows that the accuracy of recognition rate 
obtained 92% with uncontrolled environment.  
TABLE I.  COMPARISON SUMMERY OF RELATED WO RKS 
Literature  Feature 
Extraction  Classifier   
Emotion  
No. Datase t Pros  Cons  
Zhu et 
al.[20]  EPCA  LRC  7 Yale 
and 
JAFFE  
 Error free 
recognition  
achievement  Pollution 
problem in 
face 
recogni tion  
Bilkhu et 
al.[22] Cascade 
Regression  SVM  
NN 
Logistic 
regression  6 Ck+ Suitable for 
real time 
applications  Less 
effective 
recognition 
performance  
Peter et al . 
[23] PCA and 
Kernel 
PCA  KNN  4 Imperial  
College 
London  Efficient for 
3D face 
recognition  Less 
accuracy  
Emmanuel 
and Revina 
[24]  LDN 
+LTrP  SVM  4 JAFFE  Robust in 
noisy 
removal  Less 
number of 
recognized 
emotion  
Abdallah et 
al. [25] LDT  
feature 
space  SVM  6 CK+  More 
Reliable  Less 
accuracy  
       
Researchers in the related  papers used various methods of 
feature extraction and classification wi th different numbers of 
facial expressions. Our system is able to recognize all eight  
basic facial expressions while  researchers in [22-25] is able to 
recognize fe wer number of facial expressions and with lower 
recognition rate compared with our method.  Table I show the 
comparison between our system and others in accuracy and 
methods been used.  
III. METHODOLOGY  
       In this paper, The Extended Cohn -Kanade (CK+) 
Dataset  was used. The dataset contains different emotions for 
each person. Dataset is consisting of image sequences 
converted from video frame. There are many missing label 
sequences in the dataset. The labeled emotion was considered 
and used in ou r experiment. First image frame used as a natural 
emotion, last image frame used as a labeled emotion in the 
dataset.  Our work is the process for identifying the human‚Äôs 
expressions with eight  emotions whi ch are: neutral, anger , 
disgust, fear, happy, sadness and surprise . Fig .1. shows  the 
block diagram of the presented  method.  
 
Fig. 1. Block diagram for the presented method  
A. Preprocessing  
      This step is considered as a first step for any techniques of  
image processing.  In this work we used Viola -Jones algorithm 
for face detec tion. The original images  were digitized into 
either 640x490 or 640x480 pixel arrays with  8- bit gray -scale 
or 24 -bit color values.   
                
  
Fig. 2. Some emotions on detected face  
Viola -Jones is one of the most popular face detection 
algorithms because o f having a robust and real time 
characteristic in face detection. The algorithm goes through 
four stages which are Haar features selection  [26], creating an 
integral image, ad boost training and cascading classifiers. 
After face detection, the images were normalized and resized 
to 256 by 256 -pixel  arrays  in gray-scale . Table 2  show t he tot al 
emotion labeled and face detected.  Fig.2. show some emotions 
on detected faces.  To calculate the Haar transform of an array 
of n samples:  
ÔÇ∑ Treat the array as  n/2 pairs called  (a, b)  
ÔÇ∑ Calculate  (a + b) / sqrt(2)  for each pair, these values 
will be the firs t half of the output array.  
ÔÇ∑ Calculate  (a - b) / sqrt(2)  for each pair, these values 
will be the second half.  
ÔÇ∑ Repeat the process on the first half of the array.  
(the array length should be a power of two)  
2019 International Conference on Advanced Science and Engineering (ICOASE),  
University of Zakho, Duhok Polytechnic University, Kurdistan Region, Iraq  
73 
 TABLE II.  TOTAL DETECTED FACE E MOTIONS  
Value  Count  Percent  
neutral 317 50.0%  
anger  45 7.1%  
contempt  18 2.8%  
Disgust  55 8.7%  
fear 24 3.8%  
happy  68 10.7%  
sadness  26 4.1%  
surprise  81 12.8%  
Total  634 100.0  
 
B. Feature extraction  
This is the most significant stage in FER. The efficiency of 
FER is depending on the t echniques used in this stage. The 
major techniques of feature extraction are Local Binary 
Pattern (LBP), Gabor filter, Principal Component Analysis 
(PCA) etc. In this work we used HOG algorithm for feature 
extraction which was proposed by Dalal and Higgs  [27].  
                    
  
Fig. 3. Extracted HOG features from a grayscale input image  
The basic idea behind the HOG descriptor is that the shape 
and appearance of local object inside the image can be 
charac terized by the allocation of gradients intensity or edge 
directions. HOG is the combination of gradient direction 
histograms. To improve the accuracy, local histograms are 
capable to be construct -normalized which is done by choosing 
the wider region on ima ge which is called a block and 
calculating its measure of intensity,  after that the produced 
value is used to normalize all cells inside the block. Fig.3. 
shows  the visualization of Extract histogra m of oriented 
gradients (HOG).                         
Features  on face Construct -normalization produces better 
invariant to shadowing and illumination. HOG is a better 
descriptor than others because of its being operated on local 
cells and it is static for photometric and geometric 
transformation inspire of bein g object orientation.  The total 
number of extracted HOG features is 8100, it is hug, and 
therefore dimension reduction was essential.            
C. Feature Reduction  
      Generally, dimension reduction loses information; PCA 
tends to reduce that information l oss. PCA is a standard 
method utilized in signal processing and statistical pattern recognition for data reduction. Most of times the pattern 
contains much redundant information, PCA used to map it in 
to a feature vector in order to get rid from redundant 
information and obtain only important information. The role 
of those extracted features is to identify the input patterns. The 
face image of 256x256 pixel size in two -dimension produce 
8100  HOG features, which can be considered as one -
dimension vector of feature . PCA  used on HOG feature to 
reduce the dimensionality, we suggest to keep 90% of original 
information. The final feature size was 247 for each sample.  
IV. EXPERIMENTAL RESULTS  
A. Dataset  
The proposed method was validated using CK+ dataset, 
CK+ dataset con sist of the facial emotions of 210 adults, they 
were between 18 to 50 years old, 69% of members were 
female, 13% Afro -American, 81% Euro -American, and 6% 
other gatherings. CK+ consist of 539 image sequences of 
facial expressions from 123 subjects, among th em 327 
sequences have eight  emotion labels, image‚Äôs resolution 
is640x490 or 640x480 . After preprocessing, there were 634 
labeled images with eight emotion  [14]. 
B. Classifier  
In this stage the extracted features from face images are 
classified using a specific classifier to the respective classes of 
face expressions. The most widely used classifier is support 
vector machine (SVM) and Neural Networks (NN), which are 
used for classification. In this work we used three classifier 
which they are SVM, MLP and K -nearest neighbor KNN with 
comparing their results.  
V. RESULTS  
In the experiment, 10 -fold Cross -validation method is  used 
as model evaluation . The dataset is divided into 10- subsets  
with ten -time e valuation, each time the model uses nine -
subsets  as a training data, and use one remained  subset in 
testing phase, then compute the average of the  output results 
in ten tests. U sually  it is preferred method because it gives a 
model the opportunity to train on multiple train -test splits. 
This gives better estimation  of how well the model will 
perform on unseen data.   
To illustrate the results of Tables 2, 3 and 4 . The first 
attribute is the s pecificity , which is  the true negative rate of 
the considered class. The s pecificity  highest rate  value  is (1)  in 
disgust  emotion for SVM  classifier , also in fear , sadness  and 
surprise emotions  for K -NN classifier . The specificity  lowest 
rate value  is (0.6309) in neutral  emotion for K -NN classifier. 
The second attribute is the Recall . The Recall commonly 
called Sensitivity, corresponds to the true positive rate of the 
consider class. The Recall highest rate value i s (1) in happy  
emotion for K -NN classifier. The Recall lowest rate value is 
(0.0385 ) in sadness  emotion for K -NN classifier. The third 
attribute is the False Positives (FP) Rate  which mean  
incorrectly classified . The value of FP is (0) when take (1) in 
2019 International Conference on Advanced Science and Engineering (ICOASE),  
University of Zakho, Duhok Polytechnic University, Kurdistan Region, Iraq  
74 
 Specificity . The Last attribute is the F-Measure  which is 
the harmonic mean  of precision and recall . The F-Measure  
highest rate value is ( 0.9853 ) in happy  emotion for SVM 
classifie r while the F-Measure  lowest rate value is ( 0.0741 ) in 
sadness  emotion for K -NN classifier.  
The correct classification rate is 93.53% using the SVM 
classifier. The MLP correct classification rate was 82.97%, 
and the K -NN correct classification rate was 79.97%. The 
accuracy and performance average weights of each classifier 
for all emotions  are given in table 2, 3 and 4.  
TABLE III.  PERFORMANCE DETAILS F OR SVM  CLASSIFIER WITH 10-
FOLD CROSS -VALIDATION  
SVM accuracy = 93.89%.  
Label  Specificity  Recall  FP-Rate  Precision  F-Measure  
neutral  0.9117  0.9874  0.0883  0.9179  0.9514  
anger  0.9932  0.8889  0.0068  0.9091  0.8989  
contempt  0.9935  0.6111  0.0065  0.7333  0.6667  
disgust  1.0000  0.9455  0.0000  1.0000  0.9720  
fear 0.9984  0.7083  0.0016  0.9444  0.8095  
happy  0.9982  0.9853  0.0018  0.985 3 0.9853  
sadness  0.9984  0.6538  0.0016  0.9444  0.7727  
surprise  0.9964  0.9383  0.0036  0.9744  0.9560  
Average  0.9862  0.8398  0.0138  0.9261  0.8765  
TABLE IV.  PERFORMANCE DETAILS F OR K-NN CLASSIFIER WITH 10-
FOLD CROSS -VALIDATION   
K-NN accuracy = 79.97%  
Label  Specificity  Recall  FP-Rate  Precision  F-Measure  
neutral  0.6309  0.9937  0.3691  0.7292  0.8411  
anger  0.9983  0.0667  0.0017  0.7500  0.1224  
contempt  0.9984  0.1111  0.0016  0.6667  0.1905  
disgust  0.9983  0.7818  0.0017  0.9773  0.8687  
fear 1.0000  0.1250  0.0000  1.0000  0.2222  
happy 0.9876  1.0000  0.0124  0.9067  0.9510  
sadness  1.0000  0.0385  0.0000  1.0000  0.0741  
surprise  1.0000  0.8889  0.0000  1.0000  0.9412  
Average  0.9517  0.5007  0.0483  0.8787  0.5264  
TABLE V.  PERFORMANCE DETAILS F OR MLP  CLASSIFIER WITH 10-
FOLD CROSS -VALIDATION  
NN accuracy = 8 2.97%  
Label  Specificity  Recall  FP-Rate  Precision  F-Measure  
neutral  0.7539  0.9811  0.2461  0.7995  0.8810  
anger  0.9847  0.5778  0.0153  0.7429  0.6500  
contempt  0.9919  0.1111  0.0081  0.2857  0.1600  
disgust  0.9983  0.7818  0.0017  0.9773  0.8687  
fear 0.9984  0.4583  0.0016  0.9167  0.6111  
happy  0.9894  0.8676  0.0106  0.9077  0.8872  
sadness  0.9918  0.4231  0.0082  0.6875  0.5238  
surprise  0.9946  0.7778  0.0054  0.9545  0.8571  
Average  0.9629  0.6223  0.0371  0.7840  0.6799  
       
Experimental results show that our method provides be tter 
accuracy comparing to the results in research [28] which used 
the same data set, it used facial landmarks to extract features 
and applied different clas sifiers, in this research the 
combination of three dataset used CK, Jaffe and PSL to train and test data, the proposed method tested on two emotions and 
obtained 65.35% of accuracy with SVM classifier, 70.87% 
accuracy with KNN classifier. Researchers in pa per [29] 
employed a Deep Convolution Neural Network (DCNN)  in 
order to extra ct deeper features for automatic emotion 
detection, KNN classifier and tested on their system, the 
recognition rate was 77.27%  on CK+ dataset. Researchers in 
[22] obtained 80.0% and 89.0% accuracy with  SVM and  
neural network . With KNN classifier [23] enhanced the 
recognition rate to 77.29%.  
TABLE VI.  THE COMPARISON OF REC OGNITION RATE OF PRE SENTED 
METHOD WITH STATE -OF-ART % 
Literature  Dataset  Classifier  Accuracy  
[22] CK+  SVM  
NN 80.0%  
89.0%  
[23] CK+  KNN  77.29  
[28] CK+Jaffe+PSL  SVM  
KNN  65.35%  
70.87%  
[29] CK+  KNN  77.27%  
ours CK+  SVM  
NN 
KNN  93.53%  
82.57%  
79.97%  
 
VI. CONCLUSION  
In this research  FER system  based on Histograms of 
oriented gradients  (HOG) using various machine learning 
algorithms was presented . The Extended Cohn -Kanade (CK+) 
dataset used as good data recourse to exam the classi fication 
of human Facial  Expression . Principal  component analysis 
(PCA) used to reduce the dimensionality of the features. This 
paper presents a system with ability to recognize all eight  
basic face expressions. 10-fold validation is used to train and 
test the classifiers. The t hree classifiers were utilized to 
perform the classification of facial expressions , SVM, NN and 
K-NN. The experiments results show that SVM proven to be 
better classifier with 93.53 % accuracy  of correct classification 
rate. In future,  we can use our own novel dataset which is in 
collecting progress and test the presented method with 
different machine learning algorithms  to provide better 
accuracy . 
       REFERENCES  
 
[1] T. Gehrig and H. K. Ekenel, ""A common framewor k for real -time 
emotion recognition and facial action unit detection,"" in Computer 
Vision and Pattern Recognition Workshops (CVPRW), 2011 IEEE 
Computer Society Conference on, 2011, pp. 1 -6. 
[2] S. H. Abdurrahim, S. A. Samad, and A. B. Huddin, ""Review on th e 
effects of age, gender, and race demographics on automatic face 
recognition,"" The Visual Computer, vol. 34, pp. 1617 -1630, 2018.  
[3] S. L. Fernandes and G. J. Bala, ""A Study on Face Recognition Under 
Facial Expression Variation and Occlusion,"" in Proceed ings of the 
International Conference on Soft Computing Systems, 2016, pp. 371 -
377. 
[4] G. Lei, X. -h. Li, J. -l. Zhou, and X. -g. Gong, ""Geometric feature based 
facial expression recognition using multiclass support vector machines,"" 
in 2009 IEEE Internationa l Conference on Granular Computing, 2009, 
pp. 318 -321. 
2019 International Conference on Advanced Science and Engineering (ICOASE),  
University of Zakho, Duhok Polytechnic University, Kurdistan Region, Iraq  
75 
 [5] M.-P. Loh, Y. -P. Wong, and C. -O. Wong, ""Facial expression recognition 
for e -learning systems using Gabor wavelet & neural network,"" in null, 
2006, pp. 523 -525. 
[6] C. Martin, U. Werner, and H. -M. G ross, ""A real -time facial expression 
recognition system based on active appearance models using gray 
images and edge images,"" in Automatic Face & Gesture Recognition, 
2008. FG'08. 8th IEEE International Conference on, 2008, pp. 1 -6. 
[7] P. Burkert, F. Trie r, M. Z. Afzal, A. Dengel, and M. Liwicki, 
""Dexpression: Deep convolutional neural network for expression 
recognition,"" arXiv preprint arXiv:1509.05371, 2015.  
[8] I. T. Meftah, N. Le Thanh, and C. B. Amar, ""Emotion recognition using 
KNN classification for user modeling and sharing of affect states,"" in 
International Conference on Neural Information Processing, 2012, pp. 
234-242. 
[9] M. Abdulrahman and A. Eleyan, ""Facial expression recognition using 
support vector machines,"" in 2015 23nd Signal Processing an d 
Communications Applications Conference (SIU), 2015, pp. 276 -279. 
[10] H. Boughrara, M. Chtourou, C. B. Amar, and L. Chen, ""Facial 
expression recognition based on a mlp neural network using constructive 
training algorithm,"" Multimedia Tools and Applicatio ns, vol. 75, pp. 
709-731, 2016.  
[11] S. Li, W. Deng, and J. Du, ""Reliable crowdsourcing and deep locality -
preserving learning for expression recognition in the wild,"" in 
Proceedings of the IEEE Conference on Computer Vision and Pattern 
Recognition, 2017, p p. 2852 -2861.  
[12] I. M. Revina and W. S. Emmanuel, ""A survey on human face expression 
recognition techniques,"" Journal of King Saud University -Computer and 
Information Sciences, 2018.  
[13] C.-L. Huang and Y. -M. Huang, ""Facial expression recognition using 
model -based feature extraction and action parameters classification,"" 
Journal of visual communication and image representation, vol. 8, pp. 
278-290, 1997.  
[14] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. 
Matthews, ""The extended cohn -kanade dataset (ck+): A complete 
dataset for action unit and emotion -specified expression,"" in Computer 
Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE 
Computer Society Conference on, 2010, pp. 94 -101. 
[15] D. Chawla and M. C. Trivedi, ""A comparat ive study on face detection 
techniques for security surveillance,"" in Advances in Computer and 
Computational Sciences, ed: Springer, 2018, pp. 531 -541. 
[16] T. PAUL, U. A. SHAMMI, M. U. AHMED, R. RAHMAN, S. 
KOBASHI, and M. A. R. AHAD, ""A Study on Face Dete ction Using 
Viola -Jones Algorithm in Various Backgrounds, Angles and Distances,"" 
International Journal of Biomedical Soft Computing and Human 
Sciences: the official journal of the Biomedical Fuzzy Systems 
Association, vol. 23, pp. 27 -36, 2018.  
[17] √Å. P. P ertierra, A. B. G. Gonz√°lez, J. T. Lafuente, and A. de Luis 
Reboredo, ""Communication Skills Personal Trainer Based on Viola -Jones Object Detection Algorithm,"" in International Conference on 
Intelligent Data Engineering and Automated Learning, 2018, pp. 722 -
729. 
[18] P. Carcagn√¨, M. Del Coco, M. Leo, and C. Distante, ""Facial expression 
recognition and histograms of oriented gradients: a comprehensive 
study,"" SpringerPlus, vol. 4, p. 645, 2015.  
[19] C. Y. Yong, R. Sudirman, and K. M. Chew, ""Facial expression 
monitoring system using pca -bayes classifier,"" in Future Computer 
Sciences and Application (ICFCSA), 2011 International Conference on, 
2011, pp. 187 -191. 
[20] Y. Zhu, X. Li, and G. Wu, ""Face expression recognition based on 
equable principal component analy sis and linear regression 
classification,"" in Systems and Informatics (ICSAI), 2016 3rd 
International Conference on, 2016, pp. 876 -880. 
[21] Y. Zhu, C. Zhu, and X. Li, ""Improved principal component analysis and 
linear regression classification for face rec ognition,"" Signal Processing, 
vol. 145, pp. 175 -182, 2018.  
[22] M. S. Bilkhu, S. Gupta, and V. K. Srivastava, ""Emotion Classification 
from Facial Expressions Using Cascaded Regression Trees and SVM,"" 
in Computational Intelligence: Theories, Applications an d Future 
Directions -Volume II, ed: Springer, 2019, pp. 585 -594. 
[23] M. Peter, J. -L. Minoi, and I. H. M. Hipiny, ""3D Face Recognition using 
Kernel -based PCA Approach,"" in Computational Science and 
Technology, ed: Springer, 2019, pp. 77 -86. 
[24] W. R. S. Em manuel and I. M. Revina, ""Face expression recognition 
using integrated approach of Local Directional Number and Local Tetra 
Pattern,"" in Control, Instrumentation, Communication and 
Computational Technologies (ICCICCT), 2015 International Conference 
on, 201 5, pp. 707 -711. 
[25] T. B. Abdallah, R. Guermazi, and M. Hammami, ""Facial -expression 
recognition based on a low -dimensional temporal feature space,"" 
Multimedia Tools and Applications, vol. 77, pp. 19455 -19479, 2018.  
[26] R. S. Stankoviƒá and B. J. Falkowski , ""The Haar wavelet transform: its 
status and achievements,"" Computers & Electrical Engineering, vol. 29, 
pp. 25 -44, 2003.  
[27] N. Dalal and B. Triggs, ""Histograms of oriented gradients for human 
detection,"" in international Conference on computer vision &  Pattern 
Recognition (CVPR'05), 2005, pp. 886 --893. 
[28] K. Lawrence, R. Campbell, and D. Skuse, ""Age, gender, and puberty 
influence the development of facial emotion recognition,"" Frontiers in 
psychology, vol. 6, p. 761, 2015.  
[29] K. Shan, J. Guo, W. You , D. Lu, and R. Bie, ""Automatic facial 
expression recognition based on a deep convolutional -neural -network 
structure,"" in 2017 IEEE 15th International Conference on Software 
Engineering Research, Management and Applications (SERA), 2017, 
pp. 123 -128. 
 
 
 
"
http://ieeexplore.ieee.org/document/5551215/,"IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 21, NO. 10, OCTOBER 2010 1685
2) When 1 ‚â§i‚â§N1and j>N1
G/prime(Œ±i)+G/prime(Œ≥‚àísŒ±i)=di‚àís‚àíŒ±i/D (33)
Œ±new
i=DŒ∫Œ±old
i
DŒ∫+1
+Dyi[f(xj)‚àíf(xi)‚àíyj+diyi]
DŒ∫+1.(34)
3) When 1 ‚â§i,j‚â§N1
G/prime(Œ±i)+G/prime(Œ≥‚àísŒ±i)=di‚àídjs
+sŒ≥/D‚àí2Œ±i/D (35)
Œ±new
i=DŒ∫Œ±old
i
DŒ∫+2
+Dyi[f(xj)‚àíf(xi)‚àídjyj+diyi+yjŒ≥/D]
DŒ∫+2.
(36)
4) When 1 ‚â§j‚â§N1andi>N1
G/prime(Œ±i)+G/prime(Œ≥‚àísŒ±i)=1‚àídjs
+sŒ≥/D‚àíŒ±i/D (37)
Œ±new
i=DŒ∫Œ±old
i
DŒ∫+1
+Dyi[f(xj)‚àíf(xi)‚àídjyj+yi+yjŒ≥/D]
DŒ∫+1.
(38)
Finally, we clip Œ±new
ito ensure it is in the range of [U,V].
The value of Œ±new
jis given by
Œ±new
j=Œ±old
j+yiyj(Œ±old
i‚àíŒ±new
i). (39)
REFERENCES
[1] V . Vapnik, The Nature of Statistical Learning Theory .N e wY o r k :
Springer, 1995.
[2] P. Williams, S. Li, J. Feng, and S. Wu, ‚ÄúA geometrical method to improve
performance of the support vector machine,‚Äù IEEE Trans. Neural Netw. ,
vol. 18, no. 3, pp. 942‚Äì947, May 2007.
[3] K. Kobayashi and F. Komaki, ‚ÄúInfo rmation criteria for support vector
machines,‚Äù IEEE Trans. Neural Netw. , vol. 17, no. 3, pp. 571‚Äì577, May
2006.
[4] Q. Tao, D. Chu, and J. Wang, ‚ÄúRecursive support vector machines for
dimensionality reduction,‚Äù IEEE Trans. Neural Netw. , vol. 19, no. 1, pp.
189‚Äì193, Jan. 2008.
[5] S. Decherchi, S. Ridella, R. Zunino, P. Gastaldo, and D. Anguita, ‚ÄúUsing
unsupervised analysis to constrain generalization bounds for support
vector classiÔ¨Åers,‚Äù IEEE Trans. Neural Netw. , vol. 21, no. 3, pp. 424‚Äì
438, Mar. 2010.
[6] D. D. Cox and R. L. Savoy, ‚ÄúFunc tional magnetic resonance imaging
(fMRI) ‚Äòbrain reading‚Äô: Detecting and classifying distributed patterns offMRI activity in human visual cortex,‚Äù Neuroimage , vol. 19, no. 2, pp.
261‚Äì270, 2003.
[7] S. LaConte, S. Strother, V . Cherkassky, J. Anderson, and X. Hu,
‚ÄúSupport vector machines for tempor al classiÔ¨Åcation of block de-
sign fMRI data,‚Äù Neuroimage , vol. 26, no. 2, pp. 317‚Äì329,
2005.
[8] S. Li, D. Ostwald, M. Giese, and Z. Kourtzi, ‚ÄúFlexible coding for
categorical decisions in the human brain,‚Äù J. Neurosci. , vol. 27, no. 45,
pp. 12321‚Äì12330, 2007.
[9] K. A. Norman, S. M. Polyn, G. J. Detre, and J. V . Haxby, ‚ÄúBeyond
mind-reading: Multi-voxel pattern analysis of fMRI data,‚Äù Trends Cogn.
Sci., vol. 10, no. 9, pp. 424‚Äì430, Sep. 2006.
[10] T. Mitchell, R. Hutchinson, R. Nicu lescu, F. Pereira, X. Wang, M. Just,
and S. Neman, ‚ÄúLearning to decode cognitive states from brain images,‚ÄùMach. Learn. , vol. 57, nos. 1‚Äì2, pp. 145‚Äì175, Oct.‚ÄìNov. 2004.
[11] Z. Wang, A. Childress, J. Wang, and J. Detre, ‚ÄúSupport vector machine
learning based fMRI data group analysis,‚Äù Neuroimage , vol. 36, no. 4,
pp. 1139‚Äì1151, Jul. 2007.
1045-9227/$26.00 ¬© 2010 IEEE[12] K. J. Friston, A. P. Holmes, K. J. Worsley, J. Poline, C. D. Frith, and
R. S. J. Frackowiak, ‚ÄúStatistical parametric maps in functional imaging:A general linear approach,‚Äù Human Brain Map. , vol. 2, no. 11, pp. 189‚Äì
210, 1995.
[13] M. Montemurro, R. Senatore, and S. Panzeri, ‚ÄúTight data-robust
bounds to mutual information combining shufÔ¨Çing and model selec-tion techniques,‚Äù Neural Computat. , vol. 19, no. 11, pp. 2913‚Äì2957,
Nov. 2007.
[14] C. Bishop, Neural Networks for Pattern Recognition . New York: Oxford
Univ. Press, 1995.
[15] L. Liang, V . Cherkassky, and D. R ottenberg, ‚ÄúSpatial SVM for feature
selection and fMRI activation detection,‚Äù in Proc. Int. Joint Conf. Neural
Netw. , 2006, pp. 1463‚Äì1469.
[16] A. Graf, F. Wichmann, H. Bulth√∂ff, and B. Sch√∂lkopf, ‚ÄúClassiÔ¨Åcation
of faces in man and machine,‚Äù Neural Computat. , vol. 18, no. 1, pp.
143‚Äì165, Jan. 2006.
[17] B. Sch√∂lkopf and A. Smola, Learning with Kernels: Support Vector
Machines, Regularization, Optimization and Beyond . Cambridge, MA:
MIT Press, 2002.
[18] J. C. Platt, ‚ÄúFast training of support vector machines using sequential
minimal optimization,‚Äù in Advances in Kernel Methods: Support Vector
Learning , B. Sch√∂lkopf, C. Burgess, and A. Smola, Eds. Cambridge,
MA: MIT Press, 1999, pp. 185‚Äì208.
[19] L. Glass, ‚ÄúMoire effect from random dots,‚Äù Nature , vol. 223, no. 5206,
pp. 578‚Äì580, Aug. 1969.
[20] D. Ostwald, J. M. Lam, S. Li, and Z. Kourtzi, ‚ÄúNeural coding of global
form in the human visual cortex,‚Äù J. Neurophysiol. , vol. 99, no. 5, pp.
2456‚Äì2469, May 2008.
[21] J. Duncan, ‚ÄúAn adaptive coding model of neural function in pre-
frontal cortex,‚Äù Nat. Rev. Neurosci. , vol. 2, no. 11, pp. 820‚Äì829, Nov.
2001.
[22] E. K. Miller, ‚ÄúThe prefrontal cortex and cognitive control,‚Äù Nat. Rev.
Neurosci. , vol. 1, no. 1, pp. 59‚Äì65, Oct. 2000.
[23] N. Sigala and N. K. Logothetis, ‚ÄúVi sual categorization shapes feature
selectivity in the primate temporal cortex,‚Äù Nature , vol. 415, no. 6869,
pp. 318‚Äì320, Jan. 2002.
[24] S. Li, S. D. Mayhew, and Z. Kourtzi, ‚ÄúLearning shapes the representation
of behavioral choice in the human brain,‚Äù Neuron , vol. 62, no. 3, pp.
441‚Äì452, May 2009.
Facial Expression Recognition in JAFFE Dataset Based
on Gaussian Process ClassiÔ¨Åcation
Fei Cheng, Jiangsheng Yu, and Huilin Xiong
Abstract ‚Äî The Gaussian process (GP) approaches to clas-
siÔ¨Åcation synthesize Bayesian methods and kernel techniques,which are developed for the purpose of small sample analysis.Here we propose a GP model and investigate it for the facialexpression recognition in the Japanese female facial expressiondataset. By the strategy of leave-one-out cross validation, theaccuracy of the GP classiÔ¨Åers reaches 93.43% without any featureselection/extraction. Even when tested on all expressions of anyparticular expressor, the GP classiÔ¨Åer trained by the other sam-ples outperforms some frequently used classiÔ¨Åers signiÔ¨Åcantly.In order to survey the robustness of this novel method, therandom trial of 10-fold cross validations is repeated many timesto provide an overview of recognition rates. The experimentalresults demonstrate a promising performance of this application.
Manuscript received October 26, 2009; revised July 21, 2010; accepted July
29, 2010. Date of publication August 19, 2010; date of current version October
6, 2010. This work was supported in part by Peking University through the
985 Project, in part by the Beijing Natural Science Foundation under Grant048SG/46810707-001 and Grant 4032013, and in part by the Natural ScienceFoundation of China through Project 60775008.
F. Cheng is with the Department of Mathematics, Beijing Jiaotong Univer-
sity, Beijing 100044, China (e-mail: fcheng@bjtu.edu.cn).
J. S. Yu is with the Department of Computer Science and Technology, Key
Laboratory of High ConÔ¨Ådence Softwar e Technologies, Ministry of Education,
Peking University, Beijing 100871, China (e-mail: yujs@pku.edu.cn).
H. L. Xiong is with the Department o f Automation, Shanghai Jiaotong
University, Shanghai 200240, China (e-mail: hlxiong@sjtu.edu.cn).
Digital Object IdentiÔ¨Åer 10.1109/TNN.2010.2064176
1686 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 21, NO. 10, OCTOBER 2010
Index Terms ‚Äî ClassiÔ¨Åcation, facial expression recognition,
Gaussian process model, kernel method.
I. I NTRODUCTION
Facial expression recognition (FER) is always one of the
most challenging issues in the research of artiÔ¨Åcial intelligenceand pattern recognition [1]. It also has many potential appli-
cations to human-computer interaction, data-driven animation,
image retrieval, human emotio n analysis, medical treatment,
etc. The statistical approach es of machine learning to FER
are mainly the training of cla ssiÔ¨Åers and the extraction of
semantic-level attributes [2 ] from a set of labeled digital
images of facial expressions. Due to the problem of high
dimensionality and relatively small sample in FER, manytechniques for the reduction of dimensionality and the feature
selection/extraction [3], such as wavelet transform [4], princi-
pal component analysis [5], independent component analysis(ICA) [6], kernel canonical correlation analysis [7], etc., have
been tested in the last two decades [8], as well as various
classiÔ¨Åcation methods, for instance, artiÔ¨Åcial neural network(ANN) [9], support vector machine (SVM) [10], [11], Fisher
discriminant analysis [12], and locally linear embedding [13].
The combinations of feature extraction and classiÔ¨Åcation
have been tried in many ways, such as Gabor Ô¨Ålter and ICA
combined with SVMs of distinct kernels [6]. The reported
accuracy of FER reported in the literature is about 90%, or
even better. For instance, [11] showed a recognition rate of95.71% using a combination of 2-D-LDA and an SVM classi-
Ô¨Åer. With previous feature selection/extraction engines, some
well-designed classiÔ¨Åers promise to yield good performance.The goal of this brief is to show that the Gaussian process
(GP) approach is able to give good results, close to the state
of the art, even without any feature selection/extraction.
Among the frequently used classiÔ¨Åers, ANNs and SVMs
often achieve good performance in practice and play the roleof baseline whenever comparisons are made. It has been
proved that the two-layer perceptrons are capable of simulating
any continuous function on a compact domain [14], [15].
If the hidden layer is composed of inÔ¨Ånitely many hidden
units, Neal showed in [16] that the distribution of functionsgenerated by such neural networks tends to GP for a broad
class of prior distributions over the parameters. With the
help of kernel methods, the separability of the nonlinearlytransformed observations is likely to be improved in a higher
dimensional space. And the ‚Äúkernel trick‚Äù makes the nonlinear
map not involved explicitly in the algorithms, which facilitates
the training and testing pro cedures. Recently, there has been
growing interest in applying the kernel methods in machinelearning to the data analysis [17]. GPs, as a Bayesian kernel
method [18], have become a promising scheme for the problem
of classiÔ¨Åcation/regression in recent years. Together with themodels of SVMs and Bayesian neural networks [19], GP mod-
els offer more and more desirable applications [20], [21], [22].
This brief focuses on the GP approach to FER. Section II
introduces the proposed method, and Section III describes the
Japanese female facial expression (JAFFE) dataset and thenpresents the experiments of GPs with polynomial kernels and
Gaussian radial basis function (RBF) kernels by three versionsof leave-one-out cross validation (LOO CV). Moreover, the
GP classiÔ¨Åer is compared with some other frequently usedclassiÔ¨Åers, such as SVM, k-nearest neighbor ( k-NN), naive
Bayes, classiÔ¨Åcation tree, etc., for the capability of recognizing
all expressions of a new person whose images are not inthe training set. In order to examine the robustness of the
proposed model, the novel method is tested by sufÔ¨Åciently
many independent random trials of 10-fold cross validations.
All the results show a promising performance in such applica-
tions. Finally, some further investigations and conclusions arementioned in Section IV .
II. C
LASSIFICATION MODEL OF GAUSSIAN PROCESS
Given the training data of x1,..., xn‚àà Rdwith the
corresponding target labels t=(t1,..., tn)T,t h ea i mo f
Bayesian multiple classiÔ¨Åcation of a new input xnew‚àà
Rdis the calculation of the conditional probability of
p(tnew=k|x1,..., xn,t,xnew),o rb r i e Ô¨Ç y p(tnew=k|D,t),
where k=1,2,..., mdenote the class labels and D=
(x1,..., xn,xnew)d√ó(n+1)is the data matrix of observations.
For the binary classiÔ¨Åcation, k=0,1 are often adopted as the
class labels.
For each sample point x, it is assumed that there is
an associated hidden variable y(x)such that the vector of
yn+1=(y1,..., yn,ynew)T=(y(x1) ,..., y(xn),y(xnew))T
is normally distributed. That is, y(x)is a GP prior over
functions in the following Bayesian inference.
Without loss of generality, let yn+1‚àºNn+1(0,Kn+1),
where the covariance matrix Kn+1is deÔ¨Åned by a parame-
terized kernel function Œ∫(¬∑,¬∑), which guarantees that Kn+1
is positive deÔ¨Ånite. The kernel approach to determining the
covariance matrix is a highlighted contribution of GP model to
Bayesian classiÔ¨Åcation. The common kernel functions include,
for instance, polynomial kernel Œ∫(u,v)=(Œ∏1uTv+Œ∏2)Œ∏3,
hyperbolic tangent kernel Œ∫(u,v)=tanh(uTv+Œ∏),G a u s s i a n
RBFŒ∫(u,v)=exp{‚àíŒ∏/bardblu‚àív/bardbl2}, etc. The choice of parame-
ters is concerned to the kernel learning [23].
The covariance matrix Kn+1=/parenleftBig
Knk
kTc/parenrightBig
is well deÔ¨Åned, in
which the n√ónsubmatrix Kn=(Œ∫(xi,xj))n√ón+œÉ2I,t h e
vector k=(Œ∫(xnew,x1) ,...,Œ∫( xnew,xn))T‚ààRnand the real
number cinKn+1isc=Œ∫(xnew,xnew)+œÉ2,s ot h a t Kn+1
is positive deÔ¨Ånite. The model relies on the fact that the GP
is uniquely determined by the covariance matrix whenever the
mean is given [20], [24].
By the well-known predictive distribution [25], p(tnew=
k|D,t), the probability of xnew in the k-th class can be
predicted by means of the following equation :
p(tnew=k|D,t)=/integraldisplay
Rp(tnew=k|ynew)p(ynew|D,t)dynew (1)
where p(tnew=k|ynew)is further speciÔ¨Åed by
p(tnew=k|ynew)=softmax (ynew)
=exp(ynew)[exp(ynew)+n/summationdisplay
j=1exp(yj)]‚àí1.
IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 21, NO. 10, OCTOBER 2010 1687
In particular, the binary classiÔ¨Åcation uses
p(tnew=1|ynew)=sigmoid (ynew)=[1+exp(‚àíynew)]‚àí1.
While the posterior distribution of p(ynew|D,t)in (1) is
hard to be expressed explicitly in a closed form because of
the intractable integral in the equation
p(ynew|D,t)=/integraldisplay
Rnp(ynew|yn)p(yn|D,t)dyn (2)
some numerical techniques, such as sampling methods [26],
[27], variational approximations [28], etc., are optional to get
the acceptable replacement of p(ynew|D,t). In this brief, the
Laplace‚Äôs method [29], [30] is adopted to yield its Gaussianapproximation. The process is introduced brieÔ¨Çy as follows.
The item p(y
new|yn)in (2) is determined by the assumption
yn+1‚àºNn+1(0,Kn+1). More precisely, by the fundamental
property of multivariate normal distribution [31], we get
the conditional distribution of ynew|yn‚àºN(kTK‚àí1
nyn,c‚àí
kTK‚àí1
nk).
The item p(yn|D,t)in (2) can be derived from the fact
p(yn|D,t)‚àùp(yn|D)p(t|D,yn),w h e r e yn|D‚àºNn(0,Kn)
andp(t|D,yn)=/producttextn
j=1p(tj|D,yn). For the binary classiÔ¨Åca-
tion, p(tj|D,yn)=stj
j(1‚àísj)1‚àítj,w h e r e sj=sigmoid (yj).
The extension to the multiclass situation is straightforward.
Letting ‚àálnp(yn|D,t)=0, we get the stationary point
of log-likelihood ln p(yn|D,t)to be yn=Kn(t‚àís),w h e r e
s=(s1,..., sn)T. By the Laplace‚Äôs method [30], [32], the
distribution of yn|D,tmay be approximated by a Gaussian
distribution N n(Kn(t‚àís),A‚àí1),w h e r e Ais the negative
Hessian matrix of ln p(yn|D,t)
A=‚àí ‚àá2lnp(yn|D,t)=W+K‚àí1
n
in which W=diag[s1(1‚àís1) ,..., sn(1‚àísn)]is a diagonal
matrix. Consequently
ynew|D,t‚àºN(kT(t‚àís),c‚àíkT(W‚àí1+Kn)k). (3)
By Monte Carlo techniques, the numerical computation
of (1) is implemented in the way of p(tnew=k|D,t)‚âà
1/r/summationtextr
j=1sigmoid (y(j)
new),w h e r e y(1)
new,..., y(r)
neware randomly
sampled from the distribution of (3).
III. FER E XPERIMENTS IN JAFFE D ATASET
The basic emotional expressions of concern include anger,
disgust, fear, happiness, sadness, and surprise. For conve-
nience, we use their two letter acronyms of ‚Äúan,‚Äù ‚Äúdi,‚Äù ‚Äúfe,‚Äù
‚Äúha,‚Äù ‚Äúsa,‚Äù and ‚Äúsu‚Äù in the following text. As a benchmarkdatabase, the JAFFE dataset consists of 213 grayscale images
of seven expressions (neutral plus six basic emotional expres-
sions) from 10 female expressors, whose names are ‚Äúka,‚Äù ‚Äúkl,‚Äù
‚Äúkm,‚Äù ‚Äúkr,‚Äù ‚Äúmk,‚Äù ‚Äúna,‚Äù ‚Äúnm,‚Äù ‚Äútm,‚Äù ‚Äúuy,‚Äù and ‚Äúym‚Äù for short.
Each image size is of 256 √ó256 pixels and each expressor has
2‚Äì4 samples for each expression.
At the Ô¨Årst step of the experimental procedure, the original
images are cropped into 128 √ó128 to reduce the inÔ¨Çuences
of background as usual. The cropping remains the central partof facial expression and no more complicated techniques of
locating eyebrows and mouth are utilized in the preprocessing.
Fig. 1. Cropped samples of anger, disgus t, fear, happiness, neutral, sadness,
and surprise expressions in turn.
Fig. 2. In the Ô¨Årst line, the shape of mouth varies widely in the anger and
fear expressions. In the second line, th e difference between anger and sadness
is not signiÔ¨Åcant.
And then, the cropped images are further resized to 64 √ó64
so that the dimension of each observation is reduced to 4096.
After the naive feature selection, all images are normalized
such that the range of pixel intensity is 0 to 255. Without
any class-based feature selection/extraction, the Ô¨Årst two steps
have nothing to do with the labeled class information in thedata. Fig. 1 shows seven different facial expressions from two
persons in the JAFFE dataset.
It is assumed in the GP models that the adjacent obser-
vations convey information about each other. For instance,the kernel of Œ∫(u,v)=Œ∏
0exp{(‚àí1/2)(u‚àív)TM(u‚àív)}+
Œ∏d+1uTvis often used to deÔ¨Åne the covariance matrix Kn+1,
where M=diag(Œ∏1,...,Œ∏ d)and the parameters are Œ∏=
(Œ∏0,Œ∏1,...,Œ∏ d,Œ∏d+1)T. The parameters in the covariance ma-
trixKn+1are determined by the maximum likelihood estimate
ofŒ∏, whose time complexity is O(n3)by Laplacian approx-
imation. In consideration of the efÔ¨Åciency of evaluation, we
skip over the time-consuming learning of complicated kernels
and only adopt the speciÔ¨Åc polynomial and Gaussian RBF
kernels in the experiments. It is believed that the performance
should improve if the training of kernels is involved in thewhole learning procedure.
Firstly, the performance of GP classiÔ¨Åers deÔ¨Åned by the
polynomial kernel Œ∫(u,v)=(Œ∏
1uTv+Œ∏2)Œ∏3and Gaussian
RBF kernel Œ∫(u,v)=exp{‚àíŒ∏/bardblu‚àív/bardbl2}is simply examined
by the closed test: that is, the classiÔ¨Åer is tested on the training
set itself. The recognition rate of such test is 100% for many
assignments of Œ∏. For instance, Œ∏1=Œ∏2=1,Œ∏3=2,3,4a n d
Œ∏=0.1.
In the JAFFE dataset, the same expression of one person
may differ greatly in different samples, or distinct expressions
may not be very distinguishable (see Fig. 2). To test the GPclassiÔ¨Åer, we design three versions of LOO CV . The Ô¨Årst
version is the traditional one, which tests one image each time
and trains the classiÔ¨Åer on the remaining data. Since thereis at least one properly tagged example in the training set
that looks close to the one under prediction, some researchers
1688 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 21, NO. 10, OCTOBER 2010
TABLE I
CONFUSION MATRI X OF THE FIRST VERSION OF LOO CV
Prediction an di fe ha ne sa su total
a n 2 9 000010 3 0
d i12 60 0 0 2 0 2 9f e 00 2 7 0050 3 2
h a0 0 03 10 0 0 3 1n e 0000 2 9 103 0
s a 00100 3 0 03 1
su 0 0 0 1 2 0 27 30
TABLE II
ASSOCIA TED STATISTICS OF SENSITIVITY ,SPECIFICITY ,ETC.,OFGP
CLASSIFICATION MODEL BY THE FIRST VERSION OF LOO CV
an di fe ha ne sa su
Sensitivity 0.97 1.00 0.97 0.97 0.94 0.77 1.00
SpeciÔ¨Åcity 0.99 0.98 0.97 1.00 0.99 0.99 0.98
Pos. pred. value 0.97 0.90 0.84 1.00 0.97 0.97 0.90
Neg. pred. value 0.99 1.00 0.99 0.99 0.99 0.95 1.00
Prevalence 0.14 0.12 0.13 0.15 0.15 0.18 0.13
Detection rate 0.14 0.12 0.13 0.15 0.14 0.14 0.13
Prevalence 0.14 0.14 0.15 0.15 0.14 0.15 0.14
Fig. 3. As shown in Table I, there are nine samples misclassiÔ¨Åed to the classof ‚Äúsadness.‚Äù They are 1 anger, 2 disgust, 5 fear, and 1 neural samples.
suggest a more rigorous examination so that the images of
the same person with the same expression are not considered
in the training set. Therefore, in the second version of LOO
CV , ‚Äúone‚Äù means one expression of a particular person. More
strictly, the third version leaves one person out of the trainingset and this strategy examines the capability of predicting all
expressions of one new expressor based on the experience
about the others.
Using the parameterized Gaussian RBF kernel, the overall
accuracy of the Ô¨Årst version of LOO CV is 0 .9343. The
confusion matrix of prediction is shown in Table I and some
other statistics for the evaluation are shown in Table II.The low sensitivity of ‚Äúsadness‚Äù is caused by nine misclas-
siÔ¨Åed samples illustrated in Fig. 3. Taking all the positive
anger and disgust expressions for example, Fig. 4 shows thepredictions of GP models on them.
Several frequently used classiÔ¨Åers, such as SVM, k-NN,
naive Bayes, classiÔ¨Åcation tree, etc., are also tested by the
third version of LOO CV . For each person, the accuracy ofFER based on a speciÔ¨Åc classiÔ¨Åer is recorded in Table III.
The SVM in Table III uses a Gaussian RBF kernel with
parameter 0 .05, which may be improved by the further kernel
learning. Among the tested classiÔ¨Åers, the GP model achieves
the best performance, especia lly on the person named ‚Äúmk.‚Äù
The most interesting phenomenon is that the result of the
second version of LOO CV on any expression of a par-
ticular person is not so good. For instance, the GP modelmisclassiÔ¨Åes 10 expressions of ‚Äúmk‚Äù and the recognition
rate is only 52%. In the errors, three ‚Äúne‚Äù expressions are
Probabilit y0.10 0.15 0.20 0.25
an di fe ha ne sa s u
0.10 0.15 0.20 0.25
an di fe ha ne sa s u
Fig. 4. Left panel illustrates the predicted results of the 30 positive anger
samples by GP classiÔ¨Åers in the LOO CV , in which the solid line is the meanprediction. Similarly, the right panel i s for the 29 positive disgust samples.
frequency0 50 100 150
02468 1 0 1 2 1 4
0.6 0.7 0.8
reco gnition rate0.9 1.0 0.83 0.85 0.87
reco gnition rate0.89
Fig. 5. Left panel is the histogram of all 600 accuracies of FER in 60
independent trials of 10-fold cross va lidations. The right panel demonstrates
the empirical distribution of evaluations, with the mean recognition rate of86.89% and the standard deviation of 0 .01439204.
labeled as ‚Äúsa‚Äù and vice versa. Compared to that of the
third version of LOO CV , the performance of the second
version is worse, even when the classiÔ¨Åer is trained by
more image samples of ‚Äúmk.‚Äù The implies that the training
expressions of ‚Äúmk‚Äù substantially impairs the experience ofidentifying the testing expression of ‚Äúmk‚Äù based on the other
samples. In our experiments, similar things happen to the other
expressors.
In order to test the average performance, especially the
robustness, of the GP model, the classiÔ¨Åer is investigatedby the 10-fold cross validations. For each trial of 10-fold
cross validation, the dataset is randomly partitioned into 10
subsets of the same size, and then the procedure of ‚Äúone for
testing and the others for training‚Äù is repeated 10 times in
parallel. The accuracy of one trial is the arithmetic mean ofthe 10 recognition rates. After repeating such random trial
60 times independently, it is easy to survey the empirical
distribution of those accuracies. The histograms of accuraciesof the GP model with a Gaussian RBF kernel (in which,
parameter =0.1) are shown in Fig. 5.
Due to the random partition in a multifold cross validation,
the performance of classiÔ¨Åcation may vary greatly. That is
why the 10-fold cross validation is required to be repeated
many times independently. A well-designed classiÔ¨Åer shouldreach a high accuracy on average, as well as a small deviation
of accuracies. We pick out one random trial of 10-fold
IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 21, NO. 10, OCTOBER 2010 1689
TABLE III
RECOGNITION PERFORMANCE OF SOME FREQUENTLY USED CLASSIFIERS TESTED BY THE THIRD VERSION OF LOO CV
ka kl km kr mk na nm tm uy ym
Gaussian process 0.5652 0.5000 0.6364 0.5500 0.7619 0.6190 0.4000 0.3333 0.4762 0.6818
SVM 0.2609 0.1364 0.1364 0.1500 0.1429 0.1429 0.1500 0.1429 0.1429 0.1818
3-NN 0.3913 0.3182 0.4545 0.1000 0.5714 0.2857 0.2500 0.3810 0.2857 0.5455
5-NN 0.3043 0.4545 0.6364 0.3000 0.6667 0.2381 0.2000 0.3810 0.1905 0.5000
Naive Bayes 0.3478 0.6364 0.2273 0.1000 0.4286 0.3333 0.3500 0.4762 0.4286 0.6819
ClassiÔ¨Åcation tree 0.1739 0.3636 0.3636 0.3500 0.2381 0.1905 0.3500 0.0952 0.4286 0.1818
C4.5 decision tree 0.1739 0.3636 0.5000 0.3000 0.4286 0.4286 0.2500 0.2857 0.2857 0.2727
TABLE IV
CONFUSION MATRI X OF THE WORST CASE OF FER T ESTED BY ONE
RANDOM TRIAL OF 10-F OLD CROSS VALIDA TION
P r e d i c t i o na nd if eh an es as ut o t a l
a n 2000000 2
d i 1300000 4f e 0020001 3
h a 0001000 1n e 0000220 4
s a 0000030 3
s u 0010003 4
cross validation, with best accuracy of 0 .9524 and the worst
accuracy of 0 .7619. As illustrated in Fig. 5, the case of an
accuracy less than 0 .8 happens infrequently. For the worst case
of this 10-fold cross validation, the confusion matrix is listed
in Table IV.
IV . C ONCLUSION
In this brief, we investigated the GP classiÔ¨Åcation model
for the identiÔ¨Åcation of seven f acial expressions in the JAFFE
dataset. Three strategies of LOO CV were proposed to testthe performance of FER. The experiment of traditional LOO
CV achieved the recognition rate of 93 .43%, which shows that
GP classiÔ¨Åer accomplished success in a small sample analysis.
The second version of LOO CV extracted all images of one
expression of a particular person as testing set in each trial, andtrained the classiÔ¨Åer on the left samples. The third version of
LOO CV seemed stricter, which leaves one person out of the
training set and tests all expressions of the excluded person.We discover the interesting fact that, for each expressor, the
FER rate of the second version of LOO CV is worse, instead
of better, than that of the third version of LOO CV . In the
experiments of FER of a new person, the GP approach to FER
works much better than some other frequently used classiÔ¨Åers,such as SVM, k-NN, naive Bayes, and classiÔ¨Åcation tree.
By many repeated 10-fold cross validations, we found
that the recognition rate of a GP model had a very small
deviation, that is, the novel method is robust in classiÔ¨Åcation.
Since the Ô¨Ånal decision of a GP model depends upon the
predicted probabilities, a soft decision based on the descend-ing ordering of the estimates becomes feasible. Some more
experiments on the capability of the proposed system to recog-
nize a completely unknown expression are being carried out.What is more, the exploration of kernel learning and the
application of the GP model to a more authoritative and largerdatabase of FER, like the AR database of Purdue University,
would also be worthy to consider in further work.
A
CKNOWLEDGMENT
The authors would like to thank the anonymous reviewers
for their valuable suggestions and helpful comments on the
original manuscript.
REFERENCES
[1] P. Ekman and W. Friesen, Unmasking the Face . Englewood Cliffs, NJ:
Prentice-Hall, 1975.
[2] C. F. Shan, S. G. Gong, and P. W. McOwan, ‚ÄúFacial expression
recognition based on local binary patterns: A comprehensive study,‚ÄùImage Vis. Comput. , vol. 27, no. 6, pp. 803‚Äì816, May 2009.
[3] A. K. Jain, P. W. Duin, and J. Mao, ‚ÄúStatistical pattern recognition:
Ar e v i e w , ‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 22, no. 1,
pp. 4‚Äì37, Jan. 2000.
[4] M. J. Lyons, J. Budynek, and S. Akamatsu, ‚ÄúAutomatic classiÔ¨Åcation of
single facial images,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 21,
no. 12, pp. 1357‚Äì1362, Dec. 1999.
[5] A. J. Calder, A. M. Burton, and P. Miller, ‚ÄúA principals component
analysis of facial expressions,‚Äù Vis. Res. , vol. 41, no. 9, pp. 1179‚Äì1208,
Apr. 2001.
[6] I. Buciu, C. Kotropoulos, and I. P itas, ‚ÄúICA and gabor representation
for facial expression recognition,‚Äù IEEE ICIP , vol. 2, nos. 14‚Äì17, pp.
855‚Äì858, Sep. 2003.
[7] W. M. Zheng, X. Y . Zhou, C. R. Zou, and L. Zhao, ‚ÄúFacial expression
recognition using kernel canonical correlation analysis (KCCA),‚Äù IEEE
Trans. Neural Netw. , vol. 17, no. 1, pp. 233‚Äì238, Jan. 2006.
[8] B. Fasel and J. Luettin, ‚ÄúAutomatic facial expression analysis: A survey,‚Äù
Pattern Recogni. , vol. 36, no. 1, pp. 259‚Äì275, Jan. 2003.
[9] Y . Tian, T. Kanade, and J. F. Cohn, ‚ÄúRecognizing action units for facial
expression analysis,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 23,
no. 2, pp. 97‚Äì115, Feb. 2001.
[10] C. F. Chuang and F. Y . Shih, ‚ÄúRecognizing facial action units using
independent component analysis and support vector machine,‚Äù Pattern
Recognit. , vol. 39, no. 9, pp. 1795‚Äì1798, 2006.
[11] F. Y . Shih, C. F. Chuang, and P. S. P. Wang, ‚ÄúPerformance comparisons
of facial expression recognition in JAFFE database,‚Äù Int. J. Pattern
Recognit. ArtiÔ¨Åcial Intell. , vol. 22, no. 3, pp. 445‚Äì459, 2008.
[12] T. Zhao, Z. Z. Liang, D. Zhang, and Q. Zou, ‚ÄúInterest Ô¨Ålter vs. interest
operator: Face recognition using Ô¨Åsher linear discriminant based on
interest Ô¨Ålter representation,‚Äù Pattern Recognit. Lett. , vol. 29, no. 13,
pp. 1849‚Äì1857, Oct. 2008.
[13] D. Liang, J. Yang, Z. L. Zheng, and Y . C. Chang, ‚ÄúA facial expression
recognition system based on supervised locally linear embedding,‚Äù
Pattern Recognit. Lett. , vol. 26, no. 15, pp. 2374‚Äì2389, Nov. 2005.
[14] B. D. Ripley, Pattern Recognition and Neural Networks . Cambridge,
U.K.: Cambridge Univ. Press, 1996.
[15] C. M. Bishop, Neural Networks for Pattern Recognition . London, U.K.:
Oxford Univ. Press, 1995.
[16] R. M. Neal, ‚ÄúBayesian learning for neural networks,‚Äù in Lecture Notes
in Statistics , vol. 118. New York: Springer-Verlag, 1996, p. 204.
[17] T. Hofmann, B. Sch√∂lkoft, and A. J. Smola, ‚ÄúKernel methods in machine
learning,‚Äù Ann. Statist. , vol. 36, no. 3, pp. 1171‚Äì1220, 2008.
[18] A. J. Smola and B. Sch√∂lkopf , ‚ÄúBayesian kernel methods,‚Äù in Advanced
Lectures on Machine Learning: Machine Learning Summer School ,
S. Mendelson and A. J. Smola, Eds. New York: Spring-Verlag, 2002,
pp. 65‚Äì117.
1690 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 21, NO. 10, OCTOBER 2010
[19] J. Lampinen and A. Vehtari, ‚ÄúBayesian approach for neural networks-
review and case studies,‚Äù Neural Netw. , vol. 14, no. 3, pp. 257‚Äì274, Apr.
2001.
[20] C. E. Rasmussen and C. K. I. Williams, Gaussian Processes for Machine
Learning . Cambridge, MA: MIT Press, 2006.
[21] H. Nickisch and C. E. Rasmussen, ‚ÄúApproximations for binary Gaussian
process classiÔ¨Åcation,‚Äù J .M a c h .L e a r n .R e s . , vol. 9, pp. 2035‚Äì2078, Oct.
2008.
[22] C. K. I. Williams and D. Barber, ‚ÄúBayesian classiÔ¨Åcation with Gaussian
processes,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 20, no. 12,
pp. 1342‚Äì1351, Dec. 1998.
[23] B. Sch√∂lkoft and A. J. Smola, Learning with Kernels . Cambridge, MA:
MIT Press, 2002.
[24] W. Liu, J. C. Principe, and S. Haykin, Kernel Adaptive Filtering:
A Comprehensive Introduction . New York: Wiley, 2010.
[25] A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin, Bayesian Data
Analysis , 2nd ed. Boston, MA: Chapman & Hall, 2004.
[26] C. P. Robert and G. Casella, Monte Carlo Statistical Methods , 2nd ed.
New York: Springer-Verlag, 2004.
[27] R. M. Neal, ‚ÄúMonte Carlo implementation of Gaussian process models
for Bayesian regression and classiÔ¨Åcation,‚Äù Dept. Comput. Sci., Univ.
Toronto, ON, Canada, Tech. Rep. CRG-TR-97-2, 1997.
[28] T. Jaakkola and M. I. Jordan, ‚ÄúBayesian parameter estimation via
variational methods,‚Äù Statist. Comput. , vol. 10, no. 1, pp. 25‚Äì37, Jan.
2000.
[29] L. Tierney and J. Kadane, ‚ÄúAccu rate approximations for posterior
moments and marginal densities,‚Äù J. Amer. Statist. Assoc. , vol. 81, no.
393, pp. 82‚Äì86, Mar. 1986.
[30] M. A. Tanner, Tools for Statistical Inference-Methods for the Exploration
of Posterior Distributions and Likelihood Functions , 3rd ed. New York:
Springer-Verlag, 1996.
[31] C. R. Rao, Linear Statistical Inference and its Applications .N e wY o r k :
Wiley, 1973.
[32] C. M. Bishop, Pattern Recognition and Machine Learning .N e wY o r k :
Springer-Verlag, 2006.
Continuous Attractors of Lotka‚ÄìVolterra Recurrent
Neural Networks with InÔ¨Ånite Neurons
Jiali Yu, Zhang Yi, and Jiliu Zhou
Abstract ‚Äî Continuous attractors of Lotka‚ÄìVolterra recurrent
neural networks (LV RNNs) with inÔ¨Ånite neurons are studied inthis brief. A continuous attractor is a collection of connected
equilibria, and it has been recognized as a suitable model
for describing the encoding of continuous stimuli in neuralnetworks. The existence of the continuous attractors dependson many factors such as the connectivity and the externalinputs of the network. A continuous attractor can be stable orunstable. It is shown in this brief that a LV RNN can possessmultiple continuous attractors if the synaptic connections and theexternal inputs are Gussian-like in shape. Moreover, both stableand unstable continuous attractors can coexist in a network.Explicit expressions of the continuous attractors are calculated.Simulations are employed to illustrate the theory.
Index Terms ‚Äî Continuous attractors, Lotka‚ÄìVolterra recur-
rent neural networks, stable, unstable.
Manuscript received June 12, 2009; revised July 11, 2010; accepted July 31,
2010. Date of publication September 8, 2010; date of current version October
6, 2010. This work was supported in part by the National Science Foundation
of China under Grant 60970013, the National Basic Research Program ofChina under Program 973, under Grant 2011CB302201, and the FundamentalResearch Funds for the Central U niversities under Grant ZYGX2009J102.
J. Yu is with the Institute for Infocomm Research, Agency for Science
Technology and Research, 138632, Singapore (e-mail: jlyu@i2r.a-star.edu.sg).
Z. Yi and J. Zhou are with the College of Computer Science,
Sichuan University, Chengdu 610065, China (e-mail: zhangyi@scu.edu.cn;
zhoujl@scu.edu.cn).
Color versions of one or more of the Ô¨Ågures in this brief are available online
at http://ieeexplore.ieee.org.
Digital Object IdentiÔ¨Åer 10.1109/TNN.2010.2067224I. I NTRODUCTION
The Lotka‚ÄìV olterra (LV) model of recurrent neural networks
(RNNs) was Ô¨Årst proposed in [1]. It was derived from theconventional membrane dynamics of neurons with a sigmoid
response function and its dynamic properties were analytically
studied. The LV RNNs have found successful applications in
winner-take-all, winner-share-all, and k-winner-take-all prob-
lems (see [2]‚Äì[5]). A recent interesting work on LV RNNs canbe found in [6], in which, LV RNNs were successfully used
for implementing the competitive layer model. The model of
LV RNNs can be looked upon as a special model of the well-known Grossberg neural network model [7], [8].
Recently, there has been increasing interest in multista-
bility analysis for neural networks [9]‚Äì[11]. In multistability
analysis, the networks are allowed to have multiple attractors
[12]. There are two kinds of attractors: discrete attractorsand continuous attractors. Continuous attractors are suitable to
describe the encoding of continuous external stimuli, such as
the orientation, the moving direction, and the spatial location
of objects, or those continuous features that underlie the
categorization of complicated objects [13]‚Äì[15]. It is knownthat bump behaviors are observed everywhere in the cortex,
and continuous attractors with Gaussian shape have been
widely studied [14]‚Äì[17]. There are several computationallydesirable properties with continuous attractors. For example,
they can encode continuous stimuli efÔ¨Åciently, track objects
smoothly, and act as a link between neurodynamics andcognitive behaviors. This brief proposes to study continuous
attractors of LV RNNs.
Continuous attractors have been studied in two different
classes of RNNs. In the Ô¨Årst class, the networks are with
Ô¨Ånite neurons. Networks of this class are analyzed in Ô¨Ånite
dimensional space. Line attractors have been found in some
networks of this class [18], [19]. The theory of line attractorsof linear RNNs has been used successfully to explain how the
brain can keep the eyes still [20]. A line attractor is a special
kind of continuous attractors, it is embedded in some 1-Dmanifolds. In the second class, the RNNs are with continuous
distribution of inÔ¨Ånite neurons. The networks of second class
have been widely studied by many authors (see [21]‚Äì[23],[14], [15], [17]). The networks in the second class should
be studied in inÔ¨Ånite dimensional space. Recently, it was
reported in [13] that continuous attractors can be designed by
tuning the external inputs to networks that have a connectivity
matrix with Toeplitz symmetry. It should be pointed out thatthese two classes of networks are different in dynamics from
a mathematical point of view. This brief studies continuous
attractors of the networks that belong to the second class.
A continuous attractor is a set of connected equilibria and
it can be stable or unstable. If each point of a continuousattractor is stable, then the continuous attractor is said to be
stable; otherwise, it is said to be unstable. A stable continuous
attractor has the property that any trajectory starting from apoint close to it will remain close to it forever, i.e., the stable
continuous attractor ‚Äúattracts‚Äù trajectories around it. On the
other hand, for any unstable continuous attractor, there must
exist a trajectory that cannot be ‚Äúattracted‚Äù by it. In this brief,
1045-9227/$26.00 ¬© 2010 IEEE
"
https://ieeexplore.ieee.org/document/4032815,"172 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 1, JANUARY 2007
Facial Expression Recognition in Image Sequences
Using Geometric Deformation Features
and Support Vector Machines
Irene Kotsia and Ioannis Pitas , Senior Member, IEEE
Abstract‚Äî In this paper, two novel methods for facial expres-
sion recognition in facial image sequences are presented. Theuser has to manually place some of Candide grid nodes to facelandmarks depicted at the Ô¨Årst frame of the image sequenceunder examination. The grid-tracking and deformation systemused, based on deformable models, tracks the grid in consecutivevideo frames over time, as the facial expression evolves, until theframe that corresponds to the greatest facial expression intensity.The geometrical displacement of certain selected Candide nodes,deÔ¨Åned as the difference of the node coordinates between the Ô¨Årstand the greatest facial expression intensity frame, is used as aninput to a novel multiclass Support Vector Machine (SVM) system
of classiÔ¨Åers that are used to recognize either the six basic facial
expressions or a set of chosen Facial Action Units (FAUs). Theresults on the Cohn‚ÄìKanade database show a recognition accu-racy of 99.7% for facial expression recognition using the proposedmulticlass SVMs and 95.1% for facial expression recognitionbased on FAU detection.
Index Terms‚Äî Candide grid, Facial Action Coding S (FACS),
Facial Action Unit (FAU), facial expression recognition, machinevision, pattern recognition, Support Vector Machines (SVMs).
I. I NTRODUCTION
DURING the past two decades, facial expression recog-
nition has attracted a signiÔ¨Åcant interest in the scientiÔ¨Åc
community, as it plays a vital role in human centered interfaces.Many applications, such as virtual reality, video-conferencing,
user proÔ¨Åling, and customer satisfaction studies for broadcast
and web services, require efÔ¨Åcient facial expression recognitionin order to achieve the desired results. Therefore, the impact offacial expression recognition on the above-mentioned applica-
tion areas is constantly growing.
Several research efforts have been done regarding facial ex-
pression recognition. The facial expressions under examinationwere deÔ¨Åned by psychologists as a set of six basic facial expres-
sions (anger, disgust, fear, happiness, sadness, and surprise) [1].
In order to make the recognition procedure more standardized, aset of muscle movements known as Facial Action Units (FAUs)
that produce each facial expression, was created, thus forming
Manuscript received May 5, 2005; revised July 2, 2006. This work was sup-
ported by the research project 01ED312 ‚ÄúUse of Virtual Reality for training
pupils to deal with earthquakes‚Äù Ô¨Ånanced by the Greek Secretariat of Research
and Technology and the ‚ÄùSIMILAR‚Äù European Network of Excellence on Mul-
timodal Interfaces of the IST Programme of the European Union. The associate
editor coordinating the review of this manuscript and approving it for publica-tion was Dr. Beniot Macq.
The authors are with the Department of Informatics, Aristotle University of
Thessaloniki, 54124 Thessaloniki, Greece (e-mail: ekotsia@aiia.csd.auth.gr;
pitas@aiia.csd.auth.gr).
Digital Object IdentiÔ¨Åer 10.1109/TIP.2006.884954the so-called Facial Action Coding System (FACS) [2]. These
FAUs are combined in order to create the rules responsible forthe formation of facial expressions as proposed in [3].
A. Facial Expression Recognition
A survey on the research made regarding facial expression
recognition can be found in [4] and [5]. The approaches re-ported regarding facial expression recognition can be distin-guished in two main directions, the feature-based ones and the
template-based ones, according to the method they use for fa-
cial information extraction. The feature-based methods use tex-ture or geometrical information as features for expression infor-mation extraction. The template-based methods use 3-D or 2-D
head and facial models as templates for expression information
extraction.
1) Feature-Based Approaches: Facial feature detection and
tracking is based on active InfraRed illumination in [6], in
order to provide visual information under variable lighting and
head motion. The classiÔ¨Åcation is performed using a DynamicBayesian Network (DBN).
A method for static and dynamic segmentation and classiÔ¨Åca-
tion of facial expressions is proposed in [7]. For the static case,
a DBN is used, organized in a tree structure. For the dynamicapproach, multi level Hidden Markov Models (HMMs) classi-Ô¨Åers are employed.
The system proposed in [8] automatically detects frontal
faces in the video stream and classiÔ¨Åes them in seven classesin real time: neutral, anger, disgust, fear, joy, sadness, andsurprise. An expression recognizer receives image regions
produced by a face detector and then a Gabor representation
of the facial image region is formed to be later processed by abank of SVMs classiÔ¨Åers.
Gabor Ô¨Ålters are also used in [9] for facial expression recogni-
tion. Facial expression images are coded using a multiorienta-
tion, multiresolution set of Gabor Ô¨Ålters which are topograph-ically ordered and aligned approximately with the face. Thesimilarity space derived from this facial image representation is
compared with one derived from semantic ratings of the images
by human observers. The classiÔ¨Åcation is performed by com-paring the produced similarity spaces.
The images are Ô¨Årst transformed using a multiscale, multi-
orientation set of Gabor Ô¨Ålters in [10]. The grid is then regis-tered with the facial image region either automatically, usingelastic graph matching [11] or by manual clicking on Ô¨Åducial
face points. The amplitude of the complex valued Gabor trans-
form coefÔ¨Åcients are sampled on the grid and combined into a
1057-7149/$25.00 ¬© 2006 IEEE
KOTSIA AND PITAS: FACIAL EXPRESSION RECOGNITION IN IMAGE SEQUENCES 173
single vector, called a Labeled Graph Vector (LGV). The clas-
siÔ¨Åcation is performed using the distance of the LGV from each
facial expression cluster center. Gabor features are used for fa-
cial feature extraction given a set of Ô¨Åducial points in [12]. The
classi Ô¨Åcation is performed using Bayes, SVMs, Adaboost, and
linear programming classi Ô¨Åers.
A Neural Network (NN) is employed to perform facial ex-
pression recognition in [13]. The features used can be either
the geometric positions of a set of Ô¨Åducial points on a face or
a set of multiscale and multiorientation Gabor wavelet coef Ô¨Å-
cients extracted from the facial image at the Ô¨Åducial points. The
recognition is performed by a two layer perceptron NN. A con-
volutional NN was used in [14]. The system developed is robustto face location changes and scale variations. Feature extrac-tion and facial expression classi Ô¨Åcation were performed using
neuron groups, having as input a feature map and properly ad-
justing the weights of the neurons for correct classi Ô¨Åcation. A
method that performs facial expression recognition is presentedin [15]. Face detection is performed using a Convolutional NN,
while the classi Ô¨Åcation is performed using a rule-based algo-
rithm. Optical Ô¨Çow is used for facial region tracking and facial
feature extraction in [16]. The facial features are inserted ina Radial Basis Function (RBF) NN architecture that performs
classi Ô¨Åcation. The Discrete Cosine Transform (DCT) is used
in [17], over the entire face image as a feature detector. Theclassi Ô¨Åcation is performed using a one-hidden layer feedfor-
ward NN.
A feature selection process that is based on principal compo-
nent analysis (PCA) is proposed in [18]. A decision tree-basedclassi Ô¨Åer that uses successive projections onto more precise rep-
resentation subspaces, is employed. The image pixels are used
in [19] as input to PCA and Linear Discriminant Analysis (LDA)
to reduce the original feature space dimensionality. The resultedfeatures are lexicographically ordered and concatenated to a fea-ture vector, which is used for classi Ô¨Åcation according to the
nearest neighbor rule.
The approach followed in [20] uses structured and geomet-
rical features of a user sketched expression model. The classi Ô¨Å-
cation is performed using Linear Edge Mapping (LEM). Expres-
sive face modelling, using an Active Appearance Model (AAM)
is employed in [21]. The facial model is constructed based oneither three or one PCA. The classi Ô¨Åcation is performed in the
space of AAM.
2) Model Template-Based Approaches: Two methods for fa-
cial expression recognition are proposed in [22], based on a 3-Dmodel enriched with muscles and skin. The Ô¨Årst method esti-
mates facial muscle actuations from optical Ô¨Çow data. The clas-
siÔ¨Åcation is performed according to its similarity to the classical
patterns of muscle actuation. The second method uses the clas-sical patterns of muscle actuation to generate the classical pat-tern of motion energy associated with each facial expression,
thus resulting in a set of simple facial expression ‚Äúdetectors, ‚Äù
each of which looks for the particular space-time pattern of mo-tion energy associated with each facial expression.
A face model, de Ô¨Åned as a point-based model composed of
two 2-D facial views (frontal and pro Ô¨Åle views) is used in [3].
The deformation of facial features is extracted from both thefrontal and pro Ô¨Åle views and its correspondence with the FAUsis established. The facial expression recognition is performed
based on a set of decision rules.
A 3-D facial model is proposed in [23]. Anatomically-based
muscles are added to it. A Kalman Ô¨Ålter in correspondence
with optical Ô¨Çow computation are used to extract muscle action
in order to form a new model of facial action, the so-called
.
A 3-D facial model used for facial expression recognition is
also proposed in [24]. First, the head pose is estimated in a facialvideo sequences. Subsequently, face images are warped onto aface model with canonical face geometry, then they are rotated
to frontal ones, and are projected back onto the image plane.
Pixels brightness is linearly rescaled and resulting images areconvolved with a bank of Gabor kernels. The Gabor representa-tions are then channelled to a bank of SVMs to perform facial
expression recognition.
B. FAU-Based Facial Expression Recognition
For FAUs detection, the approaches followed were also fea-
ture based. Many techniques for FAUs recognition are proposed
in [25]. PCA, Independent Component Analysis (ICA), LocalFeatures Analysis (LFA), LDA, Gabor wavelet representations,and Local Principal Components (LPC) are investigated more
thoroughly.
A group of FAUs is detected in [26]. The facial feature
contours are adjusted and both permanent and transient facialfeatures changes are automatically detected and tracked in the
image sequence. The facial parameters are then fed into two
NN classi Ô¨Åers, one for the upper face and one for the lower
face.
FAUs detection is also investigated in [27]. Facial expression
information extraction is performed either by using optical Ô¨Çow,
or by facial feature point tracking. The extracted information isused as an input in a HMMs system that has as an output upperface expressions at the forehead and brow regions.
HMMs are also used in [28]. Dense optical Ô¨Çow extraction
is used to track Ô¨Çow across the entire face image, after the
input image sequence is aligned. Facial feature tracking of asmall set of preselected features is performed and high-gradient
component detection uses a combination of horizontal, vertical,
and diagonal line and edge feature detectors to detect and trackchanges in standard and transient facial lines and furrows. Theresults from the above system are fed to a HMMs system to
perform facial expression recognition.
A NN is employed for FAUs detection in [29]. The geometric
facial features (including mouth, eyes, brows, and cheeks) areextracted using multistate facial component models. After ex-
traction, these features are represented parametrically. The re-
gional facial appearance patterns are captured using a set of mul-tiscale and multiorientation Gabor wavelet Ô¨Ålters at speci Ô¨Åc lo-
cations. The classi Ô¨Åcation is performed using a back-propaga-
tion NN.
In the current paper, two novel fast feature-based methods
are proposed that use SVMs classi Ô¨Åers for recognizing dynamic
facial expressions either directly or by Ô¨Årst detecting the FAUs.
SVMs were chosen due to their good performance in various
practical pattern recognition applications [30] ‚Äì[33], and their
solid theoretical foundations. A novel class of SVMs, which
174 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 1, JANUARY 2007
incorporates statistic information about the classes under exami-
nation, is also proposed. The classi Ô¨Åcation on both cases (facial
expression recognition using multiclass SVMs or based on FAU
detection) is performed using only geometrical information,
without taking into consideration any facial texture information.
Let us consider a video shot containing a face, whose facial
expression evolves from a neutral state ( Ô¨Årst frame) to a fully
expressed state (last frame). The proposed method is based on
mapping and tracking the facial model Candide onto the videoframes. The proposed facial expression recognition system issemi-automatic, in the sense that the user has to manually place
some of the Candide grid nodes [34] on face landmarks depicted
at the Ô¨Årst frame of the image sequence under examination. The
tracking system allows the grid to follow the evolution of thefacial expression over time till it reaches its highest intensity,
producing at the same time the deformed Candide grid at each
video frame. A subset of the Candide grid nodes is chosen, thatpredominantly contribute to the formation of the facial defor-mations described by the FACS. The geometrical displacement
of these nodes, de Ô¨Åned as the difference of each node coordi-
nates at the Ô¨Årst and the last frame of the facial image sequence,
is used as an input to a SVMs classi Ô¨Åer (either the classical or
the proposed one). When facial expression recognition using
multiclass SVMs is performed, the SVMs system consists of a
six-class SVMs classi Ô¨Åer, each class representing one of the six
basic facial expressions (anger, disgust, fear, happiness, sadness,and surprise). When FAU-based facial expression recognition
is .performed, 8 or 17 FAUs are chosen that corresponds to the
new empirically derived facial expressions rules and to the rulesproposed in [3]. Thus, the recognition system used is composedof a bank of two-class SVMs, each one detecting the presence
or absence of a particular FAU that corresponds to a speci Ô¨Åc
facial expression. The experiments were performed using theCohn ‚ÄìKanade database and the results show that the proposed
novel facial expression recognition system can achieve a recog-
nition accuracy of 99.7% or 95.1%, when recognizing six basic
facial expressions on the Cohn ‚ÄìKanade database by the multi-
class SVMs approach or by the FAU detection-based approach,respectively.
Summarizing, the contributions of this paper are as follows.
‚Ä¢The presentation of a real-time system able to correctly
classify facial expressions and FAUs, taking under consid-eration only geometrical displacement information based
on the standard and well known Candide grid [35], con-
trary to other approaches that use their own models withouthaving explicitly de Ô¨Åned them [7], [9], [23], [36], [37].
‚Ä¢The introduction of a new class for multiclass SVMs clas-
siÔ¨Åcation, based on the extension of the approach described
in [30].
‚Ä¢The presentation of a new set of empirical rules for facial
expression recognition using FAUs, as well as a simpli Ô¨Åed
Candide model whose nodes correspond to the above men-
tioned FAUs.
Our system is different from the method proposed in [38] as:
‚Ä¢it uses a general and well known model (Candide facial
grid) for tracking and information extraction, and not an
arbitrary grid that the author chose, not having been prop-erly de Ô¨Åned for public use;‚Ä¢a method for FAU recognition and facial expression recog-
nition through the FAUs appearing in a facial grid is alsopresented;
‚Ä¢a novel modi Ô¨Åed SVMs system is proposed and used to
solve the facial expression recognition system, proving atthe same time that its performance greatly outperforms themaximum margin SVMs approach [39].
This paper is organized as follows. The system used for facial
expression classi Ô¨Åcation is described in Section II. The facial
expression rules used for the synthesis of the six basic facial ex-pressions as proposed in [3] are presented in Section II-C. The
modi Ô¨Åed SVMs for a two-class and multiclass problem are pre-
sented in Sections III and Section IV, respectively. The data-base used for the experiments and their description for bothapproaches are presented in Section V-A. The newly proposed
rules for the simpli Ô¨Åed Candide grid and the facial expressions
are described in Section V-B. The accuracy rates achieved forwhen the chosen subset of FAUs was chosen as well as whenfacial expression recognition was attempted using multiclass
SVMs are shown in Sections V-C and E, respectively. Conclu-
sions are drawn in Section VI.
II. S
YSTEM DESCRIPTION
The facial expression recognition system is composed of two
subsystems: one for Candide grid node information extractionand one for grid node information classi Ô¨Åcation. The grid node
information extraction is performed by a tracking system, while
the grid node information classi Ô¨Åcation is performed by a SVMs
system. The Ô¨Çow diagram of the proposed system is shown in
Fig. 1.
A. Tracking System Initialization
The initialization procedure is performed in a semi-auto-
matic way in order to attain reliability and robustness of theinitial grid displacement. The facial wireframe model used in
the tracking procedure is the well-known Candide wireframe
model [34], contrary to the other approaches that use their ownmodels without having explicitly de Ô¨Åned them. Candide is a pa-
rameterized face mask speci Ô¨Åcally developed for model-based
coding of human faces. A frontal and a pro Ô¨Åle view of the
model can be seen in Fig. 7. The low number of its trianglesallows fast face animation with moderate computing power.
In the beginning, the Candide wireframe grid is randomly
placed on the facial image depicted at the Ô¨Årst frame. The grid is
in its neutral state. The user has to manually select a number ofpoint correspondences that are matched against the facial fea-tures of the actual face image. Future research involves the au-
tomatical placement of the grid on the face, using elastic graph
matching algorithms. The most signi Ô¨Åcant nodes (around the
eyes, eyebrows and mouth) should be chosen, since they are re-sponsible for the formation of facial deformations modelled by
FACS. It has been empirically determined that Ô¨Åve to eight node
correspondences are enough for a good model Ô¨Åtting. These
correspondences are used as the driving power which deformsthe rest of the model and matches its nodes against face image
points. The result of the initialization procedure, when seven
nodes (four for the inner and outer corner of the eyes and threefor the upper lip) are placed by the user, can be seen in Fig. 2.
KOTSIA AND PITAS: FACIAL EXPRESSION RECOGNITION IN IMAGE SEQUENCES 175
Fig. 1. System architecture for facial expression recognition in facial videos.
Fig. 2. Result of initialization procedure when seven Candide nodes are placed by the user on a facial image.
B. Model-Based Tracking
Wireframe node tracking is performed by a pyramidal variant
of the well-known Kanade ‚ÄìLucas ‚ÄìTomasi (KLT) tracker [40].
The loss of tracked features is handled through a model defor-mation procedure that increases the robustness of the tracking
algorithm.
The algorithm, initially Ô¨Åts and subsequently tracks the Can-
dide facial wireframe model in video sequences containing theformation of a dynamic human facial expression from the neu-
tral state to the fully expressive one. The facial features are
tracked in the video sequence using a variant of KLT tracker[40]. If needed, model deformations are performed by mesh Ô¨Åt-
ting at the intermediate steps of the tracking algorithm. Such
deformations provide robustness and tracking accuracy.
The facial model is assumed to be a deformable 2-D mesh
model. The facial model elements (springs) are assumed to havea certain stiffness. The driving forces that are needed, i.e., the
forces that deform the model, are determined from the point
correspondences between the facial model nodes and the faceimage features. Each force is de Ô¨Åned to be proportional to the
difference between the model nodes and their correspondingmatched feature points on the face image. If a node correspon-
dence is lost, the new node position is the result of the grid defor-mation. This solves a major problem of feature-based trackingalgorithms, the gradual elimination of features points with re-
spect to time. In the modi Ô¨Åed tracking algorithm, the incorpo-
ration of the deformation step enables the tracking of featuresthat would have been lost otherwise. The tracking algorithm
provides a dynamic facial expression model for each video se-
quence, which is de Ô¨Åned as a series of frame facial expression
models, one for each video frame.
An example of the deformed frame facial expression models
produced for each one of the six basic facial expressions can be
seen in Fig. 3
C. Grid Node Displacement Extraction
In the proposed approach, the facial expression classi Ô¨Åcation
is performed based only on geometrical information, withouttaking directly into consideration any facial texture information.
The geometrical displacement information of the grid node co-
ordinates is used either for facial expression recognition usingmulticlass SVMs or for FAU-based facial expression recogni-tion. In the FAU-based recognition, the activated FAUs should
176 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 1, JANUARY 2007
Fig. 3. Example of the deformed Candide grids for each one of the six facial expressions.
be detected in the grid, before employing them to produce one
of the six basic facial expressions using a set of rules that maps
them to facial expressions.
Let
 be the video database that contains the facial mage se-
quences. In the case of facial expression recognition using mul-ticlass SVMs, it is clustered into six different classes
,
, each one representing one of six basic facial expres-
sions (anger, disgust, fear, happiness, sadness, and surprise). Inthe case of FAU-based facial expression recognition, for everyFAU, the database is clustered into two different classes
,
1, 2 for the
 th FAU,
 . The Ô¨Årst class,
 ,
represents the presence of the FAU under examination at thegrid being processed, while the second one,
, represents its
absence.
The geometrical information used for facial expression recog-
nition is the displacement of one node
 ,d eÔ¨Åned as the dif-
ference of the
 th grid node coordinates at the Ô¨Årst and the fully
formed expression facial video frame
and
(1)
where
 ,
 are the
 ,
coordinate displacement of the
th node in the
 th image, respectively.
 is the total number ofnodes (
 for the Candide model) and
 is the number
of the facial image sequences. This way, for every facial image
sequence in the training set, a feature vector
 is created, called
grid deformation feature vector containing the geometrical dis-
placement of every grid node
(2)
having
 dimensions. We assume that each
grid deformation feature vector
 belongs to one
of the six facial expression classes
 ,
 (for facial
expression recognition using multiclass SVMs) and activates anumber of FAUs (for FAUs detection-based facial expression
recognition).
Facial expressions can be described as combinations of FAUs,
as proposed in [3]. As can be seen in the original rules (Table I),the FAUs that are necessary for fully describing all facial ex-
pressions are FAUs 1, 2, 4, 5, 6, 7, 9, 10, 12, 15, 16, 17, 20, 23,
24, 25, and 26. Therefore, these 17 FAUs are responsible for de-scribing face deformations according to FACS. The operators
,
refer to the logical AND, OR operations, respectively. There-
fore, FAUs can be easily used for facial expression recognition.
In the following, we will formulate the SVMs-based classi Ô¨Åca-
tion problems used for FAUs detection in the grid and for facialexpression recognition.
KOTSIA AND PITAS: FACIAL EXPRESSION RECOGNITION IN IMAGE SEQUENCES 177
TABLE I
FACIAL EXPRESSION SYNTHESIS RULES AS PROPOSED IN [3]
III. FAU D ETECTION USING SVM S
Here, we shall describe two such classi Ô¨Åcation algorithms.
One is based on the well-known SVMs [39] and the other oneis a modi Ô¨Åed version of SVMs as proposed in [30].
A. Two Class SVMs FAU Detection
In our approach in order to detect the activated FAUs, the
grid deformation feature vector
is
used as an input to 17 two class SVMs systems, each one
detecting a speci Ô¨Åc FAU. Each SVMs system, uses the grid
nodes geometrical displacements to decide whether a speci Ô¨Åc
FAU is activated at the grid under examination or not. The
th SVM
 is trained with the examples in
as positive ones and all
other examples
 as
negative ones.
In order to train the
 th SVMs network, the following mini-
mization problem has to be solved [41]
(3)
subject to the separability constraints
where
 is the bias for the
 th SVM,
 is
the slack variable vector and
 is the term that penalizes the
training errors.
After solving the optimization problem (3) subject to the
separability constraints (4) [39], [42], the function that decides
whether the
 th FAU is activated by a test displacement feature
vector
 is
(4)
where
 is an arbitrary dimensional Hilbert space [43] and
. In this formulation, a nonlinear mapping
has been used for a high-dimensional feature mapping for ob-
taining a linear SVMs system in which it should be
 .This mapping is de Ô¨Åned by a positive kernel function,
 ,
specifying an inner product in the feature space and satisfying
the Mercer condition [39], [42]
(5)
The functions used as SVMs kernels were the
 degree polyno-
mial function
(6)
and the Radial Basis Function (RBF) kernel
(7)
where
 is the spread of the Gaussian function.
B. ModiÔ¨Åed Two Class SVMs
The other classi Ô¨Åer tested for FAU detection is based on a
modi Ô¨Åed two class SVMs formulation proposed in [30]. The
approach in [30] was motivated by the fact that the Fisher ‚Äôs
discriminant optimization problem for two classes is a con-
straint least-squares optimization problem [30], [44], [45]. Theproblem of minimizing the within-class variance has been re-formulated so that it can be solved by constructing the optimal
separating hyperplane for both separable and nonseparable
cases. The modi Ô¨Åed SVMs [30] has been applied successfully
in order to weight the elastic graph nodes local similarity valueaccording to their corresponding discriminant power for frontal
face veri Ô¨Åcation. It has been shown that it outperforms the
classical
1SVMs approach. More details about the motivations
of this modi Ô¨Åed SVMs class can be found in [30].
1) Linear Case: In order to form the optimization problem
of the SVMs proposed in [30], we should de Ô¨Åne the within class
scatter matrix of the training set in (8), shown at the bottom ofthe page, where
and
 are the mean vectors of the classes
and
 , respectively. In this approach, we assume that the within
scatter matrix
 is invertible (which is true in our case, since
1The term classical SVMs refers to the maximal margin SVMs proposed in
[39]
(8)
178 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 1, JANUARY 2007
the dimensionality of the vector
 is classically smaller than
the number of available training examples). The optimizationproblem of the modi Ô¨Åed SVMs is [30]
(9)
subject to the separability constraints (4) (here, we refer to the
linear case where
 ). The solution of the optimization
problem (9) subject to the constraints (4) is given by the saddle
point of the Lagrangian
(10)
where
 and
 are the vec-
tors of Lagrangian multipliers for the constraints (4). The vector
can be derived from the Kuhn ‚ÄìTucker (KT) conditions [30]
(11)
Instead of Ô¨Ånding the saddle point of the Lagrangian (10), we
Ô¨Ånd the maximization point of the Wolf dual problem [30]
(12)
subject to
(13)
The above optimization problem can be solved using optimiza-
tion packages like [46] or using the ‚Äúquadprog ‚Äùfunction of
MATLAB [47].
The linear decision function that decides whether the
 th FAU
is activated in the geometrical displacement vector
 , or not, is
(14)
2) Nonlinear Case: The nonlinear multiclass decision
surfaces can be created in the same manner as the two class
nonlinear decision surfaces that have been proposed in [30].
That is, in the dual Wolf problem the term
 is
employed. Assuming that the within scatter matrix is invertible,
this term can be written as
 .
Applying the nonlinear function
 to the vectors
,w eh a v e
[30]. Then, we can apply
kernel functions in (15) as
(15)
The corresponding nonlinear decision function that detects the
th FAU in the geometrical displacement vector
 is given by
(16)
IV . F ACIAL EXPRESSION RECOGNITION USING
MULTICLASS SVM S
For facial expression recognition using multiclass SVMs, the
grid deformation feature vector
 is used as an input to
a multi class SVMs system [48]. Six classes were considered
for the experiments, each one representing one of the basic fa-
cial expressions (anger, disgust, fear, happiness, sadness, andsurprise). The SVMs system, classi Ô¨Åes the set of the grid ge-
ometrical displacements to one of the six basic facial expres-
sions. More speci Ô¨Åcally, the grid deformation vectors
,
, are used as an input to the SVMs system. The output
of the SVMs system is a label that classi Ô¨Åes the grid deforma-
tion under examination to one of the six basic facial expressions.
In this section, we will also show how the two class SVMs
described in Section III-B2 can be extended to multiclass classi-Ô¨Åcations problems using the multiclass SVMs formulation pre-
sented in [39], [49], and [50]. In the experimental results section,
we will show that the modi Ô¨Åed multiclass SVMs outperforms
the ones proposed in [39], [49], and [50].
A. Multiclass SVMs
A brief conversation about the optimization problem of the
multiclass SVMs will be given below. The interested reader can
refer to [39], [41] [49], [50] and the references therein for for-
mulating and solving multiclass SVMs optimization problems.
The training data are
where
the are grid deformation vectors and
 are the
facial expression labels of the feature vector. The multiclass
SVMs problem solves only one optimization problem [49]. Itconstructs six facial expressions rules, where the
th function
separates training vectors of the class
 from the
rest of the vectors, by minimizing the objective function
(17)
subject to the constraints
(18)
is the function that maps the deformation vectors to a higher
dimensional space, where the data are supposed to be linearly ornear linearly separable.
is the term that penalizes the training
errors. The vector
 is the bias vector and

KOTSIA AND PITAS: FACIAL EXPRESSION RECOGNITION IN IMAGE SEQUENCES 179
is the slack variable vector. Then, the de-
cision function is
(19)
Using this procedure, a test grid deformation feature vector is
classi Ô¨Åed to one of the six facial expressions using (19). Once
the six-class SVMs system is trained, it can be used for testing,i.e., for recognizing facial expressions on new facial imagesequences. For the solution of the optimization problem (17)
subject to the constraints (18) someone can refer to [39], [49],
and [50].
B. Modi Ô¨Åed Class of Multiclass SVMs
In this section, a novel multiclass SVMs method extending
the constraint optimization problem in (9), is proposed. Thisnovel multiclass SVMs method is the generalization of the twoclass modi Ô¨Åed SVMs described in Section III-B.
1) Linear Case: Let that the within class scatter matrix of
our grid deformation feature vectors
is deÔ¨Åned as
(20)
where six is the number of facial expression classes and
 is the
geometrical displacement vector for the class
 . In this section,
we assume that the within class scatter matrix
 is invertible,
which holds in our case, since classically for our deformation the
feature vector dimension is smaller than the available training
examples.
The modi Ô¨Åed constraint optimization problem is
(21)
subject to the separability constraints in (18) (in the linear case
). The solution of the above constraint optimization
problem can be given by Ô¨Ånding the saddle point of the La-
grangian
(22)
where
 and
are the Lagrangian multipliers
for the constraints (18) with
(23)and constraints
(24)
The Lagrangian (22) has to be maximized with respect to
and
 and minimized with respect to
 and
 . In order to pro-
duce a more compact equation form, let us de Ô¨Åne the following
variables:
and
if
if
 .(25)
After a series of manipulations shown in Appendix I, the search
of the saddle point of the Lagrangian (22) is reformulated to themaximization of the Wolf dual problem
(26)
which is a quadratic function in terms of
 with the linear
constraints
(27)
The above optimization problem can be solved using optimiza-
tion packages like [49]. The corresponding decision hyperplane
is
(28)
as is detailed in Appendix I.
2) Nonlinear Case: The nonlinear multiclass decision sur-
faces can be created in the same manner as the two class non-
linear decision surfaces that have been proposed in [30] and aredescribed in Section III-B2. That is, we exploit the fact thatthe term
can be written in terms of dot products as
. Then, we can apply kernels in (26)
as
(29)
the corresponding decision surface is
(30)
180 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 1, JANUARY 2007
Fig. 4. Example of each facial expression for a poser from the Cohn ‚ÄìKanade
database.
V. E XPERIMENTAL RESULTS
A. Database Description
The Cohn ‚ÄìKanade database [2] was used for the facial
expression recognition in six basic facial expressions classes
(anger, disgust, fear, happiness, sadness, and surprise). This
database is annotated with FAUs. These combinations of FAUswere translated into facial expressions according to [3], in
order to de Ô¨Åne the corresponding ground truth for the facial
expressions. All the subjects were taken under consideration to
form the database for the experiments.
In Fig. 4, a sample of an image for every facial expression for
one poser from this database, is shown.
The most usual approach for testing the generalization perfor-
mance of a SVMs classi Ô¨Åer, is the leave-one cross-validation ap-
proach. The leave-one out method [7] was used in order to make
maximal use of the available data and produce averaged classi Ô¨Å-
cation accuracy results. The term leave-one out cross-validation,
does not correspond to the classic leave-one-out de Ô¨Ånition here,
as a variant of leave-one-out is used (i.e., leave 20% out) for the
formation of the test dataset. However, the procedure followed
will be called leave-one-out from now on. More speci Ô¨Åcally, all
image sequences contained in the database are divided into six
classes, each one corresponding to one of the six basic facial ex-
pressions to be recognized. Neutral state is not considered as a
class, as the system tries to recognize the fully expressed facial
expression starting from the neutral state. Five sets containing20% of the data for each class, chosen randomly, were created.
One set containing 20% of the samples for each class is used for
the test set, while the remaining sets form the training set. After
the classi Ô¨Åcation procedure is performed, the samples forming
the testing set are incorporated into the current training set, anda new set of samples (20% of the samples for each class) is ex-
tracted to form the new test set. The remaining samples create
the new training set. This procedure is repeated Ô¨Åve times. A di-
agram of the leave-one-out cross-validation method can be seen
in Fig. 5. The average classi Ô¨Åcation accuracy is the mean value
of the percentages of the correctly classi Ô¨Åed facial expressions.
Fig. 5. Diagram of leave-one-out method.
The accuracy achieved for each facial expression is averaged
over all facial expressions and does not provide any information
with respect to a particular expression. The confusion matrices
[10] have been computed to handle this problem. The confusion
matrix is a
 matrix containing the information about the
actual class label
 (in its columns) and the label obtained
through classi Ô¨Åcation
 (in its rows). The diagonal entries of
the confusion matrix are the rates of facial expressions that are
correctly classi Ô¨Åed, while the off-diagonal entries correspond
to misclassi Ô¨Åcation rates. The abbreviations an, di, fe, ha, sa,
andsurepresent anger, disgust, fear, happiness, sadness, and
surprise, respectively.
B. Representative FAU and Grid Node Selection
The rules proposed in [3] require the detection of 17 FAUs.
The use of so many FAUs makes the rules sensitive to false FAUdetection or rejection. In order to simplify the rules, a small
set of rules are proposed for facial expression classi Ô¨Åcation that
yield better performance in the experiments performed.
From all the FAUs appearing in the facial expression descrip-
tion rules, many describe two or more facial expressions. Those
that appear once in every facial expression rule are chosen to
describe uniquely the facial expressions under examination. Forexample, FAU 26 appears in every facial expression. Thus, its
presence is irrelevant when de Ô¨Åning a facial expression, as no
facial expression could be speci Ô¨Åed. Therefore, a FAU that ex-
ists in only one facial expression rule should be speci Ô¨Åed for
each facial expression. Where it is not possible, a unique com-
bination of FAUs should be de Ô¨Åned instead.
‚Ä¢For facial expression anger, the FAUs that appear once
are the FAUs 23 and 24. The rest FAUs that participate in
the facial expression rule are observed two or more times.
FAUs 23 and 24 do not participate in the rest of facial ex-
pressions rules for the other Ô¨Åve facial expressions. Anger
should, therefore, be de Ô¨Åned by the appearance of those
two FAUs.
KOTSIA AND PITAS: FACIAL EXPRESSION RECOGNITION IN IMAGE SEQUENCES 181
Fig. 6. Eight most representative FAUs used for facial expression recognition.
Fig. 7. PFEG according to FACS, used for the experiments.
‚Ä¢For the disgust facial expression, the FAU that appears only
once is the FAU 9. It is also uniquely observed in disgust
rules, thus making it appropriate for disgust classi Ô¨Åcation.
‚Ä¢For the fear facial expression, the FAU that appears only
once is the FAU 20. Since it appears only in fear facial ex-
pression rule, it will be the only one taken under consider-
ation when recognizing fear.
‚Ä¢For the happiness facial expression, the FAU that appears
only once is the FAU 12. However, in Fig. 6, it can be seen
that FAUs 12 and 16 appear the same. Therefore, facialexpression happiness will be recognized if FAUs 12 and
16 exist (both of them).
‚Ä¢For the sadness facial expression, the FAU that appears
only once is the FAU 15. Since it appears only the in sad-
ness facial expression rule, it will be the only one taken
under consideration when recognizing sadness.
‚Ä¢For the surprise facial expression, the FAUs that appear
only once are FAUs 2, 5. Since they appear only in the
surprise facial expression rule, they will be the ones taken
under consideration when recognizing surprise.
The deformed Candide grid produced by the grid-tracking al-
gorithm [51] that corresponds to the greatest intensity of the fa-cial expression shown, contains 104 nodes. Only some of thesenodes are important for facial expression recognition. For ex-
ample, nodes on the outer face contour do not contribute muchto facial expression recognition. Thus, a subset of 62 nodes
is chosen that controls the facial deformations. The grid that
is composed of these nodes can be seen in Fig. 7. From thispoint onward, this grid will be called Primary Facial Expres-
sion Grid (PFEG) . Fig. 6 presents the eight FAUs chosen as the
most representative for each facial expression. The FAU de Ô¨Åni-
tion image, as provided by Ekman and Friesen [52], is depicted,as well as its application to a poser from the Cohn ‚ÄìKanade data-
base used in our experiments.
C. FAUs Detection
In this section, only FAUs detection is described. The method
followed was the application of either the classical two class
SVMs (described in Section III-A) or the modi Ô¨Åed two class
SVMs (described in Section III-B). The accuracy rates obtainedfor FAUs detection using RBF and polynomial functions as ker-nels, for both the classical two class SVMs as well as the mod-
iÔ¨Åed two class SVMs and the original set of FAUs (FAUs 1, 2,
4, 5, 6, 7, 9, 10, 12, 15, 16, 17, 20, 23, 24, 25, and 26, 17 FAUstotal) proposed in [3], are presented in Fig. 8. The equivalentFAUs detection accuracies obtained when using our proposed
182 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 1, JANUARY 2007
Fig. 8. Accuracy rates obtained for the set of 17 FAUs detection.
Fig. 9. Accuracy rates obtained for the set of eight FAUs detection.
set of rules (corresponding to FAUs 5, 9, 12, 15, 16, 20, 23, and
24, eight FAUs total), are presented in Fig. 9.
1) FAUs Detection Using Candide Grid: The FAUs detection
accuracy was measured as the percentage of the correctly rec-ognized FAUs. The ground truth for FAUs was provided by theCohn ‚ÄìKanade database annotation. The achieved FAUs detec-
tion accuracy when the set of 17 FAUs was under examination
and the modi Ô¨Åed two class SVMs and Candide grid were used,
was equal to 82.7%. The equivalent FAUs detection accuracyrate, when our set of FAUs (subset of the 17 set of FAUs) was
taken under consideration was equal to 93.5%.
2) FAUs Detection Using PFEG Grid: The achieved FAUs
recognition accuracy when the original set of FAUs was underexamination and the modi Ô¨Åed two class SVMs and PFEG grid
were used, was equal to 84.7%. The equivalent FAUs recogni-
tion accuracy when our set of FAUs was taken under considera-tion was equal to 94.5%. The detection accuracy achieved by theproposed method for FAUs detection is quite satisfactory, when
compared with the state of the art facial expression recognition
performance for the Cohn ‚ÄìKanade database [26]. More specif-
ically, in [26], the recognition accuracy achieved was equal to95,6%, when using the Cohn ‚ÄìKanade database. The detected
FAUs can be separated in two groups, those of upper and those
of lower face. The accuracies achieved were 95.4% and 95.6%,respectively, although the classi Ô¨Åcation method followed was
not the leave-one-out procedure, as used in this paper.D. Facial Expression Recognition Using the Detected FAUs
In this section, facial expression recognition from the already
detected FAUs is performed, either using the original set of rulesas proposed in [3], or the newly proposed one. When the originalset of FAUs (17 FAUs total) was used to describe the six basic
facial expressions, and the Candide grid, taking under consider-
ation 104 nodes was applied, the facial expression recognitionaccuracy achieved was equal to 87.9%. When the chosen FAUssubset (FAUs 5, 9, 12, 15, 16, 20, 23, and 24, eight FAUs total)
was used to describe the six basic facial expressions, and the
Candide grid was applied, the equivalent recognition accuracyachieved was equal to 92.5%.
Regarding the application of PFEG grid (taking under consid-
eration 62 grid nodes), the recognition accuracy obtained for the
six basic facial expressions when the original set of 17 FAUs wasused, was equal to 93.75%. The equivalent recognition accu-racy achieved when the PFEG grid and the proposed set of eight
FAUs were used, was equal to 95.1%. Thus, when the FAUs an-
notation is available, the equivalent depicted facial expressioncan be recognized with an recognition accuracy of 95.1%, ifonly the FAUs 5, 9, 12, 15, 16, 20, 23, and 24, are taken into
consideration.
The accuracy rates obtained for facial expression recognition
from the detected FAUs using the proposed set of rules (eightFAUs) and applying the PFEG grid are presented in Fig. 10.
KOTSIA AND PITAS: FACIAL EXPRESSION RECOGNITION IN IMAGE SEQUENCES 183
Fig. 10. Accuracy rates obtained for facial expression recognition from the detected FAUs using the PFEG grid.
TABLE II
CONFUSION MATRICES FOR FAU D ETECTION -BASED FACIAL EXPRESSION RECOGNITION
WHEN USING THE CANDIDE AND PFEG G RID AND THE PROPOSED SET OF FAU R ULES
TABLE III
CONFUSION MATRICES FOR FACIAL EXPRESSION RECOGNITION USING MULTICLASS
SVM SWHEN APPLYING THE MODIFIED SVM S TO THE CANDIDE AND PFEG G RID
TheÔ¨Årst confusion matrix shown in Table III presents the results
obtained while using the Candide grid and the new set of rules
proposed. As can be seen, the most ambiguous facial expressionwas disgust, since it was misclassi Ô¨Åed the most times (as anger
and then sadness). The facial expressions that follow, are anger
and sadness, with a similar misclassi Ô¨Åcation rate. The second
confusion matrix shown in Table II presents the results obtainedwhile using the PFEG grid and the new set of rules proposed.As can be seen, the most ambiguous facial expression remaineddisgust, since it was misclassi Ô¨Åed most times, followed by anger
and then sadness.
E. Facial Expression Recognition Using Multiclass SVMs
In this section, facial expression recognition directly from the
grid nodes displacements is described. The method followed
was the application of either the classical six class SVMs (de-scribed in Section IV-A) or the modi Ô¨Åed six class SVMs (de-
scribed in Section IV-B).
184 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 1, JANUARY 2007
Fig. 11. Accuracy rates obtained for facial expression recognition using multiclass SVMs.
1) Facial Expression Recognition Using Candide Grid:
When the classical six class SVMs were applied to the classicCandide grid, taking under consideration 104 nodes, the facial
expression recognition accuracy achieved was equal to 91.4%.
The equivalent facial expression recognition accuracy, whenthe modi Ô¨Åed six class SVMs were used, was equal to 98.2%.
Therefore, the introduction of the modi Ô¨Åed six class SVMs
increases the recognition accuracy by 6.8%. The Ô¨Årst confusion
matrix shown in Table III presents the results obtained whileapplying the modi Ô¨Åed six class SVMs to the Candide grid. As
can be seen, the most ambiguous facial expression was anger,
being misclassi Ô¨Åed as sadness or disgust.
2) Facial Expression Recognition Using PFEG Grid: When
the classical six class SVMs were applied to the PFEG grid,
taking under consideration 62 nodes, the facial expression
recognition accuracy achieved was equal to 95.75%. The equiv-alent facial expression recognition accuracy when the modi Ô¨Åed
six class SVMs were used, was equal to 99.7%. Therefore,
the introduction of the modi Ô¨Åed six class SVMs increases the
recognition accuracy by 3.95%. The second confusion matrix
shown in Table III, presents the results obtained while applyingthe modi Ô¨Åed six class SVMs to the PFEG grid. As can be
seen, the most ambiguous facial expression remains anger,
since it was the only one being misclassi Ô¨Åed as another facial
expression (sadness). The recognition accuracies obtained
for facial expression recognition using six class SVMs, RBF,and polynomial functions as kernels for both the classical as
well as the modi Ô¨Åed six class SVMs, are presented in Fig. 11.
Polynomial kernels offer better recognition performance.
The recognition accuracy achieved by the proposed method
for facial expression recognition is better than any other re-ported in the literature so far for the Cohn ‚ÄìKanade database,
at least according to the authors knowledge. More speci Ô¨Åcally,
in [7] the recognition accuracy achieved was equal to 74.5%
for the Cohn ‚ÄìKanade database and leave-one-out approach,
while in [8], it was 90.7%. Generally speaking, the best fa-
cial expression recognition accuracy reported so far, is equal
to 96.1% [53].
In order to understand if the proposed modi Ô¨Åed class of
SVMs approach is statistically signi Ô¨Åcant better than the clas-
sical SVMs approach, the McNemar ‚Äôs [54] has been used.
McNemar ‚Äôs test is a null hypothesis statistical test based ona Bernoulli model. If the resulting
-value is below a desired
signi Ô¨Åcance level (for example, 0.02), the null hypothesis is re-
jected and the performance difference between two algorithmsis considered to be statistically signi Ô¨Åcant. Using this test, it
has been veri Ô¨Åed that the modi Ô¨Åed class of SVMs outperforms
the other tested classi Ô¨Åers in the demonstrated experiments at a
signi Ô¨Åcant level less that
.
The experiments indicated that for both approaches, the whole
system is fast enough to perform almost real-time facial expres-
sion recognition, on a PC having an Intel Centrino (1.5 GHz)
processor with 1-GB RAM memory, since it is able to classify
expressions at a rate of 20 frames per second during testing.
VI. C ONCLUSION
Two novel methods for facial expression recognition using
SVMs for facial expression recognition are proposed in thispaper. A novel class for SVMs classi Ô¨Åers that incorporates sta-
tistical information of the classes under examination, is also pro-
posed. The user initializes some of the Candide grid nodes on thefacial image depicted at the Ô¨Årst frame of the image sequence.
The tracking system used, based on deformable models, tracksthe facial expression as it evolves over time, by deforming theCandide grid, eventually producing the grid that correspondsto the facial expression ‚Äôs greatest intensity classically depicted
at the last facial video frame. Only Candide nodes that in Ô¨Çu-
ence the formation of FAUs are used in our system.Their ge-ometrical displacement, de Ô¨Åned as their coordinate difference
between the last and the Ô¨Årst frame, is used as an input to the
SVMs system (either the classical one or the modi Ô¨Åed one). In
the case of facial expression recognition, this system is com-posed of one six-class SVMs, one for each one of the six basicfacial expressions (anger, disgust, fear, happiness, sadness, andsurprise) to be recognized. When FAUs detection-based facialexpression recognition is attempted, the SVMs system consistsof eight one-class SVMs, one for each one of the eight chosenFAUs used. The proposed methods, achieve a facial expressionrecognition accuracy of 99.7% and 95.1%, respectively. Theachieved accuracy for facial expression recognition using mul-ticlass SVMs (99.7%) is better than any other reported in the lit-erature so far for the Cohn ‚ÄìKanade database, at least according
to the authors knowledge.
KOTSIA AND PITAS: FACIAL EXPRESSION RECOGNITION IN IMAGE SEQUENCES 185
APPENDIX
WOLF DUAL PROBLEM FOR THE MODIFIED MULTICLASS
SVM S:In order to Ô¨Ånd the optimum separating hyperplanes for
the optimization problem (21) subject to the constraints (18), we
have to de Ô¨Åne the saddle point of the Langragian (22).
In the saddle point, the solution should satisfy the KT condi-
tions for
(31)
(32)
and
(33)
Substituting (31) back into (22), we obtain
(34)
Adding the constraint (33), the terms in
 disappear. Consid-
ering the two terms in
 , only
and
(35)
but from (32), we have
(36)so
 and the two terms cancel, giving
(37)
Since
 ,w eh a v e
but
 so
(38)
which is a quadratic function in terms of alpha with linear
constraints
(39)
and
(40)
This gives the decision function
(41)
or equivalently

186 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 1, JANUARY 2007
REFERENCES
[1] P. Ekman and W. V. Friesen , Emotion in the Human Face . Engle-
wood Cliffs, NJ: Prentice-Hall, 1975.
[2] T. Kanade, J. Cohn, and Y. Tian, ‚ÄúComprehensive database for facial
expression analysis, ‚ÄùinProc. IEEE Int. Conf. Face and Gesture Recog-
nition , Mar. 2000, pp. 46 ‚Äì53.
[3] M. Pantic and L. J. M. Rothkrantz, ‚ÄúExpert system for automatic anal-
ysis of facial expressions, ‚ÄùImage Vis. Comput. , vol. 18, no. 11, pp.
881‚Äì905, Aug. 2000.
[4]‚Äî‚Äî ,‚ÄúAutomatic analysis of facial expressions: The state of the art, ‚Äù
IEEE Trans. Pattern Anal. Mach. Intell. , vol. 22, no. 12, pp. 1424 ‚Äì1445,
Dec. 2000.
[5] B. Fasel and J. Luettin, ‚ÄúAutomatic facial expression analysis: A
survey, ‚ÄùPattern Recognit. , vol. 36, no. 1, pp. 259 ‚Äì275, 2003.
[6] Y. Zhang and Q. Ji, ‚ÄúActive and dynamic information fusion for facial
expression understanding from image sequences, ‚ÄùIEEE Trans. Pattern
Anal. Mach. Intell. , vol. 27, no. 5, pp. 699 ‚Äì714, May 2005.
[7] I. Cohen, N. Sebe, S. Garg, L. S. Chen, and T. S. Huanga, ‚ÄúFacial
expression recognition from video sequences: temporal and static
modelling, ‚ÄùComput. Vis. Image Understand. , vol. 91, pp. 160 ‚Äì187,
2003.
[8] M. S. Bartlett, G. Littlewort, I. Fasel, and J. R. Movellan, ‚ÄúReal time
face detection and facial expression recognition: Development and ap-
plications to human computer interaction, ‚ÄùinProc. Conf. Computer
Vision and Pattern Recognition Workshop , Madison, WI, Jun. 16 ‚Äì22,
2003, vol. 5, pp. 53 ‚Äì58.
[9] M. J. Lyons, S. Akamatsu, M. Kamachi, and J. Gyoba, ‚ÄúCoding facial
expressions with G abor wavelets, ‚ÄùinProc. 3rd IEEE Int. Conf. Auto-
matic Face and Gesture Recognition , 1998, pp. 200 ‚Äì205.
[10] M. J. Lyons, J. Budynek, and S. Akamatsu, ‚ÄúAutomatic classi Ô¨Åcation
of single facial images, ‚ÄùIEEE Trans. Pattern Anal. Mach. Intell. , vol.
21, no. 12, pp. 1357 ‚Äì1362, Dec. 1999.
[11] L. Wiskott, J. Fellous, N. Kr √ºger, and C. v. d. Malsburg, ‚ÄúFace recog-
nition by elastic bunch graph matching, ‚ÄùIEEE Trans. Pattern Anal.
Mach. Intell. , vol. 19, no. 7, pp. 775 ‚Äì779, Jul. 1997.
[12] G. Guo and C. R. Dyer, ‚ÄúLearning from examples in the small sample
case: Face expression recognition, ‚ÄùIEEE Trans. Syst., Man, Cybern.
B, Cybern. , vol. 35, no. 3, pp. 477 ‚Äì488, Jun. 2005.
[13] Z. Zhang, M. Lyons, M. Schuster, and S. Akamatsu, ‚ÄúComparison be-
tween geometry-based and G abor-wavelets-based facial expression
recognition using multi-layer perceptron, ‚ÄùinProc. 3rd IEEE Int. Conf.
Automatic Face and Gesture Recognition , Nara, Japan, Apr. 14 ‚Äì16,
1998, pp. 454 ‚Äì459.
[14] B. Fasel, ‚ÄúMultiscale Facial Expression Recognition Using Convolu-
tional Neural Networks, ‚ÄùTech. Rep., IDIAP, 2002, .
[15] M. Matsugu, K. Mori, Y. Mitari, and Y. Kaneda, ‚ÄúSubject independent
facial expression recognition with robust face detection using a convo-
lutional neural network, ‚ÄùNeural Netw. , vol. 16, no. 5 ‚Äì6, pp. 555 ‚Äì559,
Jun.-Jul. 2003.
[16] M. Rosenblum, Y. Yacoob, and L. S. Davis, ‚ÄúHuman expression
recognition from motion using a radial basis function network archi-
tecture, ‚ÄùIEEE Trans. Neural Netw. , vol. 7, no. 5, pp. 1121 ‚Äì1138,
Sep. 1996.
[17] L. Ma and K. Khorasani, ‚ÄúFacial expression recognition using con-
structive feedforward neural networks, ‚ÄùIEEE Trans. Syst., Man, Cy-
bern. B, Cybern. , vol. 34, no. 3, pp. 1588 ‚Äì1595, Jun. 2004.
[18] S. Dubuisson, F. Davoine, and M. Masson, ‚ÄúA solution for facial
expression representation and recognition, ‚ÄùSignal Process.: Image
Commun. , vol. 17, no. 9, pp. 657 ‚Äì673, Oct. 2002.
[19] X.-W. Chen and T. Huang, ‚ÄúFacial expression recognition: A clus-
tering-based approach, ‚ÄùPattern Recognit. Lett. , vol. 24, no. 9 ‚Äì10, pp.
1295 ‚Äì1302, Jun. 2003.
[20] Y. Gao, M. Leung, S. Hui, and M. Tananda, ‚ÄúFacial expression recog-
nition from line-based caricatures, ‚ÄùIEEE Trans. Syst., Man, Cybern.
A: Syst. Humans , vol. 33, no. 3, pp. 407 ‚Äì412, May 2003.
[21] B. Abboud, F. Davoine, and M. Dang, ‚ÄúFacial expression recognition
and synthesis based on an appearance model, ‚ÄùSignal Process.: Image
Commun. , vol. 19, no. 8, pp. 723 ‚Äì740, 2004.
[22] I. A. Essa and A. P. Pentland, ‚ÄúFacial expression recognition using a
dynamic model and motion energy, ‚Äùpresented at the Int. Conf. Com-
puter Vision, Cambrdige, MA, Jun. 20 ‚Äì23, 1995.
[23] ‚Äî‚Äî ,‚ÄúCoding, analysis, interpretation, and recognition of facial ex-
pressions, ‚ÄùIEEE Trans. Pattern Anal. Mach. Intell. , vol. 19, no. 7, pp.
757‚Äì763, Jul. 1997.[24] M. S. Bartlett, G. Littlewort, B. Braathen, T. J. Sejnowski, and J. R.
Movellan, ‚ÄúAn approach to automatic analysis of spontaneous facial
expressions, ‚Äùpresented at the 5th IEEE Int. Conf. Automatic Face and
Gesture Recognition, Washington, DC, 2002.
[25] G. Donato, M. S. Bartlett, J. C. Hager, P. Ekman, and T. J. Sejnowski,
‚ÄúClassifying facial actions, ‚ÄùIEEE Trans. Pattern Anal. Mach. Intell. ,
vol. 21, no. 10, pp. 974 ‚Äì989, Oct. 1999.
[26] Y. L. Tian, T. Kanade, and J. F. Cohn, ‚ÄúRecognizing action units for
facial expression analysis, ‚ÄùIEEE Trans. Pattern Anal. Mach. Intell. ,
vol. 23, no. 2, pp. 97 ‚Äì115, Feb. 2001.
[27] J. J. Lien, T. Kanade, J. Cohn, and C. C. Li, ‚ÄúAutomated facial ex-
pression recognition based on facs action units, ‚ÄùinProc. 3rd IEEE
Int. Conf. Automatic Face and Gesture Recognition , Apr. 1998, pp.
390‚Äì395.
[28] J. J. Lien, T. Kanade, J. F. Cohn, and C. Li, ‚ÄúDetection, tracking, and
classi Ô¨Åcation of action units in facial expression, ‚ÄùJ. Robot. Auton.
Syst., Jul. 1999.
[29] Y. L. Tian, T. Kanade, and J. Cohn, ‚ÄúEvaluation of G abor wavelet-
based facial action unit recognition in image sequences of increasing
complexity, ‚ÄùinProc. 5th IEEE Int. Conf. Automatic Face and Gesture
Recognition , 2002, pp. 229 ‚Äì234.
[30] A. Tefas, C. Kotropoulos, and I. Pitas, ‚ÄúUsing support vector machines
to enhance the performance of elastic graph matching for frontal face
authentication, ‚ÄùIEEE Trans. Pattern Anal. Mach. Intell. , vol. 23, no. 7,
pp. 735 ‚Äì746, Jul. 2001.
[31] H. Drucker, W. Donghui, and V. Vapnik, ‚ÄúSupport vector machines
for spam categorization, ‚ÄùIEEE Trans. Neural Netw. , vol. 10, no. 5, pp.
1048 ‚Äì1054, Sep. 1999.
[32] A. Ganapathiraju, J. Hamaker, and J. Picone, ‚ÄúApplications of support
vector machines to speech recognition, ‚ÄùIEEE Trans. Signal Process. ,
vol. 52, no. 8, pp. 2348 ‚Äì2355, Aug, 2004.
[33] M. Pontil and A. Verri, ‚ÄúSupport vector machines for 3D object recog-
nition, ‚ÄùIEEE Trans. Pattern Anal. Mach. Intell. , vol. 20, no. 6, pp.
637‚Äì646, Jun. 1998.
[34] M. Rydfalk, ‚ÄúCANDIDE: A Parameterized Face, ‚ÄùTech. Rep.,
Linkoping Univ., 1978.
[35] F. Dornaika and F. Davoine, ‚ÄúSimultaneous facial action tracking and
expression recognition using a particle Ô¨Ålter,‚Äùpresented at the IEEE
Int. Conf. Computer Vision, Beijing, China, Oct. 17 ‚Äì20, 2005.
[36] S. B. Gokturk, C. Tomasi, B. Girod, and J.-Y. Bouguet, ‚ÄúModel-based
face tracking for view-independent facial expression recognition, ‚Äùin
Proc. 5th IEEE Int. Conf. Automatic Face and Gesture Recognition ,
Cambridge, U.K., May 2002, pp. 287 ‚Äì293.
[37] M. Malciu and F. Preteux, ‚ÄúTracking facial features in video sequences
using a deformable model-based approach, ‚ÄùProc. SPIE , vol. 4121, pp.
51‚Äì62, 2000.
[38] P. Michel and R. Kaliouby, ‚ÄúReal time facial expression recognition
in video using support vector machines, ‚ÄùinProc. 5th Int. Conf. Multi-
modal interfaces , Vancouver, BC, Canada, 2003, pp. 258 ‚Äì264.
[39] V. Vapnik , Statistical learning theory . New York: Wiley, 1998.
[40] J. Y. Bouguet, ‚ÄúPyramidal Implementation of the Lucas-Kanade Fea-
ture Tracker, ‚ÄùTech. Rep., Intel Corporation, Microprocessor Research
Labs, 1999.
[41] C. W. Hsu and C. J. Lin, ‚ÄúA comparison of methods for multiclass
support vector machines, ‚ÄùIEEE Trans. Neural Netw. , vol. 13, no. 2,
pp. 415 ‚Äì425, Mar. 2002.
[42] C. J. C. Burges, ‚ÄúA tutorial on support vector machines for pattern
recognition, ‚ÄùData Mining Knowl. Disc. , vol. 2, no. 2, 1998.
[43] B. Scholkopf, S. Mika, C. Burges, P. Knirsch, K.-R. Muller, G. Ratsch,
and A. Smola, ‚ÄúInput space vs. feature space in kernel-based methods, ‚Äù
IEEE Trans. Neural Netw. , vol. 10, no. 5, pp. 1000 ‚Äì1017, Sep. 1999.
[44] K.-R. Muller, S. Mika, G. Ratsch, K. Tsuda, and B. Scholkopf, ‚ÄúAn
introduction to kernel-based learning algorithms, ‚ÄùIEEE Trans. Neural
Netw. , vol. 12, no. 2, pp. 181 ‚Äì201, Mar. 2001.
[45] R. Duda and P. Hart , Pattern Classi Ô¨Åcation and Scene Analysis .N e w
York: Wiley, 1973.
[46] S. Gunn, ‚ÄúSupport vector machines for classi Ô¨Åcation and regression, ‚Äù
MP-TR-98-05, Image, Speech, Intell. Syst. Group, Univ. Southampton,
1998.
[47] MATLAB User ‚Äôs Guide . Natick, MA: The MathWorks, Inc.,
1994 ‚Äì2001.
[48] L. Bottou, C. Cortes, J. Denker, H. Drucker, I. Guyon, L. Jackel, Y.
LeCun, U. Muller, E. Sackinger, P. Simard, and V. Vapnik, ‚ÄúCompar-
ison of classi Ô¨Åer methods: A case study in handwritting digit recogni-
tion,‚ÄùinProc. Int. Conf. Pattern Recognition , 1994, pp. 77 ‚Äì87.
KOTSIA AND PITAS: FACIAL EXPRESSION RECOGNITION IN IMAGE SEQUENCES 187
[49] J. Weston and C. Watkins, ‚ÄúMulti-Class Support Vector Machines, ‚Äù
Tech. Rep. CSD-TR-98-04, 2004.
[50] J. Weston and C. Watkins, ‚ÄúMulti-class support vector machines, ‚Äùpre-
sented at the ESANN, Brussels, Belgium, 1999.
[51] S. Krinidis and I. Pitas, ‚Äú2-D physics-based deformable shape models:
Explicit governing equations, ‚ÄùinProc. 1st Int. Workshop on Interactive
Rich Media Content Production: Architectures, Technologies, Applica-
tions, Tools , Lausanne, Switzerland, Oct. 16 ‚Äì17, 2003, pp. 43 ‚Äì55.
[52] P. Ekman and W. V. Friesen, ‚ÄúFACS ‚ÄîFacial Action Coding System, ‚Äù
1978 [Online]. Available: http://www-2.cs.cmu.edu/afs/cs/project/
face/www/facs.htm
[53] F. Dornaika and F. Davoine, ‚ÄúView- and texture-independent facial ex-
pression recognition in videos using dynamic programming, ‚Äùpresented
at the IEEE Int. Conf. Image Processing Genova, Italy, Sep. 11 ‚Äì14,
2005.
[54] L. Gillick and S. Cox, ‚ÄúSome statistical issues in the comparison of
speech recognition algorithms, ‚ÄùinProc. ICASSP , 1989, pp. 532 ‚Äì535.
Irene Kotsia received the B.S. degree from the
Department of Informatics, Aristotle University of
Thessaloniki, Thessaloniki, Greece, in 2002, where
she is currently pursuing the Ph.D. degree in the
Department of Informatics.
She is currently a Researcher and Teaching Assis-
tant. Her current research interests lie in the areas offacial expression recognition from static images and
image sequences, as well as in the area of graphics
and animation.
Ioannis Pitas (SM‚Äô94) received the Diploma of
electrical engineering and the Ph.D. degree in
electrical engineering from the Aristotle University
of Thessaloniki, Thessaloniki, Greece, in 1980 and
1985, respectively.
Since 1994, he has been a Professor at the De-
partment of Informatics, Aristotle University of
Thessaloniki, where he served as Scienti Ô¨Åc Assis-
tant, Lecturer, Assistant Professor, and Associate
Professor in the Department of Electrical and Com-
puter Engineering from 1980 to 1993. He served as a
Visiting Research Associate at the University of Toronto, Toronto, ON, Canada;
the University of Erlangen-Nuernberg, Nuernberg, Germany; and the Tampere
University of Technology, Tampere, Finland. He also served as Visiting
Assistant Professor at the University of Toronto and Visiting Professor at theUniversity of British Columbia, Vancouver, BC, Canada. He was a Lecturer in
short courses for continuing education. He has published over 380 papers and
contributed to 13 books in his areas of interest. He is the co-author of the books
Nonlinear Digital Filters: Principles and Applications (Norwell, MA: Kluwer,
1990), 3-D Image Processing Algorithms (New York: Wiley, 2000), Nonlinear
Model-Based Image/Video Processing and Analysis (New York: Wiley, 2001),
and author of Digital Image Processing Algorithms and Applications (New
York: Wiley, 2000). He is also the Editor of the book Parallel Algorithms
and Architectures for Digital Image Processing, Computer Vision and Neural
Networks (New York: Wiley, 1993). His current interests are in the areas of
digital image and video processing and analysis, multidimensional signal
processing, watermarking, and computer vision.
Dr. Pitas has been member of the European Community ESPRIT Parallel
Action Committee. He has also been an invited speaker and/or member of the
program committee of several scienti Ô¨Åc conferences and workshops. He was
an Associate Editor of the IEEE T
RANSACTIONS ON CIRCUITS AND SYSTEMS ,
the IEEE T RANSACTIONS ON NEURAL NETWORKS , the IEEE T RANSACTIONS ON
IMAGE PROCESSING , the EURASIP Journal on Applied Signal Processing , and
Co-Editor of Multidimensional Systems and Signal Processing andSignal Pro-
cessing . He was the General Chair of the 1995 IEEE Workshop on Nonlinear
Signal and Image Processing, Technical Chair of the 1998 European Signal Pro-
cessing Conference, and General Chair of IEEE International Conference on
Image Processing 2001.
"
https://ieeexplore.ieee.org/document/7518582,"1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE
Transactions on Affective Computing
1
Facial Expression Recognition in Video with
Multiple Feature Fusion
Junkai Chen, Zenghai Chen, Zheru Chi, Member, IEEE and Hong Fu,
Abstract‚ÄîVideo based facial expression recognition has been a long standing problem and attracted growing attention recently. The
key to a successful facial expression recognition system is to exploit the potentials of audiovisual modalities and design robust features
to effectively characterize the facial appearance and conÔ¨Åguration changes caused by facial motions. We propose an effective
framework to address this issue in this paper. In our study, both visual modalities (face images) and audio modalities (speech) are
utilized. A new feature descriptor called Histogram of Oriented Gradients from Three Orthogonal Planes (HOG-TOP) is proposed to
extract dynamic textures from video sequences to characterize facial appearance changes. And a new effective geometric feature
derived from the warp transformation of facial landmarks is proposed to capture facial conÔ¨Åguration changes. Moreover, the role of
audio modalities on recognition is also explored in our study. We applied the multiple feature fusion to tackle the video-based facial
expression recognition problem under lab-controlled environment and in the wild, respectively. Experiments conducted on the extended
Kohn-Kanada (CK+) database and the Acted Facial Expression in Wild (AFEW) 4.0 database show that our approach is robust in
dealing with video-based facial expression recognition problem under lab-controlled environment and in the wild compared with the
other state-of-the-art methods.
Index Terms‚ÄîFacial expression recognition; Multiple feature fusion; HOG-TOP; Geometric warp feature; Acoustic feature.
F
1 I NTRODUCTION
FACIAL expression, as a powerful nonverbal channel,
plays an important role for human beings to convey
emotions and transmit messages. Automatic facial expres-
sion recognition (AFEC) can be widely applied in many
Ô¨Åelds such as medical assessment, lie detection and human
computer interaction [1]. AFEC has attracted great interest
in the past two decades. However, facial expression analysis
is a very challenging task because facial expressions caused
by facial muscle movements are subtle and transient [2]. To
capture and represent these movements is a key issue to be
addressed in facial expression analysis.
Two main streams of facial expressions analysis are
widely adopted in the current research and development.
One stream is to detect facial actions. The study reported
in [3], [4] showed that each facial expression contains a u-
nique group of facial action units. The Facial Action Coding
System (FACS), which was Ô¨Årst proposed by Ekman and
Friesen in 1978 [5] and then enhanced in 2002 [6], is the
best known system developed for human beings to describe
facial actions. Another stream of facial expression analysis is
to carry out facial affect (emotion) recognition directly. Most
researchers deal with the recognition task of six universal
emotions: happy, sad, fear, disgust, angry and surprise [7].
Many efforts have been made for facial expression recog-
nition. The methodologies used are commonly categorized
Junkai Chen and Zheru Chi are with the Department of Electronic and
Information Engineering, The Hong Kong Polytechnic University, Hong
Kong. E-mail: Junkai.Chen@connect.polyu.hk, chi.zheru@polyu.edu.hk
Zenghai Chen is with the School of Electrical and Electronic Engi-
neering, Nanyang Technological University, Singapore. E-mail: zeng-
haichen@ntu.edu.sg
Hong Fu is with the Department of Computer Science, Chu Hai College
of Higher Education, Hong Kong. E-mail: hongfu@chuhai.edu.hkinto appearance based methods and geometry based meth-
ods [8]. An appearance based method applies feature de-
scriptors to model facial texture changes. A geometry based
method captures facial conÔ¨Ågurations in which a set of facial
Ô¨Åducial points is used to characterize the face shape.
Previous works mainly focused on static and single
face image based facial expression recognition. Recently,
facial expression recognition in video has attracted great
interest. Compared with a static image, a video sequence
can not only provide spatial appearance but also include
facial motions and accompanied speech. The key to solve
the problem of video based facial expression recognition is
to exploit the representation capability of multi modalities
(e.g. visual and audio information) and design robust fea-
tures to effectively characterize the facial appearance and
conÔ¨Åguration changes caused by facial muscular activities.
To achieve this goal, we propose an effective frame-
work based on multiple feature fusion for facial expression
recognition in video. We explore the potentials of visual
modalities (face images) and audio modalities (speech) in
our study. In addressing visual modalities, we extend the
Histograms of Oriented Gradients (HOG) [9] to temporal
Three Orthogonal Planes (TOP), inspired by a temporal
extension of Local Binary Patterns, LBP-TOP [10]. The pro-
posed HOG-TOP is used to characterize facial appearance
changes. We show that HOG-TOP performs as well as LBP-
TOP for facial expression recognition. In addition, compared
with LBP-TOP , HOG-TOP is more compact and effective
to characterize facial appearance changes. Moreover, an
effective geometric warp feature derived from the warp
transformation of facial landmarks is proposed to capture
facial conÔ¨Åguration changes. We show that the proposed
geometric warp feature is more effective compared with
other proposed geometric features [11], [12]. We also ex-
1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE
Transactions on Affective Computing
2
Input video  
Face sequences  
Facial landmarks  Facial 
configuration 
changes  
Acoustic feature  
Features pool  Geometric 
feature 
‚Ä¶ 
Dynamic textures (HOG-TOP) 
Acoustic 
Kernel  
Geometric 
Kernel  Appearance 
Kernel  Multiple kernel SVM for 
facial expression recognition  
in the wild  
Multiple kernel SVM for   
lab-controlled  
facial expression recognition  
Fig. 1  
Fig. 1: Block diagram of our proposed framework. Geometric features coupled with dynamic textures (HOT-TOP) are used
to deal with lab-controlled facial expression recognition while acoustic features and dynamic textures (HOG-TOP) are fused
to tackle facial expression recognition in the wild.
plore the role of audio modalities on affect recognition. We
Ô¨Ånd that audio modalities can provide some complemen-
tary information, especially for facial expression recognition
in the wild. We further apply a multiple feature fusion
method to deal with facial expression recognition under
lab-controlled environment and in the wild, respectively. As
shown in Fig. 1, geometric features coupled with dynamic
textures (HOT-TOP) are used to deal with lab-controlled fa-
cial expression recognition. Acoustic features and dynamic
textures (HOG-TOP) are fused to tackle facial expression
recognition in the wild.
Our contributions are summarized as follows:
1. We develop a framework which can effectively tackle
facial expression recognition in video. A multiple feature
fusion method is used to deal with facial expression
recognition under lab-controlled environment and in the
wild, respectively.
2. We propose a new feature descriptor HOG-TOP to
characterize facial appearance changes and a new effective
geometric feature to capture facial conÔ¨Åguration changes.
3. We show that multiple features can make different
contributions and can achieve better performance than
the individual features applied alone. We also show that
multiple feature fusion can enhance the discriminative
power of multiple features.
The remainder of the paper is organized as follows. In
Section 2, we review some related work. Section 3 presents
our proposed approach. Experimental results and discus-
sions are presented in Section 4. The paper is concluded in
Section 5.2 R ELATED WORK AND MOTIVATION
2.1 Static Image Based Methods
Many researchers apply static image based models to han-
dle facial expression problem. One or several peak frames
are usually selected for extracting appearance or geometric
features. For instance, the methods reported in [11], [12], [13]
applied the facial landmarks to characterize the whole face
shape. And the method [14] measured the displacements
of several selected candidate Ô¨Åducial points. Bag of Words
(BoW) based on the multi-scale dense SIFT features were
applied to represent facial appearance textures in [15]. Local
Fisher discriminant analysis (LFDA) was used for feature
extraction in [16]. The method [17] applied Gabor Ô¨Ålters
to extract facial movement features. A novel framework
for expression recognition by using appearance features of
selected facial patches was proposed in [18]. However, au-
tomatically picking out key frames from a video sequence is
usually difÔ¨Åcult. The methods [19], [20] attempted to classify
every frame Ô¨Årst and adopt a voting strategy to label the
video sequence. It is necessary to extract features from each
frame. LBP was applied in [19]. Pyramid of Histograms of
Oriented Gradients (PHOG) and Local Phase Quantization
(LPQ) features were used in [20].
2.2 Dynamic Texture Based Methods
There exists a drawback for a static image based method:
extracting features from an individual frame fails to utilize
dynamic information which is important to describe facial
motions. Dynamic texture based methods can effectively
deal with this problem. Dynamic texture based methods
1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE
Transactions on Affective Computing
3
attempt to simultaneously model the spatial appearance
and dynamic motions in a video sequence. Zhao et al. [10]
proposed LBP-TOP , a temporal extension of local binary
patterns, for facial expression analysis in video. A facial
component LBP-TOP was proposed in [21]. A Local Gabor
Binary Patterns from Three Orthogonal Planes (LGBP-TOP)
was proposed in [22]. A SpatioTemporal Local Monogenic
Binary Pattern (STLMBP) feature descriptor was proposed
in [23], [24]. In addition, Long et al. [25] employed Inde-
pendent Component Analysis (ICA) to learn spatiotemporal
Ô¨Ålters from videos, and then extracted dynamic textures
using the learned Ô¨Ålters. Chew et al. [26] employed sparse
temporal representation to model the temporal dynamics of
facial expressions in video. Li et al. [27] developed a dy-
namic Bayesian network to simultaneously and coherently
represent the facial evolvement at different levels.
2.3 Audiovisual Based Methods
Static image based methods or dynamic texture based
methods only rely on visual modalities. However, audio
or speech is also important for human beings to convey
emotions and intentions. Audio modalities can provide
some complementary information in addition to visual
modalities. Recently, audiovisual based methods for affect
recognition have attracted growing attention from the affec-
tive computing community. A number of approaches have
been proposed to combine audio and visual modalities for
affect recognition (e.g. [28], [29], [30], [31]). A comprehensive
survey can be found in [32]. Acoustic features extracted
from voice or speech and visual features extracted from
face images are combined to tackle this problem. For ex-
ample, voice and lip activity were used in [33]. Face images
and speech were employed in [34]. The methods reported
in [35], [36] applied several feature descriptors such as SIFT,
HOG, PHOG etc. to encode face images and combined them
with acoustic features to recognize facial expression in the
wild.
2.4 Motivation
We can see that feature extraction plays a center role on
affect recognition in video. Designing an effective feature
is important and meaningful. LBP-TOP is widely used for
modeling dynamic textures. However, there are two limita-
tions of LBP-TOP . One is the high dimensionality. The size of
LBP-TOP coded using a uniform pattern is 593[10]. More-
over, although LBP-TOP is robust to deal with illumination
changes, it is insensitive to facial muscle deformations. In
this work, we propose a new feature called HOG-TOP ,
which is more compact and effective to characterize facial
appearance changes. More details on HOG-TOP can be
found in Section 3.1.
In addition, conÔ¨Åguration and shape representations
play an important role in human vision for the perception
of facial expressions [37]. We believe that previous works
have not yet fully exploited the potentials of conÔ¨Ågura-
tion representations. Characterizing face shape [11], [12] or
measuring displacements of Ô¨Åducial points [14], [38] only
are not sufÔ¨Åcient to capture facial conÔ¨Åguration changes,
especially the subtle non-rigid changes. In this work, we
introduce a more robust geometric feature to capture facial
x 
y t 
X-Y X-T 
Y-T 
Fig. 2  Fig. 2: The textures in XY, XT and YT planes.
Y X T 
X-T 
Y-T X-Y 
Fig. 3  
Fig. 3: The HOG from Three Orthogonal Planes (TOPs).
conÔ¨Åguration changes. More discussion on our proposed
geometric feature is given in Section 3.2.
3 M ETHODOLOGY
This section presents the details of our proposed approach.
We introduce the three types of features and multiple feature
fusion employed in our study.
3.1 Histogram of Oriented Gradients from Three Or-
thogonal Planes
Histograms of oriented gradients (HOG) [9] were Ô¨Årst pro-
posed for human detection. The basic idea of HOG is that
local object appearance and shape can often be characterized
rather well by the distribution of local intensity gradients or
edge directions. HOG is sensitive to object deformations.
Facial expressions are caused by facial muscle movements.
For example, mouth opening and raised eyebrows will gen-
erate a surprise facial expression. These movements could
be regarded as types of deformations. HOG can effectively
capture and represent these deformations [39]. However,
the original HOG is limited to deal with a static image.
In order to model dynamic textures from a video sequence
with HOG, we extend HOG to 3-D to compute the oriented
gradients on three orthogonal planes XY, XT, and YT (TOP),
i.e. HOG-TOP . The proposed HOG-TOP is used to charac-
terize facial appearance changes.
A video sequence includes three orthogonal directions,
i.e. X, Y, and T (time) directions. The XY plane provides spa-
tial appearance, and XT and YT planes record temporal or
motion information. Fig. 2 illustrates the textures extracted
from the three orthogonal planes. In our study, we compute
the distributions of oriented gradients of each plane and
obtain HOG features, namely HOG-XY, HOG-XT and HOG-
YT, as shown in Fig. 3. Each point in a video sequence
includes three orthogonal neighborhoods lying on XY, XT
and YT planes, respectively. We Ô¨Årst compute the gradients
along X, Y and T directions with a 33Sobel mask. The gra-
dient orientations are deÔ¨Åned as XY=tan 1(GY=GX),
XT=tan 1(GT=GX),YT=tan 1(GT=GY), where
GX,GY, andGTare the gradients along the X, Y and T
1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE
Transactions on Affective Computing
4
directions, respectively. These angles are quantized into K
(K is 9 in our work) orientation bins with a range of 0 360
or0 180.
We enumerate the appearance of these gradient orien-
tations and obtain a histogram in each plane. The three
histograms are concatenated to form a global description
with the spatial and temporal features. Fig. 3 shows that the
three histograms from the three planes are combined into a
single one. The HOG-TOP computation algorithm is shown
in Algorithm 1.
Algorithm 1 Compute the HOG-TOP .
Input: Video sequence V , which contains Nframes with
the same width and height.
Output: The histograms of oriented gradients from three
orthogonal plans (HOG-TOP).
Algorithm:
Get the number of frames N, frame width Wand height
H.
fort= 2 :N 1do
forx= 2 :W 1do
fory= 2 :H 1do
get the local patch in XY, XT, and YT planes.
Pxy=V(x 1 :x+ 1;y 1 :y+ 1;t);
Pxt=V(x 1 :x+ 1;y;t 1 :t+ 1);
Pyt=V(x;y 1 :y+ 1;t 1 :t+ 1);
Compute the gradients GX,GY, andGT; and
gradient orientations XY,XT,YT. Quantize the
XY,XT,YTinto one of 9 bins. Get a histogram
in each plan, i.e. HOG-XY, HOG-XT and HOG-YT.
end for
end for
end for
Normalize the HOG-XY, HOG-XT and HOG-YT respec-
tively. Concatenate the three histograms into a long his-
togram.
LBP-TOP computes the difference of a pixel with respect
to its neighborhood, making LBP-TOP robust in dealing
with illumination changes. HOG-TOP computes the orient-
ed gradients of a pixel, which is more sensitive to object
deformations [9]. Facial expressions are caused by facial
muscle movements, which can be regarded as types of
muscle deformations. HOG-TOP is therefore more effective
to characterize facial appearance changes than LBP-TOP .
Another advantage of HOG-TOP is the feature dimen-
sionality. Compared with LBP-TOP , the size of HOG-TOP is
much smaller than that of LBP-TOP . The size of LBP-TOP
coded using a uniform pattern is 593[10], [40]. and the
size of HOG-TOP quantized into 9 bins is 93, which is
much more compact than that of LBP-TOP .
In order to utilize local spatial information, a block-based
method is introduced in our study, as shown in Fig. 4. We
can divide the image sequence into many blocks and extract
the HOG-TOP features from each block. The HOG-TOP
features of all the blocks can be concatenated to represent
the whole sequence. In our experiments, the face is Ô¨Årst
cropped from the original image and resized to 128128.
We partition the face image into 88blocks with each block
having a size of 1616. The number of bins is set to 9 with
‚Ä¶ Features of one block  
Features extracted from the whole sequence.  
Fig. 4  Fig. 4: The HOG-TOP features extracted from each block are
concatenated together to represent the whole sequence.
an angle range of 0 180.
3.2 Geometric Warp Feature
In this section, we introduce a more robust geometric feature
namely geometric warp feature, which is derived from the
warp transform of the facial landmarks. Facial expressions
are caused by facial muscle movements. These movements
result in the displacements of the facial landmarks. Here we
assume that each face image consists of many sub-regions.
These sub-regions can be formed with triangles with their
vertexes located at facial landmarks, as shown in Fig. 5. The
displacements of facial landmarks cause the deformations
of the triangles. We propose to utilize the deformations to
represent facial conÔ¨Åguration changes.
Facial expression can be considered as a dynamic process
including onset, peak and offset. We consider the displace-
ment of the corresponding facial landmarks between onset
(neutral face) and peak (expressive face). Given a set of
facial landmarks s= (x1;y1;x2;y2;:::xn;yn), where (xi;yi)
denote the coordinates of the i-th facial landmark. These
facial landmarks make up the mesh of a face, as shown in
Fig. 5.
As we can see, there are many small triangles in the face,
and each triangle is determined by three facial landmarks.
Facial muscle movements cause the deformations of the
triangles when a neutral face transforms to an expressive
face. We consider a pixel (x;y)which lies in a triangle
ABC belonging to the neutral face and the corresponding
pixel (u;v)lies in a triangle A0B0C0belonging to the
expressive face, as shown in Fig. 6. From [41], we know that
the pixel (x;y)can be expressed with a linear combination
of the three vertexes.

x
y
=
x1
y1
+1
x2 x1
y2 y1
+2
x3 x1
y3 y1
(1)
And the coefÔ¨Åcients 1;2can be obtained as
1=(x x1)(y3 y1) (y y1)(x3 x1)
(x2 x1)(y3 y1) (y2 y1)(x3 x1)(2)
2=(x2 x1)(y y1) (y2 y1)(x x1)
(x2 x1)(y3 y1) (y2 y1)(x3 x1)(3)
The point (u;v)in the triangle A0B0C0of the expres-
sive face can be deÔ¨Åned with the three vertexes and 1;2,
1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE
Transactions on Affective Computing
5
Fig. 5  
Fig. 5: Facial landmarks describe the shape of a face.
Fig. 6  
11()u ,v
22()u ,v
33()u ,v
11()x , y
33()x , y
22()x , y
A
C
B
'A
'C
'B
()x,y
()u,v
Fig. 6: A pixel (x;y)in a triangle ABC of the neutral face
is transformed to another pixel (u;v)in a triangle A0B0C0
of the expressive face.

u
v
=
u1
v1
+1
u2 u1
v2 v1
+2
u3 u1
v3 v1
(4)
Combining Eq. (2) with Eq. (3), Eq. (4) can be rewritten
as:

u
v
=
a1+a2x+a3y
a4+a5x+a6y
(5)
Each pair of triangles between the neutral face and the
expressive face can deÔ¨Åne a unique transform and each
afÔ¨Åne transform is determined by 6 parameters a1;a2;:::;a 6.
We compute the 6 parameters for each warp transform
and concatenate all the parameters as a long global feature
vector, which is used to characterize facial conÔ¨Åguration
changes. We will show by experiments that the proposed
geometric warp feature is more effective than the other
geometric features [11], [14], [38].
3.3 Acoustic Feature
Visual modalities (face images) and audio modalities
(speech) can both convey the emotions and intentions of
human beings. Audio modalities also provide some useful
clues for affect recognition in video. For instance, with voice
signal, the method [42] proposed an enhanced autocorrela-
tion (EAC) feature for emotion recognition in video.
One successful acoustic feature extraction is to obtain
the time series of multiple paralinguistic descriptors and
then using pooling operations on each time series to ex-
tract feature vectors. Schuller et al. [43] showed how to
compute the acoustic features by taking 21 functionals of 38
low level descriptors and their Ô¨Årst regression coefÔ¨Åcients.
The 38 low-level descriptors shown in Table 1 are Ô¨Årst
extracted and smoothed by simple moving average low-
pass Ô¨Åltering. After that, 21 functionals are employed and 16
zero-information features are eliminated. Finally, two single
features: the number of onsets (F0) and turn duration are
added. A total of 1,582 acoustic features are extracted fromTABLE 1: Acoustic features: 38 low level descriptors along
with their Ô¨Årst regression coefÔ¨Åcients and 21 functional-
s [43].
Descriptors Functionals
PCM loudness Position max./min.
MFCC (0-14) Arithmetic Mean
log Mel Freq. Band (0-7) skewness, kurtosis
LSP Frequency (0-7) lin. regression coeff.
F0 lin. regression error
F0 Envelope quartile
Voicing Prob. quartile range
Jitter local percentile
Jitter consec. frame pairs percentile range
Shimmer local up-level time
each video. These acoustic features include energy/spectral
Low Level Descriptors (LLD) (top 6 items in Table 1) and
voice related LLD (bottom 4 items in Table 1).
We explore the representation ability of acoustic features
for affect recognition in our study. Experiments show that
audio modalities (speech) can provide useful complemen-
tary information in addition to visual modalities. The visual
features coupled with acoustic features can achieve better
performance for facial expression recognition in the wild.
3.4 Multiple Feature Fusion
Features from different modalities can make different con-
tributions. Traditional SVM concatenates different features
into a single feature vector and built a single kernel for all
these different features. However, constructing a kernel for
each type of features and integrating these kernels optimally
can enhance the discriminative power of these features.
The study in [44] showed that using multiple kernels with
different types of features can improve the performance of
SVM. A multiple kernel SVM is designed to learn both the
decision boundaries between data from different classes and
the kernel combination weights through a single optimiza-
tion problem [45].
Given a training set with labeled samples D=
f(xi;yi)jxi2Rn;yi2 f  1;1ggN
i=1. A decision line is
obtained by solving the following primal optimization prob-
lem,
min
w;b1
2kwk2
s:t: yi(wxi+b)1;i= 1;2;:::N(6)
In general, we solve the dual form of the primal opti-
mization problem. The dual formulation of the traditional
single kernel SVM optimization problem is given by
max
2
4NX
i=1i 1
2NX
i=1NX
j=1ijyiyjKij3
5
s:t:NX
i=1iyi= 0;0iC(7)
1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE
Transactions on Affective Computing
6
whereKijis the kernel matrix, and Kij=k(xi;xj), here
k(;)is the kernel function and xi;xjare the feature vec-
tors.
Multiple kernel fusion applies a linear combination of
multiple kernels to substitute for the single kernel. In our
study, we adopt the formulation proposed in [46] in which
the kernel is actually a convex combination of basis kernels:
Kij=MX
i=1mkm(xi;xj)
s:t: m0;MX
i=1m= 1(8)
We apply a multiple kernel fusion framework to deal
with facial expression recognition under lab-controlled en-
vironment and in the wild, respectively, as shown in Fig. 1.
HOG-TOP and acoustic feature are optimally fused to han-
dle the problem of facial expression recognition in the wild,
while HOG-TOP and geometric warp feature are combined
to tackle the problem of facial expression recognition under
lab-controlled environment.
In the followings, we detail how to Ô¨Ånd an optimal
combination of HOG-TOP and acoustic feature for facial
expression recognition in the wild. It can be easily extended
to the problem of facial expression recognition under lab-
controlled environment.
We denote the dynamic texture HOG-TOP as xand
acoustic feature as z, then we have
Kij=k1(xi;xj) + (1 )k2(zi;zj) (9)
with 01, whereKis the kernel matrix, k1(;);k 2(;)
are the basis kernels. The basis kernels could be linear
kernel, radial basis function (RBF) kernel and polynomial
kernel, etc. We need to learn the kernel weight and
coefÔ¨Åcients . In our study, we construct a linear kernel for
each type of feature and build a two-step method to search
for the optimal values of and. We set two nested iterative
loops to optimize both the classiÔ¨Åer and kernel combination
weights. In the outer loop, we adopt the grid search to Ô¨Ånd
the kernel weight . In the inner iteration, a solver of SVM
(LIBSVM [47] is used in our work) is implemented by Ô¨Åxing
the kernel weight to Ô¨Ånd the coefÔ¨Åcients . Then given a
new sample which contains visual feature HOG-TOP xand
acoustic feature z, the predict label y can be obtained by
y=sgn(NX
i=1yii(k 1(xi;x) + (1 )k2(zi;z)) +b)(10)
In our work, the one-vs-one method is employed to deal
with the multiclass-SVM problem and we adopt the max-
win voting strategy to do the classiÔ¨Åcation. Finally, the 
value andvalues with the highest overall classiÔ¨Åcation
accuracy in the validation data set are obtained as the
optimal kernel weight and coefÔ¨Åcients.
4 E XPERIMENTAL RESULTS AND DISCUSSIONS
4.1 Data sets
In order to evaluate our methods, we conduct the experi-
ments on three public data sets: the Extended Cohn-Kanade(CK+) data set [11], GEMEP-FERA 2011 data set [19] and the
Acted Facial Expression in Wild (AFEW) 4.0 data set [48]. We
Ô¨Årst give a brief description of the three data sets.
TheExtended Cohn-Kanade (CK+) data set contains 593
image sequences from 123 subjects. The face images in the
database are lab-controlled. The image sequences vary in
duration from 10 to 60 frames. In total, 327 of 593 image
sequences have emotion labels and each is categorized into
one of the following seven emotion classes: anger (An), con-
tempt (Co), disgust (Di), fear (Fe), happiness (Ha), sadness
(Sa) and surprise (Su). Each image sequence changes from
the onset (the neutral frame) to the peak (the expressive
frame). In addition, the X-Y coordinates of 68 facial land-
mark points were given for each image in the database. The
landmark points of key frames within each video sequence
were manually labelled, while the remaining frames were
automatically aligned using the AAM Ô¨Åtting algorithm [41].
TheGEMEP-FERA 2011 data set contains 289 sequences
of 10 actors, who are trained by a professional director. It
is divided into a training set of 155 sequences and a test
set of 134 sequences. Each sequence is categorized into the
following Ô¨Åve emotions: anger (An), fear (Fe), happiness
(Ha), relief (Re) and sadness (Sa). Only the training set
provides emotion labels. This database is more challenging
than the CK+ database, since there are head movements and
gesture variations in image sequences.
TheActed Facial Expression in Wild (AFEW) 4.0 data
setincludes video clips collected from different movies
which are believed to be close to real world conditions.
The database splits into a training set, a validation set
and a test set. There are 578 video clips in the training
set. The validation and test sets have 383 video clips and
407 video clips, respectively. Each video clip belongs to
one of the seven categories: anger (An), disgust (Di), fear
(Fe), happiness (Ha), neutral (Ne), sadness (Sa), and sur-
prise (Su). This database provides original video clips and
aligned face sequences. They applied the model proposed
in [49] to extract the faces from video clips and aligned
the faces. Different from the CK+ and GEMEP-FERA 2011
data sets, facial expressions in AFEW 4.0 are more natural
and spontaneous. The variations in illumination, pose and
background in image sequences increase the complexity of
facial expression analysis.
Fig. 7 shows the selected image sequences from the three
databases. The Ô¨Årst row is the face images from the CK+
database, which are frontal-view and lab-controlled faces.
The middle row shows the images from the GEMP-FERA
2011 database; there exist head movements and gesture vari-
ations. The bottom row is an image sequence from AFEW
4.0 database. We can see that the background is complex
and there exist illumination changes and pose variations.
4.2 Feature Extraction
In our experiments, three types of features are employed,
namely HOG-TOP , geometric warp feature and acoustic
feature.
In extracting HOG-TOP from image sequences, each face
image is Ô¨Årst cropped and resized to 128128. The resized
face image is then partitioned into 88blocks with a size
of1616. The bin number is set to 9 with an angle range
1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE
Transactions on Affective Computing
7
Fig. 7  
Fig. 7: The selected image sequences from the three databases. From top to bottom: CK+, GEMEP-FERA2011 and AFEW
4.0.
of0 180. In each block, we can obtain a HOG-TOP with
a dimension of 39 = 27 . We then concatenate the HOG-
TOP of the 88blocks into a long feature vector with a
dimension of 3988 = 1728 .
Facial landmarks are used to compute the geometric
warp features. We compute the warp transform of facial
landmarks between the neutral face and an expressive face.
Each face contains 68 facial landmarks. These facial land-
marks divide the face into many non-overlap sub regions
by Delaunay triangulation. In our work, we take 109 pair
of triangles (the smallest number of triangles available in
face images). Each pair of triangles between the neutral
face and an expressive face can deÔ¨Åne a unique transform
and each afÔ¨Åne transform is determined by six parameters
(see Section 3.2). The warp transform coefÔ¨Åcients are Ô¨Ånally
concatenated as a feature vector of 6109 = 654 elements
to represent the geometric warp feature.
The acoustic features with a length of 1582 used in our
work are provided by the database [34], [48]. The acoustic
features are extracted by applying the open-source Emotion
Affect Recognition (openEAR) toolkit [50] backend OpenS-
MILE [51].
4.3 Experimental Results
4.3.1 A Comparison of HOG-TOP and LBP-TOP
We Ô¨Årst compare the performance of HOG-TOP proposed
in our work with LBP-TOP proposed in [10]. When we
compute the LBP-TOP features, we take the general settings
adopted in most reported works. The resized face image is
partitioned into 44blocks. The LBP-TOP is coded with a
uniform pattern. The LBP-TOP histogram of each block is a
feature vector of 359 = 177 elements. The length of the
feature vector consists of 44blocks is 35944 = 2832 .
There are 327 image sequences with emotion labels be-
longing to 118 subjects in the CK+ database. We follow the
protocol proposed in [11] and take the leave-one-subject-out
cross validation strategy. Each time the samples from one
subject are used for testing and the remaining samples from
all other subjects are used for training. In order for each
subject to be evaluated once, we carry out 118 validations.
The classiÔ¨Åcation accuracy acquired on the CK+ database by
using two types of features is shown Table 2.
We also compare the performance of the two features in
the GEMEP-FERA 2011 database. Since only the emotionTABLE 2: The classiÔ¨Åcation accuracy of LBP-TOP and HOG-
TOP on the CK+ database (%).
LBP-TOP HOG-TOP
Anger 75.6 88.9
Contempt 88.9 66.7
Disgust 93.2 94.9
Fear 80.0 76.0
Happiness 98.5 95.6
Sadness 78.6 67.9
Surprise 92.8 97.6
Overall 89.3 89.6
TABLE 3: The classiÔ¨Åcation accuracy of LBP-TOP and HOG-
TOP on the GEMEP-FERA 2011 database (%).
LBP-TOP HOG-TOP
Anger 56.2 43.7
Fear 26.7 36.7
Joy 58.1 61.3
Relief 51.6 54.8
Sad 74.2 74.2
Overall 53.6 54.2
labels of training set are publicly available, we do the
evaluation on the training set. There are seven subjects in
the training set. We adopt the leave-one-subject-out strategy
and carry out seven cross validations. Table 3 shows the
performance obtained by applying the two features.
As for the AFEW 4.0 database, we utilize the training
set to train an SVM classiÔ¨Åer and test the classiÔ¨Åer on the
validation set. The database provided a baseline method [34]
which employed LBP-TOP to represent the dynamic tex-
tures of the video sequence and trained an SVM with non-
linear RBF kernel for emotion classiÔ¨Åcation. The accuracy
acquired on the AFEW 4.0 database by applying two types
of features is shown in Table 4.
We use the overall accuracy to evaluate the performance.
1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE
Transactions on Affective Computing
8
TABLE 4: The classiÔ¨Åcation accuracy of LBP-TOP and HOG-
TOP on validation set of the AFEW 4.0 database (%).
LBP-TOP HOG-TOP
Neutral 19.0 58.7
Anger 50.0 73.4
Disgust 25.0 22.5
Fear 15.2 4.3
Happiness 57.1 60.3
Sadness 16.4 4.9
Surprise 21.7 2.2
Overall 30.6 35.8
The overall accuracy is deÔ¨Åned as
Oacc=PN
n=1PK
k=1mnkPN
n=1PK
k=1Mnk(11)
whereKis the number of classes, Nis the number of cross
validation folds, mnkis the number of correctly predicted
samples of the k-th class in the n-th fold, and Mnkdenotes
the total samples of the k-th class in the n-th fold. The
classiÔ¨Åcation rate of each individual facial expression (k -th
class) isPN
n=1mnkPN
n=1Mnk.
From the experimental results, we can see that the over-
all classiÔ¨Åcation accuracy obtained by using HOG-TOP on
the CK+ database and GEMEP-FERA 2011 database is 89.6%
and 54.2%, respectively. It is competitive with the result of
89.3% and 53.6% obtained by applying LBP-TOP on the two
databases. While the overall classiÔ¨Åcation rate of HOG-TOP
on the AFEW 4.0 database is 35.8%, which is better than
30.6% obtained by using LBP-TOP , meaning that HOG-TOP
is more robust in capturing the subtle facial appearance
changes in the wild. In addition, HOG-TOP with a length of
1728 is more compact than LBP-TOP with a length of 2832.
We further examine the conÔ¨Ådence intervals (the variances
across cross-validation folds) of two feature sets. Since each
fold in the CK+ database contains several samples only
(118 folds (subjects) all together), the variance across cross-
validation folds is very large and therefore it is not very
meaningful to report the variances for this data set. On the
other hand, the training set and validation set are Ô¨Åxed in
the AFEW 4.0 data set and therefore no variance cross the
folds for this data set can be reported. We only compare
the variances of the two feature sets on the GEMEP-FERA
2011 database. The variances of HOG-TOP and LBP-TOP are
12.7% and 12.6%, respectively, which are comparable with
each other. We also compare the computational speeds of
the two features under the 64-bit Win 7 operating system
with a Core i7 CPU. We computed the two features with
Matlab 8.2. The computation time depends on the block size
and sequence duration. With the same block size (16 16)
and sequence duration (11 frames), the computation time of
HOG-TOP and LBP-TOP is 0.027s and 0.042s, respectively,
showing the computational efÔ¨Åciency of HOG-TOP .TABLE 5: The comparison results of different geometric
features on the CK+ database (%). (GWF is our proposed
geometric warp feature).
GWF [11] [14] [38]
Anger 86.7 35.0 75.6 62.2
Contempt 94.4 25.0 ‚Äì 72.2
Disgust 96.6 68.4 88.2 86.4
Fear 36.0 21.7 76.0 56.0
Happiness 98.5 98.4 97.1 91.3
Sadness 75.0 4.0 89.3 39.3
Surprise 96.4 100.0 98.7 95.2
Overall 89.0 66.7 87.5 79.2
TABLE 6: The classiÔ¨Åcation accuracy obtained by using four
different feature sets on the CK+ database (%).
HOG-TOP Geometric
FeatureHybrid
Feature IHybrid
Feature II
Anger 88.9 86.7 95.6 100.0
Contempt 66.7 94.4 94.4 94.4
Disgust 94.9 96.6 94.9 96.6
Fear 76.0 36.0 52.0 84.0
Happiness 95.6 98.5 98.5 100.0
Sadness 67.9 75.0 78.6 78.6
Surprise 97.6 96.4 96.4 98.8
Overall 89.6 89.0 91.4 95.7
4.3.2 Facial Expression Recognition Under Lab-controlled
Environment
We develop a model which combines HOG-TOP and ge-
ometric warp to handle the problem of facial expression
recognition under lab-controlled environment. We evalu-
ate the following different feature sets: geometric warp
feature, dynamic appearance feature (HOG-TOP), hybrid
feature I and hybrid feature II. Hybrid feature I denotes the
feature vector of concatenating HOG-TOP and geometric
warp feature directly and hybrid feature II is the optimal
combination of the HOG-TOP and geometric warp feature.
We Ô¨Årst compare our proposed geometric warp feature
with the other geometric features on the CK+ data set.
All the methods take the leave-one-subject-out cross vali-
dation. The comparison results are shown in Table 5. The
method [11] applied a set of facial landmarks to character-
ize the face shape. The relative distance of eight selected
Ô¨Åducial points are measured to represent the geometric
feature in [14]. The shifts of the facial landmarks between
the neutral face and the expressive face are computed to
represent the geometric feature in [38]. We can see that
our proposed geometric warp feature achieves a superior
performance compared with the other geometric features,
meaning that the geometric warp feature is more effective
to capture facial conÔ¨Åguration changes.
1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE
Transactions on Affective Computing
9
Fig. 8 An Co Di Fe Ha Sa Su 
An 0.87  0.00  0.06  0.00  0.00  0.07  0.00  
Co 0.00  0.94  0.00  0.00  0.00  0.06  0.00  
Di 0.01  0.00  0.97  0.02  0.00  0.00  0.00  
Fe 0.00  0.00  0.04  0.36  0.28  0.04  0.28  
Ha 0.00  0.00  0.00  0.01  0.99  0.00  0.00  
Sa 0.14  0.00  0.07  0.04  0.00  0.75  0.00  
Su 0.00  0.02  0.00  0.00  0.00  0.02  0.96  An Co Di Fe Ha Sa Su 
An 0.89  0.02  0.02  0.02  0.00  0.04  0.00  
Co 0.17  0.67  0.00  0.05  0.05  0.00  0.06  
Di 0.01  0.00  0.95  0.02  0.00  0.00  0.02  
Fe 0.00  0.00  0.04  0.76  0.08  0.12  0.00  
Ha 0.00  0.00  0.00  0.01  0.96  0.00  0.03  
Sa 0.21  0.00  0.00  0.07  0.00  0.68  0.04  
Su 0.00  0.01  0.00  0.00  0.00  0.01  0.98  (a) (b) 
(a)
Fig. 8 An Co Di Fe Ha Sa Su 
An 0.87  0.00  0.06  0.00  0.00  0.07  0.00  
Co 0.00  0.94  0.00  0.00  0.00  0.06  0.00  
Di 0.01  0.00  0.97  0.02  0.00  0.00  0.00  
Fe 0.00  0.00  0.04  0.36  0.28  0.04  0.28  
Ha 0.00  0.00  0.00  0.01  0.99  0.00  0.00  
Sa 0.14  0.00  0.07  0.04  0.00  0.75  0.00  
Su 0.00  0.02  0.00  0.00  0.00  0.02  0.96  An Co Di Fe Ha Sa Su 
An 0.89  0.02  0.02  0.02  0.00  0.04  0.00  
Co 0.17  0.67  0.00  0.05  0.05  0.00  0.06  
Di 0.01  0.00  0.95  0.02  0.00  0.00  0.02  
Fe 0.00  0.00  0.04  0.76  0.08  0.12  0.00  
Ha 0.00  0.00  0.00  0.01  0.96  0.00  0.03  
Sa 0.21  0.00  0.00  0.07  0.00  0.68  0.04  
Su 0.00  0.01  0.00  0.00  0.00  0.01  0.98  (a) (b) (b)
An Co Di Fe Ha Sa Su 
An 0.96  0.00  0.04  0.00  0.00  0.00  0.00  
Co 0.00  0.94  0.00  0.00  0.00  0.00  0.06  
Di 0.03  0.00  0.95  0.00  0.02  0.00  0.00  
Fe 0.00  0.00  0.04  0.52  0.24  0.04  0.16  
Ha 0.00  0.00  0.00  0.01  0.99  0.00  0.00  
Sa 0.14  0.00  0.03  0.04  0.00  0.79  0.00  
Su 0.00  0.02  0.00  0.00  0.00  0.02  0.96  (c) 
An Co Di Fe Ha Sa Su 
An 1.00  0.00  0.00  0.00  0.00  0.00  0.00  
Co 0.00  0.94  0.00  0.00  0.00  0.00  0.06  
Di 0.01  0.00  0.97  0.00  0.00  0.02  0.00  
Fe 0.00  0.00  0.04  0.84  0.12  0.00  0.00  
Ha 0.00  0.00  0.00  0.00  1.00  0.00  0.00  
Sa 0.21  0.00  0.00  0.00  0.00  0.79  0.00  
Su 0.00  0.01  0.00  0.00  0.00  0.00  0.99  (d) 
Fig. 8 (c)
An Co Di Fe Ha Sa Su 
An 0.96  0.00  0.04  0.00  0.00  0.00  0.00  
Co 0.00  0.94  0.00  0.00  0.00  0.00  0.06  
Di 0.03  0.00  0.95  0.00  0.02  0.00  0.00  
Fe 0.00  0.00  0.04  0.52  0.24  0.04  0.16  
Ha 0.00  0.00  0.00  0.01  0.99  0.00  0.00  
Sa 0.14  0.00  0.03  0.04  0.00  0.79  0.00  
Su 0.00  0.02  0.00  0.00  0.00  0.02  0.96  (c) 
An Co Di Fe Ha Sa Su 
An 1.00  0.00  0.00  0.00  0.00  0.00  0.00  
Co 0.00  0.94  0.00  0.00  0.00  0.00  0.06  
Di 0.01  0.00  0.97  0.00  0.00  0.02  0.00  
Fe 0.00  0.00  0.04  0.84  0.12  0.00  0.00  
Ha 0.00  0.00  0.00  0.00  1.00  0.00  0.00  
Sa 0.21  0.00  0.00  0.00  0.00  0.79  0.00  
Su 0.00  0.01  0.00  0.00  0.00  0.00  0.99  (d) 
Fig. 8 (d)
Fig. 8: The confusion matrices obtained by using the four feature sets on the CK+ database: (a) HOG-TOP , (b) geometric
feature, (c) hybrid feature I and (d) hybrid feature II. (An: Anger, Co: Contempt, Di: Disgust, Fe: Fear, Ha: Happiness, Sa:
Sadness and Su: Surprise).
We further evaluate hybrid feature I and hybrid feature
II with the leave-one-subject-out cross validation on the CK+
database and compare the performance with that obtained
by applying geometric feature and HOG-TOP alone. Table 6
shows the classiÔ¨Åcation accuracy obtained by the four differ-
ent feature sets. Fig. 8 shows the confusion matrices of using
the four different feature sets. We can see that the emotions
‚Äùdisgust‚Äù, ‚Äùhappiness‚Äù and ‚Äùsurprise‚Äù have higher classiÔ¨Å-
cation rates than the other emotions, indicating that these
three emotions are easier to distinguish than the others. We
also note that hybrid feature I (91.4%) and hybrid feature II
(95.7%) outperform the geometric warp feature (89.0%) and
HOG-TOP (89.6%) applied individually. We can conclude
that different features (hybrid feature I) can provide com-
plementary information and multiple feature fusion (hybrid
feature II) can further enhance the discriminative ability of
the combined features .
We also compare our method with the other meth-
ods. All the methods we compared follow the baseline
method [11] and take the leave-one-subject-out cross vali-
dation. The methods [11], [12] combined geometric feature
and appearance feature and trained an SVM to perform the
classiÔ¨Åcation. In [21], a weighted component-based feature
descriptor to extract dynamic appearance feature was u-
tilized and multiple kernel learning was applied to train
the SVM for recognition. A sparse temporal representation
classiÔ¨Åer was proposed for facial expression recognition
in [26]. The method in [24] applied spatiotemporal local
monogenic binary pattern (STLMBP) feature to handle the
problem of facial expression recognition.
As can be seen in Table 7, the HOG-TOP (89.6%) and ge-
ometric feature (89.3%) proposed in our method can achieve
a competitive performance compared with SPTS+CAPP [11]
(88.38%), CLM [12] (82.4%) and STLMBP [24] (88.4%). It
demonstrates the effectiveness of our proposed features.
The hybrid feature II as the optimal combination of HOG-
TOP and geometric feature achieves a superior performance
compared with the other methods tested, showing the effec-
tiveness of the multiple feature fusion.
4.3.3 Facial Expression Recognition in the Wild
HOG-TOP and acoustic feature are fused to tackle the
problem of facial expression recognition in the wild. We Ô¨ÅrstTABLE 7: Performance comparison with other methods on
CK+ database.
Method Accuracy (%)
HOG-TOP 89.6
Geometric Feature 89.3
Hybrid Feature II 95.7
SPTS+CAPP [11] 88.4
CLM [12] 82.4
STLMBP [24] 88.4
STR [26] 94.9
CFD [21] 93.2
TABLE 8: The classiÔ¨Åcation accuracy obtained by using four
feature sets on validation set of the AFEW 4.0 database (%).
HOG-TOP Acoustic
FeatureHybrid
Feature IHybrid
Feature II
Neutral 58.7 57.1 65.1 69.8
Anger 73.4 64.1 75.0 76.6
Disgust 22.5 15.0 12.5 17.5
Fear 4.3 26.1 8.7 15.2
Happiness 60.3 34.9 57.1 63.5
Sadness 4.9 14.7 13.1 9.8
Surprise 2.1 0.0 4.4 2.1
Overall 35.8 32.9 37.6 40.2
evaluate our method on the validation set. Four feature sets
are explored: HOG-TOP only, acoustic feature only, hybrid
feature I and hybrid feature II. Hybrid feature I concatenates
the HOG-TOP and acoustic feature directly. Hybrid feature
II is the optimal combination of the HOG-TOP and acoustic
feature.
Table 8 shows the classiÔ¨Åcation accuracy obtained by
applying four different feature sets. The corresponding con-
fusion matrices are shown in Fig. 9. We can see that the
1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE
Transactions on Affective Computing
10
Fig. 9 An Di Fe Ha Ne Sa Su 
An 0.73  0.05  0.06  0.02  0.14  0.00  0.00  
Di 0.23  0.23  0.08  0.13  0.33  0.03  0.00  
Fe 0.52  0.11  0.04  0.11  0.17  0.04  0.00  
Ha 0.05  0.06  0.08  0.60  0.19  0.02  0.00  
Ne 0.16  0.03  0.05  0.10  0.59  0.08  0.00  
Sa 0.31  0.13  0.00  0.08  0.43  0.05  0.00  
Su 0.26  0.02  0.07  0.09  0.50  0.04  0.02  (a) (b) 
An Di Fe Ha Ne Sa Su 
An 0.64  0.03  0.05  0.13  0.11  0.02  0.03  
Di 0.18  0.15  0.00  0.18  0.35  0.10  0.05  
Fe 0.33  0.02  0.26  0.13  0.15  0.02  0.09  
Ha 0.24  0.03  0.11  0.35  0.27  0.00  0.00  
Ne 0.08  0.10  0.06  0.14  0.57  0.03  0.02  
Sa 0.02  0.15  0.11  0.23  0.30  0.15  0.05  
Su 0.17  0.09  0.13  0.17  0.35  0.09  0.00  
(a)
Fig. 9 An Di Fe Ha Ne Sa Su 
An 0.73  0.05  0.06  0.02  0.14  0.00  0.00  
Di 0.23  0.23  0.08  0.13  0.33  0.03  0.00  
Fe 0.52  0.11  0.04  0.11  0.17  0.04  0.00  
Ha 0.05  0.06  0.08  0.60  0.19  0.02  0.00  
Ne 0.16  0.03  0.05  0.10  0.59  0.08  0.00  
Sa 0.31  0.13  0.00  0.08  0.43  0.05  0.00  
Su 0.26  0.02  0.07  0.09  0.50  0.04  0.02  (a) (b) 
An Di Fe Ha Ne Sa Su 
An 0.64  0.03  0.05  0.13  0.11  0.02  0.03  
Di 0.18  0.15  0.00  0.18  0.35  0.10  0.05  
Fe 0.33  0.02  0.26  0.13  0.15  0.02  0.09  
Ha 0.24  0.03  0.11  0.35  0.27  0.00  0.00  
Ne 0.08  0.10  0.06  0.14  0.57  0.03  0.02  
Sa 0.02  0.15  0.11  0.23  0.30  0.15  0.05  
Su 0.17  0.09  0.13  0.17  0.35  0.09  0.00  (b)
(c) (d) 
Fig. 9 An Di Fe Ha Ne Sa Su 
An 0.75  0.06  0.02  0.02  0.11  0.03  0.02  
Di 0.23  0.13  0.00  0.23  0.38  0.05  0.00  
Fe 0.41  0.11  0.09  0.15  0.20  0.04  0.00  
Ha 0.11  0.03  0.06  0.57  0.19  0.02  0.02  
Ne 0.08  0.14  0.00  0.06  0.65  0.06  0.00  
Sa 0.21  0.23  0.03  0.07  0.33  0.13  0.00  
Su 0.15  0.11  0.07  0.15  0.48  0.00  0.04  An Di Fe Ha Ne Sa Su 
An 0.77  0.06  0.02  0.02  0.11  0.03  0.00  
Di 0.28  0.18  0.00  0.15  0.33  0.08  0.00  
Fe 0.39  0.09  0.15  0.15  0.20  0.02  0.00  
Ha 0.08  0.05  0.06  0.63  0.16  0.02  0.00  
Ne 0.10  0.08  0.00  0.08  0.70  0.05  0.00  
Sa 0.21  0.18  0.00  0.07  0.44  0.10  0.00  
Su 0.22  0.04  0.07  0.15  0.50  0.00  0.02  (c)
(c) (d) 
Fig. 9 An Di Fe Ha Ne Sa Su 
An 0.75  0.06  0.02  0.02  0.11  0.03  0.02  
Di 0.23  0.13  0.00  0.23  0.38  0.05  0.00  
Fe 0.41  0.11  0.09  0.15  0.20  0.04  0.00  
Ha 0.11  0.03  0.06  0.57  0.19  0.02  0.02  
Ne 0.08  0.14  0.00  0.06  0.65  0.06  0.00  
Sa 0.21  0.23  0.03  0.07  0.33  0.13  0.00  
Su 0.15  0.11  0.07  0.15  0.48  0.00  0.04  An Di Fe Ha Ne Sa Su 
An 0.77  0.06  0.02  0.02  0.11  0.03  0.00  
Di 0.28  0.18  0.00  0.15  0.33  0.08  0.00  
Fe 0.39  0.09  0.15  0.15  0.20  0.02  0.00  
Ha 0.08  0.05  0.06  0.63  0.16  0.02  0.00  
Ne 0.10  0.08  0.00  0.08  0.70  0.05  0.00  
Sa 0.21  0.18  0.00  0.07  0.44  0.10  0.00  
Su 0.22  0.04  0.07  0.15  0.50  0.00  0.02  (d)
Fig. 9: The confusion matrices obtained by using the four feature sets on the validation set of AFEW 4.0 database: (a) HOG-
TOP , (b) acoustic feature, (c) hybrid feature I and (d) hybrid feature II. (An: Anger, Di: Disgust, Fe: Fear, Ha: Happiness,
Ne: Neutral, Sa: Sadness and Su: Surprise).
TABLE 9: Performance comparison with other methods on
the test set of the AFEW 4.0 database.
Method Accuracy (%)
Hybrid Feature II (our method) 45.2
LBP-TOP + Voice [34] 33.7
Lip activity + Voice [33] 35.3
STLMBP [23] 41.5
EAC [42] 40.1
ELM [52] 44.2
classiÔ¨Åcation rates are much lower than the results shown
in Table 6. Different from facial expressions under lab-
controlled environment in which the actors or subjects can
pose distinguished facial expressions, the facial expressions
in the wild may be more subtle. The factors including head
movements, pose variations etc. also increase classiÔ¨Åcation
difÔ¨Åculties. And sometimes, several facial expressions in the
wild may appear together, which makes a facial expression
to be confused with other expressions. We observe that
the classiÔ¨Åcation rate of emotion ‚Äùsurprise‚Äù is the lowest.
From the confusion matrices shown in Fig. 9, we Ô¨Ånd
the emotion ‚Äùsurprise‚Äù is mostly misclassiÔ¨Åed as emotions
‚Äùanger‚Äù, ‚Äùhappiness‚Äù and ‚Äùneutral‚Äù. The emotions ‚Äùanger‚Äù
and ‚Äùneutral‚Äù have higher recognition accuracies than the
other emotions. Hybrid feature I and hybrid feature II
outperform the HOT-TOP and acoustic feature used indi-
vidually, indicating that two feature sets are complementary
with each other. Hybrid feature II achieves a superior perfor-
mance compared with hybrid feature I, demonstrating that
the effectiveness of the multiple feature fusion in dealing
with the facial expression recognition problem in the wild.
We further apply hybrid feature II which achieves the
best performance on the validation set (the kernel weights
of HOG-TOP and acoustic feature are 0.73 and 0.27) to
evaluate the test set. The overall recognition accuracy on
the test set is 45.2%. Table 9 shows the results compared
with the other methods. The baseline method [34] combined
LBP-TOP and the acoustic feature. Lip activity was incorpo-
rated with voice in [33] to tackle the emotion recognition
Fig. 8  35.8 
32.9 37.6 40.2 
HOG-TOP Audio Feature Decision-level
FusionFeature-level
FusionOverall Accuracy (%)  
35.8  
32.9  37.6  40.2  
HOG-TOP Audio Feature Decision-level
FusionFeature-level
Fusion (hybrid
feautre II)Overall Accuracy (%)  Fig. 10: The comparison results of different methods on the
validation set of AFEW 4.0 database.
problem. The method [23] used dynamic textures only and
the method [42] applied the voice only. The method [35]
employed audiovisual feature for emotion recognition. We
can see that our method (45.2%) improves signiÔ¨Åcantly com-
pared with the baseline method [34] and the method [33],
with an improvement of about 11% and 10%, respectively.
Our method is also better than [23] (41.5%) and EAC [42]
(40.1%). Compared with the method [52] (44.2%), our per-
formance is still competitive. Moreover, we applied our
method to participate in the second emotion recognition
in the wild challenge (EmotiW 2014) [34] and achieved the
second runner-up award.
4.3.4 Decision-level fusion vs Feature-level fusion
The multiple feature fusion applied in our work is a kind
of feature-level fusion method. Another technique, namely
decision-level fusion, is also widely used in computer vision
community to deal with multiple sets of features. In a
preliminary study, we explore the effectiveness of the two
techniques for facial expression recognition in video. A pre-
liminary experiment is conducted on AFEW 4.0 database.
As we have mentioned above, we employ the one-vs-one
technique to tackle the multiclass-SVM problem, and use
max-win voting strategy to conduct the classiÔ¨Åcation. For
decision-level fusion, we Ô¨Årst apply the HOG-TOP and a-
coustic feature separately and then saved the predict results,
1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE
Transactions on Affective Computing
11
TABLE 10: The parameters used for extracting HOG-TOP .
Image Size Block Size Overlap Blocks
9696 12 12 No 88
9696 24 24 Half 77
128128 16 16 No 88
128128 32 32 Half 77
i.e. the number of votes for each class of the two features,
respectively. After that, we add the votes obtained by each
individual feature together and based on the combined
votes, we carry out the max-win voting strategy again
to make the Ô¨Ånal decision. The overall classiÔ¨Åcation rate
is computed as the performance of decision-level fusion
method.
Fig. 10 shows the results of the different methods tested.
The overall accuracy of HOG-TOP , acoustic feature and
feature-level fusion (hybrid feature II) shown in Fig. 10 is the
same as shown in Table 8. From the experimental results, we
Ô¨Ånd that feature-level fusion outperforms the decision-level
fusion method, although they both utilize the same multiple
sets of features. We also Ô¨Ånd that the improvement acquired
by decision-level fusion over individual features sets is not
as signiÔ¨Åcant as that achieved by feature-level fusion.
4.3.5 The Effect of Block Size on HOG-TOP
We further explore the representation ability of HOG-TOP
with different block sizes, from 1212to3232. Table
10 shows the parameters used for extracting HOG-TOP . The
blocks with a small size (12 and 16) are not overlapped and
large blocks (2424and3232) are half overlapped. We
employ HOG-TOP on the CK+ database and the AFEW 4.0
database. Experiment results are shown in Tables 11 and 12.
We can see that the HOG-TOP with various block sizes
achieve the similar overall accuracy. We can therefore con-
clude that HOG-TOP is robust to scales. We can further
examine the experimental results. For facial expressions
under lab-controlled environment (Table 11), HOG-TOP
with a small size (12) is more effective to recognize the
facial expressions ‚Äùfear‚Äù and ‚Äùcontempt‚Äù which have subtle
facial muscle activities. A small block size is more robust to
capture local subtle appearance changes than a large block
size. For facial expressions in the wild (Table 12), HOG-TOP
with a large size (24 and 32) achieves a superior performance
for facial expression ‚Äùsurprise‚Äù, indicating that HOG-TOP
with a large block size is more robust to distinguish this
expression from others in the wild. Table 12 also shows that
HOG-TOP with various block sizes outperforms the LBP-
TOP (30.6%) for facial expression recognition in the wild.
4.4 Discussion
From the experimental results reported above, we can see
that our proposed framework can efÔ¨Åciently handle the
problem of facial expression recognition in video. Facial
expressions under lab-controlled environment are different
from those in the wild which are more natural and spon-
taneous. We propose two approaches to tackle the two
different facial expression recognition problems. The twoTABLE 11: The performance of HOG-TOP with various
block sizes on the CK+ database (%).
1212 24 24 16 16 32 32
Anger 84.4 80.0 88.9 84.4
Contempt 72.2 66.7 66.7 66.7
Disgust 93.2 93.2 94.9 96.6
Fear 88.0 80.0 76.0 72.0
Happiness 95.7 95.7 95.7 97.1
Sadness 64.3 64.3 67.9 60.7
Surprise 96.4 96.4 97.6 97.6
Overall 89.3 87.8 89.6 88.7
TABLE 12: The performance of HOG-TOP with various
block sizes on the validation set of AFEW 4.0 database (%).
1212 24 24 16 16 32 32
Neutral 54.0 38.1 58.7 46.0
Anger 71.9 71.9 73.4 73.4
Disgust 20.0 15.0 22.5 20.0
Fear 6.50 15.2 4.30 13.0
Happiness 57.1 63.5 60.3 60.3
Sadness 4.90 9.80 4.90 9.80
Surprise 2.20 13.0 2.20 8.70
Overall 34.2 35.2 35.8 36.0
approaches both apply HOG-TOP , indicating that facial ap-
pearance plays an important role for both facial expression
recognition problems. Compared with LBP-TOP , HOG-TOP
is more compact and effective to characterize facial appear-
ance changes. Facial conÔ¨Åguration changes also provide use-
ful clues for facial expression analysis. The facial landmarks
can be located exactly on a face image under lab-controlled
environment, representing the facial conÔ¨Åguration changes
caused by facial muscle movements. We propose a new
effective geometric feature based on warp transform of
facial landmarks and the proposed geometric warp feature
is robust to capture facial conÔ¨Åguration changes. On the
other hand, it is very challenging to locate facial landmarks
on face images in the wild. However, the speech also plays
an important role on affect recognition. Instead of using
geometric feature, acoustic feature is employed for facial
expression recognition in the wild. Experimental results
show that different features can make different contribution-
s to facial expression recognition and the multiple feature
fusion can enhance the discriminative ability of the multiple
features. We also note that for facial expression recognition
in the wild, although our method outperforms the baseline
method, the performance is in general not as good as that
in facial expression recognition under lab-controlled envi-
ronment. Facial expression recognition in the wild is much
more challenging and it will be one of our future research
focuses.
1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE
Transactions on Affective Computing
12
5 C ONCLUSION
Video based facial expression recognition is a challenging
and long standing problem. In this paper, we exploit the
potentials of audiovisual modalities and propose an effec-
tive framework with multiple feature fusion to handle this
problem. Both the visual modalities (face images) and audio
modalities (speech) are utilized in our study. A new feature
descriptor called Histogram of Oriented Gradients from
Three Orthogonal Planes (HOG-TOP) is proposed to extract
dynamic textures from video sequences to characterize fa-
cial appearance changes. Experiments conducted on three
public databases (CK+, GEMEP-FERA 2011, AFEW4.0) have
shown that HOG-TOP performs as well as a widely used
feature LBP-TOP in representing dynamic textures from
video sequences. Moreover, HOG-TOP is more effective
to capture subtle facial appearance changes and robust in
dealing with facial expression recognition in the wild. In
addition, HOG-TOP is more compact. In order to capture
facial conÔ¨Ågure changes, we introduce an effective geomet-
ric feature deriving from the warp transform of the facial
landmarks. Realizing that voice is another powerful way
for human beings to transmit message, we also explore
the role of speech and employ the acoustic feature for
affect recognition in video. We applied the multiple feature
fusion to deal with facial expression recognition under lab-
controlled environment and in the wild. Experiments con-
ducted on two facial expression datasets, CK+ and AFEW
4.0, demonstrate that our approach can achieve a promising
performance in facial expression recognition in video.
ACKNOWLEDGMENTS
The work reported in this paper was partially supported
by a research grant from the Natural Science Foundation of
China (NSFC) grant (Project Code: 61473243).
REFERENCES
[1] R. A. Calvo and S. D‚ÄôMello, ‚ÄùAffect Detection An Interdisci-
plinary Review of Models, Methods, and Their Applications,‚Äù
IEEE Transactions on Affective Computing, vol. 1, pp. 18-37, 2010.
[2] Y. l. Tian, T. Kanade, and J. F. Cohn, ‚ÄùRecognizing action units for
facial expression analysis,‚Äù IEEE Transactions on Pattern Analysis
and Machine Intelligence, vol. 23, pp. 97-115, 2001.
[3] K. Scherer and P . Ekman, ‚ÄùHandbook of Methods in Nonverbal
Behavior Research,‚Äù UK: Cambridge Univ. Press, 1982.
[4] J. F. Cohn and P . Ekman, ‚ÄùMeasuring facial action,‚Äù 2005.
[5] P . Ekman and W. V . Friesen, ‚ÄùFacial Action Coding System: A
Technique for the Measurement of Facial Movement,‚Äù Consulting
Psychologists Press, 1978.
[6] P . Ekman, W. V . Friesen, and J. C. Hager, ‚ÄùFacial Action Coding
System: The Manual on CD ROM. A Human Face,‚Äù 2002.
[7] P . Ekman, ‚ÄùAn argument for basic emotions,‚Äù Cognition & Emo-
tion, vol. 6, pp. 169-200, 1992.
[8] S. Z. Li and A. K. Jain, ‚ÄùHandbook of face recognition,‚Äù springer,
2011.
[9] N. Dalal and B. Triggs, ‚ÄùHistograms of Oriented Gradients for
Human Detection,‚Äù IEEE Conference on Computer Vision and Pat-
tern Recognition, 2005, pp. 886-893.
[10] G. Zhao and M. Pietikainen, ‚ÄùDynamic texture recognition using
local binary patterns with an application to facial expressions,‚Äù
IEEE Transactions on Pattern Analysis and Machine Intelligence, vol.
29, pp. 915-928, 2007.
[11] P . Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and
I. Matthews, ‚ÄùThe Extended Cohn-Kanade Dataset (CK+) A
complete dataset for action unit and emotion-speciÔ¨Åed expres-
sion,‚Äù IEEE Conference on Computer Vision and Pattern Recognition
Workshops (CVPRW), 2010, pp. 94-101.[12] S. W. Chew, P . Lucey, S. Lucey, J. Saragih, J. F. Cohn, and
S. Sridharan, ‚ÄùPerson-independent facial expression detection
using constrained local models,‚Äù IEEE International Conference on
Automatic Face & Gesture Recognition and Workshops, 2011, pp. 915-
920.
[13] S. Taheri, P . Turaga, and R. Chellappa, ‚ÄùTowards view-invariant
expression analysis using analytic shape manifolds,‚Äù IEEE Inter-
national Conference on Automatic Face & Gesture Recognition and
Workshops, 2011, pp. 306-313.
[14] A. Saeed, A. Al Hamadi, R. Niese, and M. Elzobi, ‚ÄùEffective
geometric features for human emotion recognition,‚Äù IEEE 11th
International Conference on Signal Processing (ICSP), 2012, pp. 623-
627.
[15] K. Sikka, T. Wu, J. Susskind, and M. Bartlett, ‚ÄùExploring bag of
words architectures in the facial expression domain,‚Äù in Computer
Vision-ECCV Workshops and Demonstrations, 2012, pp. 250-259.
[16] Y. Rahulamathavan, R. C. W. Phan, J. A. Chambers, and
D. J. Parish, ‚ÄùFacial Expression Recognition in the Encrypted
Domain Based on Local Fisher Discriminant Analysis,‚Äù IEEE
Transactions on Affective Computing, vol. 4, pp. 83-92, 2013.
[17] L. Zhang and D. Tjondronegoro, ‚ÄùFacial expression recognition
using facial movement features,‚Äù IEEE Transactions on Affective
Computing, vol. 2, pp. 219-229, 2011.
[18] S. Happy and A. Routray, ‚ÄùAutomatic facial expression recogni-
tion using features of salient facial patches,‚Äù IEEE Transactions on
Affective Computing, vol. 6, pp. 1-12, 2015.
[19] M. F. Valstar, B. Jiang, M. Mehu, M. Pantic, and K. Scherer, ‚ÄùThe
Ô¨Årst facial expression recognition and analysis challenge,‚Äù IEEE
International Conference on Automatic Face & Gesture Recognition
and Workshops, 2011, pp. 921-926.
[20] A. Dhall, A. Asthana, R. Goecke, and T. Gedeon, ‚ÄùEmotion
recognition using PHOG and LPQ features,‚Äù IEEE International
Conference on Automatic Face & Gesture Recognition and Workshops,
2011, pp. 878-883.
[21] X. Huang, G. Zhao, M. Pietikainen, and W. Zheng, ‚ÄùExpression
Recognition in Videos Using a Weighted Component-Based Fea-
ture Descriptor,‚Äù in Proceedings of the 17th Scandinavian conference
on Image analysis, 2011, pp. 569-578.
[22] T. R. Almaev and M. F. Valstar, ‚ÄùLocal Gabor Binary Patterns
from Three Orthogonal Planes for Automatic Facial Expression
Recognition,‚Äù in Affective Computing and Intelligent Interaction
(ACII), 2013, pp. 356-361.
[23] X. Huang, Q. He, X. Hong, G. Zhao, and M. Pietikainen, ‚ÄùIm-
proved Spatiotemporal Local Monogenic Binary Pattern for Emo-
tion Recognition in The Wild,‚Äù in ACM International Conference on
Multimodal Interaction, 2014, pp. 514-520.
[24] X. Huang, G. Zhao, W. Zheng, and M. Pietikainen, ‚ÄùSpatio
temporal Local Monogenic Binary Patterns for Facial Expression
Recognition,‚Äù IEEE Signal Processing Letters, vol. 19, pp. 243-246,
2012.
[25] F. Long, T. Wu, J. R. Movellan, M. S. Bartlett, and G. Littlewort,
‚ÄùLearning spatiotemporal features by using independent compo-
nent analysis with application to facial expression recognition,‚Äù
Neurocomputing, vol. 93, pp. 126-132, 2012.
[26] S. W. Chew, R. Rana, P . Lucey, S. Lucey, and S. Sridharan, ‚ÄùSparse
Temporal Representations for Facial Expression Recognition,‚Äù in
Advances in Image and Video Technology, 2012, pp. 311-322.
[27] Y. Li, S. Wang, Y. Zhao, and Q. Ji, ‚ÄùSimultaneous Facial Feature
Tracking and Facial Expression Recognition,‚Äù IEEE Transactions
on Image Processing, vol. 22, pp. 2559-2573, 2013.
[28] J. Chen, Z. Chen, Z. Chi, and H. Fu, ‚ÄùEmotion Recognition in
the Wild with Feature Fusion and Multiple Kernel Learning,‚Äù in
ACM International Conference on Multimodal Interaction, 2014, pp.
508-513.
[29] S. E. Kanou, C. Pal, X. Bouthillier, P . Froumenty, ?. Gl?ehre and
R. Memisevic, et al., ‚ÄùCombining modality speciÔ¨Åc deep neural
networks for emotion recognition in video,‚Äù in Proceedings of
the 15th ACM on International conference on multimodal interaction,
2013, pp. 543-550.
[30] M. Liu, R. Wang, S. Li, S. Shan, Z. Huang, and X. Chen, ‚ÄùCom-
bining Multiple Kernel Methods on Riemannian Manifold for E-
motion Recognition in the Wild,‚Äù in ACM International Conference
on Multimodal Interaction, 2014, pp. 494-501.
[31] Y. Kim, H. Lee, and E. M. Provost, ‚ÄùDeep learning for robust
feature generation in audiovisual emotion recognition,‚Äù in IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2013, pp. 3687-3691.
1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE
Transactions on Affective Computing
13
[32] Z. Zeng, M. Pantic, G. I. Roisman, and T. S. Huang, ‚ÄùA Survey
of Affect Recognition Methods Audio, Visual, and Spontaneous
Expressions,‚Äù IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 31, pp. 39-58, 2009.
[33] F. Ringeval, S. Amiriparian, F. Eyben, K. Scherer, and B. Schuller,
‚ÄùEmotion Recognition in the Wild Incorporating Voice and Lip
Activity in Multimodal Decision-Level Fusion,‚Äù in ACM Interna-
tional Conference on Multimodal Interaction, 2014, pp. 473-480.
[34] A. Dhall, R. Goecke, J. Joshi, K. Sikka, and T. Gedeon, ‚ÄùEmotion
Recognition In The Wild Challenge 2014: Baseline, Data and Pro-
tocol,‚Äù in ACM International Conference on Multimodal Interaction,
2014, pp. 461-466.
[35] B. Sun, L. Li, T. Zuo, Y. Chen, G. Zhou, and X. Wu, ‚ÄùCombining
Multimodal Features with Hierarchical ClassiÔ¨Åer Fusion for E-
motion Recognition in the Wild,‚Äù in ACM International Conference
on Multimodal Interaction, 2014, pp. 481-486.
[36] K. Sikka, K. Dykstra, S. Sathyanarayana, G. Littlewort, and
M. Bartlett, ‚ÄùMultiple kernel learning for emotion recognition in
the wild,‚Äù in Proceedings of the 15th ACM on International conference
on multimodal interaction, 2013, pp. 517-524.
[37] A. Martinez and S. Du, ‚ÄùA model of the perception of facial
expressions of emotion by humans Research overview and
perspectives,‚Äù The Journal of Machine Learning Research, vol. 13,
pp. 1589-1608, 2012.
[38] J. Chen, Z. Chen, Z. Chi, and H. Fu, ‚ÄùDynamic texture and
geometry features for facial expression recognition in video,‚Äù in
IEEE International Conference on Image Processing (ICIP), 2015, pp.
4967-4971.
[39] T. Gritti, C. Shan, V . Jeanne, and R. Braspenning, ‚ÄùLocal features
based facial expression recognition with face registration errors,‚Äù
in8th IEEE International Conference on Automatic Face & Gesture
Recognition, 2008, pp. 1-8.
[40] T. Ojala, M. Pietikainen, and T. Maenpaa, ‚ÄùMultiresolution gray-
scale and rotation invariant texture classiÔ¨Åcation with local bi-
nary patterns,‚Äù IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 24, pp. 971-987, 2002.
[41] I. Matthews and S. Baker, ‚ÄùActive appearance models revisited,‚Äù
International Journal of Computer Vision, vol. 60, pp. 135-164, 2004.
[42] S. Meudt and F. Schwenker, ‚ÄùEnhanced Autocorrelation in Real
World Emotion Recognition,‚Äù in ACM International Conference on
Multimodal Interaction, 2014, pp. 502-507.
[43] B. Schuller, S. Steidl, A. Batliner, F. Burkhardt, L. Devillers and
C. A. M uller, et al., ‚ÄùThe INTERSPEECH 2010 paralinguistic
challenge,‚Äù in INTERSPEECH, 2010, pp. 2794-2797.
[44] M. G onen and E. Alpaydin, ‚ÄùMultiple Kernel Learning Algo-
rithms,‚Äù The Journal of Machine Learning Research, vol. 12, pp. 2211-
2268, 2011.
[45] G. R. Lanckriet, N. Cristianini, P . Bartlett, L. E. Ghaoui, and
M. I. Jordan, ‚ÄùLearning the Kernel Matrix with Semi-DeÔ¨Ånite
Programming,‚Äù The Journal of Machine Learning Research, vol. 5,
pp. 27-72, 2004.
[46] A. Rakotomamonjy, F. R. Bach, S. Canu, and Y. Grandvalet,
‚ÄùSimpleMKL,‚Äù Journal of Machine Learning Research, vol. 9, pp.
2491-2521, 2008.
[47] C.-C. Chang and C.-J. Lin, ‚ÄùLIBSVM: a library for support vector
machines,‚Äù ACM Transactions on Intelligent Systems and Technology
(TIST) ,vol. 2, 2011.
[48] Abhinav Dhall, Roland Goecke, Simon Lucey, and T. Gedeon, ‚ÄùA
semi-automatic method for collecting richly labelled large facial
expression databases from movies,‚Äù IEEE Multimedia, 2012.
[49] X. Zhu and D. Ramanan, ‚ÄùFace detection, pose estimation and
landmark localization in the wild,‚Äù in IEEE Conference on Com-
puter Vision and Pattern Recognition , 2012, pp. 2879-2886.
[50] F. Eyben, M. Wollmer, and B. Schuller, ‚ÄùOpenEAR - Introducing
the munich open-source emotion and affect recognition toolkit,‚Äù
in3rd International Conference onAffective Computing and Intelligent
Interaction and Workshops, 2009, pp. 1-6.
[51] Florian Eyben, Martin W ollmer, and B. Schuller, ‚ÄùOpensmile: the
munich versatile and fast open-source audio feature extractor,‚Äù in
Proceedings of the 18th ACM international conference on Multimedia,
2010, pp. 1459-1462.
[52] H. Kaya and A. A. Salah, ‚ÄùCombining Modality-SpeciÔ¨Åc Extreme
Learning Machines for Emotion Recognition in the Wild,‚Äù in
ACM International Conference on Multimodal Interaction, 2014, pp.
487-493.Junkai Chen received his Bachelor and Master degrees from North-
western Polytechnic University and Xi‚Äôan Jiaotong University in 2010
and 2013, respectively. He is currently a Ph.D. candidate with Depart-
ment of Electronic and Information Engineering, The Hong Kong Poly-
technic University. His research interests include pattern recognition,
computer vision and machine learning.
Zenghai Chen received his B.Eng. in Department of Electronics and
Communication Engineering from Sun Y at-set University, Guangzhou in
2009, and Ph.D. degree in Department of Electronic and Information
Engineering from The Hong Kong Polytechnic University, Hong Kong
in 2014. He is now a postdoctoral fellow in School of Electrical and
Electronic Engineering, Nanyang Technological University, Singapore.
His research interests include pattern recognition, computer vision and
machine learning.
Zheru Chi received his B.Eng. and M.Eng. degrees from Zhejiang
University in 1982 and 1985, respectively, and his Ph.D. degree from
the University of Sydney in March 1994, all in electrical engineering.
Between 1985 and 1989, he was on the Faculty of the Department
of ScientiÔ¨Åc Instruments at Zhejiang University. He worked as a Se-
nior Research Assistant/Research Fellow in the Laboratory for Imaging
Science and Engineering at the University of Sydney from April 1993
to January 1995. Since February 1995, he has been with the Hong
Kong Polytechnic University, where he is now an Associate Professor
in the Department of Electronic and Information Engineering. Since
1997, he has served on the organization or program committees for
a number of international conferences. He was an associate editor for
IEEE Transactions on Fuzzy Systems between 2008 and 2010, and is
currently an editor for International Journal of Information Acquisition.
His research interests include image processing, pattern recognition,
and computational intelligence. Dr Chi has authored/co-authored one
book and 11 book chapters, and published more than 190 technical
papers.
Hong Fu received her Bachelor and Master degrees from Xian Jiao tong
University in 2000 and 2003, and Ph.D. degree from the Hong Kong
Polytechnic University in 2007. She is now an Associate Professor in
Department of Computer Science, Chu Hai College of Higher Educa-
tion, Hong Kong. Her research interests include eye tracking, computer
vision, pattern recognition, and artiÔ¨Åcial intelligence.
"
https://ieeexplore.ieee.org/document/8119447,"Facial Expression Recognition Using Deep
Convolutional Neural Networks
Dinh Viet Sang
Hanoi University of Science
and Technology
Email: sangdv@soict.hust.edu.vnNguyen Van Dat
Hanoi University of Science
and Technology
Email: datnguyenvan844@gmail.comDo Phan Thuan
Hanoi University of Science
and Technology
Email: thuandp@soict.hust.edu.vn
Abstract ‚ÄîFacial expressions convey non-verbal information
between humans in face-to-face interactions. Automatic facial
expression recognition, which plays a vital role in human-machine
interfaces, has attracted increasing attention from researchers
since the early nineties. Classical machine learning approaches
often require a complex feature extraction process and produce
poor results. In this paper, we apply recent advances in deep
learning to propose effective deep Convolutional Neural Networks
(CNNs) that can accurately interpret semantic information avail-
able in faces in an automated manner without hand-designing
of features descriptors. We also apply different loss functions
and training tricks in order to learn CNNs with a strong
classiÔ¨Åcation power. The experimental results show that our
proposed networks outperform state-of-the-art methods on the
well-known FERC-2013 dataset provided on the Kaggle facial
expression recognition competition. In comparison to the winning
model of this competition, the number of parameters in our
proposed networks intensively decreases, that accelerates the
overall performance speed and makes the proposed networks
well suitable for real-time systems.
I. I NTRODUCTION
Ever since computers were invented, people have wanted
to build artiÔ¨Åcially intelligent (AI) systems that are mentally
and/or physically equivalent to humans. In the past decades,
the increase of generally available computational power pro-
vided a helping hand for developing fast learning machines,
whereas the Internet supplied an enormous amount of data
for training. Among a lot of advanced machine learning
techniques that have been developed so far, deep learning is
widely considered as one of the most promising techniques to
make AI machines approaching human-level intelligence.
Facial expression recognition is the process of identifying
human emotion based on facial expressions. Humans are natu-
rally capable of recognizing emotions. In fact, children, which
are only 36 hours old, can interpret some very basic emotions
from faces. In older humans, this ability is considered one
of the most important social skills. There is a universality in
facial expressions of humans in expressing certain emotions.
Human develop similar muscular movements belonging to
a certain mental state, despite their place of birth, race,
education, etcetera. Therefore, if properly being modelled, this
universality can be a convenient feature in human-machine
interaction: a well trained system can understand emotions,
independent of who the subject is.Automated facial expression recognition has numerous prac-
tical applications such as psychological analysis, medical
diagnosis, forensics (lie-detection), studying effectiveness of
advertisement and so on. The ability to read facial expressions
and then recognize human emotions provides a new dimension
to human-machine interactions, for instance, smile detector
in commercial digital cameras or interactive advertisements.
Robots can also beneÔ¨Åt from automated facial expression
recognition. If robots can predict human emotions, they can
react upon this and have appropriate behaviors.
In this paper, we adopt deep learning technique and propose
effective architectures of Convolutional Neural Networks to
solve the problem of facial expression recognition. We also
apply different loss functions associated with supervised learn-
ing and several training tricks in order to learn CNNs with a
strong discriminative power. We show that Multiclass SVM
loss works better than cross-entropy loss (combining with
softmax function) in facial expression recognition. Besides, the
evaluation of the test sets using multiple crops with different
scales and rotations can yield an accuracy boost compared to
single crop evaluation.
Facial expression recognition competition FERC-2013 [1]
was held in 2013 by Kaggle. The winner is RBM team [14]
with the accuracy of 69.4% on public test set and 71.2% on
private test set. To the best of our knowledge, this is the state-
of-the-art on the FERC-2013 dataset so far. The experiments
show that our proposed method achieves better accuracy than
their results on both two test sets.
The rest of the paper is organized as follows. In section II
we brieÔ¨Çy summarize some related work on facial expression
recognition. In section III we describe our proposed CNN
architectures. Our experiments and evaluation are shown in
section IV . The conclusion is in section V with some discus-
sion for the future work.
II. R ELATED WORK
Classical approaches for facial expression recognition are
often based on Facial Action Coding System (FACS) [3],
which involves identifying various facial muscles causing
changes in facial appearance. It includes a list of Action
Units (AUs). Cootes et al. [15] propose a model based on an
approach called the Active Appearance Model [2]. Given input2017 9th International Conference on Knowledge and Systems Engineering(KSE)
978-1-5386-3576-6/17/$31.00 c2017 IEEE 130
image, preprocessing steps are performed to create over 500
facial landmarks. From these landmarks, the authors perform
PCA algorithm [12] and derive Action Units (AUs). Finally,
they classify facial expressions using a single layered neural
network.
With the fast growth of deep learning, the state-of-the-art in
many computer vision tasks has been considerably improved.
In image classiÔ¨Åcation, there are some well-known deep CNNs
can be mentioned as follows. The Ô¨Årst network we want to
mention is AlexNet [8], the winner of ImageNet ILSVRC
challenge in 2012. This network has a very similar architecture
to LeNet [10], but is deeper and bigger, and its convolutional
layers stack on top of each other. Previously it is common
to have only a single convolutional layer followed by a pool
layer. The next CNNs are called VGGNet [11], the runner-
up in ILSVRC 2014. One important property of VGGNet is
that there are many convolutional layers with small Ô¨Ålter size
3√ó3that stack on top of each other instead of using a single
convolutional layer with larger Ô¨Ålter size as in previous CNN
generations. The winner of ILSVRC 2015 is ResNet [5] which
can be characterized by skip connections and heavy use of
batch normalization [6]. Recent improved variants of ResNet
called Wide ResNet [17] or ResNeXt [16] also demonstrate
their impressive power in image classiÔ¨Åcation tasks.
Deep learning allows us to extract facial features in an
automated manner without requiring manual design of feature
descriptors. There have been some studies that employ CNNs
to address the problem of facial expression recognition. Gudi
et al. [4] propose a CNN model to recognize gender, race,
age and emotion from facial images. The network includes 3
convolutional layers (with large Ô¨Ålter size), 2 fully connected
layers, where the Ô¨Årst has 3072 neurons and the second is
the output layer with 7 neurons. There is only one maxpool
layer after the Ô¨Årst convolutional layer. The softmax activation
function is applied to output layer and cross-entropy loss
function is used in training process. In the task of emotion
recognition from faces, Tang and his RBM team [14] set
the state-of-the-art on the Kaggle FERC-2013 dataset. Their
network is pretty similar with Gudi‚Äôs architecture [4]. The
difference is that Tang et al. use multi-class SVM loss instead
of cross-entropy loss, and exploit augmentation techniques to
create more data for training.
III. O UR PROPOSED NETWORK ARCHITECTURES
In this section, we will propose several different CNN
architectures for facial expression recognition problem. In
all architectures, the input image size is Ô¨Åxed to 42x42x1.
Architectures are composed of convolution layers, pooling
layers, and fully connected layers. After each convolutional
layer and fullly connected layer (except the output layer),
the ReLU [8] activation function is applied. The output layer
consists of 7 neurons corresponding to 7 emotional labels:
angry, disgust, fear, happy, sad, surprise and neutral. It is
possible to optionally use a softmax layer right after the output
layer if one wants to use the cross-entropy loss function in thetraining phase. However, if the multi-class SVM loss function
is used in the training phase, the softmax layer can be omitted.
A. The BKStart architecture
Firstly, we try to reconstruct the winning model of the
Kaggle facial expression competition proposed by Tang et al.
in [14] to conduct some experiments with its modiÔ¨Åcations.
Since the details of the winning model was not fully described
in [14], so we try to make the reconstructed model, called
BKStart, as close to the origin as possible.
TABLE I: BKStart architecture
Input: 42x42x1
Conv2d: 5x5x32, stride: 1 + ReLU
Max Pooling: 3x3, stride: 2
Conv2d: 4x4x32, stride: 1 + ReLU
Average Pooling: 3x3, stride: 2
Conv2d: 5x5x64, stride: 1 + ReLU
Average Pooling: 3x3, stride: 2
FC: 3072 + ReLU
FC: 7
The BKStart architecture is described in the table I. This
architecture consists of 3 convolutional layers with (size,
number of Ô¨Ålters) is ( 5√ó5, 32), ( 4√ó4, 32), ( 5√ó5, 64)
respectively and the stride is 1. The Ô¨Årst pooling layer is a max
pooling layer with a Ô¨Ålter size of 3√ó3and the stride is 2. The
next two pooling layers are the average pooling layers with a
Ô¨Ålter size of 3√ó3and the stride is 2. The next one is a fully
connected layer with 3072 neurons. Passing this layer, from
each input image we obtain a 3072-d vector, which is a high-
level feature descriptor representing the input image. Finally,
this vector is passed to the last fully connected layer with 7
neurons, which, in turn, outputs a 7-d vector s= (s1,s2,..,s 7)
representing class scores towards 7 kinds of emotions.
When the multi-class SVM loss is used in the training
phase, the class label corresponding to the highest score is
immediately chosen as the Ô¨Ånal answer. In other case, when
the cross-entropy loss is used in the training phase, the softmax
activation function is then applied to the score vector s, and
it in turn outputs a new 7-d vector u= (u1,u2,...,u 7), that
can be deÔ¨Åned as follows:
ui=esi
/summationtext7
k=1esk,i= 1,2,..., 7. (1)
Thus, we Ô¨Ånally obtain the vector of 7 elements that
represents the probability distribution over 7 emotion classes.
The class label corresponding to the highest probability will
be then chosen as the Ô¨Ånal answer.
B. Our proposed VGG-like CNNs
As mentioned above, VGG is the runner-up in ILSVRC
2014. VGG is characterized by its simplicity, using only small
convolutional layers stacked on top of each other in increasing
depth. Unlike conventional CNNs, VGG consists of blocks,
each of which contain one to four convolutional layers. The
convolutional layers often have a very small Ô¨Ålter size of 3√ó3,131
and the stride is 1. The convolutional layers in the same block
have the same number of Ô¨Ålters. And the number of Ô¨Ålters in
each convolutional layer in this block is double or equal to the
number of Ô¨Ålters in each convolutional layer of the previous
block. Each following block is a max pooling layer which
often has a Ô¨Ålter size of 2√ó2with the stride 2.
Inspired by the simple designing idea of VGG, we propose
different effective CNNs to tackle the problem of facial
expression recognition. Since FERC-2013 dataset is much
smaller than ImageNet dataset, we propose some changes to
avoid overÔ¨Åtting phenomenon. In fact, we strongly reduce the
number of Ô¨Ålters in all convolutional and fullly connected
layers. Our four proposed architectures, called BKVGG8,
BKVGG10, BKVGG12 and BKVGG14, are shown in Table II,
one per column. All of these architectures follow the general
designing principles of VGG. They differ from each other
only in depth: from 8 layers in BKVGG8 (5 convolutional
and 3 fully connected layers) up to 14 layers in BKVGG14
(11 convolutional and 3 fully connected layers).
For example, BKVGG12 is described in more details as
follows. Overall, BKVGG12 consists of four blocks. The Ô¨Årst
block has two 3√ó3convolutional layers, the number of
Ô¨Ålters is 32, the stride is 1. The second block has two 3√ó3
convolutional layers, the number of Ô¨Ålters is 64, the stride is 1.
The third block has two 3√ó3convolutional layers, the number
of Ô¨Ålters is 64, the stride is 1. The fourth block has three 3√ó3
convolutional layers, the number of Ô¨Ålters is 64, the stride is
1. After each block except the last one, there is a max pooling
layer with Ô¨Ålter size 2√ó2and the stride is 2. After the four
blocks, there are two fully connected layers with 256 neurons
per each layer. Finally, the output layer is a fully connected
layer with 7 neurons associated with 7 emotion classes. As
mentioned previously, one can add a softmax layer after the
output layer, and then use the cross-entropy loss function to
train the models. Nevertheless, if one use the multi-class SVM
loss during the training process, the softmax layer can be
skipped.
In Table III we show the number of parameters for each ar-
chitecture. Despite of a large depth, the number of parameters
in our proposed architectures is much smaller than the one in
BKStart.
C. Implementation
1)Data preprocessing :The preprocessing step is quite
simple: Ô¨Årstly normalizing data per image, and then normal-
izing data per pixel.
‚Ä¢Normalizing data per image: Ô¨Årstly, we subtract from each
image the mean value over that image and then set the
standard deviation of the image to 3.125.
‚Ä¢Normalizing data per pixel: Ô¨Årstly, we compute the mean
image over the training set. Each pixel in the mean image
is computed from the average of all corresponding pixels
(i.e.with the same coordinates) across all training images.
For each training image, we then subtract from each pixel
its mean value, and then set the standard deviation of each
pixel over all training images to 1.TABLE II: Our proposed network architectures. The depth
increases from the left (BKVGG8) to the right (BKVGG14),
with more layers being added (the added layers are showed
in bold). Convolutional layer parameters are denoted by
‚Äúconv/angbracketleftÔ¨Ålter size/angbracketright-/angbracketleftÔ¨Ålter number/angbracketright‚Äù . ReLU activation function
is not showed for the sake of brevity.
ConvNet ConÔ¨Åguration
BKVGG8 BKVGG10 BKVGG12 BKVGG14
8 layers 10 layers 12 layers 14 layers
Input ( 42√ó42√ó1)
conv3-32 conv3-32conv3-32
conv3-32conv3-32
conv3-32
maxpool
conv3-64 conv3-64conv3-64
conv3-64conv3-64
conv3-64
maxpool
conv3-128conv3-128
conv3-128conv3-128
conv3-128conv3-128
conv3-128
conv3-128
maxpool
conv3-256
conv3-256conv3-256
conv3-256
conv3-256conv3-256
conv3-256
conv3-256conv3-256
conv3-256
conv3-256
conv3-256
FC-256
FC-256
FC-7
TABLE III: Number of parameters
Architecture Number of parameters
BKStart (reconstructed 7.17 M
FERC-2013 winning model [14])
BKVGG8 3.40 M
BKVGG10 4.14 M
BKVGG12 4.19 M
BKVGG14 4.92 M
Preprocessing step is applied to both training data and test
data.
2)Data augmentation :Due to the small amount of training
data, we apply data augmentation techniques to increase the
amount of training samples in order to avoid overÔ¨Åtting and
improve recognition accuracy. For each image, we perform the
following successive transforms:
‚Ä¢Mirror the image with a probability of 0.5.
‚Ä¢Rotate the image with a random angle from -45 to 45 (in
degrees).
‚Ä¢Rescale the image, whose original size in the FERC-2013
dataset is 48√ó48, to a random size in the range from
42√ó42to54√ó54.
‚Ä¢Take a random crop of size 42√ó42from the rescaled
image.
Fig. 1 illustrates some examples of the data augmentation
results. In the next section, we will show that the data aug-
mentation signiÔ¨Åcantly improves the accuracy of the models.
3)Training :In the training phase, we minimize the loss
function by using mini-batch gradient descent with momentum
and the back-propagation algorithm [9]) . The batch size is132
Fig. 1: Examples of the data augmentation. The left is original
images. The right is after preprocessing and augmentation.
256, the momentum is 0.9. To avoid overÔ¨Åtting, we apply the
dropout technique [13] to the fully connected layers (except
for the output one) with a dropout probability of 0.5. During
training phase, we use a strategy that decrease the learning rate
10 times if the training loss stops improving. The experiments
show that the learning rate is often decreased about 5 times,
and the training phase is often Ô¨Ånished after about 1400
epochs.
All biases are initialized by zero. All weights are randomly
initialized from a Gaussian distribution with zero mean and
the following standard deviation:
Œ¥=/radicalbigg2
nInput, (2)
wherenInput is the number of weights of each neuron. For
convolutional layer, nInput = Ô¨Ålter size√óÔ¨Ålter size√óthe
depth of the previous layer. For the fully connected layer,
nInput = the number of neurons in the previous layer.
For each iteration, the next 256 images are taken from the
training data. After performing data augmentation, we will
have a batch of 256 augmented images, each of which has
size of 42√ó42. These images are then fed to the network
for training. After each epoch, the training data is randomly
shufÔ¨Çed.
As mentioned above, we try to use different loss functions:
cross-entropy and L2 multi-class SVM.
Let us assume that:
‚Ä¢Nis the number of images in the training data;
‚Ä¢Cis the number of emotion classes ( C= 7 in FERC-
2013 dataset);
‚Ä¢siis the vector of class scores corresponding to i-th
image;
‚Ä¢liis the correct class label of i-th image;
‚Ä¢yiis the one-hot encoding of the correct class label of
i-th image (yi(li) = 1 );
‚Ä¢/hatwideyiis the probability distribution over the emotion classes
ofi-th image that can be adopted by applying the softmax
function to si.The cross-entroy loss function is deÔ¨Åned as follows:
H=‚àí1
NN/summationdisplay
i=1C/summationdisplay
j=1yi(j)log(/hatwideyi(j)), (3)
where:
‚Ä¢yi(j)‚àà{0,1}indicates whether jis the correct label of
i-th image;
‚Ä¢/hatwideyi(j)‚àà[0,1]expresses the probability that jis the
correct label of i-th image.
Meanwhile, the L2 multi-class SVM loss function can be
deÔ¨Åned as follows:
L=1
NN/summationdisplay
i=1/summationdisplay
j/negationslash=limax(0,si(j)‚àísi(li) + 1)2, (4)
where:
‚Ä¢si(j)indicates the score of class jin thei-th image;
‚Ä¢si(li)deÔ¨Ånes the score of true label liin thei-th image.
Note that in (3) and (4), for simplicity, we ignore regularized
terms such as L2 weight decay penalty.
In practice, the cross-entropy loss (combining with softmax
activation function) and L2 multi-class SVM loss are usually
comparable. Different people have different opinions on which
is better.
Fig. 2: Training accuracy (orange curve) and validation accu-
racy (blue curve)
4)Testing :We divide the original dataset into a training
set and a validation set. During the training phase, we monitor
the validation accuracy and save the model state with the
highest accuracy on the validation set. The trained model is
then applied to the test sets to estimate the Ô¨Ånal accuracy.
Fig. 2 shows us the accuracy on the training set and validation
set during training process.
In the test phase, having a trained network and an input
image of size 48x48, we use multiple crops in two ways to
predict the true class label:
1) Method Eval1 : we take 10 crops of size 42x42 of the
image (5 crops and their mirrors).
2) Method Eval2 : We Ô¨Årst rescale the image to various sizes
48√ó48,50√ó50,52√ó52, and 54√ó54. For each size,
we rotate the image with different angles -45, -33.75,
-22.5, -11.25, 0, 11.25, 22.5, 33.75, 45 (in degrees). For
each angle, take 18 crops of size 42√ó42of the image
(9 crops and their mirrors).133
In both methods Eval1 andEval2 , for each crop, we com-
pute scores over the classes (in case of L2 multi-class SVM
loss) or estimate the class probability distribution (in case of
cross-entropy loss). The Ô¨Ånal resulting vector is computed
by averaging over the results of all crops. The class label
associated with the highest value in the Ô¨Ånal vector is selected
as the predicted label.
In the section IV , we show that the second method Eval2
gives better results than the Ô¨Årst one Eval1 .
IV. E XPERIMENT
A. Datasets
We conduct experiments on the FERC-2013 dataset, which
is provided on the Kaggle facial expression competition [1].
The dataset consists of 35,887 gray images of 48x48 resolu-
tion. Kaggle has divided into 28,709 training images, 3589
public test images and 3589 private test images. Each image
contains a human face that is not posed (in the wild). Each
image is labeled by one of seven emotions: angry, disgust,
fear, happy, sad, surprise and neutral. Some images of the
FERC-2013 dataset are showed in Fig. 3.
Fig. 3: FERC-2013 dataset
B. Experiment setup
Our experiments have been conducted using Python
programing-language on the computer with the following
speciÔ¨Åcations: Intel Xeon E5-2650 v2 Eight-Core Processor
2.6GHz 8.0GT/s 20MB, Ubuntu Operating System 14.04 64
bit, 32GB RAM.
C. Result and Evaluation
1)The affection of data augmentation :Table IV shows
the accuracy of BKStart model with softmax activation func-
tion and cross-entropy loss function on the test sets in two
cases: with and without data augmentation. One can see
that data augmentation signiÔ¨Åcantly improves the model‚Äôs
accuracy. In the public test set the accuracy increases from
61.0% to 68.3%. In the private test set the accuracy increases
from 60.6% to 69.5%. We use data augmentation by default
in all next experiments.TABLE IV: The accuracy of BKStart with/without data aug-
mentation
ArchitectureData
augmen-
tationTest
methodPublic
test
accuracy
(%)Private
test
accuracy
(%)
BKStart + softmax +
cross-entropyNo Eval1 61.0 60.6
Yes Eval1 68.3 69.5
TABLE V: The accuracy when applying Eval1 test method
andEval2 test method.
ArchitectureData
augmen-
tationTest
methodpublic
test
accuracy
(%)private
test
accuracy
(%)
BKStart + softmax +
cross-entropyYesEval1 68.3 69.5
Eval2 69.2 70.4
BKVGG8 + softmax
+ cross-entropyYesEval1 66.4 68.2
Eval2 67.6 69.1
2)The affection of test methods :In section III, we
proposed two test methods called Eval1 andEval2 . Table V
shows us the accuracy of some proposed architectures on the
test sets when applying these test methods. We can see that
on both architectures BKStart and BKVGG8, method Eval2
gives better result. Thereby, we use the test method Eval2 in
the remaining experiments.
3)The affection of loss function :Table VI shows the
accuracy on test sets when applying two types of loss function:
cross-entropy and L2 multi-class SVM. One can see that on
both architectures BKStart and BKVGG8, the L2 multi-class
SVM loss achieves better result. So we believe that the L2
multi-class SVM loss works better in case of facial expression
recognition.
4)Comparison between different models :As mentioned
above, in the next experiments, all proposed architectures are
set to use data augmentation, test method Eval2 and L2 multi-
class SVM loss. Table VII shows the accuracy of all proposed
architectures on FERC-2013 dataset. One can see that the
accuracy gradually increases from BKVGG8 to BKVGG12,
but decreases with BKVGG14. The reason could be that from
TABLE VI: The accuracy when applying cross-entropy loss
and L2 multi-class SVM loss
ArchitectureData
augmen-
tationtest
methodpublic
test
accuracy
(%)private
test
accuracy
(%)
BKStart + softmax +
cross-entropyYes Eval2 69.2 70.4
BKStart + L2 SVM Yes Eval2 69.7 70.9
BKVGG8 + softmax
+ cross-entropyYes Eval2 67.6 69.1
BKVGG8 + L2 SVM Yes Eval2 68.7 70.2134
TABLE VII: Accuracy of proposed network architectures on
the FERC-2013 dataset
Architecture Public test accuracy (%) Private test accuracy (%)
BKStart 69.7 70.9
BKVGG8 68.7 70.2
BKVGG10 69.5 70.4
BKVGG12 71.0 71.9
BKVGG14 70.6 71.4
TABLE VIII: Compare our results with top 4 teams on Kaggle
competition [1]
ArchitecturePublic test
accuracy (%)Private test
accuracy (%)
SIFT+MKL [7] ( Radu +
Marius + Cristi )67.3 67.5
CNN ( Maxim Milakov ) 68.2 68.8
CNN ( team Unsupervised ) 69.1 69.3
CNN+SVM Loss [14]
(team RBM )69.4 71.2
BKStart 69.7 70.9
BKVGG14 70.6 71.4
BKVGG12 71.0 71.9
BKVGG8 to BKVGG12 the model becomes more and more
complex thanks to increasing depth and can Ô¨Åt better the
dataset. Nevertheless, BKVGG14 is too deep and becomes
over-complex towards the dataset. It causes the overÔ¨Åtting
phenomenon, that results in the drop of recognition accuracy.
Table VIII presents our results in comparison with the top
four teams on the Kaggle competition [1]. The results of the
RBM winning team are 69.4% on the public test set and 71.2%
on the private test set, which set the state-of-the-art on the
FERC-2013 dataset so far. The experiments show that our
proposed networks BKVGG14 and BKVGG12 outperforms
RBM team on both public test set and private test set, while
maintaining considerably higher performance speed thanks to
much smaller number of parameters (Table VII).
Table IX presents the confusion matrix of recognition result
achieved by BKVGG12 architecture. High accuracies are
obtained with happy (89.76%), surprised (82.45%), and neutral
(73.16%) classes. In fact, they are the most distinguishable
emotions for human. The angry, fearful, sad classes are more
often confused together, since they share many similar expres-
sions. Finally, the disgusted class get a pretty good accuracy
69.09%, despite the low amount of disgusted samples in the
training set.
TABLE IX: Confusion matrix of BKVGG12 architecture
True
LabelPredicted
Label Angry Disgusted Fearful Happy Sad Surprised Neutral
Angry 63.14 0.41 9.98 2.24 15.27 0.81 8.15
Disgusted 18.18 69.09 0.00 3.64 3.64 3.64 1.82
Fearful 11.36 0.00 51.89 2.27 20.08 6.82 7.58
Happy 1.48 0.00 1.59 89.76 2.96 1.37 2.84
Sad 7.74 0.00 8.92 3.87 61.78 0.84 16.84
Surprised 1.44 0.24 6.01 4.81 2.16 82.45 2.88
Neutral 3.51 0.16 4.15 3.04 15.34 0.64 73.16V. C ONCLUSION
In this paper, inspired by the designing principles of VGG,
we propose effective architectures of CNNs to tackle the prob-
lem of facial expression recognition. The proposed networks
are composed of a stack of convolutional blocks. Each block
has a few 3√ó3convolutional layers followed a max pooling
layers. Despite having much smaller number of parameters,
our proposed networks outperform the winning model of the
Kaggle competition. The results prove the power of small
Ô¨Ålter and very deep network in classiÔ¨Åcation tasks. We also
show that L2 multi-class SVM loss is preferable than cross-
entropy loss in facial expression recognition. Additionally, data
augmentation is shown to be an important trick in training deep
neural networks.
VI. A CKNOWLEDGMENTS
This research is funded by Vietnam National Foundation for
Science and Technology Development (NAFOSTED) under
grant number 102.01-2017.12.
REFERENCES
[1] Challenges in respresentation learning: Facial expres-
sion recognition challenge. https://www.kaggle.com/c/
challenges-in-representation-learning-facial-expression-recognition-challenge,
2013.
[2] T. F. Cootes, C. J. Taylor, et al. Statistical models of appearance for
computer vision, 2004.
[3] P. Ekman and E. L. Rosenberg. What the face reveals: Basic and applied
studies of spontaneous expression using the Facial Action Coding System
(FACS) . Oxford University Press, USA, 1997.
[4] A. Gudi. Recognizing semantic features in faces using deep learning.
arXiv preprint arXiv:1512.00743 , 2015.
[5] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pages 770‚Äì778, 2016.
[6] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep
network training by reducing internal covariate shift. arXiv preprint
arXiv:1502.03167 , 2015.
[7] R. T. Ionescu, M. Popescu, and C. Grozea. Local learning to improve bag
of visual words model for facial expression recognition. In Workshop
on challenges in representation learning, ICML , 2013.
[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiÔ¨Åcation
with deep convolutional neural networks. In Advances in neural
information processing systems , pages 1097‚Äì1105, 2012.
[9] Y . LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten
zip code recognition. Neural computation , 1(4):541‚Äì551, 1989.
[10] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning
applied to document recognition. Proceedings of the IEEE , 86(11):2278‚Äì
2324, 1998.
[11] K. Simonyan and A. Zisserman. Very deep convolutional networks for
large-scale image recognition. arXiv preprint arXiv:1409.1556 , 2014.
[12] L. Sirovich and M. Kirby. Low-dimensional procedure for the charac-
terization of human faces. Josa a , 4(3):519‚Äì524, 1987.
[13] N. Srivastava. Improving neural networks with dropout . PhD thesis,
University of Toronto, 2013.
[14] Y . Tang. Deep learning using support vector machines. CoRR,
abs/1306.0239 , 2, 2013.
[15] H. Van Kuilenburg, M. Wiering, and M. Den Uyl. A model based method
for automatic facial expression recognition. In European Conference on
Machine Learning , pages 194‚Äì205. Springer, 2005.
[16] S. Xie, R. Girshick, P. Doll ¬¥ar, Z. Tu, and K. He. Aggregated
residual transformations for deep neural networks. arXiv preprint
arXiv:1611.05431 , 2016.
[17] S. Zagoruyko and N. Komodakis. Wide residual networks. arXiv
preprint arXiv:1605.07146 , 2016.135
"
https://ieeexplore.ieee.org/document/5871583,"Facial Expression Recognition
Using Facial Movement Features
Ligang Zhang, Student Member ,IEEE , and Dian Tjondronegoro
Abstract ‚ÄîFacial expression is an important channel for human communication and can be applied in many real applications. One
critical step for facial expression recognition (FER) is to accurately extract emotional features. Current approaches on FER in staticimages have not fully considered and utilized the features of facial element and muscle movements, which represent static and
dynamic, as well as geometric and appearance characteristics of facial expressions. This paper proposes an approach to solve this
limitation using ‚Äúsalient‚Äù distance features, which are obtained by extracting patch-based 3D Gabor features, selecting the ‚Äúsalient‚Äù
patches, and performing patch matching operations. The experimental results demonstrate high correct recognition rate (CRR),
significant performance improvements due to the consideration of facial element and muscle movements, promising results under face
registration errors, and fast processing time. Comparison with the state-of-the-art performance confirms that the proposed approach
achieves the highest CRR on the JAFFE database and is among the top performers on the Cohn-Kanade (CK) database.
Index Terms ‚ÄîFacial expression analysis, feature evaluation and selection, computer vision, Gabor filter, Adaboost.
√á
1I NTRODUCTION
FACIAL expression recognition (FER) has been dramati-
cally developing in recent years, thanks to advance-
ments in related fields, especially machine learning, imageprocessing, and human cognition. Accordingly, the impact
and potential usage of automatic FER have been growing in
a wide range of applications, including human-computerinteraction, robot control, and driver state surveillance.However, to date, robust recognition of facial expressionsfrom images and videos is still a challenging task due to thedifficulty in accurately extracting the useful emotionalfeatures. These features are often represented in differentforms, such as static, dynamic, point-based geometric, orregion-based appearance.
Facial movement features , which include feature position
and shape changes, are generally caused by the movementsof facial elements and mus cles during the course of
emotional expression. The facial elements, especially keyelements, will constantly change their positions whensubjects are expressing emotions. As a consequence, thesame feature in different images usually has differentpositions, as shown in Fig. 1a. In some cases, the shape ofthe feature may also be distorted due to the subtle facial
muscle movements. For example, the mouth in the first two
images in Fig. 1b presents different shapes from that in thethird image. Therefore, for any feature representing acertain emotion, the geometry-based position and appear-ance-based shape normally change from one image toanother image in image databases, as well as in videos. This
kind of movement features represents a rich pool of bothstatic and dynamic characteristics of expressions, which
play a critical role for FER.
The vast majority of the past work on FER does not take
the dynamics of facial movement features into account [1].
Some efforts have been made in capturing and utilizing
facial movement features, and almost all of them are video-based. These efforts try to adopt either geometric features of
the tracked facial points (e.g., shape vectors [2], facial
animation parameters [3], distance and angular [4], andtrajectories [5]), or appearance difference between holisticfacial regions in consequent frames (e.g., optical flow [6],
and differential-AAM [7]), or texture and motion changes in
local facial regions (e.g., surface deformation [8], motionunits [9], spatiotemporal descriptors [10], animation units
[11], and pixel differences [12]). Although they achieved
promising results, these approaches often require accuratelocation and tracking of facial points, which remainsproblematic [13]. In addition, it is still an open question as
to how to learn the grammars in defining dynamic features
and handle ambiguities in the input data [14]. On the otherhand, image-based FER techniques provide an alternative
way to recognize emotions based on appearance-based
features in a single image, and are important for thesituation where only several images are available fortraining and testing. However, to the best of our knowl-
edge, no research has been reported on image-based FER
that considers facial movement features.
In this paper, we aim for improving the performance of
FER by automatically capturing facial movement features in
static images based on distance features. The distances areobtained by extracting ‚Äúsalient‚Äù patch-based Gabor features
and then performing patch matching operations. Patch-
based Gabor features have shown excellent performance in
overcoming position, scale, and orientation changes [15],
[16], [17], as well as extracting spatial, frequency, and
orientation information [18]. They also show a greatIEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 2, NO. 4, OCTOBER-DECEMBER 2011 219
.The authors are with the Faculty of Science and Technology, Queens-
land University of Technology, 126 Margaret Street, Brisbane CBD,Queendsland 4001, Australia. E-mail: ligang.zhang@student.qut.edu.au,
dian@qut.edu.au.
Manuscript received 2 Sept. 2010; revised 7 Feb. 2011; accepted 18 Apr. 2011;
published online 16 May 2011.Recommended for acceptance by R. Calvo.
For information on obtaining reprints of this article, please send e-mail to:
taffc@computer.org, and reference IEEECS Log NumberTAFFC-2010-09-0066.Digital Object Identifier no. 10.1109/T-AFFC.2011.13.
1949-3045/11/$26.00 /C2232011 IEEE Published by the IEEE Computer Society
advantage over the commonly used fiducial point-based
Gabor [19], [20], [21], [22], [23], graph-based Gabor [24], anddiscrete Fourier transform [25] features in capturing
regional information. Although other appearance-basedfeatures, such as local binary patterns (LBP) [26], [27],
[10], Haar [28], and histograms of oriented gradients (HOG)
[29], have shown good performance in FER, they lack thecapacity of capturing facial movement features with highaccuracy. This is due to the fact that these appearance-basedfeatures are based on statistic values (e.g., histogramsimilarity) extracted from subregions; therefore, theyproduce similar results even when facial features move abit from the original position. On the other hand, Gaborfeatures have the capacity to accurately capture movementinformation and have been proven to be robust, even in thecase of face misalignment [30].
The idea of patch matching operations has been used to
build features for object recognition [15], [16] and actionclassification [17], which remain robust when there arechanges in position, scale, and orientation. To fit for thepurpose of FER, we define the matching area and matchingscale to restrict the operations within a suitable space. Bymatching patch-based Gabor features in this space, multi-distance values are obtained. The minimum distance ischosen as the final feature for emotion classification. In this
way, one patch, which varies in its position, scale, and
shape, can still be captured provided that it is locatedwithin the defined matching space. To show the effective-ness of using the proposed distance features, we demon-strate the high performance on two widely used databases,significant improvements due to the consideration of facialmovement features, and promising results under faceregistration errors.
The remainder of the paper is organized as follows:
Section 2 describes the proposed framework, while thedetails of building distance features and ‚Äúsalient‚Äù featureselection are explained in Sections 3 and 4, respectively.Section 5 represents the recognition and speed performance,and demonstrates comparison with the state-of-the-artperformance. Finally, conclusions are drawn in Section 6.
2P ROPOSED FRAMEWORK
Fig. 2 illustrates the proposed framework, which iscomposed of preprocessing, training, and test stages. Atthe preprocessing stage, by taking the nose as the centerand keeping main facial components inclusive, facialregions are manually cropped from database images andscaled to a resolution of 48/C348pixels. No more processing
is conducted to imitate the results of real face detectors.Then, multiresolution Ga bor images are attained byconvolving eight scale, four-orientation Gabor filters with
the scaled facial regions. During the training stage, a wholeset of patches is extracted by moving a series of patcheswith different sizes across the training Gabor images. Then,
a patch matching operation is proposed to convert the
extracted patches to distance features. To capture facialmovement features, the matching area and matching scaleare defined to increase the matching space, whereas theminimum rule is used to find the best matching feature inthis space. Based on the converted distance features, a setof ‚Äúsalient‚Äù patches is selected by Adaboost. At the test
stage, the same patch matching operation is performed on
a new image using the ‚Äúsalient‚Äù patches. The resultingdistance features are fed into a multiclass support vectormachine (SVM) to recognize six basic emotions, includinganger (AN), disgust (DI), fear (FE), happiness (HA),sadness (SA), and surprise (SU).
The rest of this section gives an introduction of Gabor
filters and SVM. The details of building distance features
and feature selection are explained in Sections 4 and 5,
respectively.
In this paper, 2D Gabor filter [31] is adopted and it can be
mathematically expressed as:
F√∞x;y√û¬ºexp /C0
X2√æ/C132Y2
2/C272/C18/C19
/C2cos2/C25
/C21X/C18/C19
;
X¬ºxcos/C18√æysin/C18;Y¬º/C0xsin/C18√æycos/C18;√∞1√û
where /C18is the orientation, /C27the effective width, /C21the
wavelength, and /C13¬º0:3the aspect ratio. Instead of the
widely used five scales, eight scales (5:2:19 pixels) areadopted here to test the results using a larger number ofscales. The values of the rest of the parameters are set basedon [15] due to the high reported performance. As a result,four orientations ( /C045, 90, 45, 0 degrees) are used.
SVM [32] is one of the most widely used machine
learning algorithms for classification problems. This paper
directly uses the LIBSVM [33] implementation of SVMswith four different kernels, including linear, polynomial,radial basis function (RBF), and sigmoid. The six-emotion-class problem is solved by the one-against-the-rest strategy.
3B UILDING DISTANCE FEATURES
Fig. 3 depicts the two main processes to build distancefeatures: feature extraction and patch matching operation.Feature extraction aims to collect a set of discriminating 3Dpatches for all emotions, whereas the patch matching220 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 2, NO. 4, OCTOBER-DECEMBER 2011
Fig. 2. The proposed framework.
Fig. 1. Facial movement features. (a) Feature position (left mouth
corner) changes. (b) Feature shape (mouth) changes. Facial regions are
manually cropped from two subjects, ‚ÄúKA‚Äù and ‚ÄúKL,‚Äù on the JAFFE
database.
operation converts these patches to distance features which
can capture facial movement features.
3.1 Patch-Based Feature Extraction
The upper parts of Figs. 3 and 4 show the feature extraction
algorithm, which is comprised of four steps: First, all training
images are classified into 10 sets. For each emotion, each
Gabor scale, and each patch size, one Gabor image is
randomly selected from all the images of emotion Ek
( F i g .3 a ) .S e c o n d ,g i v e no n ep a t c h Pawith a size of
Pj/C3Pj/C3Onum, move this patch across the row and column
pixels of this Gabor image (Fig. 3b), a set of 3D patches can be
extracted (Fig. 4 line-2), one example is shown in Fig. 3c.Third, the matching area and matching scale are recorded
(Fig. 4 lines-3 and -4, details explained in Section 3.2). Finally,a patch set is constituted by combining the extracted patchesof all emotions, all scales, and all patch sizes (Fig. 3d).
To reduce the feature dimension and increase the
processing speed, we only extract a part of all patches by
moving the patch P
awith a step. As indicated by line-1 in
Fig. 4, the moving steps are set to 1, 2, 3, and 4 correspondingto four patch sizes of 2/C32/C34,4/C34/C34,6/C36/C34, and 8/C38/C34.
Given 48/C348facial images, eight-scale, and four-orientation
Gabor filters, the final set contains 148,032 patches.
As indicated and investigated by Zhao and Pietikainen
[10], current patch-based approaches only concentrate on
the location information of the selected patches, wherebyone location is shared by all emotions and only the locationinformation is preserved after feature selection. Thus, theuseful multiresolution information contained in thesepatches is discarded. On the contrary, our approachreserves both the location and multiresolution informationof patches for recognition, resulting in an equal set ofpatches for each emotion.
3.2 Patch Matching Operation
As shown in the lower part in Fig. 3, the patch matchingoperation comprises of four steps for each patch and eachtraining image: First, the matching area and matching scaleare defined to provide a bigger matching space (Figs. 3e and3f). Second, the distances are obtained by matching thispatch with all patches within its matching space in atraining image (Fig. 3h). This step takes two patches as
inputs and yields one distance value based on a distance
metric. Third, the minimum distance is chosen as thedistance feature of this patch in the training image (Fig. 3h).Finally, the distance features of all patches are combinedinto a final set with 148,032 elements (Fig. 3i).
3.2.1 Matching Area and Matching Scale Definition
The matching area and matching scale are used to
accurately capture the position and scale changes causedby facial feature movements. The idea of them stems fromZHANG AND TJONDRONEGORO: FACIAL EXPRESSION RECOGNITION USING FACIAL MOVEMENT FEATURES 221
Fig. 3. Building distance features. (a) One scale is selected from eight-scale Gabor images. (b) Patches are extracted across all rows and columns in
the selected scale image. (c) One extracted patch Pa. (d) Extracted patch set. (e) Defined matching scale. (f) Defined matching area. (g) One
matching area. (h) Distance calculation. (i) Distance feature set.
Fig. 4. Pseudocode of building distance features. (1) Defining moving
steps. (2) Extracting patches. (3) Recording matching area. (4) Record-ing matching scale.
the observation that the position and scale of one feature do
not move or change a lot in different facial images oncethese images are roughly located by a face detector. Thus,
the invariance to position and scale changes can be
accomplished by defining a bigger area and a larger scalefor each patch when performing patch matching.
Fig. 5 illustrates the matching area Area of a patch P
a
with a size of Pj/C3Pj/C3Onum, while Fig. 3e shows the
matching scales that are drawn in a gray color. In this
paper, the Area is set to two times of Pain width and height,
but with the same orientation number Onum and center
point. That is, Area ¬º√∞2/C3Pj√û/C3√∞2/C3Pj√û/C3Onum. The match-
ing scale is the same with that of Pabecause the cropped
facial regions generally belong to the same scale. However,
it is flexible to increase the matching scale in the case of
large scale variations.
3.2.2 Distance Metric Definition
The distance metric is used to compute the similarity
between two patches. Several metrics have been adopted in
previous work, such as Gaussian-like euclidean [34] and
normalized dot-product [17]. In this paper, four metrics,
including dense L1√∞DL 1√û, dense L2√∞DL 2√û, sparse L1√∞SL1√û,
and sparse L2√∞SL2√û, are used due to the computational
simplicity, and they can be mathematically expressed as
DL 1:kPb/C0Pck¬º1
Pj/C3Pj/C3OnumXPj
i¬º1XPj
j¬º1XOnum
o¬º1/C13/C13Pijo
b/C0Pijo
c/C13/C13;
√∞2√û
DL 2:kPb/C0Pck
¬º1
Pj/C3Pj/C3OnumÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨É
XPj
i¬º1XPj
j¬º1XOnum
o¬º1/C0
Pijo
b/C0Pijo
c/C12vuut;√∞3√û
SL
1:kPb/C0Pck
¬º1
Pj/C3PjXPj
i¬º1XPj
j¬º1maxXOnum
o¬º1Pijo
b/C0maxXOnum
o¬º1Pijo
c !
;√∞4√û
SL2:kPb/C0Pck
¬º1
Pj/C3PjÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨É
XPj
i¬º1XPj
j¬º1maxXOnum
o¬º1Pijo
b/C0maxXOnum
o¬º1Pijo
c !2vuut;√∞5√û
where, P
bandPcrepresent two patches, Pijorepresents the
pixel values in the ith row, jth column, and oth orientationof the patches. The distances are normalized by dividing the
number of pixels in the patches. The dense distances can be
conceived as taking into account all orientations of each
pixel in one patch, whereas the sparse distances only
consider the dominant orientation [16] of each pixel in one
patch. Therefore, they differ in representing all features or
only dominant features of all orientations.
4P ATCH-BASED FEATURE SELECTION AND
ANALYSIS
In this section, we use Adaboost for discriminative (called
‚Äúsalient‚Äù here) patch selection on the Japanese female facial
expression (JAFFE) and CK databases. To give a deeperunderstanding of the selected ‚Äúsalient‚Äù patches and provideuseful information on the design of Gabor filters and feature
extraction algorithms, we also present a description on their
position, number, size, scale, and overlap distributions.
4.1 Databases
The JAFFE database [35] contains 213 gray images of sevenfacial expressions (six basic + neutral) poses of 10 Japanesefemales. Each image has a resolution of 256/C3256pixels.
Each object has three or four frontal face images for each
expression and their faces are approximately located in themiddle of the images. All images have been rated on six
emotion adjectives by 60 subjects.
The Cohn-Kanade AU coded facial expression (CK)
database [36] is one of the most comprehensive bench-
marks for facial expression tests. The released portion ofthis database includes 2,105 digitized image sequencesfrom 182 subjects ranged in age from 18 to 30 years. Sixty-
five percent are female; 15 percent are African-American
and 3 percent Asian or Latino. Six basic expressions werebased on descriptions of pro totypic emotions. Image
sequences from neutral to target display were digitized
into 640/C3480or 490 pixel arrays with eight-bit precision
for gray scale values.
In this paper, all the images of six basic expressions from
the JAFFE database are used. For the CK database, 1,184
images that represent one of the six expressions are selected,
four images for each expression of 92 subjects. The images arechosen from the last image (peak) of each sequence, then oneevery two images. The images of 10 subjects in the JAFFE
database are classified into 10 sets, each of which includes
images of one subject. Similarly, all images in the CKdatabase are classified into 10 similar sets and all images
of one subject are included in the same set.
4.2 ‚ÄúSalient‚Äù Patch Selection
The feature extraction step produces a feature set contain-
ing 148,032 patches. To reduce the feature dimension andthe redundant information, it is necessary to select a subsetof ‚Äúsalient‚Äù patches. In this paper, the widely used and
efficiency proven boosting algorithm‚ÄîAdaboost [37]‚Äîis
used for ‚Äúsalient‚Äù patch selection.
Since Adaboost was designed to solve two-class pro-
blems, in this research the one-against-the-rest strategy is
used to solve the six-emotion-class problem. The training
process stops when the empirical error is below 0.0001 with
an initial error of 1. This setting is inspired by the stopping
condition in [20] that there is no training error and the
generalization error becomes flat. For the training set, the222 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 2, NO. 4, OCTOBER-DECEMBER 2011
Fig. 5. Matching area. (a) One patch Pa. (b) The corresponding matching
area ‚Äú Area ‚Äù in a new image.
JAFFE database includes all database images, whereas the
CK database is only composed of the peak frames.
4.3 Position Distribution of ‚ÄúSalient‚Äù Patches
The position distribution of the ‚Äúsalient‚Äù patches demon-
strates the most important facial areas for each emotion. In
Fig. 6, the patches are distributed over different Gabor
scales, and they are drawn in one scale image for a simpleand clear demonstration. Based on this figure, we can see
that the positions are distributed differently over six
emotions. However, most of these patches for all emotionstend to concentrate on the areas around the mouth and
eyes. For sadness and surprise, the ‚Äúsalient‚Äù patches on
JAFFE focus on the eye areas, while those on CK focus onthe mouth area. For the remaining four emotions, they have
similar distributions between two databases. As shown in
the second and third rows in Fig. 6, the positions of the‚Äúsalient‚Äù patches in our work and those of the point-based
‚Äúsalient‚Äù features in [38] for the same emotion tend to focus
on the same areas. This suggests that there exist the same‚Äúsalient‚Äù areas for each emotion regardless of use of point-
based or patch-based Gabor features. However, the overall
number of the ‚Äúsalient‚Äù patches is much less than that of thepoint-based ‚Äúsalient‚Äù features (177 versus 538).
4.4 Number and Size Distributions of ‚ÄúSalient‚Äù
Patches
The number and size distributions can provide useful hints
on the number of patches for different emotions and howto choose suitable patch sizes during feature extraction. As
seen in Fig. 7, two databases have a similar overall number
of the ‚Äúsalient‚Äù patches. Among six emotions, fear andsadness need the largest numbers of patches to achieve thepreset recognition accuracy, whereas surprise requires the
least number. Within four patch sizes, the size 4/C34takes a
significant proportion of the overall number of the‚Äúsalient‚Äù patches.
On the other hand, there are also some differences
between two databases. The number for anger on JAFFE is
much less than that on CK, while the number for disgust onJAFFE is much bigger than that on CK. Moreover, four patch
sizes are evenly distributed among six emotions on JAFFE,
but the patch size 2/C32takes a significant proportion of the
overall number of the ‚Äúsalient‚Äù patches on CK. This reflects
that emotions in JAFFE images need big sizes of patches to
represent useful information, whereas those in CK imagesonly require small sizes of patches. The reason may be thatthe emotions in CK are more distinct than those in JAFFE.
4.5 Scale Distribution of ‚ÄúSalient‚Äù Patches
The scale distribution is a very important factor for
determining the scale number of Gabor filters. As observed
in Fig. 8, the ‚Äúsalient‚Äù patches of two databases areunevenly distributed across eight scales. JAFFE emphasizes
the eighth scale and CK focuses on the fourth scale. For both
databases, the higher scales (fourth to eighth) contain morepatches than the lower scales (first to third). Therefore, the
emotional information is distributed across all scales with
an emphasis on the higher scales, which confirms Little-wort‚Äôs argument that a wider range of spatial frequencies,
particularly high frequencies, could potentially improve
performance [20].
4.6 Overlap Distribution of ‚ÄúSalient‚Äù Patches
Table 1 demonstrates the characteristics of number, size,
scale, emotion pair of the overlapping patches, which areselected as ‚Äúsalient‚Äù patches more than one time. As can beseen, the JAFFE database has a larger number of theoverlapping patches than the CK database (eight versusthree). As for the patch size, 4/C34dominates the over-
lapping patches on JAFFE, while 2/C32takes most of these
patches on CK. With respect to the patch scale, the patchesof JAFFE tend to distribute on the sixth, seventh, and eighth
scales, whereas those of CK are all included by the fourth
scale. The patch size and scale distributions again revealZHANG AND TJONDRONEGORO: FACIAL EXPRESSION RECOGNITION USING FACIAL MOVEMENT FEATURES 223
Fig. 6. Position distribution of the selected ‚Äúsalient‚Äù patches for six
emotions. The first and second rows show positions of the selectedpatches in the proposed approach on the JAFFE and CK databases,
respectively, whereas the third row reveals positions of the selected
point-based Gabor features in [38] on the CK database.
Fig. 7. Number and size distributions of the selected ‚Äúsalient‚Äù patches for six emotions on (a) the JAFFE and (b) the CK databases. Note that Onumis
four for all patch sizes of Pj/C3Pj/C3Onumand it is not shown.
that JAFFE needs larger patches than CK. For the emotion
pair, the majority of the overlapping patches are shared bythe same emotion. As shown in Fig. 9, the overlapping
patches are mainly distributed over disgust and anger.
5E XPERIMENTAL RESULTS
In this section, we present the recognition and computa-
tional performance of the proposed approach. The perfor-mance obtained with and without matching area is also
compared. Finally, a performance comparison with pre-
vious approaches is conducted.
5.1 Recognition Performance
5.1.1 JAFFE Database
The performance results are obtained by averaging the
correct recognition rate (CRR) of all sets in 10 leave-one-set-out cross validations. Table 2 shows the results obtainedusing four SVMs and four distances. From this table, we cansee that the proposed approach performs the best with aCRR of 92.93 percent using DL
2and linear SVM. Regardingthe performance of distances, DL 2achieves higher CRRs than
the other three distances for all SVMs. When L1is used, sparse
distances outperform dense distances for linear, RBF, and
sigmoid SVMs. On the contrary, when L2is used, dense
distances outperform sparse distances for all SVMs (note thatthe CRR of DL
2and sigmoid SVM is not shown). For both
sparse and dense distances, L2performs better than L1for all
SVMs. Among four SVMs, linear and RBF outperformpolynomial and sigmoid for all distances. More exactly, thebest performance is obtained by linear, which is followed by
RBF, whereas sigmoid ranks the lowest.
Table 3 demonstrates the confusion matrix of six
emotions using DL
2and linear SVM. Observed from this
table, disgust and surprise belong to the most difficult facial
expressions to be correctly recognized with the same CRR
of 90.00 percent, whereas anger is the easiest one with aCRR of 96.67 percent. Regarding the misrecognition rate,
anger contributes the most; as a result, it has a major
negative impact on the overall performance. The emotionthat follows in misrecognition rate is fear.224 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 2, NO. 4, OCTOBER-DECEMBER 2011
Fig. 8. Scale distribution of the selected ‚Äúsalient‚Äù patches for six emotions on (a) the JAFFE and (b) the CK databases.
Fig. 9. Position distribution of the overlapping patches for six emotions.
The upper row is for JAFFE and the lower row is for CK.TABLE 1
Overlapping Patches on the JAFFE and CK Databases
The figures before parentheses stand for the number of overlapped
patches, while content in parentheses indicates patch sizes, scales, and
emotion pairs. As an instance of emotion pairs, ‚Äú1(FE-SA)‚Äù means onepatch is shared by fear and sadness.TABLE 2
CRRs of Six Emotions on the JAFFE Database
TABLE 3
Confusion Matrix of Six Emotions on the JAFFE Database

5.1.2 CK Database
The CRRs using four SVMs and four distance metrics are
shown in Table 4, in which the proposed approach obtainsthe highest CRR of 94.48 percent using DL
2and RBF SVM.
Regarding the performance of distances, DL 2keeps the
highest CRRs for all SVMs (note that the CRR of DL 2and
sigmoid SVM is not shown). Moreover, dense distances
have a higher overall performance than sparse distances.This reflects that emotional information in the CK images isdistributed over all orientations rather than the dominantorientation of Gabor features. As for SVMs, RBF performsthe best for dense distances, while linear performs the bestfor sparse distances. This confirms with the results in [20]
that RBF and linear perform better than polynomial on the
CK database.
Table 5 shows the confusion matrix of six emotions using
DL
2and RBF SVM. As can be seen, surprise performs the
best, with a CRR of 100 percent, the following one ishappiness, with a CRR of 98.07 percent. On the other hand,anger is the most difficult facial expression to be correctlyrecognized, with a CRR of only 87.10 percent. Theperformance of surprise and anger on CK contrasts with
that on JAFFE, in which surprise and anger are the most
difficult and easiest emotions, respectively. The reason
probably is that surprise images on CK are often character-ized as an exaggerated ‚Äúopen mouth,‚Äù while those onJAFFE are normally with a ‚Äúclose or slightly open mouth.‚ÄùIt can be seen from Fig. 6 that the selected patches for CKfocus on the mouth region, but those for JAFFE are mainly
distributed around the eyes regions. Similarly, anger
images are better expressed by the patches selected in theeye regions on JAFFE than those selected in the whole faceon CK. Among six emotions, anger and sadness contributemost to the misrecognition rate.
5.2 Performance versus Number of Patches
We also test the relationship between the performance andthe number of patches. Table 6 shows the error thresholds
used to control the number of the patches selected by
Adaboost. These thresholds are set based on our observa-tion that the empirical errors of Adaboost decrease with afactor of 10 and the numbers are evenly distributed betweendecimal intervals. For instance, the number of errorsbetween 0.01 and 0.02 is similar to that between 0.003 and0.004. Accordingly, 38 groups of features are obtained by
selecting patches with empirical errors bigger than the
corresponding error thresholds.
For the JAFFE database, as can be seen from Fig. 10, the
proposed approach achieves the highest CRR of 93.48 percentusing DL
2and linear SVM when the error threshold equals to
0.0001 and the number of patches equals to 185. The overallperformance of four distances grows up rapidly at theZHANG AND TJONDRONEGORO: FACIAL EXPRESSION RECOGNITION USING FACIAL MOVEMENT FEATURES 225
TABLE 4
CRRs of Six Emotions on the CK Database
TABLE 5
Confusion Matrix of Six Emotions on the CK Database
TABLE 6
Error Thresholds Used to Control the Number of Patches
Fig. 10. Recognition performance versus the number of patches on the JAFFE database using (a) linear and (b) RBF SVMs.
starting stage; however, it begins to level off when the
number of patches exceeds 150 for linear and 80 for RBF. For
the overall performance of SVMs, linear performs better than
RBF for all distances. Regarding the overall performance of
distances, for both linear and RBF, the best performance is
achieved by DL 2, which is followed by SL2. On the other
hand, SL1andDL 1rank the last two.
For the CK database, seen in Fig. 11, the proposed
approach obtains the highest CRR of 94.48 percent using
DL 2and RBF SVM when the error threshold is 0 and the
number of patches is 180. This implies that a performance
improvement can still be achieved using a larger number of
patches. Similarly to that on JAFFE, the CRR grows up rapidly
at the starting stage and L2outperforms L1for both linear and
RBF. On the other hand, the CRR reaches the plateau with a
quicker speed than that on JAFFE and DL 1performs better
than SL1. Moreover, the performance difference between
linear and RBF is smaller than that on JAFFE.
5.3 Matching Area versus No Matching Area
To evaluate the performance improvement rising from the
use of facial movement features, we compare the perfor-
mance of with and without matching area. In the latter case,
the distance features are obtained by performing subtraction
between two patches at the exact same position. Therefore,
the resulting features do not include the information of
feature movements. Fig. 12 shows the comparison results
obtained when the error threshold of Adaboost is 0. The
classifiers of JAFFE and CK are linear and RBF SVMs,respectively. As can be seen, for the JAFFE database, the
recognition performance of the proposed approach using
four distances is greatly boosted due to the use of matchingarea. There is a CRR increase of 11.41 percent using DL
2. For
the CK database, the CRRs of DL 1and DL 2are improved
about 2.5 percent due to the use of matching area, while the
performance of SL1andSL2does not benefit from matching
area. Considering the highest CRR of four distances, we can
see that taking facial movement features into account helps
to improve the recognition performance.
5.4 Perforamnce under Registration Errors
To test the performance of the proposed approach usingimages with registration errors, we add uniform random
noises into the coordinate of the top-left corner and the scale
of each face region produced by the widely used Viola-Jones face detector [39]. The noises are controlled so that
both the coordinate and the image scale randomly change
within a range of ¬Ω/C0/C11%;/C11%/C138(/C11/C26¬Ω1;2;3;4;5;6/C138) of face
width. In addition, we also include neutral images in the
experiment. In videos, the starting and ending frames for an
expression can be determined by classifying the current
frame into ‚Äúneutral‚Äù or ‚Äúemotion.‚Äù In this way, two sets of
database images with different levels of noises are created.
Fig. 13 shows sample images with 3 and 6 percent errors
from the JAFFE and CK databases. After scaling the
simulated images into 48/C348, there will be a maximum of
/C11pixels √∞48/C32/C3/C11%√ûchanges of position and scale for a
level of /C11percent errors.226 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 2, NO. 4, OCTOBER-DECEMBER 2011
Fig. 12. Recognition accuracy (percent) obtained with and without matching area (a) on the JAFFE database using linear SVM (b) and on the CK
database using RBF SVM.
Fig. 11. Recognition performance versus the number of patches on the CK database using (a) linear and (b) RBF SVMs.
To handle scale changes, the matching scale is set to
include two neighbor scales and the scale itself. In each ofthe 10 sets cross validation, nonerror images in nine sets areused for training, while the error-simulated images of theone set left are used for testing. This is important for the realsituation that only ideally registered images are availablefor the training. To testify the usefulness of matching scale,we give the results of using both matching area andmatching scale (AreaScale), as well as only using matchingarea (Area). Note that SL
1is used here. We also compare
the results with using point-based four orientations andeight scales Gabor features + Adaboost feature selector +RBF SVM classifier (point-based Gabor).
Fig. 14 shows the performance of three approaches under
the simulated errors. As expected, all approaches sufferdecreasing performances using a larger percentage oferrors. The two proposed approaches achieve higher overall
performances than the approach using point-based Gabor
features, for both JAFFE and CK. This again demonstratesthe advantage of patch-based Gabor over point-basedGabor features in terms of the performance under faceregistration errors. Using both matching area and scale(AreaScale) performs better than using matching area only(Area) for both the databases. At the error level of 4 percent,which can be considered as larger than the errors producedby real face detectors, AreaScale still keeps a CRR of 69.5and 83.9 percent for JAFFE and CK, respectively. Therefore,the proposed approach achieves promising results underthe simulated registration errors.5.5 Computational Time Performance
Fig. 15 illustrates the average computational time at three
stages, including Gabor images (Gab), patch matching (PM),
and classification (SVM). The program was developed byMatlab 7.6.0 under a laptop configuration of core duo
1.66 GHz CUP and 2 GB memory. The proposed approach
achieves a speed of 0.1258 seconds per image for the JAFFEdatabase (using DL
2and linear) and 0.1185 seconds per
image for the CK database (using DL 2and RBF). Thus, a
real-time processing is expected if our approach could be
developed by time efficient languages, such as C, C++.Among three stages, computing Gabor images takes the
biggest proportion of the overall time, while the classifica-
tion requires the least amount of time.
5.6 Comparison with State-of-the-Art Performance
Table 7 demonstrates the r esults compared with the
reported results of the benchmarked approaches. Theseapproaches are selected because they produced the state-of-
the-art performance using a similar testing strategy and the
same databases. In [40], only the recognition results of theleave-one-subject-out strategy is used here as this strategy is
more similar to our leave-one-set-out cross validations. The
recognition results in [41] were obtained by removing twoJAFFE images named ‚ÄúKR.SR3.79‚Äù and ‚ÄúNA.SU1.79.‚ÄùZHANG AND TJONDRONEGORO: FACIAL EXPRESSION RECOGNITION USING FACIAL MOVEMENT FEATURES 227
Fig. 14. Recognition performance under face registration errors on (a) the JAFFE and (b) the CK databases.Fig. 15. The used time (in seconds) per image at three stages. ‚ÄúGab‚Äù
and ‚ÄúPM‚Äù indicate the stages of Gabor image and patch matching, ‚ÄúJ‚Äù
and ‚ÄúC‚Äù indicate using the JAFFE and CK databases, ‚ÄúD‚Äù and ‚ÄúS‚Äù standfor using dense and sparse distances, ‚ÄúL1‚Äù and ‚ÄúL2‚Äù represent L
1and
L2distances, respectively. The time is obtained using the SVM type with
the highest CRR for each distance.
Fig. 13. Sample images with errors simulated by uniform random noisesranged (a) [‚Äì3 percent, 3 percent] and (b) [‚Äì6 percent, 6 percent] of facewidth.
As shown in Table 7, the proposed approach outperforms
all nine benchmarked approaches [23], [26], [40], [41], [42],[43], [44], [45], [46] when the JAFFE database is used, and
three out of four benchmarked approaches [20], [21], [26],
[46] when the CK database is used. When six emotions areused, the CRR of the proposed approach is 9.73 and 15.88
percent higher than those in [42] and [43], respectively, on the
JAFFE database, as well as 1.88 percent higher than that
obtained using LBP features in [26] on the CK database. The
result of the proposed approach on the CK database is 0.62percent lower than the result obtained using the boosted-LBP
features in [26] and 1.39 percent lower than the result in [46].
However, the work [26] normalized the face based onmanually labeled eye locations and improved the results
by optimizing the SVM parameters. While the proposed
approach does not involve normalization of face regions anduses the default parameters in LIBSVM. Another difference
is that the database images in the proposed approach
represent bigger emotional intensity than those in [26]. Tobe specific, the proposed approach collects images using a
‚Äúevery two images from the peak frame‚Äù strategy, while [26]
just used the three peak frames. Wong and Cho [46] obtained
the results based on five-fold cross validations and five
expressions; therefore it used more training images andclassified less emotions compared to our approach.
6C ONCLUSION AND FUTURE WORK
This paper explores the issue of facial expression recogni-
tion using facial movement features. The effectiveness ofthe proposed approach is testified by the recognitionperformance, computational time, and comparison withthe state-of-the-art performance. The experimental resultsalso demonstrate significant performance improvements
due to the consideration of facial movement features and
promising performance under face registration errors.
The results indicate that patch-based Gabor features show
a better performance over point-based Gabor features interms of extracting regional features, keeping the positioninformation, achieving a better recognition performance, andrequiring a less number. Different emotions have different
‚Äúsalient‚Äù areas; however, the majority of these areas are
distributed around mouth and eyes. In addition, these‚Äúsalient‚Äù areas for each emotion seem to not be influencedby the choice of using point-based or using patch-basedfeatures. The ‚Äúsalient‚Äù patches are distributed across all
scales with an emphasis on the higher scales. For both theJAFFE and CK databases, DL
2performs the best among four
distances. As for emotion, anger contributes most to the
misrecognition. The JAFFE database requires larger sizes of
patches than the CK database to keep useful information.
The proposed approach can be potentially applied into
many applications, such as patient state detection, driverfatigue monitoring, and intelligent tutoring system. In our
future work, we will extend our approach to a video-based
FER system by combining patch-based Gabor features withmotion information in multiframes. Recent progress onaction recognition [47] and face recognition [48] has laid afoundation for using both appearance and motion features.
ACKNOWLEDGMENTS
The authors would like to thank Nicki Ridgeway for
providing the Cohn-Kanade AU-Coded Facial Expression
Database and the providers of the JAFFE database.
REFERENCES
[1] Z. Zeng, M. Pantic, G.I. Roisman, and T.S. Huang, ‚ÄúA Survey of
Affect Recognition Methods: Audio, Visual, and SpontaneousExpressions,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence,
vol. 31, no. 1, pp. 39-58, Jan. 2009.
[2] T. Yan, C. Jixu, and J. Qiang, ‚ÄúA Unified Probabilistic Framework
for Spontaneous Facial Action Modeling and Understanding,‚Äù
IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 32, no. 2,
pp. 258-273, Feb. 2010.
[3] P.S. Aleksic and A.K. Katsaggelos, ‚ÄúAutomatic Facial Expression
Recognition Using Facial Animation Parameters and MultistreamHMMs,‚Äù IEEE Trans. Information Forensics and Security, vol. 1,
no. 1, pp. 3-11, Mar. 2006.
[4] D. Hamdi, V. Roberto, S.A. Ali, and G. Theo, ‚ÄúEyes Do Not Lie:
Spontaneous versus Posed Smiles,‚Äù Proc. Int‚Äôl Conf. Multimedia,
pp. 703-706, 2010.
[5] T.-H. Wang and J.-J.J. Lien, ‚ÄúFacial Expression Recognition System
Based on Rigid and Non-Rigid Motion Separation and 3D PoseEstimation,‚Äù J. Pattern Recognition, vol. 42, no. 5, pp. 962-977, 2009.
[6] M. Yeasin, B. Bullot, and R. Sharma, ‚ÄúRecognition of Facial
Expressions and Measurement of Levels of Interest from Video,‚ÄùIEEE Trans. Multimedia, vol. 8, no. 3, pp. 500-508, June 2006.
[7] Y. Cheon and D. Kim, ‚ÄúNatural Facial Expression Recognition
Using Differential-AAM and Manifold Learning,‚Äù Pattern Recogni-
tion, vol. 42, pp. 1340-1350, 2009.
[8] F. Tsalakanidou and S. Malassiotis, ‚ÄúReal-Time 2D+3D Facial
Action and Expression Recognition,‚Äù Pattern Recognition, vol. 43,
pp. 1763-1775, 2010.
[9] I. Cohen, N. Sebe, A. Garg, L.S. Chen, and T.S. Huang, ‚ÄúFacial
Expression Recognition from Video Sequences: Temporal andStatic Modeling,‚Äù Computer Vision and Image Understanding, vol. 91,
pp. 160-187, 2003.
[10] G. Zhao and M. Pietikainen, ‚ÄúBoosted Multi-Resolution Spatio-
temporal Descriptors for Facial Expression Recognition,‚Äù Pattern
Recognition Letters, vol. 30, pp. 1117-1127, 2009.
[11] F. Dornaika and F. Davoine, ‚ÄúSimultaneous Facial Action Tracking
and Expression Recognition in the Presence of Head Motion,‚Äù Int‚Äôl
J. Computer Vision, vol. 76, pp. 257-281, 2008.
[12] A. Kapoor, W. Burleson, and R.W. Picard, ‚ÄúAutomatic Prediction
of Frustration,‚Äù Int‚Äôl J. Human-Computer Studies, vol. 65, pp. 724-
736, 2007.
[13] L. Peng and S.J.D. Prince, ‚ÄúJoint and Implicit Registration for Face
Recognition,‚Äù Proc. IEEE Conf. Computer Vision and Pattern
Recognition, pp. 1510-1517, 2009.
[14] T. Huang, A. Nijholt, M. Pantic, and A. Pentland, ‚ÄúHuman
Computing and Machine Understanding of Human Behavior: ASurvey,‚Äù Artifical Intelligence for Human Computing, vol. 4451,
pp. 47-71, 2007.228 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 2, NO. 4, OCTOBER-DECEMBER 2011
TABLE 7
Comparison with State-of-the-Art Performance
The figures in parentheses stand for the number of the testing facial
expressions.
[15] T. Serre, L. Wolf, S. Bileschi, M. Riesenhuber, and T. Poggio,
‚ÄúRobust Object Recognition with Cortex-Like Mechanisms,‚Äù IEEE
Trans. Pattern Analysis and Machine Intelligence, vol. 29, no. 3,
pp. 411-426, Mar. 2007.
[16] J. Mutch and D.G. Lowe, ‚ÄúMulticlass Object Recognition with
Sparse, Localized Features,‚Äù Proc. IEEE CS Conf. Computer Vision
and Pattern Recognition, pp. 11-18, 2006.
[17] H. Jhuang, T. Serre, L. Wolf, and T. Poggio, ‚ÄúA Biologically
Inspired System for Action Recognition,‚Äù Proc. IEEE 11th Int‚Äôl
Conf. Computer Vision, pp. 1-8, 2007.
[18] L. Zhen, L. Shengcai, H. Ran, M. Pietikainen, and S.Z. Li, ‚ÄúGabor
Volume Based Local Binary Pattern for Face Representation and
Recognition,‚Äù Proc. IEEE Eighth Int‚Äôl Conf. Automatic Face and
Gesture Recognition, pp. 1-6, 2008.
[19] L. Wiskott, J.M. Fellous, N. Kuiger, and C. von der Malsburg,
‚ÄúFace Recognition by Elastic Bunch Graph Matching,‚Äù IEEE Trans.
Pattern Analysis and Machine Intelligence, vol. 19, no. 7, pp. 775-779,
Jul. 1997.
[20] G. Littlewort, M.S. Bartlett, I. Fasel, J. Susskind, and J. Movellan,
‚ÄúDynamics of Facial Expression Extracted Automatically fromVideo,‚Äù Image and Vision Computing, vol. 24, pp. 615-625, 2006.
[21] H.Y. Chen, C.L. Huang, and C.M. Fu, ‚ÄúHybrid-Boost Learning for
Multi-Pose Face Detection and Facial Expression Recognition,‚ÄùPattern Recognition, vol. 41, pp. 1173-1185, 2008.
[22] S. Hoch, F. Althoff, G. McGlaun, and G. Rigoll, ‚ÄúBimodal Fusion
of Emotional Data in an Automotive Environment,‚Äù Proc. IEEE
Int‚Äôl Conf. Acoustics, Speech, and Signal Processing, pp. 1085-1088,
2005.
[23] G. Guo and C.R. Dyer, ‚ÄúLearning from Examples in the Small
Sample Case: Face Expression Recognition,‚Äù IEEE Trans. Systems,
Man, and Cybernetics, Part B: Cybernetics, vol. 35, no. 3, pp. 477-488,
June 2005.
[24] S. Zafeiriou and I. Pitas, ‚ÄúDiscriminant Graph Structures for Facial
Expression Recognition,‚Äù IEEE Multimedia, vol. 10, no. 8, pp. 1528-
1540, Dec. 2008.
[25] T. Xiang, M.K.H. Leung, and S.Y. Cho, ‚ÄúExpression Recognition
Using Fuzzy Spatio-Temporal Modeling,‚Äù Pattern Recognition,
vol. 41, pp. 204-216, 2008.
[26] C. Shan, S. Gong, and P.W. McOwan, ‚ÄúFacial Expression
Recognition Based on Local Binary Patterns: A ComprehensiveStudy,‚Äù Image and Vision Computing, vol. 27, pp. 803-816, 2009.
[27] Z. Guoying and M. Pietikainen, ‚ÄúDynamic Texture Recognition
Using Local Binary Patterns with an Application to FacialExpressions,‚Äù IEEE Trans. Pattern Analysis and Machine Intelligence,
vol. 29, no. 6, pp. 915-928, June 2007.
[28] P. Yang, Q. Liu, and D.N. Metaxas, ‚ÄúBoosting Encoded Dynamic
Features for Facial Expression Recognition,‚Äù Pattern Recognition
Letters, vol. 30, pp. 132-139, 2009.
[29] C. Orrite, A. Gan Àúa¬¥n, and G. Rogez, ‚ÄúHOG-Based Decision Tree
for Facial Expression Classification,‚Äù Proc. Fourth Iberian Conf.
Pattern Recognition and Image Analysis, pp. 176-183, 2009.
[30] S. Shiguang, G. Wen, C. Yizheng, C. Bo, and Y. Pang, ‚ÄúReview the
Strength of Gabor Features for Face Recognition from the Angle ofIts Robustness to Mis-Alignment,‚Äù Proc. 17th Int‚Äôl Conf. Pattern
Recognition, vol. 1, pp. 338-341, 2004.
[31] D. Gabor, ‚ÄúTheory of Communication,‚Äù
J. Institution of Electrical
Engineers‚ÄîPart III: Radio and Comm. Eng., vol. 93, pp. 429-441,
1946.
[32] C. Cortes and V. Vapnik, ‚ÄúSupport-Vector Networks,‚Äù Machine
Learning, vol. 20, pp. 273-297, 1995.
[33] C.C. Chang and C.J. Lin, ‚ÄúLIBSVM: A Library for Support Vector
Machines, 2001,‚Äù http://www.csie.ntu.edu.tw/cjlin/libsvm,2001.
[34] T. Serre, L. Wolf, and T. Poggio, ‚ÄúObject Recognition with Features
Inspired by Visual Cortex,‚Äù Proc. IEEE CS Conf. Computer Vision
and Pattern Recognition, vol. 2, pp. 994-1000, 2005.
[35] M. Lyons, S. Akamatsu, M. Kamachi, and J. Gyoba, ‚ÄúCoding Facial
Expressions with Gabor Wavelets,‚Äù Proc. IEEE Third Int‚Äôl Conf.
Automatic Face and Gesture Recognition, pp. 200-205, 1998.
[36] T. Kanade, J.F. Cohn, and T. Yingli, ‚ÄúComprehensive Database for
Facial Expression Analysis,‚Äù Proc. IEEE Fourth Int‚Äôl Conf. Automatic
Face and Gesture Recognition, pp. 46-53, 2000.
[37] Y. Freund and R.E. Schapire, ‚ÄúA Decision-Theoretic General-
ization of On-Line Learning and an Application to Boosting,‚ÄùJ. Computer and System Sciences, vol. 55, no. 1, pp. 119-139, 1997.[38] M.S. Bartlett, G. Littlewort, I. Fasel, and J.R. Movellan, ‚ÄúReal Time
Face Detection and Facial Expression Recognition: Developmentand Applications to Human Computer Interaction,‚Äù Proc. Compu-
ter Vision and Pattern Recognition Workshop, p. 53, 2003.
[39] P. Viola and M.J. Jones, ‚ÄúRobust Real-Time Face Detection,‚Äù Int‚Äôl
J. Computer Vision, vol. 57, pp. 137-154, 2004.
[40] M. Kyperountas, A. Tefas, and I. Pitas, ‚ÄúSalient Feature and
Reliable Classifier Selection for Facial Expression Classification,‚ÄùPattern Recognition, vol. 43, pp. 972-986, 2010.
[41] C. Zhengdong, S. Bin, F. Xiang, and Z. Yu-Jin, ‚ÄúAutomatic
Coefficient Selection in Weighted Maximum Margin Criterion,‚Äù
Proc. 19th Int‚Äôl Conf. Pattern Recognition, pp. 1-4, 2008.
[42] W. Yuwen, L. Hong, and Z. Hongbin, ‚ÄúModeling Facial Expres-
sion Space for Recognition,‚Äù Proc. IEEE/RSJ Int‚Äôl Conf. Intelligent
Robots and Systems, pp. 1968-1973, 2005.
[43] Z. Wenming, Z. Xiaoyan, Z. Cairong, and Z. Li, ‚ÄúFacial Expression
Recognition Using Kernel Canonical Correlation Analysis(KCCA),‚Äù IEEE Trans. Neural Networks, vol. 17, no. 1, pp. 233-
238, Jan. 2006.
[44] Y. Horikawa, ‚ÄúFacial Expression Recognition Using KCCA with
Combining Correlation Kernels and Kansei Information,‚Äù Proc.
Int‚Äôl Conf. Computational Science and Its Applications, pp. 489-498,
2007.
[45] J. Bin, Y. Guo-Sheng, and Z. Huan-Long, ‚ÄúComparative Study of
Dimension Reduction and Recognition Algorithms of DCT and
2DPCA,‚Äù Proc. Int‚Äôl Conf. Machine Learning and Cybernetics, pp. 407-
410, 2008.
[46] J.-J. Wong and S.-Y. Cho, ‚ÄúA Face Emotion Tree Structure
Representation with Probabilistic Recursive Neural NetworkModeling,‚Äù Neural Computing and Applications, vol. 19, pp. 33-54,
2010.
[47] K. Schindler and L. van Gool, ‚ÄúAction Snippets: How Many
Frames Does Human Action Recognition Require?‚Äù Proc. IEEE
Conf. Computer Vision and Pattern Recognition, pp. 1-8, 2008.
[48] A. Hadid and M. Pietikainen, ‚ÄúCombining Appearance and
Motion for Face and Gender Recognition from Videos,‚Äù Pattern
Recognition, vol. 42, pp. 2818-2827, 2009.
Ligang Zhang is currently working toward the
PhD degree at the Queensland University of
Technology, Brisbane, Australia. His researchinterests include face-related technologies (e.g.,face expression recognition and face recogni-tion), affective computing, affective contentanalysis, pattern recognition, and computervision. He is a student member of the IEEE.
Dian Tjondronegoro is an associate profes-
sor on the Faculty of Science and Technology
at Queensland University of Technology. He
leads the Mobile Multimedia Research Groupand has published more than 60 refereedarticles in the field. His research interestsinclude video analysis, summarization, andvisualization, multichannel content analysis,mobile applications, and interaction design.
.For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.ZHANG AND TJONDRONEGORO: FACIAL EXPRESSION RECOGNITION USING FACIAL MOVEMENT FEATURES 229

"
https://ieeexplore.ieee.org/document/8371638,"1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2844085, IEEE
Transactions on Multimedia
> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 
 1 
 
Abstract‚ÄîFacial expression recognition (FER) has long 
been a challenging task in computer vision. In this paper, 
we propose a novel method, named Deep Comprehensive 
Multi-patches Aggregation Convolutional Neural Networks 
(DCMA-CNNs), to solve FER problem. The proposed 
method is a deep-based framework, which mainly consists 
of two branches of Convolutional Neural Network (CNN). 
One branch extracts local features from image patches 
while the other extracts holistic features from the whole 
expressional image. In our model, local features depict 
expressional details and holistic features characterize the 
high-level semantic information of an expression. We 
aggregate both local and holistic features before making 
classification. These two types of hierarchical features 
represent expressions in different scales. Compared with 
most current methods with single type of feature, our 
model can represent expressions more comprehensively. 
Additionally, in the training stage, a novel pooling strategy 
named Expressional Transformation-invariant pooling 
(ETI-pooling) is proposed for handling nuisance variations, 
such as rotations, variant illuminations, etc. Extensive 
experiments are conducted on the famous CK+ and JAFFE 
expression datasets, where the recognition results obtained 
by our model are superior to most existing FER methods.  
 
Index Terms ‚ÄîConvolutional neural network, expressional 
transformation-invariant, facial expression recognition , feature 
aggregation. 
I. INTRODUCTION  
ACIAL  expressions are important carriers for human to 
convey emotions in communications. Study on nonverbal 
communication [1] reveals that 55% of a person‚Äôs emotional or 
intentional information is conveyed through facial expressions. 
Recently, researches on emotion al analysis have made great 
achievements. On one hand, development of neuroscience and 
cognitive science well propel the progress of emotional 
analysis. On the other hand, technical advance in computer 
vision and machine learning makes applications related to 
emotional analysis available to the public. As an important 
subfield of emotional analysis, researches on facial expression 
recognition (FER) develop quickly as well. Applications based 
on FER can be found in many cases, such as human-computer 
interaction system [2], multimedia [3], surveillance [4] and driver safety [5]. 
Systematical studies on facial expression analysis can date 
back to the work of Ekman et al. [6]. The main target of facial 
expression analysis is to establish a system that can 
automatically classify different expressions. In general, facial 
expressions can be categorized into six basic expressions [7], 
which include anger, disgust, fear, happiness, sadness and 
surprise. Therefore, the primary task of current expressional 
analysis is to classify these six basic expressions. 
Previous methods on FER can be categorized into two 
groups: detecting facial actions that are related to a specific 
expression or making classification based on the extracted 
image features. In Facial Action Coding System (FACS) [8], 
expressions are encoded by Action Units (AUs), which refer to 
some tiny but discriminable facial muscle changes. Thus, 
researchers usually convert FER problem to the task of AU 
detection. Some other methods represent expressions by some 
hand-crafted patterns or features, which can be utilized to train 
an expression classifier. However, some intractable problems 
are inevitable for these methods. For example, when encoding 
expressions by AUs, it‚Äôs hard for researchers to accurately 
detect every AU in an image as facial muscle moves are 
sometimes difficult to be track ed. It is also hard to design a type 
of feature that can adapt to different environments. Variations 
such as illuminations and image rotations can weaken the 
representative capacity of hand-crafted features. 
In recent years, Convolutional Neural Network (CNN) has 
achieved great success in the field of computer vision. It can 
adaptively adjust its convolutional kernels of each layer to 
obtain some desired features, which makes it adapt to various 
classification problems without much prior knowledge. In 
general, the output of CNN, i.e. the feature maps of the last 
layer, will be treated as the high-level semantic concept to 
represent the input. In FER problem, previous studies [31], [32] 
have revealed that expressional changes usually occur on some 
salient facial regions such as neighborhood of mouth, eyes and 
nose. This implies that details of local facial regions can be 
discriminative for expressional recognition. However, most 
current CNN-based methods on FER only extract features from 
the whole expressional image. These methods emphasize the 
integrality of a facial expression but ignore the information of 
local details. In the other words, typical CNN can ‚Äôt take full 
advantage of recognition-effective information encoded in Siyue Xie and Haifeng Hu   Facial Expression Recognition Using 
Hierarchical Features with Deep Comprehensive 
Multi -Patches Aggregation Convolutional 
Neural Networks  
F 
1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2844085, IEEE
Transactions on Multimedia
> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 
 2 
expressional images. Therefore, we attempt to make some 
modification in commonly used CNN architecture for 
improving performance on FER problem. 
In this paper, we present a novel framework, named Deep 
Comprehensive Multi-Patches Aggregation Convolutional 
Neural Networks (DCMA-CNNs), for solving the FER 
problem. Fig.1 illustrates the flowchart of our method. The 
framework consists of two individual CNN branches: one 
extracts holistic features from the whole image and the other 
extracts local features from some overlapped image patches. 
Holistic features aim to represent the integrity of an expression 
while local features focus on describing details on local regions, 
which can indirectly indicate some active expressional regions 
on the face. These two separated branches represent an 
expressional image from two different scales, each of which is 
complementary to the other. Compared with current works, 
which mostly characterize expressions using single type of 
features, DCMA-CNNs can represent an expressional image 
more comprehensively by aggregating two types of hierarchical 
features. In addition, we improve the TI-pooling [9] and 
propose the Expressional Transformation-invariant pooling 
(ETI-pooling) strategy for handling nuisance variations such as 
illuminations, poses and noises. ETI-pooling enhances the 
discriminative ability of our model by fusing a certain number 
of features that from the same class. This pooling strategy is 
applied to modify the structure of typical CNN and well 
improve the recognition performance. Extensive experiments 
are conducted on the CK+ and JAFFE dataset and classification 
results demonstrate the effectives of the proposed method. 
Contributions of our model can be summarized as follow: 
(1) A novel two-branch framework is proposed to solve the 
challenging FER problem. One branch is implemented 
to extract local features from image patches, which 
highlights the detailed information of facial expressions. 
Salient patches and active regions related to FER 
problem can be determined and visualized based on the 
extracted local features. 
(2) We incorporate both the holistic and local features in to 
our model. These two types of hierarchical features 
represent images in different scales. By aggregating 
these features, we can obtain the image representation with more discriminative power, which significantly 
improves the classification performance. 
(3) We modify typical CNN structure with the proposed 
ETI-pooling, which can distinguish expression-sensitive 
elements from extracted features . Owing to the 
modification, our model can be more robust to some 
variations such as illuminations, image rotations, noises, 
etc. 
The remainder of this paper is organized as follow. We make 
a brief review of some existing works on FER in section II. The 
details of DCMA-CNNs will be specialized in Section III. In 
Section IV, experiments and result analysis are presented. We 
conclude our method in the Section V.  
II. RELATED W ORKS  
Expression recognition has long been a challenging problem 
in emotional analysis. Conventional approaches can be 
generally categorized into two groups: AU-based and 
feature-based approaches. Recently, methods based on 
deep-learning algorithms make great achievements in 
expression recognition, which is regarded as an effective 
alternative to solve FER problem.  
AU-based methods attempt to detect expression-related AUs 
on a facial image, which are inspired by the studies on FACS 
[10]. Researchers can recognize an expression according to the 
combination of detected AUs [11], [12], [13], [14], [15]. Some 
other methods even focus on 3D AU detection [16], [17]. As for 
feature-based method, hand-crafted patterns or features are 
usually used to represent an expressional image. Geometric 
relationships among facial organs or landmark points are 
typical features to represent expressions [ 19], [20], [21], [ 22]. 
Some image operators , such as Local Binary Patterns (LBP) 
[23], Local Description Patterns (LDP) [ 24], Gabor-features 
[25] Local Phase Quantization (LPQ) [ 26], [27] or Scale 
Invariant Feature Transform(SIFT) [ 28], are used in expression 
analysis as they can extract significant information from 
images. Moreover, some methods divide images into patches 
with different scales. Features extracted from these patches can 
highlight some detailed information of local facial regions [ 29], 
[30], [31], which significantly improve the recognition 
performance. To enhance the comprehensive representation 
Fig. 1  Flowchart of the proposed method  

1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2844085, IEEE
Transactions on Multimedia
> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 
 3 
ability, methods like [32 ] even fuse different types of features 
before making classification. 
In recent years, deep-based algorithms have been applied to 
expression recognition. Zhao et al.  [18] propose the Deep 
Region and Multi-label Learning (DRML) algorithm to 
conduct AUs detection, which addresses  the problem of Region 
learning (RL) and multi-label learning (ML). Li et al . [36] 
extract Gabor-wavelet features from images to train a deep 
network for FER task. Mollahosseini et al.  [37] construct a 
CNN with inception layers. Their experiments are conducted 
on seven standard face datasets and obtained comparable 
results. Liu et al.  [38] model a system named Boosted Deep 
Belief Network (BDBN) to classify different expressions. They 
divide expressional images into patches. Some patches with 
high discriminative power will be selected and combined to 
train a strong classifier.  
Some works aggregate or fuse different types of features 
through deep network to produce a comprehensive 
representation, which usually results in a better recognition 
result than using single type of features. For example, 
Majumder et al.  [32] extract LBP features as well as facial 
geometrical features from expressional images. These two 
types of features are finally fused by a 2-layer autoencoder. 
Hamester et al.  [39] construct 2-channel architecture for feature 
extraction, which utilizes CNNs and an autoencoder to extract 
features. Jung et al. [40] train a CNN and a deep neural network 
independently and combine them through fully connected 
layers with joint fine-tuning.  However, it should be noted that 
most existing deep-based methods merely focus on extracting 
high-level semantic concept of expressions but ignore 
fine-grained information in local facial regions. Different from 
some existing works, in this paper, we present an individual 
CNN branch to extract local features, which highlights the 
importance of local detailed information in expression analysis.  III. PROPOSED METHOD  
 The proposed model is based on a two-branch CNN 
framework, as shown in Fig.2. In order to extract 
variation-robust features, we propose the ETI-pooling strategy 
to modify the typical structure of CNN, which forms the CNN 
module. A CNN module is a basic unit for feature extraction in 
the proposed DCMA -CNNs model , and it helps extract local 
and holistic features from an input image.  
In the following subsections, we briefly review the structure 
of typical CNN (Section III.A) and then describe the special 
case of obtain ing the desired CNN module by adopting 
ETI-pooling (Section III.B). The obtained CNN module will be 
used to construct the DCMA-CNNs model (Section III.C). 
Additionally, a salient patch learning algorithm is presented 
based on the proposed DCMA-CNNs model (Section III.D). 
This algorithm can indicate some active facial regions that are 
relevant to changes in expression.   
A. Convolutional Neural Network 
CNN has been successfully applied in many tasks related to 
computer vision in the recent years. Typically, a CNN 
comprises stacks of multiple convolutional layers and pooling 
layers. In general, the features generated by the deeper layers 
can represent the content of a larger region in the input image. 
Feature maps yielded by the last layer can be treated as the 
Fig. 2  Framework of DCMA -CNN s 
Fig. 3  A  typical structure of CNN.  

1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2844085, IEEE
Transactions on Multimedia
> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 
 4 
representation of the input. The typical structure of a CNN is 
shown in Fig.3. 
In our model, the CNN module has a fo ur-layer network 
structure . Convolutional layers and pooling layers are stack ed 
up alternately to forms the backbone of the CNN module. The 
output feature maps in the final layer serve as the high-level 
semantic concept of the input, and they are used for the 
processing as described in the following subsections. 
B. CNN with Expressional Transformation-invariant Pooling 
In the stage of feature extraction, variations such as 
illumination or image rotation can degrade the representative 
capacity of the extracted features. One approach commonly 
used to address this problem is dataset augmentation. However, 
this approach has some side-effects. Models trained with data 
augmentation still need to learn feature representations 
separately for different variations. Extra transformation of data 
can mislead models to learn some useless information from 
noises samples or wrong labels [9]. 
In order to reduce the negative impact of image variations 
and to overcome the disadvantages of data augmentation, we 
introduce a novel feature-fusing strategy, namely, Expressional 
Transformation-invariant pooling (ETI-pooling), to modify the 
typical CNN structure. It follows the concept of TI-pooling [9], 
which was proposed to handle one specific variation in images. 
TI-pooling accumulates all responses of the extracted features 
and uses their maximal value. Such processing allows for more 
efficient data usage and can help discover ‚Äúcanonical ‚Äù 
instances, leading to the generation of variation-independent 
and transformation-invariant features.  
Unlike TI-pooling, our ETI-pooling scheme was extended to 
handle different variations simultaneously. In our method, 
different variations can be regarded as different transformations 
for images in the same class. For example, for two images with 
expressions of anger but different illuminations, one image can 
be regarded as a transformation of the other. We can 
accumulate the responses of all samples from the same class 
instead of treating them individually. Such processing not only 
allows TI-pooling to learn from ‚Äúcanonical ‚Äù instances, but also 
yields features that are robust to different variations. 
The main structure of ETI-pooling is illustrated in Fig.4. To 
extract invariant features from expressional images, we train 
the network using samples from the transformation sets, which consist of a certain number of expressional images with the 
same class label. In the training stage, transformation sets are 
treated as the input of the networks. Each image of a 
transformation set will be fed into a channel, i.e., an 
expressional image corresponds to a channel of the CNN 
module. In each channel, the input image will pass through a 
CNN, whose interior structure is similar to that shown in Fig.3. 
The output of each channel comprises vectorized features that 
are denoted via a concatenated feature vector (refer to Fig.3). In 
the training stage, weight-sharing is implemented among these 
parallel channels, indicating that the model only requires the 
same amount of memory as one CNN. The output features of 
every individual channel will finally be fused to generate a 
feature vector. 
The concatenated feature vector of the k-th (ùëò = 1,2, ‚Ä¶ , ùëÄ)  
channel is denoted as ùíõùëò=(ùëß1ùëò, ùëß2ùëò, ‚Ä¶ , ùëß ùë°ùëò, ‚Ä¶ , ùëßùëÅùëò), where N is 
the dimension of the vector and ùëßùë°ùëò ( ùë° = 1,2, ‚Ä¶ , ùëÅ)  is the t-th 
element of the vector. The vector obtained after ETI-pooling 
can be denoted as ùíõ = (ùëß 1, ùëß2, ‚Ä¶ , ùëß ùë°, ‚Ä¶ , ùëß ùëÅ). The non-linear 
operator conducts an element-wise fusion operation across all 
M channels, formulated as follows: 
ùëßùë°=ùëöùëéùë•  (ùëßùë°1, ùëßùë°2, ‚Ä¶ , ùëß ùë°ùëÄ)                    Ôºà3Ôºâ 
This operation only responds to the maximal element in the 
same position among different feature vectors, which lowers 
the probability of propagating an odd variation-related feature 
to the following parts. In the other words, ETI-pooling attempt 
to improve the classification performance by highlighting the 
most discriminative feature elements that are beneficial to most 
instances. Such non-linear operations increase the competition 
among different channels, inducing networks to learn 
adaptively the expression-discriminative features and to 
suppress the back propagation of information related to 
nuisance variations . Unlike the max-pooling strategy, ETI- 
pooling considers all instances in a transformation set. This 
makes it efficiently utilize all available and effective 
information concealed in multiple expressional images. By 
training the network using transformation sets, the network can 
learn invariant features across variations. This is beneficial to 
classification as described below.  
C. DCMA- CNNs for Classification 
Many existing deep-based models for facial expressional 
analysis are built on single-branch CNN [36], [37], [42] , 
which usually focus es on extracting features from a whole 
image but ignores detailed descriptions. However, some 
studies [21], [29], [31] validated the effectiveness of local 
expressional features in solving the FER problem. This means 
that both holistic and local features are important for 
expressional analysis. Some recent methods fuse features from 
different representative spaces [39], [32], [40] or aggregate 
different features [43], [44] that achieved satisfactory 
performance in different classification tasks. It is reasonable 
that fused or aggregated features contain more 
classification-effective information than a single type of 
features. Motivated by these works, we attempt ed to 
incorporate both holistic and local information and aggregate 
them for improving the representation ability of our model. 
As shown in Fig. 2, the DCMA-CNNs consist of two 
Fig. 4  Modified CNN structure with ETI -pooling (a CNN module).  

1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2844085, IEEE
Transactions on Multimedia
> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 
 5 
branches. One extracts holistic features from input images, 
while the other focuses on local feature extraction. The output 
features obtained from the two branches are aggregated and 
used in expression recognition task.  
In the branch of holistic feature extraction, images from a 
transformation set are treated as the input of the CNN module. 
The output of the branch is the holistic feature vector, which 
represents a set of images with different transformations. In 
the other branch, original images are equally divided into 
image patches. Individual CNN modules are implemented to 
extract the local features with respect to each patch. All the 
steps of local feature extraction are the same as those of the 
holistic feature extraction except for some detailed 
configuration of CNN modules. Although both branches are 
based on CNN, the difference in input data results in 
complementary features with different semantic concepts. The 
local branch focuses on extracting patch-specific semantic 
concepts, while the holistic branch concentrates on extracting 
an abstract representation from the whole image. Aggregation 
of these two branches not only increases the number of 
extracted features, but also improves the representative ability 
of our model. 
In this paper, we denote the holistic feature vector as ùíó‚Ñé0, and 
the m-th local feature vector as ùíóùëôùëö, where ùëö = 1,2, ‚Ä¶ , ùêø , L is 
the number of local feature vectors, which equals the number of 
image patches. The aggregated feature vector ùíóùëé is obtained by 
concatenating holistic feature vector and all local feature 
vectors, which can be formulated as follows: 
ùíóùëé= (ùíó‚Ñé0; ùíóùëô1; ùíóùëô2; ‚Ä¶ ; ùíóùëôùëö; ‚Ä¶ ; ùíóùëôùêø)                  (4) 
The aggregated feature vector will be fed into a 
fully-connected layer for feature fusion. A classifier is followed 
by the fully-connected layer, which maps the output of the 
previous layer to an expression class. In our method, we choose 
softmax as the classifier, which can be formulated as shown 
below: 
ùëì(ùíõ ùíã) =  ùëíùíõùíã
‚àëùëíùíõùíäùëñ                                      (5) 
Each element of equation (5) corresponds to a unique 
expression class. The class with the maximal element value is 
regarded the predicted class . Thus, the loss function of our 
model can be formulated as follows: 
ùêø =1
2‚Äñùíöùíë‚àí ùíöùíà‚Äñ2=1
2‚àë (ùë¶ùëêùëù‚àí ùë¶ùëêùëî)2ùê∂
ùëê=1              (6) 
where ùíöùíë is the predicted label; ùíöùíà is the ground truth label; 
ùë¶ùëêùëùand ùë¶ùëêùëî are the c-th element of ùíöùíë and ùíöùíà respectively. C is 
the total class number of expressions. All related parameters of 
the method can be updated through back-propagation using 
the stochastic gradient descent (SGD). 
D. Learning Salient Patches for Expression Analysis 
Intuitively, expressions can be regarded as regional changes 
in the appearance where the changes are triggered by some 
facial muscles. This means that analysis of expressions can be 
associated with limited facial regions. Some previous works on 
FER are based on the study of local regions [ 29], [31], [32]. In 
this subsection , we define the salient patches as the image patches that make large contribution to the expression 
recognition task. Because local regions can be represented by 
the extracted local features, we can detect the se salient patches 
by analyzing the features extracted from them. 
 In classification tasks, features with large norms are much 
more discriminative than random features [ 43]. Therefore, we 
can try to detect salient patches according to the L2-norm value 
of the extracted local feature vectors. We assume that the 
discriminative power of a local feature vector is proportional to 
the magnitude of its L2-norm.  
Based  on this assumption, we can bypass some patches with 
a small norm but preserve patches with a large norm. The 
procedure of learning salient patches is implemented in two 
steps in the training stage: For the first step, all feature vectors 
are involved in the classification task. As the training loss 
converges, the second step is initiated to learn salient the 
patches and to continue fine- tuning  the model. The general 
procedure of selecting salient patches is summarized in 
Algorithm 1. We record the norm of each local feature vector in 
each iteration and compute the Averaged Cumulative 
Summation (ACS) of the norm of each vector every T iterative 
epochs. According to our assumption, vectors with larger ACS 
play a more important role in recognition. The vector with the 
smallest ACS contributes the least; in other words, we can 
bypass this patch and exclude it in the next iteration when 
continu ing fine-tuning. By successively removing irrelevant 
patches from the input, we can optimize the model and increase 
the representation efficiency. 
IV. EXPERIMENTAL EVALUATION  
In this section, a number of experiments are carried out on 
two publicly available facial expression datasets: the Extended 
Cohn-Kanade (CK+) dataset [45] and the Japanese Female 
Facial Expression (JAFFE) database [46 ]. Experiments are in 
support of the following objectives: 
(1) Evaluate the performance of DCMA-CNNs on FER task 
by the recognition accuracy and compare it with some 
competing conventional approaches as well as 
deep-based methods.  
(2) Investigate the role of aggregated features and ETI- 
pooling in solving FER problem. 
Algorithm 1  Leaning Salient Patc hes for Expression Analysis  
Input : Localized feature vectors set Œ¶={v1,v2,‚Ä¶,vm,‚Ä¶,vN} 
corresponds to N patches, Salient patches number P, bypass period T  
1: Initialize averaged cumulative  norm summation set 
Œ©={ùíî1,ùíî2, ‚Ä¶ ,ùíîùëö, ‚Ä¶ ,ùíîùëÅ} for N patches, ùíîùëö= 0,ùëö= 1,2, ‚Ä¶ , ùëÅ; 
2: Begin training until the training loss converges; 
3: While number of elements in ùõ∑ has not met P , do 
   for ùë°= 1:ùëá do 
      Fine-tune the model with localized feature vectors in Œ¶; 
      for each  element in  Œ© do 
           ùíîùëö‚Üêùíîùëö+1
ùëá‚Äñùíóùëö‚Äñ2; 
      end for each 
   end for 
   ùëò=ùëéùëüùëîùëöùëñùëõ ùëöùíîùëö; 
   Œ¶‚ÜêŒ¶‚àíùíóùëò; 
   Œ©‚ÜêŒ©‚àíùíîùëò; 
end while 
4: Output : Localized feature vectors set Œ¶ 
 
1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2844085, IEEE
Transactions on Multimedia
> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 
 6 
Fig. 5  Example of the six prototypic expressions.  
(a) the CK+ dataset   
(b) the JAFFE database 
(left to right: anger, disgust, fear, happiness, sadness, surprise).  
(a) 
(b) (3) Investigate the effect of salient patch learning algorithm 
and the impact on recognition accuracy after bypassing 
some irrelevant patches.  
A. Experimental Data 
Experiments are conducted on CK+ and JAFFE datasets. For 
each dataset, six classes of basic expressions, i.e., anger, disgust, 
fear, happiness, sadness and surprise, are chosen as the target of 
our classification task.  
The CK+ dataset contains 327 expression-labeled image 
sequences from 123 subjects. All sequences are from the 
neutral face to the peak expression. Some samples of the CK+ 
dataset are shown in Fig.5 (a). 309 sequences with one of the 
six prototypical expressions are selected to conduct 
experiments. For each chosen sequence, the last 3 frames with 
peak expression are collected to form the training and testing 
sets, i.e., 927 images are involved in our experiments. 
The JAFFE database contains 213 images of 7 facial 
expressions posed by 10 Japanese females. 183 images with six 
basic expressions are chosen for the experiment. Fig.5 (b) 
shows some samples of the JAFFE database. As for JAFFE 
dataset, data augmentation is applied for improving recognition 
performance. Images are horizontally flipped, which obtains 
the corresponding mirror image. Each image is rotated by the 
angles of 5¬∫  clockwise and counterclockwise. Furthermore, 
Gaussian noise with a zero mean and 0.01 variance is added to 
the original image. Thus, the sample set of experiments on 
JAFFE database is largely extended, which contains 915 facial 
expression images. 
To simplify the recognition task, human faces in all selected 
images are detected by the Viola-Jones faces detector [47 ]. 
Facial regions are cropped from images and resized to the scale of 60√ó60. These resized facial expression images are then 
equally divided into partially overlapped patches, which forms 
the sample set of the branch of local feature extraction.  
B. Experimental Settings 
 In our experiment, two individual branches are set with 
different CNN structural configuration. Table I shows the 
detailed configuration of the CNN module in each branch. The 
stride of the convolutional layer and the max-pooling layer is 
set as 1. The convolutional operation is conducted without 
padding. When applying the ETI-pooling, we set 3 parallel 
channels in the model. 
In the training stage, the learning rate Œ± decays in an 
exponential form, which can be formulated as: 
ùõº =  ùõº 0√ó 0. 95‚åäùëñ
10‚åã                                  (7) 
where ùõº0= 0.1  is the initial value, i indicates the i-th iterative 
epoch of the training stage and symbol ‚åäùë•‚åã refers to the largest 
integer that is smaller than ùë•. In our experiment, images are 
divided into 3 √ó 3  patches with overlap. The overlap ratio 
between two adjacent patches is 0.5.  
To evaluate the performance of the proposed method, 
10-fold cross-validation is applied to all the experiments (i.e. 
images are randomly divided into ten equal-sized subsets, nine 
for training and one for testing). To fairly compare with other 
methods, in CK+, all ten subsets are subject-independent. As 
for the JAFFE database, an original image and its augmented 
images can only be allocated either to training or testing sets. 
The final results are reported by averaging the recognition 
accuracy of all ten folds experiments. 
C. Expression Recognition Results 
1) Experiments on the CK+ Dataset 
 Table II is the confusion matrix obtained by DCMA-CNNs 
model, which shows the detailed classification result of all six 
classes of expressions. To evaluate the performance of the 
DCMA-CNNs model, we compare the recognition accuracy of 
our methods with several competitive approaches, including 
conventional methods (LAP [ 48], MPSD [ 49] and MCSPL [ 29]) 
and deep-based methods (DTAN [ 50], DTGN [50] and 
3DCNN-DAP [51]). In addition, we reproduce the model of the 
work of Khorrami et al.  [56], which is a recently-proposed 
deep-based model and is denoted as Khor-net in the following. 
Table III shows the result of these comparative studies.  
TABLE  I 
Network Configuration 
  Layer1  Layer2  Layer3  Layer4  Layer5  Layer6  
Holistic Branch  type Input  Convolutional  Max Pooling  Convolutional  Max Pooling  Fully -connected  
size - 5√ó5√ó6 2√ó2 5√ó5√ó18 2√ó2 
8424 √ó6 Localized  
Branch  type Input  Convolutional  Max Pooling  Convolutional  Max Pooling  
size - 3√ó3√ó6 2√ó2 3√ó3√ó18 2√ó2 
 
1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2844085, IEEE
Transactions on Multimedia
> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 
 7 
From Table II and Table III, we can observe the following. 
(1) The proposed method achieves an averaged recognition 
accuracy of 93.46% in the CK+ dataset. DCMA-CNNs 
performs the best when recognizing the expression of 
surprise. Some samples of fear and sadness are 
misclassified as anger. This is because expressions of 
fear and anger have some similar actions in local facial 
regions, such as fall of eyebrow and rise of the upper 
eyelid. In FACS, these two expressions have shared 
AUs. The same phenomenon appears when dealing with 
sadness and anger. This may be due to the reason that 
only 75 samples of fear and 84 samples of sadness are 
used for training, which is far less than that of other four 
expressions.    
(2) The performance of DCMA -CNNs is far better than 
these  conventional methods. LAP and MCSPL extract 
LBP features to represent expressions while MPSD 
utilizes SIFT features. All these features are artificially 
predefined and incapable to be adaptive to some 
complex environments. Compar ed with these methods, 
our deep- based model learns features adaptively, which 
are more robust than hand- crafted features.  
(3) DCMA- CNNs outperforms the competitive deep 
models. The deep -learning based methods such as 
DTAN and DTGN utilize one -branch architecture to 
extract desired features while our method extracts 
features with an additional branch, which leads to a 
more comprehensive representation of expressions. The 
result indicates that the branch of local feature extraction 
really benefits expression classification.  
(4) DCMA -CNNs outperforms Khor- net. This is because 
Khor -net is based on the structure of typical CNN while 
our model adopt ETI-pooling and feature aggregation schemes . Thus our DCMA -CNNs model can capture 
more detailed information and is robust to different 
variations. Note that the recognition accuracy of 
Khor -net in Table III is different from its reported result 
in [56]. Although we use the same dataset (CK+), 
validation settings (10 -fold cross -validation) and 
network architecture as [56], images used in each fold of 
experiments can be different as training and testing 
samples are randomly selected from dataset. Such 
difference results in the fluctuation of the final 
recognition performance.  
2) Experiments on the JAFFE Database 
Details of classification result are shown in the confusion 
matrix in Table IV. In Table V, we compare the proposed 
DCMA-CNNs model with some other existing works.  
From Table IV and Table V, some conclusions can be 
summarized as the following.  
(1) The proposed method achieves an averaged recognition 
accuracy of 94.75% in the JAFFE database. The best 
averaged recognition accuracy occurs when recognizing 
the expression of anger, which reaches the accuracy of 
97.04%. The averaged recognition accuracy on fear is 
relatively lower than all others but still reach 88.89%. In 
the experiment, nearly 5% samples of fear expressions 
are misclassified as surprise. A reasonable explanation is 
that the expression of fear and surprise act similarly in 
several facial regions such as the inner brows and the 
upper eyelids. We can also find that these two 
expressions are coded by some identical AUs in FACS.  
(2) The proposed DCMA -CNNs model outperforms all 
other competitive methods. In Table V , some methods 
using hand -crafted features are involved in comparison 
but the recognition accuracy of them is far lower than 
the proposed model, which validates the effectiveness of 
Table  II 
Confusion Matrix of DCMA-CNNs on CK+ (%) 
(An: anger, Di: disgust, Fe: fear, Ha: happiness, Sa: sadness, Su: surprise) 
 An Di Fe Ha Sa Su 
An 90.74  0 0 0 9.26 0 
Di 1.85 95.06  1.85 0 1.23 0 
Fe 11.11  0 74.07  3.70 0 11.11  
Ha 0 2.12 3.17 94.71  0 0 
Sa 12.35  1.23 1.23 0 81.48  3.70 
Su 1.39 0 0.46 0.46 0 97.69  
 
Table III  
Performance Comparison with Existing Works on the CK+ Dataset (%) 
Method  Accuracy  
LAP [48]  88.26  
MPSD [49]  88.52  
Khor -Net [56]  91.25  
MCSPL [29]  91.53  
DTAN [50]  91.44  
DTGN [50]  92.35  
3DCNN -DAP [51]  92.40  
DCMA -CNNs (proposed)  93.46  
 
Table IV  
Confusion Matrix of DCMA-CNNs on JAFFE (%) 
 An Di Fe Ha Sa Su 
An 97.04  2.22 0 0 0 0.74 
Di 3.97 93.65  0.79 0 0.79 0.79 
Fe 2.78 1.39 88.89  0.69 1.39 4.86 
Ha 0.69 0 0.69 95.14  0.69 2.78 
Sa 2.22 3.70 2.22 0 90.37  1.48 
Su 0 0.74 1.48 2.22 0 95.56  
 
Table V  
Performance Comparison with Existing Works on the JAFFE Database 
(%) 
Method  Accuracy  
McFIS [52]  87.60 
LPTP [53]  90.20 
SRC+LBP [54]  90.30 
SRC+Gabor [54]  91.21  
SRC+LPQ [54]  91.67  
SFPL [31]  91.80 
LSDP [55]  92.30 
DCMA -CNNs (proposed)  94.75  
 
1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2844085, IEEE
Transactions on Multimedia
> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 
 8 
deep -based features.  
(3) The proposed method is robust to small variations. 
Images with variations such as rotations and noises are 
involved in the experiment on JAFFE but DCMA -CNNs 
can still correctly classify most expressions, which 
demonstrates the robustness of the proposed method.  
3) Experiments across Databases 
To evaluate the generalization capability of our model, we 
conduct an experiment on DCMA-CNNs across different 
databases. To be specific, we train DCMA-CNNs with the data 
from the JAFFE databases and validate it on the CK+ dataset.  
Note that there are few works had conducted cross-databases 
testing on facial expression recognition. It is hard to make a full 
and fair comparison with other models. Here we compared our 
model with the work of da Silva et al.  [57]. To make a fair 
comparison, our experimental settings and preprocessing 
follow the work of da Silva et al.  [57]. The results are shown in 
Table VI . From the table, we can observe that DCMA-CNNs is 
still comparable to the work of [57], which demonstrate the 
effectiveness of our model.  
D. Comparison Studies on DCMA-CNNs Properties 
In our method, we aggregat e two types of features and 
introduce the ETI-pooling into the model. To assess these two 
properties, we conduct some experiments on the CK+ dataset to 
evaluate their effect on recognition. 
1) The Effects of Feature Aggregation 
We construct another two models to take on the evaluation 
task. The model only utilizes holistic features to make 
classification is denoted as HFCNN. The model that recognizes 
expressions only with local features is denoted as LFCNN. The 
recognition performances of these two models are listed in 
Table VII. 
From Table VII, we can observe that the recognition 
accuracy of DCMA-CNNs is much higher than HFCNN and LFCNN, which means that feature aggregation really improve 
expression recognition. This is reasonable as holistic features or 
local features only focus on representing expressional 
information with a specific scale. The improvement on 
recognition accuracy by aggregation indicates that these two 
kinds of features are complementary to each other. 
2) The Effects of ETI-pooling 
In this experiment, t he model without ETI-pooling is denoted 
as DCMA-CNNs-WTE. In other words, in this model, the CNN 
module is replaced by a typical CNN structure as shown in 
Fig.3. We compare the performance of DCMA-CNNs-WTE 
with the proposed model. The recognition result is shown in 
Table VIII. 
From Table VIII, we can see that the proposed model 
performs better than DCMA-CNNs-WTE. This can be owed to 
the parallel structure and the non-linear operator of 
ETI-pooling. These two operations help distinguish 
discriminative elements in the feature vector and suppress 
nuisance variations collaboratively. The improvement in 
recognition accuracy demonstrates the effectiveness of 
ETI-pooling.  
E. Experiments on Learning Active Regions 
Following the assumptions and the learning algorithm we 
suggested in the Section III.D, we attempt to make a selection 
of expressional patches in our experiments. The period of 
selecti on is 30 epochs in the experiments on both the CK+ 
dataset and JAFFE database, i.e., we bypass an image patch 
every 30 iterative epochs during the procedure of fine-tuning. 
Experiments follow the principle of 10-fold cross-validation 
and we finally retain 5 patches in the end of each fold of 
experiment. We count the times that a patch is retained  after all 
ten folds of experiments. The more times a patch is retained, the 
more importantly it acts in the expressional analysis. Note that 
an expressional image is divided by the scale of 3√ó 3 with 50% 
overlap area between two adjacent patches, an image can be 
seen as being equivalently divided into 4√ó 4 disjoint regions in 
the visualization task. Each patch is composed of four regions. 
Regions in the retained patch can be regarded as active regions 
whereas regions in bypassed patches are regarded as less-active 
regions. 
We visualize the salient patches and their corresponding 
active regions in Fig.7. The intensity of red color of a region is 
proportional to the times it is retained in the 10-fold 
cross-validation experiments. We can observe that in both the 
CK+ dataset and the JAFFE database, the ‚Äúmost active ‚Äù regions 
are located around some primary facial organs such as eyes, 
nose and mouse. Some marginal regions are with low intensity, 
which means that these ‚Äúless-active ‚Äù regions and their 
corresponding patches contribute less to expression recognition. 
Distribution of active regions roughly accord with the intuition 
of places where actions of an expression should take place in 
face.  
The recognition performance after bypassing less-active 
patches is listed in Table IX. We denote DCMA-CNNs-5p as 
the model trained with five retained patches. We can observe 
that the averaged recognition accuracy of DCMA-CNNs-5p in 
the CK+ dataset is slightly lower than the accuracy of 
DCMA-CNNs. This is reasonable as DCMA-CNNs can make 
use of more local information for making classification. 
Table VII  
Recognition Accuracy on CK+ with Different Types of Features (%) 
 Accuracy  
DCMA -CNNs  93.46  
HFCNN  90.67  
LFCNN  88.67  
 
Table VIII  
Effect of ETI-pooling on CK+ Dataset (%) 
 Accuracy  
DCMA-CNN s 93.46  
DCMA-CNN s-WTE  92.67  
 
Table IX  
Recognition Accuracy of DC MA-CNNs with and without Patches 
Exclusion in CK+ and JAFFE (%) 
 CK+  JAFFE  
DCMA -CNNs  93.46  94.75  
DCMA -CNNs -5p 92.10  94.75  
 
Table VI  
Generalization performance of DCMA-CNNs across databases (trained on 
JAFFE, validated on CK+) (%) 
 Accuracy  
da Silva et al.  [57] 48.20  
DCMA -CNNs  46.28  
 
1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2844085, IEEE
Transactions on Multimedia
> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 
 9 
However, these two models achieve the same averaged 
recognition accuracy in the JAFFE database, which implies that 
the retained five patches have contained most discriminative 
information of expressions. To sum up, the proposed model and 
the salient patch learning algorithm help distinguish sensitive 
patches that are related to the FER problem. 
We conduct another experiment to verify the assumption 
mentioned in Section III.D, i.e., patches with larger ACS plays 
a more important role in FER. To be specific, w e prune the 
local branch of DCMA-CNNs. Each pruned DCMA-CNNs can 
extract features from only one specific patch. Th us we can 
totally construct nine different pruned models corresponding to 
each different patch . The recognition performance of each 
network is shown in Fig.8, where we sort all patches by the 
magnitude of ACS ( in ascending order). From the figure, we 
can observe that models with the patch that have larger ACS 
tend to have a better recognition performance. In both the CK+ 
and JAFFE dataset, the model with the largest patch ACS 
achieves the highest recognition accuracy. This verifies our 
assumption that patches with larger ACS can be more important 
in FER task. 
V. CONCLUSIONS  
In this paper, a novel method, named DCMA-CNNs, is 
proposed for facial expression recognition. The architecture of 
our proposed model consists of two individual CNN branches. 
One of the branches conducts holistic features extraction on the 
whole expressional images while the other extracts local 
features from segmented expressional image patches.  These 
two types of hierarchical features depict an expressional image 
in two different scales and being complementary to each other. 
Expressional images represented by these two types of features 
can be more discriminative than many existing works. We 
aggregate the holistic and local features to yield fused features 
and classification is conducted based on the fused features. 
Furthermore, we modify the typical CNN structure with the 
proposed ETI-pooling strategy, which reduces the impact of 
nuisance variations in classification tasks. W e additionally 
proposed a method to learn salient expressional image patches 
based on the L2-norm of local feature vectors and visualize the 
active regions relevant to expression changes. The selected 
salient patches are regarded as important parts for facia l expression recognition. Extensive experiments on two public ly 
available expression datasets (the CK+ dataset and the JAFFE 
database) demonstrate the effectiveness of our proposed 
method on FER task. The classification result of our 
DCMA-CNNs model on these two datasets outperforms many 
other competitive works, which include both conventional and 
deep based methods.  
ACKNOWLEDGEMENT  
This work was supported in part by the National Natural 
Science Foundation of China (61673402, 61273270, 
60802069), the Natural Science Foundation of Guangdong 
Province (2017A030311029, 2016B010123005, 
2017B090909005), the Science and Technology Program of 
Guangzhou of China (201704020180, 201604020024), and the 
Fundamental Research Funds for the Central Universities of 
China. 
REFERENCES  
[1] G. Mehrabian, Nonverbal Communication . New Brunswick, NJ, USA: 
Aldine, 2007. 
[2] A. Ryan, J. F. Cohn, S. Lucey, J. Saragih , P. Lucey , F. De la Torre, and A. 
Rossi, ‚ÄúAutomated facial expression recognition system,‚Äù in Proc. 43rd 
Annu. Int. Carnahan Conf. Security Technol., Zurich, Switzerland, 2009, 
pp. 172 ‚Äì177. 
[3] A. Vinciarelli, M. Pantic, and H. Bourlard, ‚ÄúSocial signal processing: 
Survey of an emerging domain,‚Äù Image Vis. Comput., vol. 31, no. 1, pp. 
1743 ‚Äì1759, 2009. 
[4] Q. Wang, K. Jia, and P. Liu, ‚ÄúDesign and Implementation of Remote 
Facial Expression Recognition Surveillance System Based on PCA and 
KNN Algorithms ‚Äù. In Intelligent Information Hiding and Multimedia 
Fig. 7  Visualization  of significant patches and the corresponding active 
regions following the learning strategy of salient patches. The intensity of 
red color is proportional to the total times a region is retained in 
experiments.  
(a) Example on the CK+ dataset.  
(b) Example on the JAFFE database.  
(a) 
 (b) 
Fig. 8  Recognition Performance with respect to different patches.  
(a) Performance on the CK+ dataset. 
(b) Performance on the JAFFE database.  
(a) 
(b) 
1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2844085, IEEE
Transactions on Multimedia
> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 
 10 
Signal Processing (IIH-MSP), 2015 International Conference on IEEE , 
Adelaide, Australia , 2015, pp. 314-317. 
[5] E. Vural et al ., ‚ÄúAutomated drowsiness detection for improved 
driversafety comprehensive databases for facial expression analysis,‚Äù in 
Proc. Int. Conf. Autom. Technol., vol. 1. Istanbul, Turkey, 2008, pp. 96 ‚Äì
105. 
[6] P. Ekman and W. Friesen, ‚ÄúThe Facial Action Coding System: A 
Techniquefor the Measurement of Facial Movement ‚Äù, Santa Clara, CA, 
USA: Consulting Psychologists Press, 1978. 
[7] C. E. Izard, The Face of Emotion, vol. 1. New York, NY, USA: 
Appleton-Century-Crofts, 1971. 
[8] P. Ekman, W. V. F riesen, and J. C. Hager, ‚ÄúFACS Manual,‚Äù Salt  Lake 
City, UT, USA: A Human Face, May 2002. 
[9] D. Laptev, N. Savinov, J. M. Buhmann, M. Pollefeys, ""Ti-pooling: 
Transformation-invariant pooling for feature learning in convolutional 
neural networks"", Proc. Int. Conf. Comput. Vis. Pattern Recognit., pp. 
289-297, 2016. 
[10] P. Ekman and W. Friesen, ‚ÄúThe Facial Action Coding System: A 
Technique for the Measurement of Facial Movement ‚Äù. Santa Clara, CA, 
USA:   Consulting Psychologists Press, 1978. 
[11] Y. Tian, T. Kanade, and J. F. Cohn, ‚ÄúRecognizing upper face action units  
for facial expression analysis,‚Äù in Proc. Int. Conf. Comput. Vis. Pattern  
Recognit., 2000, pp. 294 ‚Äì301. 
[12] Y. Tian, T. Kanade, and J. F. Cohn, ‚ÄúRecognizing action units for facial  
expression analysis,‚Äù IEEE Trans. Pattern Anal. Mach. Intell., vol. 23,   no. 
2, pp. 97 ‚Äì115, Feb. 2001. 
[13] Y. Tong, W. Liao, and Q. Ji, ‚ÄúFacial action unit recognition by exploiting  
their dynamic and semantic relationships,‚Äù IEEE Trans. Pattern Anal.   
Mach. Intell., vol. 29, no. 10, pp. 1683 ‚Äì1699, Oct. 2007. 
[14] M. S. Bartlett et al ., ‚ÄúFully automatic facial action recognition in 
spontaneous behavior,‚Äù in Proc. 7th Int. Conf. Autom. Face Gesture 
Recognit., Southampton, U.K., 2006, pp. 223 ‚Äì230. 
[15] J. F. Cohn, L. Reed, Z. Ambadar, J. Xiao, and T. Moriyama, ‚ÄúAutomatic  
analysis and recognition of brow actions and head motion in spontaneous 
facial behavior,‚Äù in Proc. IEEE Int. Conf. Syst., Man, Cybern., 2004,   pp. 
610‚Äì 616. 
[16] G. Sandbach, S. Zafeiriou, and M. Pantic, ‚ÄúBinary p attern analysis for 3D 
facial action unit detection,‚Äù in Proc. Brit. Mach. Vis. Conf., Surrey,   U.K., 
2012, pp. 119.1 ‚Äì119.12. 
[17] G. Sandbach, S. Zafeiriou, M. Pantic, and D. Rueckert, ‚ÄúRecognition of  
3D facial expression dynamics,‚Äù Image Vis. Comput., vol. 30 , no. 10, pp. 
762‚Äì 773, Oct. 2012. 
[18] Zhao, Kaili, Wen- Sheng Chu, and Honggang Zhang. ‚ÄúDeep Region  and 
Multi- Label Learning for Facial Action Unit Detection.‚Äù In  Proceedings 
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 
3391-3399. 2016. 
[19] H. Kobayashi, F. Hara, ""Facial interaction between animated 3d face 
robot and human beings"", Proceedings of the International Conference on 
Systems Man and Cybernetics, pp. 3732-3737, 1997 
[20] G. Gao, K. Jia, B. Jiang , ‚ÄúAn Automatic Geometric Features Extrac ting 
Approach for Facial Expression Recognition Based on Corner Detection‚Äù , 
Intelligent Information Hiding and Multimedia Signal Processing 
(IIH-MSP), 2015 International Conference on. IEEE, pp. 302-305, 2015. 
[21] A. Majumder, L. Behera, and V. K. Subramanian, ""Emotion recognition 
from geometric facial features using self-organizing map"", Pattern 
Recognition, vol. 47, no. 3, pp. 1282-1293, 2014. 
[22] Mao, Q., Rao, Q., Yu, Y., & Dong, M. ""Hierarchical Bayesian Theme 
Models for Multipose Facial Expression Recognition."" IEEE 
Transactions on Multimedia vol. 19, no.4, pp. 861-873 , 2017.  
[23] M. Huang, Z. Wang, Z. Ying, ""A new method for facial expression 
recognition based on sparse representation plus LBP"", Proc. IEEE Int. 
Congress Image Signal Process., pp. 1750-1754, 2010. 
[24] T. Jabid, M. Kabir, and O. Chae, ‚ÄúRobust facial expression recognition  
based on local directional pattern,‚Äù ETRI J., vol. 32, pp. 784‚Äì 794, 2010. 
[25] M. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. Fasel, and J. 
Movellan, ‚ÄúRecognizing facial expression: M achine learning and 
application to spontaneous behavior,‚Äù in Proc. IEEE Comput. Soc. Conf. 
Comput. Vis. Pattern Recog., 2005, pp. 568 ‚Äì573. 
[26] A. Dhall, A. Asthana, R. Goecke, and T. Gedeon, ‚ÄúEmotion recognition  
using PHOG and LPQ features,‚Äù in Proc. IEEE Int.  Conf. Autom. Face 
Gesture Recog. Workshops, 2011, pp. 878 ‚Äì883. 
[27] A. Tawari, M. M. Trivedi, ""Face expression recognition by cross modal 
data association"", IEEE Transactions on Multimedia, vol. 15, no. 7, pp. 
1543-1552, [28] Zhang, T., Zheng, W., Cui, Z., Zong, Y., Yan, J., & Yan, K. ‚ÄúA Deep 
Neural Network-Driven Feature Learning Method for Multi-view Facial 
Expression Recognition ‚Äù. IEEE Transactions on Multimedia, vol. 18, 
no.12, pp. 2528-2536, 2016. 
[29] L. Zhong, Q. Liu, P. Yang, J. Huang, D. N. Metaxas, ""Learning multiscale 
active facial patches for expression analysis"", IEEE Trans. Cybern., vol. 
45, no. 8, pp. 1499-1510, Aug. 2015. 
[30] S. L. Happy, Aurobinda Routray, ""Robust facial expression classification 
using shape and appearance features"", Advances in Pattern Recognition 
(ICAPR) 2015 Eighth International Conference on, pp. 1-5, 2015. 
[31] S. Happy, A. Routray, ""Automatic facial expression recognition using 
features of salient facial patches"", IEEE Trans. Affect. Comput., vol. 6, no. 
1, pp. 1-12, Jan ‚ÄìMar. 2015. 
[32] A. Majumde r, L. Behera, and V. K. Subramanian, ‚ÄúAutomatic  facial 
expression recognition system using deep network-based data fusion,‚Äù 
IEEE Trans. Cybern., 99 (2016), pp. 1- 12. 
[33] B. Girshick, J. Donahue, T. Darrell, J. Malik, ""Rich feature hierarchies for 
accurate object detection and semantic segmentation"", Proc. IEEE Conf. 
Comput. Vis. Pattern Recognit. (CVPR), pp. 580-587, Jun. 2014. 
[34] K. He, X. Zhang, S. Ren, J. Sun, ""Deep residual learning for image 
recognition"", Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 
2016.  
[35] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. 
Ozair et al ., Generative adversarial nets. In Advances in Neural 
Information Processing Systems, pp. 2672-2680, 2014. 
[36] J. Li, E. Lam, ""Facial expression recognition using deep neural networks"", 
IEEE International Conference on Imaging Systems and Techniques (IST) 
2015, pp. 1-6, Sept 2015. 
[37] A. Mollahosseini, D. Chan, and M. H. Mahoor, ‚ÄúGoing deeper in  facial 
expression recognition using deep neural networks,‚Äù in 2016  IEEE 
Winter Conference on Applications of Computer Vision (WACV), pp. 1 ‚Äì
10, IEEE, 2016. 
[38] P. Liu, S. Han, Z. Meng, Y. Tong, ""Facial expression recognition via a 
boosted deep belief network"", Computer Vision and Pattern Recognition 
(CVPR) 2014 IEEE Conference on, pp. 1805-1812, 2014. 
[39] D. Hamester, P. Barros, S. Wermter, ""Face expression recognition with a 
2-channel Convolutional Neural Network"", International Joint 
Conference on Neural Networks (IJCNN), pp. 1-8, 2015.  
[40] H. Jung, S. Lee, J. Yim, S. Park, J. Kim, ""Joint fine-tuning in deep neural 
networks for facial expression recognition"", Proc. Int. Conf. Comput. Vis., 
pp. 2982-2991, Dec. 2015. 
[41] D. Ciresan, U. Meier, and J. Schmidhuber. ‚ÄòMulti -column deep neural 
networks for image classification‚Äô, 2012 IEEE Conference on Computer 
Vision and Pattern Recognition (CVPR). Jun. 2012. pp. 3642-3649. 
[42] Xu, M., Cheng, W., Zhao, Q., Ma, L., Xu, F.: Facial expression 
recognition based on transfer learning from deep convolutional networks. 
In: 2015 11th International Conference on Natural Computation (ICNC), 
pp. 702 ‚Äì708. IEEE (2015). 
[43] A. Babenko, V. Lempitsky, ""Aggregating deep convolutional features for 
image retrieval"", Proc. Int. Conf. Comput. Vis., pp. 1269- 1277, 2015. 
[44] X. Lu, Z. Lin, X. Shen, R. Mech, J. Z. Wang, ""Deep multi-patch 
aggregation network for image style aesthetics and quality estimation"", 
ICCV, 2015. 
[45] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, and Z. Ambadar, ‚ÄúThe  
extended Cohn-Kanade dataset (CK√æ ): A complete dataset for action unit 
and emotion- specified expression,‚Äù in Proc. IEEE Int. Conf. Comput. Vis. 
Pattern Recog., 2010, pp. 94 ‚Äì101. 
[46] M. Lyons, S. Akamatsu, M. Kamachi, and J. Gyoba, ‚ÄúCoding facial 
expressions with gabor wavelets,‚Äù in Proc. IEEE Int. Conf. Autom. Face 
Gesture Recog., 1998, pp. 200 ‚Äì205. 
[47] P. Viola, M. Jones, ""Robust Real-Time Face Detection"", Int'l J. Computer 
Vision, vol. 57, no. 2, pp. 137-154, May 2004. 
[48] L. Zhong, Q. Liu, P. Yang, B. Liu, J. Huang, and D. N. Metaxas, 
‚ÄúLearning active facial patches for expression analysis,‚Äù in Proc.  IEEE 
Conf. Comput. Vis. Pattern Recog., 2012, pp. 2562 ‚Äì2569 
[49] S. Taheri, Q. Qiang, R. Chellappa, ""Structure-preserving sparse 
decomposition for facial expression analysis"", IEEE transactions on 
image processing: a publication of the IEEE Signal Processing Society, 
vol. 23, no. 8, pp. 3590, 2014. 
[50] H. Jung, S. Lee, J. Yim, S. Park, J. Kim, ""Joint fine-tuning in deep neural 
networks for facial expression recognition"", Proc. Int. Conf. Comput. Vis., 
pp. 2982- 2991, Dec. 2015. 
[51] M. Liu, S. Li, S. Shan, R. Wang, and X. Chen. Deeply learning 
deformable facial action parts model for dynamic expression analysis.  In 
ACCV, 2014, pages 1749 ‚Äì1756. IEEE, 2014. 2, 5, 7 
1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2844085, IEEE
Transactions on Multimedia
> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 
 11 
[52] K. Subramanian, S. Suresh, and R. Venkatesh Babu, ""Meta-Cognitive 
Neuro-Fuzzy Inference System for human emotion recognition"", in The 
20I2 International Joint Conference on Neural Networks (IJCNN), Jun. 
20 12, pp. 1- 7. 
[53] J. A. R. Castillo A. R. Rivera and O. Chae, ‚ÄúRecognition of face 
expressions using local prin cipal texture pattern,‚Äù in Image Processing 
(ICIP), 2012 19th IEEE International Conference on, september 2012. 
[54] S. H. Lee, K. Plataniotis, Y. M. Ro, ""Intra-class variation reduction using 
training expression images for sparse representation based facial 
expression recognition"", IEEE Transactions on Affective Computing, pp. 
1, 2014. 
[55] J. A. R. Castillo, A. R. Rivera, and O. Chae, ‚ÄúFacial expression 
recognition based on local sign directional pattern,‚Äù in Image Processing 
(ICIP), 2012 19th IEEE International Conference on, september 2012. 
[56] Khorrami, Pooya, Thomas Paine, and Thomas Huang. ""Do deep neural 
networks learn facial action units when doing expression recognition?."" 
Proceedings of the IEEE International Conference on Computer Vision 
Workshops. 2015. 
[57] da Si lva, FlƒÇƒÑvio Altinier Maximiano, and Helio Pedrini. ""Effects of 
cultural characteristics on building an emotion classifier through facial 
expression analysis."" Journal of Electronic Imaging 24.2 (2015): 
023015-023015. 
 
 
"
https://ieeexplore.ieee.org/document/1593706,"IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 17, NO. 1, JANUARY 2006 233
Letters __________________________________________________________________________________________
FacialExpressionRecognitionUsingKernelCanonical
CorrelationAnalysis(KCCA)
Wenming Zheng, Xiaoyan Zhou, Cairong Zou , Member, IEEE , and
LiZhao
Abstract‚ÄîInthiscorrespondence,weaddressthefacialexpressionrecog-
nitionproblemusingkernelcanonicalcorrelationanalysis(KCCA).Fol-
lowingthemethodproposedbyLyons et al.[7]andZhang et al.[8],we
manuallylocate34landmarkpointsfromeachfacialimageandthencon-vertthesegeometricpointsintoalabeledgraph(LG)[7]vectorusingthe
Gaborwavelettransformationmethodtorepresentthefacialfeatures.On
theotherhand,foreachtrainingfacialimage,thesemanticratingsde-
scribingthebasicexpressionsarecombinedintoasix-dimensionalsemantic
expressionvector.LearningthecorrelationbetweentheLGvectorandthe
semanticexpressionvectorisperformedbyKCCA.Accordingtothiscor-
relation,weestimatetheassociatedsemanticexpressionvectorofagiven
testimageandthenperformtheexpressionclassiÔ¨Åcationaccordingtothis
estimatedsemanticexpressionvector.Moreover,wealsoproposeanim-
provedKCCAalgorithmtotacklethesingularityproblemoftheGram
matrix.TheexperimentalresultsontheJapanesefemalefacialexpression
databaseandtheEkman‚Äôs‚ÄúPicturesofFacialAffect‚Äùdatabaseillustrate
theeffectivenessoftheproposedmethod.
Index Terms‚ÄîFacialexpressionrecognition(FER),generalizeddiscrimi-
nantanalysis(GDA),kernelcanonicalcorrelationanalysis(KCCA),kernel
method.
I. INTRODUCTION
Automatic facial expression recognition (FER) could be traced back
to the preliminary work of Suwa et al. [15] in 1978 and gained much
popularity starting with the pioneering work of Mase and Pentland [16]
in the 1990s. More recently, facial expression analysis has become a
very hot research topic in computer vision and pattern recognition, and
various approaches have been proposed to this goal. For literature sur-veys, see [12]‚Äì[14]. The contribution in this correspondence is two
fold: 1) we present an effective facial expression analysis and recog-
nition method based on kernel canonical correlation analysis (KCCA),
and 2) we propose an improved algorithm for KCCA to tackle the sin-
gularity problem of the Gram matrix.
Following the work done by Lyons et al. [6], [7] and Zhang et al.
[8], and being motivated by the kernel-based machine learning algo-
rithms[18],weÔ¨Årstmanuallylocate34landmarkpointsfromeachfacial
image and then convert these geometric points into a labeled graph (LG)
[7] vector using the Gabor wavelet transformation method [20], [21] to
represent the facial features. On the other hand, a semantic expression
Manuscript received March 16, 2004; revised November 6, 2004. This work
was supported in part by the Jiangsu Natural Science Foundations under Grants
BK2005407 and BK2005122 and in part by the Natural Science Foundations
under Grant 60503023.
W. Zheng is with the Research Center for Learning Science, Southeast
University, Nanjing, Jiangsu 210096, China (e-mail: wenming_zheng@
seu.edu.cn).
X. Zhou is with the Department of Electrics Engineering, Nanjing Univer-
sity of Information Science and Technology, Nanjing, Jiangsu 210044, China(e-mail: xiaoyan_zhou@nuist.edu.cn).
C. Zou and L. Zhao are with the Engieneering Research Center of Information
Processing and Application, Southeast University, Nanjing, Jiangsu 210096,
China (e-mail: cairong@seu.edu.cn; zhaoli@seu.edu.cn).
Digital Object IdentiÔ¨Åer 10.1109/TNN.2005.860849vector consisting of the semantic ratings of each training facial image
is used as the semantic features. Learning the correlation between the
LG vector and the semantic expression vector is performed by KCCA.
According to this correlation, we estimate the associated semantic ex-
pression vector of a given test image and then perform the expression
classiÔ¨Åcation according to this estimated semantic expression vector.
Canonical correlation analysis (CCA) is a powerful method to corre-
late the linear relation between two multidimensional variables. How-ever, if there is nonlinear correlation between the two variables, linear
CCA may not correctly correlate this relationship. KCCA is a nonlinear
extension of CCA via the ‚Äúkernel trick‚Äù to overcome this drawback
[10], [11]. Although the idea of kernelizing CCA is not new, all the
previous algorithms of KCCA [2], [3] tackle the singularity problem of
the Gram matrix by simply adding a regularization to the Gram matrixsuch that the Gram matrix becomes invertible [2], [3]. In this correspon-
dence, we propose an improved KCCA algorithm based on eigenvalue
decomposition approach rather than the regularization method to over-come this problem.
The remainder of this correspondence is organized as follows: we
derive the KCCA formulations in Section II, and the facial expression
classiÔ¨Åcation formulations based on KCCA in Section III. Experiments
are conducted in Section IV, and conclusion are given in Section V.
II. KCCA
Given two centered random multivariables
/120 /50 /82/110
and /121 /50 /82/110
,
the goal of CCA is to Ô¨Ånd a pair of directions /33/33/33 /120and /33/33/33 /121such that
the correlation /26 /40 /120 /59 /121 /41between the two projections /33/33/33/84
/120 /120and /33/33/33/84
/121 /121is
maximized [1], where
/26 /40 /120 /59 /121 /59 /33/33/33 /120 /59/33 /33/33 /121 /41
/61/69
 /33/33/33/84
/120 /120/121/84/33/33/33 /121
/69 /102 /33/33/33/84/120 /120/120/84 /33/33/33 /120 /103
 /69
 /33/33/33/84/121 /121/121/84 /33/33/33 /121
/61/33/33/33/84
/120 /69 /102 /120/121/84/103 /33/33/33 /121
/33/33/33/84/120 /69 /102 /120/120/84 /103 /33/33/33 /120
 /33/33/33/84/121 /69 /102 /121/121/84 /103 /33/33/33 /121/58 (1)
Suppose that /102 /120 /105 /103 /105 /61/49 /59 /46/46/46 /59/78and /102 /121 /105 /103 /105 /61/49 /59 /46/46/46 /59/78are /78observations of /120
and /121respectively. Then the goal of CCA is equivalent to Ô¨Ånding /33/33/33 /120
and /33/33/33 /121that maximize
/26 /40 /120 /59 /121 /59 /33/33/33 /120 /59/33 /33/33 /121 /41/61/33/33/33/84
/120 /88/89/84/33/33/33 /121
/33/33/33/84/120 /88/88/84/33/33/33 /120
 /33/33/33/84/121 /89/89/84/33/33/33 /121(2)
where
/88 /61/91 /120 /49 /120 /50 /1/1/1 /120 /78 /93 /59 /89 /61/91 /121 /49 /121 /50 /1/1/1 /121 /78 /93 /58
Let /120be mapped into a Hilbert space /70through a nonlinear mapping /8
/8/58 /82/110
/33 /70 /59 /120 /33 /8/40 /120 /41 /58
Also, we suppose that /8/40 /120 /41is centered in /70. For a method to center
/8/40 /120 /41, see [11]. From (2), we obtain that the correlation function in /70
can be formulated as
/26 /40/8/40 /120 /41 /59 /121 /59 /33/33/33 /8/40 /120 /41 /59/33 /33/33 /121 /41
/61/33/33/33/84
/8/40 /120 /41 /8/40 /88 /41 /89/84/33/33/33 /121
/33/33/33/84
/8/40 /120 /41/8/40 /88 /41/40/8/40 /88 /41/41/84 /33/33/33 /8/40 /120 /41
 /33/33/33/84/121 /89/89/84/33/33/33 /121/58
1045-9227/$20.00 ¬© 2006 IEEE
234 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 17, NO. 1, JANUARY 2006
Therefore, the goal of KCCA is equivalent to Ô¨Ånding /33/33/33 /8/40 /120 /41and /33/33/33 /121that
maximize /33/33/33/84
/8/40 /120 /41 /8/40 /88 /41 /89/84/33/33/33 /121under the constraints
/33/33/33/84
/8/40 /120 /41 /8/40 /88 /41/40/8/40 /88 /41/41/84/33/33/33 /8/40 /120 /41 /61 /33/33/33/84
/121 /89/89/84/33/33/33 /121 /61/49 (3)
where
/8/40 /88 /41/61 /91 /8 /40 /120 /49 /41/8 /40 /120 /50 /41 /1/1/1 /8/40 /120 /78 /41/93 /58 (4)
According to [17], there exists direction /11/11/11 /8/40 /120 /41such that
/33/33/33 /8/40 /120 /41 /61/40 /8 /40 /88 /41/41 /11/11/11 /8/40 /120 /41 /58 (5)
Assume that /107 /40 /120 /105 /59 /120 /106 /41is a kernel function that can be expressed as
the dot product form on the Hilbert space /70
/40 /107 /41 /105/106 /61 /107 /40 /120 /105 /59 /120 /106 /41/61 /104 /8/40 /120 /105 /41 /59 /8/40 /120 /106 /41 /105 /61/40 /8 /40 /120 /105 /41/41/84/8/40 /120 /106 /41(6)
where /104 /8/40 /120 /105 /41 /59 /8/40 /120 /106 /41 /105stands for the dot product of /8/40 /120 /105 /41and /8/40 /120 /106 /41.
From (4) and (6), we obtain that the /78 /2 /78matrix /75 /61
/40/40 /107 /41 /105/106 /41 /105 /61/49 /59 /46/46/46 /59/78 /59 /106 /61/49 /59 /46/46/46 /59/78(also known as the Gram matrix) can be
written as
/75 /61/40 /8 /40 /88 /41/41/84/8/40 /88 /41 /58 (7)
From (5), (3), and (7), we obtain
/33/33/33/84
/8/40 /120 /41 /8/40 /88 /41 /89/84/33/33/33 /121 /61 /11/11/11/84
/8/40 /120 /41 /75/89/84/33/33/33 /121 (8)
/33/33/33/84
/121 /89/89/84/33/33/33 /121 /61 /33/33/33/84
/8/40 /120 /41 /8/40 /88 /41/40/8/40 /88 /41/41/84/33/33/33 /8/40 /120 /41
/61 /11/11/11/84
/8/40 /120 /41 /75/75 /11/11/11 /8/40 /120 /41 /61/49 /58 (9)
Thus, solving the KCCA problem is equivalent to Ô¨Ånding /11/11/11 /8/40 /120 /41and
/33/33/33 /121that maximize /11/11/11/84
/8/40 /120 /41 /75/89/84/33/33/33 /121under the constraints
/33/33/33/84
/121 /89/89/84/33/33/33 /121 /61 /11/11/11/84
/8/40 /120 /41 /75/75 /11/11/11 /8/40 /120 /41 /61/49 /58
The corresponding Lagrangian is
/76
 /11/11/11 /8/40 /120 /41 /59/33 /33/33 /121 /59/21 /59 /22
/61 /11/11/11/84
/8/40 /120 /41 /75/89/84/33/33/33 /121 /0 /21
 /11/11/11/84
/8/40 /120 /41 /75/75 /11/11/11 /8/40 /120 /41 /0 /49
 /50
/0 /22
 /33/33/33/84
/121 /89/89/84/33/33/33 /121 /0 /49
 /50 /58
Taking derivatives with respect to /11/11/11 /8/40 /120 /41and /33/33/33 /121and set to zeros, we
obtain
/64/76
/64/11 /11/11 /8/40 /120 /41/61 /75/89/84/33/33/33 /121 /0 /21 /75/75 /11/11/11 /8/40 /120 /41 /61 /48 (10)
/64/76
/64/33 /33/33 /121/61 /89/75 /11/11/11 /8/40 /120 /41 /0 /22 /89/89/84/33/33/33 /121 /61 /48 /58 (11)
From (9)‚Äì(11), we obtain
/22 /61 /21/59 /33 /33/33 /121 /61/40 /89/89/84/41/0 /49/89/75
/22/11/11/11 /8/40 /120 /41(12)
/75/89/84/40 /89/89/84/41/0 /49/89/75 /11/11/11 /8/40 /120 /41 /61 /21/50/75/75 /11/11/11 /8/40 /120 /41 /58 (13)
If the Gram matrix /75has full rank, then (13) can be rewritten as
/75/0 /49/89/84/40 /89/89/84/41/0 /49/89/75 /11/11/11 /8/40 /120 /41 /61 /21/50/11/11/11 /8/40 /120 /41 /58 (14)
However, in most cases, /75is often singular because it is a centered
Gram matrix [2]. To overcome this problem, one may add a regular-
ization matrix /78 /75 /73to /75and then resolve the following generalized
eigenequation (14), [2], [3], [5]
/40 /75 /43 /78 /75 /73 /41/0 /49/89/84/40 /89/89/84/41/0 /49/89/75 /11/11/11 /8/40 /120 /41 /61 /21/50/11/11/11 /8/40 /120 /41 (15)
where /73is the identity matrix and /78 /75is a small constant. The drawback
of this approach is that the selection of the regularization to the Grammatrix could inÔ¨Çuence the performance of KCCA. In what follows, we
will propose an improved algorithm for KCCA based on eigenvalue de-
composition to avoid the regularization to the Gram matrix. In fact, the
largest eigenvalue of (13) gives the maximum of the following Rayleigh
quotient [4]
/21/50/61/11/11/11/84
/8/40 /120 /41 /75/89/84/40 /89/89/84/41/0 /49/89/75 /11/11/11 /8/40 /120 /41
/11/11/11/84
/8/40 /120 /41/75/75 /11/11/11 /8/40 /120 /41/58 (16)
Let /87 /61 /89/84/40 /89/89/84/41/0 /49/89, then (16) can be rewritten as
/21/50/61/11/11/11/84
/8/40 /120 /41 /75/87/75 /11 /8/40 /120 /41
/11/11/11/84
/8/40 /120 /41/75/75 /11/11/11 /8/40 /120 /41/58 (17)
Thus, solving KCCA is equivalent to Ô¨Ånding /11/11/11 /8/40 /120 /41that maximizes the
Rayleigh quotient in (17), which is equivalent to solving the same op-timal problem as that of generalized discriminant analysis (GDA) [4].
Therefore, we can adopt the same eigenvalue decomposition approach
as that used in GDA for the Gram matrix to system (17). Moreover,
note that there exists the degenerate eigenvalue problem in solve system
(17), we propose to use the method used in the modiÔ¨Åed generalized
discriminant analysis (MGDA) [9] algorithm to tackle this problem.
III. P
ATTERN CLASSIFICATION USING KCCA
Let /102 /40 /33/33/33/105
/8/40 /120 /41 /59/33 /33/33/105
/121 /41 /103/116
/105 /61/49be the /116pair directions of KCCA, and
/26 /49 /59 /46/46/46 /59/26 /116the /116corresponding correlation values. Let /97 /105and /98 /105be the
projections of the variables /8/40 /120 /41and /121onto the projection vectors
/33/33/33/105
/8/40 /120 /41and /33/33/33/105
/121, respectively. Thus, we get
/97 /105 /61
 /33/33/33/105
/8/40 /120 /41
/84
/8/40 /120 /41 /59/98 /105 /61
 /33/33/33/105
/121
/84
/121 /40 /105 /61/49 /59 /46/46/46 /59/116 /41 /58
Suppose that /26 /105 /25 /49. Then, we can obtain that /97 /105and /98 /105are proximately
linearly correlated; thus, we can assume that there exists coefÔ¨Åcient /107 /105
and constant /34 /105such that
/98 /105 /61 /107 /105 /97 /105 /43 /34 /105 /40 /105 /61/49 /59 /46/46/46 /59/116 /41 /58 (18)
Note that both /98 /105and /97 /105are centered projections. Thus, according to
regression analysis [19], the coefÔ¨Åcients /107 /105and the constant /34 /105can be
calculated using the observations of /8/40 /120 /41and /121, as follows:
/107 /105 /61/78
/106 /61/49
/33/33/33/105
/8/40 /120 /41
/84
/8/40 /120 /106 /41
 /33/33/33/105
/121
/84
/121 /106
/78
/106 /61/49
/33/33/33/105
/8/40 /120 /41
/84
/8/40 /120 /106 /41
/50
(19)
/34 /105 /61/49
/78/78
/106 /61/49
/33/33/33/105
/121
/84
/121 /106
/0/107 /105
/78/78
/106 /61/49
/33/33/33/105
/8/40 /120 /41
/84
/8/40 /120 /106 /41/61 /48 /58 (20)
Let /8/40 /120 /116/101/115 /116 /41be a test sample and /121 /116/101/115/116the corresponding observa-
tion of /121. Let /97 /116/101/115/116be the projection of /8/40 /120 /116/101/115/116 /41onto the directions
/33/33/33/49
/8/40 /120 /41 /59 /46/46/46 /59/33 /33/33/116
/8/40 /120 /41, and /98 /116/101/115/116the projection of /121 /116/101/115/116onto the directions
/33/33/33/49
/121 /59 /46/46/46 /59/33 /33/33/116
/121. Then, we get
/97 /116/101/115/116 /61
 /97/49
/116/101/115/116 /59 /46/46/46 /59/97/116
/116/101/115/116
/61
 /33/33/33/49
/8/40 /120 /41 /59 /46/46/46 /59/33 /33/33/116
/8/40 /120 /41
/84/8/40 /120 /116/101/115/116 /41/61 /80/84
/120 /75 /116/101/115/116 (21)
/98 /116/101/115/116 /61
 /98/49
/116/101/115/116 /59 /46/46/46 /59/98/116
/116/101/115/116
/61
 /33/33/33/49
/121 /59 /46/46/46 /59/33 /33/33/116
/121
/84/121 /116/101/115/116 /61 /80/84
/121 /121 /116/101/115/116 (22)
IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 17, NO. 1, JANUARY 2006 235
Fig. 1. Example of the 34 landmark points selected from a facial image.
where
/80 /120 /61
 /11/11/11/49
/8/40 /120 /41 /59 /46/46/46 /59/11 /11/11/116
/8/40 /120 /41
/75 /116/101 /115 /116 /61/40 /8 /40 /88 /41/41/84/8/40 /120 /116/101/115/116 /41
/80 /121 /61
 /33/33/33/49
/121 /59 /46/46/46 /59/33 /33/33/116
/121
 /58
The projection /121 /116/101/115/116in (22) can be solved by
/121 /116/101/115/116 /61
 /80 /121 /80/84
/121
/0 /49
/80 /121 /98 /116/101/115/116 /58 (23)
If /80 /121 /80/84
/121is singular, we add a regularization /34 /73on /80 /121 /80/84
/121such that
/80 /121 /80/84
/121 /43 /34 /73is a nonlinear matrix. Then (23) can rewritten by
/121 /116/101/115/116 /61
 /80 /121 /80/84
/121 /43 /34 /73
/0 /49
/80 /121 /98 /116/101/115/116 /58 (24)
Thus, for a given test observation /120 /116/101/115/116, we can estimate the corre-
sponding observation /121 /116/101/115/116using (21), (18), (19), (20) and (24). Let
/121/105
/116/101/115/116denote the /105th element of /121 /116/101/115/116, then the index of the most matched
expression class of the test image is signed by
/99/3/61/97 /114 /103 /109 /97 /120
/105/121/105
/116/101/115/116 /58 (25)
IV. E XPERIMENTS
We will use the Japanese female facial expression (JAFFE) database
[6]‚Äì[8] and Ekman‚Äôs ‚ÄúPictures of Facial Affect‚Äù database [22], respec-
tively, to conduct the FER based on KCCA. In the preprocessing stage,
we manually locate 34 landmark points from each facial image by re-
ferring to the method in [7]. Fig. 1 illustrates an example of the 34
landmark points.
Similar with Lyons et al. [7], after locating the 34 landmark points,
we use the Gabor wavelet representation of each facial image at the
landmark points to represent the facial features of each image, where
all of the wavelet convolution values (magnitudes) at these landmarkpoints are combined into a 1020-dimensional LG vector. The Gabor
kernel is deÔ¨Åned as follows [7], [20], [21]:
/32 /117/59 /118 /40 /122 /41/61/107 /107 /117/59/118 /107/50
/27/50/101/120 /112
 /0/107 /107 /117/59/118 /107/50/107 /122 /107/50
/50 /27/50
/2
 /101/120/112/40 /105 /107 /117/59/118 /1 /122 /41 /0 /101/120/112
 /0/27/50
/50
(26)
Fig. 2. Semantic ratings (1: happiness; 2: sadness; 3: surprise; 4: anger; 5:
disgust; 6: fear) of the facial image in Fig. 1, where the left Ô¨Ågure illustrates
the semantic ratings from psychologists, whereas the right one illustrates the
semantic ratings estimated by the KCCA method.
where /117and /118represent the orientation and scale of the Gabor kernels,
and /107 /117/59/118is deÔ¨Åned as
/107 /117/59/118 /61 /107 /118 /101/120/112/40 /105/30 /117 /41 (27)
where /107 /118 /61 /25/61 /50/118/40 /118 /50/102 /49 /59 /46/46/46 /59 /53 /103 /41and /30 /117 /61 /25/117 /61 /54/40 /117 /50/102 /48 /59 /46/46/46 /59 /53 /103 /41.
It is notable that both JAFFE database and Ekman‚Äôs expression
database provide semantic ratings evaluated by psychologists foreach facial image. These semantic ratings are used for quantitatively
evaluating the six basic expressions (happiness, sadness, surprise,
anger, disgust, and fear). In this experiment, the semantic ratings of
each image are combined into a six-dimensional semantic expression
vector for expression analysis.
Let
/120and /121, respectively, denote the LG vector and semantic expres-
sion vector. The goal of the KCCA-based facial expression analysis
method is to estimate the associated value of /121for a given observation
of /120using (24) and then perform the facial expression classiÔ¨Åcation
task using (25). We use the ‚Äúleave-one-image-out‚Äù and ‚Äúleave-one-sub-
ject-out‚Äù cross validation strategies, respectively, to perform the experi-
ment. In the ‚Äúleave-one-image-out‚Äù strategy, one facial image is used as
the testing data and the other ones as the training data. This is repeated
for all of the possible trials and the recognition results are averaged as
the Ô¨Ånal recognition rate. In the ‚Äúleave-one-subject-out‚Äù strategy, the
images belonging to one subject are used as the testing data and the
remainder ones as the training data. This is also repeated for all of thepossible trials until all the subjects are used as the testing data. The ex-
periment results are also averaged as the Ô¨Ånal recognition rate.
A. FER on JAFFE Facial Expression Database
We use the JAFFE database to test the performance of the proposed
method in this experiment. There are 213 images covering seven
categories of facial expressions (neutral, happiness, sadness, surprise,anger, disgust, and fear) in JAFFE database. Except for the neutral
expression images, there are 183 images in this database. The original
images are all sized 256
/2256 pixels with a 256-level gray scale.
For a new test facial image, we locate the 34 landmark points and
then get the LG vector. The corresponding semantic ratings of each
facial image are estimated using (24), and then perform the expressionclassiÔ¨Åcation according to (25). Fig. 2 illustrates the semantic ratings
of the facial image in Fig. 1, where the left Ô¨Ågure illustrates the
semantic ratings evaluated from psychologists whereas the right onethe semantic ratings estimated from our experiment (both Ô¨Ågures are
normalized into the scope [0, 1]). From Fig. 2, we can see that the
236 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 17, NO. 1, JANUARY 2006
TABLE I
EXPERIMENTAL RESULTS ON JAFFE D ATABASE USING
THE‚ÄúLEAVE-ONE-IMAGE -OUT‚ÄùCROSS VALIDATION
WITHSEMANTIC INFORMATION USED
TABLE II
EXPERIMENTAL RESULTS ON JAFFE D ATABASE USING
THE‚ÄúLEAVE-ONE-SUBJECT -OUT‚ÄùCROSS VALIDATION
WITHSEMANTIC INFORMATION USED
right Ô¨Ågure approximately coincides with the left one. Based on the
semantic ratings in Fig. 2, we can classify this facial image into sadness
category. The experimental results based the ‚Äúleave-one-image-out‚Äù
and the ‚Äúleave-one-subject-out‚Äù cross validation strategies are shown
in Table I and Table II, respectively, where the kernel functions are
deÔ¨Åned as:
Monomial kernel: /107 /40 /120/59 /121 /41/61 /40 /104 /120/59 /121 /105 /41/100, where /100is the monomial
degree; Gaussian kernel: /107 /40 /120/59 /121 /41/61/101 /120 /112 /40 /0/107 /120 /0 /121 /107/50/61/27 /41, where /27is
the parameter of the kernel function.
To further test the performance of the proposed method, we set the
values of the semantic expression vector of each training image by re-
ferring to the class label information: Assume that /121 /105is the semantic
expression vector corresponding to the /105th training image /120 /105, and let
/121 /105/107denote the /107thelement of /121 /105.I f /120 /105belongs to class /106, then we set
/121 /105/106 /61/49; else /121 /105/107 /61/48 /40 /107 /54/61 /106 /41. In this case, the directions /33/33/33 /8/40 /120 /41of
KCCA could be equivalent to the discriminant vectors of GDA. For the
comparison purpose, we also conduct the same expression classiÔ¨Åca-
tion task using the GDA method, where the nearest neighbor classiÔ¨Åer
is used for classiÔ¨Åcation. Table III and Table IV show the experimental
results.
From Tables I‚ÄìIV, we can see that the KCCA method achieves much
better performance than the CCA method for the FER task. When the
class label information of each facial image is used for constructing
the semantic expression vector, the recognition rate of KCCA can be
achieved as high as 98.36% for the ‚Äúleave-one-image-out‚Äù cross val-
idation experiment and 77.05% for the ‚Äúleave-one-subject-out‚Äù crossvalidation experiment. Additionally, it is notable that the best recog-
nition results in Tables I and II are, respectively, lower than those in
Tables III and IV. The reason could be attributed to the fact that the se-mantic expression vectors used in Tables III and IV consider the class
label information; thus, it will be advantageous when used for the clas-
siÔ¨Åcation task. The drawback of doing this, however, is that it may not
quantitatively estimate all the basic expressions from the facial image.
Besides, although KCCA and GDA are equivalent when the semanticexpression vector of each facial image consists of the class label infor-TABLE III
EXPERIMENTAL RESULTS ON JAFFE D ATABASE USING
THE‚ÄúLEAVE-ONE-IMAGE -OUT‚ÄùCROSS VALIDATION
WITHCLASS LABEL INFORMATION USED
TABLE IV
EXPERIMENTAL RESULTS ON JAFFE D ATABASE USING
THE‚ÄúLEAVE-ONE-SUBJECT -OUT‚ÄùCROSS VALIDATION
WITHCLASS LABEL INFORMATION USED
mation in KCCA, our experimental results also show that KCCA may
achieve better performance than GDA. The reason could be attributed
to the degenerate eigenvalue problem of GDA [9]. Due to the degen-
erate eigenvalue problem, GDA becomes instable and could not achievethe best recognition result [9]. The improved KCCA algorithm we pro-
posed in this correspondence, however, can overcome this problem and,
thus, could obtain better results.
B. FER on Ekman‚Äôs Facial Expression Database
In this experiment, we use Ekman‚Äôs ‚ÄúPictures of Facial Affect‚Äù data-
base [22] to further test the performance of the proposed FER method.
This database contains 110 images consisting of six male and eight fe-male subjects. We only choose 96 images (excluding all the 14 neutral
images) covering all the 14 persons with six basic facial expressions
as the training data. The experiment scheme is designed to be sim-ilar with the one in Section A. In the Ô¨Årst example of the experiment,
we use the semantic ratings provided by the Ekman‚Äôs database to gen-
erate the semantic expression vectors, whereas the second example uses
the class label information of each image to generate the semantic ex-
pression vectors. The ‚Äúleave-one-image-out‚Äù cross validation and the
‚Äúleave-one-subject-out‚Äù cross validation are also respectively adoptedin both examples. The best experimental results of the two examples
are shown in Tables V and VI, respectively. Moreover, in Fig. 3 we
also show the plot of recognition rates of KCCA and GDA as functions
of gaussian kernel parameter
/27when the ‚Äúleave-one-subject-out‚Äù cross
validation with class label information is used for constructing the se-mantic expression vector.
V. D
ISCUSSION AND CONCLUSION
The FER problem based on KCCA has been addressed in this cor-
respondence. The correspondence shows that KCCA is a very effec-
IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 17, NO. 1, JANUARY 2006 237
Fig. 3. Recognition rates as functions of the Gaussian kernel parameter /27.
TABLE V
EXPERIMENTAL RESULTS ON EKMAN‚ÄôS DATABASE
WITHSEMANTIC INFORMATION USED
TABLE VI
EXPERIMENTAL RESULTS ON EKMAN‚ÄôS DATABASE
WITHCLASS LABEL INFORMATION USED
tive method for correlating the nonlinear relationship between the fa-
cial features and the associated semantic features, which also provideus an effective technique to predict the semantic expression informa-
tion of a facial image. The experimental results on JAFFE database and
Ekman‚Äôs facial expression database are encouraging. In addition, fromthe experimental results, we note that the performance of the proposed
method is closely related to selection of the semantic expression vec-
tors used for training KCCA. Although the best classiÔ¨Åcation perfor-mance in the experiment using JAFFE database is obtained when the
class label information is used for constructing the semantic expressionvectors, a major drawback of doing this is that these semantic expres-
sion vectors convey relatively less basic expression information. Thus,
they may not produce better results when used for quantitatively pre-dicting all the six basic expressions. To improve the performance of the
semantic ratings-based approach, a natural and better way could resort
to obtain more accurate semantic ratings of each facial image to con-struct the corresponding semantic expression vector. Moreover, in this
correspondence we also provide an improved algorithm for KCCA to
overcome the singularity problem of the Gram matrix and the degen-
erate eigenvalue problem of solving the eigenequation of KCCA.
A
CKNOWLEDGMENT
The authors would like to thank the anonymous reviewers for their
valuable comments. They would also like to thank Dr. M. J. Lyons for
providing the JAFFE facial expression database and the semantic rating
data of the database.
REFERENCES
[1] H. Hotelling, ‚ÄúRelations between two sets of variates,‚Äù Biometrika , vol.
28, pp. 321‚Äì377, 1936.
[2] F. R. Bach and M. I. Jordan, ‚ÄúKernel independent component analysis,‚Äù
J. Mach. Learn. Res. , vol. 3, pp. 1‚Äì48, 2002.
[3] P. L. Lai and C. Fyfe, ‚ÄúKernel and nonlinear canonical correlation anal-
ysis,‚ÄùInt. J. Neural Syst. , vol. 10, no. 5, pp. 365‚Äì377, 2000.
[4] G. Baudat and F. Anouar, ‚ÄúGeneralized discriminant analysis using a
kernel approach,‚Äù Neural Comput. , vol. 12, pp. 2385‚Äì2404, 2000.
[5] T. V. Gestel, J. A. K. Suykens, J. De Brabanter, B. De Moor, and J. Van-
dewalle, ‚ÄúKernel canonical correlation analysis and least square support
vector machines,‚Äù in Proc. Int. Conf. ArtiÔ¨Åcial Neural Networks , 2001,
pp. 384‚Äì389.
[6] M. Lyons, S. Akamatsu, M. Kamachi, and J. Gyoba, ‚ÄúCoding facial ex-
pressions with gabor wavelets,‚Äù in Proc. 3rd IEEE Int. Conf. Automatic
Face and Gesture Recognition , Japan, Apr. 1998, pp. 200‚Äì205.
[7] M. Lyons, J. Budynek, and S. Akamatsu, ‚ÄúAutomatic classiÔ¨Åcation of
single facial images,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 21,
no. 12, pp. 1357‚Äì1362, Dec. 1999.
[8] Z. Zhang, M. Lyons, M. Schuster, and S. Akamatsu, ‚ÄúComparison
between geometry-based and Gabor-wavelets-based facial expressionrecognition using multi-layer perceptron,‚Äù in Proc. 3rd IEEE Int.
Conf. Automatic Face and Gesture Recognition , Nara, Apr. 1998, pp.
454‚Äì459.
[9] W. Zheng, L. Zhao, and C. Zou, ‚ÄúA modiÔ¨Åed algorithm for generalized
discriminant analysis,‚Äù Neural Comput. , vol. 16, no. 6, pp. 1283‚Äì1297,
2004.
[10] V. Vapnik, The Nature of Statistical Learning Theory . New York:
Springer, 1995.
[11] B. Sch√∂lkopf, A. Smola, and K. R. M√ºller, ‚ÄúNonlinear component
analysis as a kernel eigenvalue problem,‚Äù Neural Comput. , vol. 10, pp.
1299‚Äì1319, 1998.
[12] B. Fasel and J. Luettin, ‚ÄúAutomatic Facial Expression Analysis: A
Survey,‚ÄùPattern Recognit. , vol. 36, pp. 259‚Äì275, 2003.
[13] M. Pantic and L. J. M. Rothkrantz, ‚ÄúAutomatic analysis of facial expres-
sions: The state of the art,‚Äù IEEETrans.PatternAnal.Mach.Intell. , vol.
22, no. 12, pp. 1424‚Äì1445, Dec. 2000.
[14] M. Pantic and L. J. M. Rothkrantz, ‚ÄúToward an affect-sensitive mul-
timodal human-computer interaction,‚Äù Proc. IEEE , vol. 91, no. 9, pp.
1370‚Äì1390, Sep. 2003.
[15] M. Suwa, N. Sugie, and K. Fujimora, ‚ÄúA preliminary note on pattern
recognition of human emotional expression,‚Äù in Proc.4thInt.JointConf.
Pattern Recognition , 1978, pp. 408‚Äì410.
[16] K. Mase and A. Pentland, ‚ÄúRecognition of facial expression from optical
Ô¨Çow,‚ÄùTrans. IEICE , pp. 3474‚Äì3483, 1991.
[17] D. R. Hardoon, S. Szedmark, and J. S. Taylor, Canonical Correlation
Analysis: An Overview With Application to Learning Methods, Tech.Rep. CSD-TR-03-02, 2003.
[18] B. Sch√∂lkopf and A. J. Smola, Learning With Kernels . Cambridge,
MA: MIT Press, 2001.
[19] J. M. Lattin, J. D. Carroll, and P. E. Green, Analyzing multivariate
data. PaciÔ¨Åc Grove, CA: Brooks/Cole, 2003.
238 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 17, NO. 1, JANUARY 2006
[20] L. Wiskott, J. M. Fellous, N. Kr√ºger, and C. von der Malsburg, ‚ÄúFace
recognition by elastic bunch graph matching,‚Äù IEEETrans.PatternAnal.
Mach. Intell. , vol. 19, no. 7, pp. 775‚Äì779, Jul. 1997.
[21] C. Liu and H. Wechsler, ‚ÄúIndependent component analysis of Gabor fea-
tures for face recognition,‚Äù IEEETrans.NeuralNetw. , vol. 14, no. 4, pp.
919‚Äì928, Aug. 2003.
[22] P. Ekman and W. V. Friesen, ‚ÄúPictures of Facial Affect,‚Äù in Human In-
teraction Laboratory . San Francisco, CA: Univ. California Medical
Center, 1976.
StudyonModelingofMultispectralEmissivityand
OptimizationAlgorithm
Chunling Yang, Yong Yu, Dongyang Zhao, and Guoliang Zhao
Abstract‚ÄîTarget‚Äôsspectralemissivitychangesvariously,andhowtoob-
taintarget‚ÄôscontinuousspectralemissivityisadifÔ¨Åcultproblemtobewell
solvednowadays.Inthisletter,anactivation-function-tunableneuralnet-
workisestablished,andamultistepsearchingmethodwhichcanbeusedto
trainthemodelisproposed.Theproposedmethodcaneffectivelycalculate
theobject‚Äôscontinuousspectralemissivityfromthemultispectralradiation
information.Itisauniversalmethod,whichcanbeusedtorealizeon-line
emissivitydemarcation.
Index Terms‚ÄîNeuralnetworks,optimizationalgorithm,spectralemis-
sivity,tunableactivationfunction.
I. INTRODUCTION
The emissivity is not only dependent on the inherence of the object to
be measured, but also lies with the physical state of the object‚Äôs surface,
such as surface smoothness, material granularity, temperature, radiantangle, wavelength, and so on. A great of error exists in the emissivity
obtained by experienced methods because of the factors listed above.
At present, this problem has not been well solved. Classical emissivitymeasuring method, such as calorimetry, the thermal return reÔ¨Çection
method and radiometry [1]‚Äì[3] need to install some assistant equipment
around the object to be detected and they cannot realize the on-linedemarcation. Thus their applications are conÔ¨Åned.
Multispectral thermometry is an improved method in recent years
for obtaining radiation temperature and emissivity, which can sepa-
rate emissivity and the real temperature from the object‚Äôs radiation
information. We can use this new method to overcome the aboveproblem. However, current multispectral thermometries usually rely
on an assumption model [4]‚Äì[6] and an experienced formula. If the
formula cannot describe the object to be measured accurately, the
error will exist in the measurement results of emissivity and the
real temperature. It restricts the further development of multispectral
thermometry.
This letter has established an activation- function-tunable neural net-
work (AFT-CNNE) and a multistep searching method used to train the
model in order to calculate object‚Äôs continuous spectral emissivity. This
Manuscript received July 11, 2004 ; revised December 17, 2004.
C. Yang is with the Department of Electrical Engineering, Harbin Institute of
Technology, Harbin, Heilongjiang 150001, China, and also with the Department
of Automation, Harbin Engineering University, Harbin, Heilongjiang 150001,
P.R. China (e-mail: yangcl1@hit.edu.cn).
Y. Yu, D. Zhao, and G. Zhao are with the Department of Electrical Engi-
neering, Harbin Institute of Technology, Harbin, Heilongjiang 150001, China.
Digital Object IdentiÔ¨Åer 10.1109/TNN.2005.860837
Fig. 1. Neural network structure.
model does not require the assumption of a Ô¨Åxed emissivity formula,
and can realize on-line demarcation, and can also get very high preci-
sion for any object.
II. CNNE M ODEL BASED ON FIXED ACTIVATION FUNCTION
First of all, we need to use the neural network to establish the map-
ping relationship between emissivity and wavelength, that is /34 /61 /34 /40 /21 /41.
As we cannot get the sample data of wavelength and emissivity from
the actual measurement, so it is impossible to get the functional rela-tionship model of
/34directly from the BP network. Therefore, we com-
bine the radiant temperature measurement theory with neural network
to establish the combined neural network model. In this way, we canget the weight coefÔ¨Åcients of the mapping relationship for
/34 /61 /34 /40 /21 /41.
Now we introduce the speciÔ¨Åc steps as follows.
A. IdentiÔ¨Åcation of the Nonlinear Relationship Between Wavelength
and Emissivity
It has been proved in theory that a forward neural network with a
hidden layer can approximate any nonlinear curve deÔ¨Åned in a com-pact set in
/82/110with any precision. Since there is a nonlinear mapping
relationship between the spectral emissivity and the wavelength of the
object, a single hidden layer neural network is used to represent this
relationship as shown in Fig. 1. The network has a structure with one
input and one output, with /107nodes. The input and output signals are
denoted by /21and /34, respectively. The hidden layer errors are denoted
by /98 /49 /49 /59/98 /49 /50 /59 /46/46/46 /59/98 /49 /107, respectively, and the output layer error is denoted
by /98 /50. According to the theory of neural network, the activation func-
tion /102 /40 /58 /41of output and hidden layers can be either the same or different.
Suppose the activation function is the same denoted by /102 /40 /58 /41, then the
relationship between the wavelength /21and the emissive /34can be ob-
tained from Fig. 1 as follows.
/34 /61 /102
/107
/109 /61/49/86 /109 /102 /40 /87 /109 /21 /43 /98 /49 /109 /41
 /43 /98 /50
 /58 (1)
Since the sample data of wavelength and emissivity cannot be ob-
tained by using multispectral thermometry, the weights in above neural
network cannot be calculated, which means the network has to be im-
proved. Since the brightness temperature of each spectrum can be mea-
sured, a mathematical model can be constructed based on the bright-
ness temperature so as to obtain the best approximation to the weights
of the object‚Äôs emissivity, and then the nonlinear mapping relationshipbetween the emissivity and the wavelength can be achieved. The CNNE
model below can realize above purpose.
B. Construction of the Combined Neural Network Model
Let the number of channels of the multispectral radiation ther-
mometer be
/110, then, according to the theory of radiation thermometry,
the relationship among the real object temperature /84, the brightness
1045-9227/$20.00 ¬© 2006 IEEE
"
https://ieeexplore.ieee.org/document/9226437,"0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE
Transactions on Instrumentation and Measurement
IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 1
Facial Expression Recognition using Local
Gravitational Force Descriptor based Deep
Convolution Neural Networks
Karnati Mohan, Ayan Seal, Senior Member IEEE , Ondrej Krejcar, Anis Yazidi, Senior Member IEEE
Abstract ‚ÄîAn image is worth a thousand words, hence a face
image illustrates extensive details about the speciÔ¨Åcation, gender,
age, and emotional states of mind. Facial expressions play an im-
portant role in community-based interactions and are often used
in the behavioral analysis of emotions. Recognition of automatic
facial expressions from a facial image is a challenging task in the
computer vision community and admits a large set of applications
such as driver safety, human-computer interactions, health care,
behavioral science, video conferencing, cognitive science, and
others. In this work, a deep-learning-based scheme is proposed
for identifying the facial expression of a person. The proposed
method consists of two parts. The former one Ô¨Ånds out local
features from face images using a local gravitational force de-
scriptor while in the latter part, the descriptor is fed into a novel
deep convolution neural networks model (DCNN). The proposed
DCNN has two branches. The Ô¨Årst branch explores geometric
features such as edges, curves, and lines whereas holistic features
are extracted by the second branch. Finally, the score-level fusion
technique is adopted to compute the Ô¨Ånal classiÔ¨Åcation score.
The proposed method along with twenty-Ô¨Åve state-of-the-art
methods is implemented on Ô¨Åve benchmark available databases
namely, Facial Expression Recognition 2013, Japanese Female
Facial Expressions, Extended CohnKanade, Karolinska Directed
Emotional Faces, and Real-world Affective Faces. The databases
consist of seven basic emotions viz., neutral, happiness, anger,
sadness, fear, disgust, and surprise. The proposed method is
compared with existing approaches using four evaluation metrics
namely, accuracy, precision, recall, and f1-score. The obtained
results demonstrate that the proposed method outperforms all
state-of-the-art methods on all the databases.
Index Terms ‚ÄîFacial expression recognition, local gravitational
force descriptor, deep convolution neural networks, softmax
classiÔ¨Åcation, score-level fusion.
I. I NTRODUCTION
Karnati Mohan is with PDPM Indian Institute of Information Technology,
Design and Manufacturing, Jabalpur, 482005, India E-mail: 1811011@iiit-
dmj.ac.in.
Ayan Seal is with PDPM Indian Institute of Information Technology, Design
and Manufacturing, Jabalpur, 482005, India and Center for Basic and Applied
Science, Faculty of informatics and management, University of Hradec
Kralove, Rokitanskeho 62, 500 03 Hradec Kralove, Czech Republic E-mail:
ayanseal30@ieee.org.
Ondrej Krejcar is with Center for Basic and Applied Science, Faculty of
informatics and management, University of Hradec Kralove, Rokitanskeho 62,
500 03 Hradec Kralove, Czech Republic Email: ondrej.krejcar@uhk.cz. He
is also associated with Malaysia-Japan International Institute of Technology
(MJIIT), Universiti Teknologi Malaysia, Jalan Sultan Yahya Petra, 54100
Kuala Lumpur, Malaysia.
Anis Yazidi is with the Research Group in Applied ArtiÔ¨Åcial Intelligence,
Oslo Metropolitan University, 460167, NorwayEmail:anisy@oslomet.no.AFFECTIVE computing is a Ô¨Åeld of study that attempts to
develop instruments/devices and systems that can iden-
tify, interpret, process, and simulate human affects. Nowadays
it has got a considerable amount of attention towards the
research communities in the Ô¨Åelds of artiÔ¨Åcial intelligence and
computer vision owing to its noticeable academic and commer-
cial applications such as human-computer interaction (HCI),
virtual reality, health care, deceptive detection, multimedia,
augmented reality, driver safety, surveillance, etc. Generally,
computational models of human expression process affective
state and they are of two types namely, decision-making
models and predictive models. The former one accounts for
the effect of expression whereas the latter one can identify the
state of the emotion. Models of non-verbal expression of dif-
ferent forms of facial expressions deduced from speech, body
gesture, and physiological signals provide valuable sources for
affective computing. Interested readers are referred to [1] to
know more about various methods, models, and applications
of affective computing. In this study, computer-based facial
expression recognition (FER) is considered due to its ability to
mimic human coding skills. FER is indispensable in affective
computing. Facial expression is an essence of non-verbal com-
munication to express the internal behaviors in interpersonal
relations. Moreover, it is a sentiment analysis technology
that uses biometric to automatically recognize seven basic
emotions, namely, neutral (NE), anger (AN), disgust (DI),
fear (FE), happiness (HA), sadness (SA), and surprise (SU)
from still images or videos. Although a considerable amount
of works was conducted for developing instruments to access
emotions, recognizing human expressions is still a challenging
task that is affected by deÔ¨Ånite circumstances especially when
performed in the wild. Some of the notable difÔ¨Åculties associ-
ated with FER are as follows: a) when the difference between
two facial expressions is small then it is difÔ¨Åcult to distinguish
them with high precision [2], b) generally, the expression of
a particular facial emotion by different people is not the same
due to the inter-person variability and their face biometric
shapes [3]. All the recent studies focus on FER methods that
can be categorized into two groups viz., hand-crafted feature-
based methods, and deep-learning feature-based methods. The
former one is also divided into appearance-based features and
geometric features. Appearance-based methods rely on various
statistics of the pixels‚Äô values within the face image. Examples
include Gobar wavelets [4], Haar wavelet [5], Local Binary
Pattern (LBP) [6], [7], Histogram of oriented gradients (HOG)
[7], [8], Histogram of bunched intensity values (HBIV) [9],
Authorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. 
0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE
Transactions on Instrumentation and Measurement
IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 2
Dynamic Bayesian Network (DBN), [10], etc. On the other
hand, geometric features are obtained by transforming the
image into geometric primitives such as corner or minutiae
points [11], edges, and curves [12]. This is accomplished, for
example, by locating unique features such as eyes, mouth,
nose, and chin, and measuring their relative position [13],
[14], width, and perhaps other parameters. However, extracting
distinctive features based on traditional methods is limited to
the human experience, so it is difÔ¨Åcult to acquire as well
as it is arduous to achieve better performance on large data.
Traditional approaches are not up to the real FER application
requirements and they also require high computational cost
and space [15].
Over the past few years, feature extraction from image data
using deep convolution neural networks (DCNN) has gained
popularity in various computer vision tasks. By virtue of using
DCNN, many breakthroughs were achieved for image clas-
siÔ¨Åcation problems especially face related recognition tasks
[16]‚Äì[18]. It is observed that DCNN has outperformed the
traditional methods with hand-crafted features in recent years
[19]‚Äì[21]. DCNN is able to extract hypothetical features from
a low-level to a high-level of facial images with the help
of several non-linear connections [16]. Furthermore, DCNN
can extract useful unique features by solving several issues
caused by traditional methods. In [22], a DCNN for FER was
designed to provide better discrimination ability by combining
the central loss function and the veriÔ¨Åcation recognition model.
In another work [23], a conditional generative adversarial net-
work was presented to increase the size of the data and DCNN
used for the facial expression classiÔ¨Åcation [24]. Fathallah et
al. discussed a recognition algorithm based on the geometry
group model [25].
A. Motivation and contribution
It is clear from the literature that most of the existing works
perform reasonably well on databases having images which
were captured in controlled lab environments. However, these
works do not yield satisfactory results on more challenging
and real-time databases consisting of images with greater
variations. Thus, there is a need to improve the performance
of a FER system. The performance of a FER system relies on
feature engineering. Engineering new features from existing
ones can improve the performance of a system. This motivates
us to work further in this direction. It is also clear from
state-of-the-art methods that most of the works related to
FER tasks are based on edge information [12], [26]‚Äì[30]
because it varies in individual expression. Important features
such as corners, lines, curves can be extracted from edges
of an image. Edges are signiÔ¨Åcant local changes of intensity
in an image. In the recent past, various DCNN models were
exploited to extract hypothetical deep features for developing
FER systems. However, the number of features is quite large.
Sometimes deep features may lead to overÔ¨Åtting. Moreover,
the extraction of deep features is time-consuming and it
requires powerful resources. Furthermore, only a small fraction
of these overwhelming numbers of features are used. On the
other hand, edge detection using gradient captures the smallchange in the x and y directions, which are known as gradients.
The gradient is a vector that has a certain magnitude (M)
and direction (D). The M would be higher when there is a
sharp change in intensity, such as around the edges. The M
provides information about edge strength. On the other hand,
the D is always perpendicular to the D of the edge. The D
represents the geometric structure of the image. So, in the
Ô¨Årst step of the proposed method, edge descriptor based on
Gravitational Force (GF) [31] is adopted because it uses sur-
rounding pixel information instead of considering the adjacent
pixels difference in x and y directions while computing M and
D images. However, the proposed system does not depend
only on local edge information, but it also depends on holistic
features. Thus, in the second step, M and D images are fed into
a novel DCNN to extract useful information. The proposed
DCNN consists of two branches, the Ô¨Årst one consists of
shallow DCNN and extracts the local features whereas the
second one fetches the holistic features from M and D images
as it consists of major DCNN. Finally, a score-level fusion
technique is adopted on classiÔ¨Åcation results obtained from
M and D images to get Ô¨Ånal results. The overview of the
proposed method is shown in Fig. 1. The performance of the
proposed method is compared with twenty-Ô¨Åve state-of-the-
art methods. All the methods are implemented on Ô¨Åve bench-
mark databases namely, Facial Expression Recognition 2013
(FER2013) [32], Japanese Female Facial Expressions (JAFFE)
[33], Extended CohnKanade (CK+) [34], Karolinska Directed
Emotional Faces (KDEF) [35], and Real-world Affective Faces
(RAF) [36], [37]. To measure the efÔ¨Åciencies of all the
methods including the proposed one, four classiÔ¨Åcation metrics
viz., accuracy, precision, recall, and f1-score are considered for
the quantitative evaluation. Empirical outcomes illustrate that
the proposed method defeats all the twenty-Ô¨Åve state-of-the-art
methods.
The rest of the work is organized as follows: In Section
II, a review of earlier works related to FER is conducted.
The proposed method is described in Section III. Experimental
results and discussion are presented in Section IV. Finally,
Section V concludes the work.
II. R ELATED WORK
All the methods in the FER task can be categorized
into two groups based on feature extraction techniques, viz.
hand-crafted features, and deep-learning features. This section
presents them brieÔ¨Çy. Mainly two steps, namely feature ex-
traction, and classiÔ¨Åcation are associated with the FER task.
Conventional features such as Gobar wavelets [4], curves
[12], Scale Invariant Feature Transform [21], HOG [8], LBP
[6], minutiae points [11], Haar wavelet [5], HBIV [9], DBN
[10], and edges [38] were exploited with advanced domain
comprehension in the Ô¨Årst step. In the second step, support
vector machine (SVM) [39], feed-forward neural network
[40], and extreme learning machine [41] were adopted for
classiÔ¨Åcation. In [42], Chen et al. offered a feature de-
scriptor called HOG from Three Orthogonal Planes (HOG-
TOP) to extract dynamic textures from video sequences to
characterize facial appearance changes. However, hand-crafted
Authorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. 
0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE
Transactions on Instrumentation and Measurement
IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 3
 
 
       M 
        D 
    DCNN  
    DCNN  
         Pre-processing  
 Feature extraction & c lassification  
 Score level 
fusion  
Final s core  
 
Input i mage  
NE 
 AN  
 DI 
 FE 
 HA 
 SA 
 SU 
 Fusion  
        Rotation by +50 
       Rotation by -50 
       Horizontal flip 
       Gaussian  noise 
Fig. 1. An overview of the proposed FER scheme
features based methods have limited performance in real-
life applications, especially for FER tasks. In recent years,
it is observed from the literature that deep-learning-based
methods are superior to hand-crafted features based methods
for FER tasks [17], [18], [43], [44]. Shallow and Deep CNN
considered for extraction on gray-scale images and classiÔ¨Åed
using softmax classiÔ¨Åers on FER2013 [45]. In [46], the DCNN
framework and Softmax were considered for feature extraction
and classiÔ¨Åcation respectively on the FER2013 database [32].
In [47], Orozco presented Alexnet, VGG19, and ResNet based
transfer learning methods for the FER task. In [16], Sun et
al. presented a DCNN model and DeepID features for face
recognition. In another work, Sun et al. considered the Siamese
network to increase the efÔ¨Åciency of the FER task [48].
Barsoum et al. discussed the VGG13 network for the FER
task on the FER+ database in [49]. A weighted mixture deep
neural network considered for the FER task and it consists
of two channels, one of them was used to extract the facial
expression features on gray-scale images with partial VGG16
framework. On the other hand, features are extracted on LBP
images with shallow DCNN on JAFFE [33] and CK+ [34]
databases further, softmax classiÔ¨Åers were used to classify
the extracted features and then combined obtained outputs
from both the channels using weighted fusion in [50]. In [51],
features were extracted from pre-trained VGG19 architecture
on the ImageNet database for the FER task and SVM used
for expression classiÔ¨Åcation on JAFFE and CK+ databases. In
[52], three DCNN sub-networks were considered and trained
independently on FER2013 and AffectNet database [53], fur-
ther ensembled three networks using weighted fusion. Fur-
thermore larger weights were assigned to the network, which
obtained higher recognition accuracy. Moreover, appearance-
based features were extracted using DCNN and obtained
features were fused with geometric feature-based DCNN in
the hierarchical constitution according to [54]. However, ap-
pearance features were extracted on LBP images likewise
gray-scale images were considered for geometric feature-based
networks on JAFFE and CK+ databases. In [55], ensembledResNet50 and VGG16 frameworks were utilized to extract
facial features and classify individual expressions on the
KDEF database [35]. Hasani and Mahoor presented a DCNN
framework which consists of 3D Inception-ResNet layers
followed by an LSTM unit that together extract the spatial and
temporal relations from facial images (3D Inception-ResNet +
landmarks) [56]. Geometric and regional local binary pattern
features were merged by autoencoders followed by Kohonen
self-organizing map (SOM)-based classiÔ¨Åer (Autoencoders +
SOM) to recognize facial expressions [57]. Kim et al. [58],
considered a Spatio-temporal feature representation learning
for solving the FER problem by encoding the characteristics of
facial expressions using DCNN and long short-term memory
(LSTM) (Spatio-temporal feature + LSTM). Pons and Masip
considered ensembles of DCNNs for solving the FER problem
[59]. Garcia and Ramirez presented a DCNN for classifying
two facial expressions viz., happy and sad only [60]. Meng
et al. [61] and Liu et al. [62] worked on identity-aware FER
models. In [61], Meng et al. used two identical DCNN streams
to jointly estimate various expressions and identity features
(IACNN) to Ô¨Ånd relief inter-subject variations initiated by
personal attributes for the FER task. On the other hand, Liu et
al. [62] employed deep metric learning (2B (N+M)Softmax)
to jointly optimize a deep metric and softmax loss. In [63],
Alam et al. resorted to a sparse-deep simultaneous recurrent
network (S-DSRN) for the FER problem and incorporated
a dropout rate to the model. Benitez-Quiroz et al. [64],
presented a FER system based on discriminant color features
and a Gabor transform-based algorithm (Color features +
Gabor transform) to gain invariance to the timing of facial
action units (AUs) changes. In [65], a model called deep
comprehensive multi-patches aggregation convolutional neural
networks (DCMA-CNNs) was presented. It had two branches.
One branch extracted holistic features whereas the other
branch obtained local features from segmented expressional
image patches. Then both feature vectors were combined
to classify expressions using DCNN with ETI-pooling. In
[66], Zhang et al. developed a broad learning system for
Authorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. 
0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE
Transactions on Instrumentation and Measurement
IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 4
FER. A multilevel DCNN was developed to extract mid-
level and high-level features within facial images to solve
the FER problem (Ensemble of MLCNNs) [67]. In [68], an
attentional DCNN named a Deep-Emotion to tackle the FER
problem was devised. In [69], a deep AUs graph network
was presented based on a psychological mechanism. In the
Ô¨Årst step, the face image is divided into small key areas
using segmentation techniques. Further, these key areas are
then converted into corresponding AU-related facial expression
regions. Second, from these regions, local appearance features
were extracted for further AUs analysis. Then, considering
AU related regions as vertices and distance between every
two landmarks as edges, AUs facial graph is constructed to
represent expressions. Finally, to learn hybrid features for FER
adjacency matrices of the facial graph are put into a graph-
based convolutional neural network to combine the local-
appearance and global-geometry information. In [7], Kopaczka
et al. presented a high-resolution thermal facial image database
for the FER task. Besides, they extend existing approaches for
infrared landmark detection with a head pose estimation for
improved robustness and analyze the performance of a deep-
learning method on this task.
III. P ROPOSED METHOD
This section presents a brief overview of an edge descriptor
of an image using GF followed by a detailed description of
our proposed DCNN for the FER task.
A. An edge descriptor
In [70], Roy et al. stated that each pixel value of an image
is parallel to a universal body and therefore it is considered
as a mass of the body. The GF of an image is employed by
the central pixel on its adjacent pixels. The Law of Universal
Gravitation states that everybody mass (m 1) attracts every
other body mass (m 2) in the universe by a force pointing in a
straight line (d ) between the centers-of-mass of both bodies,
and this force, GF, is proportional to the masses of the bodies
and inversely proportional to the square of their separation.
Mathematically, GF is computed using Eq. 1.
GF=G(m1m2)
d2; (1)
where G is gravitational constant and its value is 6:67259 
10 11. Let A be a gray-scale image. Let us consider a center
pixel,Ac, of a local 3mask. It means Acis surrounded
by eight neighboring pixels, Ai. It is clear from the law of
universal gravitation that all the eight neighboring pixels, Ai,
exert forces on Ac. Thus, the force exerted on Acby theith
neighboring pixel can be represented by GFic. Then the x
and y components of GFicareGFicx=GFicsin and
GFicy=GFiccos respectively when GFicis at an angle
ofw. r. t. x-axis. The GFicxandGFicycan be computed
using Eq. 2 and Eq. 3 respectively. Figure 2 shows an input
image and edge strengths i.e. Ms in x and y directions.
GFicx=NX
i=1(GAcAi
d2
icsin ic); (2)GFicy=NX
i=1(GAcAi
d2
iccos ic); (3)
where N is the total number of neighboring pixels of a mask
andd2
icis squared Euclidean distance between the ithpixel and
the center pixel. The GFicMandGFicDofGFicare calculated
by Eqs. 4 and 5 respectively.
GFicM=q
(GFicx)2+ (GF icy)2 (4)
GFicD=tan 1GFicy
GFicx
(5)
Eqs. 4 and 5 can be used repeatably by considering every
pixel as a center pixel to Ô¨Ånd out M and D of gradients of a
gray-scale image, A.
 
 
  
  
           a) 
            b) 
            c) 
Fig. 2. a) A sample gray-scale image from JAFFE database b) M in x
direction, and c) M in y direction
B. The architecture of the proposed DCNN
DCNNs learn features automatically and tend to describe
the aimed task more accurately due to the parameters learning
by back-propagation from the loss function of the aimed task.
Existing DCNN based models such as VGG-16 and VGG-19,
are built on a single branch sequentially connected with con-
volutional layers, and usually focus on homogeneously scaled
receptive Ô¨Åelds and ignore detailed edge information. Thus,
they lack gathering adequate features of spatial structure for
facial appearance. To address this problem multi convolutional
networks were introduced. In this section, we introduce a novel
DCNN for the FER task. The architecture of the proposed
DCNN is shown in Fig. 3. It consists of two branches. The
Ô¨Årst branch is able to extract signiÔ¨Åcant local features like
edges, lines, curves, and corners from the M and D of an
image as shallow DCNN is designed. On the other hand,
the second branch is responsible for extracting the holistic
features, which can differentiate one expression from others
since major DCNN is considered. Since M and D of an image
are considered, the proposed DCNN is able to extract the
features which are relevant to the individual expressions. The
Ô¨Årst branch of DCNN consists of three convolutional layers,
which are connected sequentially, namely two max-pooling,
one average pooling, and zero-padding, these are connected se-
quentially. On the other hand, the second branch of the DCNN
network contains Ô¨Åve convolutional, three max-pooling, one
average-pooling, and up-sampling layer. Moreover, these two
branches are concatenated and forwarded to the two dense
Authorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. 
0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE
Transactions on Instrumentation and Measurement
IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 5
         
 
      Final output 
L1 
L11 
 L12 
 L13
 L14 
 L15 
 L16 
L11 
 L12 
 L13
 L14 
 L15 
 L16 
L1 
L21 
L22 
 L23 
 L24 
 L25 
 L26 
L27 
 L28 
L29 
L21 
L22 
 L23 
 L24 
 L25 
 L26 
L27 
 L28 
L29 
F 
 FC1 
FC2 
Score  
                                            Feature e xtraction  
         Classification  
 Score level f usion  
 
 
Magnitude  
 Direction  
Fig. 3. The detailed DCNN architecture
layers for the classiÔ¨Åcation of facial expressions. The detailed
description of each layer and its parameters are shown in Fig.
4. Moreover, to capture the enriched contextual information
Ô¨Ålters 55and44are employed. These Ô¨Ålters allow the
network to learn true edge variations. The biggest advantage
of convolutional layers is that it is able to extract the features
automatically, kthconvolutional layer consists of nkfeature
maps, denoted as Fk
p, wherep= 1;2;3;:::nkandkrepresents
a particular convolutional layer. Each feature map i.e, Fk 1
q,
whereq= 1; 2;3;::::nk 1from the (k 1)thconvolutional
layer is convolved with the Ô¨Ålter Wk
pqand biasbk
pis added.
Further, convolved feature maps are fed into the non-linear
activation function RectiÔ¨Åed Linear Unit (ReLu). Equation 6
shows how we can obtain a convolved feature map, Fk
p.
Fk
p=  nk 1X
q=1Fk 1
qWk
pq+bk
p!
;p= 1;2;3;:::nk;(6)
where indicates convolution operation. The responsibility
of an activation function is to rework the weighted sum of
input from one node to a different activated nodes. Here, ReLU
is adopted because it can reduce the value of cost/loss function
by mitigating the vanishing gradient problem to some extent.
It can compute faster and better performance on complex
databases [71]. The mathematical representation of the ReLU
activation function is shown in Eq. 7. Max-pooling is applied
to convolved feature maps obtained by Eq. 6 to defeat the
overÔ¨Åtting problem by providing an abstracted style of the
representation of the convolved feature maps. Max-pooling
calculates the utmost value of every patch from each feature
map to spotlight the foremost presented feature within thepatch. It also reduces the number of parameters in order to
make the model simple. Moreover, it provides translation,
rotation, and scale-invariant feature maps.
 nk 1X
q=1Fk 1
qWk
pq+bk
p!
=
=(0 if
Pnk 1
q=1Fk 1
qWk
pq+bk
p<0
Pnk 1
q=1Fk 1
qWk
pq+bk
p otherwise
(7)
The feature maps, FM16andFM29, obtained from the
layersL16andL29are concatenated. Then the concatenated
feature maps FM16;FM29are Ô¨Çattened by Ô¨Çattening layer,
F, before feeding them as input, FC0into the Ô¨Årst fully-
connected layer. The output, FC1, of the Ô¨Årst fully connected
layer, is fed into the second fully connected layer to generate
FC2as output. The mathematical operation involved in two
fully connected layers is denoted by Eq. 8.
FCi=WiFCi 1+bi;i= 1;2 (8)
whereWiandbiare the weight and bias of the ithfully
connected layer. It is observed from the experiments that
the overÔ¨Åtting problem arises in both fully connected layers
as a number of learnable parameters are associated with
them. In this work, the dropout technique is adopted to
resolve the overÔ¨Åtting problem that occurred in both the fully
connected layers. The output of the second fully connected
layer,FC2, is further fed into the softmax layer. The softmax
layer consists of seven neurons and produces a probability
vector, ^y= [ ^y1;^y2;^y3;^y4;^y5;^y6;^y7]. The probability vector
Authorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. 
0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE
Transactions on Instrumentation and Measurement
IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 6
     Layer              Type                                                                             Network Parameters  
         L1        Convolution  Filter size: 5 x 5       Filter number: 32           Stride: 1 x 1          Padding: Same     Activation F unctio n = ReLU  
         L11           Pooling                    Pool size: 3 x 3                                 Stride: 3 x 3          Padding: Valid      Pooling Type: Max -Pooling  
         L12        Convolution  Filter size: 4 x 4        Filter number: 32           Stride: 1 x 1          Padding: Same     Activation F unctio n = ReLU  
         L13           Pooling                    Pool size: 3 x 3                                 Stride: 2 x 2           Padding: Valid      Pooling Type: Max -Pooling  
         L14        Convolution  Filter size: 5 x 5       Fil ter number: 64            Stride: 1 x 1          Padding: Same     Activation F unctio n = ReLU  
         L15           Pooling                    Pool size: 3 x 3                                 Stride: 2 x 2           Padding: Vali d      Pooling Type: Average -Pooling  
         L16          padding                    Pad size: 4  x 4                                          -                                 -                   Padding  Type: Zero -padding  
         L21        Convolution  Filter size: 5 x 5       Filter number: 32           Stride: 1 x 1          Padding: Same     Activation F unctio n = ReLU  
         L22           Pooling                    Pool size: 5  x 5                                 Stride: 2 x 2           Padding: Valid      Pooling Type: Max -Pooling  
         L23        Convolution  Filter size: 4 x 4        Filter number: 32           Stride: 1 x 1          Padding: Same     Activation F unctio n = ReLU  
         L24           Pooling                    Pool size: 3 x 3                                 Stride: 2 x 2           Padding: Valid      Pooling Type: Max -Pooling  
         L25        Convolution  Filter size: 5 x 5       Filter number: 64            Stride: 1 x 1          Padding: Same     Activation F unctio n = ReLU  
         L26           Pooling                    Pool size: 3 x 3                                 Stride: 2 x 2           Padding: Valid      Pooling Type: Max -Pooling  
         L27        Convolution  Filter size: 5 x 5       Filter number: 64            Stride: 1 x 1          Padding: Same     Activation F unctio n = ReLU  
         L28           Pooling                    Pool size: 3 x 3                                 Stride: 2 x 2           Padding: Vali d      Pooling Type: Average -Pooling  
         L29        Up-sampling                    Sample  size: 4  x 4                                     -                                 -                  Sample Type: Up -sampling  
      L16, L29        Early Fusion                                                               Type:           ----     Concatenation  
         F          Flatten                                                                         -----------------------  
         FC1    Full Connection                           Number of Neurons = 1024                                                                   Dropout = 0.3  
         FC2    Full Connection                           Number of Neurons = 1024                                                                   Dropout = 0.3  
      Score         Output                                                                    Number of Neurons = 7                                                                 
 Final Output   Score level Fusion                                                              Type:          -----     Late Score Level Fusion  
 
Fig. 4. Various parameters used in the proposed DCNN architecture shown in Fig. 3 and their values
consists of seven probability values as seven classes of facial
expressions are considered in this study. The kthprobability
value is obtained by Eq. 9.
^yk=eFC2
k
P7
k=1eFC2
k;k= 1;2;::::; 7: (9)
1) Network training: The proposed network trains inde-
pendently on M and D of gradients of facial images and
estimates the probability of each class. The proposed network
weights are initialized using glorot uniform method and adam
optimization is employed to prevent local optimum [72], along
with learning rate decay is introduced to advance the training
effect. In adam optimization, initial learning rate and learning
rate decay are assigned as 0.00001 and 1e-4 respectively.
Categorical cross-entropy is exploited to Ô¨Ånd the loss for
multiclass classiÔ¨Åcation and it is a measure to quantify the
error of our model. The categorical cross-entropy is computed
using Eq. 10.
 (y;^y) = 1
77X
j=1yjlog ^yj; (10)
where,y(= [y1;y2;y3;y4;y5;y6;y7])is one-hot encoding
vector of the actual labels. Batch size 16 is considered while
training the proposed DCNN since the network can occupy
less memory in our system and 60 epochs are considered for
JAFFE, CK+, KDEF, and RAF databases while 200 epochs
are used for FER2013.
2) Score-level fusion: To estimate the Ô¨Ånal prediction of
seven basic expressions, the score-level fusion technique [73]
is performed on M and D of gradients. Mathematically, score-
level fusion is done by Eq. 11.SFi= arg max
cNX
j=1jPijc; (11)
wherecindicates the various expressions, and that irepresents
the input sample, and Nindicates the two different modalities,
in this study MandDof the input image, are considered as
two different modalities. Pijccould be a prediction probability
to belong to a class cfor input sample iof modality j.
The worth of jis chosen by searching values from 0 to
5 with a step size of 0.2. Score-level fusion is simpler to
weigh individual scores of modalities and it gave a better
performance on the FER task.
IV. E XPERIMENTAL RESULTS AND DISCUSSION
A. Environment settings
In this study, the Keras framework and Anaconda de-
velopment platform are considered for training and testing
the proposed model. Python language is used since many
of the deep-learning libraries are developed using it. The
speciÔ¨Åcations of the system are reported in Table I.
TABLE I
SPECIFICATION OF THE SYSTEM
Name Parameter
GPU RAM 16GB
Graphics processor NVIDIA Quadro P5000
Cuda cores 2560
Memory interface 256-bit
Memory type GDDR5X
Bandwidth 288.5 GB/s
Language Python
Authorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. 
0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE
Transactions on Instrumentation and Measurement
IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 7
B. Database description
In this work, Ô¨Åve well-known benchmark databases namely,
FER2013 [32], JAFFE [33], CK+ [34], KDEF [35], and RAF
[36], [37] are considered for the evaluation of the proposed
network because these databases contain seven basic universal
facial expressions namely, NE, AN, DI, FE, HA, SA, and SU.
The upper part of Table II describes the statistical information
of the aforementioned databases.
TABLE II
STATISTICAL INFORMATION OF THE DATABASES
Database
Image SizeNumber of imagesTotalNE
AN DI FE HA SA SU
Before
Augmentation
FER2013 4848 6183
4916 545 5089 8977 6069 3993 35772
J
AFFE 256256 30 30 29 32 31 31 30 213
CK+ 256256 50
47 61 24 59 28 62 331
KDEF 256256 70
70 70 70 70 70 70 490
RAF 100100 3204
867 877 355 5957 2460 1463 15183
After
Augmentation
FER2013 256256 6183
4916 2725 5089 8977 6069 3993 37952
J
AFFE 256256 150 150 145 160 155 155 150 1065
CK+ 256256 250
235 305 120 295 140 310 1655
KDEF 256256 350
350 350 350 350 350 350 2450
RAF 256256 3204
4335 4385 1755 5957 2460 1463 23559
C. Data augmentation
Having a large database is important for measuring the
performance of a DCNN model. Moreover, we can prevent a
DCNN model from learning irrelevant features because irrele-
vant or partially relevant features can negatively impact model
performance. However, the performance can be improved to
some extent by augmenting the data we already have. A
DCNN can be invariant to translation, viewpoint, size, or
illumination. So, some of the image processing techniques like
the rotation of images by 5clockwise and anti-clockwise
directions, horizontal Ô¨Çip, and adding Gaussian noise are
considered to extend the total number of images of seven facial
expressions of JAFFE, CK+, and KDEF databases to increase
the diversity of these databases. On the other hand, augmenta-
tion is done on DI facial expression of FER2013 and AN, DI,
and FE facial expressions of RAF only due to their imbalance
classes. The lower half of Table II describes the statistical
information of Ô¨Åve databases after data augmentation. Further,
each database is divided into three parts such as training,
validation, and testing. A tenfold cross-validation technique
is adopted for all the experiments to evaluate the performance
of the proposed method. In other words, out of ten subsets,
8 subsets are used for training, 1 subset is considered for
validation, and the rest of the one subset is adopted for testing.
The average classiÔ¨Åcation results are reported in this study.
Table III shows the number of facial images used in the
training, validation, and testing processes for each fold.
D. Experimental results using M
In the Ô¨Årst experiment, only M followed by the proposed
DCNN is considered. In other words, the upper half of the
proposed model is only used for training and validation. So,
the upper half of the model is implemented in the above men-
tioned Ô¨Åve databases. Figure 5 shows training and validation
performances with respect to the epoch on each database. TheTABLE III
TRAIN ,VALIDATION ,AND TEST SPLIT FOR THE FIVE DATABASES
Database Train Validation Test
FER2013 30362 3795 3795
JAFFE 851 107 107
CK+ 1323 166 166
KDEF 1960 245 245
RAF 18847 2356 2356
 
 
 
 
 
 
  
    a) 
    b) 
     c) 
    d) 
     e) 
Fig. 5. Training and validation performances using M of gravitational force
on Ô¨Åve databases namely, a) FER2013, b) JAFFE, c) CK+, d) KDEF, and e)
RAF
average testing accuracy and average loss on each database
are reported in Table IV. It is clear from Table IV that results
are very good on three databases namely, JAFFE, CK+, and
KDEF. However, the results on FER2013 and RAF databases
are relatively poor, but these could be accepted.
TABLE IV
TESTING ACCURACY AND LOSS WHEN MOF GRAVITATIONAL FORCE IS
USED
Database Testing Accuracy Loss
FER2013 77.10% 0.4204
JAFFE 95.63% 0.1602
CK+ 97.99% 0.0627
KDEF 96.36% 0.1485
RAF 82.33% 0.3820
E. Experimental results using D
In the second experiment, only D followed by the proposed
DCNN is adopted. In other words, the lower half of the
proposed model is only considered for training and validation.
Thus, the lower half of the model is executed on the above
mentioned Ô¨Åve databases. Figure 6 shows training and valida-
tion accuracies with respect to iteration on each database. The
average testing accuracy and average loss on each database are
Authorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. 
0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE
Transactions on Instrumentation and Measurement
IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 8
TABLE V
TESTING ACCURACY AND LOSS WHEN DOF GRAVITATIONAL FORCE IS
CONSIDERED
Database Testing Accuracy Loss
FER2013 76.27% 0.4589
JAFFE 94.38% 0.1988
CK+ 96.79% 0.0685
KDEF 94.12% 0.2040
RAF 79.27% 0.3910
noted in Table V. It is observed from Table V that the results
follow the same trend as Table IV. However, the performance
is deteriorated when D is used followed by the proposed
DCNN.
 
 
 
 
 
 
  
    a) 
    b) 
     c) 
    d) 
     e) 
Fig. 6. Training and validation performances using D of gravitational force
on Ô¨Åve databases namely, a) FER2013, b) JAFFE, c) CK+, d) KDEF, and e)
RAF
F . Experimental results using both M and D
In the third experiment, the complete model depicted in Fig.
3 is used. The proposed model is run on Ô¨Åve databases. Figures
7 and 8 display the intermediate features maps obtained by
the proposed DCNN. Figures 9, 10, 11, 12, and 13 show
the one out of ten confusion matrices and the classiÔ¨Åcation
report obtained from the given confusion matrix on FER2013,
JAFFE, CK+, KDEF, and RAF databases respectively. The
reported classiÔ¨Åcation report of each database is obtained by
averaging the classiÔ¨Åcation reports of 10-folds separately. The
average accuracy obtained by the proposed model on Ô¨Åve
databases is reported in the last row of Table VIII.
G. Model Analysis
The proposed method consists of two steps. Extraction of
edge information using the GF feature descriptor is done in
the Ô¨Årst step, whereas the proposed DCNN tunes the edge
 
 
 
 
 
Fig. 7. Feature maps of the proposed DCNN, the primary row represents 1st
branch followed by the 2ndbranch before the secondary pooling layer
 
 
 
 
 
Fig. 8. Feature maps of proposed DCNN, the primary row represents 1st
branch and followed by 2ndbranch before the secondary pooling layer
 
        
NE       
AN       
DI       
FE       
HA       
SA       
SU 
      
NE       
39         
0          
0         
1         
0          
0         
0 
        
AN                
0        
40         
0         
1         
0         
0         
0 
      
DI        
0         
0        
43         
0         
0         
1         
0 
                                                                                                                                                                                                              
FE        
0         
0         
0        
55         
0         
1         
0 
      
HA        
0         
0         
0         
0        
64         
1         
0 
      
SA        
0         
0         
2         
0         
0        
50         
0 
      
SU        
0         
0         
0         
3         
0         
0        
56 
 
        
NE       
AN       
DI       
FE       
HA       
SA       
SU 
      
NE     
460                              
 32          
  2          
 13          
 35          
 63          
  6 
        
AN                 
 31         
362          
 14          
 28          
 28          
 26          
  9 
      
DI         
  2          
  3        
232          
 15          
 14          
  3          
  4 
                                                                                                                                                                                                              
FE         
 32          
 34          
  9         
342          
 17          
 39          
 36 
      
HA         
 30          
 19          
  3          
 14         
794          
 20          
 18 
      
SA         
 70          
 32          
  9          
 45          
 29         
416          
  6 
      
SU         
  7          
  7          
  5          
 17          
  8          
  5         
350 
 
   
 
Class es         
      
Accuracy                              
      
Precision                   
    
Recall                    
        
F1-Score                    
            
NE  
   0.76   
    0.73    
  0.76   
    0.75  
             
AN  
   0.74  
    0.73   
  0.74    
     0.74  
             
DI  
   0.85   
    0.82   
  0.89   
     0.85  
                                                                                                                                                                                                              
FE  
   0.67   
    0.75  
  0.68  
     0.71  
            
HA  
   0.88   
    0.85   
  0.88   
     0.86  
             
SA  
   0.69   
    0.73   
  0.76   
     0.75  
                
SU  
   0.89     
    0.88   
  0.89   
     0.88  
 
Fig. 9. Performance in terms of confusion matrix and classiÔ¨Åcation report on
FER2013 database
 
        
NE       
AN       
DI       
FE       
HA       
SA       
SU 
      
NE       
39         
0          
0         
1         
0          
0         
0 
        
AN                
0        
40         
0         
1         
0         
0         
0 
      
DI        
0         
0        
43         
0         
0         
1         
0 
                                                                                                                                                                                                              
FE        
0         
0         
0        
55         
0         
1         
0 
      
HA        
0         
0         
0         
0        
64         
1         
0 
      
SA        
0         
0         
2         
0         
0        
50         
0 
      
SU        
0         
0         
0         
3         
0         
0        
56 
 
        
NE       
AN       
DI       
FE       
HA       
SA       
SU 
      
NE           
 15                              
  0          
  0          
  0          
  0          
  0          
  2 
        
AN                 
  0         
 15          
  0          
  0          
  0          
  0          
  0 
      
DI         
  0          
  0          
 19          
  0          
  0          
  0          
  0 
                                                                                                                                                                                                              
FE         
  0          
  0          
  0         
 12          
  0          
  0          
  0 
      
HA         
  0          
  0          
  0          
  0         
 11          
  0          
  0 
      
SA         
  0          
  0          
  0          
  0          
  0         
 18          
  1 
      
SU         
  0          
  0          
  0          
  0          
  0          
  0         
 14 
 
   
 
Class es         
      
Accuracy                              
      
Precision                   
    
Recall                    
        
F1-Score                    
            
NE  
   1.00   
    0.94    
  1.00   
    0.97  
             
AN  
   1.00   
    0.96   
  1.00    
     0.98  
             
DI  
   0.961   
    1.00   
  0.92   
     0.96  
                                                                                                                                                                                                              
FE  
   1.00  
    1.00   
  1.00   
     1.00  
            
HA  
   1.00   
    0.86   
  1.00   
     0.91  
             
SA  
   0.903   
    0.96   
  0.84   
     0.90  
                
SU  
   1.00     
    1.00   
  1.00   
     1.00  
 
Fig. 10. Performance in terms of confusion matrix and classiÔ¨Åcation report
on JAFFE database
 
        
NE       
AN       
DI       
FE       
HA       
SA       
SU 
      
NE       
39         
0          
0         
1         
0          
0         
0 
        
AN                
0        
40         
0         
1         
0         
0         
0 
      
DI        
0         
0        
43         
0         
0         
1         
0 
                                                                                                                                                                                                              
FE        
0         
0         
0        
55         
0         
1         
0 
      
HA        
0         
0         
0         
0        
64         
1         
0 
      
SA        
0         
0         
2         
0         
0        
50         
0 
      
SU        
0         
0         
0         
3         
0         
0        
56 
 
        
NE       
AN       
DI       
FE       
HA       
SA       
SU 
      
NE           
 26                              
  1          
  0          
  0          
  0          
  0          
  0 
        
AN                 
  0         
 17          
  1          
  0          
  0          
  0          
  0 
      
DI         
  1          
  0          
 29          
  0          
  0          
  0          
  0 
                                                                                                                                                                                                              
FE         
  1          
  0          
  0         
 20          
  0          
  0          
  0 
      
HA         
  0          
  0          
  0          
  0         
 29          
  0          
  0 
      
SA         
  0          
  0          
  0          
  0          
  0         
 12          
  0 
      
SU         
  0          
  0          
  0          
  0          
  0          
  0         
 29 
 
   
 
Class es         
      
Accuracy                              
      
Precision                   
    
Recall                    
        
F1-Score                    
            
NE  
   0.933   
    1.00    
  0.90   
    0.95  
             
AN  
   1.00   
    0.98   
  1.00    
     0.99  
             
DI  
   1.00   
    1.00   
  1.00   
     1.00  
                                                                                                                                                                                                              
FE  
   0.954   
    1.00   
  0.90   
     0.95  
            
HA  
   1.00   
    0.98   
  1.00   
     0.99  
             
SA  
   1.00   
    0.88   
  1.00   
     0.94  
                
SU  
   1.00     
    0.98   
  1.00   
     0.99  
 
Fig. 11. Performance in terms of confusion matrix and classiÔ¨Åcation report
on CK+ database
 
        
NE       
AN       
DI       
FE       
HA       
SA       
SU 
      
NE       
39         
0          
0         
1         
0          
0         
0 
        
AN                
0        
40         
0         
1         
0         
0         
0 
      
DI        
0         
0        
43         
0         
0         
1         
0 
                                                                                                                                                                                                              
FE        
0         
0         
0        
55         
0         
1         
0 
      
HA        
0         
0         
0         
0        
64         
1         
0 
      
SA        
0         
0         
2         
0         
0        
50         
0 
      
SU        
0         
0         
0         
3         
0         
0        
56 
 
        
NE       
AN       
DI       
FE       
HA       
SA       
SU 
      
NE           
 36                              
  0          
  0          
  0          
  0          
  0          
  0 
        
AN                 
  0         
 30          
  1          
  2          
  0          
  0          
  0 
      
DI         
  0          
  1          
 29          
  0          
  0          
  0          
  1 
                                                                                                                                                                                                              
FE         
  1          
  0          
  0         
 35          
  0          
  1          
  1 
      
HA         
  0          
  0          
  0          
  0         
 44          
  0          
  0 
      
SA         
  2          
  0          
  0          
  0          
  0         
 28          
  0 
      
SU         
  0          
  0          
  0          
  2          
  0          
  0         
 31 
 
   
 
Class es         
      
Accuracy                              
      
Precision                   
    
Recall                    
        
F1-Score                    
            
NE  
   0.975   
    0.97    
  0.95   
    0.96  
             
AN  
   0.975   
    1.00   
  0.98    
     0.99  
             
DI  
   0.977   
    0.96   
  0.98   
     0.97  
                                                                                                                                                                                                              
FE  
   0.982   
    0.92   
  0.96   
     0.94  
            
HA  
   0.984   
    1.00   
  0.98   
     0.99  
             
SA  
   0.961   
    0.91   
  0.94   
     0.92  
                
SU  
   0.949     
    1.00   
  0.95   
     0.97  
 
Fig. 12. Performance in terms of confusion matrix and classiÔ¨Åcation report
on KDEF database
Authorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. 
0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE
Transactions on Instrumentation and Measurement
IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 9
 
        
NE       
AN       
DI       
FE       
HA       
SA       
SU 
      
NE       
39         
0          
0         
1         
0          
0         
0 
        
AN                
0        
40         
0         
1         
0         
0         
0 
      
DI        
0         
0        
43         
0         
0         
1         
0 
                                                                                                                                                                                                              
FE        
0         
0         
0        
55         
0         
1         
0 
      
HA        
0         
0         
0         
0        
64         
1         
0 
      
SA        
0         
0         
2         
0         
0        
50         
0 
      
SU        
0         
0         
0         
3         
0         
0        
56 
 
        
NE       
AN       
DI       
FE       
HA       
SA       
SU 
      
NE           
218                              
  7          
 10          
  3          
 20          
 45          
 17 
        
AN                 
  2         
396          
 12          
  1          
 14          
  3          
  2 
      
DI         
 11          
  7         
401          
  2          
 11          
  4          
  3 
                                                                                                                                                                                                              
FE         
  4          
  3          
  3         
160          
  2          
  4          
  0 
      
HA         
  20          
 12          
 10          
  4         
539          
  9          
  3 
      
SA         
 30          
 12          
 12          
  7          
 15         
169          
  2 
      
SU         
 20          
  2          
  2          
  5          
  4          
  3         
111 
 
   
 
Class es         
      
Accuracy                              
      
Precision                   
    
Recall                    
        
F1-Score                    
            
NE  
   0.697   
    0.68    
  0.69   
    0.68  
             
AN  
   0.940   
    0.90   
  0.94    
     0.92  
             
DI  
   0.927   
    0.88   
  0.93   
     0.90  
                                                                                                                                                                                                              
FE  
   0.916   
    0.89   
  0.90   
     0.89  
            
HA  
   0.906  
    0.89   
  0.90   
     0.89 
             
SA  
   0.684   
    0.66   
  0.67   
     0.66  
                
SU  
   0.765     
    0.78   
  0.71   
     0.74  
 
Fig. 13. Performance in terms of confusion matrix and classiÔ¨Åcation report
on RAF database
information to extract local as well as holistic features in the
second step. In [31], Roy and Bhattacharjee already showed
that the GF feature descriptor performs better than other edge
information extraction techniques. So, we have not conducted
the same experiment again. However, the performance of
the GF feature extractor is compared with two well-known
texture measures named as Local Binary Pattern (LBP) and
Gabor face descriptor in this study. A face image of size
256256 pixels is fed as an input to the above mentioned
three descriptors separately and they produce a feature vector
of size 165536 as an output. Then the obtained feature
vector is mapped into a d-dimensional polyhedron to get a
point, where the value of d is 256256 = 65536 . This process
would be repeated for all the face images of a database. We
will get a d-dimensional polyhedron at the end of this process,
where a face image would be represented as a point. A feature
descriptor is not good enough if the points of two classes
are highly overlapping. When the points of the two classes
are highly overlapping then the accuracy would decrease.
Here, the overlap is computed based on compactness and
separation. Compactness and separation deÔ¨Åne the quality of
clustering results. A cluster has good compactness when points
are close to each other and good separation when clusters do
not overlap. In other words, the ideal values of compactness
and separation are zero and inÔ¨Ånity respectively. Initially, k-
means is applied to the points to divide them into k= 7
clusters as 7 basic expressions are considered in this study.
The values of compactness and separation are computed for
the three above mentioned feature descriptors on Ô¨Åve databases
that are reported in Table VI.
TABLE VI
COMPARISON OF GRAVITATIONAL FORCE DESCRIPTOR WITH OTHER
DESCRIPTORS
DatabaseLBP
Gobar Filter Gravitational Force
compactness
separation compactness separation compactness separation
FER2013
2.1007 11.1001 1.6723 13.3321 0.3792 17.2005
J
AFFE 2.5133 16.9478 0.3947 18.3839 0.2783 32.0319
CK+
0.9196 24.2100 0.4641 22.5207 0.2941 33.2091
KDEF
1.7509 18.0879 0.1164 11.9973 0.0976 28.9084
RAF
1.4702 7.8901 0.9493 9.4510 0.2301 13.0923
It is noticed from Table VI that the value of compactness of
the GF is less as compared to LBP and Gabor face descriptor.
On the other hand, the value separation of the GF is the
highest among the three feature descriptors. Thus, we can
conclude that the GF is better compared to LBP and Gabor
face descriptor. In other words, the feature vector is more
informative and is able to distinguish a facial expression fromothers when GF is used. The same experiment is conducted
after extracting features by the proposed DCNN and other
state-of-the-art models and the values of compactness and
separation are noted in Table VII.
Two conclusions can be drawn from Table VII: GF and the
proposed DCNN jointly generates features, which have more
distinctive capabilities than the features produced by the GF
alone as the value of compactness is less and the value of
separation is high. We can thus state that GF followed by the
proposed DCNN is better than the state-of-the-art methods for
the same reason.
H. Comparative results
In the last experiment, we provide comparative results
against twenty-Ô¨Åve state-of-the-art algorithms, for example,
HOG-TOP [42], Shallow CNN [45], Major CNN [45], Shallow
CNN on LBP images [50], Shallow CNN on gray-scale images
[50], Partial VGG16 [50], Weighted mixture of double channel
[50], Weighted fusion of three subnetworks [52], Appearance-
based CNN on LBP images [54], Fusion of appearance and
geometric features [54], 3D Inception-ResNet + landmarks
[56], Autoencoders + SOM [57], Spatio-temporal feature
+ LSTM [58], Ensemble DCNNs [59], DCNN for binary
classiÔ¨Åcation [60], IACNN [61], 2B(N+M)Softmax [62], S-
DSRN [63], Color features + Gabor transform [64], DCMA-
CNNs [65], Broad Learning [66], Ensemble MLCNNs [67],
Deep-Emotion [68], VGG19 [47], and ResNet150 [47], on
Ô¨Åve publicly available databases. However, the comparison is
done based on average recognition accuracy only. Some of the
above-mentioned methods were implemented on videos. Few
works considered less number of classes. So, we change a few
of these algorithms for this study by keeping the overall archi-
tecture the same. Table VIII shows the average classiÔ¨Åcation
accuracies achieved by the above-said state-of-the-art methods.
It is clear from Table VIII that the proposed model defeats
all the twenty-Ô¨Åve existing methods on Ô¨Åve databases and it
happens due to the use of GF-based local edge features along
with holistic features extracted by the proposed DCNN, which
is our main focus. However, all the methods are also compared
based on training and testing times. Generally, the training
time of a method depends on the size of the network, size
of the input, number of epochs, number of folds, and others.
In this study, all the models are implemented according to
their respective speciÔ¨Åcations. However, we consider 10-folds
cross-validation and 60 epochs while training the proposed
method on all the databases except FER2013. The proposed
method is trained for 200 epochs for the FER2013 database
only. The training and testing times required by all the state-
of-the-art methods including the proposed method on all the
Ô¨Åve databases are reported in Table 8. However, testing time
for one-fold cross-validation is noted only in Table 8. Testing
time per image (TTPI) is the same for all the images of a
database as their size is equal. However, it varies from one
method to another. It is clear from Table 8 that the proposed
DCNN takes an average training and testing time across all
the databases except the FER2013 database. The proposed
method takes about 960 minutes to train the proposed method
for FER2013, which is quite large.
Authorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. 
0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE
Transactions on Instrumentation and Measurement
IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 10
TABLE VII
FEATURE EXTRACTION ANALYSIS OF ALL THE COMPARED METHODS AND PROPOSED ONE FOR ALL THE FIVE DATABASES
MethodFER2013
JAFFE CK+ KDEF RAF
compactness
separation compactness separation compactness separation compactness separation compactness separation
HOG-T
OP [42] 6.3401 11.9010 11.4307 18.1801 9.1060 24.1107 12.4987 18.2312 5.9019 9.2809
Shallo
w CNN [45] 5.2903 13.6850 13.1721 16.8956 10.1040 21.1652 12.9472 17.9843 2.4212 13.5219
Major
CNN [45] 3.2760 16.0395 7.2167 19.9009 4.3400 36.6180 9.8661 23.2107 1.0192 15.9305
Shallo
w CNN on LBP images [50] 5.0081 12.5627 3.3133 23.6127 4.9092 34.4410 8.3283 25.9169 2.8358 13.9102
Shallo
w CNN on gray-scale images [50] 2.0843 18.8401 2.1050 27.1873 2.0969 41.9901 6.3166 28.0498 1.1023 15.8001
P
artial VGG16 [50] 2.1037 18.9823 1.6430 31.0262 3.0974 38.5981 6.0116 29.0963 4.6783 10.3281
W
eighted mixture of double channel [50] 0.1702 20.9437 2.0051 27.9642 1.1268 46.0533 4.2403 31.1106 1.7281 14.1421
W
eighted fusion of three subnetworks [52] 3.0103 15.6149 2.7613 26.2167 3.1943 40.9165 7.2389 26.9901 2.3929 13.6780
Appearance
based CNN on LBP images [54] 7.9823 10.6843 2.9124 26.8913 1.8412 43.0935 8.9982 22.9307 1.5816 14.4389
Fusion
of appearance and geometric features [54] 5.8124 12.0844 2.0962 27.6893 2.0082 42.2190 8.6826 24.2411 1.0741 16.0381
3D
Inception-ResNet + landmarks [56] 2.1862 19.0125 1.9863 28.3013 2.5137 40.9810 3.9198 32.0250 2.4927 13.4617
Autoencoders
+ SOM [57] 1.9810 19.5213 1.8047 29.1677 2.4176 41.0869 3.9826 32.6810 1.9074 14.4837
Spatio-temporal
feature + LSTM [58] 2.3814 18.9046 2.5913 26.9033 5.1905 32.1890 4.3001 31.0241 2.6481 14.0102
Ensemble
DCNNs [59] 5.6321 12.8610 12.1622 17.4612 8.9567 26.6010 11.1725 19.4607 2.4217 13.6550
DCNN
for binary classiÔ¨Åcation [60] 7.1027 11.0913 12.8907 17.9917 6.0230 30.3851 7.9604 26.0021 3.0429 12.3489
IA
CNN [61] 2.9467 19.8437 6.3401 21.2157 1.8102 43.1880 9.6007 24.0420 1.8489 14.1013
2B(N+M)Softmax
[62] 2.7890 19.5246 5.8804 21.9672 4.0801 36.7013 4.2509 30.9103 3.0134 12.6581
S-DSRN
[63] 2.0133 19.0081 2.2081 27.8534 3.0162 39.4693 3.8964 32.8158 1.7618 14.1023
Color
features + Gabor transform [64] 2.6448 18.6162 6.0182 20.6420 3.9180 36.9401 4.8321 30.9903 2.5001 13.2978
DCMA-CNNs
[65] 1.8844 19.8916 1.8441 30.8757 2.4503 41.2251 3.7987 32.6380 1.0127 16.4190
Broad
Learning [66] 8.9162 10.2761 1.9962 29.0123 5.2629 31.0829 2.0312 38.1023 3.7320 11.8902
Ensemble
MLCNNs [67] 1.2491 20.0617 1.8001 29.8962 2.4984 41.0012 2.6512 33.8138 0.9326 16.8721
Deep-Emotion
[68] 2.5193 19.2013 1.9125 28.9102 2.2630 41.8230 4.3612 31.0883 2.5216 14.1852
V
GG19 [47] 1.1347 20.1933 1.3237 30.9817 1.4866 44.1805 4.3791 31.1990 2.9987 12.9029
ResNet150
[47] 0.1672 20.0347 2.4340 27.1583 2.8682 38.0113 7.5981 26.0894 2.4901 13.4823
Pr
oposed method 0.0916 23.4823 0.1525 36.3913 0.2760 51.9503 0.0844 46.5627 0.0727 17.1048
TABLE VIII
CLASSIFICATION ACCURACY (%) AND EXECUTION TIME IN SECONDS ON FIVE DATABASES VIZ . FER2013, JAFFE, CK+, KDEF, AND RAF BY VARIOUS
METHODS
ClassiÔ¨Åcation
Accuracy (%) on Five Databases Execution Time in Minutes
Sl.
No. Method FER2013 JAFFE CK+ KDEF RAF TTPITraining Time Testing Time for all the Images
FER2013
JAFFE CK+ KDEF RAF FER2013 JAFFE CK+ KDEF RAF
1.
HOG-TOP [42] 52 60 65 55 53 0.4500 220.0 86.0 92.33 115.0 165.0 2.1623 1.2124 1.3264 1.1576 2.1529
2.
Shallow CNN [45] 55 50 61 55 70 0.1812 5.5 1.0 1.5 2.5 4.0 0.9992 0.8845 0.8985 0.9001 0.9845
3.
Major CNN [45] 64 68 85 67 78 0.1872 9.0 1.5 2.0 2.5 6.0 1.1060 0.8883 0.9127 0.9168 1.0921
4.
Shallow CNN on LBP images [50] 54 88 83 70 71 0.1801 75.0 8.33 16.33 25.0 41.66 1.0721 0.8821 0.8991 0.9008 0.9698
5.
Shallow CNN on gray-scale images [50] 72 92 94 78 78 0.1801 75.0 8.33 16.33 25.0 41.66 1.0721 0.8821 0.8991 0.9008 0.9698
6.
Partial VGG16 [50] 72 96 91 78 57 0.4309 441.66 150.0 158.33 175.0 316.66 1.9348 1.4508 1.5238 1.5523 1.8900
7.
Weighted mixture of double channel [50] 75 92 97 81 75 0.4699 516.66 158.33 174.99 200.0 358.32 2.2602 1.9665 1.9789 1.9965 2.1056
8.
Weighted fusion of three subnetworks [52] 63 90 91 74 70 0.8956 150.0 9.63 10.0 16.6 70.0 2.2012 1.9804 1.9912 1.9989 2.1020
9.
Appearance based CNN on LBP images [54] 50 90 95 66 77 0.3945 105.0 10.0 20.0 30.0 70.0 1.3563 1.1230 1.1321 1.1394 1.2953
10.
Fusion of appearance and geometric features [54] 53 90 94 69 78 0.4612 280.0 70.0 110.0 135.0 210.0 2.3906 1.9023 1.9984 2.0102 2.2134
11.
3D Inception-ResNet + landmarks [56] 72 93 93 83 70 0.8926 340.0 110.0 140.0 186.0 213.0 2.3472 1.9245 1.8612 1.9946 2.1980
12.
Autoencoders + SOM [57] 73 93 94 84 76 0.3298 240.0 85.0 96.66 110.0 153.33 2.1089 1.1426 1.1623 1.1982 1.9036
13.
Spatio-temporal feature + LSTM [58] 71 90 82 81 73 0.4329 460.0 205.0 215.33 240.0 340.0 3.1652 2.6743 2.8628 2.9845 3.1107
14.
Ensemble DCNNs [59] 53 55 67 58 70 0.4917 420.0 200.0 220.0 300.0 360.0 3.8138 2.5827 2.6296 2.7013 3.4238
15.
DCNN for binary classiÔ¨Åcation [60] 50 56 73 70 68 0.1725 103.33 24.66 28.0 37.33 75.0 1.6239 1.3469 1.4430 1.4523 1.5426
16.
IACNN [61] 68 75 95 67 74 0.3946 1033.0 600.0 633.0 700.0 866.66 2.1623 1.8920 1.9165 1.9730 2.0935
17.
2B(N+M)Softmax [62] 67 78 87 81 69 0.2814 265.0 80.0 85.33 93.33 165.0 1.1721 0.9643 0.9892 0.9946 1.0021
18.
S-DSRN [63] 72 92 92 83 75 0.1927 980.0 320.0 340.0 380.33 460.0 2.3042 1.9756 1.9878 2.0262 2.1040
19.
Color features + Gabor transform [64] 66 77 86 80 70 0.3218 240.0 98.66 100.0 115.33 180.66 2.8794 2.2167 2.4510 2.5503 2.7165
20.
DCMA-CNNs [65] 73 95 93 83 78 0.3129 340.0 160.0 180.0 190.0 230.0 1.2439 1.1092 1.1123 1.1706 1.2034
21.
Broad Learning [66] 44 93 81 89 64 0.1023 7.12 1.0 1.5 2.0 4.5 0.4812 0.3101 0.3323 0.3812 0.4102
22.
Ensemble MLCNNs [67] 74 94 93 85 79 0.5612 88.33 23.33 25.0 28.33 63.33 2.3024 1.9823 1.9986 2.0145 2.1876
23.
Deep-Emotion [68] 70 93 94 81 72 0.2908 316.0 41.6 50.0 66.66 241.66 1.0982 0.8943 0.9165 0.9346 0.9978
24.
VGG19 [47] 74 95 96 81 60 0.6128 45.5 22.0 23.0 26.5 34.5 2.2123 1.9821 1.9901 2.0981 2.1823
25.
ResNet150 [47] 75 91 89 72 70 0.7123 130.0 54.0 67.0 76.5 105.5 3.1190 2.5981 2.6180 2.7833 2.9009
26. Pr
oposed method 78 98 98 96 83 0.3039 960.0 200.0 240.0 320.0 400.0 2.0974 1.6919 1.7346 1.7983 1.9029
V. C ONCLUSION
It is clear from the empirical results that the proposed
method can efÔ¨Åciently handle the problem of FER using
static/still images. Facial expressions under lab-controlled en-
vironments are different from those in the wild, which are more
natural and spontaneous. So, three databases namely, JAFFE,
CK+, and KDEF, developed in a lab-controlled environment
are considered in this work. This study also adopted two
databases viz., FER2013, and RAF built in the wild to demon-
strate the efÔ¨Åcacy of the proposed method over state-of-the-art
methods. A novel DCNN framework is introduced to extract
holistic features for identifying facial expression. However,
before the use of the proposed DCNN model, a GF-based
edge descriptor is adopted to fetch the low-level local features.
The GF-based edge descriptor produces two intermediate local
features namely, M and D. At the end of the proposed DCNN
model, a softmax classiÔ¨Åer is used to compute the probability
values in favor of either seven facial expressions. Finally,
a score-level fusion technique is employed to combine theoutputs obtained by the proposed model using M and D. The
proposed method achieves an average recognition accuracy of
78%, 98%, 98%, 96%, and 83% for FER2013, JAFFE, CK+,
KDEF, and RAF respectively. Empirical results demonstrate
that local as well as holistic features can together enhance
the FER task. Experimental results also illustrate that the
proposed method outperforms twenty-Ô¨Åve baseline methods
by considering the average time. However, the performance
is generally not as good as that in FER under lab-controlled
environment, which deserves further study. Moreover, it is
worth investigating to deploy the proposed model in some
real-life applications.
This work is partially supported by the project ‚ÄúPre-
diction of diseases through computer assisted diagnosis
system using images captured by minimally-invasive and
non-invasive modalities‚Äù, Computer Science and Engineer-
ing, PDPM Indian Institute of Information Technology, De-
sign and Manufacturing, Jabalpur India (under ID: SPARC-
MHRD-231). This work is also partially supported by the
project IT4Neuro(degeneration), reg. nr. CZ.02.1.01/0.0/0.0/18
Authorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. 
0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE
Transactions on Instrumentation and Measurement
IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 11
069/0010054, by the project ‚ÄúSmart Solutions in Ubiquitous
Computing Environments‚Äù, Grant Agency of Excellence, Uni-
versity of Hradec Kralove, Faculty of Informatics and Man-
agement, Czech Republic (under ID: UHK-FIM-GE-2020);
project at Universiti Teknologi Malaysia (UTM) under Re-
search University Grant V ot-20H04, Malaysia Research Uni-
versity Network (MRUN) V ot 4L876 and the Fundamental
Research Grant Scheme (FRGS) V ot5F073 supported under
Ministry of Education Malaysia for the completion of the
research.
REFERENCES
[1] R. A. Calvo and S. D‚ÄôMello, ‚ÄúAffect detection: An interdisciplinary
review of models, methods, and their applications,‚Äù IEEE Transactions
on affective computing, vol. 1, no. 1, pp. 18‚Äì37, 2010.
[2] C. A. Corneanu, M. O. Sim ¬¥on, J. F. Cohn, and S. E. Guerrero,
‚ÄúSurvey on rgb, 3d, thermal, and multimodal approaches for facial
expression recognition: History, trends, and affect-related applications,‚Äù
IEEE transactions on pattern analysis and machine intelligence, vol. 38,
no. 8, pp. 1548‚Äì1568, 2016.
[3] P. Werner, A. Al-Hamadi, K. Limbrecht-Ecklundt, S. Walter, S. Gruss,
and H. C. Traue, ‚ÄúAutomatic pain assessment with facial activity
descriptors,‚Äù IEEE Transactions on Affective Computing, vol. 8, no. 3,
pp. 286‚Äì299, 2016.
[4] C. Liu and H. Wechsler, ‚ÄúGabor feature based classiÔ¨Åcation using the
enhanced Ô¨Åsher linear discriminant model for face recognition,‚Äù IEEE
Transactions on Image processing, vol. 11, no. 4, pp. 467‚Äì476, 2002.
[5] A. Seal, S. Ganguly, D. Bhattacharjee, M. Nasipuri, and D. K. Basu,
‚ÄúThermal human face recognition based on haar wavelet transform and
series matching technique,‚Äù in Multimedia Processing, Communication
and Computing Applications. Springer, 2013, pp. 155‚Äì167.
[6] D. Bhattacharjee, A. Seal, S. Ganguly, M. Nasipuri, and D. K. Basu,
‚ÄúA comparative study of human thermal face recognition based on haar
wavelet transform and local binary pattern,‚Äù Computational intelligence
and neuroscience, vol. 2012, 2012.
[7] M. Kopaczka, R. Kolk, J. Schock, F. Burkhard, and D. Merhof, ‚ÄúA ther-
mal infrared face database with facial landmarks and emotion labels,‚Äù
IEEE Transactions on Instrumentation and Measurement, vol. 68, no. 5,
pp. 1389‚Äì1401, 2018.
[8] N. Dalal and B. Triggs, ‚ÄúHistograms of oriented gradients for human
detection,‚Äù in 2005 IEEE computer society conference on computer
vision and pattern recognition (CVPR‚Äô05), vol. 1. IEEE, 2005, pp.
886‚Äì893.
[9] A. Seal, D. Bhattacharjee, M. Nasipuri, C. Gonzalo-Martin, and
E. Menasalvas, ‚ÄúHistogram of bunched intensity values based thermal
face recognition,‚Äù in Rough Sets and Intelligent Systems Paradigms.
Springer, 2014, pp. 367‚Äì374.
[10] S. Onta Àún¬¥on, J. L. Monta Àúna, and A. J. Gonzalez, ‚ÄúA dynamic-bayesian
network framework for modeling and evaluating learning from observa-
tion,‚Äù Expert Systems with Applications, vol. 41, no. 11, pp. 5212‚Äì5226,
2014.
[11] A. Seal, S. Ganguly, D. Bhattacharjee, M. Nasipuri, and D. K. Basu,
‚ÄúAutomated thermal face recognition based on minutiae extraction,‚Äù
arXiv preprint arXiv:1309.1000, 2013.
[12] Y . Gao, M. K. Leung, S. C. Hui, and M. W. Tananda, ‚ÄúFacial expression
recognition from line-based caricatures,‚Äù IEEE Transactions on Systems,
Man, and Cybernetics-Part A: Systems and Humans, vol. 33, no. 3, pp.
407‚Äì412, 2003.
[13] M. D. Cordea, E. M. Petriu, and D. C. Petriu, ‚ÄúThree-dimensional head
tracking and facial expression recovery using an anthropometric muscle-
based active appearance model,‚Äù IEEE transactions on instrumentation
and measurement, vol. 57, no. 8, pp. 1578‚Äì1588, 2008.
[14] Z. Xu, H. R. Wu, X. Yu, K. Horadam, and B. Qiu, ‚ÄúRobust shape-
feature-vector-based face recognition system,‚Äù IEEE Transactions on
Instrumentation and Measurement, vol. 60, no. 12, pp. 3781‚Äì3791, 2011.
[15] J. Li, D. Zhang, J. Zhang, J. Zhang, T. Li, Y . Xia, Q. Yan, and L. Xun,
‚ÄúFacial expression recognition with faster r-cnn,‚Äù Procedia Computer
Science, vol. 107, pp. 135‚Äì140, 2017.
[16] Y . Sun, X. Wang, and X. Tang, ‚ÄúDeep learning face representation from
predicting 10,000 classes,‚Äù in Proceedings of the IEEE conference on
computer vision and pattern recognition, 2014, pp. 1891‚Äì1898.[17] P. Liu, S. Han, Z. Meng, and Y . Tong, ‚ÄúFacial expression recognition via
a boosted deep belief network,‚Äù in Proceedings of the IEEE conference
on computer vision and pattern recognition, 2014, pp. 1805‚Äì1812.
[18] H. Jung, S. Lee, J. Yim, S. Park, and J. Kim, ‚ÄúJoint Ô¨Åne-tuning in deep
neural networks for facial expression recognition,‚Äù in Proceedings of
the IEEE international conference on computer vision, 2015, pp. 2983‚Äì
2991.
[19] C. Liu and H. Wechsler, ‚ÄúEnhanced Ô¨Åsher linear discriminant models for
face recognition,‚Äù in Proceedings. Fourteenth International Conference
on Pattern Recognition (Cat. No. 98EX170), vol. 2. IEEE, 1998, pp.
1368‚Äì1372.
[20] T. Ahonen, A. Hadid, and M. Pietikainen, ‚ÄúFace description with local
binary patterns: Application to face recognition,‚Äù IEEE transactions on
pattern analysis and machine intelligence, vol. 28, no. 12, pp. 2037‚Äì
2041, 2006.
[21] D. G. Lowe, ‚ÄúDistinctive image features from scale-invariant keypoints,‚Äù
International journal of computer vision , vol. 60, no. 2, pp. 91‚Äì110,
2004.
[22] Z. Li, ‚ÄúA discriminative learning convolutional neural network for facial
expression recognition,‚Äù in 2017 3rd IEEE international conference on
computer and communications (ICCC). IEEE, 2017, pp. 1641‚Äì1646.
[23] M. Mirza and S. Osindero, ‚ÄúConditional generative adversarial nets,‚Äù
arXiv preprint arXiv:1411.1784, 2014.
[24] H. Yang, Z. Zhang, and L. Yin, ‚ÄúIdentity-adaptive facial expression
recognition through expression regeneration using conditional generative
adversarial networks,‚Äù in 2018 13th IEEE International Conference on
Automatic Face & Gesture Recognition (FG 2018). IEEE, 2018, pp.
294‚Äì301.
[25] A. Fathallah, L. Abdi, and A. Douik, ‚ÄúFacial expression recognition
via deep learning,‚Äù in 2017 IEEE/ACS 14th International Conference
on Computer Systems and Applications (AICCSA). IEEE, 2017, pp.
745‚Äì750.
[26] M. T. B. Iqbal, M. Abdullah-Al-Wadud, B. Ryu, F. Makhmudkhujaev,
and O. Chae, ‚ÄúFacial expression recognition with neighborhood-aware
edge directional pattern (nedp),‚Äù IEEE Transactions on Affective Com-
puting, 2018.
[27] A. Bhavsar and H. M. Patel, ‚ÄúFacial expression recognition using neural
classiÔ¨Åer and fuzzy mapping,‚Äù in 2005 Annual IEEE India Conference-
Indicon. IEEE, 2005, pp. 383‚Äì387.
[28] T. Jabid, M. H. Kabir, and O. Chae, ‚ÄúRobust facial expression recog-
nition based on local directional pattern,‚Äù ETRI journal, vol. 32, no. 5,
pp. 784‚Äì794, 2010.
[29] P. Zhao-Yi, Z. Yan-Hui, and Z. Yu, ‚ÄúReal-time facial expression recogni-
tion based on adaptive canny operator edge detection,‚Äù in 2010 Second
International Conference on MultiMedia and Information Technology,
vol. 2. IEEE, 2010, pp. 154‚Äì157.
[30] R. Samad and H. Sawada, ‚ÄúEdge-based facial feature extraction using
gabor wavelet and convolution Ô¨Ålters.‚Äù in MVA, 2011, pp. 430‚Äì433.
[31] D. Bhattacharjee and H. Roy, ‚ÄúPattern of local gravitational force (plgf):
A novel local image descriptor,‚Äù IEEE transactions on pattern analysis
and machine intelligence, 2019.
[32] I. J. Goodfellow, D. Erhan, P. L. Carrier, A. Courville, M. Mirza,
B. Hamner, W. Cukierski, Y . Tang, D. Thaler, D.-H. Lee et al.,
‚ÄúChallenges in representation learning: A report on three machine
learning contests,‚Äù in International Conference on Neural Information
Processing. Springer, 2013, pp. 117‚Äì124.
[33] M. J. Lyons, S. Akamatsu, M. Kamachi, J. Gyoba, and J. Budynek, ‚ÄúThe
japanese female facial expression (jaffe) database,‚Äù in Proceedings of
third international conference on automatic face and gesture recognition,
1998, pp. 14‚Äì16.
[34] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews,
‚ÄúThe extended cohn-kanade dataset (ck+): A complete dataset for action
unit and emotion-speciÔ¨Åed expression,‚Äù in 2010 ieee computer soci-
ety conference on computer vision and pattern recognition-workshops.
IEEE, 2010, pp. 94‚Äì101.
[35] D. Lundqvist, A. Flykt, and A. ¬®Ohman, ‚ÄúThe karolinska directed
emotional faces (kdef),‚Äù CD ROM from Department of Clinical Neu-
roscience, Psychology section, Karolinska Institutet, vol. 91, no. 630,
pp. 2‚Äì2, 1998.
[36] S. Li and W. Deng, ‚ÄúReliable crowdsourcing and deep locality-
preserving learning for unconstrained facial expression recognition,‚Äù
IEEE Transactions on Image Processing, vol. 28, no. 1, pp. 356‚Äì370,
2019.
[37] S. Li, W. Deng, and J. Du, ‚ÄúReliable crowdsourcing and deep locality-
preserving learning for expression recognition in the wild,‚Äù in 2017
IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
IEEE, 2017, pp. 2584‚Äì2593.
Authorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. 
0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE
Transactions on Instrumentation and Measurement
IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 12
[38] X. Chen and W. Cheng, ‚ÄúFacial expression recognition based on edge
detection,‚Äù International Journal of Computer Science and Engineering
Survey, vol. 6, no. 2, p. 1, 2015.
[39] M. Abdulrahman and A. Eleyan, ‚ÄúFacial expression recognition using
support vector machines,‚Äù in 2015 23nd Signal Processing and Commu-
nications Applications Conference (SIU). IEEE, 2015, pp. 276‚Äì279.
[40] C. Laurent, G. Pereyra, P. Brakel, Y . Zhang, and Y . Bengio, ‚ÄúBatch
normalized recurrent neural networks,‚Äù in 2016 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2016, pp. 2657‚Äì2661.
[41] D. Ghimire and J. Lee, ‚ÄúExtreme learning machine ensemble using
bagging for facial expression recognition.‚Äù JIPS, vol. 10, no. 3, pp. 443‚Äì
458, 2014.
[42] J. Chen, Z. Chen, Z. Chi, and H. Fu, ‚ÄúFacial expression recognition
in video with multiple feature fusion,‚Äù IEEE Transactions on Affective
Computing, vol. 9, no. 1, pp. 38‚Äì50, 2016.
[43] Z. Yu and C. Zhang, ‚ÄúImage based static facial expression recognition
with multiple deep network learning,‚Äù in Proceedings of the 2015 ACM
on international conference on multimodal interaction , 2015, pp. 435‚Äì
442.
[44] X. Zhao, X. Liang, L. Liu, T. Li, Y . Han, N. Vasconcelos, and
S. Yan, ‚ÄúPeak-piloted deep network for facial expression recognition,‚Äù in
European conference on computer vision. Springer, 2016, pp. 425‚Äì442.
[45] S. Alizadeh and A. Fazel, ‚ÄúConvolutional neural networks for facial
expression recognition, 1704.06756,‚Äù 2017.
[46] Y . Tang, ‚ÄúDeep learning using support vector machines,‚Äù arXiv preprint
arXiv:1306.0239, 2013.
[47] D. Orozco, C. Lee, Y . Arabadzhi, and D. Gupta, ‚ÄúTransfer learning for
facial expression recognition.‚Äù
[48] Y . Sun, Y . Chen, X. Wang, and X. Tang, ‚ÄúDeep learning face rep-
resentation by joint identiÔ¨Åcation-veriÔ¨Åcation,‚Äù in Advances in neural
information processing systems, 2014, pp. 1988‚Äì1996.
[49] E. Barsoum, C. Zhang, C. C. Ferrer, and Z. Zhang, ‚ÄúTraining deep
networks for facial expression recognition with crowd-sourced label
distribution,‚Äù in Proceedings of the 18th ACM International Conference
on Multimodal Interaction, 2016, pp. 279‚Äì283.
[50] B. Yang, J. Cao, R. Ni, and Y . Zhang, ‚ÄúFacial expression recognition
using weighted mixture deep neural network based on double-channel
facial images,‚Äù IEEE Access, vol. 6, pp. 4630‚Äì4640, 2017.
[51] A. Ravi, ‚ÄúPre-trained convolutional neural network features for facial
expression recognition,‚Äù arXiv preprint arXiv:1812.06387, 2018.
[52] W. Hua, F. Dai, L. Huang, J. Xiong, and G. Gui, ‚ÄúHero: Human emotions
recognition for realizing intelligent internet of things,‚Äù IEEE Access,
vol. 7, pp. 24 321‚Äì24 332, 2019.
[53] A. Mollahosseini, B. Hasani, and M. H. Mahoor, ‚ÄúAffectnet: A database
for facial expression, valence, and arousal computing in the wild,‚Äù IEEE
Transactions on Affective Computing, vol. 10, no. 1, pp. 18‚Äì31, 2017.
[54] J.-H. Kim, B.-G. Kim, P. P. Roy, and D.-M. Jeong, ‚ÄúEfÔ¨Åcient facial
expression recognition algorithm based on hierarchical deep neural
network structure,‚Äù IEEE Access, vol. 7, pp. 41 273‚Äì41 285, 2019.
[55] P. Dhankhar, ‚ÄúResnet-50 and vgg-16 for recognizing facial emotions,‚Äù
International Journal of Innovations in Engineering and Technology
(IJIET), vol. 13, no. 4, pp. 126‚Äì130, 2019.
[56] B. Hasani and M. H. Mahoor, ‚ÄúFacial expression recognition using
enhanced deep 3d convolutional neural networks,‚Äù in Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition
Workshops, 2017, pp. 30‚Äì40.
[57] A. Majumder, L. Behera, and V . K. Subramanian, ‚ÄúAutomatic facial
expression recognition system using deep network-based data fusion,‚Äù
IEEE transactions on cybernetics, vol. 48, no. 1, pp. 103‚Äì114, 2016.
[58] D. H. Kim, W. J. Baddar, J. Jang, and Y . M. Ro, ‚ÄúMulti-objective based
spatio-temporal feature representation learning robust to expression
intensity variations for facial expression recognition,‚Äù IEEE Transactions
on Affective Computing, vol. 10, no. 2, pp. 223‚Äì236, 2017.
[59] G. Pons and D. Masip, ‚ÄúSupervised committee of convolutional neural
networks in automated facial expression analysis,‚Äù IEEE Transactions
on Affective Computing, vol. 9, no. 3, pp. 343‚Äì350, 2017.
[60] M. G. Villanueva and S. R. Zavala, ‚ÄúDeep neural network architecture:
application for facial expression recognition,‚Äù IEEE Latin America
Transactions, vol. 18, no. 07, pp. 1311‚Äì1319, 2020.
[61] Z. Meng, P. Liu, J. Cai, S. Han, and Y . Tong, ‚ÄúIdentity-aware con-
volutional neural network for facial expression recognition,‚Äù in 2017
12th IEEE International Conference on Automatic Face & Gesture
Recognition (FG 2017). IEEE, 2017, pp. 558‚Äì565.
[62] X. Liu, B. Vijaya Kumar, J. You, and P. Jia, ‚ÄúAdaptive deep metric
learning for identity-aware facial expression recognition,‚Äù in Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition
Workshops, 2017, pp. 20‚Äì29.
[63] M. Alam, L. S. Vidyaratne, and K. M. Iftekharuddin, ‚ÄúSparse simulta-
neous recurrent deep learning for robust facial expression recognition,‚Äù
IEEE transactions on neural networks and learning systems, vol. 29,
no. 10, pp. 4905‚Äì4916, 2018.
[64] C. F. Benitez-Quiroz, R. Srinivasan, and A. M. Martinez, ‚ÄúDiscriminant
functional learning of color features for the recognition of facial action
units and their intensities,‚Äù IEEE transactions on pattern analysis and
machine intelligence, vol. 41, no. 12, pp. 2835‚Äì2845, 2018.
[65] S. Xie and H. Hu, ‚ÄúFacial expression recognition using hierarchical fea-
tures with deep comprehensive multipatches aggregation convolutional
neural networks,‚Äù IEEE Transactions on Multimedia, vol. 21, no. 1, pp.
211‚Äì220, 2018.
[66] T. Zhang, Z. Liu, X.-H. Wang, X.-F. Xing, C. P. Chen, and E. Chen,
‚ÄúFacial expression recognition via broad learning system,‚Äù in 2018 IEEE
International Conference on Systems, Man, and Cybernetics (SMC).
IEEE, 2018, pp. 1898‚Äì1902.
[67] D. H. Nguyen, S. Kim, G.-S. Lee, H.-J. Yang, I.-S. Na, and S. H.
Kim, ‚ÄúFacial expression recognition using a temporal ensemble of multi-
level convolutional neural networks,‚Äù IEEE Transactions on Affective
Computing, 2019.
[68] S. Minaee and A. Abdolrashidi, ‚ÄúDeep-emotion: Facial expression
recognition using attentional convolutional network,‚Äù arXiv preprint
arXiv:1902.01019, 2019.
[69] Y . Liu, X. Zhang, Y . Lin, and H. Wang, ‚ÄúFacial expression recognition
via deep action units graph network based on psychological mechanism,‚Äù
IEEE Transactions on Cognitive and Developmental Systems, 2019.
[70] H. Roy and D. Bhattacharjee, ‚ÄúLocal-gravity-face (lg-face) for
illumination-invariant and heterogeneous face recognition,‚Äù IEEE Trans-
actions on Information Forensics and Security, vol. 11, no. 7, pp. 1412‚Äì
1424, 2016.
[71] X. Glorot, A. Bordes, and Y . Bengio, ‚ÄúDeep sparse rectiÔ¨Åer neural
networks,‚Äù in Proceedings of the fourteenth international conference on
artiÔ¨Åcial intelligence and statistics, 2011, pp. 315‚Äì323.
[72] Y . Bengio, ‚ÄúPractical recommendations for gradient-based training of
deep architectures,‚Äù in Neural networks: Tricks of the trade. Springer,
2012, pp. 437‚Äì478.
[73] A. Ross and A. Jain, ‚ÄúInformation fusion in biometrics,‚Äù Pattern
recognition letters, vol. 24, no. 13, pp. 2115‚Äì2125, 2003.
Karnati Mohan is pursuing PhD in computer
science department, PDPM Indian Institute of
Information Technology, Design and Manufactur-
ing Jabalpur, Madhya Pradesh, India and he has
also received the M. tech degree from the same
institute in 2020. He received his B. Tech degree in
computer science and engineering from Jawahar-
lal Technological University, Hyderabad, India
in 2016. His research interests include machine
learning, affect recognition, deep learning, and
Image processing. He has served as a reviewer
for IEEE Access journal. He has also served as a program committee
member for ISAC conference.
Authorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. 
0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE
Transactions on Instrumentation and Measurement
IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 13
Ayan Seal [M‚Äô14, SM‚Äô19] received the PhD degree
in Engineering from Jadavpur University, West
Bengal, India, in 2014. He is currently an As-
sistant Professor with the Computer Science and
Engineering Department, PDPM Indian Institute
of Information Technology, Design and Manu-
facturing Jabalpur, Jabalpur, Madhya Pradesh,
India. He has visited the Universidad Politecnica
de Madrid, Spain as a visiting research scholar.
He is the recipient of several awards. He has re-
ceived Sir Visvesvaraya Young Faculty Research
Fellowship from Media Lab Asia, Ministry of Electronics and Information
Technology, Government of India. He is a senior member of IEEE.
He has authored or co-authored of several journals, conferences and
book chapters in the area of biometric and medical image processing.
His current research interests include image processing and pattern
recognition.
Ondrej Krejcar is a full professor in systems
engineering and informatics at the University
of Hradec Kralove, Czech Republic. In 2008 he
received his Ph.D. title in technical cybernetics at
Technical University of Ostrava, Czech Republic.
He is currently a vice-rector for science and
creative activities of the University of Hradec
Kralove from June 2020. At present, he is also
a director of the Center for Basic and Applied
Research at the University of Hradec Kralove. In
years 2016-2020 he was vice-dean for science and
research at Faculty of Informatics and Management, UHK. His h-index
is 18, with more than 1150 citations received in the Web of Science.
In 2018, he was the 14th top peer reviewer in Multidisciplinary in the
World according to Publons and a Top Reviewer in the Global Peer
Review Awards 2019 by Publons. Currently, he is on the editorial board
of the MDPI Sensors IF journal (Q1/Q2 at JCR), and several other
ESCI indexed journals. He is a Vice-leader and Management Committee
member at WG4 at project COST CA17136, since 2018. He has also been
a Management Committee member substitute at project COST CA16226
since 2017. Since 2019, he has been Chairman of the Program Committee
of the KAPPA Program, Technological Agency of the Czech Republic as
a regulator of the EEA/Norwegian Financial Mechanism in the Czech
Republic (2019-2024). Since 2020, he has been Chairman of the Panel
1 (Computer, Physical and Chemical Sciences) of the ZETA Program,
Technological Agency of the Czech Republic. Since 2014 until 2019, he has
been Deputy Chairman of the Panel 7 (Processing Industry, Robotics, and
Electrical Engineering) of the Epsilon Program, Technological Agency
of the Czech Republic. At the University of Hradec Kralove, he is a
guarantee of the doctoral study program in Applied Informatics, where
he is focusing on lecturing on Smart Approaches to the Development
of Information Systems and Applications in Ubiquitous Computing
Environments. His research interests include Control Systems, Smart
Sensors, Ubiquitous Computing, Manufacturing, Wireless Technology,
Portable Devices, biomedicine, image segmentation and recognition,
biometrics, technical cybernetics, and ubiquitous computing. His second
area of interest is in Biomedicine (image analysis), as well as Biotelemetric
System Architecture (portable device architecture, wireless biosensors),
development of applications for mobile devices with use of remote or
embedded biomedical sensors.
Anis Yazidi received the M.Sc. and Ph.D. degrees
from the University of Agder, Grimstad, Nor-
way, in 2008 and 2012, respectively. He was a
Researcher with Teknova AS, Grimstad, Norway.
From 2014 till 2019 he was an associate professor
with the Department of Computer Science, Oslo
Metropolitan University, Oslo, Norway. He is cur-
rently a Full Professor with the same department
where he is leading the research group in Applied
ArtiÔ¨Åcial Intelligence. He is also Professor II
with the Norwegian University of Science and
Technology (NTNU), Trondheim, Norway. His current research interests
include machine learning, learning automata, stochastic optimization, and
autonomous computing.
Authorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. 
"
https://ieeexplore.ieee.org/document/8308363,"Facial Expression Recognition
via Deep Learning
Abir Fathallah
ENISO, Sousse university
Sousse, Tunisia
Email: abir.fathallah1803@gmail.comLotÔ¨Å Abdi
ENISO, Sousse university
Sousse, Tunisia
Email: lotÔ¨Åabdi@hotmail.comAli Douik
ENISO, Sousse university
Sousse, Tunisia
Email: ali.douik@enim.rnu.tn
Abstract ‚ÄîAutomated Facial Expression Recognition has re-
mained a challenging and interesting problem in computer vision.
The recognition of facial expressions is difÔ¨Åcult problem for
machine learning techniques, since people can vary signiÔ¨Åcantly
in the way they show their expressions. Deep learning is a new
area of research within machine learning method which can
classify images of human faces into emotion categories using Deep
Neural Networks (DNN). Convolutional neural networks (CNN)
have been widely used to overcome the difÔ¨Åculties in facial expres-
sion classiÔ¨Åcation. In this paper, we present a new architecture
network based on CNN for facial expressions recognition. We
Ô¨Åne tuned our architecture with Visual Geometry Group model
(VGG) to improve results. To evaluate our architecture we tested
it with many largely public databases (CK+, MUG, and RAFD).
Obtained results show that the CNN approach is very effective
in image expression recognition on many public databases which
achieve an improvements in facial expression analysis.
Index T erms ‚ÄîFacial Expression; Recognition; Deep Learning;
CNN; Architecture; ClassiÔ¨Åcation.
I. I NTRODUCTION
Interaction Human-Machine (IHM) has long conÔ¨Åned re-
searches to develop techniques based on the use of triplet
screen-keyboard-mouse. Today, it is moving towards new
paradigms: the user must be able to evolve unimpeded in its
natural environment; Ô¨Ångers,hand, face or familiar objects are
seen as many devices input/output; the boundary between the
physical and electronic worlds is blurring. These new forms
of interaction require usually the capture of the observable
behavior of the user and his environment. That is why they
rely on artiÔ¨Åcial perception techniques, including computer
vision. IHM is a rapidly evolving discipline. Future genera-
tions of human-machine environment will become multimodal
integrating new information, from the consideration of the
dynamic behavior, speech and/or facial expressions, so as to
make the use of machines the most intuitive and natural as
possible.
The face is the most expressive and communicative part of
a human being [1], it represents a major focus in current
research concerning the improvement of IHM for establishing
a dialogue between the two entities.
Facial expression is a visible manifestation of a face from
the state of mind (emotion, reÔ¨Çection), cognitive activity,
physiological (fatigue, pain), personality and psychopathology
of a person. The essential of facial expression informationis contained in the deformation of main permanent facial
features, characterized by a change visually perceptible.
Today, analysis computer assisted of face and its facial ex-
pressions is an emerging Ô¨Åeld. Emotion recognition consists
in associating an emotion to face image. So the goal is
to determine from a face, the internal emotional state of a
person. An automatic facial expression recognition system
is an important component in human machine interaction. It
consists to evaluate the possibility of emotions recognition.
However, this is not an easy task.
Facial expression recognition usually employs a three-stage
training consisting of face Acquisition [2], facial feature
extraction [3] and classiÔ¨Åer construction [4, 5].
Recently, Many works [6, 7] demonstrated that expression
recognition can beneÔ¨Åt from collecting two stages together
facial feature extraction and classiÔ¨Åer construction.
Deep learning methods have been successfully applied to
extract features and classiÔ¨Åcation, in particular Convolutional
Neural Networks (CNN) architectures which are biologically-
inspired multi-stage one that learned automatically hierarchies
of invariant features [8]. The ConvNets consist of a multi-
stage processing of an input image to extract hierarchical and
high-level feature representations.
Motivated by this, we present in this paper an effective
approach system based on ConvNets for facial expression
recognition. We proposed a new architecture which the input
of the system is an image; then, we use CNN to predict the
facial expression label which should be one these labels[9]:
anger,happiness, sadness, disgust, surprise and neutral.
II. R ELATED WORK
Many facial expression recognition methods represent faces
by high-dimensional over-complete face descriptors, followed
by shallow models. In this context, MLIKI et al. [10] proposed
a method which is able to monitor the intensity variation of
facial expression and to reduce the classiÔ¨Åcation confusion
between facial expressions classes. The approach uses Vector
Field Convolution (VFC) method to segment facial feature
contours. COTRET et al. [11] also proposed a wavelet-based
face recognition method robust against face position and light
variations for real-time applications. Wang et al. [12] proposed
expression recognition method based on evidence theory and
local texture where the facial image is divided into regions
2017 IEEE/ACS 14th International Conference on Computer Systems and Applications
2161-5330/17 $31.00 ¬© 2017 IEEE
DOI 10.1109/AICCSA.2017.124745

with important recognition features, and the Local Binary
Patterns (LBP) textural features of the regions are extracted.Recent works showed how deep learning models could beapplied on facial expression recognition, in [13] the authorspresent a novel Boosted Deep Belief Network (BDBN) forperforming the three training stages iteratively in a uniÔ¨Åedloopy framework. Burkert et al. [14] proposed a CNN archi-tecture for facial expression recognition. Mollahosseini [15]also proposed a deep neural network architecture to addressthe facial expression recognition problem across multiple well-known standard face datasets. In [16] Zhang et al. proposed anovel deep neural network: DNN-driven facial feature learn-ing model which employs several layers to characterize thecorresponding relationship between the SIFT feature vectorsand their corresponding high-level semantic information. Theauthors in [17] extract from facial landmarks Ô¨Åxed number ofSIFT features used as an input matrix to CNN architecture.The matrix size is X√ó Y where X is the number of SIFT fea-
tures, and Y is the size of each feature. Moreover, in [18] Sunet al. proposed a mixture of SIFT and deep convolution, theauthors used Partial least squares regression and linear SVMto train these features. Then, the output from all classiÔ¨Åersare combined with fusion network. In [19] a deep learningtechnique is adopted. The idea is to combine two models,in the Ô¨Årst deep network temporal appearance features fromimage sequences are extracted. Meanwhile temporal geometryfeatures from temporal facial landmark points are extracted bythe second deep network. The authors in [20] proposed a noveltechnique of taking Inter Vector Angles (IV A) as geometricfeatures, which proved to be scale invariant and person inde-pendent. A feature redundancy-reduced convolutional neuralnetwork (FRR-CNN) is presented in [21].
III. P
ROPOSED APPROACH
Our purpose is to ameliorate the accuracy of facial expres-
sion classiÔ¨Åcation by using a new CNN architecture. As deepnetworks need a big database for the trainig, we combine manydatabases to get a Ô¨Ånal one.As a Ô¨Årst step, After preparing the database we Ô¨Åxed the batchsize input of CNN architecture to 165 √ó165 then we trained
the architecture with Ô¨Åne tuning by Visual Geometry Group(VGG) model to generate the Ô¨Årst model.In second step to improve the classiÔ¨Åcation we repeat thetraining of our CNN architecture but the Ô¨Åne tuning here isachieved with the obtained Ô¨Årst model, and Ô¨Ånally we get ourÔ¨Ånal model as shown in Figure 1.
A. Network architecture
Deep Neural Networks (DNN) are models inspired of the
human brain, and particularly its ability to extract structures
(patterns) from raw data. From raw data input, deep learningmodels operate a large number of successive transformationsto discover representations increasingly abstract of such data.The operated transformations are combinations of linear andnonlinear operations. These transformations are used to repre-sent the data at different levels abstraction. The most popular
Fig. 1. Proposed approach.
image processing structure of DNN is CNN [8] which isconstructed by three main processing layers: ConvolutionalLayer, Pooling Layer and Fully Connected Layer.The adopted architecture of the convolutional neural networkis given in Figure 2.The proposed deep ConvNet contain four convolutional layers(with three max-pooling layers) to extract features hierarchi-cally, followed by the fully-connected layer and the softmaxoutput layer indicating 6 expression classes. The input of thenetwork is 165√ó 165√ók for all patches, where k = 3 for color
patches and k = 1 for gray patches. The output is one of the6 expression classes.
Fig. 2. Architecture of the convolutional neural network.
CNN units are described below:Convolutional layer: A convolution layer C
i(i network
layer) is parameterized by its number N of convolution cardsM
i
j(j‚àà{1,...,N}),the size of the convolution kernels
Kx√óKy(often square), and the connection diagram in the
previous layer Li‚àí1. Each convolution card Mi
jis the result
of a convolution sum of cards previous layer Mi‚àí1
j by its
respective convolution kernel. In the case of a fully connected
746
TABLE I
NETWORK CONFIGURATION .
Layer type Size/Stride Output
Dropout Probability
Data 165√ó165 -
Convolution 1 4√ó4/1 20
Max Pooling 1 2√ó2/2 -
Convolution 2 3√ó3/2 40
Max Pooling 2 2√ó2/2 -
Convolution 3 3√ó3/2 60
Max Pooling 3 2√ó2/2 -
Convolution 4 2√ó2/2 80
Fully Connected - 160
Dropout=0.5
Fully Connected - 7
card to the cards of the previous layer, the result is calculated
by the equation 1.
N/summationdisplay
n=1Mi
j=œÜ/parenleftBigg
bi
j+N/summationdisplay
n=1Mi‚àí1
n‚àóki
n/parenrightBigg
(1)
where * is the convolution operator.
Pooling layer: In the classical architectures of convolutional
neural networks, convolution layers are followed by sub-
sampling layers. A sub-sampling layer reduces the size of
cards, and introduces invariance to (low) rotations and transla-
tions can appear as input. The output of max-pooling layer is
given by the maximum activation value, in the input layer for
different regions of size Kx√óKynon-overlapping. Similarly
to a convolution layer, a bias is added and the result is passed
to the transfer function œÜdeÔ¨Åned above.
Fully connected layer: After several max pooling and convo-
lutional layers, the high-level reasoning in the neural network
is done via fully connected layers. Neurons in a fully con-
nected layer have full connections to all activations in the
previous layer, as seen in regular Neural Networks. Their
activations can hence be computed with a matrix multipli-
cation followed by a bias offset. Table I present network
conÔ¨Åguration, the patch size od data input is 165 √ó165. Then,
the convolutional and Max Pooling layers are chosen with
different kernel sizes ( 4 √ó4, 3√ó3, 2√ó2) and different strides
(1, 2).
B. Fine tuning
In the step of Fine tuning, we chose to use VGG model[22].
For Ô¨Åne-tuning with VGG, we used Caffe [23] framework to
get the model weights. VGG model was trained on the large
CASIA WebFace dataset [24] and the Static Facial Expressions
in the Wild (SFEW) dataset, which is a smaller database of
labeled facial emotions and it has been developed by selecting
frames from Acted Facial Expressions In The Wild (AFEW: is
a dynamic temporal facial expressions data corpus consisting
of close to real world environment extracted from movies)[25].
IV . E XPERIMENTAL RESULTS
Several databases were used to train and evaluate the
proposed architecture.TABLE II
DATABASES DESCRIPTION .
Base Images Number Pose
Learning baseCk+ 8000 1
KDEF 4900 5
MUG 21000 1
RAFD 1400 1
Test baseCk+ 150 1
RaFD 120 1
MUG 3006 1
A. Databases
To train our CNN architecture, we used many databases
and we standardize the size of all images to 224 √ó242
pixels.
The CK+ database [26] The CK+ database presented by
327 expression sequences. From each image sequence, we
selected only the last four frames to save the most clear
expression. Meanwhile the Ô¨Årst frames from each sequence
are collected for neutral expression.
We use also KDEF database is a set of totally 4900 pictures of
human facial expressions of emotion. This database proposed
7 facial expressions in different positions. We choose to use
the images of this database in the training step but only with
3 positions.
The Radboud Faces Database (RaFD) [27] is a set of
pictures of 67 models (including Caucasian males and
females, Caucasian children, both boys and girls, and
Moroccan Dutch males) displaying 8 emotional expressions.
MUG database [28] consists of image sequences of 86
subjects performing facial expressions. In the database
participated 35 women and 51 men all of Caucasian origin
between 20 and 35 years of age. Each image was saved with
a jpg format, 896896 pixels.
By this way, we built an experimental dataset with a total of
37000 images. 330000 images are used in training step, and
the rest of images are used as test images. As shown in Table
II, for learnig base we uses Ck+, KDEF, RaFD and MUG
datasets while for test base we tested only with Ck+, RaFD
and MUG datasets.
1) Perfermances of the proposed method: To verify the ef-
fectiveness of our proposed approach, we opted for a validation
on standard databases MUG, Rafd and CK+. Figure 3 illustrate
the confusion matrix on CK+ database. We can observe that
our proposed method achieves the best perfermance on only 5
database classes (Disgust, Happy,Neutral, Sad and Surprise)
with recognition rate 100 %. However recognition rate of
Angry emotion is still difÔ¨Åcult (96 %) because of the database
characteristics. Our model excelled with classifying the CK+
dataset with recognition rate of 99.33 %. Figure 4 shows the
confusion matrix on MUG database we achieved only 87.65 %
as recognition rate, the differences between different emotions
among the subjects is very subtle. Also, MUG images are in
747
Fig. 3. Confusion matrix of proposed method on CK+ database.
grey-scale format, whereas our model work is optimized for
RGB images. The lack of RGB format can exacerbate the
ability of the network to make distinction between important
features and background elements. Our model is also evalu-
Fig. 4. Confusion matrix of proposed method on MUG database.
ated with RaFD and we achieved a accuracy rate of 93.33 %as
shown in Figure5. In this database we observe some incorrect
classiÔ¨Åcation for Sad and Neutral emotions and specially the
Sad expression due to the unclarity features in this class.
B. Method comparison
To get accuracy close to 99 %in training step, it took about
4 days to Ô¨Ånish the training (300000 iterations). To accelerate
the learning process with parallelization, we used CAFFE on
ubuntu 14.04 LTS and a GeForce GT 525M GPU, which
has 2GB of memory. we tried an architecture with 3, 5 and
6 hidden layers but our chosen architecture present the best
Fig. 5. Confusion matrix of proposed method on RaFD database.
accuracy of features extraction and emotions classiÔ¨Åcation.
To demonstrate the perfermance of our method we compared
it with other methods tested on Ck+ database. As shown in
Fig. 6. Performance comparison on the CK+ database, A1[20] , A2 [19], A3
[21].
Figure 6, our method outperformed all the methods in compar-
ison. This demonstrated that the features learned and selected
through our method contain more discriminative information
for facial expression recognition.
Our Deep learning method achieves the best performance on
CK+ database. The deep learning work developed by [19]
achieved the second best performance of 96.93 %accuracy
on Ck+. The third best performance of 95.34 %accuracy is
presented by [20]. The work proposed at the same time of this
paper achieved the last best performance of 71.04 %accuracy
on Ck+.
V. C ONCLUSION AND DISCUSSION
A part of automatic analysis of facial expression Ô¨Åeld is
presented in this paper. We proposed a new deep neural
network architecture for facial expression recognition. The
proposed network consists of four convolutional layers, th Ô¨Årst
three layers are followed by max pooling and the last one
748
is followed by fully connected layer. It takes facial images
as the input and classiÔ¨Åes them into either of the six facial
expressions: angry, disgust, happy, neutral, sad and surprise.
The proposed architecture obtained is evaluated with MUG,
RAFD and Ck+ databases. Results and recognition rates prove
that our method outperforms the state-of-the-art methods.
For this project, we trained the model with images which the
face was in one position. In the futurework, we would like to
extend our model to different face positions. This will allow
us to investigate the efÔ¨Åcacy of pre-trained models such as
VGGNet for facial emotion recognition.
REFERENCES
[1] R. G. Harper, A. N. Wiens, and J. D. Matarazzo, Non-
verbal communication: The state of the art. John Wiley
& Sons, 1978.
[2] P. Viola and M. J. Jones, ‚ÄúRobust real-time face detec-
tion,‚Äù International journal of computer vision , vol. 57,
no. 2, pp. 137‚Äì154, 2004.
[3] S. M. Lajevardi and M. Lech, ‚ÄúFacial expression recog-
nition from image sequences using optimized feature
selection,‚Äù in Image and Vision Computing New Zealand,
2008. IVCNZ 2008. 23rd International Conference .
IEEE, 2008, pp. 1‚Äì6.
[4] A. Ben-Hur and J. Weston, ‚ÄúA users guide to support
vector machines,‚Äù Data mining techniques for the life
sciences , pp. 223‚Äì239, 2010.
[5] R. Samad and H. Sawada, ‚ÄúExtraction of the minimum
number of gabor wavelet parameters for the recogni-
tion of natural facial expressions,‚Äù ArtiÔ¨Åcial Life and
Robotics , vol. 16, no. 1, pp. 21‚Äì31, 2011.
[6] J. Susskind, V . Mnih, G. Hinton et al. , ‚ÄúOn deep
generative models with applications to recognition,‚Äù in
Computer Vision and Pattern Recognition (CVPR), 2011
IEEE Conference on . IEEE, 2011, pp. 2857‚Äì2864.
[7] S. Rifai, Y . Bengio, A. Courville, P. Vincent, and
M. Mirza, ‚ÄúDisentangling factors of variation for facial
expression recognition,‚Äù Computer Vision‚ÄìECCV 2012 ,
pp. 808‚Äì822, 2012.
[8] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner,
‚ÄúGradient-based learning applied to document recogni-
tion,‚Äù Proceedings of the IEEE , vol. 86, no. 11, pp. 2278‚Äì
2324, 1998.
[9] P. Ekman and D. Keltner, ‚ÄúUniversal facial expressions
of emotion,‚Äù California Mental Health Research Digest ,
vol. 8, no. 4, pp. 151‚Äì158, 1970.
[10] H. Mliki, N. Fourati, M. Hammami, and H. Ben-
Abdallah, ‚ÄúData mining-based facial expressions recog-
nition system.‚Äù in SCAI , 2013, pp. 185‚Äì194.
[11] P. Cotret, S. Chevobbe, and M. Darouich, ‚ÄúEmbedded
wavelet-based face recognition under variable position,‚Äù
inSPIE/IS&T Electronic Imaging . International Society
for Optics and Photonics, 2015, pp. 94 000A‚Äì94 000A.
[12] W. Wang, F. Chang, Y . Liu, and X. Wu, ‚ÄúExpression
recognition method based on evidence theory and localtexture,‚Äù Multimedia Tools and Applications , pp. 1‚Äì15,
2016.
[13] P. Liu, S. Han, Z. Meng, and Y . Tong, ‚ÄúFacial expres-
sion recognition via a boosted deep belief network,‚Äù in
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , 2014, pp. 1805‚Äì1812.
[14] P. Burkert, F. Trier, M. Z. Afzal, A. Dengel, and
M. Liwicki, ‚ÄúDexpression: Deep convolutional neu-
ral network for expression recognition,‚Äù arXiv preprint
arXiv:1509.05371 , 2015.
[15] A. Mollahosseini, D. Chan, and M. H. Mahoor, ‚ÄúGoing
deeper in facial expression recognition using deep neural
networks,‚Äù in Applications of Computer Vision (WACV),
2016 IEEE Winter Conference on . IEEE, 2016, pp. 1‚Äì
10.
[16] T. Zhang, W. Zheng, Z. Cui, Y . Zong, J. Yan, and
K. Yan, ‚ÄúA deep neural network-driven feature learning
method for multi-view facial expression recognition,‚Äù
IEEE Transactions on Multimedia , vol. 18, no. 12, pp.
2528‚Äì2536, 2016.
[17] ‚Äî‚Äî, ‚ÄúA deep neural network-driven feature learning
method for multi-view facial expression recognition,‚Äù
IEEE Transactions on Multimedia , vol. 18, no. 12, pp.
2528‚Äì2536, 2016.
[18] B. Sun, L. Li, G. Zhou, and J. He, ‚ÄúFacial expression
recognition in the wild based on multimodal texture
features,‚Äù Journal of Electronic Imaging , vol. 25, no. 6,
pp. 061 407‚Äì061 407, 2016.
[19] H. Jung, S. Lee, J. Yim, S. Park, and J. Kim, ‚ÄúJoint
Ô¨Åne-tuning in deep neural networks for facial expression
recognition,‚Äù in Proceedings of the IEEE International
Conference on Computer Vision , 2015, pp. 2983‚Äì2991.
[20] R. Islam, K. Ahuja, S. Karmakar, and F. Barbhuiya,
‚ÄúSention: A framework for sensing facial expressions,‚Äù
arXiv preprint arXiv:1608.04489 , 2016.
[21] S. Xie and H. Hu, ‚ÄúFacial expression recognition with
frr-cnn,‚Äù Electr onics Letters , vol. 53, no. 4, pp. 235‚Äì237,
2017.
[22] K. ChatÔ¨Åeld, K. Simonyan, A. Vedaldi, and A. Zisser-
man, ‚ÄúReturn of the devil in the details: Delving deep
into convolutional nets,‚Äù arXiv preprint arXiv:1405.3531 ,
2014.
[23] (2016, september) Caffe: Deep learning framework by
the bvlc. http://caffe.berkeleyvision.org/.
[24] D. Yi, Z. Lei, S. Liao, and S. Z. Li, ‚ÄúLearning face repre-
sentation from scratch,‚Äù arXiv preprint arXiv:1411.7923 ,
2014.
[25] A. Dhall et al. , ‚ÄúCollecting large, richly annotated facial-
expression databases from movies,‚Äù 2012.
[26] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar,
and I. Matthews, ‚ÄúThe extended cohn-kanade dataset
(ck+): A complete dataset for action unit and emotion-
speciÔ¨Åed expression,‚Äù in 2010 IEEE Computer Society
Conference on Computer Vision and Pattern Recognition-
Workshops . IEEE, 2010, pp. 94‚Äì101.
[27] O. Langner, R. Dotsch, G. Bijlstra, D. H. Wigboldus,
749
S. T. Hawk, and A. van Knippenberg, ‚ÄúPresentation and
validation of the radboud faces database,‚Äù Cognition and
emotion , vol. 24, no. 8, pp. 1377‚Äì1388, 2010.
[28] N. Aifanti, C. Papachristou, and A. Delopoulos, ‚ÄúThe
mug facial expression database,‚Äù in Image Analysis for
Multimedia Interactive Services (WIAMIS), 2010 11th
International Workshop on . IEEE, 2010, pp. 1‚Äì4.
750
"
https://ieeexplore.ieee.org/document/8528894,"1949-3045 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2018.2880201, IEEE
Transactions on Affective Computing
IEEE TRANSACTION ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, AUGUST 2015 1
Facial Expression Recognition with Identity and
Emotion Joint Learning
Ming Li,Member,IEEE , Hao Xu, Xingchang Huang, Zhanmei Song, Xiaolin Liu, and Xin Li Fellow, IEEE
Abstract‚ÄîDifferent subjects may express a speciÔ¨Åc expression in different ways due to inter-subject variabilities. In this work, besides
training deep-learned facial expression feature (emotional feature), we also consider the inÔ¨Çuence of latent face identity feature such as
the shape or appearance of face. We propose an identity and emotion joint learning approach with deep convolutional neural networks
(CNNs) to enhance the performance of facial expression recognition (FER) tasks. First, we learn the emotion and identity features
separately using two different CNNs with their corresponding training data. Second, we concatenate these two features together as a
deep-learned Tandem Facial Expression (TFE) Feature and feed it to the subsequent fully connected layers to form a new model.
Finally, we perform joint learning on the newly merged network using only the facial expression training data. Experimental results
show that our proposed approach achieves 99.31% and 84.29% accuracy on the CK+ and the FER+ database, respectively, which
outperforms the residual network baseline as well as many other state-of-the-art methods.
Index Terms‚ÄîFacial expression recognition, Emotion recognition, Face recognition, Joint learning, Transfer learning
F
1 I NTRODUCTION
Facial Expression Recognition (FER) is a well deÔ¨Åned
task, aiming to recognize facial expressions with discrete
categories (e.g., neutral, sad, contempt, happy, surprise,
angry, fear, disgust, etc.) or continuous levels (e.g., valance,
arousal) from still images or videos. Although many recent
works focus on video or image sequence based FER tasks
[1], [2], still image based FER still remains as a challenging
problem. First, the differences between some facial expres-
sions might be subtle and thus difÔ¨Åcult to classify them ac-
curately in some cases [3]. Second, different subjects express
the same speciÔ¨Åc facial expression in different ways due to
the inter-subject variability and their facial biometric shapes
[4], [5], [6], etc.
These two challenging problems can be visualized in the
following examples in Fig. 1. The left part of Fig. 1 contains
two representative faces and both of them are labeled with
the ‚Äùsad‚Äù facial expression. However, their eight-category
classiÔ¨Åcation scores have great differences in our initial
experiment, which is shown in Table 1. The prediction for
the left subject is reasonable as the ‚Äùsad‚Äù emotion ranks
the top. However, for the person on the right, the score of
‚Äùangry‚Äù is a little higher than ‚Äùsad‚Äù, which did not correctly
predict her emotion.
Generally, the inter-speaker variability of the faces could
potentially lead to errors in emotion classiÔ¨Åcation because
the neutral face of one subject could already be very similar
to the typical faces of other emotion categories (e.g. the right
subject in Fig. 1 and the ‚Äùangry‚Äù emotion). Furthermore,
neural science studies also show that the facial expression
and identity representations in human cortex are closely
Ming Li is with the Data Science Research Center at Duke Kunshan
University and the ECE department of Duke University. Hao Xu and
Xingchang Huang are with the School of Data and Computer Science,
Sun Yat-sen University. Zhanmei Song, Xiaolin Liu, and Xin Li are with
the School of Preschool Education, Shandong Yingcai University.
Corresponding author: Zhanmei Song, E-mail: songzhanmei@126.com
Manuscript received; revised.connected to each other [7]. Therefore, we believe that if we
add a compact description of the subject‚Äôs facial identity or
biometric information as an auxiliary input to our model,
the FER system can become more subject adapted and
robust against the inter-speaker variability just as the role
of speaker adaptation technique in speech recognition tasks
[8], [9].
Previous works show that deep neural network based
methods have achieved excellent performance in face relat-
ed recognition tasks [1] [10] [11] [12] [13] [14] [15]. In face
recognition, Deep Convolutional Neural Networks (CNNs)
outperform traditional methods with hand-crafted features
[16] [17] [18] [19], and even perform better than human
beings [12] [10] [11] [20]. However, in FER tasks, the system
performance still needs to be further enhanced. Lack of
large scale labeled training data, inconsistent and unreliable
emotion labels and inter-subject variabilities all limit the
performance of CNN on the FER task. Therefore, in this
work, we aim to utilize additional face recognition training
data to perform identity and emotion joint learning for FER.
Related to our work, Xu et al. [21] proposed a transfer
learning method from face recognition to FER using CNN
directly. Also, Jung et al. [1] proposed a joint Ô¨Åne-tuning
method that jointly learns the parameters from image se-
quences. However, unlike these two methods, we do not
transfer the network structures and parameters from face
recognition to FER directly. Instead, we extract the high-
level identity feature from the face recognition network and
consider it as an auxiliary input feature for our FER model.
As shown in Fig. 2, we concatenate both the high-level
emotion and identity features as Tandem Facial Expression
(TFE) features and feed it to the subsequent fully connected
layers to form a new network.
In this paper, we adopt the CNN architecture to discover
latent identity and emotion features. First, we pre-train
latent emotion and identity features separatively using two
different CNNs (ResNet [22] for emotion and DeepID [10]
1949-3045 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2018.2880201, IEEE
Transactions on Affective Computing
IEEE TRANSACTION ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, AUGUST 2015 2
Fig. 1. Two example face images with ‚Äùsad‚Äù facial expressions. The left part is the original faces of these two subjects and the right part is the faces
with detected landmarks.
TABLE 1
The predicted scores on 8 facial expression categories for the two example images in Fig. 1.
Figure Neutral Angry Contempt Disgust Fear Happy Sad Surprise
Left 0.2536 0.0042 0.0038 0.0004 0.0025 0.0003 0.7332 0.0020
Right 0.0002 0.5110 0.0 0.0009 0.0001 0.0001 0.4876 0.0001
Fig. 2. Our model consists of two convolutional neural networks. The left one represents the DeepID network learning the identity features. The
right deep residual network is trained with facial expression databases. After training separatively, the identity feature and the deep-learned emotion
feature are concatenated as the TFE features and feed to the subsequent fully connected layers. Finally, we perform joint learning on the new
merged network using only the facial expression database.
for identity) with their own training data. Furthermore, we
merge these two networks together by concatenating the
deep-learned features and feed to a new fully connected
layer. Finally, we use FER training data to jointly learn the
parameters of the merged new network. To the best of our
knowledge, there is no previous work using auxiliary deep
identity feature with deep emotion feature together for joint
facial expression learning.
2 R ELATED WORK
In this section, we will introduce two main types of features
used in the FER task, namely hand-crafted features and
deep-learned features.2.1 Hand-Crafted Feature Based Method
Before deep learning based approaches dominate face recog-
nition and FER tasks, many works have been conducted
based on the hand-crafted features [3]. These approaches
usually perform frontend feature extraction and backend
classiÔ¨Åcation separately [15]. During the stage of feature
extraction, traditional features, such as Local Binary Pat-
terns (LBP) [16] [18], Gabor wavelet coefÔ¨Åcients [17], Scale-
Invariant Feature Transform (SIFT) [19], and Gaussian Face
[20] are designed with prior domain knowledge. Moreover,
supervised classiÔ¨Åers, such as SVM [23], feedforward Neu-
ral Network (NN) [24] and Extreme Learning Machine [25],
[26] are adopted for the subsequent modeling.
1949-3045 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2018.2880201, IEEE
Transactions on Affective Computing
IEEE TRANSACTION ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, AUGUST 2015 3
2.2 Deep-Learned Feature Based Method
Generally, deep learning based methods outperform hand-
crafted feature based approaches and achieve state-of-the-
art performance on both face recognition [12] [10] [11] and
FER [27] [1] [15] [2] tasks. For example, Sun et al. [12] pro-
posed CNN model and DeepID features for face recognition.
In order to boost the performance, Sun et al. [10] proposed
a Siamese network to train in face pairs. Furthermore, CNN
models have been widely used in the FER task. Tang et al.
[13] replace the softmax layer with SVM in the CNN frame-
work and achieved the best accuracy on FER2013 dataset
[28] in the ICML 2013 Representation Learning Challenge.
Emad et al. [29] then proposed a FER+ dataset with more
accurate labels and produced a benchmark on this dataset
with VGG13 network. Jung et al. [1] and Zhao et al. [2] both
consider temporal structures on top of CNNs to model the
image sequences.
Recent high performance models normally accompany
with deep architectures and a large number of convolutional
kernels. Alex et al. [30] has proposed AlexNet for the Ima-
geNet challenge and achieve superior performance at that
time. Subsequently, Karen et al. [31] presented their deeper
networks with 16 and 19 layers respectively and found
that deeper structures can achieve better performance. Fur-
thermore, He et al. [22] proposed deep residual network
(ResNet) and trained a CNN with 152 layers. ResNet can
converge faster and perform more accurately due to its
residual learning mechanism, shortcut connection [22] and
batch normalization [32]. Also, Christian et al. [33] proposed
GoogleNet and its inception v4 architecture. They further
combined the residual network architecture with Inception-
v4 as Inception-ResNet [34] and achieve better performance
on the ImageNet dataset [35]. In this work, our approach
adopts deep ResNet and CNN for their high performance
and less chance of overÔ¨Åtting on image related tasks.
2.3 Transfer Learning with CNN
A recent work proposed by Xu et al. [21] used transfer
learning with CNN for FER. The difference between our
work and their work is that we learn both the emotion and i-
dentity features using two separate deep convolutional neu-
ral networks and construct a deep-learned Tandem Facial
Expression (TFE) feature in the merged model instead of just
transferring the weights of the pre-trained face recognition
networks and Ô¨Åne-tuning. Compared with the work by Jung
et al. [1], we use feature-level concatenation of deep-learned
identity and emotion features to form a new network with
joint learning rather than Ô¨Åne-tuning two softmax layers
with the landmark-based features.
3 O URAPPROACH
In this section, we will introduce our proposed method in
details.
3.1 Overview of our Network Architecture
As shown in Fig. 2, our model consists of two CNNs. The left
one is the DeepID network proposed in [10], [12], containing
four convolutional layers. Actually, the architecture we use
is the same as the ConvNet structure proposed in [10], which
Fig. 3. Our ResNet12 and ResNet18 architectures. ‚Äùproject‚Äù, ‚Äùidentity‚Äù
and ‚Äùgap‚Äù are projection block, identity block and global average pooling,
respectively.
learns fully-connected layer (DeepID2 feature) from both the
third and fourth convolutional layers, generating a compact
feature representation with 160 dimensions. The right net-
work is constructed according to the deep ResNet [22] and
we choose the ResNet18 structure and also build a shallower
network called ResNet12 for different tasks based on the
size of the input images and the size of datasets. In ResNet,
we use shortcut connection for deep residual learning and
batch normalization layers [32] for faster convergence and
better accuracy. We do not use fully-connected layer (FC)
to Ô¨Çatten the feature maps. Instead, we use global average
pooling (Gap) to generate a compact representation with
reduced number of parameters [22]. But for convenience,
we still use fully-connected layer to model the concatenated
TFE features.
Actually, the left and the right networks are considered
as a merged joint network in Fig. 2. During training, the
features of DeepID network and ResNet are concatenated
as the input for fully-connected layers and jointly learn the
entire network for FER tasks. In order to guide this merged
network to extract identity and emotion features, we do not
train from scratch but pre-train the weights of both two sub-
networks separately with the corresponding datasets except
the fully connected layer.
Deep residual network (ResNet) has achieved great suc-
cess in multiple challenges [22]. ResNet is based on the deep
residual learning framework and it is easier to optimize
the residual rather than the original mapping. Therefore,
in our work we adopt ResNet for training emotion features.
SpeciÔ¨Åcally, the architecture of residual networks is made
up of the following two blocks shown in Fig. 4. The left one,
called ‚Äùidentity block‚Äù, contains a shortcut link connecting
the input x to the convolution output. The right one, called
‚Äùprojection block‚Äù, contains a convolution operation in the
shortcut connection, aiming to ensure the same output size
of feature maps using a 11convolution with a stride of 2
1949-3045 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2018.2880201, IEEE
Transactions on Affective Computing
IEEE TRANSACTION ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, AUGUST 2015 4
Fig. 4. The building blocks of our residual network. ndenotes the
number of Ô¨Ålters in convolution.
while doing the element-wise sum at the output.
In our work, the right block is used to increase the
number of convolutional kernels and decrease the output
size of feature maps by half. The stride for each convolution
operation is 2 and there is no max-pooling layer. After the
last convolution operation, we use Global Average Pooling
(Gap) layer to generate a 64-dimensional emotion feature as
shown in Fig. 2. Considering the size of datasets, we use two
ResNet structures. The Ô¨Årst one is ResNet18 structure for
processing the FER+ dataset [29] with over 35k 4848im-
ages. For the smaller dataset, CK+, we reduce the number of
parameters by removing four identity building blocks when
n= 16; 16;32;64and adding one project building block
when n= 16 . We call this shallower network as ResNet12.
We adopt batch normalization after each convolution and
before the rectiÔ¨Åed linear unit (ReLU) activations [36] [22].
The architectures of these two ResNets are shown in Fig. 3.
3.2 Identity and Emotion Feature Concatenation
Suppose that the identity and emotion features of an arbi-
trary input image are represented as ZiandZe, respectively.
Then, we can reconstruct the new TFE representation Ztfe
by concatenating ZiandZetogether.
However, sometimes the deep-learned ZiandZefea-
tures are not in the same scale because both network struc-
ture and training data are different. In our work, we Ô¨Årstly
normalize ZiandZewith batch normalization [32]. Then
we concatenate these two features together to form the TFE
feature Ztfe.
4 E XPERIMENTAL RESULTS
4.1 Datasets
In this work, we evaluate the proposed method on t-
wo popular FER datasets, namely Extended Cohn-Kanade
(CK+) database [37] and FER+ database [29]. These two
FER datasets are used for deep-learned emotion feature
extraction and joint learning, while the identity features are
learned from the CASIA-WebFace database [38].
CASIA-WebFace [38]: In this dataset, the face images
are collected from the Internet, containing 10,575
subjects and 494,414 images. It is usually considered
as a standard large scale training dataset for face
recognition challenges [39].LFW: LFW (Labeled Faces in the Wild) dataset con-
tains 13,233 face images from 5,749 identities collect-
ed on the Internet. As a benchmark for comparison,
LFW suggests reporting performance with 10-fold
cross validation using splits they have randomly
generated (6,000 pairs) [39]. In this study, the LFW
database is adopted purely as a stand alone testing
data to evaluate the quality of our identity features
trained by the CASIA-WebFace dataset.
CK+: The CK+ database includes 327 image se-
quences with labeled facial expressions. For each
image sequence, only the last frame is provided with
an expression label. In order to collect more images
for training, we usually selected the last three frames
of each sequence for training or validation purpose.
Additionally, the Ô¨Årst frame from each of the 327
labeled sequences would be chosen as the ‚Äùneutral‚Äù
expression. As a result, this dataset can provide total-
ly 1308 images with 8 labeled facial expressions. For
testing, we follow the 10-fold cross validation testing
protocol on the CK+ database.
FER+: This dataset comes from the face expression
recognition challenge [28] in the ICML 2013 Rep-
resentation Learning Workshop. It consists 28,709
4848face images for training. The test set has
3,589 images and there are totally 7 discrete facial
expressions (anger, disgust, fear, happiness, sadness,
and surprise) for classiÔ¨Åcation. However, due to it-
s noisy labels, this dataset is labeled again using
crowd-sourced services [29]. In this study, we simply
use majority voting to derive the new set of labels for
our experiments.
4.2 Parameter Settings
For the DeepID network, we follow the same parameter
setting in [12], with a 160-dimensional representation in the
fully-connected layers. A dropout layer is used after the
DeepID layer, with a probability of 0.4 to reduce over-Ô¨Åtting
[40].
For the deep ResNet, we have two different settings for
the number of layers. We use the ResNet18 architecture for
the FER+ dataset but use a shallower network ResNet12 for
the CK+ dataset, which has much less data for training and
testing.
As for Stochastic Gradient Descent (SGD) method dur-
ing back propagation [41], we apply different parameters
in CK+ dataset and FER+ dataset. For CK+, we initialize
the learning rate as 0.16 and 0.01 respectively for ResNet
and DeepID network with a mini-batch size of 128 and
a momentum of 0.9 [42]. The networks in our model are
trained for up to 200 epochs and Ô¨Åne-tuned for up to 100
epochs as well. For FER+, the learning rate is initialized as
0.1 for ResNet training and 0.001 for Ô¨Ånal joint learning.
4.3 Pre-Processing
For CASIA-WebFace dataset, we need to use pre-processing
pipeline, including face detection, face landmarks detection
face alignments and face cropping. We use the tools from
mmlab, CUHK [43] to detect face and landmarks. After
these processing steps, those missed faces are removed
1949-3045 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2018.2880201, IEEE
Transactions on Affective Computing
IEEE TRANSACTION ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, AUGUST 2015 5
TABLE 2
Average accuracy on CK+ of our models using 10-fold cross validation
Our Methods Average Accuracy on CK+
ResNet12 97.56%
TFE-JL 99.31%
and there are totally 435,863 faces remaining. Then we use
a template (the Ô¨Årst image) in the LFW [39] dataset to
align the faces in the CASIA-WebFace. Finally, images from
the datasets we use (CASIA-WebFace, CK+, LFW) will be
cropped in the same way retaining the eye brow and jaw. As
LFW provides a deep-funneled version and CK+ is collected
containing the frontal whole faces, there is no need to do
face alignment. FER+ has been pre-processed and cropped
as well.
During training the DeepID network using CASIA-
WebFace, we randomly select one image from each person
for validation and therefore generate a validation set of
10,575 images, while the remaining images are used for
training. It is worth noting that the training set of CASIA-
WebFace is augmented with horizontal Ô¨Çipping while the
training set in FER+ is augmented with horizontal Ô¨Çipping,
shifting and rotation. In addition, all the images are pre-
processed with per-pixel mean subtraction and standard
deviation normalization [13].
4.4 Evaluation of identity features on the LFW database
After the pre-processing step, we train our DeepID net-
work for extracting the auxiliary identity feature on CASIA-
WebFace. After training 200 epochs for DeepID network, the
accuracy on LFW for face veriÔ¨Åcation can achieve 91% using
cosine similarity with a 0.15 threshold.
Since our goal is to extract a reasonable good quality
identity feature, we may not need to fully optimize the face
veriÔ¨Åcation performance on LFW. We use a single DeepID
network and single patch for each face image without any
ensemble method.
4.5 Evaluation of FER on the CK+ database
During the 10-fold cross validation testing, we train our
ResNet from scratch for each rotation with 200 epochs. After
the Ô¨Årst step training, we combine these two networks,
which extract identity features and emotion features respec-
tively, to form a 224-dimensional TFE representation. Final-
ly, we jointly learn the parameters of the merged network
with the CK+ training data.
One example of the training and joint learning process
on the CK+ database is provided in Fig. 5. The training accu-
racy (blue) increase gradually while the validation accuracy
(orange) increase with Ô¨Çuctuation but Ô¨Ånally converge to
97.56%. In the joint learning stage using the TFE feature, the
accuracy on the validation set converges faster and better
than the Ô¨Årst training stage and can achieve up to 99.24%. As
shown in Table 2, the performance of our proposed method
outperforms the ResNet baseline by 1.68% absolutely.
Besides the comparisons with our own implemented
baselines, we also compare our method with other state-
of-the-art approaches. Our method can achieve around 2%
Fig. 5. Training and validation performance on the CK+ database during
the training and the joint learning stage.
absolute improvement compared with PPDN model [2] as
shown in Table 3. Actually, the sequence-based PPDN also
can achieve 99.3% accuracy on the test set but it was pre-
trained on CASIA-WebFace and used the whole sequence of
images in CK+ for training, which does not match with our
experimental setting (only the Ô¨Årst one and the last three
frames of each CK+ sequence are used).
TABLE 3
Comparion with state-of-the-art methods
Methods Average Accuracy on CK+
DTAGN(Weighted Sum) [1] 96.9
DTAGN(Joint) [1] 97.3
IntraFace [44] 96.4
BDBN [15] 96.7
CNN [45] 95.8
PPDN [2] 97.3%
TFE-JL 99.3%
4.6 Evaluation FER on the FER+ database
Similarly, we show the performance of our proposed meth-
ods on the FER+ dataset in Table 4. We train these two
models on the training set and evaluate them on the private
test set. We mainly focus on the private test set for direct
comparison with VGG13(MV) proposed by [29]. SpeciÔ¨Åcally,
we train our ResNet18 on the training set with 200 epochs
1949-3045 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2018.2880201, IEEE
Transactions on Affective Computing
IEEE TRANSACTION ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, AUGUST 2015 6
Fig.
6. Training and validation performance on the FER+ dataset during
the training and the joint learning stage.
TABLE 4
Accuracy on FER+ of our proposed methods
Methods Accuracy
on FER+
ResNet18 83.1%
ResNet18
+ FC 83.4%
TFE-joint
learning 84.3%
and
the pre-trained accuracy is 83.1% on the private test set.
Then we jointly learn the network using TFE features gener-
ated from DeepID and ResNet, getting an improvement up
to 1.2% from 83.1% to 84.3%.
In Table 5, we compare our methods with the state-
of-the-art approaches on FER2013 and FER+. The work
DLSVM-L2 has been presented in [13] and ranks the top
in the ICML2013 Representation Learning Challenge on
the FER2013 dataset. The VGG13(MV) system outperforms
the DLSVM-L2 baseline as it used new labels on FER+.
Therefore, we just compare our proposed TFE-JL method
with VGG13(MV). As shown in Table 5, the proposed TFE-JL
method also achieves 0.5% accuracy gain compared with the
average performance of VGG13(MV) model, which again
shows the advantage and effectiveness of the proposed
identity and emotion joint learning framework.
We also implement the single network transfer learning
method in [21] by directly using the deep-id identity feature
learned from CASIA-WebFace as inputs for the subsequent
SVM modeling on FER+ database. Results in Table 5 showTABLE 5
Accuracy on FER2013 with old and new labels compared with the
state-of-the-art methods
Labels Methods Accuracy
on FER2013
Old DLSVM-L2
[13] 71.2%
FER2013 Zhou
et al. [28] 69.3%
Maxim
Milakov [28] 68.8%
Radu+Marius+Cristi
[28] 67.5%
Our
implementation of [21] 71.1%
New VGG13(MV)
[29] 83.8%
FER+ TFE-JL 84.3%
that
our proposed joint learning method also outperforms
the single network transfer learning approach.
Furthermore, considering the problem that the face im-
ages in FER2013 or FER+ dataset are not aligned as well as in
CASIA-WebFace used for pre-training the DeepID network,
the gain of our joint learning approach on FER+ may not be
as large as in the CK+ database.
In order to demonstrate how our proposed identity and
emotion joint learning method improves the FER perfor-
mance, we list the output scores of 8 facial expression
categories on ten representative face images from the test
set in Table 6. These face images are misclassiÔ¨Åed using our
baseline (ResNet) model but are corrected by the proposed
identity and emotion joint learning method. As shown in
Fig. 7, these images were Ô¨Årstly misclassiÔ¨Åed, the reason
might be their appearances. For example, image (5) looks
‚Äùsad‚Äù and image (7-8) look ‚Äùangry‚Äù. By adding their identity
information as an auxiliary input to our FER model, the
TFE joint learning approach could reduce the inter-subject
variability.
5 C ONCLUSION AND FUTURE WORK
In this work, we learn both the emotion and identity features
using two separate deep convolutional neural networks
and construct a deep-learned Tandem Facial Expression
(TFE) feature by feature level concatenation. We perform
Ô¨Åne-tuning on the newly merged model instead of just
transferring the weights of the pre-trained face recognition
networks. Experimental results show that the proposed
approach outperforms the residual network baseline as well
as many other state-of-the-art methods on two popular FER
databases, namely CK+ and FER+. Future works include
investigating the effect of face image format differences or
alignment mismatch between face recognition data and FER
data as well as exploring other transfer learning and multi-
task learning methods for the FER task.
ACKNOWLEDGMENTS
This research was funded in part by the National Nat-
ural Science Foundation of China (61773413), Natural
Science Foundation of Guangzhou City (201707010363),
National Key Research and Development Program
(2016YFC0103905).
REFERENCES
[1] H. Jung, S. Lee, J. Yim, and S. Park, ‚ÄúJoint Ô¨Åne-tuning in deep
neural networks for facial expression recognition,‚Äù in IEEE Inter-
national Conference on Computer Vision, 2015, pp. 2983‚Äì2991.
1949-3045 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2018.2880201, IEEE
Transactions on Affective Computing
IEEE TRANSACTION ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, AUGUST 2015 7
Fig. 7. Ten representative face images whose prediction is corrected by our joint learning method. These face images are indexed with number 1 to
10 from left to right, top to bottom.
TABLE 6
The predicted scores of those representative images in Fig. 7 using ResNet and our TFE joint learning method. For each face image, the Ô¨Årst line
is the output score using the ResNet baseline and the second line is the output score of our TFE joint learning method.
Image Neutral Happy Surprise Sad Angry Disgust Fear Contempt
1 0.0 0.0093 0.9905 0.0767 0.0 0.0001 0.0001 0.0
0.0 0.6180 0.3726 0.0 0.0005 0.0012 0.00072 0.0004
2 0.0023 0.3341 0.6633 0.0 0.0001 0.0 0.0 0.0001
0.0005 0.6191 0.3493 0.0 0.0200 0.0076 0.0032 0.0002
3 0.4771 0.0 0.0 0.5229 0.0 0.0 0.0 0.0
0.7262 0.0 0.0 0.2735 0.0 0.0002 0.0 0.0
4 0.3601 0.6396 0.0 0.0 0.0 0.0 0.0 0.0003
0.6632 0.3219 0.0002 0.0001 0.0058 0.0005 0.0003 0.0081
5 0.4883 0.0 0.0 0.5097 0.0020 0.0001 0.0 0.0
0.7153 0.0 0.0 0.2837 0.0003 0.0005 0.0001 0.0002
6 0.4172 0.0 0.0 0.5798 0.0002 0.0006 0.0 0.0022
0.7978 0.0 0.0 0.1997 0.0001 0.0013 0.0001 0.0009
7 0.4940 0.0 0.0 0.0019 0.5037 0.0004 0.0 0.0
0.5742 0.0 0.0006 0.0047 0.4027 0.0097 0.0017 0.0063
8 0.0059 0.0 0.0 0.1972 0.7967 0.0 0.0 0.0001
0.0095 0.0 0.0 0.7818 0.2000 0.0017 0.0049 0.0020
9 0.3260 0.0007 0.2076 0.0275 0.0016 0.0020 0.4337 0.0010
0.7291 0.0081 0.0612 0.0347 0.0397 0.0683 0.0420 0.0168
10 0.0983 0.0 0.3654 0.0006 0.0096 0.0 0.5260 0.0002
0.0560 0.0 0.5034 0.0046 0.0176 0.0106 0.4046 0.0031
[2] X. Zhao, X. Liang, L. Liu, T. Li, Y. Han, N. Vasconcelos, and S. Yan,
‚ÄúPeak-piloted deep network for facial expression recognition,‚Äù in
European Conference on Computer Vision. Springer, 2016, pp. 425‚Äì
442.
[3] C. A. Corneanu, M. O. Sim ¬¥on, J. F. Cohn, and S. E. Guerrero,
‚ÄúSurvey on rgb, 3d, thermal, and multimodal approaches for
facial expression recognition: History, trends, and affect-related
applications,‚Äù IEEE transactions on pattern analysis and machine
intelligence, vol. 38, no. 8, pp. 1548‚Äì1568, 2016.
[4] A. Mohammadian, H. Aghaeinia, and F. Towhidkhah, ‚ÄúIncorpo-
rating prior knowledge from the new person into recognition of
facial expression,‚Äù Signal, Image and Video Processing, vol. 10, no. 2,
pp. 235‚Äì242, 2016.
[5] S. Rifai, Y. Bengio, A. Courville, P . Vincent, and M. Mirza, ‚ÄúDisen-
tangling factors of variation for facial expression recognition,‚Äù in
European Conference on Computer Vision, 2012, pp. 808‚Äì822.
[6] P . Werner, A. Al-Hamadi, K. Limbrecht-Ecklundt, S. Walter,
S. Gruss, and H. C. Traue, ‚ÄúAutomatic pain assessment with facial
activity descriptors,‚Äù IEEE Transactions on Affective Computing,
vol. 8, no. 3, pp. 286‚Äì299, 2017.
[7] K. Dobs, J. Schultz, I. B ¬®ulthoff, and J. L. Gardner, ‚ÄúTask-dependent
enhancement of facial expression and identity representations in
human cortex,‚Äù NeuroImage, vol. 172, pp. 689‚Äì702, 2018.
[8] G. Saon, H. Soltau, D. Nahamoo, and M. Picheny, ‚ÄúSpeaker
adaptation of neural network acoustic models using i-vectors,‚Äù
inAutomatic Speech Recognition and Understanding, 2014, pp. 55‚Äì59.
[9] V . Gupta, P . Kenny, P . Ouellet, and T. Stafylakis, ‚ÄúI-vector-based
speaker adaptation of deep neural networks for french broadcast
audio transcription,‚Äù in IEEE International Conference on Acoustics,
Speech and Signal Processing, 2014, pp. 6334‚Äì6338.
[10] Y. Sun, X. Wang, and X. Tang, ‚ÄúDeep learning face representationby joint identiÔ¨Åcation-veriÔ¨Åcation,‚Äù Advances in Neural Information
Processing Systems, vol. 27, pp. 1988‚Äì1996, 2014.
[11] Y. Sun, D. Liang, X. Wang, and X. Tang, ‚ÄúDeepid3: Face recognition
with very deep neural networks,‚Äù Computer Science, 2015.
[12] Y. Sun, X. Wang, and X. Tang, ‚ÄúDeep learning face representation
from predicting 10,000 classes,‚Äù in IEEE Conference on Computer
Vision and Pattern Recognition, 2014, pp. 1891‚Äì1898.
[13] Y. Tang, ‚ÄúDeep learning using support vector machines,‚Äù ICML
Workshop on Representational Learning, 2013.
[14] Y. Sun, X. Wang, and X. Tang, ‚ÄúHybrid deep learning for face
veriÔ¨Åcation,‚Äù IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 38, no. 10, pp. 1997‚Äì2009, 2016.
[15] P . Liu, S. Han, Z. Meng, and Y. Tong, ‚ÄúFacial expression recognition
via a boosted deep belief network,‚Äù in IEEE Conference on Computer
Vision and Pattern Recognition, 2014, pp. 1805‚Äì1812.
[16] X. Feng, M. Pietikinen, and A. Hadid, ‚ÄúFacial expression recog-
nition based on local binary patterns,‚Äù Computer Engineering and
Applications, vol. 17, no. 4, pp. 592‚Äì598, 2007.
[17] C. Liu and H. Wechsler, ‚ÄúGabor feature based classiÔ¨Åcation using
the enhanced Ô¨Åsher linear discriminant model for face recogni-
tion.‚Äù IEEE Transactions on Image Processing , vol. 11, no. 4, p. 467,
2002.
[18] T. Ahonen, A. Hadid, and M. Pietikainen, ‚ÄúFace description with
local binary patterns: Application to face recognition,‚Äù IEEE Trans-
actions on Pattern Analysis and Machine Intelligence , vol. 28, no. 12,
pp. 2037‚Äì2041, 2006.
[19] D. G. Lowe, ‚ÄúDistinctive image features from scale-invariant key-
points,‚Äù International Journal of Computer Vision, vol. 60, no. 2, pp.
91‚Äì110, 2004.
[20] C. Lu and X. Tang, ‚ÄúSurpassing human-level face veriÔ¨Åcation
performance on lfw with gaussianface,‚Äù Computer Science, 2014.
1949-3045 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2018.2880201, IEEE
Transactions on Affective Computing
IEEE TRANSACTION ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, AUGUST 2015 8
[21] M. Xu, W. Cheng, Q. Zhao, L. Ma, and F. Xu, ‚ÄúFacial expression
recognition based on transfer learning from deep convolutional
networks,‚Äù in International Conference on Natural Computation, 2016,
pp. 702‚Äì708.
[22] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning
for image recognition,‚Äù in IEEE Conference on Computer Vision and
Pattern Recognition, June 2016, pp. 770‚Äì778.
[23] B. E. Boser, I. M. Guyon, and V . N. Vapnik, ‚ÄúA training algorithm
for optimal margin classiÔ¨Åers,‚Äù in The Workshop on Computational
Learning Theory, 1992, pp. 144‚Äì152.
[24] L. Ma and K. Khorasani, ‚ÄúFacial expression recognition using
constructive feedforward neural networks,‚Äù IEEE Transactions on
Systems Man Cybernetics Part B Cybernetics , vol. 34, no. 3, p. 1588,
2004.
[25] S. J. Wang, H. L. Chen, W. J. Yan, Y. H. Chen, and X. Fu, ‚ÄúFace
recognition and micro-expression recognition based on discrim-
inant tensor subspace analysis plus extreme learning machine,‚Äù
Neural Processing Letters, vol. 39, no. 1, pp. 25‚Äì43, 2014.
[26] D. Ghimire and J. Lee, ‚ÄúExtreme learning machine ensemble using
bagging for facial expression recognition,‚Äù Journal of Information
Processing Systems, vol. 10, no. 3, p. 443 458, 2014.
[27] Z. Yu and C. Zhang, ‚ÄúImage based static facial expression recog-
nition with multiple deep network learning,‚Äù in ACM on Interna-
tional Conference on Multimodal Interaction, 2015, pp. 435‚Äì442.
[28] I. J. Goodfellow, D. Erhan, P . L. Carrier, A. Courville, M. Mirza,
B. Hamner, W. Cukierski, Y. Tang, D. Thaler, and D. H. Lee,
‚ÄúChallenges in representation learning: A report on three machine
learning contests,‚Äù Neural Networks, vol. 64, p. 59, 2015.
[29] E. Barsoum, C. Zhang, C. C. Ferrer, and Z. Zhang, ‚ÄúTraining deep
networks for facial expression recognition with crowd-sourced
label distribution,‚Äù in ACM International Conference on Multimodal
Interaction, 2016, pp. 279‚Äì283.
[30] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImagenet classiÔ¨Å-
cation with deep convolutional neural networks,‚Äù Communications
of the ACM, vol. 60, no. 2, p. 2012, 2012.
[31] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional net-
works for large-scale image recognition,‚Äù Computer Science, 2014.
[32] S. Ioffe and C. Szegedy, ‚ÄúBatch normalization: accelerating deep
network training by reducing internal covariate shift,‚Äù in Interna-
tional Conference on Machine Learning, 2015, pp. 448‚Äì456.
[33] C. Szegedy, W. Liu, Y. Jia, P . Sermanet, S. Reed, D. Anguelov,
D. Erhan, V . Vanhoucke, and A. Rabinovich, ‚ÄúGoing deeper with
convolutions,‚Äù in IEEE Conference on Computer Vision and Pattern
Recognition, 2015, pp. 1‚Äì9.
[34] C. Szegedy, S. Ioffe, V . Vanhoucke, and A. A. Alemi, ‚ÄúInception-
v4, inception-resnet and the impact of residual connections on
learning,‚Äù in AAAI Conference on ArtiÔ¨Åcial Intelligence , 2017, pp.
4278‚Äì4284.
[35] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, and M. Bernstein, ‚ÄúImagenet
large scale visual recognition challenge,‚Äù International Journal of
Computer Vision, vol. 115, no. 3, pp. 211‚Äì252, 2015.
[36] V . Nair and G. E. Hinton, ‚ÄúRectiÔ¨Åed linear units improve restricted
boltzmann machines,‚Äù in International Conference on Machine Learn-
ing, 2010, pp. 807‚Äì814.
[37] T. Kanade, J. F. Cohn, and Y. Tian, ‚ÄúComprehensive database
for facial expression analysis,‚Äù in IEEE International Conference
on Automatic Face and Gesture Recognition, 2000. Proceedings , 2002,
p. 46.
[38] D. Yi, Z. Lei, S. Liao, and S. Z. Li, ‚ÄúLearning face representation
from scratch,‚Äù in IEEE Conference on Computer Vision and Pattern
Recognition, 2014.
[39] G. B. Huang, M. A. Mattar, H. Lee, and E. Learned-Miller, ‚ÄúLearn-
ing to align from scratch,‚Äù Advances in Neural Information Processing
Systems, pp. 764‚Äì772, 2012.
[40] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov, ‚ÄúDropout: a simple way to prevent neural
networks from overÔ¨Åtting,‚Äù Journal of Machine Learning Research,
vol. 15, no. 1, pp. 1929‚Äì1958, 2014.
[41] Y. L. Cun, B. Boser, J. S. Denker, R. E. Howard, W. Habbard, L. D.
Jackel, and D. Henderson, ‚ÄúHandwritten digit recognition with
a back-propagation network,‚Äù in Advances in Neural Information
Processing Systems, 1990, pp. 396‚Äì404.
[42] I. Sutskever, J. Martens, G. Dahl, and G. Hinton, ‚ÄúOn the im-
portance of initialization and momentum in deep learning,‚Äù in
International Conference on Machine Learning, 2013, pp. 1139‚Äì1147.[43] Y. Sun, X. Wang, and X. Tang, ‚ÄúDeep convolutional network
cascade for facial point detection,‚Äù in IEEE Conference on Computer
Vision and Pattern Recognition, 2013, pp. 3476‚Äì3483.
[44] F. D. L. Torre, W. S. Chu, X. Xiong, and F. Vicente, ‚ÄúIntraface,‚Äù in
IEEE International Conference and Workshops on Automatic Face and
Gesture Recognition, 2015, pp. 1‚Äì8.
[45] A. T. Lopes, E. D. Aguiar, A. F. D. Souza, and T. Oliveira-
Santos, ‚ÄúFacial expression recognition with convolutional neural
networks: Coping with few data and the training sample order,‚Äù
Pattern Recognition, vol. 61, pp. 610‚Äì628, 2017.
Ming Li received his Ph.D. in Electrical Engi-
neering from University of Southern California in
May 2013. He is currently an associate professor
of Electrical and Computer Engineering at Duke
Kunshan University and a research scholar at
the ECE department of Duke University. His re-
search interests are in the areas of speech pro-
cessing and multimodal behavior signal analysis
with applications to human centered behavioral
informatics notably in health, education and se-
curity.
Hao Xu is an undergraduate student at School
of Data and Computer Science, Sun Y at-sen
University. He is a research assistant in Speech
and Multimodal Intelligent Information Process-
ing Laboratory. His research interests include
facial expression recognition and gaze recogni-
tion.
Xingchang Huang is an undergraduate student
at Sun Y at-sen University, major in computer
science. He is a research assistant in Speech
and Multimodal Intelligent Information Process-
ing Laboratory. His research interests include
machine learning, computer vision and their ap-
plication on face recognition and facial expres-
sion recognition.
Zhanmei Song received the Ph.D. degree in
Early Childhood Education(ECE) from East Chi-
na Normal University in 2012. She is currently
a professor at Department of Early Childhood
Education of Shandong Yingcai University. Her
research interests include Compensation Edu-
cation for Children Left-behind and Kindergarten
Curriculum. She is the General Secretary of the
National Association of ECE,Chinese Society of
Education.
Xiaolin Liu received a master degree in Early
Childhood Education from Northeast China Nor-
mal University in 2009,and now studying Doctor
Degree of Education Management at Dhurakij
Pundit University, Thailand. She is currently an
Associate Professor in the School of Preschool
Education at Shandong Yingcai University. Her
research interests include multimodel technolo-
gy in early childhood education.
Xin Li received the Ph.D. degree in Electrical
and Computer Engineering from Carnegie Mel-
lon University in 2005. He is currently a Pro-
fessor in the School of Preschool Education at
Shandong Yingcai University. His research inter-
ests include signal processing and data analyt-
ics. He is a Fellow of IEEE.
"
https://ieeexplore.ieee.org/document/8734943,"5th Conference on Knowledge-Based Engineering and Innovation , Iran University of Science and Technology, Tehran, Iran  
 
978-1-7281-0872-8/19/$31.00 ¬©2019 IEEE  Fast Facial emotion recognition Using Convolutional 
Neural Networks and Gabor Filters 
 
Milad Mohammad Taghi Zadeh  
Department of Electrical Engineering  
Khatam University 
Tehran, Iran 
m.mohammadtaghizadeh@khatam.ac.ir  
 Maryam Imani 
Department of Electrical Engineering  
Tarbiat Modarres University 
Tehran, Iran 
maryam.imani@modares.ac.ir 
   Babak Majidi 
Department of Computer Engineering  
Khatam University 
Tehran, Iran 
b.majidi@khatam.ac.ir   
 
Abstract - The emotions evolved in human face have a great 
influence on decisions and arguments about various subjects. In 
psychological theory, emotional states of a person can be classified 
into six main categories: surprise, fear, disgust, anger, happiness 
and sadness. Automatic extraction of these emotions from the face 
images can help in human computer interaction as well as many 
other applications. Machine learning algorithms and especially 
deep neural network can learn complex features and classify the 
extracted patterns. In this paper, a deep learning based 
framework is proposed for human emotion recognition. The 
proposed framework uses the Gabor filters for feature extraction 
and then a Convolutional Neural Network (CNN) for 
classification. The experimental results show that the proposed 
methodology increases both of the speed training process of CNN 
and the recognition accuracy.  
Keywords‚ÄîFacial emotion r ecognition, Gabor filter, 
Convolution neural network 
I. INTRODUCTION  
Emotions have an important role in our everyday lives, and 
directly affect decisions, reason ing, attention, prosperity, and 
quality of life of human. Establishing communication between 
people is through emotions and facial expressions. Nowadays, 
with the influence of computers on human lives and the 
mechanization of lives of individuals, establishment of human 
and computer interaction (HCI) has played a crucial and very 
important role [1]. There is a strong interest in improving the 
interaction between humans and computers. Many people 
believe in this theory and there is a positive and useful emotional 
response for establishing a good and useful cognitive link 
between computers with users. This interaction between the 
computer and the human beings can be created through speech 
[2, 3]. Psychological theory states that human emotions can be 
classified into six different forms:  surprise, fear, disgust, anger, 
happiness, and sadness. By making changes in the facial 
muscles, human can represent this group of emotions. 
Recently, the deep neural networks achieved significantly 
good results in modelling complex patterns [4, 5, 6]. In this 
paper, a deep learning based framework for human emotion 
detection is presented. The proposed framework uses the Gabor 
filters for feature extraction and then the deep convolutional 
neural network. The experimental results show that the proposed 
features increase the speed and accuracy of training the neural network. The rest of this paper is organized as follows. After 
describing the related literature in Section II, the proposed fast 
facial emotion recognition framework is detailed in Section III. 
The experimental design and the simulation scenarios are 
discussed in Section IV. Finally, Section V concludes the paper. 
II. RELATED WORKS  
Kwolek et al. [7] performed facial recognition using the 
Gabor filter to extract features. It was shown in that paper that 
using the Gabor filter improves the accuracy of convolutional 
neural network from 79% to 87.5%. Wu et al. [8] suggested that 
Gabor motion energy filters (GME) could be used to detect 
facial expressions. In this method, GME filter was compared 
with Gabor energy (GE) filter and the results showed that the 
proposed method was 7% better. Li et al. [9] presented a deep 
fusion convolutional neural network (DF-CNN) method using 
2D+3D images. In this method, three-dimensional scan images 
of the faces are used. These images make up a total of 32 
dimensions. This paper explains that prediction is done using 
two methods. 1- Classification by using the SVM from the 32-
dimensional features. 2- The normal prediction of the Softmax 
function using the six-state probability vector. Experimental 
results show DF-CNN achieved good results. Li et al. [10] 
argued that the existing face data bases are not reliable for the 
real world applications. This article presents a new database 
called RAF-DB that contains 30,000 facial expressions from 
thousands of people. In the gathering of this database, it has been 
seen that real faces often have mixed feelings, in other words, a 
mixture of several feelings. By reviewing the RAF-DB database, 
it is the first natural database that  is much more diverse than the 
seven most commonly used datasets. As a result, a DLP-CNN 
suggested a way of expressing a difference between feelings. 
These experiments were performed based on 7 basic conditions 
and 11 combinations. By examining this method on the SFEW 
and CK + databases, it was shown that the proposed DLP-CNN 
method is the best method for de tecting the state of the face  
emotion. Tzirakis et al. [11] suggested that both voice 
processing and image processing can be used to detect human 
situations. In this method, the sound is separately conveyed to a 
neural network. In another direction, the image also provides a 
50-layer convolutional neural network. This method was 
performed on the RECOLA database and it is argued that the 
results of the experiments show  better performance than other 
577
methods. Lee et al. [12] suggested that the eye can be used to 
extract emotions.  
In this method, which was performed using STFT and CNN, 
the STFT extracts the necessary features in two cases of eye size 
and motion and serves as an in put to the CNN network with two 
layers. This review shows that the CNN network is also effective 
in the eye-based diagnosis, in addition to being effective in 
recognizing emotions in different ways. Pons et al. [13] 
proposed to train a CNN network for the final face emotion 
classification, which can be used to evaluate the combination of 
CNN networks. In this method, 72 CNN networks were trained 
with four layers and with different parameters, and 64 CNN 
networks were trained with VGG-16. In the final analysis, the 
output of different networks wa s classified using the proposed 
classification and the result was presented. The results of this 
study show that this method can increase accuracy by 5% in 
different classification methods. Tang et al. [14] presented three 
different methods called DGFN, DFSN, and DFSN-I. DGFN is 
actually based on the critical points of psychology and 
physiology rules and uses the traditional machine learning 
network. DFSN is designed on the basis of CNN. In the end, 
DFSN-I combines both DGFN and DFSN, which has both 
advantages for better performance. Experimental results show 
that the performance in the CK + database and Oulu CASIA 
improved. 
III. METHODOLOGY  
A. Gabor filter 
Gabor filters [15] are generally used in texture analysis, edge 
detection, feature extraction . In (1), the Gabor filter is described. 
When a Gabor filter is applied to an image, it gives the highest 
response at edges and at points where texture changes. The 
following images show a test image and its transformation after 
the filter is applied. 
22 2
2'' '( , ; , , , , ) exp( )exp( (2 ))2xy xgxy iŒ≥ŒªŒ∏œàœÉŒ≥ œÄ œàœÉŒª+=‚àí + (1) 
in which the real part is: 
22 2
2'' '(, ; , , , , ) e x p ( ) c o s ( 2 )2xy xgxyŒ≥ŒªŒ∏œà œÉ Œ≥ œÄ œàœÉŒª+=‚àí +  (2) 
and the imaginary part is: 
22 2
2'' '(, ; , , , , ) e x p ( ) s i n ( 2 )2xy xgxyŒ≥ŒªŒ∏œàœÉ Œ≥ œÄ œàœÉŒª+=‚àí +  (3) 
where: 
'c o s s i nxxyŒ∏Œ∏ =+  (4) 
and 
's i n c o syx y Œ∏Œ∏ =‚àí +  (5) 
The proposed filter is shown in Figure 1. As shown in Figure 
1, the original image of the first Gabor filter is displayed and 
then we pass the filtered image again from the second Gabor 
filter. In Figure 2, four samples of the original images are shown.  
 
  
 
 
 
Figure 1‚Äì The proposed filter 
 
 
Figure 2- Original Images  
 
In Figure 3, four sample images are shown after applying the 
Gabor filter on Orginal images with the following parameters: 
() (, ) 1 8 , 1 8 1 . 5 , / 4 , 5 , 1 . 5
  0, xy œÉŒ∏ œÄ Œª Œ≥
œà== = = =
=
 
Table 1 - Definition the parameters of Gabor filte r
Definition  Parameters  
The size of the Gabor kernel  K=(x, y)  
The wavelength of the sinusoidal factor in 
the above equation. Œª  
The orientation of the normal to the 
parallel stripes of the Gabor function. Œ∏  
Phase offset. œà 
Standard deviation of the Gaussian 
function used in the Gabor filter.  œÉ 
Spatial aspect ratio. Œ≥ 
578
 
Figure 3 -Sample image after applying first Gabor Filter  
 
In Figure 4, four sample images are shown after applying the 
Gabor filter on first Gabor filtered images with the following 
parameters: 
() ( , ) 18 ,18 1. 5, 3 /4, 5, 1. 5
  0, xy œÉŒ∏ œÄ Œª Œ≥
œà== = = =
= 
 
     
Figure 4‚Äì Sample image after applying second Gabor Filter 
B. CNN Architecture 
The structure of the convolutional neural network is shown 
in Figure 4 . As shown in Table 2,  first, the image is received 
and after passing through different layers and the learning 
process returns a vector with seven modes as output. In fact, 
these seven modes are: Angry, Disgust, Fear, Happy, Neutral, 
Sad and Surprise. 
  
Figure 5‚Äì The CNN architecture  
 
As shown in Table 1, in the first stage, the deep neural 
network applies a convolution of a 6 √ó 6 filter on the image. In 
the next step, using MaxPooling, the dimensions are reduced to 
128 √ó 128 √ó 6. Next, another convolutional network will be 
applied to the 16 √ó 16 filter size, and in the next step, using the 
MaxPooling function, we will reduce the size to 64 √ó 64 √ó 16. 
The next convolution is applied to the data with a 120 √ó 120 
filter size. In the next step, using the Flatten function, all data is 
converted to a vector of the size 432000. Then, the vector is 
converted to a vector of length 84, and at the end, it is reduced 
to seven, which is the 7 categories of emotional states. 
 
Table 2- Proposed CNN architecture method details 
Output Shape  Details  Layer type  
256, 256, 6  Conv (6x6)  Conv  
256, 256, 6  Relu  Activation  
128, 128, 6  Pool size (2,2)  MaxPooling  
128, 128, 16  Conv (16x16)  Conv  
128, 128, 16  Relu  Activation  
64, 64, 16  Pool size (2,2)  MaxPooling  
60, 60, 120  Conv (120x120)  Conv  
60, 60, 120  Relu  Activation  
60, 60, 120  --------------------  Dropout            
432000  Flatten to a vector  Flatten           
84  Input ÔÉ†84   Dense                
84  Relu  Activation   
84  --------------------  Dropout            
7  Input ÔÉ†  
Classese Num =7  Dense                
7  softmax  activation  
 
Rectified Linear Unit (Relu) function: 
() m a x ( 0 ,)fxx x+==  (6)
 
 
579
Softmax function: 
1[ 0 , 1 ]ii
iXx==ÔÉ•  (7) 
C. Proposed facial emotion recognition algorithm: 
 The proposed method is to first apply a Gabor filter to the 
images and then convey the output results as inputs to the neural 
network. The output of the Gabor filter is given to the 
convolutional neural network. 
 
 
Figure 6- The proposed method  
IV. EXPERIMENTAL RESULTS  
For the experimental results, the JAFFE database [16] is used. 
The database contains 213 Japanese female model images that 
include seven emotional states of the face, in which six modes, 
natural states of the face and normal face. Table 3 shows the 
specification of the simulation hardware. According to Figure 
8 and Figure 9 and Table 4, the results show that after 10 
epochs, the proposed method reaches 86% accuracy, but 
conventional CNN method reach 51% accuracy. After 15 
epochs, the proposed method reaches 92% accuracy but 
conventional CNN reaches 73% accuracy. After 25 epochs, the 
proposed method reaches 97% accuracy while its competitor 
achieves 90% accuracy. At the end, the proposed approach 
obtains 97% accuracy and the other reaches 91% accuracy. 
According to Table 5, the accuracy of 70-73% is achieved after 
8 epochs which take 308 seconds using the proposed approach 
while it takes 521 seconds using the convolutional method. In 
addition, the accuracy of 91-92% takes 541 seconds to be 
achieved using the proposed approach and it takes 1189 
seconds using the convolutional method. Therefore, the 
proposed approach needs less time than the convolutional 
approach to achieve the same  accuracy. Incr ease of running 
time in the proposed method is expected because the sub-
features extracted by two cons ecutive Gabor filters simplify the 
learning process (feature extraction and classification) of the 
CNN. The first used Gabor filter extracts texture information in 
different scales and directions while the second one extracts 
more sub-features from the previous Gabor feature map. 
 
  
Table 3 ‚Äì The system specifications 
Model Dell XPS L502 
Processo r Intel Core i7 CPU ‚Äì 2.00GHz
RAM 8 GB  
System type 64- bit Operating Syste m
 
             
Figure 7‚Äì sample of correct emotional recognition 
 
      Table 4 ‚Äì Comparison accuracy of proposed method and simple CNN  
Epoch CNN Method 
Accuracy 2Gabor + CNN 
Method 
1 0.1050 0.1326 
10 0.5138 0.8619 
15 0.7348 0.9227 
20 0.8343 0.9558 
25 0.9006 0.9779 
30 0.9116 0.9716 
 
 
Table 5 ‚Äì Comparison speed of proposed method and simple CNN  
Accuracy CNN Method  2Gabor + CNN  
Epoch Time(s) Epoch Time(s) 
10-15% 1 42 1 41 
50-55% 9 330 6 232 
70-73% 14 521 8 308 
86-87% 18 683 10 390 
91-92% 30 1189 14 541 
 
Input 
Images
Resize
First Gabor 
Filter
Second 
Gabor Filter
CNN
 Emotions
Sub feature Extraction 
580
 
Figure 8- Accuracy of simple CNN method 
 
 
Figure 9‚Äì Accuracy of 2Gabor filter+CNN method 
 
V. CONCLUSION  
 After the Gabor filter applied, the system learning became 
faster and the accuracy has improved. As seen in Figures 7 and 
8. The learning speed of the convolutional neural network has 
increased profoundly. This is be cause the Gabor filter actually 
extracts the image subfeature and gives the neural network. By 
doing this, the convolutional neural network receives a number 
of subfeature and takes one step further in extracting the 
emotions from the faces. 
REFERENCES  
 
[1] S. L. Happy and A. Routray, ""Automatic facial expression recognition 
using features of salient facial patches,"" IEEE Transactions on Affective 
Computing, vol. 6, pp. 1-12, 2015. [2] A. Nicolai and A. Choi, ""Facial Emotion Recognition Using Fuzzy 
Systems,"" in 2015 IEEE International Conference on Systems, Man, and 
Cybernetics , 2015, pp. 2216-2221. 
[3] M. Imani, G. A. Montazer, GLCM Features and Fuzzy Nearest Neighbor 
Classifier for Emotion Recognition from Face,  7th International 
Conference on Computer and Knowledge Engineering (ICCKE 2017), 
Mashhad, Iran, pp. 8-13, 26-27 October 2017. 
[4] M. H. Abbasi, B. Majidi, and M. T. Manzuri, ""Deep cross altitude visual 
interpretation for service robotic agents in smart city,"" in 2018 6th Iranian 
Joint Congress on Fuzzy and Intelligent Systems (CFIS), 2018, pp. 79-82. 
[5] M. H. Abbasi, B. Majidi, and M. T. Manzuri, ""Glimpse-gaze deep vision 
for Modular Rapidly Deployable Decision Support Agent in smart 
jungle,"" in 2018 6th Iranian Joint Congress on Fuzzy and Intelligent 
Systems (CFIS), 2018, pp. 75-78. 
[6] A. Fadaeddini, M. Eshghi, and B. Majidi, ""A deep residual neural network 
for low altitude remote sensing image classification,"" in 2018 6th Iranian 
Joint Congress on Fuzzy and Intelligent Systems (CFIS), 2018, pp. 43-46. 
[7] B. Kwolek, ""Face Detection Using Convolutional Neural Networks and 
Gabor Filters,"" in Artificial Neural Networks: Biological Inspirations ‚Äì 
ICANN 2005 , Berlin, Heidelberg, 2005, pp. 551-556. 
[8] T. Wu, M. S. Bartlett, and J. R. Movellan, ""Facial expression recognition 
using Gabor motion energy filters,"" in 2010 IEEE Computer Society 
Conference on Computer Vision and Pattern Recognition - Workshops , 
2010, pp. 42-47. 
[9] H. Li, J. Sun, Z. Xu, and L. Chen, ""Multimodal 2D+3D Facial Expression 
Recognition With Deep Fusion Convolutional Neural Network,"" IEEE 
Transactions on Multimedia, vol. 19, pp. 2816-2831, 2017. 
[10] S. Li, W. Deng, and J. Du, ""Reliable Crowdsourcing and Deep Locality-
Preserving Learning for Expression Recognition in the Wild,"" in 2017 
IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 
2017, pp. 2584-2593. 
[11] P. Tzirakis, G. Trigeorgis, M. Nicolaou, B. Schuller, and S. Zafeiriou, 
End-to-End Multimodal Emotion Recognition Using Deep Neural 
Networks  vol. PP, 2017. 
[12] H. Lee and S. Lee, ""Arousal-valence recognition using CNN with STFT 
feature-combined image,"" Electronics Letters, vol. 54, pp. 134-136, 2018. 
[13] G. Pons and D. Masip, ""Supervised Committee of Convolutional Neural 
Networks in Automated Facial Expression Analysis,"" IEEE Transactions 
on Affective Computing, vol. 9, pp. 343-350, 2018. 
[14] Y. Tang, X. M. Zhang, and H. Wang, ""Geometric-Convolutional Feature 
Fusion Based on Learning Propagation for Facial Expression 
Recognition,"" IEEE Access, vol. 6, pp. 42532-42540, 2018. 
[15] Maryam Imani , 3D Gabor Based Hyperspectral Anomaly 
Detection, Amirkabir International Journal of Modeling, Identification, 
Simulation & Control (AUT Journal of Modeling and 
Simulation), vol.  50, no. 2, pp. 101-110, 2018. 
[16] The Japanese Female Facial Expression (JAFFE) Database . Available: 
http://www.kasrl.org/jaffe.html 
 
581
"
http://ieeexplore.ieee.org/abstract/document/7789677/,"Abstract
Face alignment can fail in  real-world conditions, 
negatively impacting the performance of automatic facial 
expression recognition (FER) s ystems. In this study, we 
assume a realistic situation including non-alignable faces 
due to failures in facial la ndmark detection. Our proposed 
approach fuses information about non-aligned and aligned 
facial states, in order to boost FER accuracy and efficiency. 
Six experimental sc enarios using discriminative deep 
convolutional neural networks (DCNs) are compared, and 
causes for performance differ ences are identified. To 
handle non-alignable faces b etter, we further introduce 
DCNs that learn a mapping from non-aligned facial states 
to aligned ones, alignment-ma pping networks (AMNs). We 
show that AMNs represent geometric transformations of 
face alignment, providing features benefic ial for FER. Our 
automatic system based on ens embles of the discriminative 
DCNs and the AMNs achieves impressive results on a 
challenging database for FER in the wild. 
1. Introduction 
Facial expression is a primary means for understanding 
human emotions and has been actively studied over two 
decades [see 1-3 for survey]. Rapid progress has been made 
on technologies for automatic facial expression recognition 
(FER), particularly in controlled laboratory settings. 
Nevertheless, FER still remains a challenge in uncontrolled 
real-life situations in which a FER system must handle 
unpredictable variability in head poses, lighting conditions, 
occlusions, and subjects. To r esolve this issue, researchers
have collected large volumes of data ‚Äúin the wild‚Äù [4 -7], 
have held many grand challenges [8-11], and have 
presented excellent approaches notably with deep learning 
techniques [12-18]. 
Face alignment commonly performed in preprocessing 
modules is essential for achieving good FER performance [6, 16, 18]. However, many  studies have applied some 
‚Äúmanual‚Äù steps when this pr eprocessing fails. For real-life 
applications, automatic FER systems in which human 
intervention is not necessar y are preferred, and this 
demands automatic face alignment. Recently, such 
alignment techniques have been extensively studied by
using holistic deformable models (DMs) [19, 20] and 
parts-based DMs [21, 22], by taking advantage of both of 
the DMs [23, 24], and by applying regression-based 
methods [25, 26]. However, when alignment errors or 
failures occur ‚Äúin the wild‚Äù and propagate to later stages of 
FER systems, final performance declines. 
In this paper, we propose a framework based on an 
ensemble of deep convolutional neural networks (DCNs) 
toward automatic FER in the wild. We begin with the 
following realistic assumptio n (validated in Section 3):  
/g132Face alignment under real-world conditions is 
assumed to be not always successful. 
/g132Consequently, face imag es are either ‚Äúalignable‚Äù, 
i.e., cap able of being aligned or ‚Äúnon -alignable‚Äù, i.e.,
not capable of being aligned. Fusing Aligned and Non-Aligned Face Information 
for Automatic Affect Recognition in the Wild: A Deep Learning Approach
Bo-Kyeong Kim, Suh-Yeon Dong, Jihyeon Roh, Geonmin Kim, Soo-Young Lee
Computational NeuroSystems Laboratory (CNSL)
Korea Advanced Institute of Science and Technology (KAIST)
 
{bokyeong1015, suhyeon.dong}@gmail.com, {rohleejh, gmkim90, sylee}@kaist.ac.kr
Figure 1: Overview of our approach. Our automatic FER 
system contains several DCNs to fuse information of 
non-aligned and aligned facial states.
2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops
978-1-5090-1437-8/16 $31.00 ¬© 2016 IEEE
DOI 10.1109/CVPRW.2016.1871499

Our FER system is designed to separately deal with 
alignable faces and non-alignable faces, as shown in Figure 
1. For alignable faces, we combine information from both of 
the ‚Äún on-aligned state‚Äù and the ‚Äúaligned state‚Äù. For 
non-alignable faces, we initially obtain class predictions 
using the ‚Äúnon -aligned state‚Äù. In order to determine the 
method to train discrimin ative DCNs, we assess several 
experimental scenarios for a better information fusion 
(Section 4). Then, in order to estimate aligned states of
non-alignable faces, we introduce DCNs which learn a 
mapping  from the non-aligned facial state to the aligned one 
(Section 5), called alignment-mapping networks (AMNs). 
Experimental results demonstrate that these AMNs 
represent similarity transformations performed in face 
alignment and yield hidden fe atures beneficial for FER. To 
summarize, our empirical findings in developing the 
automatic FER system show that  
/g132In the learning phase of individual discriminative 
DCNs, it is beneficial to use the merged training dataset 
of alignable and non-alignable faces for efficiency in 
evaluation time as well as for effectiveness in FER 
performance. 
/g132In the testing phase for alignable faces, combining 
information of both non-aligned and aligned states from 
discriminative DCNs improves FER accuracy. 
/g132In the testing phase for non-alignable and alignable 
faces, it is better to add decision-level information from 
classifying the hidden features of AMNs. 
We evaluate the proposed framework on the facial 
expression recognition 2013 (FER-2013) database [7], a 
challenging benchmark collected from the Web. Our final 
ensemble-based FER system achieves great performance on 
this in-the-wild database. 
2. Related Work
Diversity and ensembles  of neural networks.  Combining 
decisions of multiple artificial neural networks is a method 
with a long history [27-29] in the field of classifier ensemble. 
Here, employing ‚Äúdiversity‚Äù of individual networks has 
been shown to improve ensemble performance, by 
providing uncorrelated, different, and thus informative 
decisions [30, 31]. For high diversity, individual networks 
are built using several diversification strategies [28, 29], 
e.g., by altering network architectu res and random weight 
initializations as well as by using variously normalized and 
differently divided datasets. Su ch strategies are now applied 
to ensembles of deep  neural networks, resulting in 
remarkable successes in image classification [32-34]. In this 
work, our DCNs are also built towards high diversity, 
specifically by changing in put normalization and weight initialization. 
Deep learning for facial expression recognition.  In recent 
years, similar to other computer vision problems, applying 
deep learning techniques to FER has attracted considerable 
attention. For disentangling laten t factors in face images, 
Rifai et al.  [12] gradually separated discriminative 
expression information from non-discriminative pose and 
morphology factors based on convolutional networks and 
auto-encoders, while Reed et al.  [13] modeled higher-order 
interactions of expression an d pose manifolds based on a 
restricted Boltzmann machine. In addition, Reed et al.  [14] 
used a bootstrapping-based ap proach regarding prediction 
consistency, resolving a problem  of noisy- and subjective 
labels in FER. Liu et al. [15] used convolutional kernels for 
capturing facial action units r elevant to facial expressions 
and extracted higher-level fe atures based on deep belief 
networks. 
To improve affect recognition in the wild, there are 
several ensemble-based deep learning approaches, which 
inspire our work. Kahou et al. [16] proposed a multimodal 
framework including a DCN and a deep belief network for 
extracting visual and audio features, respectively, and 
applied a random-search-based weighted decision fusion. 
Moreover, in order to combine multiple DCNs using late 
fusion schemes, Yu et al. [17] applied a hinge-loss-based 
weighted fusion in a single-level committee, while Kim et al.
[18] used an exponentially weighted fusion in a hierarchical 
committee. These three studies used the FER-2013 database 
described in Sect. 3 to pre-tr ain their DCNs for transfer 
learning or feature extraction. However, they did not cope 
with the failure in face alignment nor consider its influence 
on FER performance. 
Converting pose st ates of face images.  Many deep models 
for face recognition aim to learn a mapping from 
non-frontal faces to frontal ones, which share the similar 
goal with our AMNs. Kan et al.  [35] used a stacked 
auto-encoder network to model gradual change in poses, 
while Zhu et al. [36] designed a deep convolutional network 
including locally-connected layers to extract pose- and 
illumination-robust features. In order to model the 
continuous pose space, Zhu et al.  [37] used stochastic 
neurons in their multi-view perceptron which disentangled 
pose and identity factors. In addition, Yim et al.  [38] 
employed a multi-task learning scheme to convert head 
poses while concurrently maintain ing identity information.  
One limitation of the aforementioned deep models is that 
they focused on changes only in yaw rotation. In addition, 
they dealt with data only ca ptured under controlled lab 
settings, and 2-D aligned faces were used as inputs and 
outputs of their models. In contrast, our work attempts to 
handle all rotations of yaw, pitch, and roll as well as 
cropping and resizing operations. We deal with faces in the 
wild, and the faces before and after 2-D alignment are used 
1500
as inputs and target outputs of our AMNs, respectively. 
Very recently, interesting approaches have been 
proposed to yield frontal facial poses in unconstrained 
settings. After localizing facial landmarks, Hassner et al. 
[39] used a 3-D reference model common to all faces and 
efficiently estimated visible facial parts, improving 
performance. On the other hand, Sagonas et al.  [40] 
employed an effective statistical model to localize 
landmarks and convert facial poses at the same time, 
notably using only frontal faces. Although these studies did 
not use deep learning techniques, excellent results on 
in-the-wild datasets were ob tained particularly due to 
rigorous mathematical derivation. 
3. Data and Face Alignment 
This section describes the FER-2013 database used in our 
work. To investigate the validity of assumption in Sect. 1, 
we present face alignment results on this database. Finally, 
experimental datasets which we construct are introduced.  
3.1. The FER-2013 database 
The FER-2013 database was collected from the Web, and 
most images were captured under real-world imaging 
conditions. This database has been reported to include some 
noisy or confusing annotation and show a low human FER 
accuracy of approximately 65% [11, 41, 42]. This is
possibly due to the method of its construction [11]. 
Specifically, more than 100 fine-grained emotion keywords 
searching out images were clustered into seven target 
classes: anger, disgust, fear, ha ppiness, sadness, surprise, 
and neutral expression. Here, clustering errors could result 
in noisy and subjective labels. 
The original FER-2013 was introduced for the 
sub-challenge of ICMLW‚Äô13 [11], and consists of 35,887 
detected faces in grayscale at a size of 48-by-48 pixels: 28,709 faces for training, 3,589 for public test, and 3,589 
for private test. In the present study, after removing 11 
non-number-filled training images, we randomly divide the 
training data into two parts: 25,110 faces (about seven 
eighths) for training our models and 3,588 faces for 
validation. For fair comparison with previous studies on this 
database, we only use the pr ivate test data for evaluation, 
and do not use any public test data for validation or 
evaluation. 
3.2. Face alignment 
For face registration, we conduct a conventional 2-D
alignment using IntraFace [25, 43], a publicly-available 
landmark detector. After localizin g the 49 predefined facial 
landmarks, face images ar e automatically rotated and 
cropped based on the eye coordinates, and finally resized 
into 48-by-48 pixels. Notice that manual preprocessing 
steps are not employed, i.e., erroneously-aligned and 
non-alignable faces are not removed, and so are used in later
processing stages. 
 Alignment performance on the FER-2013 is reported in 
Table 1. Here, 85 p ercent of faces are ‚Äúalignable‚Äù, meaning 
that IntraFace can provide the set of landmarks with the 
corresponding confidence sc ore and pose angle of pitch, 
yaw and roll. Figure 2 shows histograms of the number of 
alignable faces as a function of the pose angle, some aligned 
examples, and their localized  landmarks. Due to failed 
Figure 2: Statistics of head pose and examples of 2-D aligned result on the FER-2013 DB. The graph (a) shows the number of
alignable f aces as a function of the pose angle of yaw, pitch, and roll on a log scale, and the (b) shows the percent ratio of aliganble 
faces for each pose angle on a linear scale. The ranges of (-31.7, 32.7) degree for yaw rotation, (-17.5, 29.1) for pitch, and (-2 0.5, 20.3) 
for roll contain 95% of the alignable faces , indicating in-the-wild conditions of the FER-2013 DB.
FER-2013 DBNumber of Faces (Ratio)
Non-Alignable Alignable Total
DataTrain 3764 (15.0 %) 21346 (85.0 %) 25110
Valid 537 (15.0 %) 3051 (85.0 %) 3588
Test 543 (15.1 %) 3046 (84.9 %) 3589
StateNon-Aligned XNA XA X
Aligned - ZA -
Table 1: Alignment performance and summary of 
experimental datasets according to aligned facial states.
1501
landmark detection, 15 percent of faces are ‚Äún on-alignable‚Äù .
Most of such failures are und er extreme pose, occlusion, 
and/or bad illumination conditions. Examples of 
non-alignable faces are depicted in Figure 7. Clearly, face 
alignment in real-world situations is not always successful, 
posing a specific challen ge for FER research. 
3.3. Formation of ex perimental datasets 
We form experimental datasets  based on the alignment 
results in Sect 3.2. More spec ifically, after the data division 
described in Sect 3.1, face alignment with IntraFace is 
performed on each of training, validation, and testing data. 
Here, depending on whether IntraFace can localize the 
facial landmarks or not, face images are able to be aligned 
(alignable) or not to be aligned (non-alignable). 
Non-aligned and aligned facial states thus constitute 
distinct classes, and we form  our experimental datasets 
accordingly. Let X= {x} be a set of whole faces before the 
2-D alignment, which are in the non-aligned  state. We 
further divide this set X into two disjoint subsets: XNA= {xNA
is a non-alignable x}, a set of non-alignable  faces in the 
non-aligned state, and XA = {xA is an alignable x}, a set of 
alignable  faces in the non-aligned state. Furthermore, let ZA
= {zA is obtained from xA by face alignment} be a set of 
alignable  faces in the aligned state. Hence, a one-to-one 
correspondence exists between XA and ZA. See Table 1 for 
summary of the notations. 
4. Information Fusion of Non-Aligned and 
Aligned Facial States 
This section discusses four  scenarios with information 
fusion of non-aligned and alig ned states and for comparison 
two scenarios without information fusion. Next, we present 
individual deep models and their ensembles used in the 
considered scenarios. We then  move onto the experimental 
result and analysis. 
4.1. Scenarios of information fusion 
As shown in Sect. 3.2, face alignment during 
preprocessing is not always successful in uncontrolled 
environments. Therefore, both alignable faces (AF) and 
non-alignable faces (N-AF) exist in real-world situations, 
and FER researchers are faced with the question of using 
either aligned or non-aligned state information, or using 
both kinds of information during system development. To 
answer this question, we consid er six scenarios depicted in 
Figure 3. Firstly, the scenarios are categorized according to 
three types of  training dataset  for deep models, i.e.,X(in 
non-aligned facial states), ZA (in aligned facial states), and 
X+ZA. We mark them with red, blue, and yellow color, respectively, in Figure 3. The dataset of X+ZA is constructed 
by merging X  and ZA into one set. The expectation is that 
models trained using X+ZA could extract some features 
representing the fused knowledge  about non-aligned and 
aligned states, since these models simultaneously handle 
faces in both states during learning.  
Moreover, they are categorized  according to whether the 
AF are evaluated with a decision-level  fusion. The scenarios 
with the fusion compute an  average of posterior class 
probabilities estimated from deep models, P( y|xA) and 
P(y|zA), for combining information of non-aligned and 
aligned facial states. On the other hand, the scenarios 
without the fusion use either P( y|xA) or P( y|zA). Now, we 
introduce the details of each scenario. 
Scenarios without information fusion.  The first two 
scenarios, S1 and S2 in Figure 3, do not use any information 
fusion and are designed for comparison. The models in S1 
are trained and evaluated using X without considering 
whether data are AF or N-AF. In other words, FER systems 
in S1 do not apply face alignment in preprocessing steps, 
and they have been commonly  used for the FER-2013
database. In contrast, the models in S2 are trained only 
using ZA. Then, the AF are tested in the aligned state zA to 
maintain a consistency with training, while the N-AF are 
evaluated inevitably in the non-aligned state xNA. 
Scenarios with information fusion 1.  The next two 
scenarios, S3 and S4, fuse information of non-aligned and 
Figure 3: Scenarios to combine information of aligned and 
non-aligned facial states. See Sect. 3.3 and Sect. 4.2 for the 
notations of datasets and deep models, respectively.
Figure 4: Architecture of the discriminative DCN and
evaluation procedure with data augmentation.
1502
aligned states in evaluating the AF. S3 is similar to S2, 
except that it combines P( y|xA) and P( y|zA) for the AF. In S4, 
we consider two types of m odels, which are trained using X
and ZA, respectively. Then, the N-AF are tested with the 
models trained using Xin the same way as S1. The AF are 
tested with information fusion of xA and zA. Here, P( y|xA)
and P( y|zA) in S4 are obtained from the two types of models 
respectively, whereas these two class probabilities in S3 are 
obtained from only the one type of models. 
Scenarios with information fusion 2.  In the last two 
scenarios, S5 and S6, the models are trained using the 
merged set of X+ZA, and we expect them to learn fused 
knowledge of non-aligned and aligned facial states. The 
evaluation procedure in S5 is identical to that in S2, without 
information fusion for the AF. In contrast, S6 applies 
information fusion of combining both facial states to the 
testing phase for the AF as well as to the training phase of 
the models.   
4.2. Deep models and their ensemble 
To achieve ‚Äúdiverse‚Äù FER deci sions, which are necessary 
for a good ensemble, we design nine discriminative DCNs. 
These deep models are trained us ing three different methods 
for input normalization  as well as using three different 
weight initialization  by changing the random seed numbers. 
For input normalization, the pixel values are rescaled by 
applying the min-max normalization (denoted as Raw), the 
illumination variation among faces is reduced by applying 
the illumination normalization ( iNor) based on the isotropic 
diffusion [44], or the global contrast of faces is increased by 
applying the contrast enhancement ( cEnh ) based on 
histogram equalization. Examples of input normalization 
are shown in Figure 7. 
As illustrated in Figure 4, each DCN consists of three 
stages of convolutional and max-pooling layers, followed 
by two fully-connected layers. The convolutional layers use 
32, 32, and 64 filters of size 5x5, 4x4, and 5x5, respectively. 
In the max-pooling layers, ov erlapping-pooling is applied 
with the kernels of size 3x3 an d stride 2, and the size of resulting maps becomes halved. The fully-connected hidden 
and output layers contain 1024 and 7 neurons, respectively, 
where each output neuron co rresponds to ea ch expression 
class of FER. For nonlinearity, Rectified linear unit (ReLU) 
activation is used in all convolutional and penultimate 
layers, while softmax activation is  used in the output layer. 
Following the notation rule in [32], our model can be 
denoted as 1x42x42 - 32C5 - MP3 - 32C4 - MP3 - 64C5 - 
MP3 - 1024N - 7N and we shall use this rule in the rest of 
this paper for brevity. The DC N model is learned using the 
augmented training data obtained from label-preserving 
translation and reflection. At  the evaluation phase, to 
maintain a consistency with the training phase, ten patches 
extracted from each face image are fed to the model, and the 
corresponding ten predictions  are averaged. For other 
training details, refer to Appendix A. 
Depending on the training dataset as described in Sect. 
3.3, we organize our nine DC Ns into a unit of ensemble: 
            (1) 
where k‡¨≤K= {X,ZA,X+ZA} denotes the type of training 
dataset, n‡¨≤N= {Raw,iNor,cEnh } denotes the input 
normalization method, and r‡¨≤R= {1,2,3} denotes the 
random seed number for weight initialization. To combine 
the nine decisions in the ensemble, we apply two widely 
used fusion rules, the majority vote rule and the average rule.
In order to select a final class, the majority vote directly uses 
the predicted labels to compute the largest number of votes, 
while the average rule uses th e posterior class probabilities 
to compute the highest mean class score.
4.3. Experimental result 
Overall performance.  The 7-class FER test accuracies of 
DCNs and their ensembles und er the six experimental 
scenarios are reported in Tab le 2. We first observe that both 
of the best single model and th e best ensemble are achieved 
under the S6 scenario. Second , the scenarios S4 and S6 
show better ensemble performance than other scenarios. 
Furthermore, the ensemble ac curacies of S2 and S3 are Scenario Individual DCN models Ensemble
No.Train
SetTest Set
(AF, N-AF)n = Raw n = iNor n = cEnhMeanMaj.
VoteAve.
Rule r = 1 r = 2 r = 3 r = 1 r = 2 r = 3 r = 1 r = 2 r = 3
S1 X (XA,XNA) 70.38 69.24 69.55 69.63 70.08 69.30 69.18 69.49 69.16 69.56 72.39 72.47
S2 ZA (ZA,XNA) 65.48 64.84 63.95 64.17 64.86 65.09 65.84 64.78 65.00 64.89 66.76 67.46
S3 ZA (ZA+XA,XNA) 63.92 63.97 63.39 63.14 63.81 63.97 64.45 64.20 63.69 63.84 65.98 66.93
S4 X,ZA (ZA+XA,XNA) 70.88 70.74 70.99 70.80 70.88 70.38 71.08 71.22 71.25 70.91 72.97 72.81
S5 X+ZA(ZA,XNA) 70.66 70.41 69.80 69.07 69.16 69.49 69.30 70.66 69.99 69.84 72.33 72.42
S6 X+ZA(ZA+XA,XNA)71.86* 70.88 70.83 70.02 70.33 70.69 70.63 71.75 71.22 70.91 73.31 73.31
Table 2: Classification accuracy (%) of individual deep m odels and their ensemble for each experimental scenario on the 
FER-2013 DB. For a given model or an ensemble (each column), th e highest accuracy indicating the best scenario is written in bold.
The asterisk* denotes the best single DCN. Note that test sets of all scenarios contain ex actly the same face images, indicating a fair 
comparison. The evaluati on strategies for test sets differ in how to deal with alignable faces (AF) and non -alignable faces (N-AF).
1503
lower than those of other scenarios. The aforementioned 
observations will be examined deeply in the followed 
analysis with Figure 5. Notice that there is no much 
difference or clear trend in the ensemble accuracies 
depending on the majority vote and the average rule. It 
indicates that what to combine for ensemble has more 
influence on the final performance rather than how to 
combine decisions. 
It is worth noting that S6 not only yields higher ensemble 
accuracy than S4 but also uses the less number of models. 
Specifically, the ensemble in S6 includes 9 DCNs (trained 
using X + ZA), whereas that in S4 does 18 DCNs (9 trained 
using X and 9 using  ZA). It shows the strengths of the 
scenario S6, i.e., the efficiency  in the evaluation time with 
less models as well as the superiority in FER performance. 
For a later usage in Sect. 5.3, the best ensemble using the 
average rule in S6 is denoted as ES 6.
Accuracies for alignable an d non-alignable faces.  The 
six different scenarios perfor m differently given AF and 
N-AF. Here, we identify the causes for performance 
differences. Figure 5 shows the ensemble accuracies using 
the average rule in each scen ario, separately computed for 
AF and N-AF. We also plot overall test accuracies for total 
faces (TF) as reported in the last column in Table 2. 
Accuracy for AF is computed  as the proportion of correctly 
estimated AF over the whole AF, and the same for N-AF.  
For both AF and N-AF, the en semble performances of S4 
and S6 are improved over th e single best DCN. Also, the 
ensemble of S6 achieves the highest accuracies for both. 
These results sugg est the following. 
/g132Using the merged training dataset of X+ZA is 
beneficial in that the fused knowledge  about non-aligned 
and aligned facial states can be learned. It is verified by 
the improved accuracies for both AF and N-AF in S6. 
/g132For AF, applying the late information fusion to 
combine P( y|xA) estimated from non-aligned states and 
P(y|zA) from aligned states is beneficial. It is supported 
by the improved accuracies for AF in S4 and S6. 
In addition, as shown in the right graph of Figure 5, much lower accuracies for N-AF in S2 and S3 are reported, 
resulting in poor ensemble perf ormances. It is because a 
non-alignable face in non-aligned states ( xNA) for testing is 
fed to the DCNs trained using AF in aligned states ( ZA), 
resulting in an inconsistency of data character istics between 
training and testing. 
Notably, S1 in which face alignment is not applied is a 
common setting for the FER- 2013 database. Compared to 
S1, S4 and S6 show better en semble accuracies for AF. This 
suggests that even if some faces are non-alignable, it is 
better to apply face alignment to other alignable faces and 
then to conduct the improved information fusion. 
5. Mapping from Non-Aligned State to 
Aligned State 
This section describes our mapping function from 
non-aligned states to aligned states. When compared to 
auto-encoding functions, we show the effectiveness of our 
mapping. Then, we present ex perimental results including 
qualitative analysis and FER accuracy using the individual 
mappings as well as the final ensemble performance. 
5.1. Deep models to learn our desired mapping 
The basic idea driving this section is that a mapping from 
non-aligned facial states to aligned ones can represent 
similarity transformations of face alignment. First, we 
examine whether this mapping can be learned using deep 
neural networks (known as universal approximators). 
Specifically, given an alignable face in its non-aligned state 
xA, a deep model is trained to  generate the corresponding 
aligned state zA by minimizing the L2 Euclidean objective 
function. Therefore, XA (as inputs) and ZA (as target outputs) 
containing only AF are used for training the models, as 
depicted in Figure 6.  
Preliminary experiments comp ared deep architectures in 
learning mapping, e.g., multi-layer perceptrons (MLPs) and 
DCNs (see Appendix B for more details). The final DCN 
consists of two stages of convolutional and max-pooling 
layers followed by two fully-connected layers: 1x42x42 - 
64C5 - MP3 - 64C5 - MP3 - 1000N - 1764N. This 
alignment-mapping network (AMN) is denoted as AMN n
where n‡¨≤N= {Raw,iNor,cEnh } is the input 
normalization. For training details, see Appendix C.
Notice that there are no ground-truth outputs for XNA,i.e.,
the aligned states of N-AF. We expect that the mapping 
output for XNA (usually with extreme pose angles and 
occlusions) can be in the closely-aligned state if AMN n
satisfies the following two requirements. First, AMN n can 
rotate, crop, and resize faces in the non-aligned states by 
learning important features for face alignment. Second, 
Figure 5: Classification accuracy (%) of the e nsemble in 
each scenario for alignable faces, non -alignable faces, and 
total faces. The horizontal lines show performance of the 
single best DCN model in S6.
1504
AMN n can be generalized successfully, thus working well 
not only on AF but also on unobserved N-AF. Sect. 5.4 
provides qualitative analysis to confirm these properties 
using the mapping output for XA and for XNA. 
5.2. Comparison with au to-encoding functions 
A popular method for feature extraction is using neural 
networks that learn auto-encoding functions. These 
networks are conventionally trained to produce outputs 
which are identical to inputs, an d the activations of hidden 
neurons are used as features. Here, whether AMN n provides 
better features than auto-encoding networks (AEs) in terms 
of classification is examined. As shown in Figure 6, we 
compare our AMN n, which learns the mapping from XA to 
ZA, with two AEs. One network learns an auto-encoding 
function from XA to XA using raw face images, denoted as 
AE X. The other network similarly learns an auto-encoding 
function from ZA to ZA, denoted as AE Z. For a fair 
comparison, we use the sa me architectures and training 
schemes for AMN n,AE X, and AE Z. 
For qualitative analysis, outputs of the examined 
networks are observed. Then, in terms of FER performance, 
we investigate the effectiveness of AMN n as follows. The 
1000-D hidden activations of penultimate layer in AMN n
are extracted. Then, we use th ese features to train a 3-layer 
MLP classifier of 1000N - 1000N - 1000N - 7N. For 
training details, see Appendix C. This procedure is repeated for each of AMN n,AE X, and AE Z, for comparison. 
5.3. Improving the ensemble for FER 
To improve the best ensemble from the scenario S6 in 
Sect. 4.3, i.e.,ES6, we additionally use decision-level 
information obtained using AMN n. Specifically, the 
features extracted from AMN Raw,AMN iNor, and AMN cEnh
are fed to the 3-layer MLP class ifiers in Sect. 5.2, yielding 
estimated posterior class probabilities. After that, we 
compute the average of th ese class probabilities and the 
output probability from ES6. This improved ensemble is 
denoted as ES6+AMN Raw+AMN iNor+AMN cEnh, used for our 
final FER system. 
The same procedure is conducted with AE X and AE Z,
resulting in ES6+AE X and ES6+AE Z, respectively. To 
ensure that the performance improvement is not just coming 
from adding other information, ES6+AMN Raw is also 
compared with the aforementioned ensembles. 
5.4. Experimental result 
Figure 7 shows examples of the inputs, the corresponding 
target outputs, and the estimated outputs of examined 
networks. Outputs of AE X and AE Z are identical to their 
input images. Notice that outputs of AE Z for unseen N-AF 
Figure 6: Training procedures for AMNs and AEs to learn 
the examined mapping s.
Figure 7: Examples of inputs, target outputs, and 
estimated outputs yielded from AMNs and AEs .Model L2 Euclidean Loss Accuracy (%)
AE X 6.77 55.45
AE Z 4.57 59.52
AMN Raw 18.09 62.94
AMN iNor 16.74 59.99
AMN cEnh 30.08 63.36
Ensemble Type Accuracy (%)
ES6 73.31
ES6+AE X 73.29
ES6+AE Z 73.31
ES6+AMN Raw 73.47
ES6+AMN Raw+AMN iNor+AMN cEnh 73.73
Existing Method Acc. (%)
Zhang et al. [45]A DCN using multi-task loss
with using external databases75.10
Kim et al. [18]An ensemble of 36 DCNs 
in a hierarchical committee72.72
A DCN using cross-entropy loss 70.58
Devries et al. [46] A DCN using multi task loss 67.21
Tang [ 42]A DCN using L2-SVM loss 71.16
A DCN using cross-entropy loss 70.1
Ionescu et al. [47]Multiple kernel learning 
based on SIFT descriptors67.48
Table 3: Performance comparison of AMNs and AEs 
(upper), ensemble accuracies obtained by adding AMNs
or AEs to discriminative DCNs in ES 6(middle ), and 
existing results on the FER- 2013 database (bottom).
1505
are in non-aligned states. In contrast, AMN Raw,AMN iNor,
and AMN cEnh provide closely-aligned  outputs for both the 
AF and the N-AF. These results support that our mapping 
represents geometric transformations of face alignment. 
Interestingly, outputs of AMN cEnh are less blurred and 
clearly show facial expressions while preserving identity 
information. It could be linked to higher accuracy using 
AMN cEnh in Table 3, by giving better features for FER. 
In the upper part of Table 3, we report the L2 Euclidean 
loss and the classification accura cy of extracted hidden 
features for each network. Lower L2 loss values are 
achieved by AE X and AE Z, indicating that they are trained 
well for their purpose of auto-encoding functions. However, 
the classification accuracies using AMN n are higher than 
those of the AEs. It demonstrates that our AMNs provide 
informative features for FER and correct unsuitable 
knowledge particularly by transforming N-AF to be 
closely-aligned.  
In the middle part of Table 3, ensemble performances 
using the examined networks are shown. Combining ES6
and AE decreases ( ES6+AE X) or does not improve 
(ES6+AE Z) the accuracy of ES6. In contrast, ES6+AMN Raw 
and ES6+AMN Raw+AMN iNor+AMN cEnh achieve higher 
accuracies than only using ES6. It indicates that decision 
information from our AMNs can be complementary to that 
of discriminative DCNs in ES6, thus improving FER. 
In the bottom part of Table 3,  the existing results on the 
FER-2013 database are shown for performance comparison. 
Without using external databases, our final system of 
ES6+AMN Raw+AMN iNor+AMN cEnh yields the best FER 
accuracy. Note that the deep model in [45] has used much 
bigger architecture than ours  as well as huge external data 
for its multi-task loss. Compar ed to the ensemble in [18], 
our ensemble includes fewer models having the similar 
architecture, but we achieve b etter results. Compared to 
other single DCNs trained only using the FER-2013, our 
single best DCN in S6 yields a higher accuracy of 71.86 %.  
6. Conclusion 
Towards automatic facial ex pression recognition (FER), 
we present a framework based on an ensemble of deep 
convolutional neural networks. We aim at overcoming a 
specific challenge to FER researchers: there are 
non-alignable faces in real-w orld conditions. To start, 
alignment results on a challenging FER database are 
analyzed. Then, we evalu ate possible scenarios of 
information fusion with discriminative deep models. Here, 
an efficient and effective means to combine information of 
aligned and non-aligned facial states is proposed. In order to 
better deal with non-alignable faces, we introduce 
alignment-mapping networks which learn the operations in face alignment. Using these networks as well as the 
discriminative deep models, the final ensemble achieves 
excellent performance on the examined database collected 
in the wild. We believe that th e proposed approach can be 
applied not only to FER but also to other face analysis 
research using face alignment under unconstrained 
conditions. 
Acknowledgement
We would like to thank Jeffrey White, Kyeongho Lee, and Jisu 
Choi for their discussions and suggestions. This work was 
supported by the Industrial Strategic Technology Development 
Program (10044009, Development of a self-improving 
bidirectional sustainable HRI technol ogy for 95% of  successful 
responses with understanding user‚Äôs complex emotion and 
transactional intent through cont inuous interactions) funded by 
the Ministry of Knowledge Economy (MKE, Korea). 
Appendix 
A. Training details for discriminative DCNs in Sect. 4.2.  We 
use the MatConvNet toolbox [ 48] on NVIDIA GeForce GTX 690 
GPUs. To train the DCNs, stochastic gradient descent is used to 
minimize the cross-entropy objective function with a mini-batch 
size of 200 samples. The dropout probability of the penultimate 
layer is set to 0.5, and weight decay of  0.0001 and momentum of 
0.9 are also applied. The initial learning rate is set to 0.01 and 
reduced by a factor of 2 at every 25 epoch where the number of 
total epochs is 100. Moreover, th e training data were augmented 
by 10 times, through using 5 crops of size 42x42 (1 from resizing 
an original 48x48 face and 4 from  extracting its 4 corners) and 
their horizontal flopping.  
B. Preliminary experiments for architecture selection of 
AMN n in Sect. 5.1. To learn the mapping from the non-aligned 
state to the aligned state, we have explored several candidates: 
MLPs having 2 or 3 fully-connected layers (FC), DCNs having 1 
or 2 convolutional layer s (CONV) ‚Äúwithout‚Äù max -pooling layers 
(MP) followed by 2 or 3 FC, and DCNs having 1 or 2 CONV 
‚Äúwith‚Äù MP followed by 2 or 3 FC. After comparing the L2 
Euclidean validation loss, we have finally selected the best DCN 
described in the main text. We also empir ically find that ‚Äúusing 
CONV and MP‚Äù and ‚Äúincreasing the number of hidden neurons in 
FC (500‚Üí1000)‚Äù are beneficial for learning the desired mapping, 
while increasing the number of FC (2‚Üí3) is not.
C. Training details for AMN n in Sect. 5.1 and for MLP 
classifiers in Sect. 5.2.  We apply the similar training schemes 
introduced in Appendix A. For AMNs, a mini-batch size of 500 
samples and a constant learnin g rate of 0.0001 during a total 1000 
epochs are used. For MLP classifiers, a mini-batch size of 200 is 
used, while the initial learning rate is set to 0.01 and halved at 
every 100 epoch where the number of total epochs is 400.
References 
[1] B. Fasel and J. Luettin. Automatic facial expression analysis: 
a survey. Pattern Recognit ., 36(1):259 ‚Äì275, 2003. 
1506
[2] R. A. Calvo and S. D'Mello. Affect detection: An 
interdisciplinary review of models, methods, and their 
applications.  IEEE Trans. Affect Comput ., 1(1):18 ‚Äì37, 2010. 
[3] E. Sariyanidi, H. Gunes, and A. Cavallaro. Automatic 
analysis of facial affect: A survey of registration, 
representation, and recognition. IEEE Trans. Pattern Anal. 
Mach. Intell ., 37(6):1113 ‚Äì1133, 2015. 
[4] J. Whitehill, G. Littlewort, I. Fasel, M. Bartlett, and J. 
Movellan. Toward practical smile detection. IEEE Trans. 
Pattern Anal. Mach. Intell ., 31(11):2106 ‚Äì2111, 2009. 
[5] A. Dhall, R. Goecke, S. Lucey, and T. Gedeon. Collecting 
large, richly annotated facial-expression databases from 
movies. IEEE MultiMedia Mag ., 19(3):34 ‚Äì41, 2012. 
[6] A. Dhall, R. Goecke, S. Lu cey, and T. Gedeon. Static facial 
expression analysis in tough conditions: Data, evaluation 
protocol and benchmark. In ICCV Workshops , 2011. 
[7] P.-L. Carrier, A. Courville, I. J. Goodfellow, M. Mirza, and 
Y. Bengio. FER-2013 Face Database. Technical report ,
1365, Universit√© de Montr√©al, 2013. 
http://www.kaggle.com/c/challenges-in-representation-learn
ing-facialexpression-recognition-challenge. 
[8] M. Valstar, B. Schuller, K. Smith, F. Eyben, B. Jiang, S.
Bilakhia, and M. Pantic. AVEC 2013: the continuous 
audio/visual emotion and depression recognition challenge. 
In The 3rd ACM International Workshop on Audio/Visual 
Emotion Challenge , 2013. 
[9] A. Dhall, OVR. Murthy, R. Go ecke, J. Joshi, and T. Gedeon. 
Video and image based emotion recognition challenges in 
the wild: Emotiw 2015. In ICMI , 2015. 
[10] M. F. Valstar, B. Jiang, M. Me hu, M. Pantic, and K. Scherer. 
The first facial expression recognition and analysis challenge. 
In FG, 2011. 
[11] I. J. Goodfellow, D. Erhan, P. L. Carrier, and A. Courville. 
Challenges in representation learning: A report on three 
machine learning contests.  Neural Networks , 64:59 ‚Äì63, 
2015.
[12] S. Rifai, Y. Bengio, A. Courville, P. Vincent, and M. Mirza. 
Disentangling factors of variation for facial expression 
recognition. In ECCV , 2012. 
[13] S. Reed, K. Sohn, Y. Zhang, and H. Lee. Learning to 
disentangle factors of variation with manifold interaction. In 
ICML , 2014 
[14] S. Reed, H. Lee, D. Anguelov, C. Szegedy, D. Erhan, and A. 
Rabinovich. Training deep neural networks on noisy labels 
with bootstrapping. In ICLR Workshops , 2015. 
[15] M. Liu, S. Li, S. Shan, and X. Chen. Au-inspired deep 
networks for facial expression feature learning. 
Neurocomputing , 159:126 ‚Äì136, 2015. 
[16] S. E. Kahou, et al . Emonets: Multimodal deep learning 
approaches for emotion recognition in video. Journal on 
Multimodal User Interfaces , 2015. (online) 
[17] Z. Yu and C. Zhang. Imag e based static facial expression 
recognition with multiple deep network learning. In ICMI ,
2015.
[18] B.-K. Kim, J. Roh, S.-Y. Dong, and S.-Y. Lee. Hierarchical 
committee of deep convolutional neural networks for robust 
facial expression recognition. Journal on Multimodal User 
Interfaces , 2016. (online)
[19] J. Alabort-i-Medina and S. Zafeiriou. Bayesian active 
appearance models. In CVPR , 2014.[20] E. Antonakos, J. Alabort-i-Medina, G. Tzimiropoulos, and S. 
P. Zafeiriou. Feature-Based Lucas ‚ÄìKanade and Active 
Appearance Models. IEEE Trans. Image Process. ,
24(9):2617 ‚Äì2632, 2015. 
[21] X. Zhu and D. Ramanan. Face detection, pose estimation, 
and landmark localization in the wild. In CVPR , 2012. 
[22] A. Asthana, S. Zafeiriou, S.  Cheng, and M. Pantic. Robust 
discriminative response map fitting with constrained local 
models. In CVPR , 2013. 
[23] J. Alabort-i-Medina and S. Zafeiriou. Unifying holistic and 
parts-based deformable model fitting. In CVPR , 2015. 
[24] E. Antonakos, J. Alabort-i-Medina, and S. Zafeiriou. Active 
pictorial structures. In CVPR , 2015. 
[25] X. Xiong and F. de la Torre. Supervised descent method and 
its applications to face alignment. In CVPR , 2013. 
[26] S. Zhu, C. Li, C. Change Loy, and X. Tang. Face alignment 
by coarse-to-fine shape searching. In CVPR , 2015. 
[27] L. K. Hansen and P. Salamon. Neural network ensembles. 
IEEE Trans. Pattern Anal. Mach. Intell ., 12(10):993 ‚Äì1001,
1990.
[28] A. J. C. Sharkey. On combining artificial neural nets. 
Connection Science , 8(3-4):299 ‚Äì314, 1996.
[29] G. Giacinto and F. Roli. Design of effective neural network 
ensembles for image classification purposes. Image and 
Vision Computing , 19(9):699 ‚Äì707, 2001.
[30] C. A. Shipp and L. I. Kuncheva. Relationships between 
combination methods and measures of diversity in 
combining classifiers. Information Fusion , 3(2):135 ‚Äì148,
2002. 
[31] L. I. Kuncheva. Diversity in multiple classifier systems.
Information Fusion , 6(1):3 ‚Äì4 (Editorial), 2005. 
[32] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column 
deep neural networks for image classification. In CVPR ,
2012.
[33] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet 
classification with deep convolutional neural networks. In 
NIPS , 2012. 
[34] K. Simonyan and A. Zisserman. Very deep convolutional 
networks for large-scale image recognition. In ICLR , 2015. 
[35] M. Kan, S. Shan, H. Chang, and X. Chen. Stacked 
progressive auto-encoders (spae) for face recognition across 
poses. In CVPR , 2014. 
[36] Z. Zhu, P. Luo, X. Wang, and X. Tang. Deep learning 
identity-preserving face space. In ICCV , 2013. 
[37] Z. Zhu, P. Luo, X. Wang, and X. Tang. Multi-view 
perceptron: a deep model for learning face identity and view 
representations. In NIPS , 2014. 
[38] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim. 
Rotating your face using multi-task deep neural network. In 
CVPR , 2015. 
[39] T. Hassner, S. Harel, E. Paz, and R. Enbar. Effective face 
frontalization in unconstrained images. In CVPR , 2015. 
[40] C. Sagonas, Y. Panagakis, S.  Zafeiriou, and M. Pantic. 
Robust Statistical Face Frontalization. In ICCV , 2015. 
[41] A. Ruiz, J. Van de Weijer, and X. Binefa. From Emotions to 
Action Units with Hidden and Semi-Hidden-Task Learning. 
In ICCV , 2015. 
[42] Y. Tang. Deep Learning with Linear Support Vector 
Machines, In ICML Workshops , 2013. 
[43] F. de la Torre, et al. Intraface. In FG, 2015. 
1507
[44] R. Gross and V. Brajovic. An image preprocessing algorithm 
for illumination invariant face recognition. In The 4th 
International Conference on Audio- and Video-Based 
Biometric Person Authentication , 2003. 
[45] Z. Zhang, P. Luo, C. C. Loy, and X. Tang. Learning Social 
Relation Traits from Face Images. In ICCV , 2015. 
[46] T. Devries, K. Biswaranjan, and G. W. Taylor. Multi-task 
learning of facial landmarks and expression. In 2014
Canadian Conference on Computer and Robot Vision , 2014. 
[47] R. T. Ionescu, M. Popescu, and C. Grozea. Local learning to 
improve bag of visual words model for facial expression 
recognition. In ICML Workshops , 2013. 
[48] A. Vedaldi and K. Lenc. Matconvnet ‚Äì convolutional neural 
networks for matlab. In The 23rd ACM Conference on 
Multimedia , 2015. 
1508
"
https://ieeexplore.ieee.org/document/6131215,"Human-computer interaction using emotion recognition from facial expression
F. ABDAT, C. MAAOUI and A. PRUSKI
Laboratoire d‚ÄôAutomatique humaine et de Sciences Comportementales
Universite de metz
Metz, France
Email: abdat, maaoui, pruski@univ-metz.fr
Abstract ‚ÄîThis paper describes emotion recognition system
based on facial expression. A fully automatic facial expression
recognition system is based on three steps: face detection, facial
characteristic extraction and facial expression classiÔ¨Åcation. We
have developed an anthropometric model to detect facial fea-
ture points combined to Shi&Thomasi method. The variations
of 21 distances which describe the facial features deformations
from the neutral face, were used to coding the facial expression.
ClassiÔ¨Åcation step is based on SVM method (Support Vector
Machine). Experimental results demonstrate that the proposed
approach is an effective method to recognize emotions through
facial expression with an emotion recognition rate more than
90% in real time. This approach is used to control music player
based on the variation of the emotional state of the computer
user.
Keywords -Emotion recognition, facial expression, anthropo-
metric model, SVM, music controler.
I. I NTRODUCTION
Today, one of the interesting challenges in the community of
human-computer interaction is how to make computers be more
human-like for intelligent user interfaces. In several experiments,
Reeves and Nass [15] show that humans impose their interpersonal
behavioral patterns onto their computers. Thus, the design of
recent human-computer interfaces should reÔ¨Çect this observation in
order to facilitate more natural and more human-like interaction.
Emotion, one of the user affect, has been recognized as one of
the most important ways of people to communicate with each
other. Given the importance and potential of the emotions, affective
interfaces using the emotion of the human user are gradually
more desirable in intelligent user interfaces such as human-robot
interactions [12]. For such, an affective user interface and in order
to make use of user emotions, the emotional state of the human
user should be recognized or sensed in many ways from diverse
modality such as facial expression, speech, and gesture. Facial
expression constitutes 55 % of the effect of communicated message
[13] and is hence a major modality in human communication.
In this paper, we propose an automatic method for real time
emotion recognition using facial expression using a new anthropo-
metric model for facial feature points detection. The remainder of
this paper is organized as follows: Ô¨Årst, we present the facial feature
points localization. Our facial expression recognition system is
presented in section 3, where we specify the feature extraction
method and the recognition approach. In section 4, we present theobtained experimental results. Finally, conclusion and future works
are presented in section 5.
II. F ACIAL FEATURE POINTS LOCALIZATION
Facial feature points are generally referred to as facial salient
points such as the corners of the eyes, corners of the eyebrows,
corners and outer mid points of the lips, corners of the nostrils, tip
of the nose, and the edge of the face. Detection of facial feature
points is often the Ô¨Årst step in computer vision applications such
as face identiÔ¨Åcation, facial expression recognition, face tracking
and lip reading. Currently, however, this step is usually carried out
by manually labeling the required set of points.
Several methods for facial feature point detection could be clas-
siÔ¨Åed in two categories: texture-based and shape-based methods.
Texture-based methods model local texture around a given feature
point [19], for example the pixel values in a small region around a
mouth corner. Shape-based methods regard all facial feature points
as a shape [20], which is learned from a set of labeled faces, and
try to Ô¨Ånd the proper shape for any unknown face.
We propose in this paper a simple and fast method to detect
automatically facial feature points, based on an anthropometric
model and on Shi&Thomasi method for more accuracy.
A. Face detection
Face detection is the Ô¨Årst step in our facial expression recog-
nition system, to localize the face in the image. This step allows
an automatical labeling for facial feature points in a face image.
We use a real-time face detector proposed in [17] available in
OpenCV library [5] which represents an adapted version of the
original Viola-Jones face detector(Ô¨Ågure 1-a).
The next step in the automatic facial points detection is to
determine the coarse position for each point. To achieve this, we
develop a fully automatic method using an anthropometric model.
B. Anthropometric model for facial feature points localiza-
tion
Anthropometry is a biological science that deals with the mea-
surement of the human body and its different parts. Data obtained
from anthropometric measurement informs a range of enterprises
that depend on knowledge of the distribution of measurements
across human populations. After carefully performing anthropo-
metric measurement on FEEDTUM database [18], we have been
able to build an anthropometric model of human face that can
be used in localizing facial feature points from face images. The
landmarks points that have been used in our face anthropometric
model for facial feature localization are represented in Ô¨Ågure 1-e.
2011 UKSim 5th  European Symposium on Computer Modeling and Simulation
978-0-7695-4619-3/11 $26.00 ¬© 2011 IEEE
DOI 10.1109/EMS.2011.20196

Figure 1. Facial feature points detection outline
It has been observed from the statistics of proportion evolved
during our initial observation that, location of these points (P1 to
P38) can be obtained from:
‚Ä¢The distance Dmeasured between eyes axis EA and mouth
axisMA (Ô¨Ågure 5 -c);
‚Ä¢Face symmetry axis SAas reference for xposition of points.
Our proposed anthropometric face model of facial feature points
localization has been developed from these proportional constants
(Table I) using distance between eyes axis, mouth axis, symmetry
axis and the face box center Y C as the principle parameter of
measurement.
To realize the anthropometric model, we have used the following
stages:
1)Main axis localization:
a)Eyes axis localization using gradient projection;
b)Mouth axis localization using color information;
c)Symmetry axis localization using gray image projec-
tion;
2)Facial feature points localization using the proportional
position (table I);
3)Facial feature points detection using the Shi&Thomasi
method for more accuracy;
Each step will be detailed in the next part.Point X position Y position
P1 SA-0.91*D YC-0.91*D
P2 SA-0.58*D YC-1.17*D
P3 SA-0.26*D YC-1.3*D
P4 SA+0.26*D YC-1.3*D
P5 SA+0.58*D YC-1.17*D
P6 SA+0.91*D YC-0.91*D
P7 SA+1.04*D YC
P8 SA+0.71*D YC+0.91*D
P9 SA+0.52*D YC+1.17*D
P10 SA+0.13*D YC+1.30*D
P11 SA-0.13*D YC+1.3*D
P12 SA-0.52*D YC+1.17*D
P13 SA-0.71*D YC+0.91*D
P14 SA-1.04*D YC
P15 SA-0.06*D YC-0.26*D
P16 SA+0.06*D YC-0.26*D
P17 SA-0.78*D YC-0.52*D
P18 SA-0.58*D YC-0.58*D
P19 SA-0.26*D YC-0.52*DPoint X position Y position
P20 SA+0.26*D YC-0.52*D
P21 SA+0.58*D YC-0.58*D
P22 SA+0.78*D YC-0.52*D
P23 SA-0.26*D YC+0.32*D
P24 SA+0.26*D YC+0.32*D
P25 SA-0.65*D MA
P26 SA-0.32*D MA-0.1*D
P27 SA MA-0.15*D
P28 SA+0.32*D MA-0.1*D
P29 SA+0.45*D MA
P30 SA+0.32*D MA+0.1*D
P31 SA+0.19*D MA+0.13*D
P32 SA MA+0.15*D
P33 SA-0.19*D MA+0.13*D
P34 SA-0.32*D MA+0.1*D
P35 SA-0.74*D YC-0.26*D
P36 SA-0.58*D YC-0.39*D
P37 SA+0.58*D YC-0.39*D
P38 SA+0.74*D YC-0.26*D
TAB. I
PROPORTION OF FACIAL FEATURE POINTS POSITIONS MEASURED FROM
SUBJECTS OF DIFFERENT GEOGRAPHICAL TERRITORIES
C. Main axis localization
The key step of our anthropometric model realization is facial
features axis localization. In order to correctly localize facial axis,
a constraint must be considered that the face must be frontal to
the camera. Figure 5-c shows the three main axis which passe by
facial features: two horizontal axis for mouth (MA) and eyes (EA)
and vertical axis passed by nose which give the symmetry of the
face (SA). Y C is the face box center.
1) Eyes axis localization: It is determined by the maximum
of the projection curve which has a high gradient. First, we
calculate the gradient of the image I (corresponds to the face
rectangle extracted by the Viola-Jones detector):
‚àáIx=Œ¥I
Œ¥x/vectori (1)
‚àáIxcorresponds to the differences in the x(column) direction.
The spacing between points in each direction is assumed to be
one. Computing the absolute gradient value in each line is given
by:
HIx(x) =n/summationdisplay
y=1‚àáIx(x, y) (2)
Then, we Ô¨Ånd the maximum value which corresponds to the
line contains eyes (Ô¨Ågure 1-b). This line corresponds to many
transitions: skin to sclera, sclera to iris, iris to pupil and the same
thing for the other side (high gradient) [1].
2) Mouth axis localization: To locate the mouth axis, we
Ô¨Årst deÔ¨Åne a Region Of Interest (ROI) of the mouth to be the
horizontal strip whose top is at 0.67XR from the face bounding
box top and has a width equal to 0.25XR. This strip is located
around the median of the bounding box of the face with a width
of0.1XR (Ô¨Ågure 1-c), where R is the side face box.
197
Figure 2. Facial feature points localization
Then, we use the color information to locate the accurate position
of mouth axis. Given that the natural color of the lips is red, the
ROI of the mouth is transformed into the HSV color space (Hue,
Saturation, Value), in order to separate color from intensity, and
then into red domain in order to segment mouth area from the
rest of the strip image [16]. Image in red domain is obtained from
the hue and saturation components of the input frame by applying
some thresholds:
‚Ä¢120< H < 200
‚Ä¢S >70
The center of this area is calculated as:
Cx=M10/M00, C y=M01/M00 (3)
where,
M00=/summationdisplay
x,y‚ààROIROI(x, y) (4)
M10=/summationdisplay
x,y‚ààROIxROI (x, y) (5)
M01=/summationdisplay
x,y‚ààROIyROI (x, y) (6)
M00is the Ô¨Årst moment. M10andM01are the second moments
of the region.
The localization of Mouth axis corresponds to CY.
3) Symmetry axis localization: It is a vertical line which
devides the frontal face in two equal sides. In other words, it is the
line passed by the nose. To locate the symmetry axis, we Ô¨Årst deÔ¨Åne
a ROI of the nose. Since knowing the location of eyes axis (EA)
and mouth axis (MA), we deÔ¨Åne the nose region to be the vertical
strip whose top is the eyes axis and has a height equal to D. This
strip is located around the median of the bounding box of the face
with a width of 10% of the face windows width. Analysis of the
gray level vertical projection of the ROI of the nose, shows that
the maximum of the projection curves corresponds to symmetry
axis (Ô¨Ågure 1-d).
Experimental results show that the extraction of the facial
feature points using anthropometric model gives a good location
independently to skin color and illumination changes (Ô¨Ågure 2).
D. Detection of facial feature points using the Shi&Thomasi
method
Once the anthropometric model is built, we look to ensure the
localization of points using the Shi&Thomasi method which gives
the points with strong variation in a neighborhood N. This step is
necessary, to robustly track the facial feature points [1].
Figure 3 -a shows the rectangles around the points detected using
the anthropometric model. We will apply Shi&Thomasi method
only inside of the rectangles, this method is available in OpenCv
library [5].
Figures 3-b,c,d show results for feature points detection with the
combination of anthropometric based method and Shi&Thomasi
Red small box: results from the anthropometric model
White big box: results from the combination of the anthropometric
model and Shi&Thomasi method
Figure 3. The inÔ¨Çuence of the window size on the quality of detection.
method. For the Ô¨Årst image, we have used a block of 5X5; for the
second image, we have used a block of 15X15; for the last one,
we have used a block of 8X8.
There is a great inÔ¨Çuence of the window‚Äôs size on the quality of
detection. If the size is too small (like: 5X5), there is no change.
If the size is big (like: 15X15),the result of the detection change,
where the position of other points are distorted.
For the last size, the detection of eyebrows points are improved,
for this reason, we must must change the window‚Äôs size depending
to the position of the facial feature point. We have chosen different
window‚Äôs sizes for each facial features.
The facial feature points detected in the Ô¨Årst image of the
sequence will be tracked using pyramidal Lucas-Kanade algorithm
[4] which suppose that the brightness of every point of a moving
or static object does not change in time.
Figure 4 shows an example of facial feature points detection
and tracking. Where, we have a good localization of facial feature
points in the Ô¨Årst image using our developed method. Also, we
have a good localization of facial feature points in the remaining
of the sequence with the tracking step.
III. F ACIAL EXPRESSION RECOGNITION
Our approach uses a feature based representation of facial data
for SVM classiÔ¨Åer. It classiÔ¨Åes single images taken from an image
sequence with respect to six basic emotions of Ekman [7] happy,
fear, disgust, anger, sadness, surprise and neutral state.
Our work is based on the facial features deformation compared
to the neutral state.
A. Coding
For Facial Action Coding System (FACS) [8], there are twenty
two facial muscles which are closely relevant to human expressions.
In fact, the human facial expressions originate from the movements
of facial muscles beneath the skin. Thus, we represent each facial
muscle by a pair of key points [9], namely dynamic point and Ô¨Åxed
point. As shown in Ô¨Ågure 5-a, the dynamic points can be moved
during an expression, while Ô¨Ågure 5-b shows the Ô¨Åxed points which
can not be moved during a facial expression (face edge, nose root
and outer corners of the eyes).
Further, each facial muscle is represented by a distance (Ô¨Ågure
5-d), as:
a* Eyebrows motions are described by the distances from
D1 to D7,
b* Eyes motions are described by the distances D8 and D9,
c* Nose motions are described by the distances D10 and
D11,
d* Mouth motions are described by the distances from D12
to D21.
198
image 1 image 24 image 51 image 52 image 54 image 56 image 61
Figure 4. Facial features points detection and tracking in video sequence.
(a) (b) (c) (d)
Figure 5. a:Dynamic points, b:Static Points,c:Principal axis, d:facial
distances
X-Position: frame number Y-Position: distance (pixel)
Figure 6. Distances variation from video sequence for different emotions
Figure 6 shows a sample of the characteristics distances of
FEEDTUM database[18] for three emotions: happy, fear and dis-
gust. This Ô¨Ågure shows the distances evolution during a video
sequence that begins with a neutral state.
For example, the distances variation of D20characterizes the
mouth motions. This distance presents a signiÔ¨Åcant variation in the
case of happy where the lip‚Äôs corners withdrew back. This variation
is less important in the case of disgust and fear.
The analysis of these curves, shows that a facial expression can
be regarded as a combination of different distances variations from
neutral state.The used method to encode a facial expression takes into
consideration all distances Divariations of each muscle during the
sequence. If DT= (d1, d2, ..., d i, ..., d 21)is the vector parameters
extracted from a video sequence at the moment T.‚àÜDis the
distance variation from the Ô¨Årst image (a neutral expression),
whether:
‚àÜD= (D1
D01, ...,Di
D0i, ...,D21
D021) (7)
Where D0iis theithdistance of the neutral state, with i‚àà[1,21].
There are several works in the Ô¨Åeld of facial expression recog-
nition using feature points. The added value of our work is the
modelization of muscle contraction using the variation of muscle
distances relative to the neutral state. The previous studies on facial
feature modelization are based on:
1)points displacements [14],
2)facial points coordinates [3],
3)or distances between points but it is based on the deformation
of the facial contour or dimension [2],
4)deformation of the shape from the neutral state but not on
the contraction of facial muscles [10].
B. ClassiÔ¨Åcation
After the extraction of the necessary information from the facial
expression, we have trained a statistical classiÔ¨Åer Support vector
machine SVM. This method has the following advantages :
‚Ä¢A low parameter number to set,
‚Ä¢Very fast training,
‚Ä¢A low samples number is sufÔ¨Åcient for supports vectors
determination allowing discrimination between classes,
‚Ä¢Treatment of linear or nonlinear problems according to the
kernel function.
IV. R ESULTS AND DISCUSSION
A. Recognition from video sequence
For the evaluation of our work, we have used two databases
Cohn-Kanade [11] and FEEDTUM [18].
Figure 7 describes our facial expression recognition system as:
1)Figure 7(A) shows facial features extraction step which has
as input a video sequence and gives as output the distances
vector,
2)Figure 7(B) shows training step which has as input a series
of distances vector and gives as output the SVM model,
3)Figure 7(C) shows classiÔ¨Åcation step which has as input a
distances vector and gives as output the recognized expres-
sion using the model built during the learning stage.
For our system, we use libSVM, an open source SVM package
[6]. We have used the radial base function (RBF) as kernel which
is given by: K(x, xi) =exp(‚àí/bardblx‚àíxi/bardbl
œÉ).We perform k-fold cross-
validation, in order to compute the accuracy of each SVM. Every
facial expression is represented by a set of distances (Ô¨Ågure 5-d).
To evaluate the recognition rate, we have tested several distances
combinations. These distances are grouped in such a way that each
199
Figure 7. Facial expression recognition system
Combination Cohn-Kanade (%) FEEDTUM (%)
Eyebrows: 7 distances 68.05 78.33
Eyes: 2 distances 26.01 52.66
Nose: 2 distances 46.66 40.33
Mouth: 10 distances 83.05 59.75
21 distances 89.44 95.32
Variation of 21 distances 95.83 97.5
Table II
EMOTION RECOGNITION RATE
facial feature is represented separately (eyebrows, eyes, nose and
mouth).
The Ô¨Årst four rows of the table II represent emotions recognition
rate for different face parts. These results show the importance
of each part of the face. However, the facial expression is well
described by the mouth in Cohn-Kanade database and the eyebrows
in a the FEEDTUM database, so we can not conclude that this part
is more descriptive than another, which leads to use all parts of the
face to avoid any ambiguity.
We use all different parts of the face in order to have more
information on facial expression. The Ô¨Åfth row of table II shows
combinations results for each used database. We note that the best
recognition rates are achieved by merging more information (21
distances: Eyebrows+eyes+nose+mouth).
The last row of table II shows emotion recognition rate using
variations distance (equation 7). We note that using the report
distance, we get a better recognition rate because the parameters
will be more homogeneous. The variation of the distance using the
report is robust with respect to the change of scale, as in the case
of camera zoom.
Tables III and IV represent confusion matrix of different emo-
tions with Kohn-Kanade and FEEDTUM database respectively.
This matrix shows the effectiveness of classiÔ¨Åcation method with
the SVM. For our classiÔ¨Åer, we achieve the rate of 90% for all
emotions.
Kernel choice is among the most important customizations that
can be made when adjusting an SVM classiÔ¨Åer to a particular
application domain. We experimented with a Gaussian radial basis
function (RBF) and linear kernels to compared recognition perfor-
mance. Table V shows that the classiÔ¨Åcation based on the RBF
kernel is best than the classiÔ¨Åcation based on the linear kernel.1 2 3 4 5 6 7
1 98.33% 0% 0% 0% 0% 0% 1.66%
2 0% 100% 0% 0% 0% 0% 0%
3 0% 0% 98.33% 0% 0% 0% 1.66%
4 0% 0% 0% 100% 0% 0% 0%
5 0% 0% 0% 0% 91.66% 0% 8.33%
6 0% 0% 0% 0% 0% 100% 0%
7 0% 0% 0% 0% 5% 0% 95%
Table III
EMOTION CONFUSION MATRIX FOR KOHN -KANADE DATABASE .
1:Happy 2:Fear 3:Disgust 4:Anger 5:Sadness 6:Surprise 7:Neutral
1 2 3 4 5 6 7
1 98.5% 0% 0% 0.5% 0% 0% 1%
2 0% 100% 0% 0% 0% 0% 0%
3 0% 0% 99% 0% 1% 0% 0%
4 0% 0 % 0 % 100% 0% 0% 0%
5 0 % 0 % 0 % 0% 97% 0 % 3%
6 0% 0% 0% 0% 0 % 100% 0%
7 0% 0% 0% 0% 2.5% 0% 97.5%
Table IV
EMOTION CONFUSION MATRIX FOR FEEDTUM DATABASE .
B. Facial expression recognition to control music player
Emotion recognition from facial expressions is a very powerful
mean for human-machine-interaction application due to the fact
that the use of the camera allows non-intrusive application. In
this paper, we present our controller of music player based on the
emotion of computer user. The person is in front of the embedded
camera of the computer, recognition of emotions in real time is
performed with our approach based on the variation of distance
and SVM classiÔ¨Åer. The aim of this application is to change the
music depending on the change of user‚Äôs emotion. The control of
music depend to emotion state, for example if the person is sad,
the music is generated in such a way to switch the person to a
positive emotional state. This kind of application can be used by
disabilities person to order their need.
The method has achieved average recognition rates of 95%. The
resulting confusion matrix is shown in table VI.
During the real time test, the system achieved an average frame
rate of 25 frame per second for 320*240 images on a PC with
3.4 GHz Intel Pentium. Testing results have shown that the system
has a practical range of working distances ( from user to camera),
and is robust against variation in lighting and backgrounds. The
table VII presents the elapsed time for each step in our system.
The elapsed time for points detection with the combined method
depends on the windows size. For windows size of 8X8, the elapsed
time is ‚âÉ0.3s.
linear RBF
FEEDTUM database 94.25% 97.5%
Kohn-Kanade database 94.66% 95.83%
Table V
COMPARISON OF RECOGNITION RATE BETWEEN TWO KERNELS
200
happy disgust neutral
happy 90.47% 9.53% 0%
disgust 0% 100% 0%
neutral 0% 0% 100%
Table VI
EMOTION CONFUSION MATRIX IN REAL TIME (INDEPENDENT CASE ).
Operation Time (S)
Face localization ‚âÉ0.28
Point detection using anthropometric model ‚âÉ0.11
Point detection using the combination method ‚âÉ0.30
Tracking and classiÔ¨Åcation ‚âÉ0.031
Table VII
ELAPSED TIME FOR EACH STEP .
V. C ONCLUSION
In this paper we have presented an automatic approach to
emotion recognition based on facial expression analysis.To detect
the face in the image, we have used the face detector of Viola-
Jones which is fast and robust to illumination condition. For
feature points extraction, we have developed an anthropometric
model which suppose that the position of facial feature points
are proportional to the vertical distance between eyes and mouth.
In order to ensure detection robustness, we used the algorithm of
Shi&Thomasi to extract feature points in an narrow window around
the points found with the anthropometric model. The combination
of both method gives good results, when tested on images from
the Cohn-Kanade database and FEEDTUM datbase, under various
illumination. The tracking of the detected points have been realized
with Lucas-Kanade algorithm. Variations distance vector was used
as a descriptor of the facial expression. This vector is the input
of SVM classiÔ¨Åer. We have used our facial expression recognition
system to control a music player. Emotion recognition rates about
95% were achieved in real time.
In future, we intend to develop a multimodal system using
facial expression and physiological signals, in order to improve
the emotions recognition results.
R√âF√âRENCES
[1]F. Abdat, C. Maaoui, and A. Pruski. Real facial feature points
tracking with pyramidal lucas-kanade algorithm. IEEE RO-
MAN08, The 17th International Symposium on Robot and
Human Interactive Communication, Germany , 2008.
[2]Koen van de Sande Roberto Valenti Aitor Azcarate, Felix Ha-
geloh. Automatic facial emotion recognition, 2005.
[3]J. Bailonson, E. Pontikakisb, I. Maussc, J. Grossd, M. Ja-
bone, C. Hutchersond, C. Nassa, and O. Johnf. Real-time
classiÔ¨Åcation of evoked emotions using facial feature tracking
and physiological responses. International Human Computer
Studies , 66 :303‚Äì317, 2008.
[4]J.Y . Bouguet. Pyramidal implementation of the lucas kanade
feature tracker. Intel Corporation , Microprocessor Research
Labs, 2000.[5]G. Bradski, T. Darrell, I. Essa, J. Malik, P. Perona, S. Sclaroff,
and C. Tomasi. http ://sourceforge.net/projects/opencvlibrary/,
2006.
[6]C.C. Chang and C.J. Lin. Libsvm : library for support vector
machines. 2001.
[7]P. Ekman. Emotion in the human face. Cambridge University
Press , 1982.
[8]P. Ekman and W. Friesen. Facial action coding system : A
technique for the measurement of facial movement palo alto
calif. Consulting psychologists press , 1978.
[9]S. S. Ge, C. Wang, and C. C. Hang. Facial expression
imitation in human robot interaction. IEEE RO-MAN08,
The 17th International Symposium on Robot and Human
Interactive Communication, Germany , 2008.
[10] Z. Hammal. Segmentation des traits du visages, analyse et
reconnaissance des expressions faciales par le mod√®le de
croyance transf√©rable . PhD thesis, Universit√© Joseph Fourier
de Grenoble, 2006.
[11] T. Kanade, J. F. Cohn, and Y . Tian. Comprehensive database
for facial expression analysis. Fourth IEEE International
Conference on Automatic Face and Gesture Recognition
Grenoble France , FG‚Äô00 :46‚Äì53, 2000.
[12] H.R. Kim, K.W. Lee, and D.S. Kwon. Emotional interac-
tion model for a service robot. Proceedings of the IEEE
International Workshop on Robots and Human Interactive
Communication , pages 672‚Äì678, 2005.
[13] A. Mehrabian. Communication without words. Psychology
Today , 2.4 :53‚Äì56, 1968.
[14] I. Cohen Y . Sun T. Gevers N. Sebe, M.S. Lew and T.S.
Huang. Authentic facial expression analysis. Proc. IEEE
Int‚Äôl Conf.Automatic Face and Gesture Recognition (AFGR) ,
2004.
[15] B. Reeves and C. Nass. The media equation. Cambridge
University Press , 1996.
[16] S. Shamik, Q. Gang, and Sakti P. Segmentation and histogram
generation using the hsv color space for image retrieval. icip,
pages 7803‚Äì7622, 2002.
[17] P. Viola and M. Jones. Robust real-time object detection. 2nd
international workshop on statistical and computational theo-
ries of vision - modeling, learning, computing, and sampling
vancouver, canada , 2001.
[18] F. Wallhoff. Feedtum : Facial expressions and emotion
database, 2005.
[19] Z. Xin, X. Yanjun, and D. Limin. Locating facial features
with color information. IEEE International Conference on
Signal Processing , 2 :889‚Äì892, 1998.
[20] P.W. Yuille, A.L.and Hallinan and D.S. Cohen. Feature ex-
traction from faces using deformable templates. International
Journal of Computer Vision , 8 :99‚Äì111, 1992.
201
"
http://ieeexplore.ieee.org/document/8373843/,"Identity-Adaptive Facial Expression Recognition Through Expression Regeneration
Using Conditional Generative Adversarial Networks
Huiyuan Y ang, Zheng Zhang and Lijun Yin
Department of Computer Science
State University of New York at Binghamton, USA
{hyang51, zzhang27 }@binghamton.edu; lijun@cs.binghamton.edu
Abstract ‚ÄîSubject variation is a challenging issue for fa-
cial expression recognition, especially when handling unseen
subjects with small-scale lableled facial expression databases.
Although transfer learning has been widely used to tackle the
problem, the performance degrades on new data. In this paper,
we present a novel approach (so-called IA-gen ) to alleviate the
issue of subject variations by regenerating expressions from any
input facial images. First of all, we train conditional generative
models to generate six prototypic facial expressions from any
given query face image while keeping the identity related
information unchanged. Generative Adversarial Networks are
employed to train the conditional generative models, and each
of them is designed to generate one of the prototypic facial
expression images. Second, a regular CNN (FER-Net) is Ô¨Åne-
tuned for expression classiÔ¨Åcation. After the corresponding
prototypic facial expressions are regenerated from each facial
image, we output the last FC layer of FER-Net as features for
both the input image and the generated images. Based on the
minimum distance between the input image and the generated
expression images in the feature space, the input image is
classiÔ¨Åed as one of the prototypic expressions consequently.
Our proposed method can not only alleviate the inÔ¨Çuence of
inter-subject variations, but will also be Ô¨Çexible enough to
integrate with any other FER CNNs for person-independent
facial expression recognition. Our method has been evaluated
on CK+, Oulu-CASIA, BU-3DFE and BU-4DFE databases,
and the results demonstrate the effectiveness of our proposed
method.
Keywords -FER; GAN; Identity-adaptive; CNN;
I. I NTRODUCTION
In the past decades, research on automatic facial expres-
sion recognition has been conducted through classifying
classic prototypic facial expressions ( i.e., anger, disgust, fear,
happiness, sadness, surprise) from static images or dynamic
image sequences. Although signiÔ¨Åcant progresses have been
made in recent years, the challenge still remains in real-
world scenarios with respect to various poses, illumination,
occlusion, and in particular, the identity related expression
variations. The inter-subject variations come from a variety
of identity components, including age, gender, race and
person-speciÔ¨Åc characteristics [1]. Compared with the VGG
face dataset [2] and Imagenet [3], the current public expres-
sion datasets are limited in size, making it more difÔ¨Åcult to
deal with identity-related variations.
Figure 1. The upper part is the illustration for features of different subjects
with a variety of expressions in the feature space. The lower part shows
the generated identity-adaptive sub-space for each subject.
Recently, several approaches have been proposed by fo-
cusing on improving performance on person-independent
facial expression recognition [4] [1] [5]. Transfer learning is
one of the mostly used methods, which Ô¨Åne-tunes a network
that has been pre-trained on a large dataset i.e. VGG [2],
FaceNet [6] and FER-2013 [7] , to a relatively small facial
expression dataset. Other methods include: using a deeper
network with more training data [8]; learning a person-
speciÔ¨Åc model [9] and adding extra constraints for identity-
related variations in FER task. Although those efforts have
alleviated the problem to a certain degree, the challenge still
remains unsolved.
For facial expression recognition task, the same expres-
sions are expected to have a smaller distance between each
other in the feature space than others of different expres-
2942018 13th IEEE International Conference on Automatic Face & Gesture Recognition
978-1-5386-2335-0/18/$31.00 ¬©2018 IEEE
DOI 10.1109/FG.2018.00050

Figure 2. Framework of our proposed method ( IA-gen )
sions. As illustrated in Fig. 1, I(happiness,A )andI(sad,A )
are the same subject A showing different expressions,
whileI(happiness,A )andI(happiness,B )are different subjects
showing the same expressions. However, due to high inter-
subject variations, the distance between I(happiness,A )and
I(happiness,B )is larger than I(happiness,A )andI(sad,A ).
V ariations are greater related to identity compared to
variations related to expression. This partially explains why
most current methods could degrade on unseen subjects. As
in the lower part of Fig. 1, we generate two sub-spaces
for subject A and subject B respectively, each of them
contains the generated six basic expression images of the
query image. Only the same identity information exists in
each sub-space, thus allowing for expression classiÔ¨Åcation
to be more reliable. This motivated us to design a new
scheme by generating a kind of sub-space for facial expres-
sion classiÔ¨Åcation while aiming each subject individually.
This identity-adaptive FER approach is robust to identity
variations, achieving the goal of person-independent facial
expression recognition.
Generative Adversarial Networks (GAN) [10], an effective
training method for training generative models, was Ô¨Årst de-
veloped by Goodfellow et al in 2014. The GAN framework
plays an adversarial game with two players, a generator and
discriminator . The discriminator is designed to distinguish
between samples from the generator and samples from the
training data. On the other hand, the generator learns to out-
put samples that can maximally confuse the discriminator.
The conditional Generative Adversarial Networks (cGAN)
[11], an extension of the basic GAN model, enables the
generative model to learn different contextual informationby adding extra conditional variations to the input. Another
extension is the combination of GAN and CNN [12], which
has facilitated a number of interesting applications, e.g,
object attributes manipulation by vector arithmetic [12], face
generation [13], and object reconstruction from edge maps
[14].
The potential application of cGAN in face generation, as
well as the current limitation of facial expression recogni-
tion, motivated us to develop an identity-adaptive structure
to address the issue of high inter-subject variations through
regenerating six basic facial expression images of a same
subject. Unlike the previous method [1] that tried to split
the facial image feature into two parts: expression-related
and identity-related. However, our method is based on the
assumption that individual facial expression is dependent on
the individual‚Äôs identity, i.e. gender, age, and r ace with var-
ious expression styles , whereby expression-related features
and identity-related features are partially overlapped, and not
separable. By regenerating six facial expression images of a
subject given a face image of the individual, our approach
limits the feature comparison in a single identity sub-space,
allowing us to further compare the features merely caused
by expression variations of the subject. The feature space
is adaptive to a single identity without involving other
individuals, thus identity variations will not be an issue
for facial expression classiÔ¨Åcation. This enables the facial
expression recognition to work in a person-independent
manner. The main contribution of this paper is two-fold:
‚Ä¢To our knowledge, this is the Ô¨Årst work to address
the facial expression recognition by expression re-
generation through the exploration of GANs and the
295
application of cGANs.
‚Ä¢The proposed method can effectively deal with the issue
of subject variations, because the feature sub-space is
generated for a single subject with no afÔ¨Åliation with
the other subjects. This ‚Äùadaptive‚Äù property allows FER
to work person-independently. In addition, the proposed
method can be easily integrated into regular networks
for performance improvement.
To validate the effectiveness of our proposed method, we
conducted four experiments on four public databases: CK+
[15], Oulu-CASIA [16], BU3DFE [17] and BU-4DFE [18].
The results show that our proposed method achieved superior
performance compared to the state-of-the-art methods.
II. R ELA TED WORKS
Facial expressions are varied from person to person with
respect to age, race, gender, and cultural background in terms
of their appearances and styles of facial actions. Such a sub-
ject variation issue could cause performance degradation in
facial expression recognition, especially on unseen subjects.
There are existing approaches developed for focusing on
improving person-independent facial expression recognition.
Chen et al. [9] attempted to learn a person-speciÔ¨Åc model
through transfer learning. Ding et al. [5] proposed a so-called
FaceNet2ExpNet structure for facial expression recognition
on relatively small datasets, which used the pre-trained
model as supervision rather than as a source of some initial
parameter values. Their method consists of two stages:
Ô¨Årst, only the convolutional layers were trained to generate
similar intermediate features to those of the pre-trained
model; second, the whole deep model was trained with
the label information. The experiments showed that their
method worked well on relatively small expression datasets.
Zhao et al. [4] achieved invariance to expression intensity
by considering both peak expressions and weak expressions
to train a network. The invariance was achieved by a peak
gradient suppression learning algorithm, which drove the
intermediate features of weak expressions towards those of
peak expressions. Meng et al. [1] proposed an identity-
aware deep model for facial expression recognition, which
was capable of learning the features that were invariant to
both expression and identity-related variations, by utilizing
an expression-sensitive contrastive loss function.
Generative Adversarial Networks (GANs) have been
vigorously studied in recent years. It was Ô¨Årst proposed by
Goodfellow et al. [10] known as the generative adversarial
net (GAN), which applied the minimax game to two players,
i.e.,generator (G) and discriminator (D) for recovering the
distribution of the training data by G while keeping D to1
2.
Radford et al. [12] had successfully scaled up GANs using
CNNs to model images, and showed good result with the
trained discriminators for image classiÔ¨Åcation tasks, as well
as the interesting vector arithmetic properties of generated
samples of generator. Gauthier [13] extended the generativeadversarial net framework by adding a conditional informa-
tion, which could deterministically control the output of the
generator, and showed how to generate faces with speciÔ¨Åc
attributes from nothing but random noise and conditional
information. Isola et al. [14] utilized conditional adversarial
networks for image-to-image translation, and showed that
the conditional adversarial networks were applicable for a
wide variety of applications, i.e. synthesizing images from
labels, reconstructing objects from edge maps and colorizing
images.
Inspired by the above-mentioned prior works, especially
by the solution of image-to-image translation with condi-
tional generative adversarial networks [14], we propose to
explicitly train a set of expression-speciÔ¨Åc models ( e.g., six
prototypic expression models) to generate the corresponding
expressions of each subject for a given image of the subject.
By doing so, we can utilize expression features oriented by
each individual adaptively in the corresponding feature sub-
space even with the identity-related variations occurred from
person to person, thus leading to the FER improvement with
unseen subjects.
III. P ROPOSED METHOD
Our proposed method is based on the regeneration of six
basic facial expressions, which are adaptive to each identity
of individual, we call it the Identity-Adaptive Generation
method (a.k.a. IA-gen ). As shown in Fig.2, the IA-gen
framework consists of two parts. The upper part is aimed
to generate six basic facial expression images of the same
subject for any query image using six cGANs, and each of
them is designed to generate one expression respectively.
The lower part is the facial expression recognition module.
A pre-trained CNN is Ô¨Årst Ô¨Åne-tuned on the database, and
then the last fully connected layer is used as features for
both the query image and regenerated images. The query
image is labeled as one of the six basic expressions based
on a minimum distance in feature space.
A. GANs for Facial Expression Regeneration
GANs have been successfully used to generate images
[14] [13] [12]. Note that CNN based methods could also
be used for image generation. However, they use a mean
squared error (MSE) based solution, resulting in overly
smooth of the generated image. However, GANs force
the regeneration towards the natural image manifold and
produce more perceptually natural results [19]. Therefore,
we apply the GANs for facial expression image regeneration.
The generator G is trained to produce outputs that cannot
be distinguished from ‚Äùreal‚Äù image pairs by discriminator
D, which is adversarially trained to detect ‚Äùfake‚Äù image
pairs. The training procedure is illustrated in Fig. 3, where G
generates an output for any input image, and the image pairs
<I input,Ioutput>are constructed as negative examples
296
Figure 3. Training a GAN to generate facial expression image. The
discriminator (D) learns to distinguish between the [input, target] and
[input, output] pairs, while the generator (G) learns not only to fool the
discriminator, but also to be close to the ground truth (target image).
for the D, while image pairs <I input,Itarget>from the
training dataset are also constructed as positive examples.
The objective for discriminator can be expressed as:
LcGAN (D)=1
NN/summationdisplay
i=1/braceleftbigg
logD (Ii
input,Ii
target )+
log/bracketleftbig
1‚àíD/parenleftbig
Ii
input,G(Ii
input )/parenrightbig/bracketrightbig/bracerightbigg
(1)
whereNis the total number of training image pairs <
Iinput,Itarget>. To against an adversarial D, G is used
to not only fool the discriminator, but also to generate an
output as similar to the target image as possible. The loss
function for the generator is deÔ¨Åned as follow:
LcGAN (G)=1
NN/summationdisplay
i=1/braceleftbigg
Ladv+Œ±¬∑Lcont/bracerightbigg
(2)
Ladv=‚àílog/bracketleftbig
D/parenleftbig
Iinput,G(Iinput )/parenrightbig/bracketrightbig
(3)
Here,Ladv is the adversarial loss, as deÔ¨Åned in Eq.3,
D/parenleftbig
Iinput,G(Iinput )/parenrightbig
is the probability that the regenerated
image pairs <I input,Ioutput>are recognized as training
example. Lcont is the content loss between the regenerated
Ioutput and training example Itarget . The most widely used
loss function is the pixel-wise MSE loss, which is calculated
as:
LMSE
cont =1
c2WHcW/summationdisplay
x=1cH/summationdisplay
y=1/braceleftbigg
Itarget (x,y)‚àíIoutput (x,y)/bracerightbigg2
(4)
Here, c, W, H are the channel, width and height of the
image, respectively. However, MSE based solution often
overly smooths textures, which results in the lack of highfrequency content, and generates perceptually unsatisfying
results [19]. In our experiment, we use L1loss rather than
L2loss, because the L1loss can reduce the effect of over-
smoothing on the generated image [14]. Another choice for
content loss is perceptual similarity that measure high-level
perceptual differences between images [20]. The perceptual
loss is deÔ¨Åned on a loss network œÜwhich is pre-trained
for image classiÔ¨Åcation, e.g., VGG network [2]. Rather
than penalizing the pixel differences between Ioutput and
Itarget , the perceptual loss allows to have similar feature
representations to be computed in œÜ. LetœÜj(I)be the j-th
convolutional layer with a shape of Cj√óHj√óWj; then
the perceptual loss is the euclidean distance between feature
representations:
Lpep
cont=1
C2
jHjWjCjWj/summationdisplay
x=1CjHj/summationdisplay
y=1/braceleftbigg
œÜx,y(Itarget )‚àíœÜx,y(Ioutput )/bracerightbigg2
(5)
The Ô¨Ånal loss function for the generator G can be written
as:
LcGAN (G)=1
NN/summationdisplay
i=1/braceleftbigg
Ladv+Œ±¬∑LMSE
cont +Œ≤¬∑Lpep
cont/bracerightbigg
(6)
and the Ô¨Ånal optimization target is to solve the adversarial
min-max problem:
G‚àó=argmin
Gmax
D/braceleftbig
LcGAN (D)+ŒªLcGAN (G)/bracerightbig
(7)
The complete architecture of GAN follows the structure
used in [14]. SpeciÔ¨Åcally, the generator G is a deconvo-
lutional neural network [21], which contains six Encoders
with output channels {64, 128, 256, 512, 512, 512 }and six
Decoders with output channels {512, 512, 256, 128, 64, 1 }.
Each layer is followed by a non-linear activation function
(ReLU) and batch normalization (BN) layer. The stride size
is set as 2 to avoid max-pooling operation, and the Ô¨Ålter size
is set as 4√ó4for all layers. Skip connections ( U-net [22]
) are also used here, which help pass low-level information
shared between the input and output image directly across
the net. The discriminator D is a regular convolutional neural
network with {64, 128, 256, 512, 1 }output channels.
B. A CNN for Facial Expression Recognition
Due to large variations across subjects, facial expression
recognition performance degrades on unseen subjects in
real world scenarios. To cope with this problem, IA-gen
is Ô¨Årst used to construct a sub-space with six expressions
of the same subject, and then a regular convolutional neural
networks (FER-net) is used for facial expression recognition.
The last fully connected layer of FER-net is used as features
for both the input image and the generated images, and
the input image is labeled as one of the six basic facial
expressions based on a distance function in the feature space:
Predict =argmin
i||Feat (Iinput )‚àíFeat (Ii)|| (8)
297
Figure 4. The generated six facial images for the input image in four
databases.
Here,Feat (¬∑)is a feature extraction function. Any method,
except the FER-net used here, that can be used to extract
the facial expression related features works as well.
IV . E XPERIMENTS
We apply the proposed IA-gen approach to the task of
facial expression recognition on four publicly available fa-
cial expression databases: extended CK+ [15], Oulu-CASIA
[16], BU-3DFE [17] and BU-4DFE [18].
A. Implementation
We apply the tree-structured deformable models (TSM)
[23] for face detection and landmarks localization. The
faces are then cropped and resized to 70√ó70. The la-
beled facial expression database is quite small, thus we
utilize conventional data augmentation method to gener-
ate more training data, where each image is rotated by
degree [‚àí15o,‚àí12o,‚àí9o,‚àí6o,‚àí3o,0o,3o,6o,9o,12o,15o]
respectively. Five 64√ó64 patches are cropped out from
Ô¨Åve locations of each image ( center and four corners,
respectively ), then each patch is Ô¨Çipped horizontally, thus
resulting in an augmented dataset which is 110 times larger
than the original one. During testing, neither the rotation nor
the Ô¨Çipping operation is used on the input image.
The cGAN models are pre-trained on BU-4DFE database,
and then Ô¨Åne-tuned on other databases. In order to have
a similar baseline as [5], the VGG [2] model is used as
baseline in Oulu-CAISA database. All the others use the
FER-Net, which is Ô¨Åne-tuned from a pre-trained CNN model
on the Facial Expression Recognition (FER-2013) database
[7]. The batch size is 100, the momentum is Ô¨Åxed to be
0.9, and the dropout is set to 0.5 during training, and 1.0
during testing. The initial learning rate is set to 0.001, and
is decreased by 0.8 for 20 epochs. Œ±,Œ≤andŒªare set to
100, 10 and 1 respectively. Both cGANs and FER-Net are
implemented using tensorÔ¨Çow [24].
B. Facial Expression Generation
Fig.4 illustrates the generated facial expression images by
cGAN. The Ô¨Årst column (from top to bottom) represents the
input images from CK+ [15], Oulu-CASIA [16], BU3DFETable I
AVERAGE ACCURACY ON THE CK+ DA TABASE FOR SEVEN CLASSES .
Method Setting Accuracy
LBP-TOP [25] sequence-based 88.99
HOG 3D [26] sequence-based 91.44
3DCNN [27] sequence-based 85.9
STM-Explet [28] sequence-based 94.19
IACNN [1] image-based 95.37
DTAGN [29] sequence-based 97.25
CNN (baseline) image-based 89.50
Ours image-based 96.57
Figure 5. Confusion matrix on the CK+ database.
[17] and BU-4DFE [18] respectively, and the rest of the
columns show the generated facial images with different
expressions. As illustrated in Fig.4, the facial expression
regeneration part is capable of generating different facial ex-
pression images while keep the identity-related information
visually unchanged.
C. Evaluation on CK+
The Extended Cohn-Kanade database (CK+) [15] is
widely used for evaluating facial expression recognition. It
contains 593 video sequences recorded from 123 subjects,
each of which displayed one of seven expressions, from
neutral to peak expression. The label information is only
provided for the last frame of each sequence. Following
the general procedure, we use the last three frames of
each sequence with the provided label, which results in
981 images. The images are further split into 10 folds by
ID in ascending order, so the subjects in any two subsets
are mutually exclusive. Both the proposed method and
CNN based baseline are trained and tested on static images.
The reported results are the average of the 8 runs. As
shown in Table I, the performance of our proposed method
outperforms the CNN based baseline in terms of the average
accuracy of seven expressions. The proposed method is also
compared to the state-of-the-art methods evaluated on the
CK+ database. Among them. IACNN [1], CNN baseline and
our proposed method are image-based methods, while others
are sequence-based, and use the temporal information. In
contrast, our method IA-gen, which is more suitable for ap-
298
Table II
AVERAGE ACCURACY ON THE OULU -CASIA DATABASE .
Method Setting Accuracy
LBP-TOP [25] sequence-based 68.13
HOG 3D [26] sequence-based 70.63
STM-Explet [28] sequence-based 74.59
Atlases [30] sequence-based 75.52
DTAGN-Joint [29] sequence-based 81.46
FN2EN [5] image-based 87.71
PPDN [4](peak expression) image-based 84.59
VGG (baseline) image-based 82.33
Ours image-based 88.92
Figure 6. Confusion matrix on the Oulu-CASIA database.
plications where videos or image sequence is not available,
achieves comparable result, with 96.57% for seven classes.
Aimed at solving the identity-related issues, IACNN [1] tried
to split the whole face image feature into two parts: identity-
related and expression-related features, but this method may
not generalize well for all six basic facial expressions. For
example, people may show something in common when
smiling, but often individuals have unique ways to express
other emotions. This difference can be related to age, gender,
race, and even the background of education. Instead of trying
to split the identity-related information (which, sometimes, is
not separable), our proposed IA-gen adaptively regenerates
a sub-space that contains six basic facial expressions of the
same subject, so the facial expression recognition is limited
in a single-identity subspace, and that‚Äôs why our proposed
method shows higher performance than IACNN [1].
Fig. 5 is the confusion matrix, from which we can see that,
our proposed method performs well in recognizing contempt
and surprise , while showing lowest recognition accuracy in
fear , which is confused mainly with contempt and anger .
D. Evaluation on Oulu-CASIA
The Oulu-CASIA database [16] contains two subsets: VIS
and NIR. Here, we only use the VIS subset, in which all
videos were captured by VIS camera. The Oulu-CASIA VIS
has 480 video sequences taken from 80 subjects and six ex-
pressions under dark, strong, weak illumination conditions.
In this experiment, only videos that were captured underTable III
AVERAGE ACCURACY ON THE BU-3DFE DATABASE .
Method Setting Accuracy
Wang et al. [31] 3D 61.79
Berretti et al. [32] 3D 77.54
Y ang et al. [33] 3D 84.80
Lopes [34] image-based 72.89
CNN (baseline) image-based 73.2
Ours image-based 76.83
Figure 7. Confusion matrix on the BU-3DFE database.
strong condition are used. As a general procedure, the last
three frames of each sequence with the provided label are
used, which result in 1440 images. Similar to experiment
settings in [4], a 10-fold subject independent cross validation
is performed.
The82.33% baseline is reported by Ô¨Åne-tuning the VGG
[2] network on the dataset. With the introduction of IA-
gen approach, the classiÔ¨Åcation performance is increased to
88.92%.We also compare with the state-of-the-art methods,
as summarized in Table II. Among them, FN2EN [5],
PPDN [4] and our proposed method are image-based, while
the others are sequence-based methods, which utilize the
temporal-spatio information for facial expression recogni-
tion.
In the confusion matrix in Fig.6, the happiness and
surprise expressions have the highest recognition rate, while
disgust shows the lowest recognition accuracy, and is mainly
confused with anger .
E. Evaluation on BU-3DFE
The BU-3DFE database [17] contains 100 subjects, rang-
ing in age from 18 to 70 years old with a variety of race.
Each subject displays six basic expressions with four levels
of intensity and a neutral expression, which results in a total
of 2,500 3D facial expression models and texture images.
In this experiment, we only use the texture images, and
high intensity expressions ( the last two frames). A 10-
fold cross-validation is performed, and the split is subject
independent. Table III shows the average result of 10
runs on the BU-3DFE database. Unlike the CK+ and Oulu-
CASIA databases, the BU-3DFE is more challenging as it
299
Table IV
AVERAGE ACCURACY ON THE BU-4DFE DATABASE .
Method Setting Accuracy
Dapogny et al. [35] sequence-based 75.8
LSH-CORF. [36] sequence-based 77.1
PCRF [37] image-based 76.1
L¬¥aszl ¬¥o et al. [38] 3D 78.2
Pan et al. [39] image-based 80.89
CNN (baseline) image-based 76.45
Ours image-based 89.55
Figure 8. Confusion matrix on the BU-4DFE database.
has a wider variety of ethnicities and a larger range of ages
while the data size is relatively small. Note that [33] has
higher performance due to the use of geometric feature of the
3D shape model. However, our proposed method improves
the baseline by 3.6%, and also outperforms the image-based
method [34].
Fig. 7 shows the confusion matrix, where the expressions
(surprise, happiness, and disgust) are classiÔ¨Åed better than
the other expressions.
F . Evaluation on BU-4DFE
The BU-4DFE [18] contains 606 facial expression se-
quences captured from 101 subjects. Each sequence shows
one of the six basic facial expressions starting from neutral
to peak expression, and ending with neutral again. Seven
frames around the peak expression are collected with the
provided sequence label, which results in 4272( 101√ó6√ó7)
images. Similar to the CK+ database setting, a 10-fold
subject-independent cross validation is performed. Table IV
reports the average accuracy of 10 runs on the BU-4DFE
database for recognizing six expressions. As shown, our
proposed method has a 13.1% improvement over the CNN
baseline, and achieves the highest accuracy compared to the
stat-of-the-art methods, including sequence-based methods
[35] [36], image-based methods [37] [39] and 3D model
based method [38]. The confusion matrix in Fig.8 shows
that happiness and surprise are the easiest expressions to
recognize, while sadness shows relatively low recognition
rate, which is mainly confused with anger .
Figure 9. Per-class precision and F1 score on CK+, Oulu-CASIA, BU-
3DFE and BU-4DFE databases
Per-expression precision and F1 score for CK+, Oulu-
CASIA, BU-3DFE and BU-4DFE databases are shown in
Fig.9. We observe that both precision and F1 score are the
lowest in BU-3DFE database. This is due to the limited data
size and high variety of ethnicities with larger range of ages,
making it more challenging to recognize facial expressions.
V. C ONCLUSIONS AND FUTURE WORKS
In this paper, we proposed an identity-adaptive training
algorithm for facial expression recognition. First, six basic
facial expression images are regenerated for any query im-
age. Then, the facial expression is recognized by comparing
the query image and the generated six facial images in
the face space. Rather than trying to split image features
into expression-related and identity-related parts, we keep
the identity-related information adaptive to each query im-
age, and make the facial expression recognition identity-
independent.
For the future work, we plan to apply our method for
facial expression recognition in the wild by training a more
general facial expression regeneration model using more
databases.
VI. A CKNOWLEDGMENTS
The material is based upon the work supported in part by
the National Science Foundation under grants CNS-1629898
and CNS-1205664.
REFERENCES
[1] Z. Meng, P . Liu, J. Cai, S. Han, and Y . Tong, ‚ÄúIdentity-
aware convolutional neural network for facial expression
recognition,‚Äù in FG. IEEE, 2017, pp. 558‚Äì565.
[2] K. Simonyan and A. Zisserman, ‚ÄúV ery deep convolutional
networks for large-scale image recognition,‚Äù arXiv preprint
arXiv:1409.1556 , 2014.
[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei, ‚ÄúImagenet: A large-scale hierarchical image database,‚Äù
inCVPR . IEEE, 2009, pp. 248‚Äì255.
300
[4] X. Zhao and X. e. a. Liang, ‚ÄúPeak-piloted deep network for
facial expression recognition,‚Äù in ECCV . Springer, 2016, pp.
425‚Äì442.
[5] H. Ding, S. K. Zhou, and R. Chellappa, ‚ÄúFacenet2expnet:
Regularizing a deep face recognition net for expression recog-
nition,‚Äù in Automatic Face & Gesture Recognition (FG 2017) .
IEEE, 2017, pp. 118‚Äì126.
[6] F. Schroff, D. Kalenichenko, and J. Philbin, ‚ÄúFacenet: A
uniÔ¨Åed embedding for face recognition and clustering,‚Äù in
CVPR , 2015, pp. 815‚Äì823.
[7] I. J. Goodfellow, D. Erhan, and P . L. e. a. Carrier, ‚ÄúChallenges
in representation learning: A report on three machine learning
contests,‚Äù in International Conference on Neural Information
Processing . Springer, 2013, pp. 117‚Äì124.
[8] A. Mollahosseini, D. Chan, and M. H. Mahoor, ‚ÄúGoing deeper
in facial expression recognition using deep neural networks,‚Äù
inApplications of Computer Vision (WACV) . IEEE, 2016.
[9] J. Chen, X. Liu, P . Tu, and A. Aragones, ‚ÄúLearning person-
speciÔ¨Åc models for facial expression and action unit recogni-
tion,‚Äù Pattern Recognition Letters , vol. 34, no. 15, 2013.
[10] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-
Farley, S. Ozair, A. Courville, and Y . Bengio, ‚ÄúGenerative ad-
versarial nets,‚Äù in Advances in neural information processing
systems , 2014, pp. 2672‚Äì2680.
[11] M. Mirza and S. Osindero, ‚ÄúConditional generative adversar-
ial nets,‚Äù arXiv preprint arXiv:1411.1784 , 2014.
[12] A. Radford, L. Metz, and S. Chintala, ‚ÄúUnsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks,‚Äù arXiv preprint arXiv:1511.06434 , 2015.
[13] J. Gauthier, ‚ÄúConditional generative adversarial nets for
convolutional face generation,‚Äù Class Project for Stanford
CS231N: Convolutional Neural Networks for Visual Recog-
nition, Winter semester , vol. 2014, no. 5, p. 2, 2014.
[14] P . Isola, J.-Y . Zhu, T. Zhou, and A. A. Efros, ‚ÄúImage-
to-image translation with conditional adversarial networks,‚Äù
arXiv preprint arXiv:1611.07004 , 2016.
[15] P . Lucey and J. F. e. a. Cohn, ‚ÄúThe extended cohn-kanade
dataset (ck+): A complete dataset for action unit and emotion-
speciÔ¨Åed expression,‚Äù in CVPR Workshop . IEEE, 2010.
[16] G. Zhao, X. Huang, and M. e. a. Taini, ‚ÄúFacial expression
recognition from near-infrared videos,‚Äù Image and Vision
Computing , vol. 29, no. 9, pp. 607‚Äì619, 2011.
[17] L. Yin, X. Wei, Y . Sun, J. Wang, and M. J. Rosato, ‚ÄúA 3d
facial expression database for facial behavior research,‚Äù in
FG. IEEE, 2006, pp. 211‚Äì216.
[18] L. Yin, X. Chen, Y . Sun, T. Worm, and M. Reale, ‚ÄúA high-
resolution 3d dynamic facial expression database,‚Äù in FG.
IEEE, 2008, pp. 1‚Äì6.
[19] C. Ledig and L. e. a. Theis, ‚ÄúPhoto-realistic single im-
age super-resolution using a generative adversarial network,‚Äù
arXiv preprint , 2016.
[20] J. Johnson, A. Alahi, and L. Fei-Fei, ‚ÄúPerceptual losses
for real-time style transfer and super-resolution,‚Äù in ECCV .
Springer, 2016, pp. 694‚Äì711.
[21] M. D. Zeiler and R. Fergus, ‚ÄúVisualizing and understanding
convolutional networks,‚Äù in ECCV . Springer, 2014, pp. 818‚Äì
833.
[22] O. Ronneberger, P . Fischer, and T. Brox, ‚ÄúU-net: Convo-
lutional networks for biomedical image segmentation,‚Äù in
International Conference on Medical image computing and
computer-assisted intervention . Springer, 2015.
[23] X. Zhu and D. Ramanan, ‚ÄúFace detection, pose estimation,
and landmark localization in the wild,‚Äù in CVPR . IEEE,
2012, pp. 2879‚Äì2886.[24] M. Abadi, A. Agarwal, and P . e. a. Barham, ‚ÄúTensorÔ¨Çow:
Large-scale machine learning on heterogeneous distributed
systems,‚Äù arXiv preprint arXiv:1603.04467 , 2016.
[25] G. Zhao and M. Pietikainen, ‚ÄúDynamic texture recognition
using local binary patterns with an application to facial
expressions,‚Äù IEEE transactions on pattern analysis and
machine intelligence , vol. 29, no. 6, pp. 915‚Äì928, 2007.
[26] A. Klaser, M. Marsza≈Çek, and C. Schmid, ‚ÄúA spatio-temporal
descriptor based on 3d-gradients,‚Äù in BMVC 2008-19th British
Machine Vision Conference . British Machine Vision Asso-
ciation, 2008, pp. 275‚Äì1.
[27] M. Liu, S. Li, S. Shan, R. Wang, and X. Chen, ‚ÄúDeeply
learning deformable facial action parts model for dynamic
expression analysis,‚Äù in Asian Conference on Computer Vi-
sion . Springer, 2014, pp. 143‚Äì157.
[28] M. Liu, S. Shan, R. Wang, and X. Chen, ‚ÄúLearning ex-
pressionlets on spatio-temporal manifold for dynamic facial
expression recognition,‚Äù in CVPR , 2014, pp. 1749‚Äì1756.
[29] H. Jung, S. Lee, J. Yim, S. Park, and J. Kim, ‚ÄúJoint Ô¨Åne-tuning
in deep neural networks for facial expression recognition,‚Äù in
ICCV , 2015, pp. 2983‚Äì2991.
[30] Y . Guo, G. Zhao, and M. Pietik ¬®ainen, ‚ÄúDynamic facial expres-
sion recognition using longitudinal facial expression atlases,‚Äù
inECCV . Springer, 2012, pp. 631‚Äì644.
[31] J. Wang, L. Yin, X. Wei, and Y . Sun, ‚Äú3d facial expression
recognition based on primitive surface feature distribution,‚Äù
inComputer Vision and Pattern Recognition, 2006 IEEE
Computer Society Conference on , vol. 2. IEEE, 2006, pp.
1399‚Äì1406.
[32] S. Berretti, A. Del Bimbo, P . Pala, B. B. Amor, and
M. Daoudi, ‚ÄúA set of selected sift features for 3d facial ex-
pression recognition,‚Äù in Pattern Recognition (ICPR) . IEEE,
2010, pp. 4125‚Äì4128.
[33] X. Y ang, D. Huang, Y . Wang, and L. Chen, ‚ÄúAutomatic
3d facial expression recognition using geometric scattering
representation,‚Äù in FG, vol. 1. IEEE, 2015, pp. 1‚Äì6.
[34] A. T. Lopes and E. e. a. de Aguiar, ‚ÄúFacial expression
recognition with convolutional neural networks: Coping with
few data and the training sample order,‚Äù Pattern Recognition ,
vol. 61, pp. 610‚Äì628, 2017.
[35] A. Dapogny, K. Bailly, and S. Dubuisson, ‚ÄúDynamic facial
expression recognition by joint static and multi-time gap
transition classiÔ¨Åcation,‚Äù in FG, vol. 1. IEEE, 2015.
[36] O. Rudovic, V . Pavlovic, and M. Pantic, ‚ÄúMulti-output lapla-
cian dynamic ordinal regression for facial expression recog-
nition and intensity estimation,‚Äù in CVPR . IEEE, 2012, pp.
2634‚Äì2641.
[37] A. Dapogny, K. Bailly, and S. Dubuisson, ‚ÄúPairwise condi-
tional random forests for facial expression recognition,‚Äù in
ICCV , 2015, pp. 3783‚Äì3791.
[38] L. A. Jeni, D. Takacs, and A. Lorincz, ‚ÄúHigh quality facial
expression recognition in video streams using shape related
information only,‚Äù in ICCV Workshops . IEEE, 2011, pp.
2168‚Äì2174.
[39] Z. Pan, M. Polceanu, and C. Lisetti, ‚ÄúOn constrained local
model feature normalization for facial expression recog-
nition,‚Äù in International Conference on Intelligent Virtual
Agents , 2016, pp. 369‚Äì372.
301
"
https://ieeexplore.ieee.org/document/7961791,"Identity-Aware Convolutional Neural Network for Facial Expression
Recognition
Zibo Meng*1Ping Liu*2Jie Cai1Shizhong Han1Yan Tong1
1Department of Computer Science and Engineering, South Carolina University, USA
2Sony Electronics, USA
Abstract ‚Äî Facial expression recognition suffers under real-
world conditions, especially on unseen subjects due to high
inter-subject variations. To alleviate variations introduced by
personal attributes and achieve better facial expression recog-
nition performance, a novel identity-aware convolutional neural
network (IACNN) is proposed. In particular, a CNN with a new
architecture is employed as individual streams of a bi-stream
identity-aware network. An expression-sensitive contrastive loss
is developed to measure the expression similarity to ensure the
features learned by the network are invariant to expression
variations. More importantly, an identity-sensitive contrastive
loss is proposed to learn identity-related information from iden-
tity labels to achieve identity-invariant expression recognition.
Extensive experiments on three public databases including a
spontaneous facial expression database have shown that the
proposed IACNN achieves promising results in real world.
I. I NTRODUCTION
Facial activity is the most powerful and natural means for
understanding emotional expression for humans. Extensive
efforts have been devoted to facial expression recognition in
the past decades [31], [51], [36]. An automatic facial expres-
sion recognition system is desired in emerging applications
in human-computer interaction (HCI), such as online/remote
education, interactive games, and intelligent transportation.
Although great progress has been made from posed or
deliberate facial displays, facial expression recognition in
real-world suffers from various factors including uncon-
strained face pose, illumination change, and high inter-
subject variations. Moreover, recognition performance usu-
ally degrades on unseen subjects, primarily due to high inter-
subject variations introduced by age, gender, and especially
person-speciÔ¨Åc characteristics associated with identity as
discussed in [43]. Since these factors are nonlinearly coupled
with facial expressions in a multiplicative way , features
extracted through existing methods are not purely related
to expressions. As illustrated in Fig. 1, I1andI2are the
same subject displaying different expressions, whereas I1
andI3are different subjects displaying the same expression.
For facial expression recognition, it is desired to have the
distanceD1between images I1andI2larger than D2
betweenI1andI3in the feature space, as in Fig. 1b.
However, due to high inter-subject variations, D1is usually
smaller than D2in the feature spaces of existing approaches,
as in Fig. 1a. This motivates us to attack the challenging
problem of learning and extracting personal-independent and
* indicates equal contribution.
Surprise
Anger
(a) (b)
Fig. 1: Illustration for features learned by (a) existing methods,
and (b) the proposed IACNN. Best viewed in color.
expression-discriminative features. To solve this problem,
we develop an identity-aware convolutional neural network
(IACNN) to learn expression-related representations and, at
the same time, to learn identity-related features to facilitate
identity-invariant facial expression recognition.
In addition to minimizing the classiÔ¨Åcation errors, a simi-
larity metric for expression is developed and employed in the
IACNN to pull the samples with the same expression together
while pushing those with different expressions apart in the
feature space. As a result, the intra-expression variations are
reduced, while the inter-expression differences are increased.
However, the learned expression representations may contain
irrelevant identity information such that the performance
of facial expression recognition is affected by high inter-
subject variations as illustrated in Fig. 1a. To alleviate the
effect of the inter-subject variations, a similarity metric for
identity is proposed in the IACNN to learn identity-related
features, which will be combined with the expression-related
representations for facial expression recognition.
The architecture of the proposed IACNN is illustrated in
Fig 2. During the training process, expression and identity
related features are jointly estimated through a deep CNN
framework, which is composed of two identical CNN streams
and trained by simultaneously minimizing the classiÔ¨Åca-
tion errors while maximizing the expression and identity
similarities. SpeciÔ¨Åcally, given a pair of images, each of
which is fed into one CNN stream, the similarity losses are
computed using the expression and identity related features,
respectively. In addition, the classiÔ¨Åcation errors in terms of
expression recognition are also calculated for both images
and used to Ô¨Åne-tune the model parameters to ensure the
learned features are meaningful for expression recognition.
During testing, an input image is fed into one CNN stream,
and predictions are generated based on both the expression-
2017 IEEE 12th International Conference on Automatic Face & Gesture Recognition
978-1-5090-4023-0/17 $31.00 ¬© 2017 IEEE
DOI 10.1109/FG.2017.140558
2017 IEEE 12th International Conference on Automatic Face & Gesture Recognition
978-1-5090-4023-0/17 $31.00 ¬© 2017 IEEE
DOI 10.1109/FG.2017.140558
2017 IEEE 12th International Conference on Automatic Face & Gesture Recognition
978-1-5090-4023-0/17 $31.00 ¬© 2017 IEEE
DOI 10.1109/FG.2017.140558
2017 IEEE 12th International Conference on Automatic Face & Gesture Recognition
978-1-5090-4023-0/17 $31.00 ¬© 2017 IEEE
DOI 10.1109/FG.2017.140558
2017 IEEE 12th International Conference on Automatic Face & Gesture Recognition
978-1-5090-4023-0/17 $31.00 ¬© 2017 IEEE
DOI 10.1109/FG.2017.140558

Fig. 2: The architecture of the IACNN used for training, where a pair of images are input into two identical CNNs with sharing weights,
respectively. In each CNN, there are two FC layers, i.e. FCexp, andFCID, on top of the Ô¨Årst FC layer for learning the expression-related
and identity-related features, respectively. LExp
Contrasitve /LID
Contrasitve is a contrastive loss used to minimize the differences between the
samples with the same expression/identity. With explicit expression labels, LExp
Softmax andLSoftmax are employed to ensure the learned
features are meaningful for expression recognition. Loss layers are highlighted by gray blocks. Best viewed in color.
related and the identity-related features. In summary, our
main contributions in this paper are:
1)Developing an IACNN, which is capable of utilizing
both expression-related and identity-related informa-
tion for facial expression recognition;
2)Introducing a new auxiliary layer with an identity-
sensitive contrastive loss to learn identity-related rep-
resentations to alleviate high inter-subject variations;
3)Proposing a joint loss function, which considers clas-
siÔ¨Åcation errors of expression recognition as well as
expression and identity similarities, to Ô¨Åne tune the
expression-related and identity-related features simul-
taneously.
Extensive experiments on two well-known posed facial
expression databases, i.e. Extended Cohn-Kanade database
(CK+) [14], [25] and MMI database [32], have demon-
strated the effectiveness of the proposed method for facial
expression recognition. Furthermore, the proposed method
was evaluated on a spontaneous facial expression dataset,
i.e. Static Facial Expressions in the Wild (SFEW) [6], which
contains face images with large head pose variations and
different illuminations and has been widely used for bench-
marking facial expression recognition. Experimental results
on the SFEW dataset have shown that the proposed approach
achieves promising results in real world.
II. R ELATED WORK
As elaborated in the surveys [31], [51], [36], facial expres-
sion recognition has been extensively studied over the past
decades. Both 2D and 3D features have been extracted fromstatic images or image sequences to capture the appearance
and geometry facial changes caused by target expressions.
The features employed can be human-designed including
Histograms of Oriented Gradients (HOG) [2], Scale Invariant
Feature Transform (SIFT) features [50], [5], histograms of
Local Binary Patterns (LBP) [43], [3], histograms of Local
Phase Quantization (LPQ) [12], and their spatiotemporal ex-
tensions [18], [38], [52], [12], [47]. Other spatiotemporal ap-
proaches, such as temporal modeling of shapes (TMS) [10],
interval temporal Bayesian network (ITBN) [46], expres-
sionlets on spatiotemporal manifold (STM-ExpLet) [22],
and spatiotemporal covariance descriptors (Cov3D) [35],
have been developed to utilize both spatial and temporal
information in an image sequence.
Human-crafted features can achieve high performance on
posed expression data. However, performance degenerates
on spontaneous data with large variations in head pose and
illumination. In contrast, features can be learned in a data-
driven manner by sparse coding [24], [54], [33], [28] or
deep learning [34], [42], [23], [29], [20], [16], [49], [30],
[13], [21], [53]. Most recently, CNN-based deep learning
approaches [16], [49], [30] have been demonstrated to be
more robust to real world conditions in EmotiW2015 [6].
Most of the aforementioned approaches focus on improv-
ing person-independent recognition, while a few of them
learn models from person-speciÔ¨Åc data. For example, Chen
et al. [3] learn a person-speciÔ¨Åc model from a few person-
speciÔ¨Åc samples based on transfer learning.
Existing approaches learn a similarity metric using either
human-designed features [17], [37] or deep metric learn-
559
559
559
559
559
Fig. 3: The proposed Exp-Net for facial expression recognition includes a pair of identical component CNNs, whose weights are shared.
As depicted in the dashed rectangle, the component CNN includes three convolutional layers, each of which is followed by a PReLU
layer and a BN layer. A max pooling layer is employed after each of the Ô¨Årst two BN layers. Following the third convolutional layer, two
FC layers are used to generate the representation for each input sample. Finally, a softmax loss layer ( LExp
Softmax ) is employed to produce
the distribution over the target expressions and to calculate the classiÔ¨Åcation errors for Ô¨Åne-tuning the parameters. A contrastive loss, i.e.
LExp
Contrastive , is employed to reduce the intra-class variations, while to decrease the inter-class similarity. Loss layers are highlighted by
gray blocks. Best viewed in color.
ing such as CNNs with pairwise-constraints [41], [4], [53]
or triplet-constraints [45]. Compared with those based on
hand-crafted features, deep metric learning achieves more
powerful representations with low intra-class yet high inter-
class distances in a data driven manner and thus, has shown
promising results in many applications. For example, Zhao
et al. [53] considered similarity between peak and non-
peak frames from the same subject to achieve invariance to
expression intensity. In this work, we explicitly learn the
expression-related and identity-related features to achieve
identity-invariant expression recognition, where pairwise-
constraints based contrastive loss is chosen as the loss func-
tion for learning the similarity metric for either expression
or identity. Unlike previous approaches, we utilize both con-
trastive loss and softmax loss to learn both features jointly.
More importantly, a new auxiliary layer plus a contrastive
loss is employed, which is responsible for learning identity-
related representations to alleviate the inter-subject variations
introduced by personal attributes. The proposed framework
is an end-to-end system, which can be optimized via standard
stochastic gradient descent (SGD).
III. P ROPOSED METHOD
A. A Component CNN
Instead of utilizing an off-the-shelf model, e.g.
AlexNet [19], a new CNN architecture is designed for the
component network of IACNN, due to limited expression-
labeled data. As shown in Fig. 3, the CNN enclosed in the
dashed rectangle includes three convolutional layers, each
of which is followed by a parametric rectiÔ¨Åed unit (PReLU)
layer [8] as the activation function. As an extension of a
rectiÔ¨Åed linear unit (ReLU) activation function, PReLU
has better Ô¨Åtting capability than the sigmoid function or
hyperbolic tangent function [19] and further boosts the
classiÔ¨Åcation performance as compared to the traditional
ReLU. After each PReLU layer, a batch normalization (BN)layer [9] is utilized to normalize each scalar feature to zero
mean and unit variance. The BN layer has been shown
to improve classiÔ¨Åcation performance and accelerate the
training process [9]. After each of the Ô¨Årst two BN layers,
there is a max pooling layer. Following the third PReLU
layer, two fully connected (FC) layers consisting of 1,024
neurons are employed. Finally, a softmax loss layer is used
to generate the probabilistic distribution over the Ktarget
expressions and to calculate the classiÔ¨Åcation loss given the
expression labels.
B. An Exp-Net for Facial Expression Recognition
In real-world scenarios, facial expression recognition suf-
fers from intra-expression variations. As a result, CNNs can
generate quite different representations for image samples
containing the same expression. To cope with the problem,
an Exp-Net composed of two identical component CNNs is
constructed, as illustrated in Fig. 3. Two softmax losses and
one contrastive loss are employed to learn representations
that are meaningful for facial expression recognition. Given
the expression labels, a softmax loss is employed on top of
each component network to calculate the classiÔ¨Åcation errors
and is used to Ô¨Åne tune the parameters in the lower layers.
In addition, a contrastive loss is utilized to learn a similarity
metric for image pairs to make sure that the samples with
the same expression have similar representations, and at the
same time, those with different expressions are far away in
the feature space.
1) Expression-Sensitive Contrastive Loss: As illustrated
in Fig. 4, an expression-sensitive contrastive loss is designed
to pull the samples with the same expression towards each
other, and to push the samples with different expressions
away from each other at the same time. SpeciÔ¨Åcally, the
similarity between two images I1andI2is deÔ¨Åned as the
squared Euclidean distance in the feature space:
D
fE(I1);fE(I2)
=kfE(I1) fE(I2)k2
2 (1)
560
560
560
560
560
Surprise (I1)CNN
CNNShared
WeightsFeature Space
f(I1)
f(I2)Lp
Surprise (I2)(a)
Feature Space
Surprise (I1)
Sadness (I2)CNN
CNNShared
Weightsf(I1)
f(I2)LnD(f(I1),f(I2)<Œ± (b)
Fig. 4: During the training process, the contrastive loss (a) pulls the samples with the same expression towards each other, and (b) pushes
the samples with different expressions apart. LpandLnrepresent the contrastive losses for the image pairs with the same expression and
different expressions, respectively.
wherefE()is the expression-related image representation,
i.e., the output of the FCexplayer in Fig. 3. A smaller
distanceD
f(I1);f(I2)
indicates that the two images are
more similar in the feature space.
Then, the expression-sensitive contrastive loss is deÔ¨Åned
as follows:
Lexp
Contrastive=MX
i=1LE
zE
i;fE(Ii;1);fE(Ii;2)
(2)
LE
zE
i;fE(Ii;1);fE(Ii;2)
=zE
i
2D
fE(Ii;1);fE(Ii;2)
+1 zE
i
2max
0;E D 
fE(Ii;1);fE(Ii;2)
(3)
whereM is the number of image pairs; and 
zE
i;fE(Ii;1);fE(Ii;2)
represents the label and the
expression-related image features for the ithpair of training
samples, respectively. When the pair of images has the same
expression label, zE
i= 1 and the loss is
LE
1;fE(Ii;1);fE(Ii;2)
=1
2D
fE(Ii;1);fE(Ii;2)
: (4)
Otherwise,zE
i= 0 and the loss becomes
LE
0;fE(Ii;1);fE(Ii;2)
=1
2max
0;E D 
fE(Ii;1);fE(Ii;2)
(5)
whereE>0is a parameter to determine how much
dissimilar pairs contribute to the loss function. When the
distance between two dissimilar samples in a pair is less
thanE, then the loss will be calculated. In our experiment,
Eis set to 10 for the expression-sensitive contrastive loss,
empirically.
C. The IACNN for Facial Expression Recognition
The performance of facial expression recognition often
drops signiÔ¨Åcantly for unseen subjects, mainly due to high
inter-subject variations introduced by age, gender, and espe-
cially person-speciÔ¨Åc characteristics associated with identity.
To deal with this problem, an IACNN is proposed by
introducing an auxiliary FC layer into the Exp-Net, which
takes the input from the lower layers of the Exp-Net, but has
its own set of learnable weights.
As shown in Fig. 2, the FCIDlayer is on top of the
Ô¨Årst FC layer of the Exp-Net after the convolutional layers.
Given the identity labels, the FCIDis responsible forlearning identity-related features using an identity-sensitive
contrastive lossLID
Contrastive , deÔ¨Åned as follows
LID
Contrastive =MX
i=1LID
zID
i;fID(Ii;1);fID(Ii;2)
(6)
wherefID()is the identity-related feature, i.e., the output
of theFCIDlayer; andLID
zID
i;fID(Ii;1);fID(Ii;2)
is
deÔ¨Åned similar to the expression-sensitive contrastive loss as
below:
LID
zID
i;fID(Ii;1);fID(Ii;2)
=zID
i
2D
fID(Ii;1);fID(Ii;2)
+1 zID
i
2max
0;ID D 
fID(Ii;1);fID(Ii;2)
(7)
wherezID
i= 1, if the pair of images comes from the same
subject; otherwise, zID
i= 0.IDis set to 10 for the identity-
sensitive contrastive loss, empirically.
The identity-related features are then concatenated with
the expression-related features encoded by the FCexplayer
to form the Ô¨Ånal feature vector FCfeat for facial expres-
sion recognition. Therefore, the overall loss function of the
proposed IACNN is deÔ¨Åned as
L=1LExp
Contrastive+2LID
Contrastive +3L1
Softmax
+4L2
Softmax +5LExp 1
Softmax+6LExp 2
Softmax(8)
where1-6are the weights of each loss, respectively.
LExp
Contrastive andLID
Contrastive are the expression-sensitive
contrastive loss and the identity-sensitive contrastive loss,
as deÔ¨Åned in Eq. 2 and Eq. 6, respectively. LExp1
Softmaxand
LExp2
Softmaxrepresent the classiÔ¨Åcation errors using only the
expression-related features from the component CNN; while
L1
Softmax andL2
Softmax represent the classiÔ¨Åcation errors
using the concatenated feature vector FCfeat.
The overall loss is back-propagated to both FCexpand
FCID. As a result, the expression-related and identity-
related features are Ô¨Åne-tuned jointly in the IACNN.
During the testing, only one stream, i.e., the component
CNN, is employed for making the decision. Given a testing
sampleIi, the expression and identity related representations,
i.e.fE(Ii)andfID(Ii)are calculated and concatenated to
construct the Ô¨Ånal feature vector, i.e. FCfeat, for facial ex-
pression recognition. In this work, classiÔ¨Åcation is performed
using the softmax classiÔ¨Åer of the CNN.
561
561
561
561
561
IV. E XPERIMENTS
To demonstrate the effectiveness of the proposed method
in terms of facial expression recognition, extensive ex-
periments have been conducted on three public databases
including two posed facial expression databases, i.e. the CK+
database [14], [25] and the MMI database [32], and more
importantly, a spontaneous facial expression database, i.e.,
the SFEW datasets [6].
A. Preprocessing
Face alignment is conducted to reduce variation in face
scale and in-plane rotation across different facial images.
SpeciÔ¨Åcally, 66landmarks are detected using a state-of-
the-art face alignment method, i.e., Discriminative Response
Map Fitting (DRMF) [1]. The face regions are aligned based
on three Ô¨Åducial points: the centers of the eyes and the
mouth, and then are cropped and scaled to a size of 6060.
It may be not sufÔ¨Åcient to learn a deep model with
the limited number of sequences or images in the facial
expression databases. To alleviate the chance of over-Ô¨Åtting,
an augmentation procedure is employed to train the CNN
models, where a 4848patch is randomly cropped from an
image and randomly Ô¨Çipped horizontally as the input of the
CNN, resulting 288 times larger than the original training
data. During testing, only the 4848patch centered at the
face image is used as the input to one stream of the IACNN.
B. Implementation Details
The proposed component CNN is Ô¨Åne-tuned from a
CNN model pretrained on the Facial Expression Recognition
(FER-2013) dataset [7] using stochastic gradient decent with
a batch size of 128, momentum of 0:9, and a weight decay
parameter of 0:005. Dropout is applied to each FC layer
with a probability of 0:6, i.e. zeroing out the output of a
neuron with probability of 0:6. In Eq. 8,2is set to 5 for
CK+/SFEW and 2 for MMI, while other parameters are set
to 1 for all datasets empirically. The CNNs are implemented
using the Caffe library [11].
Identity information is required to train the IACNN model.
Labeled subject IDs are provided in the CK+ and MMI
databases and we manually labeled subject IDs for the SFEW
database. In practice, the proposed system only requires
weakly supervised identity information, i.e., whether the two
images are from the same subject, which can be automati-
cally obtained by an off-the-shelf face veriÔ¨Åcation method.
C. Experimental Results
To better demonstrate the effectiveness of the proposed
model, two baseline methods are employed, i.e. the one-
stream component CNN described in Section III-A, denoted
byCNN , and the Exp-Net introduced in Section III-B,
denoted byExp Net.
1) Results on CK+ dataset: CK+ database [14], [25]
is widely used for evaluating facial expression recognition
system. It contains 327 image sequences collected from 118
subjects, each of which is labeled as one of 7 expressions,
i.e. anger, contempt, disgust, fear, happiness, sadness, and
surprise. For each sequence, the label is only provided forTABLE I: Confusion matrix of the proposed IACNN method
evaluated on the CK+ database [14], [25]. The ground truth and
the predicted labels are given by the Ô¨Årst column and the Ô¨Årst row,
respectively.
An Co Di Fe Ha Sa Su
An 91.1% 0% 0% 1.1% 0% 7.8% 0%
Co 5.6% 86.1% 0% 2.7% 0% 5.6% 0%
Di 0% 0% 100% 0% 0% 0% 0%
Fe 0% 4% 0% 98% 2% 0% 8%
Ha 0% 0% 0% 0% 100% 0% 0%
Sa 3.6% 0% 0% 1.8% 0% 94.6% 0%
Su 0% 1.2% 0% 0% 0% 0% 98.8%
TABLE II: Performance comparison on the CK+ database [14],
[25] in terms of the average accuracy of 7 expressions.
Method Accuracy
3DCNN [21] 85.9
MSR [33] 91.4
HOG 3D [18] 91.44
TMS [10] 91.89
Cov3D [35] 92.3
3DCNN-DAP [21] 92.4
STM-ExpLet [22] 94.19
DTAGN [13] 97.25
CNN (baseline) 89.30
Exp-Net (baseline) 92.81
IACNN 95.37
the last frame (the peak frame). To collect more data, the
last three frames of each sequence are selected as peak
frames associated with the provided expression label. Thus,
an experimental database consisting of 981 images is built.
The database is further divided into 8 subsets, where the
subjects in any two subsets are mutually exclusive. Then
an 8-fold cross-validation strategy is employed, where, for
each run, data from 6 subsets are used for training and that
from the remaining two subsets for validation and testing,
respectively.
The proposed IACNN and the two baseline methods are
trained and tested on static images. The Ô¨Ånal sequence-
level predictions are obtained by choosing the class with
the highest average score of the three images. The results
are reported as the average of the 8 runs. The confusion
matrix of the proposed IACNN model is reported in Table I,
where diagonal entries represent the recognition accuracy for
each expression. As shown in Table II, the performance of
the proposed IACNN outperforms the two baseline methods,
especially the one-stream component CNN, in terms of the
average accuracy of the 7 expressions. The IACNN model
is also compared with the state-of-the-art methods evaluated
on the CK+ database including methods using human crafted
features (HOG 3D [18], TMS [10], Cov3D [35], and STM-
ExpLet [22]), methods using sparse coding (MSR [33]), and
CNN-based methods (3DCNN and 3DCNN-DAP [21] and
DTAGN [13]). As shown in Table II, the IACNN outperforms
the methods based on human crafted features or sparse
coding and also performs better or is at least comparable
to the CNN-based methods. Note that all these methods
except the MSR employed temporal information extracted
from image sequences. In contrast, the proposed IACNN
learns and extracts features from static images, which is more
562
562
562
562
562
Anger
Contempt
Disgust
Fear
Happy
Sad
Surprise
(a) Original images (b) Features learned by CNN (c) Features learned by Exp-Net (d) Features learned by IACNNSubject504
Subject504Subject504
Subject504Subject504
CentroidCentroid
Centroid
CentroidCentroid
CentroidFig. 5: A visualization study of (a) the original raw images and the features learned by (b) the component CNN, (c) the Exp-Net, and (d)
the IACNN model on the CK+ database. The number of samples is 981including 2473training data from 6 subsets, 383validation
data, and 423testing data. The dots, stars, and diamonds represent training, validation, and testing data, respectively. The features
learned by the IACNN are better separated according to expressions for both the validation and testing data. Best viewed in color.
TABLE III: Confusion matrix of the proposed IACNN method
evaluated on the MMI database [32]. The ground truth and the
predicted labels are given by the Ô¨Årst column and the Ô¨Årst row,
respectively.
An Di Fe Ha Sa Su
An 81.8% 3% 3% 1.5% 10.6% 0%
Di 10.9% 71.9% 3.1% 4.7% 9.4% 6%
Fe 5.4% 8.9% 41.1% 7.1% 7.1% 30.4%
Ha 1.1% 3.6% 0% 92.9% 2.4% 0%
Sa 17.2% 7.8% 0% 1.6% 73.4% 0%
Su 7.3% 0% 14.6% 1.2% 0% 76.9%
suitable for applications, where videos or image sequences
are not available.
Visualization Study: To further demonstrate the effective-
ness in terms of learning good representations for expres-
sion recognition, we visualize the features learned by the
component CNN, the Exp-Net, and the IACNN, respectively,
using t-SNE [44], which is widely employed to visualize high
dimensional data. As shown in Fig. 5a, the training samples
are denoted by dots, validation samples denoted by stars,
and testing samples denoted by diamonds. As illustrated in
Fig. 5a, the original raw images are randomly distributed. In
contrast, the learned features (Fig. 5b, c, and d) are clustered
based on their expression labels. A close-up enclosed by
the rectangle is given for Fig. 5b, c, and d. Comparing
Fig. 5b (the component CNN) with Fig. 5c (Exp-Net) and d
(IACNN), the samples of the same subject with different
expressions are closer to each other rather than to their
corresponding cluster centers marked by magenta circles. As
compared to the features learned by the component CNN
and the Exp-Net, the proposed IACNN model yields a better
separation of the features, as depicted in Fig. 5d.
2) Results on MMI dataset: The MMI dataset [32] con-
sists of 213 image sequences, among which 208 sequences
containing frontal-view faces of 31 subjects will be used in
our experiment. Each sequence is labeled as one of six basic
expressions, i.e. anger, disgust, fear, happiness, sadness, and
surprise. Starting with a neutral expression, each sequence
develops the facial expression as time goes on, reaches peak
near the middle of the sequence, and ends with a neutral
expression. Since the actual location of the peak frame is not
provided, three frames in the middle of each image sequence
are collected as peak frames and associated with the providedTABLE IV: Performance comparison on the MMI database [32]
in terms of the average accuracy of 6 expressions.
Method Accuracy
3DCNN [21] 53.2
ITBN [46] 59.7
HOG 3D [18] 60.89
3DCNN-DAP [21] 63.4
3D SIFT [38] 64.39
DTAGN [13] 70.24
STM-ExpLet [22] 75.12
CNN (baseline) 57.00
Exp-Net (baseline) 64.31
IACNN 69.48
CNNCK(baseline trained from MMI+CK+) 65.17
Exp-NetCK(baseline trained from MMI+CK+) 70.55
IACNNCK(trained from MMI+CK+) 71.55
expression labels. Hence, there are a total of 2083images
used in our experiments.
Similar to that on the CK+ database, the proposed IACNN
and the two baseline methods are trained and tested on static
images and the Ô¨Ånal sequence-level predictions are made by
selecting the class with the highest average score of the three
images. The dataset is divided into 10 subsets for person-
independent 10-fold cross validation, where, for each run,
data from 8 subsets are used for training and those from
the remaining 2 subsets are used for validation and testing,
respectively. The results are reported as the average of 10
runs. The confusion matrix of the proposed IACNN model
evaluated on the MMI dataset is reported in Table III.
As shown in Table IV, the proposed IACNN outperforms
the two baseline methods signiÔ¨Åcantly. Furthermore, the
IACNN also outperforms most of the state-of-the-art meth-
ods. Note that the image sequences in the MMI database
contain a full temporal pattern of expressions, i.e., from
neutral to apex, and then released, and are especially favored
by these methods exploiting temporal information, e.g., all
the state-of-art methods in comparison.
Since the MMI dataset contains a small number of sam-
ples, i.e. only 624 face images, it is not large enough to train
a deep model. To demonstrate that the IACNN can achieve
better performance given more training data, we employed
additional data of the six basic expressions from the CK+
dataset. SpeciÔ¨Åcally, for each run, the 8 subsets of the MMI
dataset plus the data from the CK+ dataset will be used as
563
563
563
563
563
TABLE V: Confusion matrix of the proposed IACNN method
evaluated on the SFEW [6] validation set. The ground truth and
the predicted labels are given by the Ô¨Årst column and the Ô¨Årst row,
respectively.
An Di Fe Ha Ne Sa Su
An 70.7% 0% 2.7% 5.2% 6.7% 8% 6.7%
Di 19.1% 0% 0% 4.8% 33.3% 38% 4.8%
Fe 37.8% 0% 8.9% 11.1% 15.6% 13.3% 13.3%
Ha 9.9% 0% 0% 70.4% 9.9% 8.4% 1.4%
Ne 5.1% 0% 2.6% 1.3% 60.3% 24.3% 6.4%
Sa 7.4% 0% 2.9% 5.9% 20.6% 58.8% 4.4%
Su 23.1% 0% 3.8% 3.8% 28.9% 11.5% 28.9%
TABLE VI: Confusion matrix of the proposed IACNN method
evaluated on the SFEW [6] testing set. The ground truth and the
predicted labels are given by the Ô¨Årst column and the Ô¨Årst row,
respectively.
An Di Fe Ha Ne Sa Su
An 79.8% 0% 1.4% 0% 8.7% 1.4% 8.7%
Di 35.3% 0% 0% 29.4% 11.8% 23.5% 0%
Fe 34.1% 0% 9.8% 12.2% 9.8% 7.3% 26.8%
Ha 8.4% 0% 0% 75.8% 4.2% 7.4% 4.2%
Ne 17.2% 0% 1.7% 3.5% 60.4% 10.3% 6.9%
Sa 23.6% 0% 7.3% 20% 5.5% 34.6% 9%
Su 24.3% 0% 2.7% 8.1% 10.8% 8.1% 46%
training set and the remaining two subsets of the MMI dataset
are used as the validation and testing sets, respectively.
The results are reported at the bottom of Table IV denoted
as CNNCK, CNNCK, and IACNNCKfor the one-stream
CNN, the Exp-Net, and the proposed IACNN, respectively.
With additional training data, the recognition accuracy of the
IACNN further improves from 69:48to71:55.
3) Results on SFEW dataset: The SFEW [6] is the most
widely used benchmark for facial expression recognition,
which ‚Äútargets the efforts required towards affect analysis
in the wild‚Äù [6]. The SFEW database is composed of 1,766
images, i.e. 958 for training, 436 for validation, and 372 for
testing. Each of the images has been assigned to one of seven
expression categories, i.e., anger, disgust, fear, neutral, happy,
sad, and surprise. The expression labels of the training and
validation sets are provided, while those of the testing set is
held back by the challenge organizer.
As illustrated in Table VII, Kim et al. [16], Yu et al. [49],
Ng et al. [30], and Yao et al. [48] are ranked at the 1st
to4thamong the 18 teams in the EmotiW2015 challenge,
respectively. Note that the Ô¨Årst two methods, i.e. Kim et
al. [16] and Yu et al. [49], used an ensemble of CNNs.
For example, Kim et al. [16] employed an ensemble of
CNNs with different architectures, which can boost the Ô¨Ånal
performance. The proposed IACNN model outperforms the
baseline of SFEW ( 35:93on the validation set and 39:13on
the testing set) by a large margin. The proposed IACNN is
ranked at the 5thplace on both the validation set and the
testing set among all the methods compared and achieves
comparable performance with Ng et al. [30] and Yao et
al. [48], which demonstrates the effectiveness of the IACNN
model in the real world.
4) Cross-database validation: To further demonstrate that
the proposed IACNN is less affected by identity, a cross-TABLE VII: Performance comparison on the SFEW database [6]
in terms of the average accuracy of 7 expressions.
Method Validation Set Test Set
Kim et al. [16] 53.9 61.6
Yu et al. [49] 55.96 61.29
Ng et al. [30] 48.5 55.6
Yao et al. [48] 43.58 55.38
Sun et al. [40] 51.02 51.08
Zong et al. [55] N/A 50
Kaya et al. [15] 53.06 49.46
Mollahosseini et al. [29] 47.7 N/A
Dhall et al. [6] (baseline of SFEW) 35.93 39.13
CNN (baseline) 47.80 50.54
Exp-Net 49.51 52.96
IACNN 50.98 54.30
TABLE VIII: Cross-database facial expression recognition perfor-
mance in terms of average accuracy.
Test Set [26] [39] [27] CNN IACNN
CK+ 47.1 - 56 69.26 71.29
MMI 51.4 50.8 36.8 54.87 55.41
database experiment was conducted. The classiÔ¨Åers trained
on CK+ and MMI in the previous experiments are directly
applied to MMI and CK+, respectively. The results are
reported as the average of 10 runs for CK+ and 8 runs
for MMI. As shown in Table VIII, the proposed IACNN
outperforms all the state-of-the-art methods as well as the
baseline CNN method.
V. C ONCLUSION
In this work, we proposed a novel identity-aware CNN
to capture both expression-related and identity-related infor-
mation to alleviate the effect of personal attributes on facial
expression recognition. SpeciÔ¨Åcally, a softmax loss combined
with an expression-sensitive contrastive loss is utilized to
learn the expression-related representations. To alleviate the
variations introduced by different identities, a new auxiliary
layer with an identity-sensitive contrastive loss is employed
to learn identity-related representations. Both the expression-
related and identity-related features are concatenated and
employed to achieve an identity-invariant facial expression
recognition.
Experimental results on two posed facial expression
datasets have demonstrated that the proposed IACNN model
outperformed the baseline CNN methods as well as most of
the state-of-the-art methods that exploit dynamic information
extracted from image sequences. More importantly, IACNN
has shown promise on a spontaneous facial expression
dataset, which demonstrates its effectiveness in real world.
In the future, we plan to recognize face expressions from
videos by incorporating temporal information in the proposed
IACNN model.
VI. A CKNOWLEDGEMENT
This work is supported by National Science Foundation
under CAREER Award IIS-1149787.
REFERENCES
[1]A. Asthana, S. Zafeiriou, S. Cheng, and M. Pantic. Robust discrimi-
native response map Ô¨Åtting with constrained local models. In CVPR ,
pages 3444‚Äì3451, 2013.
564
564
564
564
564
[2]T. Baltrusaitis, M. Mahmoud, and P. Robinson. Cross-dataset learning
and person-speciÔ¨Åc normalisation for automatic action unit detection.
InFG, volume 6, pages 1‚Äì6, 2015.
[3]J. Chen, X. Liu, P. Tu, and A. Aragones. Learning person-speciÔ¨Åc
models for facial expression and action unit recognition. Pattern
Recognition Letters , 34(15):1964‚Äì1970, 2013.
[4]S. Chopra, R. Hadsell, and Y . LeCun. Learning a similarity metric
discriminatively, with application to face veriÔ¨Åcation. In CVPR ,
volume 1, pages 539‚Äì546, 2005.
[5]W. Chu, F. De la Torre, and J. Cohn. Selective transfer machine for
personalized facial expression analysis. IEEE T-PAMI , 2016.
[6]A. Dhall, O. Ramana Murthy, R. Goecke, J. Joshi, and T. Gedeon.
Video and image based emotion recognition challenges in the wild:
Emotiw 2015. In ICMI , pages 423‚Äì426. ACM, 2015.
[7]I. J. Goodfellow, D. Erhan, P. L. Carrier, A. Courville, M. Mirza,
B. Hamner, W. Cukierski, Y . Tang, D. Thaler, D. Lee, et al. Challenges
in representation learning: A report on three machine learning contests.
InICML , pages 117‚Äì124. Springer, 2013.
[8]K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectiÔ¨Åers:
Surpassing human-level performance on imagenet classiÔ¨Åcation. In
ICCV , pages 1026‚Äì1034, 2015.
[9]S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep
network training by reducing internal covariate shift. In ICML , pages
448‚Äì456, 2015.
[10]S. Jain, C. Hu, and J. K. Aggarwal. Facial expression recognition with
temporal modeling of shapes. In ICCV Workshops , pages 1642‚Äì1649,
2011.
[11]Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for
fast feature embedding. In ACM MM , pages 675‚Äì678. ACM, 2014.
[12]B. Jiang, M. Valstar, and M. Pantic. Action unit detection using sparse
appearance descriptors in space-time video volumes. In FG, 2011.
[13]H. Jung, S. Lee, J. Yim, S. Park, and J. Kim. Joint Ô¨Åne-tuning in deep
neural networks for facial expression recognition. In ICCV , pages
2983‚Äì2991, 2015.
[14]T. Kanade, J. F. Cohn, and Y . Tian. Comprehensive database for facial
expression analysis. In FG, pages 46‚Äì53, 2000.
[15]H. Kaya, F. G ¬®urpinar, S. Afshar, and A. A. Salah. Contrasting and
combining least squares based learners for emotion recognition in the
wild. In ICMI , pages 459‚Äì466, 2015.
[16]B.-K. Kim, H. Lee, J. Roh, and S.-Y . Lee. Hierarchical committee of
deep cnns with exponentially-weighted decision fusion for static facial
expression recognition. In ICMI , pages 427‚Äì434, 2015.
[17]J. J. Kivinen and C. K. Williams. Transformation equivariant boltz-
mann machines. In IACNN , pages 1‚Äì9. Springer, 2011.
[18]A. Klaser, M. Marsza≈Çek, and C. Schmid. A spatio-temporal descriptor
based on 3d-gradients. In BMVC , pages 275‚Äì1, 2008.
[19]A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiÔ¨Åcation
with deep convolutional neural networks. In NIPS , pages 1097‚Äì1105,
2012.
[20]M. Liu, S. Li, S. Shan, and X. Chen. AU-aware deep networks for
facial expression recognition. In FG, pages 1‚Äì6, 2013.
[21]M. Liu, S. Li, S. Shan, R. Wang, and X. Chen. Deeply learning
deformable facial action parts model for dynamic expression analysis.
InACCV , 2014.
[22]M. Liu, S. Shan, R. Wang, and X. Chen. Learning expressionlets on
spatio-temporal manifold for dynamic facial expression recognition.
InCVPR , pages 1749‚Äì1756, 2014.
[23]P. Liu, S. Han, Z. Meng, and Y . Tong. Facial expression recognition
via a boosted deep belief network. In CVPR , pages 1805‚Äì1812, 2014.
[24]P. Liu, S. Han, and Y . Tong. Improving facial expression analysis using
histograms of log-transformed nonnegative sparse representation with
a spatial pyramid structure. In FG, pages 1‚Äì7, 2013.
[25]P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and
I. Matthews. The extended cohn-kanade dataset (ck+): A complete
expression dataset for action unit and emotion-speciÔ¨Åed expression.
InCVPR Workshops , pages 94‚Äì101, 2010.
[26]C. Mayer, M. Eggers, and B. Radig. Cross-database evaluation for
facial expression recognition. Pattern recognition and image analysis ,
24(1):124‚Äì132, 2014.
[27]Y . Miao, R. Araujo, and M. S. Kamel. Cross-domain facial expression
recognition using supervised kernel mean matching. In ICMLA ,
volume 2, pages 326‚Äì332, 2012.
[28]M. R. Mohammadi, E. Fatemizadeh, and M. H. Mahoor. Simultaneous
recognition of facial expression and identity via sparse representation.
InWACV , pages 1066‚Äì1073, 2014.
[29]A. Mollahosseini, D. Chan, and M. H. Mahoor. Going deeper in facial
expression recognition using deep neural networks. In WACV , 2015.[30]H.-W. Ng, V . D. Nguyen, V . V onikakis, and S. Winkler. Deep learning
for emotion recognition on small datasets using transfer learning. In
ICMI , pages 443‚Äì449, 2015.
[31]M. Pantic, A. Pentland, A. Nijholt, and T. S. Huang. Human computing
and machine understanding of human behavior: A survey. In T. S.
Huang, A. Nijholt, M. Pantic, and A. Pentland, editors, ArtiÔ¨Åcial
Intelligence for Human Computing , LNAI. Springer Verlag, London,
2007.
[32]M. Pantic, M. Valstar, R. Rademaker, and L. Maat. Web-based
database for facial expression analysis. In ICME , pages 5‚Äìpp. IEEE,
2005.
[33]R. Ptucha, G. Tsagkatakis, and A. Savakis. Manifold based sparse
representation for robust expression recognition without neutral sub-
traction. In ICCV Workshops , pages 2136‚Äì2143, 2011.
[34]S. Rifai, Y . Bengio, A. Courville, P. Vincent, and M. Mirza. Disentan-
gling factors of variation for facial expression recognition. In ECCV ,
pages 808‚Äì822, 2012.
[35]A. Sanin, C. Sanderson, M. T. Harandi, and B. C. Lovell. Spatio-
temporal covariance descriptors for action and gesture recognition. In
WACV , pages 103‚Äì110, 2013.
[36]E. Sariyanidi, H. Gunes, and A. Cavallaro. Automatic analysis of
facial affect: A survey of registration, representation and recognition.
IEEE T-PAMI , 37(6):1113‚Äì1133, 2015.
[37]U. Schmidt and S. Roth. Learning rotation-aware features: From
invariant priors to equivariant descriptors. In CVPR , pages 2050‚Äì2057,
2012.
[38]P. Scovanner, S. Ali, and M. Shah. A 3-dimensional sift descriptor
and its application to action recognition. In ACM MM , pages 357‚Äì360,
2007.
[39]C. Shan, S. Gong, and P. McOwan. Facial expression recognition based
on Local Binary Patterns: A comprehensive study. J. IVC , 27(6):803‚Äì
816, 2009.
[40]B. Sun, L. Li, G. Zhou, X. Wu, J. He, L. Yu, D. Li, and Q. Wei.
Combining multimodal features within a fusion network for emotion
recognition in the wild. In ICMI , pages 497‚Äì502, 2015.
[41]Y . Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface: Closing
the gap to human-level performance in face veriÔ¨Åcation. In CVPR ,
pages 1701‚Äì1708, 2014.
[42]Y . Tang. Deep learning using linear support vector machines. In ICML ,
2013.
[43]M. F. Valstar, M. Mehu, B. Jiang, M. Pantic, and K. Scherer. Meta-
analysis of the Ô¨Årst facial expression recognition challenge. IEEE
T-SMC-B , 42(4):966‚Äì979, 2012.
[44]L. Van der Maaten and G. Hinton. Visualizing data using t-sne. JMLR ,
9(2579-2605):85, 2008.
[45]J. Wang, Y . Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin,
B. Chen, and Y . Wu. Learning Ô¨Åne-grained image similarity with
deep ranking. In CVPR , pages 1386‚Äì1393, 2014.
[46]Z. Wang, S. Wang, and Q. Ji. Capturing complex spatio-temporal
relations among facial muscles for facial expression recognition. In
CVPR , pages 3422‚Äì3429, 2013.
[47]P. Yang, Q. Liu, and D. N. Metaxas. Boosting encoded dynamic
features for facial expression recognition. Pattern Recognition Letters ,
30(2):132‚Äì139, Jan. 2009.
[48]A. Yao, J. Shao, N. Ma, and Y . Chen. Capturing AU-aware facial
features and their latent relations for emotion recognition in the wild.
InICMI , pages 451‚Äì458, 2015.
[49]Z. Yu and C. Zhang. Image based static facial expression recognition
with multiple deep network learning. In ICMI , pages 435‚Äì442, 2015.
[50]A. Yuce, H. Gao, and J. Thiran. Discriminant multi-label manifold
embedding for facial action unit detection. In FG, 2015.
[51]Z. Zeng, M. Pantic, G. I. Roisman, and T. S. Huang. A survey of affect
recognition methods: Audio, visual, and spontaneous expressions.
IEEE T-PAMI , 31(1):39‚Äì58, Jan. 2009.
[52]G. Zhao and M. Pieti ¬®ainen. Dynamic texture recognition using local
binary patterns with an application to facial expressions. IEEE T-PAMI ,
29(6):915‚Äì928, June 2007.
[53]X. Zhao, X. Liang, L. Liu, T. Li, Y . Han, N. Vasconcelos, and S. Yan.
Peak-piloted deep network for facial expression recognition. In ECCV ,
pages 425‚Äì442. Springer, 2016.
[54]L. Zhong, Q. Liu, P. Yang, J. Huang, and D. Metaxas. Learning
multiscale active facial patches for expression analysis. IEEE Trans.
on Cybernetics , 45(8):1499‚Äì1510, August 2015.
[55]Y . Zong, W. Zheng, X. Huang, J. Yan, and T. Zhang. Transductive
transfer lda with riesz-based volume lbp for emotion recognition in
the wild. In ICMI , pages 491‚Äì496, 2015.
565
565
565
565
565
"
https://ieeexplore.ieee.org/document/8227443,"Image based Facial Micro-Expression Recognition 
using Deep Learning on Small Datasets 
Madhumita A. Takalkar1, Min Xu2 
1,2School of Electrical and Data Engineering 
University of Technology Sydney 
15 Broadway, Ultimo, NSW 2007, Australia 
1madhumita.a.takalkar@student.uts.edu.au, 2min.xu@uts.edu.au
 
 
Abstract ‚ÄîFacial micro-expression refers to split-second 
muscle changes in the face, indicating that a person is either 
consciously or unconsciously suppressing their true emotions and 
even mental health. Therefore, micro-expression recognition 
attracts increasing research efforts in both fields of psychology 
and computer vision. Existing research on micro-expression 
recognition has mainly used hand-crafted features, for example, 
Local Binary Pattern-Three Orthogonal Planes (LBP-TOP), 
Gabor filter and optical flow. Recently, Deep Convolutional 
neural systems have demonstrated a high degree effectiveness for 
difficult face recognition tasks. This paper explores the possible 
use of deep learning for micro-expression recognition. To develop 
a reliable deep neural network extensive training sets are 
required with a huge number of labeled image samples. However, 
micro-expression recognition is a challenging task due to the 
repressed facial appearance and short duration, which results in 
the lack of training data. In this paper, we propose to generate 
extensive training datasets of synthetic images using data 
augmentation on CASME and CASME II databases. Then, these 
datasets are combined to tune a satisfactory CNN-based micro-
expression recognizer. Experimental results demonstrate the 
effectiveness of the proposed CNN approach in image based 
micro-expression recognition and present comparable results 
with the best-related works. 
Keywords‚Äî Micro-expression recognition; deep learning; 
Convolutional Neural Network (CNN); small training data; data 
augmentation 
I.  INTRODUCTION  
Breaking down facial micro-expression is a relatively new 
research theme with much-developing interests as of late. The 
principle explanation behind naturally distinguishing the 
micro-expression is that micro- expression is a critical 
emotional sign for different real-life applications. Micro-
expression is a form of non-verbal communication that 
unconsciously reveals the true sentiments of a person. As 
compared to macro-expression, micro-expression has three 
basic qualities: short duration, subtle movement, and trouble 
about concealing [32]. Micro-expressions are exhibited subtly 
and typically occur very briefly, at a length of about 1/5 to 1/25 
of a second [7], and they usually occur in several parts of the 
face where most people do not realise [18]. Similar to macro-
expressions, micro-expressions can be grouped into six basic 
expressions: happy, surprise, anger, sad, disgust, and fear. It is not straightforward to recognise the genuine emotion 
shown on one‚Äôs face. Thus recognising micro-expressions is 
beneficial in our daily life as we can read if someone is 
attempting to cover his/her feeling or trying to deceive you. To 
solve this challenging problem, Ekman developed the Micro 
Expression Training Tool (METT) which can help people to 
detect micro-expression [3]. Some research found that it was 
still hard for a trained human to detect facial micro-expression. 
In a psychological experiment [4] conducted using METT 
micro-expression training dataset, the average micro-
expression recognition rate was 50%. To solve these problems, 
the advanced computer vision technology can achieve higher 
detection and classification performance and solve this issue. 
Deep learning is a group of machine learning methods 
biologically inspired from the structure of the brain. 
Convolutional Neural Networks (CNN) learns hierarchical 
features from multiple labelled images. It has been used for 
various applications including recognition of objects, actions, 
and places, face recognition [9, 24] and face expression 
recognition [20].  
The deep convolutional neural network has recently yielded 
excellent performance in a wide variety of image classification 
tasks. The careful design for mapping of local to global feature 
learning with convolution, pooling, and layered architecture 
renders excellent visual representation ability, making it an 
effective tool for facial micro-expression recognition. In this 
paper, we concentrate on the task of the image based static 
facial micro-expression recognition on CASME, CASME II 
and both combined (CASME+2) with deep Convolutional 
Neural Networks (CNN). The input for recognition is a raw 
image; which is then pre-processed and given as input to CNN 
to predict the facial micro-expression label which should be 
one of the labels: Disgust, Fear, Happiness, Neutral, Sadness, 
and Surprise. 
Deep learning requires a lot of data but for micro-
expression recognition, there is not enough data: there are three 
spontaneous micro-expression datasets which altogether 
contain 748 videos. It is also not possible to collect millions of 
labelled training images from the internet for micro-
expressions. Considering these limitations, we decided to 
convert the labelled micro-expression videos into frames and 
use these labelled frames for training the CNN model for image 
based micro-expression recognition. The frames are extracted 
every 0.2 seconds (or 200 milliseconds) from the video. The 
978-1-5386-2839-3/17/$31.00 ¬©2017 IEEE
number of frames per video depends on the length of the video. 
For example, for a video of length 2 seconds, 157 frames could 
be extracted. All the images extracted from a video will have 
the same label as the video. Even though we have extracted the 
frames from the videos, the number of image samples is not 
enough to train the deep neural network model. This leads us to 
the idea of combining the widely used micro-expression 
datasets to form a large dataset of training samples.   
Our significant research contributions can be summarised 
as follows: 1. We propose a CNN architecture that achieves 
satisfactory recognition accuracy on micro-expression images; 
2. We also aim to present a novel thought to combine the 
widely used micro-expression databases CASME and CASME 
II to increase the number of samples for training CNN model. 
The structure of the paper is as follows: Section II reviews 
related work; Section III discusses the existing publicly 
available and widely used datasets for micro-expressions; 
Section IV introduces our proposed micro-expression 
recognition model; Section V presents the experimental setup 
and results, and the conclusions are drawn in Section VI. 
II. RELATED WORK 
Studies in psychology demonstrate that facial features of 
expression are located around mouth, nose and eyes and their 
locations are essential for categorising facial micro-
expressions. In this part, we review some existing micro-
expression analysing approaches as well as the deep learning 
techniques. Most work on micro-expressions in the field of 
Computer Vision has mainly focused on micro-expression 
recognition [10, 19, 22, 23, 28, 29]. Micro-expression 
recognition task is defined as recognising the emotional label 
of well-segmented video containing micro-expression from 
start to end. Micro-expression analysing methods can be 
separated into two major groups, the local methods and the 
holistic methods. Initial research work on micro-expression 
recognition has been conducted on posed micro-expression. 
According to the Action Units [5], the local methods partition 
the face area into some subregions. The reported results were 
carried out within each face subregions.  Wu et al. [28] 
extracted features using Gabor filters and used Support Vector 
Machine (SVM) to recognise them. Polikovsky et al. [23] 
proposed to utilise Active Shape Model (ASM) model to detect 
facial landmarks which used to segment face area into twelve 
subregions. In each subregion, 3D-gradient orientation 
descriptor was extracted as a descriptor of facial muscle 
movement. Finally, the micro-expression video was divided 
into onset, apex and offset stages. The holistic methods 
handled the whole face for analysing the micro-expression 
category. Pfister et al. [22] proposed a spatiotemporal holistic 
feature for recognising micro-expression. More specifically, 
the algorithm detected 68 landmarks in the first sequence by 
using ASM model. Then, these landmarks were utilised to 
align face for alleviating the issue of head movements. 
Temporal Interpolation Model (TIM) was used to each 
sequence so that the lengths of all the video were normalised 
[22]. Furthermore, LBP-TOP features were used to extract the 
feature of micro-expression and SVM, Multiple Kernel 
Learning (MKL), Random Forest (RF) were used to perform 
classification. Huang et al. [10] proposed SpatioTemporal Completed Local Quantization Patterns (STCLQ) feature for 
recognition. Liu et al. [19] proposed Main Directional Mean 
Optical Flow feature and used SVM for classification. Feng et 
al. [29] calculated principal optical flow direction resulting 
Facial Dynamics Map with SVM as a classifier.  From the 
literature review, it was found that the main method for facial 
landmark localisation is ASM. 
Recently, Deep Learning [13] has become very effective 
image analysis approach, such as image classification, 
semantic segmentation, object detection and image super-
resolution. Compare to traditional hand-engineered features, 
such as Local Binary Pattern (LBP) [31] and Histogram of 
Gradients (HoG) [1, 16]; a deep convolutional neural network 
consists of multiple layers which can automatically learn 
hierarchies visual features directly from the raw image pixels. 
In the research [12], Kim et al. introduced deep learning 
features for micro-expression recognition. They proposed a 
new method consisting of two sections. First, the spatial 
features of micro-expressions at different expression states 
(onset, onset to apex transition, apex, apex to offset transition, 
and offset) are converted using convolutional neural networks 
(CNN). Next, the learned spatial features with expression-state 
constraints are transferred to learn temporal features of micro-
expression. The temporal feature learning converts the 
temporal characteristics of the different states of the micro-
expression using long short-term memory (LSTM) recurrent 
neural networks (RNNs) [12]. The time scale dependent 
information that resides along the video sequences is 
consequently learned by using LSTM. 
Recent advances in micro-expression recognition focus on 
recognising more spontaneous facial micro-expressions. The 
Chinese Academy of Sciences Micro-Expression (CASME) 
[32] and CASME II [30] were collected to mimic more 
spontaneous scenarios and contain seven basic micro-
expression categories. Both these datasets contain video clips 
which were recorded in a controlled environment. The idea is 
that videos, although not truly spontaneous, at least provide 
facial micro-expressions in a much more natural and versatile 
way than posed datasets. With the introduction of deep learning 
methods, a wide range of image classification work gives the 
finest performance. The existing works focus on spatio-
temporal features for micro-expression recognition whereas we 
concentrate on the task of image-based static facial micro-
expression recognition on CASME and CASME II with deep 
CNNs.  
III. DATABASES  
There are three publicly available spontaneous micro-
expression databases for recognition task: spontaneous micro-
expression dataset (SMIC) [15], the Chinese Academy of 
Sciences Micro-Expression (CASME) [32]and CASME II [30]. 
These datasets have recorded micro-expression faces in frontal 
view. Table I lists the key features of existing micro-expression 
databases. 
In order to evaluate the proposed architecture, we use two 
of these known databases, CASME and CASME II, as well as 
present an additional database which is an aggregation of 
CASME and CASME II, we will refer it as CASME+2 in our  
TABLE  I 
DETAILS OF EXISTING SPONTANEOUS MICRO -EXPRESSION DATABASES  
Dataset Frame 
rate 
(Fps) Subjects Samples Emotion class 
SMIC HS 100 20 164 
3 (Positive, 
Negative, Surprise) VIR 25 10 71 
NIS 25 10 71 
CASME 60 35 195 8 (Contempt, 
Disgust, Fear, 
Happiness, 
Regression, Sadness, 
Surprise, Tense) 
CASME II 200 35 247 7 (Disgust, Fear, 
Happiness, Others, 
Regression, Sadness, 
Surprise, Tense) 
 
article. 
IV. PROPOSED METHOD  
The conventional pipeline consists of four stages: 1) face 
detection; 2) pre-processing; 3) feature extraction; and 4) 
classification. Fig. 1 shows the basic block diagram of the 
micro-expression recognition. In this section, we describe the 
whole structure of our micro-expression recognition. We 
explain the method to increase the number of samples in the 
dataset using data augmentation in Section A. Section B 
discusses the preparation of the training, testing and validation 
datasets for training and validating the developed CNN model. 
Section C explains about the method applied for face detection 
and pre-processing, i.e. steps 1 and 2 of the block diagram. 
Section D represents steps 3, and 4 of the block diagram where 
the deep network does the feature extraction and classification. 
We developed CNN with variable depths to evaluate the 
performance of our model for facial micro-expression 
recognition. 
A. Data Augmentation 
The absence of large training datasets is a crucial 
bottleneck that keeps the utilisation of profound (deep) learning 
techniques in such cases, as the models will overfit drastically 
when utilising small training datasets. To address this issue, a 
large number of strategies have been proposed: fine-tuning 
models trained from other large public datasets (e.g. ImageNet 
[2]), using the big synthetic training datasets explored by some 
authors [8, 12, 14]. 
There are two critical points of interest of utilising synthetic 
data (i) one can produce the same number of training samples 
as required, and (ii) it permits explicit control over the 
unwanted factors. Data augmentation is a technique that is 
commonly used to reduce the scarcity problem. It is a set of 
label-preserving transforms that introduce some new instances 
without collecting the new data. In this, the existing training 
images are transformed without affecting the semantic class 
label. Examples of such transformations are horizontal/vertical 
mirroring [17], cropping, small rotations, etc. Flipping and  
 
Fig. 1. General block diagram of micro-expression recognition 
 
TABLE  II 
MICRO -EXPRESSION DATABASE AFTER DATA AUGMENTATION  
 
Database Original After augmentation 
CASME 26,423 52,846 
CASME II 46,416 92,832 
CASME+2 69,520 1,39,040 
 
mirroring images vertically or horizontally producing two 
samples of each is a commonly used data augmentation 
technique for face recognition. In our evaluations, both original 
and vertically mirrored images are used for training. Table II 
demonstrates the data augmentation process has doubled the 
number of samples. 
B. Data preparation 
  Training and testing of the model have carried on images 
from CASME, CASME II and CASME+2 databases which 
comprise of human faces; each labelled with one of 5 emotion 
categories: disgust, fear, happiness, sadness, and surprise. We 
have also taken into consideration the 6th category as ‚Äòneutral‚Äô. 
The given images are divided into two different sets which are 
training and testing sets. For data augmentation, mirrored 
images were generated by flipping images vertically. The 
training set comprises of 80% of the total images in the 
synthetic database and remaining 2% of the images are further 
divided as the testing set (1%) and validation set (1%). 
C. Face detection and Pre-processing 
We note that all the images are pre-processed so that they 
form a bounding box around the face region. Using the raw 
images from the CASME and CASME II database, we 
implemented the DLib face detector in OpenCV to detect and 
crop the face region from the raw image. The cropped face is 
then processed for head pose correction by computing the 
angle between the eye centroids and later applying the affine 
transformation. The transformed image is again passed to DLib 
face detector to crop and save the more accurate face region.  
Fig. 2. shows the face detection and pre-processing steps. 
The face region is detected from the raw image frame given as 
the input. The Fig. 2. shows the red bounding box around the 
detected face region and the cropped face image. Since the 
frames are extracted from the videos, there is slight head 
movement observed in some of the videos. In the pre-
processing step, the head pose correction method is applied to 
the cropped face. The head pose is aligned with the use of 
affine transformation, and the face region is cropped again to 
ensure that only the face region is given as input to the CNN 
model. 
 
Fig. 3.The CNN architecture of our deep convolutional neural network
 
Fig. 2. Face detection and Pre-processing raw face images  
D. Convolutional Neural Network (CNN) framework 
Fig. 3. describes a basic framework for facial micro-
expression recognition using the deep convolutional neural 
network. The pre-processed and cropped face image is passed 
as input to CNN model where the image has to pass through 
different layers of CNN: 1) Convolutional; 2) Rectified Linear 
Unit (ReLU); 3) Pooling or Sub sampling, and 4) Classification 
(Fully Connected Layer). 
 The primary purpose of the Convolutional step is to extract 
features from the input image. It maintains the spatial 
relationship between pixels by learning image features using 
small squares of input data and creates a feature map. ReLU is 
a non-linear operation. ReLU is a component wise operation 
(applied per pixel) and replaces all negative pixel values in the 
feature map by zero. Convolution is a linearity process which 
is element wise matrix multiplication and addition, so we 
represent non-linearity by presenting a non-linear function like 
ReLU. Spatial Pooling (also called subsampling or 
downsampling) reduces the dimensionality of each feature map 
but retains the most important information. In case of Max 
Pooling, the largest element from the rectified feature map 
within that window is taken. 
Unitedly these layers select the useful features from the 
images, embed non-linearity in the network and reduce feature 
dimension while intending to make the features to some degree 
equivariant to scale and translation. The output of the third 
pooling layer acts as an input to the Fully Connected Layer. 
The Fully Connected Layer is a conventional Multi Layer 
Perceptron that uses a Softmax initiation function in the output 
layer (different classifiers like SVM can likewise be utilised,  
 
Fig. 4. Deep Network configuration  
however we are utilising Softmax). The output of the 
convolutional and pooling layers constitute high-level features 
of the input image. The purpose of the Fully Connected layer is 
to utilise these features for classifying the input image into 
several classes based on the training dataset. 
Putting it all together, the Convolution and Pooling layers 
act as Feature Extractors from the input image while the Fully 
Connected later serves as a Classifier. 
The network contains five convolutional layers where the 
Conv1 layer has 11x11 filters and rest with 3x3 filters, three 
max pooling layers of kernel size 3x3, stride 2 and three fully 
connected layers (Fig. 4). The fully connected layers contain 
dropout, a mechanism for randomization which reduces the 
risk of the network over fitting. The Rectified Linear Unit 
(ReLU) was used for activation function. The system operates 
in two main phases: Training and Testing. During training, the 
system receives a training data comprising of images of faces 
with their respective micro-expression label. To ensure that the 
training performance is not affected by order of presentation of 
the examples, a few images are separated as a validation set. 
During testing, the images in the validation set are fed to the 
network which outputs the predicted micro-expression label 
using the final network weights learned during training. The 
base learning rate (base_lr) for the model is set to 0.001, 
stepsize parameter is set to 10,000, and maximum iterations 
(max_iter) are 100,000. The batch size of the images is 50 
images per batch for training. We also changed the learning 
policy parameter (lr_policy) value to ‚Äústep‚Äù where the learning 
rate will drop at every step size. The rest of the parameter 
settings were used the default. We used a baseline Softmax 
classifier.  
The VGGFace is a network trained on a very large-scale 
face images dataset (2.6M images, 2.6k people) for the task of 
face recognition available from Visual Geometry Group at the 
University of Oxford [21]. Since the dataset was trained for a 
similar application but on a much larger dataset than ours, we 
tried fine-tuning the model for micro-expressions. 
V. EXPERIMENT AND RESULTS  
The proposed micro-expression recognition verifies the 
effectiveness by conducting experiments on the CASME, 
CASME II and CASME+2 datasets. All the images in the 
databases are pre-processed and flipped vertically to increase 
the number of samples. The new synthetic database is then 
divided into two groups as Training and Testing. Each image 
has been categorized as: 0 = Disgust, 1 = Fear, 2 = Happiness, 
3 = Neutral, 4 = Sadness, and 5 = Surprise. 
We implemented the deep convolutional neural networks 
based on the Caffe [11] (a fast open framework for deep 
learning and computer vision) and took 10-12 hours to train 
this network. The models were trained for 100,000 iterations 
on CASME and 41,000 iterations on CASME II and 
CASME+2. The learning rate is changed from 0.001 to 0.0001 
when the training iteration reaches 10,000. At each round of 
iterations in model training, the layer parameters of the 
network are updated based on the loss. We set a maximum 
number of iterations, and when the training time reaches the 
number, we obtain a trained model, which is essentially the 
parameter of all the filters. We then save the model so we can 
use the model to predict a micro-expression of images. 
The input is given from the Validation sets which are the 
raw face images collected from the original databases. For each 
experiment, a corresponding Validation set is used depending 
on the training database. The confusion matrixes for each 
experiment are as shown in the Figures 5, 6 and 7. 
The recognition accuracy results for the three databases 
used are summarised in Table III. From the table, we can 
observe that the recognition accuracy improves as the number 
 
Fig. 5.   Confusion matrix on CASME database 
 
TABLE  III 
MICRO -EXPRESSION RECOGNITION ACCURACY WITH DIFFERENT DATABASES  
 
Database CASME CASME II CASME+2
Accuracy 74.25% 75.57% 78.02% 
 
Fig. 6.   Confusion matrix on CASME II database 
 
 
Fig. 7.   Confusion matrix on CASME + CASME II database 
 
 
of training samples increases. It can be interpreted from the 
results in Table II that the larger the training dataset is, the 
better the recognition accuracy is achieved. 
 Tables IV and V lists the recognition accuracy of using our 
method and of the state-of-the-art methods in CASME dataset 
and CASME II dataset respectively. Most of the existing 
methods are video based, which more or less take advantage of 
videos temporal information. Our method is image based, 
which applies CNN on image frames extracted from videos. 
The tables testify that proposed CNN method exhibits 
satisfying micro-expression recognition accuracy. These results 
also demonstrate that image based micro-expression 
recognition delivers identical results as video based 
approaches. 
Due to larger number samples in CASME II dataset as 
compared to CASME dataset, the researchers in [12] opted to 
demonstrate deep learning results on video clips from CASME 
II dataset. In our research, we have applied data augmentation 
technique to increase the number of samples. Therefore, our 
experiments use both CASME and CASME II datasets to 
showcase the effectiveness of proposed CNN method. 
 
TABLE  IV 
MICRO -EXPRESSION RECOGNITION ON CASME  DATABASE  
 
Method Accuracy 
 LBP-TOP+ELM [6] 73.82% 
MDMO+SVM [19] 68.86% 
LBP-TOP+SVM [25] 61.85% 
Proposed CNN method 74.25% 
 
 
TABLE  V 
MICRO -EXPRESSION RECOGNITION ON CASME  II DATABASE  
 
Method Accuracy 
LBP-TOP+SVM [26] 75.30% 
MDMO+SVM [19] 67.37% 
CNN+LSTM [12] 60.98% 
Proposed CNN method 75.57% 
 
In [6, 12, 19, 25, 26], the researchers have considered the 
temporal factor from the video for recognition of micro-
expressions which have contributed an additional feature in 
the calculations. In case of our method, we tried to eliminate 
the temporal factor and simply exhibit the image based micro-
expression recognition approach.  
A. Difficulties with certain expressions 
We observe that some classes appear to be ‚Äúharder‚Äù to train 
in the sense that, (1) none of our models were able to score 
highly on them, and (2) our model predictions for those classes 
tend to fluctuate depending on our training scheme and 
architecture. This appears to be the case for classes such as 
‚Äúfear‚Äù, ‚Äúhappiness‚Äù, and ‚Äúsadness‚Äù. There might be two 
reasons discussed as follows. First, these classes tend to have 
much fewer training samples compared to classes such as 
‚Äúdisgust‚Äù, ‚Äúneutral‚Äù, and ‚Äúsurprise‚Äù, making it harder to train 
the CNN to recognise them. Second, these micro-expressions 
can be very nuanced, making it difficult even for humans to 
agree on their correct labelling [27]. 
We suspect that the inherent difficulty in assigning labels to 
some of the samples may have caused them to be 
‚Äúmislabelled‚Äù, thereby affecting the models that were trained 
on them. Lastly, we note that all except one model (CASME II) 
were unable to predict a sufficient number of samples for label 
‚Äúfear‚Äù correctly. The reason for this could be an imbalance in 
the training datasets. The imbalance in the number of training 
samples for each class of micro-expressions most likely caused 
our models to overfit the micro-expressions with more samples 
(e.g. ‚Äúdisgust‚Äù) at the expense of this class. Furthermore, the 
expression of fear is very subtle, which means that it will be 
hard for our CNN models to discover features to robustly 
distinguish this micro-expression from other similarly nuanced 
expressions such as sad, happiness, and surprise. This can be 
verified by examining the confusion matrices in Figure 3, 4 and 
5, which indicates that the ‚Äúdisgust‚Äù class is often the highest 
scoring class. The classes ‚Äúfear‚Äù, ‚Äúneutral‚Äù, ‚Äúhappiness‚Äù, ‚Äúsadness‚Äù, and ‚Äúsurprise‚Äù are often mistaken for each other by 
our models. 
The combination of these two factors makes it even harder 
to train models to predict this micro-expression accurately. The 
above observations highlight the difficulty in training CNNs 
using a small unbalanced dataset with classes that are not 
visually distinctive. 
VI. CONCLUSION  
We have shown that it is possible to obtain a significant 
improvement in accuracy over the baseline results for micro-
expression classification using CNNs pre-trained model 
utilised for the task for face recognition and fine-tuning it on 
face micro-expression databases. The experiments also 
conclude that the small sizes of the datasets do not favour them 
for being used for training CNNs. However, CNNs trained on 
sufficiently large face micro-expression datasets also can be 
used to obtain better results than the baseline without using the 
data augmentation technique to increase the size of the dataset 
artificially. This suggests that if we were to exploit deep neural 
networks such as CNN for face micro-expression recognition 
to achieve the significant gains seen in other domains, then 
having bigger datasets is crucial. This is where we implanted 
the idea of combining the two databases CASME and CASME 
II to form a larger database. Image based face expression 
recognition is a popular research topic, but we demonstrated 
through our experiments that image based micro-expression 
recognition could also yield acceptable accuracy. 
Lastly, we also noted the inherent difficulty in assigning 
correct labels to faces depicting some of the more nuanced 
micro-expressions and how that can affect the performance of 
our models. 
REFERENCES  
 
[1] G. K. Chavali, S. K. N. Bhavaraju, T. Adusumilli, and V. 
Puripanda, ""Micro-Expression Extraction For Lie Detection Using 
Eulerian Video (Motion and Color) Magnication,"" 2014. 
[2] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, 
""Imagenet: A large-scale hierarchical image database,"" in 
Computer Vision and Pattern Recognition, 2009. CVPR 2009. 
IEEE Conference on , 2009, pp. 248-255. 
[3] P. Ekman, Micro Expressions Training Tool : Emotionsrevealed. 
com, 2003. 
[4] J. Endres and A. Laidlaw, ""Micro-expression recognition training 
in medical students: a pilot study,"" BMC Medical Education, vol. 
9, p. 47, 2009. 
[5] E. Friesen and P. Ekman, ""Facial action coding system: a 
technique for the measurement of facial movement,"" Palo Alto, 
1978. 
[6] Y. Guo, C. Xue, Y. Wang, and M. Yu, ""Micro-expression 
recognition based on CBP-TOP feature with ELM,"" Optik-
International Journal for Light and Electron Optics, vol. 126, pp. 
4446-4451, 2015. 
[7] C. House and R. Meyer, ""Preprocessing and Descriptor Features 
for Facial Micro-Expression Recognition,"" 2015. 
[8] G. Hu, X. Peng, Y. Yang, T. Hospedales, and J. Verbeek, 
""Frankenstein: Learning deep face representations using small 
data,"" arXiv preprint arXiv:1603.06470, 2016. 
[9] G. Hu, Y. Yang, D. Yi, J. Kittler, W. Christmas, S. Z. Li , et al. , 
""When face recognition meets with deep learning: an evaluation of 
convolutional neural networks for face recognition,"" in 
Proceedings of the IEEE International Conference on Computer 
Vision Workshops , 2015, pp. 142-150. 
[10] X. Huang, G. Zhao, X. Hong, W. Zheng, and M. Pietik√§inen, 
""Spontaneous facial micro-expression analysis using 
Spatiotemporal Completed Local Quantized Patterns,"" 
Neurocomputing, vol. 175, pp. 564-578, 2016. 
[ 1 1 ]  Y .  J i a ,  E .  S h e l h a m e r ,  J .  D o n a h u e ,  S .  K a r a y e v ,  J .  L o n g ,  R .  
Girshick , et al. , ""Caffe: Convolutional architecture for fast feature 
embedding,"" in Proceedings of the 22nd ACM international 
conference on Multimedia , 2014, pp. 675-678. 
[12] D. H. Kim, W. J. Baddar, and Y. M. Ro, ""Micro-Expression 
Recognition with Expression-State Constrained Spatio-Temporal 
Feature Representations,"" in Proceedings of the 2016 ACM on 
Multimedia Conference , 2016, pp. 382-386. 
[13] Y. LeCun, Y. Bengio, and G. Hinton, ""Deep learning,"" Nature, 
vol. 521, pp. 436-444, 2015. 
[14] W. Li, M. Li, Z. Su, and Z. Zhu, ""A deep-learning approach to 
facial expression recognition with candid images,"" in Machine 
Vision Applications (MVA), 2015 14th IAPR International 
Conference on , 2015, pp. 279-282. 
[15] X. Li, T. Pfister, X. Huang, G. Zhao, and M. Pietik√§inen, ""A 
spontaneous micro-expression database: Inducement, collection 
and baseline,"" in Automatic Face and Gesture Recognition (FG), 
2013 10th IEEE International Conference and Workshops on , 
2013, pp. 1-6. 
[16] X. Li, H. Xiaopeng, A. Moilanen, X. Huang, T. Pfister, G. Zhao , et 
al., ""Towards Reading Hidden Emotions: A Comparative Study of 
Spontaneous Micro-expression Spotting and Recognition 
Methods,"" IEEE Transactions on Affective Computing, 2017. 
[17] X. Li, J. Yu, and S. Zhan, ""Spontaneous facial micro-expression 
detection based on deep learning,"" in Signal Processing (ICSP), 
2016 IEEE 13th International Conference on , 2016, pp. 1130-
1134. 
[18] S.-T. Liong, J. See, K. Wong, A. C. Le Ngo, Y.-H. Oh, and R. 
Phan, ""Automatic apex frame spotting in micro-expression 
database,"" in Pattern Recognition (ACPR), 2015 3rd IAPR Asian 
Conference on , 2015, pp. 665-669. 
[19] Y. J. Liu, J. K. Zhang, W. J. Yan, S. J. Wang, G. Zhao, and X. Fu, 
""A Main Directional Mean Optical Flow Feature for Spontaneous 
Micro-Expression Recognition,"" IEEE Transactions on Affective 
Computing, vol. PP, pp. 1-1, 2015. 
[20] Y. Lv, Z. Feng, and C. Xu, ""Facial expression recognition via deep 
learning,"" in Smart Computing (SMARTCOMP), 2014 
International Conference on , 2014, pp. 303-308. 
[21] O. M. Parkhi, A. Vedaldi, and A. Zisserman, ""Deep Face 
Recognition,"" in BMVC , 2015, p. 6. 
[22] T. Pfister, X. Li, G. Zhao, and M. Pietik√§inen, ""Recognising 
spontaneous facial micro-expressions,"" in 2011 International 
Conference on Computer Vision , 2011, pp. 1449-1456. 
[23] S. Polikovsky, Y. Kameda, and Y. Ohta, ""Facial micro-expressions 
recognition using high speed camera and 3D-gradient descriptor,"" 
in Crime Detection and Prevention (ICDP 2009), 3rd International 
Conference on , 2009, pp. 1-6. 
[24] Y. Taigman, M. Yang, M. A. Ranzato, and L. Wolf, ""Deepface: 
Closing the gap to human-level performance in face verification,"" 
in Proceedings of the IEEE Conference on Computer Vision and 
Pattern Recognition , 2014, pp. 1701-1708. 
[25] S. Wang, W.-J. Yan, X. Li, G. Zhao, and X. Fu, ""Micro-expression 
Recognition Using Dynamic Textures on Tensor Independent 
Color Space,"" in ICPR , 2014, pp. 4678-4683. 
[26] Y. Wang, J. See, Y.-H. Oh, R. C.-W. Phan, Y. Rahulamathavan, 
H.-C. Ling , et al. , ""Effective recognition of facial micro-
expressions with video motion magnification,"" Multimedia Tools 
and Applications, pp. 1-26, 2016. 
[27] S. C. Widen, J. A. Russell, and A. Brooks, ""Anger and disgust: 
Discrete or overlapping categories,"" in 2004 APS Annual 
Convention, Boston College, Chicago, IL , 2004. 
[28] Q. Wu, X. Shen, and X. Fu, ""The machine knows what you are 
hiding: an automatic micro-expression recognition system,"" in 
International Conference on Affective Computing and Intelligent 
Interaction , 2011, pp. 152-162. [29] F. Xu, J. Zhang, and J. Wang, ""Microexpression Identification and 
Categorization using a Facial Dynamics Map,"" IEEE Transactions 
on Affective Computing, vol. PP, pp. 1-1, 2016. 
[30] W.-J. Yan, X. Li, S.-J. Wang, G. Zhao, Y.-J. Liu, Y.-H. Chen , et 
al., ""CASME II: An improved spontaneous micro-expression 
database and the baseline evaluation,"" PloS one, vol. 9, p. e86041, 
2014. 
[31] W.-J. Yan, S.-J. Wang, Y.-H. Chen, G. Zhao, and X. Fu, 
""Quantifying micro-expressions with constraint local model and 
local binary pattern,"" in Workshop at the European Conference on 
Computer Vision , 2014, pp. 296-305. 
[32] W.-J. Yan, Q. Wu, Y.-J. Liu, S.-J. Wang, and X. Fu, ""CASME 
database: A dataset of spontaneous micro-expressions collected 
from neutralized faces,"" in Automatic Face and Gesture 
Recognition (FG), 2013 10th IEEE International Conference and 
Workshops on , 2013, pp. 1-7. 
 
"
https://ieeexplore.ieee.org/document/9674818,Error
https://ieeexplore.ieee.org/document/6874505,"Intra-Class Variation Reduction Using Training
Expression Images for Sparse Representation
Based Facial Expression Recognition
Seung Ho Lee, Student Member, IEEE , Konstantinos N. (Kostas) Plataniotis, Fellow, IEEE , and
Yong Man Ro, Senior Member, IEEE
Abstract‚Äî Automatic facial expression recognition (FER) is becoming increasingly important in the area of affective computing
systems because of its various emerging applications such as human-machine interface and human emotion analysis. Recently,
sparse representation based FER has become popular and has shown an impressive performance. However, sparse representation
could often produce less meaningful sparse solution for FER due to intra-class variation such as variation in identity or illumination. This
paper proposes a new sparse representation based FER method, aiming to reduce the intra-class variation while emphasizing the
facial expression in a query face image. To that end, we present a new method for generating an intra-class variation image of each
expression by using training expression images. The appearance of each intra-class variation image could be close to the appearance
of the query face image in identity and illumination. Therefore, the differences between the query face image and its intra-class
variation images are used as the expression features for sparse representation. Experimental results show that the proposed FER
method has high discriminating capability in terms of improving FER performance. Further, the intra-class variation images of
non-neutral expressions are complementary with that of neutral expression, for improving FER performance.
Index Terms‚Äî Facial expression recognition (FER), sparse representation based classiÔ¨Åer (SRC), intra-class variation, facial expression
features
√á
1I NTRODUCTION
AUTOMATIC facial expression recognition (FER) has been
an interesting and challenging research topic in the
area of affective computing systems because of its growing
applications such as human-machine interface and human
emotion analysis [1], [32]. Recently, the advantages of
exploiting sparsity in pattern classiÔ¨Åcation have been exten-
sively demonstrated in [2], [3], [4], [5], [6], [7], [28]. In [2],
Wright et al. suggested a framework for face recognition
(FR) using sparse representation. The experimental results
of [2] showed that the sparse representation based classiÔ¨Åer
(SRC) was superior to other widely used classiÔ¨Åers under
challenging FR conditions. Inspired by the successful use in
FR, SRC was studied for the purpose of FER in [4], [5], [6],
[7], [28]. Most of the FER methods based on SRC made use
of features which capture intensity changes of face appear-
ance itself (i.e., appearance based feature). Huang et al.
attempted to combine the SRC with local binary patterns
(LBPs) [4] features, a very powerful method to describe tex-
ture and shapes of image [8]. Experimental result in [4]showed that SRC using LBP features could achieve better
FER performance than SRC using raw pixels of face images.
In [5], it was shown that SRC outperformed nearest neigh-
bor (NN) [10] and support vector machine (SVM) classiÔ¨Åers
[11], independent of features used (e.g., raw pixel or LBP or
Gabor wavelet [9]). In [6] and [28], the effectiveness of SRC
with appearance based features such as two-dimensional
principal component analysis (2DPCA) [12] and local phase
quantization (LPQ) [24] was experimentally veriÔ¨Åed. In the
previous works in [4], [5], [6], [28], the authors showed that
SRC could achieve some improvements in FER performance
by using appearance based features. However, the experi-
ments were performed only on data sets created under rela-
tively well controlled environment (e.g., face images with
high intensity expression, and without illumination varia-
tion). In addition, under SRC framework, the use of such
appearance based features may not be a straightforward
method. This is mainly because facial identity of person is
often confused with facial expressions [7].
The authors of [7] tried to extract a meaningful expres-
sion feature of SRC by introducing difference image
between query face image and neutral face image for the
purpose of reducing the effect of identity information con-
tained in face image. However, this method did not
consider the effect of varying image acquisition conditions
(e.g., illumination change) between face images. The
authors of [7] also introduced a geometric feature for SRC,
called facial grids, which used the difference of node coor-
dinates between neutral face image and expressive face
image. However, tracking (by Kanade-Lucas-Tomasi (KLT)
tracker in [13]) of facial feature points could be highly/C15S.H. Lee and Y.M. Ro are with the Department of Electrical Engineering,
Korea Advanced Institute of Science and Technology (KAIST), Republic of
Korea. E-mail: leesh09@kaist.ac.kr, ymro@ee.kaist.ac.kr.
/C15K.N. Plataniotis is with the Department of Electrical and Computer Engi-
neering, University of Toronto, Toronto, ON, Canada.
E-mail: kostas@comm.utoronto.ca.
Manuscript received 28 Aug. 2013; revised 21 July 2014; accepted 1 Aug.
2014. Date of publication 7 Aug. 2014; date of current version 30 Oct. 2014.
Recommended for acceptance by Q. Ji.
For information on obtaining reprints of this article, please send e-mail to:
reprints@ieee.org, and reference the Digital Object IdentiÔ¨Åer below.
Digital Object IdentiÔ¨Åer no. 10.1109/TAFFC.2014.2346515340 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 5, NO. 3, JULY-SEPTEMBER 2014
1949-3045 /C2232014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
sensitive against illumination change [14], leading to deg-
radation in FER performance. Another limitation of the
both features in [7] was that the identity of query face
image needed to be known and its neutral face image
needed to be obtained in advance. In practice, however, it
is generally difÔ¨Åcult to satisfy these requirements in real-
world applications.
The above-mentioned methods of [4], [5], [6], [7], [28] do
not consider dealing with appearance variation either in
subject or in image acquisition condition. This appearance
variation is regarded as intra-class variation. Because of
intra-class variation, sparse representation is likely to con-
tain coefÔ¨Åcients corresponding to both similar expression
and similar appearance such as identity and illumination,
which is not desirable for FER.
The aim of this paper is to propose a new sparse repre-
sentation based FER method which is robust to the afore-
mentioned intra-class variation. The main contribution of
our paper is twofold:
1) For a query face image, we propose the so-called
intra-class variation image which is generated by using
the training face expression images. The intra-class
variation image is similar to the query face image in
terms of identity and illumination. To extract expres-
sion features, we subtract the intra-class variation
image from the query face image. The expression dif-
ference is emphasized while intra-class variation due
to identity and illumination is reduced. We demon-
strate that the use of the proposed intra-class varia-
tion images can improve facial expression sparsity,
which leads to high FER performance.
2) A facial expression is generally deÔ¨Åned by begin and
end with a neutral state [15]. For this reason, in some
FER methods, the difference between neutral face
and query face was used as expression feature [7],
[16], [17]. However, face image of neutral state is not
always available in reality [15]. To cope with this
issue, we investigate that expressive (or non-neutral)
state can be used for extracting facial expression. As
described in Section 2.1 and Fig. 3, expressive state
could also be useful for reducing identity information
and emphasizing the expression of a query face
image. Experimental results show that difference
between query face image and intra-class variation
image of a non-neutral expression can yield similar
discriminating power to difference using intra-class
variation image of neutral expression. Further, differ-
ences made by all expression states in training images
are able to signiÔ¨Åcantly improve FER performance.
From the extensive and comparative experiments on the
JAFFE [18], Cohn-Kanade (CK √æ) [37], MMI [38], BU-3DFE
[20], and CMU Multi-PIE [19] databases (DBs), the proposed
method is found to be effective in both person-dependent
and‚Äîindependent recognition conditions (detailed in
Section 3.1). In particular, substantial FER improvement (of
up to 21 percent enhancement) can be achieved by the pro-
posed method in person-independent recognition, com-
pared to the case of employing SRC without using the
subtraction of the intra-class variation image. In addition, it
has been demonstrated that the proposed method can befeasible for sparse representation based FER even in the
presence of variation in illumination or expression intensity.
The remainder of the paper is organized as follows:
Section 2 presents the proposed method for extracting facial
expression features. In Section 3, experimental results are
presented followed by conclusions in Section 4.
2P ROPOSED FACIAL EXPRESSION RECOGNITION
METHOD
As shown in Fig. 1a, the feature extractions of a query face
image largely consist of two sequential steps: 1) generation
of intra-class variation image for each expression and 2)
subtraction of the intra-class variation image from the query
face image. As is seen in Fig. 1a, training face images of
each expression are used for the generation of correspond-
ing intra-class variation image for the query face image. The
intra-class variation image would present the same facial
expression as the corresponding training face images.
After obtaining the intra-class variation image for every
expression, we subtract each intra-class variation image
from the query face image. Since the intra-class variation
image is similar to the query face image in appearance other
than expression, each subtraction image is likely to high-
light the expression difference between the query face
image and the intra-class variation image. As demonstrated
in our experiment, each of the subtraction images can be
discriminative for FER.
In classiÔ¨Åcation phase, we separately apply sparse repre-
sentation to the individual expression feature, i.e., subtrac-
tion image. Dictionary [2] constructions of training
expression features are depicted in Fig. 1b. Sparsity based
classiÔ¨Åcation using the dictionaries is performed as shown
in Fig 1a.
Detailed descriptions on the feature extractions and the
classiÔ¨Åcation are given in the following sections.
2.1 Generations of Intra-Class Variation Images and
Expression Feature Extractions
In this section, we describe the extractions of expression
features from a given query face image, q2<N. LetFF¬º
¬ΩFF1;FF2;...;FFC/C1382<N/C2Mdenotes the training set that con-
tains (vectorized) training face images with Cdifferent
types of expression classes. Nis the number of dimensions
of each face image and Mis the total number of training
face images. In addition, FFi¬º¬Ωti;1;ti;2;...;ti;Mi/C1382<N/C2Mi
denotes the training set of the ith expression class. Miis the
number of training face images in FFiso that M¬ºPC
i¬º1Mi:
ti;j2<Ncorresponds to the jth training face image of the
ith expression class.
For the query face image q;we generate intra-class varia-
tion image hq
i2<Nby using FFi:SpeciÔ¨Åcally, the intra-class
variation image hq
iis generated based on an approximation
of query face image that is a linear combination of the train-
ing face images of FFi:For the approximation, we need a
weight vector wq
i¬º¬Ωw1;w2;...;wMi/C138T2<Mi:The optimal
weight vector ^wq
ican be obtained by employing the follow-
ing regularized least square method:
^wq
i¬ºarg min/C8/C13/C13q/C0FFiwq
i/C13/C132
2√æ/C21/C13/C13wq
i/C13/C132
2g; (1)LEE ET AL.: INTRA-CLASS VARIATION REDUCTION USING TRAINING EXPRESSION IMAGES FOR SPARSE REPRESENTATION BASED... 341
where k/C1k2is‚Äò2-norm of a vector. Note that in (1), the
term jq/C0FFiwq
ij2
2corresponds to the reconstruction error,
while the jwq
ij2
2is regularized term which aims at making
the solution (i.e., weight vector) ^wq
iin a stable way [21].
In this paper, the ^wq
iin (1) is derived by computing^wq
i¬º√∞FFT
iFFi√æ/C21/C1I√û/C01FFT
iqi nt h es a m ew a ya sd e s c r i b e d
in [21], where the /C21is a regularization parameter (set to
0.0001 in this paper) and I2<Mi/C2Miis an identity
matrix. Note that the matrix √∞FFT
iFFi√æ/C21/C1I√û/C01FFT
iis inde-
pendent of q:Hence, this matrix is pre-calculated as a
Fig. 1. The frameworks for (a) extracting the expression features of a query face image and classifying the query face image using sparsity of the
expression features, (b) constructing dictionaries of the expression features of training face images for the sparsity based classiÔ¨Åcation.342 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 5, NO. 3, JULY-SEPTEMBER 2014
projection matrix, which yields efÔ¨Åciency in computation
time. Using the weight vector ^wq
i;we can express the
intra-class variation image hq
ifor the query face image q
as the following linear representation:
hq
i¬ºFFi^wq
i: (2)
In (2), each element of ^wq
iis the weight value describing the
contribution of the associated training face image in FFi:hq
i
presents a single expression state ( ith expression) that is com-
ing from the training face images of the corresponding ith
expression. Figs. 2a and 2b show intra-class variation images
of neutral and surprise expressions, which are generated
using (1) and (2) for the corresponding query face images.
Their appearances (e.g., illumination or accessories such as
glasses) are similar to their query faces except for expression
appearances. Hence, by using (1), it can be expected that each
intra-class variation image has larger contribution from the
training face images which have similar appearance of iden-
tity and illumination to query face image. Taking difference
between the query face image and its intra-class variation
image could reduce the effect of unnecessary appearance (i.e.,
intra-class variation) contained in face image.
After obtaining the Cintra-class variation images hq
i
√∞i¬º1;...;C√û‚Äìeach corresponding for one of the Cexpres-
sions in the training set, the expression features yq
i2<N
√∞i¬º1;...;C√ûof the query face image qcan be computed as
yq
i¬ºq/C0hq
i: (3)
Note that the intra-class variation image hq
iis obtained
by combining the training face images of the ith expres-
sion. As a result, each expre ssion feature is well-suited
for highlighting expression di fference (to a large extent)
between the intra-class variation image hq
iand the query
face image q:
The motivation for using the difference between two
expressive faces (i.e., the query face image qand the
intra-class variation image hq
i) as feature is explained as
follows. It has been demonstrated in [7], [16] that the useof the difference between an expressive query face and a
neutral face of the same subject leads to the normalization
that is capable of reducing the identity information in the
query face. Note that human f acial expressi on associated
with a basic emotion (e.g., Happy) can be represented as
a combination of action units (AUs) [1], [15], [37] and cer-
tain AU types should appear in the combination [37] (For
example, a face with ‚ÄòHappy‚Äô emotion should include
AU12 (Lip Corner Puller) [37]). For this reason, typical
faces with an emotional expression, normalized by their
neutral faces could look sim i l a rt os o m ee x t e n ti no t h e r
subjects. This makes FER discriminative. The normaliza-
tion effect and discriminating capability can be extended
to the difference between two expressive faces . Let us consider
an example of a query face with Disgust and its intra-
class variation image with Surprise. In this example, sup-
pose that the difference between the two expressive faces
is decomposed into the two following differences: 1) an
imaginary neutral face subtracted from the query face (of
the same subject) with Disgust, and 2) the intra-class vari-
ation image with Surprise, subtracted from the neutral
f a c e .B a s e do nt h er e s u l tr e p o r t e di n[ 7 ] ,w ec a ns e et h a t
each of the both difference information can reduce iden-
tity information and emphasize the expression as well.
Therefore, the combination of the two differences (that
corresponds to the difference between the two expressive
faces with Disgust and Surprise) could also reduce the
identity information and emphasize the expression. This
results in the reduced intra-class variation and the
increased inter-class variation without using neutral face.
Fig. 3 illustrates examples of intra-class variation images
and the corresponding subtraction images for expressive
(e.g., disgust) face images belonging to four distinct sub-
jects. As shown in Fig. 3, the appearances of the subtraction
images have similar pattern along the subjects. This indi-
cates that the difference between query face image and
intra-class variation image, i.e., proposed feature, could be
discriminating for FER irrespective of subjects at hand
(please see Tables 3 and 4 in Section 3). It is noted that the
subjects in training set are used to collaboratively represent
Fig. 2. Examples of intra-class variation images of neutral (a) and surprise (b) expressions for the Ô¨Åve query face images, respectively. In these
examples, 50 training face images of neutral and surprise expressions are used for generating the corresponding intra-class variation images,
respectively. For schematic description, three top training face images are present in order based on their contributions to the generation of intra -
class variation image.LEE ET AL.: INTRA-CLASS VARIATION REDUCTION USING TRAINING EXPRESSION IMAGES FOR SPARSE REPRESENTATION BASED... 343
the query face image [21]. Using the collaborative rep-
resentation [21], the intra-class variation image could resem-
ble the query face image in identity, even when the subject
in the query face image does not exist in the training set
(e.g., the subjects in the third and fourth rows in Fig. 3).
Also, as observed in Fig. 3, the subtraction image derived
by the intra-class variation image of a particular expression
type (e.g., Neutral) looks different from those derived by
the intra-class variation images of the other expression
types (e.g., Smile, Surprise, Squint, and Scream). This obser-
vation indicates that difference information derived by
different expression states could be complementary for clas-
sifying facial expression of query face image (please refer to
the results in Fig. 6 in Section 3.2 to see the effectiveness of
the proposed method).
2.2 Sparse Representation ClassiÔ¨Åcation Using
Proposed Expression Features
In this section, we present the classiÔ¨Åcation approach based
on the sparsity of the proposed expression features. Let
ys
i2<N√∞i¬º1;...;C√ûdenote the expression features from
8s2fti;1;ti;2;...;ti;Mii¬º1;...;C j g;(i.e., M¬ºPC
i¬º1Mi
training face images contained in the training set FF√û:As is
seen in Fig. 1b, ys
iis obtained by computing the differencebetween sand its intra-class variation image hs
iof the ith
expression. Note that hs
iis generated by using FFiin a similar
way to the calculation of hq
i(refer to (1) and (2)). Using the
training set FF;we construct the dictionary Ai2<N/C2M√∞i¬º
1;...;C√ûwhere Aiconsists of the expression features ys
iof
the face images in the training set FF(see Fig. 1b).
For classifying a given query face image q;we perform
the sparse representation for the query expression feature
yq
i√∞i¬º1;...;C√û:The query expression feature yq
ican
approximately lie in the linear space spanned by the train-
ing expression features within dictionary Ai:
yq
i¬ºAix0
i√æni: (4)
In (4), x0
i2<Mrepresents a sparse solution vector
whose entries are zeros except for those associated with
t h et r u ee x p r e s s i o nc l a s s .T h e ni2<Nis a noise term
bounded by a small amount of energy ""(i.e., nikk2/C20"").
To efÔ¨Åciently obtain sparse solution vector x0
idealing
with NP-hard problem [2], the ‚Äò1-norm minimization can
be computed as follows [2]:
x0
i/C25^xi¬ºarg min xikk1;s : t : yq
i/C0Aixi/C13/C13/C13/C13
2/C20"": (5)
Fig. 3. Examples of the intra-class variation images (i.e., hq
i) and the corresponding subtraction images (i.e., yq
i√ûfor the query face images (Disgust)
of four distinct subjects for (a) i¬ºNeutral , (b) i¬ºSmile , (c) i¬ºSurprise , (d) i¬ºSquint , (e) i¬ºScream . Note that the subjects of the query faces
images in the third and fourth rows do not exist in training images.344 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 5, NO. 3, JULY-SEPTEMBER 2014
In this paper, we adopt regularized orthogonal matching
pursuit (ROMP) technique [30] to solve the ‚Äò1-norm minimi-
zation problem deÔ¨Åned in (5).
To take advantage of difference information obtained
using intra-class variation images of distinct expression
states, we combine the individual difference information
along with different expressions. For this, we combine
multiple difference information at the decision level. The
combined sparse solution vector ^xcomis then computed
as the normalized sum of the individual sparse solution
vectors ^xi[22]:
^xcom¬ºXC
i¬º1√∞^xi=^xikk2√û: (6)
Each sparse solution vector ^xiin (6) makes equal contribu-
tion to the combined sparse solution vector ^xcomwith the
normalization term ( ^xikk2).
Finally, we perform the classiÔ¨Åcation for qin question,
based on density of sparse coefÔ¨Åcients of the combined
sparse solution vector ^xcom:SpeciÔ¨Åcally, the expression class
label i/C3can be determined by Ô¨Ånding the expression class
for which the maximum of the average sparse coefÔ¨Åcient is
achieved [22]:
i/C3¬ºarg maxC
i¬º11
MiXMi
j¬º1xcom
i;j; (7)
where xcom
i;jis the element (i.e., sparse coefÔ¨Åcient) of the
combined sparse solution vector ^xcom;that corresponds
to the features for the jth training face image of the ith
expression class.
3E XPERIMENTS
To evaluate our proposed method, we used Ô¨Åve public DBs,
i.e., JAFFE [18], Extended Cohn-Kanade (CK √æ) [37], MMI
[38], BU-3D FE [20], and CMU Multi-PIE [19]. Some exam-
ple face images are shown in Fig. 4. Following the recom-
mendation in [33], each face image used in our experiments
was cropped based on two eye locations. In this paper, two
eye locations were manually determined. The cropped face
image was rescaled to the size of 64 /C264 pixels.
3.1 Comparisons with State-of-the-Art Sparse
Representation Based FER Methods
In this experiment, we compared the proposed FER method
with other state-of-the-art FER method based on SRC. In
general, there are two possible FER scenarios in practical
FER-based applications: 1) person-dependent FER [34] and
2) person-independent FER [34]. The comparative experi-
ments have been conducted under the two different FER
scenarios.
The experiments for testing person-dependent recogni-
tion were performed using JAFFE DB, MMI DB, and BU-
3DFE DB. The dataset constructions were done as follows:
1) JAFFE DB: a total of 213 face images (i.e., 30, 30, 29,
32, 31, 31, 30 face images for Neutral, Anger, Disgust,
Fear, Happiness, Sadness, Surprise, respectively)
from 10 subjects were used. The face images from
the neutral expression were used as the neutral states
of faces for extracting the feature in [7]. In order toextract the feature in [7], the neutral face image with
the Ô¨Åle name including ‚ÄòNE1‚Äô [31] in JAFFE DB was
used for each subject. For the comparison, the same
six-expression classiÔ¨Åcation (without the neutral
expression) was performed for the comparison meth-
ods [4], [5], [7], [28], [36] and the proposed method.
For each of the six expressions, we used a random
partition so that 25 face images were used for the
training set and the remaining face images (i.e., Ô¨Åve,
four, seven, six, six, Ô¨Åve face images for Anger, Dis-
gust, Fear, Happiness, Sadness, Surprise, respec-
tively) were used for the test set.
2) MMI DB: a total of 150 image sequences (25 sequen-
ces/C26 expressions) were collected from 21 subjects.
Following the method in [39], the Ô¨Årst frame with
neutral face and three peak frames of each sequence
(hence 450 expressive face images and 150 neutral
face images in total) were used for the six-class
expression recognition. One hundred Ô¨Åfty neutral
face images were used for determining the neutral
states of faces for extracting the features in [7]. In
each of the seven expressions, there were 75 ( ¬º25
sequences /C23 frames) expressive face images. We
Fig. 4. Example face images used in our experiments. (a) JAFFE DB. (b)
CK√æDB. (c) MMI DB. (d) BU-3DFE DB. (e) CMU Multi-PIE DB.LEE ET AL.: INTRA-CLASS VARIATION REDUCTION USING TRAINING EXPRESSION IMAGES FOR SPARSE REPRESENTATION BASED... 345
used a random partition so that 40 face images were
used for the training set and the remaining 35 face
images were used for the test set.
3) BU-3DFE DB: this DB has been known to be a
challenging and difÔ¨Åcult mainly due to a variety
of ethnic/racial ancestries and expression intensity
[20]. A total of 2,400 expressive face images (4
intensities x 6 expressions x 100 subjects) and 100
neutral face images (each of which is for one sub-
j e c t )[ 2 0 ]w e r eu s e d .H u n d r e dn e u t r a lf a c ei m a g e s
of 100 subjects were used for determining the neu-
tral states of faces for extracting the features of [7].
By using a random partition, following the
method in [27], 1,200 face images were used for
the training set and the remaining 1,200 face
images were used for the test set. In the experi-
ment with the proposed method, the training face
images with sufÔ¨Åciently high intensities were
selected and used for generating intra-class varia-
tion images. For the sake of fair comparison, the
face images of all the four intensities [20] were
included in the test after the random partition.
The evaluation for person-independent recognition was
performed using CK √æDB and CMU Multi-PIE DB, where
the subjects in training set were completely different from
the subjects within test set (i.e., the subjects used for training
procedure cannot be used for testing phase). The dataset
constructions were done as follows:
1) CK √æDB: this DB includes 593 image sequences from
123 subjects. From the 593 sequences, we selected 325
sequences of 118 subjects, which meet the criteria for
one of the seven emotions [37]. The selected 325
sequences consist of 45, 18, 58, 25, 69, 28, and 82
sequences of Angry, Contempt, Disgust, Fear,
Happy, Sadness, and Surprise, respectively [37]. Sim-
ilar to the method in [42], 325 expressive face images
(1 peak frame /C2325 sequences) were collected for
training set and test set. For the method in [7], 325
neutral face images (each of which is obtained from
the Ô¨Årst frame of a sequence) were employed for the
purpose of extracting the features of 325 expressive
face images. Prior to dataset partitioning into training
set and test set, a neutral face image was used as the
neutral state of the expressive face image in the
sequence during the extraction of the feature in [7].
Similar to the methods in [44], [45], [46], [47], Leave-
one-subject-out (LOSO) cross validation was adopted
in the evaluation. LOSO selected 117 out of the 118
subjects for training and used the remaining subject
for test. This procedure was repeated for all the 118
subjects. The number of training data varies from 319
to 324 during the cross validation.
2) CMU Multi-PIE: By using a random partition, for
each of the six expressions (Neutral, Smile, Surprise,
Squint, Disgust, and Scream), 70 subjects were used
for the training set while the rest of 18 subjects were
used for the test set. Because a subject had one or
two face images in each expression class, the num-
bers of the training face images and the testing face
images in the expression class varied with therandom partitions. Note that CMU Multi-PIE con-
tains face images captured under 19 different illumi-
nation conditions [19]. Hence, the face dataset from
the CMU Multi-PIE contained variation in illumina-
tion (see Fig. 4b), which made FER more challenging.
For comparison purposes, pixel-based appearance fea-
ture and three widely used texture-based features (i.e., LBP,
Gabor wavelet, and LPQ) were implemented. For extracting
LBP features used in [4] and [36], we adopted a uniform
LBP operations [8] with parameters of P¬º8,R¬º1[4]. For
extracting LPQ, we used 5/C25neighborhoods [28]. As sug-
gested in [4], [28], and [36], histograms were extracted from
15 local regions and they were concatenated to form a face
feature. For obtaining the Gabor wavelet representation,
Ô¨Åve scales and eight orientations were used to construct a
set of Gabor Ô¨Ålter banks [5].
Before constructing the dictionaries, we reduced the fea-
ture dimensions by using Eigenfaces [23]. The dimension
reduction was performed in the following way. The feature
(e.g., the proposed method which is the subtraction of the
intra-class variation image from the query face image, or
LBP feature vectors in [4]) extracted from every training face
image of a database (e.g., 150 face images for JAFFE DB) was
used to generate a projection matrix (or a set of Eigenvec-
tors). To reduce feature dimension, we selected 150 signiÔ¨Å-
cant Eigenvectors corresponding to the largest Eigenvalues.
For raw pixel [4], LBP [4], Gabor [5], LPQ [28], and the feature
in [7], the dimension of the single feature vector of a face
image was reduced by using the 150 Eigenvectors. For the
proposed method, before obtaining the sparse solution ^xi;
the dimension of each query expression feature yq
i
√∞i¬º1;...;C√ûwas independently reduced by using the asso-
ciated 150 Eigenvectors. Using 150 Eigenvectors was able to
produce the 150-dimensional feature vector. However, in the
case of JAFFE DB, we discarded the 150th dimension because
only 149 (i.e., the number ( ¬º150) of training samples minus
one) Eigenvectors have nonzero values [23]. Therefore, the
feature dimension for JAFFE DB was 149 and the feature
dimension for the remaining DBs (i.e., CK √æ, MMI, CMU
Multi-PIE, and BU-3DFE) was 150. For raw pixel [4], LBP [4],
Gabor [5], LPQ [28], the feature in [7], as well as the proposed
method, classiÔ¨Åcation was performed using the density of
sparse coefÔ¨Åcients. In addition, for the fusion method [36] of
‚ÄòSRC with raw pixel‚Äô and ‚ÄòSRC with LBP‚Äô, we strictly fol-
lowed the method in [36]. SpeciÔ¨Åcally, if the classiÔ¨Åcation
results of ‚ÄòSRC with raw pixel‚Äô and ‚ÄòSRC with LBP‚Äô are same
for a given test face image, then the Ô¨Ånal class label is
unchanged [36]. If the class labels are different, then we
selected the classiÔ¨Åcation result with the larger ratio of the
smallest residual to the second smallest residual [36].
In the case of JAFFE, MMI, CMU Multi-PIE, and BU-3DFE
DBs, performance measurements presented in the experi-
ments were averaged over 20 times. In addition, In the case
of CK √æDB which uses LOSO method, and the resulting rec-
ognition rates computed for the 118 folds were averaged.
The comparative results are given in Tables 1 and 2 for
person-dependent and person-independent recognition sce-
narios, respectively. As observed in both tables, SRC with
pixel-based appearance feature shows poor performances
(i.e., less than 60 percent in person-independent recognition
under illumination change with CMU Multi-PIE DB). On346 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 5, NO. 3, JULY-SEPTEMBER 2014
the other hand, the proposed method shows signiÔ¨Åcantly
better performances (i.e., about 79 percent in CMU Multi-
PIE DB). In addition, in both person-dependent and person-
independent recognition scenarios, the proposed method
outperforms pixel-based SRC and SRC with three widelyused texture-based appearance features (i.e., LBP, Gabor,
and LPQ). Note that the feature of subtraction of a neutral
face image [7] could not be evaluated on person-indepen-
dent recognition using CMU Multi-PIE DB because the sub-
ject of a query face image did not exist in training faceTABLE 1
Recognition Rates Evaluated under Person-Dependent Recognition Scenario
(a)
FER method Recognition rate (%)
Anger Disgust Fear Happiness Sadness Surprise Overall
Proposed 97.00 98.75 98.57 95.83 86.67 92.00 94.70
SRC √æRaw pixel [4] 86.00 88.75 88.57 88.33 75.83 84.00 85.15
SRC √æLBP [4] 89.00 93.75 94.27 93.33 80.83 91.00 90.30
SRC √æRawLBP [36] 96.00 85.00 94.27 93.33 76.67 92.00 89.70
SRC √æGabor [5] 92.00 93.75 95.71 94.17 81.67 90.00 91.21
SRC √æLPQ [28] 93.00 95.00 96.43 94.17 79.17 93.00 91.67
SRC √æSubtraction of a neutral face image [7] 94.00 93.75 91.43 85.83 78.33 94.00 89.09
(b)
Proposed 93.14 92.57 93.14 96.57 89.71 97.71 93.81
SRC √æRaw pixel [4] 77.14 89.71 80.57 92.57 86.29 96.00 87.05
SRC √æLBP [4] 89.14 97.14 82.29 97.14 88.00 90.86 90.76
SRC √æRawLBP [36] 94.29 86.29 92.57 96.00 84.57 93.71 91.24
SRC √æGabor [5] 84.57 89.71 88.00 97.71 89.14 95.43 90.76
SRC √æLPQ [28] 88.57 92.57 80.57 99.43 92.00 97.71 91.81
SRC √æSubtraction of a
neutral face image [7] 91.43 91.43 82.29 92.57 92.57 98.86 91.52
(c)
Proposed 82.63 89.13 83.30 89.28 87.40 95.35 87.85
SRC √æRaw pixel [4] 57.65 51.53 51.38 74.33 57.43 82.38 62.45
SRC √æLBP [4] 63.50 70.88 61.90 82.28 75.13 86.80 73.42
SRC √æRawLBP [36] 74.48 82.53 80.55 81.10 82.85 87.63 81.52
SRC √æGabor [5] 73.98 73.88 68.10 82.45 72.03 87.55 76.33
SRC √æLPQ [28] 72.03 84.25 76.30 90.58 83.63 86.33 82.19
SRC √æSubtraction of a neutral face image [7] 72.45 73.43 71.15 83.43 70.58 87.50 76.42
(a) JAFFE DB. (b) MMI DB. (c) BU-3DFE DB.
TABLE 2
Recognition Rates Evaluated under Person-Independent Recognition Scenario
(a)
FER method Recognition rate (%)
Angry Contempt Disgust Fear Happy Sadness Surprise Overall
Proposed 84.44 77.78 91.38 80.00 98.55 67.86 100.00 90.47
SRC √æRaw pixel [4] 68.89 66.67 87.93 52.00 95.65 57.14 95.12 80.71
SRC √æLBP [4] 75.56 55.56 87.93 72.00 98.55 64.29 100.00 85.07
SRC √æRawLBP [36] 73.33 88.89 84.48 68.00 98.55 67.86 95.12 86.65
SRC √æGabor [5] 84.44 66.67 98.28 68.00 97.10 64.29 98.78 88.67
SRC √æLPQ [28] 82.22 55.56 100.00 68.00 100.00 50.00 100.00 87.13
SRC √æSubtraction of a neutral face image [7] 62.22 72.22 84.48 68.00 98.55 42.86 96.34 81.48
(b)
FER method Recognition rate (%)
Neutral Smile Surprise Squint Disgust Scream Overall
Proposed 70.67 85.98 89.98 61.66 66.36 97.49 78.72
SRC √æRaw pixel [4] 48.15 58.06 63.47 39.14 48.15 85.09 57.49
SRC √æLBP [4] 54.66 75.38 79.88 51.25 64.57 89.79 69.15
SRC √æRawLBP [36] 54.59 77.30 83.24 58.92 67.03 94.59 72.61
SRC √æGabor [5] 59.93 79.75 84.25 50.02 63.53 94.16 71.74
SRC √æLPQ [28] 58.32 79.04 81.10 49.41 67.44 93.58 71.48
(a) CK √æDB. (b) CMU Multi-PIE DB.LEE ET AL.: INTRA-CLASS VARIATION REDUCTION USING TRAINING EXPRESSION IMAGES FOR SPARSE REPRESENTATION BASED... 347
images, thus the neutral face image of the corresponding
subject was not available.
To further verify the usefulness of our proposed features
in SRC, we compared the sparsity of the proposed method
with the comparison methods using the sparsity concentra-
tion of the true class (SCTC) [35], which can be written as
SCTC √∞x√û¬ºdtrue√∞x√û kk1
xkk1; (8)
where xdenotes a sparse solution vector. dtrue√∞/C1√ûdenotes a
characteristic function that selects coefÔ¨Åcients associated
with true class. The value of SCTC √∞x√ûbecomes 1 if non-zero
sparse coefÔ¨Åcients are only concentrated on true class, while
SCTC √∞x√ûis equal to 0 if non-zero sparse coefÔ¨Åcients do not
exist in true class. The measurement in (8) quantiÔ¨Åes how
much the coefÔ¨Åcients are concentrated on true class for a
query expression feature. It reÔ¨Çects true class in facial
expression recognition.
From Tables 3 and 4, we can observe that the correspond-
ing SCTC values of the proposed method are higher than
those of the comparison methods for the both recognition
scenarios. These results show that the proposed method
works well for realizing sparsity based classiÔ¨Åcation
scheme, resulting in high recognition rates as demonstrated
in Tables 1 and 2.
3.2 Analysis of the Effectiveness of Proposed FER
Method
In order to verify the usefulness of the proposed method,
we investigated the effect of number of subjects used in gen-
erating intra-class variation image. Recognition rates for the
proposed method were measured with seven different
numbers of subjects. The face dataset from CMU Multi-PIE
DB was used in this experiment. In 70 subjects in the train-
ing set, training face images of a particular number of sub-jects were randomly selected for generating intra-class vari-
ation images. In constructing dictionaries in SRC, the fea-
tures extracted from the training face images of all the 70
subjects were used.
Fig. 5 shows experimental results of recognition rates for
the proposed method with respect to the number of subjects
used in generating intra-class variation images. For a larger
number of subjects (e.g., 70), intra-class variation image
could be more accurately approximated because of increas-
ing possibility of the presence of the similar faces to a query
face in identity and illumination. This yields increasing rec-
ognition rate as shown in Fig. 5. Even when small number
of subjects (e.g., 10) are used in generation of intra-class var-
iation images, the proposed method still yields good and
stable recognition rate which is higher than those for SRC
based FER methods with LBP, Gabor, and LPQ features
(e.g., recognition rates are 69.15, 71.74, and 71.48, respec-
tively) as seen in Table 1b. The experimental results demon-
strated that the proposed method could be feasible for the
person-independent recognition even with the limited num-
ber of subjects available in training set.
Another experiment was performed to investigate the
effectiveness of non-neutral expression used for generating
intra-class variation image. We separately measured the
recognition rate for each expression of intra-class variation
image with CMU Multi-PIE DB. In experiments, individual
sparse solution vector ^xiwas obtained and classiÔ¨Åcation
using the single sparse solution vector was performed
(instead of the combined sparse solution vector ^xcom).
Experimental results are shown in Fig. 6. As is seen, the
use of non-neutral expression for intra-class variation
image is able to yield similar recognition performance to
the case of using neutral expression. Moreover, the recog-
nition rate using the combined sparse solution vector ^xcom
is higher than that using a single sparse solution vector ^xi
(see Fig. 6 for ‚ÄòAll‚Äô). The results show that the intra-class
variation image of an expression (e.g., Neutral) are comple-
mentary with those of the other expressions (e.g., Smile,
Surprise, Squint, Disgust, and Scream), leading to improve-
ment of FER.
We measured the computation time for the proposed fea-
ture extraction and FER. They were implemented using 64
bit Matlab programming on a PC with 3.50 GHz Pentium
CPU and 32 GB memory. The feature extraction (including
dimension reduction) took 0.11 sec for each query face
Fig. 5. Recognition rates (averaged over 20 times) for the proposed fea-
tures with respect to the number of subjects that are used for generating
intra-class variation images.
TABLE 4
Comparisons in the Sparse CoefÔ¨Åcient of the
True Class in Person-Independent Recognition
Scenario (CMU Multi-PIE DB was used)
FER method Sparsity SCTC
Proposed 0.3688
SRC √æRaw pixel [4] 0.3019
SRC √æLBP [4] 0.3254
SRC √æGabor [5] 0.3431
SRC √æLPQ [28] 0.3368TABLE 3
Comparisons in the Sparse CoefÔ¨Åcient of the True Class
in Person-Dependent Recognition Scenario
(JAFFE DB was used)
FER method Sparsity SCTC
Proposed 0.3654
SRC √æRaw pixel [4] 0.2937
SRC √æLBP [4] 0.3275
SRC √æGabor [5] 0.3332
SRC √æLPQ [28] 0.3292
SRC √æSubtraction of a neutral face image [7] 0.3228348 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 5, NO. 3, JULY-SEPTEMBER 2014
image. About 0.15 sec was taken for entire process (includ-
ing feature extraction and classiÔ¨Åcation) of each query face
image, which was feasible.
3.3 Cross-Database Evaluation
To investigate the generalization performance of the pro-
posed method, we performed an inter-database experi-
ment. For the experiment, the datasets (used in Section 3.1)
from MMI DB and CK √æDB were used to construct the
training and test sets. The training and test sets contained
six expression classes, i.e., Anger, Disgust, Fear, Happi-
ness, Sadness, and Surprise, which were common in both
CK√æand MMI databases. Note that the expression class
Contempt of CK √æwas not used in the experiment,
which was not available in MMI database. From MMI DB,
450 (3 images /C2150 sequences) expressive face images
were used for constructing training set. From CK √æDB, 307
expressive face images of the six expression classes were
used for test set while 18 expressive face images of Con-
tempt were excluded in the experiment. In addition, 150
neutral face images from MMI DB and 307 neutral face
images from CK √æDB were used for the neutral states of
faces for extracting the features in [7].
Experimental result in Table 5 shows that the proposed
method achieves the recogni tion rate of 65.47 percent,which is higher than those of the comparison methods.
O n ep o s s i b l er e a s o nf o rt h eh i g h e rr e c o g n i t i o nr a t ei st h a t
the proposed FER method has been designed to deal with
person-independent recognition where the subjects of
t h eq u e r yf a c ei m a g e s( e . g . ,t h ef a c ei m a g e sf r o mC K √æDB)
are not present in the training face images (e.g., the face
images from MMI DB). However, the recognition rate is
l o w e rt h a nt h er e c o g n i t i o nr a t ei nt h ec a s ew h e r ef a c e
images from a single DB (CK √æor MMI) are used for both
training set and test set. This performance degradation is
mainly attributed to the fact that the face images are col-
lected under two different controlled conditions. In order
to generalize across image acquisition conditions, it is
required to collect large trai ning datasets with various
image acquisition conditions [40].
3.4 Comparisons with Recent Advances
This section presents the comparisons between the result of
the proposed method and the reported results of 8 recently
proposed FER methods in [42], [43], [44], [45], [46], [47] on
CK√æDB. These FER methods were chosen because they
used similar testing strategies (e.g., person-independent rec-
ognition) to that used in the proposed method.
Table 6 shows the comparisons of experimental setup and
recognition result between the different FER methods. As is
seen in Table 6, the proposed method can achieve higher rec-
ognition rate than the single frame-based methods in [42],
under 10-fold person-independent cross validation protocolTABLE 5
Generalization Performance on the Two Different DBs
FER method Recognition rate (%)
Proposed 65.47
SRC √æRaw pixel [4] 52.44
SRC √æLBP [4] 55.70
SRC √æRawLBP [36] 59.28
SRC √æGabor [5] 60.59
SRC √æLPQ [28] 62.54
SRC √æSubtraction of a
neutral face image [7]58.31
CK√æDB and MMI DB were used for testing and training, respectively.
TABLE 6
Comparisons with Recent Advances in Expression Recognition on CK √æDB
Method Video-based or
Frame-basedCross validation No. of subjects No. of expressions Recognition rate (%)
Proposed frame-based LOSO 118 7/C390.5
Proposed frame-based 10-fold 118 7/C389.6
LDN_K [42] frame-based 10-fold 118 7/C382.3
LDN_G [42] frame-based 10-fold 118 7/C389.3
AR-LBP [43] frame-based 10-fold not stated 7y83.1
STLMBP-C [44] video-based LOSO 118 7/C392.3
STLMBP-J [44] video-based LOSO 118 7/C392.6
Feature tracking [45] video-based LOSO 90 6z87.4
Avatar image [46] video-based LOSO 123 7/C382.6
MCF [47] video-based LOSO 123 7/C389.4
‚ÄúLOSO‚Äù denotes leave-one-subject-out cross validation.
‚Äú10-fold‚Äù denotes 10-fold person-independent cross validation.
/C3six basic expressions √æcontempt.
ysix basic expressions √æneutral (contempt is excluded).
zsix basic expressions (contempt is excluded).
Fig. 6. Comparisons of recognition rate (averaged over 20 times) with sin-
gle expression used for generating intra-class variation images. ‚ÄòAll‚Äô means
the recognition rate when using the combined sparse solution vector ^xcom.LEE ET AL.: INTRA-CLASS VARIATION REDUCTION USING TRAINING EXPRESSION IMAGES FOR SPARSE REPRESENTATION BASED... 349
(for details on the 10-fold person-independent cross-
validation protocol, please refer to [42]). The recognition
rates for Ô¨Åve video-based FER methods are also presented in
Table 6. The proposed method does not outperform some
video-based FER methods (e.g., STLMBP-J [44]). In this
paper, the proposed method is limited to using a single
frame of a video sequence. However, it should be noted that
the proposed method could be further improved by incorpo-
rating the temporal information present in a video sequence.
For example, to exploit temporal information, the multiple
query face images within a temporal window can be fused
by using a simple and effective technique (e.g., local average
[48]), which is followed by the generations of intra-class var-
iation images and the extractions of the expression features
using subtraction. In this way, we would expect a higher rec-
ognition rate for the proposed method, which could be com-
parable to the best reported results in Table 6.
Although some results in Table 6 cannot be directly com-
pared due to different experimental setups, different
expression classes, different preprocessing methods (e.g.,
face alignment) and so on, it is demonstrated that the pro-
posed method can yield a feasible and promising recogni-
tion rate (around 90 percent) even with static facial images
under person-independent recognition scenario.
4C ONCLUSION
In this paper, we proposed a new sparse representation
based facial expression recognition method which aims to
deal with intra-class variations in FER. For a given query
face image, we generated an intra-class variation image
using training face images of each expression. The pro-
posed intra-class variation image had similar appearance
with the query face image in terms of identity and illumi-
nation, while representing the same expression as the asso-
ciated training face images. As the features of sparse
representation, we used the differences between the query
face image and its intra-class variation images, which
are effective for FER. The reasons for the effectiveness of
the proposed method can be summarized as follows: 1)
the difference information obtained by using the intra-
class variation image of a non-neutral expression could be
used for reducing identity information (which is similar
with the normalization effect obtained by using the neutral
expression state); 2) the diff erence information could also
provide robustness to illumination variation; 3) the multi-
ple differences derived by the different expression states
available in training data could provide complementary
information to maximize discr iminating capability of the
proposed method. Experimental results showed that the
proposed method was able to y ield acceptable FER perfor-
mance even when the subjects in query face images were
not available in training images, or even in the absence of
their neutral face images.
In this paper, the face images used in the experiment
were cropped manually. Thus, we assumed that well-
aligned face images were given for the proposed method.
For future work, we will consider incorporating a robust
alignment method (such as face alignment by using a
deformable sparse recovery [41]) to realize reliable auto-
matic FER system.ACKNOWLEDGMENTS
This research was funded by the MSIP (Ministry of Science,
ICT and Future Planning), Korea in ICT R&D Program
2014. Yong Man Ro is the corresponding author.
REFERENCES
[1] I. Kotsia and I. Pitas, ‚ÄúFacial expression recognition in image
sequences using geometric deformation features and support vec-
tor machines,‚Äô‚Äô IEEE Trans. Image Process. , vol. 16, no. 1, pp. 172‚Äì
187, Jan. 2007.
[2] J. Wright, A. Yang, A. Ganesh, S. Sastry, and Y. Ma, ‚ÄúRobust face
recognition via sparse representation,‚Äù IEEE Trans. Pattern Anal.
Mach. Intell. , vol. 30, no. 2, pp. 210‚Äì227, Feb. 2009.
[3] X. Yuan, ‚ÄúVisual classiÔ¨Åcation with multi-task joint sparse repre-
sentation,‚Äù in Proc. IEEE Int. Conf. Comput. Vis. Pattern Recog. ,
2010, pp. 3493‚Äì3500.
[4] M. Huang, Z. Wang, and Z. Ying, ‚ÄúA new method for facial
expression recognition based on sparse representation plus LBP,‚Äù
inProc. IEEE Int. Cong. Image Sig. Process. , 2010, pp. 1750‚Äì1754.
[5] S. Zhang, X. Zhao, and B. Lei, ‚ÄúRobust facial expression recogni-
tion via compressive sensing,‚Äù Sensors , vol. 12, no. 3, pp. 3747‚Äì
3761, 2012.
[6] Z. Wang, M. Huang, and Z. Ying, ‚ÄúThe performance study of facial
expression recognition via sparse representation,‚Äù presented at the
9th Int. Conf. Mach. Learn. Cybern., Qingdao, China, 2010.
[7] S. Zafeiriou and M. Petrou, ‚ÄúSparse representation for facial
expression recognition via l1 optimization,‚Äù in Proc. IEEE Int.
Conf. Comput. Vis. Pattern Recog. , 2010, pp. 32‚Äì39.
[8] T. Ahonen, A. Hadid, and M. Pietik ‚Ç¨aznen, ‚ÄúFace description with
local binary pattern: Application to face recognition,‚Äù IEEE Trans.
Pattern Anal. Mach. Intell. , vol. 28, no. 12, pp. 2037‚Äì2041, Dec. 2006.
[9] L. Shen, L. Bai, and M. C. Fairhurst, ‚ÄúGabor wavelets and general
discriminant analysis for face identiÔ¨Åcation and veriÔ¨Åcation,‚Äù
Image Vis. Comput. , vol. 25, no. 5, pp. 553‚Äì563, 2007.
[10] N. Sebe, M. S. Lew, Y. Sun, I. Cohen, T. Gevers, and T. S. Huang,
‚ÄúAuthentic facial expression analysis,‚Äù Image Vis. Comput. , vol. 25,
no. 12, pp. 1856‚Äì1863, Dec. 2007.
[11] M. S. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. Fasel, and J.
Movellan, ‚ÄúRecognizing facial expression: Machine learning and
application to spontaneous behavior,‚Äù in Proc. IEEE Comput. Soc.
Int. Conf. Comput. Vis. Pattern Recog. , 2005, pp. 568‚Äì573.
[12] J. Yang, D. Zhang, A. F. Frangi, and J. Y. Yang, ‚ÄúTwo-dimensional
PCA: A new approach to appearance-based face representation
and recognition,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 26,
no. 1, pp. 131‚Äì137, Jan. 2004.
[13] J. Bouguet, ‚ÄúPyramidal implementation of the Lucas-Kanade fea-
ture tracker: Description of the algorithm,‚Äù OpenCV Documenta-
tion, Santa Clara, CA: Intel Corp., Microprocess. Res. Labs, 1999.
[14] Y. C. Lim, M. Lee, C. H. Lee, S. Kwon, and J. Lee, ‚ÄúIntegrated posi-
tion and motion tracking method for online multi-vehicle tracking-
by-detection,‚Äù Opt. Eng. , vol. 50, no. 7, pp. 077203-1‚Äì077203-10, 2011.
[15] Y. L. Tian, T. Kanade, and J. F. Cohn, ‚ÄúFacial expression analysis,‚Äù
inHandbook of Face Recognition , S.Z. Li, and A.K. Jain, eds. New
York, NY, USA: Springer, 2005, pp. 247‚Äì276.
[16] G. Donato, M. Stewart, J. C. Hager, P. Ekman, and T. J. Sejnowski,
‚ÄúClassifying facial actions,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. ,
vol. 21, no. 10, pp. 974‚Äì989, Oct. 1999.
[17] J. J. Bazzo and M. V. Lamar, ‚ÄúRecognizing facial actions using
Gabor wavelets with neutral face average difference,‚Äù in Proc.
IEEE Int. Conf. Autom. Face Gesture Recog. , 2004, pp. 505‚Äì510.
[18] M. Lyons, S. Akamatsu, M. Kamachi, and J. Gyoba, ‚ÄúCoding facial
expressions with gabor wavelets,‚Äù in Proc. IEEE Int. Conf. Autom.
Face Gesture Recog. , 1998, pp. 200‚Äì205.
[19] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker, ‚ÄúMulti-
PIE,‚Äù Image Vis. Comput. , vol. 28, no. 5, pp. 807‚Äì813, 2010.
[20] Y. Lijun, X. Wei, Y. Sun, J. Wang, and M. J. Rosato, ‚ÄúA 3D facial
expression database for facial behavior research,‚Äù in Proc. IEEE
Int. Conf. Autom. Face Gesture Recog. , 2006, pp. 211‚Äì216.
[21] L. Zhang, M. Yang, X. C. Feng, Y. Ma, and D. Zhang,
‚ÄúCollaborative representation based classiÔ¨Åcation for face recog-
nition,‚Äù arXiv:1204.2358, 2012.
[22] R. Min and J. Dugelay, ‚ÄúImproved combination of LBP and sparse
representation based classiÔ¨Åcation (SRC) for face recognition,‚Äù in
Proc. IEEE Int. Conf. Multimedia Expo. , 2011, pp. 1‚Äì6.350 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 5, NO. 3, JULY-SEPTEMBER 2014
[23] M. A. Turk and A. P. Pentland, ‚ÄúEigenfaces for recognition,‚Äù
J. Gogn. Neurosci. , vol. 3, no. 1, pp. 71‚Äì86, 1991.
[24] C. H. Chan, J. Kittler, N. Poh, T. Ahonen, and M. Pie-
tik‚Ç¨aznen, ‚Äú(Multiscale) local phase quantization histogram
discriminant analysis with score normalisation for robust face
recognition,‚Äù in P r o c .I E E EI n t .C o n f .C o m p u t .V i s .W o r k s h o p ,
2009, pp. 633‚Äì640.
[25] S. M. Lajevardi and Z. M. Hussain, ‚ÄúAutomatic facial expression
recognition: Feature extraction and selection,‚Äù Sig., Image Video
Process. , vol. 6, no. 1, pp. 159‚Äì169, 2012.
[26] S. M. Lajevardi and Z. M. Hussain, ‚ÄúEmotion recognition from
color facial images based on multilinear image analysis and log-
Gabor Ô¨Ålters,‚Äù in Proc. 25th Int. Conf. Imag. Vis. Comput. , 2010,
pp. 1‚Äì6.
[27] S. M. Lajevardi and H. R. Wu, ‚ÄúFacial expression recognition in
perceptual color space,‚Äù IEEE Trans. Image Process. , vol. 21, no. 8,
pp. 3721‚Äì3733, Aug. 2012.
[28] W. Zhen and Y. Zilu, ‚ÄúFacial expression recognition based on local
phase quantization and sparse representation,‚Äù in Proc. IEEE Int.
Conf. Nat. Comput. , 2012, pp. 222‚Äì225.
[29] S. H. Lee, H. Kim, K. N. Plataniotis, and Y. M. Ro, ‚ÄúUsing color
texture sparsity for facial expression recognition,‚Äù in Proc. IEEE
Int. Conf. Autom. Face Gesture Recog. , 2013, pp. 1‚Äì6.
[30] D. Needell and R. Vershynin, ‚ÄúUniform uncertainty principle and
signal recovery via regularized orthogonal matching pursuit,‚Äù
Found. Comput. Math. , vol. 9, no. 3, pp. 317‚Äì334, 2009.
[31] [Online]. Available: http://www.kasrl.org/jaffe_info.html, 1997.
[32] K. Anderson and P. W. McOwan, ‚ÄúA real-time automated system
for the recognition of human facial expressions,‚Äù IEEE Trans.
Syst., Man, Cybern. Part B , vol. 36, no. 1, pp. 96‚Äì105, Feb. 2006.
[33] Y. Tian, ‚ÄúEvaluation of face resolution for expression analysis,‚Äù in
Proc. IEEE Int. Conf. Comput. Vis. Pattern Recog. Workshop , 2004, p. 82.
[34] Z. Zeng, M. Pantic, G. I. Roisman, and T. S. Huang, ‚ÄúA survey of
affect recognition methods: Audio, visual, and spontaneous
expressions,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 31, no. 1,
pp. 39‚Äì58, Jan. 2009.
[35] W. Eom, W. D. Neve, and Y. M. Ro, ‚ÄúSparse feature analysis for
detection of clustered microcalciÔ¨Åcations in mammogram
images,‚Äù in Proc. Int. Forum Med. Imaging Asia , 2012, p. 4.
[36] Z. L. Ying, Z. W. Wang, and M. W. Huang ‚ÄúFacial expression rec-
ognition based on fusion of sparse representation,‚Äù in Proc. Adv.
Intell. Comput. Theories Appl., 6th Int. Conf. Intel-l. Comput. , Chang-
sha, China, 2010, pp. 457‚Äì464.
[37] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, and Z. Ambadar, ‚ÄúThe
extended Cohn-Kanade dataset (CK √æ): A complete dataset for
action unit and emotion-speciÔ¨Åed expression,‚Äù in Proc. IEEE Int.
Conf. Comput. Vis. Pattern Recog. , 2010, pp. 94‚Äì101.
[38] M. Pantic, M. Valstar, R. Rademaker, and L. Maat, ‚ÄúWeb-based
database for facial expression analysis,‚Äù in Proc. IEEE Int. Conf.
Multimedia Expo. , 2005, pp. 317‚Äì321.
[39] C. Shan, S. Gong, and P. W. McOwan, ‚ÄúFacial expression recogni-
tion based on Local Binary Patterns: A comparative study,‚Äù Image
Vis. Comput. , vol. 27, no. 6, pp. 803‚Äì816, 2009.
[40] G. Littlewort, M. S. Bartlett, I. Fasel, J. Susskind, and J. Movellan,
‚ÄúDynamics of facial expression extracted automatically from vid-
eo,‚Äù Image Vis. Comput. , vol. 24, no. 6, pp. 615‚Äì625, 2006.
[41] A. Wagner, J. Wright, A. Ganesh, Z. Zhou, H. Mobahi, and Y. Ma,
‚ÄúToward a practical face recognition system: Robust alignment
and illumination by sparse representation,‚Äù IEEE Trans. Pattern
Anal. Mach. Intell. , vol. 34, no. 2, pp. 372‚Äì386, Feb. 2012.
[42] A. R. Rivera, J. R. Castillo, and O. Chae, ‚ÄúLocal directional number
pattern for face analysis,‚Äù IEEE Trans. Image Process. , vol. 22, no. 5,
pp. 1740‚Äì1752, May 2013.
[43] C. L. S. Naika, S. S. Jha, P. K. Das, and S. B. Nair, ‚ÄúAutomatic facial
expression recognition using extended ar-lbp,‚Äù Wireless Netw.
Comput. Intell. , vol. 292, no. 3, pp. 244‚Äì252, 2012.
[44] X. Huang, G. Zhao, W. Zheng, and M. Pietik ‚Ç¨ainen,
‚ÄúSpatiotemporal local monogenic binary patterns for facial
expression recognition,‚Äù IEEE Signal Process. Lett. , vol. 19, no. 5,
pp. 243‚Äì246, May 2012.
[45] Y. Li, S. Wang, Y. Zhao, and Q. Ji, ‚ÄúSimultaneous facial feature
tracking and facial expression recognition,‚Äù IEEE Trans. Image Pro-
cess., vol. 22, no. 7, pp. 2559‚Äì2573, Jul. 2013.
[46] S. Yang and B. Bhanu, ‚ÄúUnderstanding discrete facial expressions
in video using an emotion avatar image,‚Äù IEEE Trans. Syst., Man,
Cybern. B , vol. 42, no. 4, pp. 980‚Äì992, Aug. 2012.[47] S. W. Chew, S. Lucey, P. Lucey, S. Sridharan, and J. F. Cohn,
‚ÄúImproved facial expression recognition via uni-hyperplane clas-
siÔ¨Åcation,‚Äù in Proc. IEEE Int. Conf. Comput. Vis. Pattern Recog. ,
2012, pp. 2554‚Äì2561.
[48] S. W. Chew, R. Rana, P. Lucey, S. Lucey, and S. Sridharan,
‚ÄúSparse temporal representations for facial expression recog-
nition,‚Äù in Proc. 5th PaciÔ¨Åc Rim Conf. Adv. Image Video Technol. ,
2012, pp. 311‚Äì322.
Seung Ho Lee received the BS degree from
Dongguk University, Seoul, South Korea, in 2009
and the MS degree from the Korea Advanced
Institute of Science and Technology (KAIST),
Daejeon, South Korea, in 2011, and is currently
working toward the PhD degree at KAIST. In
2011, he was a visiting researcher at the
University of Toronto, Toronto, ON, Canada. His
research interests include face detection/tracking,
facial expression recognition, face recognition,
facial age classiÔ¨Åcation, image/video indexing,
pattern recognition, machine learning, and computer vision. He is a
student member of the IEEE.
Konstantinos N. (Kostas) Plataniotis (S‚Äô90-
M‚Äô92-SM‚Äô03-F‚Äô12) received the BEng degree in
computer engineering from the University of
Patras, Patras, Greece, in 1988 and the MS and
PhD degrees in electrical engineering from Flor-
ida Institute of Technology, Melbourne, in 1992
and 1994, respectively. He is a professor with
The Edward S. Rogers Sr. Department of Electri-
cal and Computer Engineering, University of Tor-
onto in Toronto, ON, Canada, and an adjunct
professor with the School of Computer Science,
Ryerson University, Canada. He is the director of the Knowledge Media
Design Institute with the University of Toronto, where he is also the
director of Research for the Identity, Privacy and Security Institute. His
research interests include biometrics, communications systems, multi-
media systems, and signal and image processing. He is the editor-in-
chief (2009-2011) for the IEEE Signal Processing Letters , a registered
professional engineer in the province of Ontario, and a member of the
Technical Chamber of Greece. He is the 2005 recipient of the IEEE Can-
ada‚Äôs Outstanding Engineering Educator Award ‚Äúfor contributions to
engineering education and inspirational guidance of graduate students‚Äù
and the corecipient of the 2006 IEEE Transactions on Neural Networks
Outstanding Paper Award for the published paper in 2003 entitled ‚ÄúFace
Recognition Using Kernel Direct Discriminant Analysis Algorithms. He is
a fellow of the IEEE.
Yong Man Ro (S‚Äô85-M‚Äô92-SM‚Äô98) received the
BS degree from Yonsei University, Seoul and the
MS and PhD degrees from the department of
electrical engineering at KAIST. He was a
researcher at Columbia University, a visiting
researcher at the University of California, Irvine,
and a research fellow at the University of Califor-
nia, Berkeley. He was a visiting professor in the
department of electrical and computer engineer-
ing at the University of Toronto. He is currently a
professor and the chair of signals and systems
group of the department of electrical engineering in KAIST. He estab-
lished Image and video systems (IVY) Lab at KAIST in 1997. Among the
years he has been conducting research in a wide spectrum of image
and video systems research topics. Among those topics; image process-
ing, computer vision, visual recognition, medical image processing and
video representation/compression. He received the young investigator
Ô¨Ånalist award of ISMRM in 1992 and the year‚Äôs scientist award (Korea)
in 2003. He served as an associate editor for IEEE signal processing let-
ters. He serves as an associate editor in transitions on data hiding and
multimedia security (Springer-Verlag). He served as a TPC in many
international conferences including the program chair and organized
many special sessions including ‚ÄúDigital Photo Album Technology‚Äù in
AIRS 2005, ‚ÄúSocial Media‚Äù in DSP 2009 and ‚ÄúHuman 3D Perception and
3D Video Assessments‚Äù in DSP 2011.
""For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.LEE ET AL.: INTRA-CLASS VARIATION REDUCTION USING TRAINING EXPRESSION IMAGES FOR SPARSE REPRESENTATION BASED... 351
"
https://ieeexplore.ieee.org/document/7410698,"Joint Fine-Tuning in Deep Neural Networks
for Facial Expression Recognition
Heechul Jung Sihaeng Lee Junho Yim Sunjeong Park Junmo Kim
School of Electrical Engineering
Korea Advanced Institute of Science and Technology
{heechul, haeng, junho.yim, sunny0414, junmo.kim} @kaist.ac.kr
Abstract
Temporal information has useful features for recogniz-
ing facial expressions. However , to manually design useful
features requires a lot of effort. In this paper , to reduce thiseffort, a deep learning technique, which is regarded as a
tool to automatically extract useful features from raw data,
is adopted. Our deep network is based on two differentmodels. The Ô¨Årst deep network extracts temporal appear-ance features from image sequences, while the other deepnetwork extracts temporal geometry features from tempo-ral facial landmark points. These two models are combinedusing a new integration method in order to boost the perfor-mance of the facial expression recognition. Through severalexperiments, we show that the two models cooperate witheach other . As a result, we achieve superior performance toother state-of-the-art methods in the CK+ and Oulu-CASIAdatabases. Furthermore, we show that our new integrationmethod gives more accurate results than traditional meth-ods, such as a weighted summation and a feature concate-nation method.
1. Introduction
Recognizing an emotion from a facial image is a clas-
sic problem in the Ô¨Åeld of computer vision, and many stud-ies have been conducted. It can be classiÔ¨Åed into two cat-egories: image sequence-based and still image-based ap-proaches. Image sequence-based approach has been used toincrease the recognition performance by extracting usefultemporal features from the image sequences, and the per-formance is usually better than a still image-based approach[15,20,12,17,8]. Both appearance and geometric features
can be used for the spatio-temporal feature [1, 22].
Well-known deep learning algorithms, such as the deep
neural networks (DNNs) and the convolutional neural net-works (CNNs), have an ability to automatically extract use-ful representations from raw data (e.g., image data). How-ever, there is a limit when applying them directly to facial
Figure 1. Overall structure of our approach. The blue and red
boxes with dotted lines correspond to the two architectures of the
deep networks. Our two deep networks receive an image sequence
and facial landmark points as input, respectively. Conv and FCrefer to the convolutional and fully connected layers. Finally, theoutputs of these networks are integrated using a proposed jointÔ¨Åne-tuning method, which is represented in the purple box.
expression recognition databases, such as CK+ [13], MMI
[18], and Oulu-CASIA [23]. The major reason is that theamount of data is too small, so a deep network that has manyparameters can easily fall into overÔ¨Åtting when training. (Ingeneral, data collection is expensive.) Furthermore, if thetraining data is high dimensional, the overÔ¨Åtting problem
becomes more crucial.
In this paper, we are interested in recognizing facial ex-
pressions using a limited amount of (typically a few hun-
dreds of) image sequence data with a deep network. In orderto overcome the problem of having a small amount of data,
we construct two small deep networks that complementeach other. One of the deep networks is trained using image
sequences, while the other deep network learns the tempo-ral trajectories of facial landmark points. In other words,the Ô¨Årst network focuses more on appearance changes of
facial expressions over time, while the second network is di-
rectly related to the motion of facial parts. Furthermore, wepresent a new integration method called joint Ô¨Åne-tuning,which performs better than simple weighted summation
2015 IEEE International Conference on Computer Vision
1550-5499/15 $31.00 ¬© 2015 IEEE
DOI 10.1109/ICCV.2015.3412983

method. Therefore, our main contributions in this paper can
be summarized as follows:
‚Ä¢Two deep network models are presented in order to
extract useful temporal representations from twokinds of sequential data: image sequences and thetrajectories of landmark points.
‚Ä¢We observed that the two networks automatically de-
tect moving facial parts and action points, respec-tively.
‚Ä¢We presented a joint Ô¨Åne-tuning method integrating
these two networks with different characteristics, and
performance improvement was achieved in terms of
the recognition rates.
2. Related Work
2.1. Deep Learning-Based Method
Typically, a CNN uses a single image, but CNN can
also be used for temporal recognition problems, such as ac-
tion recognition [15]. In this 3D CNN method, the Ô¨Åltersare shared along the time axis. Additionally, this methodhas been applied to facial expression recognition with de-formable action part constraints, which is called 3D CNN-DAP [ 11]. The 3D CNN-DAP method is based on 3D CNN
and uses the strong spatial structural constraints of the dy-
namic action parts. It could receive a performance boostfrom using the hybrid method, but it falls short of the per-formance of other state-of-the-art methods.
2.2. Hand-Crafted Feature-Based Method
Many studies in this Ô¨Åeld have been conducted. Tradi-
tional local features, such as HOG, SIFT, LBP, and BoWhave been extended in order to be applicable to video, and
these are called 3D HOG [9], 3D SIFT [16], LBP-TOP [24],and BoW [17], respectively. Additionally, there was an at-tempt to improve accuracy through temporal modeling ofeach facial shape (TMS) [6]. They used conditional random
Ô¨Åelds and shape-appearance features created manually.
Recently, spatio-temporal covariance descriptors with
the Riemannian locality preserving projection approach
were developed (Cov3D) [15], and an interval temporal
Bayesian network (ITBN) for capturing complex spatio-temporal relations among muscles was proposed [20]. Re-cently, expressionlet-based spatio-temporal manifold repre-sentation was developed (STM-ExpLet) [12].
In addition to the ones mentioned above, perceptual
color space was considered for facial expression [10]. Ap-
pearance and geometric feature-based approaches were alsoconsidered to be a solution for facial expression recogni-
tion, so several approches were developed [1, 22]. Recently,
there is an approach that uses 3D shape model, and they im-proved facial expression recognition rate [8]. They used aZFace algorithm [7] to estimate 3D landmark points, so thealgorithm requires 3D information of face for training.
3. Our Approach
We utilize deep learning techniques in order to recognize
facial expressions. Basically, two deep networks are com-bined: the deep temporal appearance network (DTAN) andthe deep temporal geometry network (DTGN). The DTAN,which is based on a CNN, is used to extract the temporalappearance feature necessary for facial expression recog-nition. The DTGN, which is based on a fully connectedDNN, catches geometrical information about the motionof the facial landmark points. Finally, these two modelsare integrated in order to increase the expression recogni-tion performance. This network is called the deep temporalappearance-geometry network (DTAGN). The architectureof our deep network is shown in Figure 1.
3.1. Preprocessing
In general, the length of image sequences is variable,
but the input dimension is usually Ô¨Åxed in a deep network.
Consequently, the normalization along the time axis is re-quired as input for the networks. We adopt the method in[26], which makes an image sequence into a Ô¨Åxed length.
Then, the faces in the input image sequences are detected,cropped, and rescaled to 64√ó 64. From these detected faces,
facial landmark points are extracted using the algorithm
called IntraFace [21]. This algorithm provides accurate fa-
cial landmark points consisting of 49 landmark points, in-cluding two eyes, a nose, a mouth, and two eyebrows.
3.2. Deep Temporal Appearance Network
In this paper, a CNN is used for capturing temporal
changes of appearance. Conventional CNN uses still im-
ages as input, and 3D CNN was presented recently for deal-
ing with image sequences. As mentioned in Section 2,
the 3D CNN method shares the 3D Ô¨Ålters along the timeaxis [15]. However, we use the n-image sequences without
weight sharing along the time axis. This means that eachÔ¨Ålter plays a different role depending on the time. The acti-vation value of the Ô¨Årst layer is deÔ¨Åned as follows:
f
x,y,i=œÉ(Ta/summationdisplay
t=1R/summationdisplay
r=0S/summationdisplay
s=0I(t)
x+r,y +s¬∑w(t)
r,s,i+bi),(1)
wherefx,y,i is the activation value of position (x,y)of the
i-th feature map. RandSare the number of rows and
columns of the Ô¨Ålter, respectively. Tais the total frame num-
ber of the input grayscale image sequences. I(t)
x+r,y +smeans
that the value at the position (x+r,y+s)of the input frame
at timet.w(t)
r,s,iis thei-th Ô¨Ålter coefÔ¨Åcient at (r,s)for the
t-th frame, and biis the bias for the i-th Ô¨Ålter. œÉ(¬∑)is an
2984
activation function, which is usually a non-linear function.
Additionally, we utilize a ReLU, œÉ(x)=max(0,x)as an
activation function, where xis an input value [3].
The other layers are not different from the conventional
CNN as follows: the output of the convolutional layer isrescaled to half-size in a pooling layer for efÔ¨Åcient calcula-tion. Using these activation values, a convolution operationand pooling are performed one more time. Finally, theseoutput values are passed through the two fully connectedlayers and then classiÔ¨Åed using softmax. For training ournetwork, the stochastic gradient descent method is used foroptimization, and dropout [5] and weight decay methods are
utilized for regularization.
We designed our network with a moderate depth and a
moderate number of parameters to avoid overÔ¨Åtting, sincethe size of the facial expression recognition database is toosmall‚Äî there are only 205 sequences in the MMI database.
Additionally, the Ô¨Årst layer turns out to detect the temporal
difference of the appearance in input image sequences asdiscussed in Section 4.
3.3. Deep Temporal Geometry Network
DTGN receives the trajectories of facial landmark points
as input. These trajectories can be considered as one-dimensional signals and deÔ¨Åned as follows:
X
(t)=/bracketleftBig
x(t)
1y(t)
1x(t)
2y(t)
2¬∑¬∑¬∑x(t)
ny(t)
n/bracketrightBig/latticetop
,
(2)
wherenis the total number of landmark points at frame t,
andX(t)is a2ndimensional vector at t.x(t)
kandy(t)
kare
coordinates of the k-th facial landmark points at frame t.
Thesexy-coordinates are inappropriate for direct use as
an input to the deep network, because they are not normal-ized. For the normalization of the xy-coordinates, we Ô¨Årst
subtract the xy-coordinates of the nose position (the posi-
tion of the red point among the facial landmark points inthe red box with the dotted line in Figure 1) from the xy-
coordinates of each point. Then, each coordinate is dividedby each standard deviation of xy-coordinates in each frame
as follows:
¬Øx
i(t)=x(t)
i‚àíx(t)
o
œÉ(t)
x, (3)
wherex(t)
iisx-coordinate of the i-th facial landmark point
at framet,x(t)
oisx-coordinate of the nose landmark coordi-
nate at frame t.œÉ(t)
xis standard deviation of x-coordinates
at frame t. This process is also applied to the y(t)
i. Fi-
nally, these normalized points are concatenated along the
time, and these points are used for the input to the DTGN.
¬ØX=/bracketleftBig
¬Øx(1)
1¬Øy(1)
1¬∑¬∑¬∑¬Øx(Tg)
n¬Øy(Tg)
n/bracketrightBig/latticetop
, (4)
Figure 2. Joint Ô¨Åne-tuning method. The green box denotes linear
fully connected network which has logit values. The logit val-
ues are used as the input to the softmax activation. To integratetwo networks, we freeze the weight values in gray boxes of twotrained networks, and retrain the top layer in green boxes. In the
training step, we use three softmax functions for calculating three
loss functions, and we only use Softmax
3for prediction.
where¬ØXis a2nT gdimensional input vector, and ¬Øx(Tg)
kand
¬Øy(Tg)
kare coordinates of k-th normalized landmark points at
frameTg.
The Ô¨Ågure in the red box with a dotted line in Figure 1il-
lustrates the architecture of our DTGN model. Our network
receives the concatenated landmark points ¬ØXas input. Ba-
sically, we utilize two hidden layers, and the top layer is a
softmax layer. Similar to the DTAN, this network is alsotrained by using the stochastic gradient descent method.The activation function for each hidden layer is ReLU. Fur-
thermore, for regularization of the network, dropout [5] and
weight decay are used.
3.4. Data Augmentation
In order to better classify unseen data, a number of train-
ing data covering various situations are required. However,facial expression databases, such as CK+, Oulu-CASIA,and MMI, provide only hundreds of sequences. This makesa deep network easily overÔ¨Åt, because a typical deep net-work has many parameters. To overcome this problem, var-ious data augmentation techniques are required.
First, whole image sequences are horizontally
Ô¨Çipped. Then, each image is rotated by each angle in{‚àí15
‚ó¶,‚àí10‚ó¶,‚àí5‚ó¶,5‚ó¶,10‚ó¶,15‚ó¶}. This makes the model
robust against the slight rotational changes of the inputimages. Finally, we obtain 14 times more data: originalimages (1), Ô¨Çipped images (1), rotated images with sixangles, and their Ô¨Çipped versions (12).
Similar to the augmentation of image sequences, the nor-
malized facial landmark points are also horizontally Ô¨Çipped.Then, Gaussian noise is added to the raw landmark points.
Àúx
(t)
i=¬Øx(t)
i+z(t)
i, (5)
wherez(t)
i‚àºN(0,œÉ2
i)is additive noise with noise level œÉi
for thex-coordinate of the i-th landmark points at frame t.
2985
We set the value of œÉito 0.01. Additionally, we contam-
inatedy-coordinate with noise in the same way. The net-
work learns to be robust against slight pose changes using
this method. To prepare for rotational changes, we constructrotated data as follows:
/bracketleftBig
Àúx
(t)
iÀúy(t)
i/bracketrightBig/latticetop
=R(t)/bracketleftBig
¬Øx(t)
i¬Øy(t)
i/bracketrightBig/latticetop
, (6)
fori=1,...,n whereÀúx(t)
iandÀúy(t)
iarei-th rotated xy-
coordinates at time t, andR(t)is a 2√ó2 rotation matrix for
thexy-coordinates at time t, which has an angle Œ∏(t). The
value ofŒ∏(t)is drawn from a uniform distribution where
Œ∏(t)‚àºUnif[Œ≤,Œ≥]. We set the values of Œ≤andŒ≥to‚àíœÄ/10
andœÄ/10, respectively.
We performed the Ô¨Årst data augmentation methods in
equation 5three times, and the second data augmentation in
equation 6was also conducted three times. Consequently,
we obtained six times more facial landmark points. As aresult, we augmented the training data fourteen times: orig-inal coordinates (1), Ô¨Çipped coordinates (1), and six aug-mented coordinates, and their Ô¨Çipped versions (12).
3.5. Model Integration
3.5.1 Weighted Summation
The outputs from the top layers of the two networks were
integrated using equation 7.
oi=Œ±pi+(1‚àíŒ±)qi,0‚â§Œ±‚â§1, (7)
fori=1,...,c wherecis the total number of emotion
class,pi,qiare outputs of DTAN and DTGN, and oiis the
Ô¨Ånal score. Finally, the index with the maximum value is
the Ô¨Ånal prediction. The parameter Œ±usually depends on
the performance of each network. For all the experiments
in this paper, we set the value of Œ±to 0.5 that is the optimal
value as shown in Figure 11.
3.5.2 Joint Fine-Tuning Method
The above method is simple to use, but it may not use the
most of the ability of the two models. Consequently, we
propose an alternative integration method for the two net-works using a joint Ô¨Åne-tuning method, which achieves bet-ter results than the above method.First, the two trained networks are reused, as shown in Fig-
ure2. Next, we retrain the linear fully connected network,
which is located below softmax activation function, with the
loss function L
DTAGN of DTAGN deÔ¨Åned as follows:
LDTAGN=Œª1L1+Œª2L2+Œª3L3, (8)
whereL1,L2, andL3are loss functions computed by
DTAN, DTGN, and both, respectively. The Œª1,Œª2, andŒª3
are tuning parameters. Usually, the parameters Œª1andŒª2
Figure 3. Filters learned by a single frame-based CNN. The in-
put image size was 64 √ó64, and the Ô¨Ålter size was 5 √ó5. 18
Ô¨Ålters were selected for visualization from 64 learned Ô¨Ålters in the
Ô¨Årst convolutional layer. The black and white colors represent thenegative and positive values, respectively. There were several di-rectional edge and blob detection Ô¨Ålters.
Figure 4. Feature maps corresponding to Figure 3.The left im-
age represents the input image, and the right image shows the fea-ture maps extracted by each Ô¨Ålter in Figure 3. The emotion label
for the input image was surprise. The blue and red values repre-
sent the low and high response values, respectively. The edges of
the input image are detected in most of the Ô¨Ålter.
are the same, and Œª3has a smaller value than the value of
two tuning parameters. For all the experiments perfomed in
this paper, we set Œª1,Œª2, andŒª3to 1, 1, and 0.1, respec-
tively. The parameters were intuitively chosen. Each loss
function is a cross entropy loss function, which is deÔ¨Åned
as follows:
Li=‚àíc/summationdisplay
j=1yjlog(Àúyi,j), (9)
whereyjis thej-th value of the ground truth label, and Àúyi,j
is thej-th output value of softmax of network i. (For conve-
nience, we call DTAN, DTGN, and the integrated networkby network 1,2, and3, respectively.) The Àúy
3,jis deÔ¨Åned
using logit values of network 1 and 2 as follows:
Àúy3,j=œÉs(l1,j+l2,j), (10)
wherel1,jandl2,jarej-th logit values of network 1 and 2,
respectively. œÉs(¬∑)is a softmax activation function.
Finally, the Ô¨Ånal decision Àúois obtained using the output
of softmax of network 3 as follows:
Àúo=a r gm a x
jÀúy3,j, (11)
As a result, we utilized three loss functions in the training
step, and use only integrated result for prediction. Whenusing our joint Ô¨Åne-tuning method, we use the same trainingdataset used in training of each network. Also, the dropoutmethod is used for reducing over-Ô¨Åtting.
2986
Figure 5. Filters learned by DTAN. The number of input frames
was three in this Ô¨Ågure, so there are three Ô¨Ålters corresponding to
each frame. The three Ô¨Ålters in each bold black box generate onefeature map. As with Figure 3, 18 Ô¨Ålters were selected from 64
learned Ô¨Ålters. In this Ô¨Ågure, we can see that our network detectsdifferences between frames.
Figure 6. Feature maps corresponding to Figure 5.The gray
images on the left side form the image sequence used as input, andthe images on the right side are the feature maps corresponding toeach Ô¨Ålter in Figure 5. Blue and red represent the low and high
response values. The emotion label for the input image sequencewas surprise. We observed that our network responded to movingparts for expressing emotion.
4. What Will Deep Networks Learn?
4.1. Visualization of DTAN
To Ô¨Ånd out what our DTAN has learned, the learned
Ô¨Ålters were visualized. Figure 3demonstrates the Ô¨Ålters
learned by a single frame-based CNN in the Ô¨Årst convolu-
tional layers using the CK+ database. The Ô¨Ålters were sim-ilar to the edge or blob detectors. Corresponding responses
to each Ô¨Ålter are provided in Figure 4. The edge components
with several directions were detected by these Ô¨Ålters.
In our DTAN, which is a multiple frame-based CNN, the
learned Ô¨Ålters are shown in Figure 5. Unlike the Ô¨Ålters of a
single frame-based CNN, the Ô¨Ålters were not edge or blobdetectors. To exaggerate a little, these were just combina-tions of black, gray, and white Ô¨Ålters. Figure 6shows the
meaning of these Ô¨Ålters. High response values were usuallyshown in parts with big differences between input frames.In other words, we can see that the Ô¨Årst convolutional layerof our DTAN detects facial movements arising from the ex-pression of emotion.
4.2. Visualization of DTGN
The left side of Figure 7(a) shows the signiÔ¨Åcant facial
landmark points for facial expression recognition. Thesepositions were automatically identiÔ¨Åed by DTGN. The ex-tracted positions were very similar to those of emotionalfacial action coding system (EFACS) [2] in Figure 7(b). To
explain it further, the two extracted points on the nose be-come wider when people make a happy expression becauseboth cheeks are pulled up.
In order to Ô¨Ågure out the characteristics of the featuresAn Co Di Fe Ha Sa Su All
CK+ 45 18 59 25 69 28 83 327
Oulu 8 0 - 8 08 08 08 08 0 480
MMI 3 2 - 3 12 84 23 24 0 205
Table 1. The number of image sequences for each emotion:
anger (An), contempt (Co), disgust (Di), fear (Fe), happiness (Ha),
sadness (Sa), and surprise (Su).
extracted from the top layer, we also visualized the feature
vectors using t-SNE, which is a useful tool for visualizationof high dimensional data [19]. The input data were spreadrandomly in Figure 7(c), but the features extracted from the
second hidden layer were well separated according to theirlabel, as shown in Figure 7(d).
5. Experiments
In this section, we compare our approach with other
state-of-the-art algorithms in facial expression recognition,such as manifold-based sparse representation (MSR) [ 14],
AdaLBP [23], Atlases [4], and common and speciÔ¨Åc active
patches (CSPL) [25]. We excluded person dependent algo-
rithms or algorithms that utilize 3D geometry informationin the experiments. For assessing the performance of ourmethod, we used three databases: the CK+, Oulu-CASIA,and MMI databases. The number of image sequences ineach database is listed according to each emotion in Table 1.
5.1. Network Architecture
The architecture of DTGN for the CK+ is D1176-
FC100-FC600-S7. D1176 is a 1176 dimensional inputvector, and FC100 refers to a fully connected layer with100 nodes. Also, S7 is the softmax layer with seven out-puts. Our DTAN model for the CK+ is I64-C(5,64)-L5-P2-C(5,64)-L3-P2-FC500-FC500-S7, where I64 means 64√ó 64
input image sequences, and C(5,64) is a convolutional layerwith64Ô¨Ålters of5√ó5. L5 is a local contrast normalization
layer with a window size of 5√ó5. P2 means a 2√ó2max
pooling layer. The stride of each layer was 1 with the ex-ception of the pooling layer. The value of the stride for eachpooling layer was set to 2. The DTGN and DTAN models
Method Accuracy
HOG 3D [9] 91.44
MSR [14] 91.4
TMS [6] 91.89
Cov3D [15] 92.3
STM-ExpLet [12] 94.19
3DCNN [11] 85.9
3DCNN-DAP [11] 92.4
DTAN 91.44
DTGN 92.35
DTAGN(Weighted Sum) 96.94
DTAGN(Joint) 97.25
Table 2. Overall accuracy in the CK+ database. The red and
blue colors represent the Ô¨Årst and second most accurate, respec-
tively.
2987
(a) (b) (c) (d)
Figure 7. Visualization of representation extracted by DTGN. (a) (b) show extracted feature points and emotional action parts [2],
respectively. The important top-10 positions are detected by our network (red points in the left Ô¨Ågure). In order to visualize these ten
points, we calculated the average of the absolute values of weights in the Ô¨Årst layer connected to each landmark point. Then, these valueswere sorted in descending order, and the top 10 points with highest value were selected. The action parts deÔ¨Åned by EFACS [2] are shownin the right Ô¨Ågure (green colored area). (c) Visualization of original input data in CK+ database, using t-SNE [19]. The number of data was4149: (327-33)√ó14 augmented training data and 33 test data. The small dots and large squares represent training and test data, respectively.The numbers in the legend correspond to each label of the CK+ database: 0-anger, 1-contempt, 2-disgust, 3-fear, 4-happiness, 5-sadness,and 6-surprise. (d) Visualization of the outputs in the second hidden layer. The data points were automatically grouped by DTGN.
for Oulu-CASIA were the same as the models for the CK+
except the number of nodes in the top layer, because thereare six labels in the Oulu-CASIA.
For the MMI, we used the DTGN model of D1176-
FC100-FC200-FC6. Our DTAN model was designed asI64-C(5,32)-P3-C(3,32)-FC30-S6. Unlike the other twodatabases, the number of subjects and image sequences arevery small. Consequently, we decreased the total number ofparameters signiÔ¨Åcantly.
5.2. CK+
Description of the database. CK+ is a representative
database for facial expression recognition. This database iscomposed of 327 image sequences with seven emotion la-bels: anger, contempt, disgust, fear, happiness, sadness, andsurprise. There are 118 subjects, and these subjects are di-vided into ten groups by ID in ascending order. Nine subsetswere used for training our networks, and the remaining sub-set was used for validation. This process is the same as the10-fold cross validation protocol in [12]. In this database,each sequence starts with a neutral emotion and ends with apeak of the emotion.
Results. The total accuracy of 10-fold cross validation is
shown in Table 2. The performances of DTAN and DTGN
are lower than other algorithms, but the performance of the
integrated network is better than other state-of-the-art algo-
An Co Di Fe Ha Sa Su
An 100 0 000 0 0
Co 0 94.44 0 0 0 5.56 0
Di 00 100 00 0 0
Fe 00 0 84 80 8
Ha 00 0 0 100 00
Sa 10.71 0 0 0 0 89.29 0
Su 0 1.2 0 0 0 0 98.8
Table 3. Confusion matrix of the joint Ô¨Åne-tuning method for
the CK+ database. The labels in the leftmost column and on the
top represent the ground truth and prediction results, respectively.
Figure 8. Comparison of accuracy in the CK+ according to each
emotion among three networks.
rithms. The two networks were complementary, and this isshown in Figure 8. The DTAN had a good performance
with respect to contempt, whereas it had lower accuracywith fear. On the other hand, the geometry-based modelwas strong with fear. Table 3shows the confusion matrix for
CK+. Our algorithm performed well in recognizing anger,disgust, happiness, and surprise. For the other emotions,our method also performed reasonably well.
5.3. Oulu-CASIA
Description of the database. For further experiments, we
used Oulu-CASIA, which includes 480 image sequencestaken under normal illumination conditions. Each imagesequence has one of six emotion labels: anger, disgust, fear,happiness, sadness, or surprise. There are 80 subjects, and10-fold cross validation was performed in the same way asin the case of CK+. Similar to the CK+ database, each se-quence begins with a neutral facial expression and ends withthe facial expression of each emotion.
Results. The accuracy of our algorithm was superior to
the other state-of-the-art algorithms, as shown in Table 4.
The best performance from among the existing methods was
75.52%, which was achieved by Atlases, and this record hadnot been broken for three years. However, we have signif-
2988
Figure 9. Comparison of accuracy in the Oulu-CASIA accord-
ing to each emotion among three networks.
icantly improved the accuracy by about 6% using our inte-
grated two deep networks. In Figure 9, the performance of
two networks and the combined model is compared. Sim-ilar to the case of CK+, we can see that the two networksare complementary to each other. In particular, the perfor-mance of the DTGN in the case of disgust was lower thanthe DTAN, but the combined model produced good results.Table 5shows the confusion matrix for our algorithm. The
performance in the cases of happiness, sadness, and surprisewas good, but the performance for anger, disgust, and fearwas relatively poor. In particular, anger and disgust wereconfused in our algorithm.
5.4. MMI
Description of the database. MMI consists of 205 im-
age sequences with frontal faces and includes only 30 sub-jects. Similar to the Oulu-CASIA database, there are sixkinds of emotion labels. This database was also divided
Method Accuracy
3D SIFT [16] 55.83
LBP-TOP [24] 68.13
HOG 3D [9] 70.63
AdaLBP [23] 73.54
Atlases [4] 75.52
STM-ExpLet [12] 74.59
DTAN 74.38
DTGN 74.17
DTAGN(Weighted Sum) 80.62
DTAGN(Joint) 81.46
Table 4. Overall accuracy in the Oulu-CASIA database. The
red and blue colors represent the Ô¨Årst and second most accurate,
respectively.
An Di Fe Ha Sa Su
An 72.5 16.25 1.25 1.25 8.75 0
Di 21.25 75 3.75 0 0 0
Fe 2.5 1.25 77.5 6.25 2.5 10
Ha 0 0 7.5 90 2.5 0
Sa 13.75 0 2.5 0 83.75 0
Su 00 1 0 0 0 90
Table 5. Confusion matrix of the joint Ô¨Åne-tuning method for
the Oulu-CASIA database. The labels in the leftmost column
and on the top represent the ground truth and prediction results,respectively.
Figure 10. Comparison of accuracy in the MMI according to
each emotion among three networks.
into 10 groups for person independent 10-fold cross valida-
tiaon. This database is different from the other databases;each sequence begins with a neutral facial expression, andhas the facial expression of each emotion in the middle ofthe sequence. This ends with the neutral facial expression.The location of the peak frame is not provided as a priorinformation.
Results. This dataset is especially difÔ¨Åcult for a deep learn-
ing algorithm to learn from, because there are too small
number of data and subjects. The previous top recordachieved by a deep learning technique was only 63.4% us-ing 3D CNN-DAP. However, we improved the recognitionrate to 70.24% as shown in Table 6. Finally, our algorithm is
much better than 3D SIFT, which was the second best algo-rithm. In particular, our joint Ô¨Åne-tuning method achieveda signiÔ¨Åcantly improved recognition rate compared with a
Method Accuracy
HOG 3D [9] 60.89
3D SIFT [16] 64.39
ITBN [20] 59.7
CSPL [25] (73.53)
STM-ExpLet [12] 75.12
3DCNN [11] 53.2
3DCNN-DAP [11] 63.4
DTAN 62.45
DTGN 59.02
DTAGN (Weighted Sum) 65.85
DTAGN (Joint) 70.24
Table 6. Overall accuracy in the MMI database. The red and
blue colors represent the Ô¨Årst and second most accurate, respec-
tively. The CSPL used additional ground truth information, so itwas excluded from the ranking.
An Di Fe Ha Sa Su
An 61.29 25.8 0 0 12.9 0
Di 15.62 71.88 0 9.37 0 3.13
Fe 10.71 0 35.71 10.71 14.29 28.57
He 0 0 4.76 95.24 00
Sa 9.38 3.13 15.62 0 68.8 3.12
Su 2.5 0 20 2.5 0 75
Table 7. Confusion matrix of the joint Ô¨Åne-tuning method for
the MMI Database. The labels in the leftmost column and on the
top represent the ground truth and prediction results, respectively.
2989
Figure 11. Performance of DTAGN using the weighted summa-
tion method with respect to Œ±.We changed the value of Œ±from
0 to 1 with interval of 0.01.
Figure 12. All failure cases with fear in the MMI database. Our
deep network predicted fear to surprise (green box), anger (red
box), sadness (orange box), and happiness (blue box).
weighted summation method.
We compared two networks and the combined model in
Figure 10. The two networks were complementary to each
other for most of the emotions. However, with fear, our al-
gorithm was not successful enough. This is also shown inthe confusion matrix in Table 7. We observed that the accu-
racy for fear was much lower than other emotions. In partic-ular, most of the fear emotions were confused with surprise.To examine this phenomenon, we checked all the failurecases, as shown in Figure 12. The results indicated that a
variety of facial expressions are labeled as fear, even thoughmany cases were similar to surprise or sadness. To success-fully recognize these various expressions, various kinds oftraining data are additionally required. However, we had
only 27 subjects for training data. (Three subjects were
used for validation.) Unfortunately, performance of deeplearning techniques highly depends on the quality of train-ing data, so our accuracy with fear was not good enough.
6. Discussion on the Joint Fine-Tuning Method
In this section, we discuss the joint Ô¨Åne-tuning method.
First we evaluated the effectiveness of the three loss func-tions of our joint Ô¨Åne-tuning method using each database.As a result, the accuracy using the three loss functions was
better than using L
3only, as shown in Table 8. Also, we
compared our algorithm with a concatenation of high level
features, which is one natural way for integrating two deepnetworks. To evaluate the concatenation method, we con-catenated the activation values of the top hidden layers inthe two gray areas in Figure 2. Then the concatenated ac-
tivation values are used for inputs to a fully connected net-work with a softmax activation, and a dropout was used forregularizing the network. Table 9shows the experimental
results using each database. The performance of the con-# Of Loss Functions Baseline One (L 3only) Three
Accuracy (CK+) 96.94 96.64 97.25
Accuracy (Oulu) 80.62 81.04 81.46
Accuracy (MMI) 65.85 69.76 70.24
Table 8. Comparison between one and three loss function
methods. Baseline denotes the weighted summation method pre-
sented in this paper.
Concatenation Joint Fine-tuning
Accuracy (CK+) 94.5 97.25
Accuracy (Oulu) 75.63 81.46
Accuracy (MMI) 67.8 70.24
Table 9. Comparison between the concatenation method and
proposed joint Ô¨Åne-tuning method. Our joint Ô¨Åne-tuning method
showed about 3‚àº6% improvement in terms of the recognition
rates.
catenation method was worse than either one of our two
networks.
As mentioned in Section 3.5.2, we used the same dataset
for Ô¨Åne-tuning together with dropout, which reduces over-
Ô¨Åtting. Of course, without dropout, there will be little toÔ¨Åne-tune with the same dataset, as the error for the sametraining dataset would be already almost zero before Ô¨Åne-
tuning starts. Interestingly, thanks to dropout, there is some-thing to Ô¨Åne-tune with the same dataset as the dropout by
randomly inducing errors achieves an effect of providingdifferent training data to the layers to Ô¨Åne-tune. Further, asdropout creates an ensemble of many networks and eachnetwork in the ensemble experiences a different random
subset of whole dataset, each member of ensemble with a
particular dropout pattern experiences different subsets ofdata for the Ô¨Årst training and the following Ô¨Åne-tuning.
7. Conclusion
We presented two deep network models that collaborate
with each other. The Ô¨Årst network was DTAN, which wasbased on appearances of multiple frames, while the second
network was DTGN, which extracted useful temporalgeometric features from raw facial landmark points. Weshowed that the Ô¨Ålters learned by the DTAN in the Ô¨Årst
layer have the ability to obtain the difference between theinput frames. Furthermore, the important landmark pointsextracted by DTGN were also shown. We achieved best
recognition rates using the integrated deep network onthe CK+ and Oulu-CASIA databases. Furthermore, we
showed that our joint Ô¨Åne-tuning method is superior toother integration methods, such as a weighted summationand a feature concatenation method.
Acknowledgements This work was partially supported by
the National Research Foundation of Korea(NRF) grant
funded by the Korea government (MSIP) NRF-2010-0028680 and NRF-2014R1A2A2A01003140.
2990
References
[1] D. Ghimire and J. Lee. Geometric feature-based facial expression
recognition in image sequences using multi-class adaboost and sup-
port vector machines. Sensors, 13(6):7714‚Äì7734, 2013. 1,2
[2] J. Girard and J. Cohn. Ground truth facs action unit coding on the
group formation task. In Tech. rep., University of Pittsburgh, 2013.
5,6
[3] X. Glorot, A. Bordes, and Y . Bengio. Deep sparse rectiÔ¨Åer networks.
InProceedings of the 14th International Conference on ArtiÔ¨Åcial In-
telligence and Statistics. JMLR W&CP V olume , volume 15, pages
315‚Äì323, 2011. 3
[4] Y . Guo, G. Zhao, and M. Pietik ¬®ainen. Dynamic facial expression
recognition using longitudinal facial expression atlases. In ECCV ,
2012, pages 631‚Äì644. Springer, 2012. 5,7
[5] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and
R. R. Salakhutdinov. Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint arXiv:1207.0580,
2012. 3
[6] S. Jain, C. Hu, and J. K. Aggarwal. Facial expression recognition
with temporal modeling of shapes. In ICCV Workshops, 2011, pages
1642‚Äì1649. IEEE, 2011. 2,5
[7] L. A. Jeni, J. F. Cohn, and T. Kanade. Dense 3d face alignment from
2d videos in real-time. 2
[8] L. A. Jeni, A. L Àùorincz, Z. Szab ¬¥o, J. F. Cohn, and T. Kanade. Spatio-
temporal event classiÔ¨Åcation using time-series kernel based struc-tured sparsity. In Computer Vision‚ÄìECCV 2014, pages 135‚Äì150.
Springer, 2014. 1,2
[9] A. Klaser and M. Marszalek. A spatio-temporal descriptor based on
3d-gradients. 2008. 2,5,7
[10] S. M. Lajevardi and H. R. Wu. Facial expression recognition in
perceptual color space. Image Processing, IEEE Transactions on,
21(8):3721‚Äì3733, 2012. 2
[11] M. Liu, S. Li, S. Shan, R. Wang, and X. Chen. Deeply learning de-
formable facial action parts model for dynamic expression analysis.InACCV , 2014, pages 1749‚Äì1756. IEEE, 2014. 2,5,7
[12] M. Liu, S. Shan, R. Wang, and X. Chen. Learning expressionlets on
spatio-temporal manifold for dynamic facial expression recognition.InCVPR, 2014 IEEE Conference on , pages 1749‚Äì1756. IEEE, 2014.
1,2,5,6,7
[13] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and
I. Matthews. The extended cohn-kanade dataset (ck+): A complete
dataset for action unit and emotion-speciÔ¨Åed expression. In CVPRW,
2010 IEEE Computer Society Conference on, pages 94‚Äì101. IEEE,2010. 1
[14] R. Ptucha, G. Tsagkatakis, and A. Savakis. Manifold based sparse
representation for robust expression recognition without neutral sub-traction. In ICCV Workshops, 2011, pages 2136‚Äì2143. IEEE, 2011.
5
[15] A. Sanin, C. Sanderson, M. T. Harandi, and B. C. Lovell. Spatio-
temporal covariance descriptors for action and gesture recognition.
InWACV , 2013, pages 103‚Äì110. IEEE, 2013. 1,2,5
[16] P. Scovanner, S. Ali, and M. Shah. A 3-dimensional sift descrip-
tor and its application to action recognition. In Proceedings of the
15th international conference on Multimedia , pages 357‚Äì360. ACM,
2007. 2,7
[17] K. Sikka, T. Wu, J. Susskind, and M. Bartlett. Exploring bag of
words architectures in the facial expression domain. In Computer
Vision‚ÄìECCV 2012. Workshops and Demonstrations, pages 250‚Äì259. Springer, 2012. 1,2
[18] M. Valstar and M. Pantic. Induced disgust, happiness and surprise:
an addition to the mmi facial expression database. In Proc. Intl
Conf. Language Resources and Evaluation, Workshop on EMOTION,
pages 65‚Äì70, 2010. 1
[19] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Jour-
nal of Machine Learning Research, 9(2579-2605):85, 2008. 5,6[20] Z. Wang, S. Wang, and Q. Ji. Capturing complex spatio-temporal
relations among facial muscles for facial expression recognition. In
CVPR, 2013 IEEE Conference on , pages 3422‚Äì3429. IEEE, 2013. 1,
2,7
[21] X. Xiong and F. De la Torre. Supervised descent method and its
applications to face alignment. In CVPR, 2013 IEEE Conference on,
pages 532‚Äì539. IEEE, 2013. 2
[22] A. A. Youssif and W. A. Asker. Automatic facial expression recogni-
tion system based on geometric and appearance features. Computer
and Information Science, 4(2):p115, 2011. 1,2
[23] G. Zhao, X. Huang, M. Taini, S. Z. Li, and M. Pietik ¬®aInen. Facial
expression recognition from near-infrared videos. Image and Vision
Computing, 29(9):607‚Äì619, 2011. 1,5,7
[24] G. Zhao and M. Pietikainen. Dynamic texture recognition using lo-
cal binary patterns with an application to facial expressions. PAMI,
29(6):915‚Äì928, 2007. 2,7
[25] L. Zhong, Q. Liu, P. Yang, B. Liu, J. Huang, and D. N. Metaxas.
Learning active facial patches for expression analysis. In CVPR,
2012 IEEE Conference on, pages 2562‚Äì2569. IEEE, 2012. 5,7
[26] Z. Zhou, G. Zhao, and M. Pietikainen. Towards a practical lipreading
system. In CVPR, 2011 IEEE Conference on, pages 137‚Äì144. IEEE,
2011. 2
2991
"
https://ieeexplore.ieee.org/document/7956190,"1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE
Transactions on Circuits and Systems for Video Technology
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 1
Learning Affective Features with a Hybrid Deep
Model for Audio-Visual Emotion Recognition
Shiqing Zhang, Shiliang Zhang, Member, IEEE, Tiejun Huang, Senior Member, IEEE, Wen Gao, Fellow, IEEE,
and Qi Tian, Fellow, IEEE
Abstract‚ÄîEmotion recognition is challenging due to the emo-
tional gap between emotions and audio-visualfeatures. Motivated
by the powerful feature learning ability of deep neural networks,
this paper proposes to bridge the emotional gap by using a
hybrid deep model, which Ô¨Årst produces audio-visual segment
features with Convolutional Neural Networks (CNN) and 3D-
CNN, then fuses audio-visual segment features in a Deep Belief
Networks (DBN). The proposed method is trained in two stages.
First, CNN and 3D-CNN models pre-trained on corresponding
large-scale image and video classiÔ¨Åcation tasks, are Ô¨Åne-tuned
on emotion recognition tasks to learn audio and visual segment
features, respectively. Second, the outputs of CNN and 3D-
CNN models are combined into a fusion network built with a
DBN model. The fusion network is trained to jointly learn a
discriminative audio-visual segment feature representation. After
average-poolingsegmentfeatureslearnedbyDBNtoformaÔ¨Åxed-
length global video feature, a linear Support Vector Machine
(SVM) is used for video emotion classiÔ¨Åcation. Experimental
results on three public audio-visual emotional databases, in-
cluding the acted RML database, the acted eNTERFACE05
database, and the spontaneous BAUM-1s database, demonstrate
the promising performance of the proposed method. To the best
of our knowledge, this is an early work fusing audio and visual
cues with CNN, 3D-CNN and DBN for audio-visual emotion
recognition.
Index Terms ‚ÄîEmotion recognition, deep learning, convolu-
tional neural networks, deep belief networks, multimodality
fusion.
I. INTRODUCTION
RECOGNIZING human emotions with computers is usu-
ally performed with a multimodal approach due to
the inherent multimodality characteristic of human emotion
expression. Speech and facial expression are two natural
and effective ways of expressing emotions when human
beings communicate with each other. During the last two
decades, audio-visual emotion recognition integrating speech
and facial expression, has attracted extensive attention owing
to its promising potential applications in human-computer-
interaction [ 1], [2]. However, recognizing human emotions
w
ith computers is still a challenging task because it is difÔ¨Åcult
to extract the best audio and visual features characterizing
human emotions.
S.Q. Zhang, S.L. Zhang, T.J. Huang and W. Gao are with the Institute
of Digital Media, School of Electronic Engineering and Computer Science,
Peking University, China. S.Q. Zhang also works in the Institute of Intel-
ligent Information Processing, Taizhou University, China. e-mail: {tzczsq,
slzhang.jdl, tjhuang, wgao }@pku.edu.cn.
Q. Tian is with the Department of Computer Science, University of Texas
at San Antonio, San Antonio, TX 78249 USA. e-mail: qitian@cs.utsa.edu.
Manuscript received xxxx, 2016; revised xxxx, 2016.Feature extraction and multimodality fusion are two key
steps for audio-visual emotion recognition. As far as feature
extraction is concerned,there has been a number of works [ 3],
[4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14] focusing
o
n extracting low-level hand-crafted features for audio-visual
emotion recognition. Nevertheless, due to the emotional gap
between human emotions and low-level hand-crafted features,
these hand-crafted features can not sufÔ¨Åciently discriminate
human emotions. Here, the ‚Äùemotional gap‚Äù is deÔ¨Åned as ‚Äùthe
lack of coincidence between the measurable signal properties,
commonly referred to as features, and the expected affective
states in which the user is brought by perceiving the signal‚Äù
[15]. Therefore, the ‚Äùemotional gap‚Äù essentially represents th e
differences between emotions and the extracted affective fea-
tures. To bridge the ‚Äùemotional gap‚Äù, it is desirable to extract
high-level audio and visual features effectively distinguishing
emotions.
After feature extraction, multimodality fusion is employed
to integrate audio and visual modalities for emotion recogni-
tion. Previous works [ 3], [8], [12], [16], [17], [18] focus on
f
our typical fusion strategies: feature-level fusion, decision-
level fusion, score-level fusion, and model-level fusion, re-
spectively. Although most of existing fusion methods exhibit
good performance on audio-visual emotion recognition tasks,
they belong to shallow fusion models with a limited ability
in jointly modeling highly non-linear correlations of multiple
inputs with different statistical properties [ 19]. It is thus
n
eeded to design deeper fusion methods to produce a more
optimized joint discriminant feature representation for audio-
visual emotion recognition.
To alleviate above-mentioned two problems, the recently-
emerged deep leaning [ 20] techniques may present a cue.
D
ue to the large-scale available training data and the effective
training schemes, deep learning techniques have exhibited
powerful feature learning ability in a wide variety of domains,
such as speech recognition, image processing and understand-
ing, object detection and recognition, etc. Among them, two
representative deep learning models are DBN [ 21] and CNN
[22], [23], as described below.
D
BNs are built by stacking multiple Restricted Boltzmann
Machines (RBMs) [ 24]. By using multiple RBMs and the
g
reedy layer-wise training algorithm [ 21], DBNs can effec-
t
ively learn a multi-layer generative model of input data.
Based on this generative model, the distribution properties
of input data can be discovered, and the hierarchical feature
representationscharacterizinginput data can be also extracted.
Due to such good property, DBNs and its variant called
1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE
Transactions on Circuits and Systems for Video Technology
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 2
9LGHRVHJPHQW0
HOVSHFWURJUDP
VHJPHQW$XGLR1HWZRUN&11


&RQY & RQY &RQY &RQY IF
9LVXDO1HWZRUN'&11
&
RQYD &RQYD &RQYD &RQYE)XVLRQ1HWZRUN'%1
Õ∏
Õ∂ ‡µà Õ∏Õ∂ ‡µà Õµ
Õ≥Õ∑Õ≤ ‡µà Õ≥Õ≥Õ≤ ‡µà Õµ ‡µà Õ≥Õ∏IF
I
FIF
UHVL]H
U
HVL]H9LVLEOH
O
D\HU+LGGHQ
OD\HU2XWSXW
OD\HU
3RRO3RRO
3RRO3RRO
9LGHRVDPSOH6HJPHQWN
6
HJPHQW16HJPHQW26
HJPHQWi
9LGHRVHJPHQW0
HOVSHFWURJUDP
VHJPHQW$XGLR1HWZRUN&11


&RQY & RQY &RQY &RQY IF
9LVXDO1HWZRUN'&11
&
RQYD &RQYD &RQYD &RQYE)XVLRQ1HWZRUN'%1
Õ∏
Õ∂ ‡µà Õ∏Õ∂ ‡µà Õµ
Õ≥Õ∑Õ≤ ‡µà Õ≥Õ≥Õ≤ ‡µà Õµ ‡µà Õ≥Õ∏IF
I
FIF
UHVL]H
U
HVL]H9LVLEOH
O
D\HU+LGGHQ
OD\HU2XWSXW
OD\HU
3RRO3RRO
3RRO3RRO*OREDOYLGHR
I
HDWXUHV9LGHRHPRWLRQV6906HJPHQW
H
PRWLRQV
6HJPHQW
H
PRWLRQV$YHUDJHSRROLQJ
Fig. 1. The structure of our proposed hybrid deep model for aud io-visual emotion recognition.
Deep BolzmannMachines(DBM) [ 25] have beensuccessfully
u
tilized to learn high-level feature representations from low-
levelhand-craftedfeaturesformultimodalemotionrecognition
[26], [27], [28].
C
NNs employ raw image data as inputs instead of hand-
crafted features. CNNs are mainly composed of convolutional
layers and fully connected layers, where convolutional layers
learn a discriminative multi-level feature representation from
raw inputs and fully connected layers can be regarded as a
non-linear classiÔ¨Åer. Due to the large-scale available training
data and the effective training strategies introduced in recent
works, CNNs have exhibited signiÔ¨Åcant success in various
vision tasks like object detection and recognition [ 29], [30],
[31], [32]. However, such CNN models are mostly applied
o
n 2D images and fail to capture motion cues in videos,
due to the usage of 2D spatial convolution. To address this
problem,a recent work [ 33] has extendedCNNs with deep 3D
c
onvolution to produce a 3D-CNN model. 3D-CNNs compute
feature maps from both spatial and temporal dimensions,
and have exhibitedpromisingspatial-temporalfeaturelearning
ability on video classiÔ¨Åcation tasks [ 33].
I
nspired by the powerful feature learning ability of deep
models, this work proposes a hybrid deep learning framework
composedby CNN, 3D-CNN, and DBN to learn a joint audio-
visual feature representation for emotion classiÔ¨Åcation. Fig.
1presents the structure of our framework. It is comprised
o
f three steps: (1) we convert the raw audio signals into
a representation similar to the RGB image as the CNN
input. Consequently, a deep CNN model pre-trained on large-
scale ImageNet dataset can be Ô¨Åne-tuned on audio emotion
recognition tasks to learn high-level audio segment features.
(2) For multiple contiguousframes in a video segment, a deep
3D-CNN model pre-trained on the large-scale video dataset is
Ô¨Åne-tunedtolearnvisualsegmentfeaturesforfacialexpression
recognition.(3) The audio and visual segment features learned
by CNN and 3D-CNN are integrated in a fusion network built
with a deep DBN, which is trained to predict correct emotionlabels of video segments. Finally, we adopt the outputs of the
last hidden layer of DBN as the audio-visual segment feature.
Average-poolingis employedto aggregateall segmentfeatures
to form a Ô¨Åxed-length global video feature. Then, a linear
SVM is used for video emotion classiÔ¨Åcation. Note that, DBN
is used to fuse audio-visualfeatures, rather than to classify the
emotion of the whole video sample.
Learning audio-visual features for emotion recognition is
one of the critical steps in bridging the emotional gap. Pre-
vious works focus on using low-level hand-crafted features,
which have been veriÔ¨Åed not discriminative enough to human
emotions. In contrast, this work aims at automatically learning
a joint audio-visual feature representation from raw audio and
visual signals using a hybrid deep learning framework. The
hybriddeeplearningmodelconvertsthe raw 1-Daudiosignals
into a 2D representation and integrates CNN, 3D-CNN, and
DBN for audio-visual feature learning and fusion. It thus also
presents a new method of transforming 1-D audio signals into
the suitable input of CNN that conventionally processes 2-D
or 3-D images. Experimental results indicate that our learned
features present promising performance. The success of this
work also guarantees further investigation in this direction.
The remainder of this paper is organized as follows. The
related works are reviewed in Section II. Section IIIpresent
o
ur hybrid deep learning model for affective feature learning
in detail. Section IVdescribes the experimental results. The
c
onclusions and future work are given in Section V.
I
I. RELATEDWORK
An audio-visual emotion recognition system generally con-
sists of two important steps: feature extraction and multi-
modality fusion. In the following parts, we review related
works focusing on these two steps, respectively.
A. Feature Extraction
The widely-used audio affective features can be categorized
into prosody features, voice quality features, and spectral fea-
1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE
Transactions on Circuits and Systems for Video Technology
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 3
tures [34], respectively. Pitch, intensity, energy, and duration
t
ime are popular prosody features, as they are able to reÔ¨Çect
the rhythm of spoken language. The representative voice
quality features include formants, spectral energy distribution,
harmonics-to-noise-ratio, and so on. Mel-frequency Cepstral
CoefÔ¨Åcient (MFCC) is the most well-known spectral features
since it is used to model the human auditory perception sys-
tem. Zeng et al.[4] extract 20 audio features including pitch,
i
ntensity, and the Ô¨Årst four formants and their bandwidths
for audio emotion recognition. Wang et al.[3], [16] employ
p
itch, intensity, and the Ô¨Årst 13 MFCC features on audio
feature extraction tasks. Similar works, which extract prosody
features, voice quality features and spectral features for audio
emotion recognition, can be also found in [ 12], [35], [36],
[37], [38], respectively.
V
isual feature extraction methods can be summarized into
two categories according to the format of inputs that are
static or dynamic [ 39], [40]. For static images, the well-
k
nown ones are appearance-based feature extraction methods.
These methods adopt the whole-face or speciÔ¨Åc regions in
a face image to describe the subtle changes of the face
such as wrinkles and furrows. Among them, Gabor wavelet
representation [ 3], [16], [37], [41], Local Binary Patterns
(
LBP) [42] and its variants such as Local Phase Quantization
(
LPQ) [12], [43] are two representative appearance-based
f
eature extraction methods. Wang et al.[3], [16] adopt a
G
abor Ô¨Ålter bank of 5 scales and 8 orientations to extract
high-dimensional Gabor coefÔ¨Åcients from each facial image.
In recent years, CNNs are used to extract facial features
from static facial images as visual features [ 44]. In [44],
t
he authors use a Long Short-Term Memory (LSTM) [ 45]
f
or audio emotion recognition, and a CNN for video feature
extraction, and Ô¨Ånally fuse audio and visual modality at score-
level.Fordynamicimagesequencesrepresentingdeformations
and facial muscle movements, the popular visual features are
facial animation parameters or motion parameters. In [ 5], 34
m
otion parameters of head, eyebrows, eyes, and mouth are
collected as visual features from each facial image in dynamic
image sequences. In [ 46], 18 facial animation parameters are
e
xtracted with a facial feature tracking technique performed
on dynamic image sequences in video.
B. Multimodality Fusion
Multimodalityfusionis to integrateaudioand visual modal-
ities with different statistical properties. Existing fusion strate-
gies [12], [16], [17], [18] can be summarized into four cate-
g
ories,i.e., feature-level fusion, decision-level fusion, score-
level fusion, and model-level fusion, respectively.
Feature-level fusion is the most common and straightfor-
ward way, in which all extracted features are directly con-
catenated into a single high-dimensional feature vector. Then,
a single classiÔ¨Åer can be trained with this high-dimensional
feature vector for emotion recognition. Feature-level fusion
is thus also called Early Fusion (EF). A substantial number
of previous works [ 3], [5], [16], [47], [48] have testiÔ¨Åed the
p
erformance of feature-level fusion on audio-visual emotion
recognitiontasks.However,becauseitmergesaudioandvisualfeatures in a straightforward way, feature-level fusion can
not model the complicated relationships, e.g., the difference
on time scales and metric levels, between audio and visual
modalities.
Decision-level fusion aims to combine several unimodal
emotion recognition results through an algebraic combination
rule. SpeciÔ¨Åcally, each input modality is modeled indepen-
dently with an emotion classiÔ¨Åer, then these unimodal recog-
nition results are combined with certain algebraic rules such
as ‚ÄùMax‚Äù, ‚ÄùMin‚Äù, ‚ÄùSum‚Äù, etc. Thereby, decision-level fusion
is also called as Late Fusion (LF). In previousworks [ 5], [12],
[16], [47], [48], the authorshave adopteddecision-levelfusion
i
n audio-visual emotion recognition. Nevertheless, decision-
level fusion can not capture the mutual correlation among
different modalities, because these modalities are assumed
to be independent. Therefore, decision-level fusion does not
conform to the fact that human beings show audio and visual
expressionsin a complementaryredundantmanner,ratherthan
a mutually independent manner.
Score-level fusion, as a variant of decision-level fusion, has
been recently employed for audio-visual emotion recognition
[16], [18]. In [16], an equally weighted summation is adopted
t
o the obtained class score values. The emotion category
correspondingto themaximumvaluein thisfusedscore vector
is taken as the Ô¨Ånal predicted category. Note that, score-level
fusion is implemented by combining the individual classi-
Ô¨Åcation scores, which indicate the likelihood that a sample
belongs to different classes. By contrast, decision-level fusion
is performed by combining multiple predicted class labels.
Model-level fusion, as a compromise between feature-level
fusion and decision-level fusion, has also been used for audio-
visual emotion recognition. This method aims to obtain a
joint feature representation of audio and visual modalities.
Its implementation mainly depends on the used fusion model.
For instance, Zeng et al.[4] employ a Multi-stream Fused
H
idden Markov Models (MFHMM) to implement model-level
fusion. This MFHMM combines bimodal information from
audio and visual streams in terms of the maximum entropy
principle and the maximum mutual information criterion.
Linet al.[8] employ an error weighted semi-coupled Hidden
M
arkov Models (HMM) to fuse audio and visual streams for
emotionrecognition.In[ 46],aTripledHiddenMarkovModels
(
THMM) model is adopted to perform audio-visual emotion
recognition. As for neural networks, model-level fusion is
performed by Ô¨Årst concatenating feature representations of
different hidden layers of neural networks corresponding to
multiple input modalities. Then, an additional hidden layer
is added to learn a joint feature representation from the
concatenated feature representations [ 26], [27], [28]. Limited
b
y the shallow structure like the maximum entropy principle
[4] and single hidden layer [ 26], [27], [28], existing model-
l
evel fusion methods are still not effective in modeling highly
non-linear correlations between audio and visual modalities.
C. Summary
From the above-mentionedworks, we can make the follow-
ing two summarizations.
1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE
Transactions on Circuits and Systems for Video Technology
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 4
First, low-level hand-crafted features are widely-used for
audio and visual emotion recognition. However, these hand-
craftedfeaturescannotsufÔ¨ÅcientlyandefÔ¨Åcientlydiscriminate
emotions. It is thus desirable to develop automatic feature
learning algorithms to obtain high-level affective features.
CNNs [23] automatically learn features from raw pixels. With
r
aw Mel-spectrogram as inputs, CNNs may present a cue
for high-level audio feature extraction. CNNs have exhibited
promising performance on feature learning from static images
[44]. However it can not directly capture motion cues in
v
ideos.To addressthisissue, a 3D-CNN[ 33], whichcomputes
f
eature maps from both spatial and temporal dimensions, may
be a possible solution.
Second, Most of existing fusion methods belong to the
shallow fusion method, which can not effectively model
the complicated non-linear joint distribution and correlations
of multiple modalities [ 19],e .g., the feature concatenation.
Therefore, it is necessary to develop deep fusion methods
that leverage deep models for feature fusion. To alleviate
this problem, it is hence needed to design a deep fusion
model in which multiple meaningful fusion operations can be
performed to learn the complicated joint audio-visual feature
representation. Because each RBM in a DBN can be used to
learn the joint audio-visual feature representation, it may be
feasible to employ a DBN consisting of multiple layers of
RBMs as a deep fusion model.
III. PROPOSED METHOD
As described in Fig. 1, our hybrid deep learning model
c
ontains two individual input streams, i.e., the audio network
processing audio signals with a CNN model, and the visual
network processing visual data with a 3D-CNN model. The
outputs of fully connected layers of these two networks are
fused in a fusion network built with a DBN model.
Due to the limited amount of labeled data, we Ô¨Årst employ
the existing CNN and 3D-CNN models pre-trained on large-
scaleimageandvideoclassiÔ¨ÅcationtaskstoinitializeourCNN
and 3D-CNN, respectively. Then, Ô¨Åne-tuning is conducted for
these two CNN models with the labeled emotion data. To this
end, we adopt the AlexNet [ 23] for CNN network initializa-
t
ion,andtheC3D-Sports-1Mmodel[ 33]for3D-CNNnetwork
i
nitialization,respectively.TheAlexNet[ 23]has5convolution
l
ayers (Conv1-Conv2-Conv3-Conv4-Conv5), 3 max-pooling
layers(Pool1-Pool2-Pool5),and3 fully connected(FC) layers.
The Ô¨Årst two FC layers (fc6, fc7) consist of 4096 units and
the last FC layer (fc8) has 1000 dimensions corresponding
to 1000 image categories. The C3D-Sports-1M model [ 33]
c
ontains 8 convolution layers (Conv1a-Conv2a- ¬∑¬∑¬∑-Conv5a-
Conv5b), 5 max-pooling layers (Pool1-Pool2-Pool3-Pool4-
Pool5), followed by 3 FC layers. In this 3D-CNN, its fc6,
fc7 also have 4096 units, and its fc8 corresponds to 487 video
categories.ToinitializetheaudioandvisualnetworksinFig. 1,
w
e copytheinitial networkparametersfromthecorresponding
pre-trainedCNN and 3D-CNN models mentionedabove. Note
that the fc8 parametersin these two pre-trainedmodelsare not
used.In the followings, we describe how to generate the inputs of
both CNN and 3D-CNN, and how this hybrid deep learning
model is trained.
A. Generation of Network Inputs
Since emotional video samples may have different duration,
we split each of them into a certain number of overlapping
segments and then learn audio-visual features from each
segment. This also enlarges the amount of training data for
our deep models. In detail, we Ô¨Årst extract the whole log
Mel-spectrogram from audio signals. The extracted log Mel-
spectrogram is computed with the output of Mel-frequency
Ô¨Ålter banks, and shows more discriminant power than MFCC
for audio emotion recognition [ 49]. Then, we use a Ô¨Åxed
c
ontext window to split the spectrogram into overlapping
segments which are converted into the suitable input of CNN.
The corresponding video segment in this context window is
used as the input of 3D-CNN after preprocessing. In this
way, for each video segment, we produce its Mel-spectrogram
segment and video frames in the framework as illustrated in
Fig.1. In the followings, we present how these audio and
v
isual cues are processed in detail.
1)Audio input generation: It is known that the 1-D
spectrogram, represented by the squared magnitude of the
time-varying spectral characteristics of audio signals, con-
tains tremendous low-level acoustic information related to the
speaker‚Äôs emotion expression, such as energy, pitch, formants,
andso on[ 50]. However,CNNs arecommonlyusedto process
2
-D or 3-D images in vision tasks [ 23]. To leverage the
a
vailable CNN models and make our deep model initialization
easier, it is hence intuitive to transform the 1-D spectrogram
into a 2-D array as the input of CNN.
Recently, Abdel-Hamid et al., [51] have employed a CNN
w
ith a shallow 1-layer structure for speech recognition. Spe-
cially, the authors extract the log Mel-spectrogram from raw
audio signals and reorganize it into a 2-D array as the input
of CNN. Then 1-D convolution can be applied along the
frequency axis. Nevertheless, audio emotion recognition is
different from speech recognition [ 51]. First, 1-D convolution
o
peration along the frequency axis can not capture the useful
temporal information along the time axis for emotion recog-
nition. Second, a speech segment length of 15 frames (about
165 ms) widely-used for speech recognition does not carry
sufÔ¨Åcient temporal cues for distinguishing emotion. Some
previous studies also show that 250 ms is the suggested
minimum segment length required for identifying emotion
[52], [53].
A
s shown in Fig. 1, to convert the 1-D audio signals
i
nto the suitable input of CNN, we extract three channels
of log Mel-spectrogram segment ( i.e., thestatic,deltaand
delta‚àídelta) with size 64√ó64√ó3. SpeciÔ¨Åcally, for a
given utterance we adopt 64 Mel-Ô¨Ålter banks from 20 to
8000 Hz to obtain the whole log Mel-spectrogram by using
a 25ms Hamming window and a 10ms overlapping. Then,
a context window of 64 frames is used to divide the whole
log Mel-spectrogram into audio segments with size 64√ó64.
A shift size of 30 frames is used during segmentation, i.e.,
1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE
Transactions on Circuits and Systems for Video Technology
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 5
two adjacent segments are overlapped with 30 frames. Each
divided segment hence has a length of 64 frames and its time
duration is 10ms√ó(64‚àí1)+25ms = 655ms . In this case, the
divided segment length is 2.5 times longer than the suggested
minimum segment length (250 ms) for identifying emotion
[52], [53]. Consequently, each divided segment conveys sufÔ¨Å-
c
ienttemporalcuesforidentifyingemotion.Theproduced2-D
Mel-spectrogramsegmentwith size 64√ó64is takenas the Ô¨Årst
channel (static) among three channels of Mel-spectrogram.
After extracting the static Mel-spectrogram segment with
size64√ó64, we compute its Ô¨Årst-order ( delta) and second-
order (delta‚àídelta) frame-to-frame time derivatives. This
is used to better capture the temporal information of Mel-
spectrogram, i.e.,the feature trajectories over time, as usually
done in speech recognition tasks [ 54].
T
o calculate the delta coefÔ¨Åcients of the static 2-D Mel-
spectrogram segment, the following regression formula is
used:
dt=N/summationtext
n=1n(ct+n‚àíct‚àín)
2N/summationtext
n=
1n2, (1)
wheredtis a delta coefÔ¨Åcients of frame tcomputed using
the static Mel-spectrogram segment coefÔ¨Åcients ct+ntoct‚àín.
The value of Nrepresents the regression window with a
typical value of 2. Then, in the same way we can calculate the
delta‚àídeltacoefÔ¨Åcients from the obtained deltacoefÔ¨Åcients.
As a result, we can obtain three channels of Mel-spectrogram
segment with size: 64√ó64√ó3, as illustrated in Fig. 1.
T
his extracted Mel-spectrogram can be regarded as the
RGB image feature representation of audio data. It has two
desired properties. First, we can use it to implement the 2-
D convolution operation along the frequency and time axis,
ratherthanthe 1-D convolutionoperation.Second,as theRGB
image feature representation, it is convenient to resize it into
the suitable size as the input of the pre-trained CNN models.
SpeciÔ¨Åcally, we initialize the audio network with the AlexNet
[23], which has the input size 2 27√ó227√ó3. Therefore, we
resize the original spectrogram with size 64√ó64√ó3into
new size: 227√ó227√ó3with bilinear interpolation. In the
followings, we denote the audio input as a.
2)Visual input generation: After splitting the video sample
into segments, we use the video segments as the 3D-CNN
input. For each frame in the video segment, we run face
detection, estimate the eye distance, and Ô¨Ånally crop a RGB
face image of size 150√ó110√ó3, as done in [ 55], [56]. In
d
etail, we employ the robust real-time face detector presented
by Viola and Jones [ 57] to perform automatic face detection
o
n each frame. From the results of automatic face detection,
the centers of two eyes can be located in a typical up-right
face. Then, we calculate the eye distance of facial images and
normalized it to a Ô¨Åxed distance of 55 pixels. For a facial
image, it is usually observed that its height is roughly three
timeslongerthantheeyedistance,whereasitswidthisroughly
twice. Consequently, based on the normalized eye distance, a
resized RGB image of 150√ó110√ó3is Ô¨Ånally cropped from
each frame. To conduct a Ô¨Åne-tuning task, the cropped facialimage for each frame is resized to 227√ó227√ó3as the input
of the pre-trained 3D-CNN model. Similar resize operation is
also used in a previous work [ 58].
T
o make sure each video segment has 16 frames, i.e., the
input size in C3D-Sports-1M model [ 33], we delete the Ô¨Årst
a
nd lastL‚àí16
2overlapping frames if a video segment has L‚â•
16frames. On the contrary, for L <16we repeat the Ô¨Årst and
last16‚àíL
2overlapping frames. It should be noted that, since
we employ 64 audio frames in a context window to divide
the extracted log Mel-spectrogram into audio segments, the
durance of each segment is 655ms corresponding to about 20
videoframesin eachvideosegment, i.e.,0.655s√ó30frame/s.
In this case, our implementationdoesnot needto deal with the
case with L <16frames. By contrast, when using 15 audio
frames of Mel-spectrogram segments corresponding to about
5 video frames ( L= 5) for experiments, we need to repeat
the Ô¨Årst 5 and last 6 overlapping frames. We denote the visual
input asv.
B. Network Training
Given audio-visual data X={(ai,vi,yi)}i=1,2,¬∑¬∑¬∑,K, where
iis the index of the divided audio-visual segments, aiand
videnote the audio data and visual data, respectively, and yi
represents the class label of a segment. Note that, we use the
class label of the global video sample as the class label of a
segmentyi. LetŒ•A(ai;Œ∏A)denotes the 4096-D output of fc7
in audio network (denoted as A) with network parameters Œ∏A.
Similarly, Œ•V(vi;Œ∏V)denotes the 4096-D visual feature (fc7)
of the visual network (denoted as V) with network parameters
Œ∏V.Duringnetworktraining,weÔ¨Årst traintheaudioandvisual
networks respectively in the Ô¨Årst stage, then jointly train the
fusion network in the second stage.
1)Training audio and visual networks: The audio and
visual networks are Ô¨Årst trained individually with a Ô¨Åne-
tuning scheme. For the CNN and 3D-CNN, we replace their
Ô¨Ånal fully connected layers, i.e., fc8 layer, with two new
FC layers, which correspond to the emotion categories on
target audio-visual emotion recognition dataset. For instance,
for 6 emotions, fc8 should produces 6 outputs. Accordingly,
we predict the emotional labels with the audio and visual
networks, respectively,then calculate the prediction errors and
Ô¨Ånallyupdatethenetworkparameterstominimizethenegative
log likelihood Lover the training data.
On audiotrainingdata,we solvethe followingminimization
problemto updatetheaudionetwork Awithbackpropagation:
min
WA,Œ∏AK/summationdisplay
i=1L(softmax( WA¬∑Œ•A(ai;Œ∏A)),yi),(2)
whereWAis the weight values of the softmax layer, and the
softmax log-loss is calculated by
L(A,y) =‚àíl/summationdisplay
j=1yjlog(yA
j), (3)
whereyjis thej-th value of the ground truth label, yA
j
represents the j-th output value of the softmax layer for the
networkA, andlrepresents the total number of class labels.
1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE
Transactions on Circuits and Systems for Video Technology
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 6
On visual training data, we solve a minimization problem
similar to the audio network, since a 3D-CNN has the same
minimization problem as CNNs. In this way, we can minimize
the prediction error of Vto update the visual network V.
During the Ô¨Årst stage of training, we can separately update
the parameters in the audio and visual networks, producing
more discriminativeaudio and visual features, i.e.,Œ•A(ai;Œ∏A)
andŒ•V(vi;Œ∏V). To fuse the audio and visual features, we
proceed to describe the training of our fusion network.
2)Training fusion network: After training the audio and
visual networks, we discard their fc8 layers and merge their
fc7 layers into the fusion network illustrated in Fig. 1. In this
w
ay, two 4096-D features i.e.,Œ•A(ai;Œ∏A)andŒ•V(vi;Œ∏V)
are concatenated to constitute a 8192-D feature as the in-
put of the fusion network f([Œ•A
i,Œ•V
i];Œ∏F)(denoted as F)
with network parameters Œ∏F. Here,Œ•A
i= Œ•A(ai;Œ∏A)and
Œ•V
i= Œ•V(vi;Œ∏V).
Our fusion network is built with a deep DBN model,
which aims to capture highly non-linear relationships across
modalities,andformajointdiscriminantfeaturerepresentation
for emotion classiÔ¨Åcation. It contains one visible layer, two
hidden layers and one output layer ( i.e., softmax layer), as
depicted in Fig. 1. This DBN model is constructedby stacking
t
wo RBMs, each of which is a bipartite graph and its hidden
nodes are able to obtain higher-order correlation of input data
of visible nodes.
Following [ 59], we train the fusion network through two
t
raining steps. First, an unsupervised pre-training is imple-
mented in the bottom-up manner by using a greedy layer-wise
training algorithm [ 59]. This unsupervised pre-training aims
t
o minimize the following reconstruction error, i.e.,
min
WF,Œ∏FK/summationdisplay
i=1C(zi,z‚Ä≤
i), (4)
whereKis the number of training samples, C(zi,z‚Ä≤
i)denotes
the cross-entropy loss function between the input data ziand
the reconstructed data z‚Ä≤
i. Here,C(zi,z‚Ä≤
i)is deÔ¨Åned as
C(zi,z‚Ä≤
i) =D/summationdisplay
d=1(‚àízi,jlogz‚Ä≤
i,d+(1‚àízi,d)log(1‚àíz‚Ä≤
i,d)),(5)
Second,afterpre-training,eachlayerofRBMs isinitialized.
Then, a supervised Ô¨Åne-tuning is performed to optimize the
network parameters. In detail, we take the last hidden layer
output as the input of a classiÔ¨Åer, and compute the classiÔ¨Å-
cation error. Then, back propagation is used to readjust the
network parameters.
Since the input features of DBNs are continuous values, we
use a Gaussian-Bernoulli RBM with 4096 hidden nodes for
its Ô¨Årst layer, a Bernoulli-Bernoulli RBM with 2048 hidden
nodes for its second layer, outputting 2048-D features for
emotion classiÔ¨Åcation. In this way, we get a 8192-4096-2048-
Cstructure of DBNs, which is used to identify Cemotions
on target audio-visual emotional datasets. Note that, we Ô¨Åx
the parameters in AandVduring the second stage training,
and update the parameters of the fusion network Fto produce
moreaccurateemotionalpredictions,resultinginbetter feature
fusion results.
DQJHU M R\ VDGQHVV VXUSULVH IHDU GLVJXVW
Fig. 2. Some samples of the cropped facial images from the RML d ataset.
DQJHU M R\ VDGQHVV VXUSULVH IHDU GLVJXVW
Fig. 3. Some samples of the cropped facial images from the eNTE RFACE05
dataset.
C. Emotion ClassiÔ¨Åcation
After Ô¨Ånishing training the fusion network, a 2048-D joint
feature representation can be computed on each audio-visual
segment. Since each audio-visual video sample has a different
numberofsegments,average-poolingisappliedonall segment
features from each video sample to form the Ô¨Åxed-length
globalvideofeaturerepresentation.Ourexperimentscompared
average-pooling and max-pooling, and found average-pooling
performs better. Therefore, we employ average poling to
processfeaturesextractedfromsegments.Based on thisglobal
video feature representation, the linear SVM classiÔ¨Åer can be
easily employed for emotion identiÔ¨Åcation.
IV. EXPERIMENTS
To testify the effectiveness of our proposed hybrid deep
learning networks for audio-visual emotion recognition, we
conduct emotion recognition experiments on three public
audio-visual emotional datasets, including the acted RML
dataset [3], the acted eNTERFACE05 dataset [ 60], and the
s
pontaneous BAUM-1s dataset [ 12]. To evaluate the perfor-
m
ance of our proposed method, we present the unimodal
audio and video emotion recognition results, and then give
the multimodal emotion recognition results integrating audio
and video cues.
A. Datasets
RML: The RML audio-visual dataset [ 3] is composed of
7
20 video samples from 8 subjects, speaking the six different
languages (English, Mandarin, Urdu, Punjabi, Persian, and
Italian). It contains the six emotions: anger, disgust, fear,
joy, sadness, and surprise. The audio samples are recorded
with a sampling rate of 22,050 Hz with 16-bit resolution and
mono channel. At least two participants who do not know the
corresponding language are employed in human perception
1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE
Transactions on Circuits and Systems for Video Technology
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 7
DQJHU MR\ VDGQHVV V XUSULVH IHDU GLVJXVW
ƒÇ∆µ≈µÕ≤œ≠∆ê
Fig.4. Somesamples ofthecropped facial imagesfromtheBAUM -1sdataset.
test to evaluate whether the correct emotion is expressed. A
video sample is added to this dataset, when all testing subjects
are able to perceive the intended emotion categories. The
average duration of each video sample is around 5 seconds.
The size of original video frame is 720√ó480√ó3. In our
experiments, for each frame in a video segment, we crop a
facial image with size 150√ó110√ó3, as described in Section
III-A. Fig.2shows some samples of the croppedfacial images
o
n the RML dataset.
eNTERFACE05 : The eNTERFACE05 [ 60] audio-visual
a
cted dataset includes the six emotions, i.e., anger, disgust,
fear,joy,sadness,andsurprise,from43subjectswith14differ-
ent nationalities. It contains 1290 video samples. Each audio
sample is recorded with a sampling rate of 48,000 Hz with
16-bit resolution and mono channel. Each subject is asked to
listen to the six successive short stories, each of which is used
to induce a particular emotion. Two experts are employed to
evaluate whether the reaction expresses the intended emotion
inan unambiguousway.Thespeechutterancesarepulledfrom
video Ô¨Åles of the subjects speaking in English. The video Ô¨Åles
are in average 3-4 seconds long. The size of original video
frames is 720√ó576√ó3. Fig.3gives some samples of the
c
ropped facial images on the eNTERFACE05 dataset.
BAUM-1s : The BAUM-1s [ 12] audio-visual spontaneous
d
ataset contains1222 video samples from31 Turkish subjects.
The dataset has the six basic emotions (joy, anger, sadness,
disgust, fear, surprise) as well as boredom and contempt. It
also contains four mental states, namely unsure, thinking,
concentrating and bothered. To obtain spontaneous audio-
visual expressions, emotion elicitation by watching Ô¨Ålms is
employed. The size of original video frames is 720√ó576√ó3.
Similar to [ 3], [12], [60], this work focus on recognizing the
s
ix basic emotions, which appear in total 521 video clips. Fig.
4gives some samples of the cropped facial images on the
B
AUM-1s dataset.
We divide a video sample into a certain number of audio
andvideosegmentsasthe inputofCNN and3D-CNN, respec-
tively. Because multiple segments can be generated from each
videosample,the amountoftrainingdatawill beenlarged.For
example, we generate 11,316 audio-video segments from 720
video samples on the RML dataset, generate 16,186 segments
from 1290 video samples on the eNTERFACE05 dataset,
and generate 6386 segments from 521 video samples on the
BAUM-1s dataset, respectively.TABLE I
SUBJECT-INDEPENDENT UNIMODALITY RECOGNITION ACCURACY (%)ON
THREE DATASETS .AlexAudioANDC3DV isual ARE AUDIO AND VISUAL
FEATURES EXTRACTED BY THE ORIGINAL PRE -TRAINED ALEXNET AND
C3D-S PORTS-1MMODELS,RESPECTIVELY .AnetANDVnetARE THE
LEARNED FEATURES OF THE FINE -TUNED AUDIO NETWORK (CNN) AND
VISUAL NETWORK (3D-CNN), RESPECTIVELY .
Unimodality Features RMLeNTERFACE05 BAUM-1s
AudioAlexA udio59.46 51.33 36.10
Anet 66.17 78.08 42.26
VisualC3DV isual53.03 48.97 41.69
Vnet 68.09 54.35 50.11
B. Experimental Setup
F
or training our deep models, we use a mini-batch size of
30, and a stochastic gradient descent with stochastic momen-
tum of 0.9. The learning rate is 0.001 for Ô¨Åne-tuning. The
number of epochs is set to 300 for CNNs, 400 for 3D-CNNs,
and 100 for DBNs, respectively. For the ‚ÄùFC‚Äù fusion method,
the dropout parameter is set to 0.3. We implement CNNs with
theMatConvNettoolbox,13D-CNNswiththeCaffetoolbox,2
as well as DBNs with the DeeBNet toolbox.3One NVIDIA
G
TXTITANXGPUwith12GBmemory,isusedtotrainthese
deep models. For emotion classiÔ¨Åcation, we utilize the LIB-
SVM package,4to performtheSVM algorithmwith the linear
k
ernel function and one-versus-one strategy. As suggested in
[61], we adopt a subject-independent Leave-One-Subject-Out
(
LOSO)and Leave-One-Speakers-Group-Out(LOSGO) cross-
validation strategies for experiments, which are commonly
used in real-world applications. In detail, on the RML dataset
with less than 10 persons, we employ the LOSO scheme.
On the eNTERFACE05 and BAUM-1s datasets with more
than 10 persons, we adopt the LOSGO scheme with Ô¨Åve
speaker groups. The average accuracy in the test-runs are
Ô¨Ånally reported to evaluate the performance of all compared
methods.
C. Experimental Results and Analysis
Inthissection,we presentexperimentalresultsofunimodal-
ity and multimodality features on the RML, eNTERFACE05,
and BAUM-1s datasets, respectively.
1) UnimodalityPerformance: Totestifytheeffectivenessof
feature learning with deep models, we present the recognition
performance of two types of features, i.e., features extracted
with the original AlexNet and C3D-Sports-1M models, and
the learned features extracted with the Ô¨Åne-tuned AlexNet and
C3D-Sports-1M models. For the features extracted with the
originalAlexNetandC3D-Sports-1Mmodels,we directlytake
our generated audio and visual data as the inputs of AlexNet
and C3D-Sports-1M models, producing 4096-D features from
the outputs of their fc7 layers, respectively.
TableIshows the recognition performanceof these features
o
n the RML, eNTERFACE05, and BAUM-1s datasets. From
1available at http://www.vlfeat.org/matconvnet/
2available at http://caffe.berkeleyvision.org/
3available at http://ceit.aut.ac.ir/ keyvanrad/
4available at https://www.csie.ntu.edu.tw/ cjlin/libsvm/
1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE
Transactions on Circuits and Systems for Video Technology
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 8
TABLE II
SUBJECT-INDEPENDENT AUDIO EMOTION RECOGNITION PERFORMANCE
(%)COMPARISONS WITH PREVIOUS WORKS USING HAND -CRAFTED
FEATURES ON THREE DATASETS .AnetIS THE LEARNED FEATURES OF THE
FINE-TUNED AUDIO NETWORK (CNN).
Datasets Refs. Audio features Accuracy
RMLGaoe t al. ,[36] Prosody 51.04
Elmadany e t al. , [38] Prosody 56.25
Elmadany e t al. , [37] PNCC 58.33
Zhange t al. , [62] LLD 61.86
Ours Anet 66.17
eNTERFACE05Zhalehpour e t al.,[12]MFCC,RASTA-PLP 72.95
Schullere t al. , [35]Prosody, MFCC 72.40
Mansoorizadeh e t al., [5]Prosody 43.00
Bejanie t al. , [14]Prosody, MFCC 54.99
Ours Anet 78.08
BAUM-1sZhalehpour e t al.,[12]MFCC,RASTA-PLP 29.41
Ours Anet 42.26
TABLE III
SU
BJECT-INDEPENDENT VISUAL EMOTION RECOGNITION PERFORMANCE
(%)COMPARISONS WITH PREVIOUS WORKS USING HAND -CRAFTED
FEATURES ON THREE DATASETS .VnetIS THE LEARNED FEATURES OF THE
FINE-TUNED VISUAL NETWORK (3D-CNN).
Datasets Refs. Visual features Accuracy
RMLElmadany e t al. , [37]Gabor wavelet 64.58
Zhange t al. , [62] LBP 56.90
Ours Vnet 68.09
eNTERFACE05Zhalehpour e t al.,[12]LPQ 42.16
Mansoorizadeh e t al., [5]Facial points 37.00
Bejanie t al. , [14] QIM 39.27
Ours Vnet 54.35
BAUM-1sZhalehpour e t al.,[12]LPQ 45.04
Ours Vnet 50.11
the results in Table I, we can see that the learned features
w
ith Ô¨Åne-tuned deep models (AlexNet and C3D-Sports-1M)
signiÔ¨Åcantly outperform the features extracted with the origi-
nal pre-trained deep models. In detail, our Ô¨Åne-tuning strategy
improves the accuracies on the RML dataset from 59.46 %
to 66.17%for audio features, and 53.03 %to 68.09%for
visual features, respectively. Similarly, on the eNTERFACE05
dataset, our method also makes an improvement from 51.33 %
to 78.08%foraudiofeatures,and48.97 %to 54.35%forvisual
features,respectively.OntheBAUM-1sdataset, improvements
of 6.16%for audio features, and 8.42 %for visual features are
achieved, respectively. The experimental results demonstrate
the effectiveness of our feature learning strategy, i.e., using
deep model to learn emotional features. Our learned features
have potentialto leveragethe powerfullearningability ofdeep
models to extract more discriminative cues than the manually
designed features. The experimental results also show the
validity of our Ô¨Åne-tuning strategy. Fine-tuning allows deep
models pre-trained on other domains to learn meaningful
feature representations for emotion recognition.
To present the advantages of the learned features, we
directly compare our performance with the reported results of
previous works using hand-crafted features on these datasets.
Because these compared works use the same experimental
settings with ours, i.e., subject-independent test-runs, we take
their reported results for comparison. It is not suitable tocompare our work with previous works adopting subject-
dependent test-runs. Table IIand Table IIIseparately give
p
erformance comparisons of audio and visual emotion recog-
nition between our learned features and the corresponding
hand-crafted features.
From Table II, we can see that our learned audio features
w
ithCNNsoutperformthehand-craftedaudiofeatureswidely-
used for audio emotion recognition [ 5], [12], [35], [36],
[37], [38], [62], such as prosody features, MFCC, Relative
S
pectral Transform - Perceptual Linear Prediction (RASTA-
PLP), Power Normalized Cepstral CoefÔ¨Åcients (PNCC), and
other acoustic Low-level Descriptors (LLD). This shows that
our audio features learned by the Ô¨Åne-tuned AlexNet model
is more discriminative than the hand-crafted audio features
for audio emotion classiÔ¨Åcation. In addition, the promising
performanceofourlearnedaudiofeaturesclearlyindicatesthat
it is reasonable to employ three channels of Mel-spectrogram
with size 64√ó64√ó3as the input of AlexNet. It is also
interesting to know that AlexNet trained on image domain
can be applied to audio feature extraction. This might be
because of the powerful feature learning ability of AlexNet,
e.g., higher-level convolutions progressively infer semantics
from larger receptive Ô¨Åelds. The extracted Mel-spectrogram is
similar to the RGB image representation. This representation
makes it possible to Ô¨Årst extract meaningful low-level time-
frequency features by low-level 2-D convolutions, then infer
more discriminative features by higher levels of convolutions.
It is also possible that, three channels of Mel-spectrogram
present emotions as certain shapes and structures, which thus
can be effectively perceived by AlexNet trained on the image
domain. This thus presents a new method of transforming 1-D
audio signals into the suitable input of CNN that convention-
ally processes 2-D or 3-D images.
From Table III, it can be observed that our learned visual
f
eatures with 3D-CNNs yield better performance than the
comparedhand-craftedfeatures[ 5], [12], [14], [37], [62], such
a
s Gabor wavelet, LBP, LPQ, facial points and Quantized
Image Matrix (QIM). This demonstrates the advantages of
our learned visual features produced by the Ô¨Åne-tuned C3D-
Sports-1M model, which presents more discriminative power
than the hand-craftedvisual features for visual emotion recog-
nition. The above experimentsclearly show that deep model is
powerful in feature learning and producesmore discriminative
features than manually designed feature extraction models.
However,deep models require a large amount of training data.
This thus motivates us to transfer pre-trained models on other
domain for emotional feature learning.
TableIIandIIIshow that our learned features are more
d
iscriminative to the hand-crafted features on emotion recog-
nition tasks. However, our feature learning needs a large
training set, and is easier to suffer from overÔ¨Åtting than
the hand-crafted features. Moreover, extracting features with
deep models requires more expensive computation due to the
massive network parameters.
To explain why we extract Mel-spectrogram segments with
a length of 64 frames rather than 15 frames widely used
in speech recognition [ 51], we compare the performance of
t
wo types of extracted Mel-spectrogramswith different length,
1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE
Transactions on Circuits and Systems for Video Technology
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 9
TABLE IV
AUDIO EMOTION RECOGNITION PERFORMANCE (%)COMPARISONS
BETWEEN MEL-SPECTROGRAM SEGMENTS WITH SIZES 64√ó64√ó3AND
64√ó15√ó3.
Mel-spectrogram RMLeNTERFACE05 BAUM-1s
64√ó1 5√ó350.06 52.33 33.64
64√ó6 4√ó366.17 78.08 42.26
&11
Õ∏
Õ∂ ‡µà Õ∏Õ∂ ‡µà Õµ
 $XGLRQHWZRUN
'&11
9LVXDOQHWZRUN'IF
'I
F'$YHUDJH

SRROLQJ6909LGHR
H
PRWLRQV
)HDWXUHOHYHOIXVLRQ
&11
Õ∏
Õ∂ ‡µà Õ∏Õ∂ ‡µà Õµ
Õ≥Õ∑Õ≤ ‡µà Õ≥Õ≥Õ≤ ‡µà Õµ ‡µà  $XGLRQHWZRUN
'&11
9LVXDOQHWZRUN'IF
'I
F'HFLVLRQOHYHO
I
XVLRQ$YHUDJH

SRROLQJ690
$YHUDJH

SRROLQJ690D

E
Õ≥Õ∑Õ≤ ‡µà Õ≥Õ≥Õ≤ ‡µà Õµ ‡µà  
9LGHR
H
PRWLRQV
Fig. 5. The structure of feature-level fusion (a) and decisio n-level fusion (b)
with two CNN models.
i.e.,64√ó64√ó3and64√ó15√ó3. The experimental results
are summarized in Table IV. From Table IV, we can see
t
hat the extracted Mel-spectrogram with size 64√ó64√ó3
clearly outperforms the other one. This indicate that the
segment length of 15 frames is not suitable for audio emotion
recognition. This might be because 15 frames is too short to
conveysufÔ¨Åcient informationfordistinguishingemotions[ 52],
[53].
2
) Multimodality Performance: To verify the effectiveness
of our fusion method, we compare our method with four mul-
timodality fusion schemes, i.e.,feature-level fusion, decision-
level fusion, score-level fusion, as well as our recently-
presented method in [ 62]. Note that, this work employs a deep
D
BNmodeltobuildthefusionnetwork,whereas[ 62]usestwo
F
C layers.
Our goal is to recognize the emotion of the global video
samples. Therefore, feature fusion methods are required to
aggregate features extracted on audio-visual segments into a
TABLE V
MULTIMODALITY EMOTION RECOGNITION PERFORMANCE (%)
COMPARISON OF SIX ENSEMBLE RULES AT DECISION -LEVEL FUSION WITH
THE LEARNED FEATURES OF AnetANDVnet.
Decision-level RMLeNTERFACE05 BAUM-1s
Majority vote 63.58 69.14 45.35
Max 72.05 80.31 48.89
Sum 72.79 80.85 50.01
Min 72.91 78.76 50.08
Average 72.79 80.85 50.01
Product 74.60 81.62 51.73TABLE VI
SU
BJECT-INDEPENDENT MULTIMODALITY EMOTION RECOGNITION
ACCURACY (%)WITH THE LEARNED FEATURES OF AnetANDVnet. FC
DENOTES THE FUSION METHOD BUILT WITH TWO FCLAYERS IN [62],
D
BNDENOTE THE FUSION METHOD BUILT WITH A DBNMODEL.
Fusion method RMLeNTERFACE05 BAUM-1s
Feature-level 74.04 81.02 51.72
Product 74.60 81.62 51.73
Score-level 73.92 80.85 51.58
FC 78.84 83.55 52.35
DBN 80.36 85.97 54.57
global video feature representation. Then, a linear SVM coul d
be used to perform emotion classiÔ¨Åcation on the generated
global video features. To this end, average-pooling is used, as
doneinSection III-C. Fig.5givesthe structureoffeature-level
f
usion and decision-level fusion with our two CNN models.
Note that, the structure of score-level fusion is completely
similar to decision-level fusion.
For decision-level fusion, six typical ensemble rules [ 63],
[64], including ‚ÄúMajority vote‚Äù, ‚ÄúMax‚Äù, ‚ÄúSum‚Äù, ‚ÄúMin‚Äù, ‚ÄúAv-
e
rage‚Äù and ‚ÄúProduct‚Äù are testiÔ¨Åed. For more details about the
six ensemblerules,referto [ 63], [64]. Ondecision-levelfusion
t
asks, we Ô¨Årst investigate the performance of each ensemble
rule, and then Ô¨Ånd the best one, which is used to generate
the reported performance. Table Vpresents the performance
c
omparison of six ensemble rules on our learned features. As
shown in Table V, the ‚ÄúProduct‚Äù rule yields best performance.
T
herefore, in the following experiments, we only report the
performance of the ‚ÄúProduct‚Äù rule for decision-level fusion.
We implementscore-levelfusionreferringto the schemesin
[16]. SpeciÔ¨Åcally, a equally weighted summation is adopted in
t
erms of the obtained class score values, as described below:
Scorefusion= 0.5Scoreaudio+0.5Scorevisual.(6)
TableVIshows the recognitionperformanceof audio-visual
m
odalities by using different fusion strategies. From Table VI,
w
e can observe that the FC fusion method [ 62] outperforms
f
eature-levelfusion,decision-levelfusion(Product),andscore-
level fusion. This demonstrates the advantages of the fusion
network built with two FC layers. This also implies that the
FC fusion network is able to learn a joint audio-visual feature
representation from the outputs of two Ô¨Åne-tuned deep models
for emotion identiÔ¨Åcation through the back-propagationlearn-
ing algorithms.
It also can be observed from Table VIthat, our DBN fusion
m
ethod outperforms the other fusion methods. Compared
with feature-levelfusion,decision-levelfusion,andscore-level
fusion methods, the DBN fusion can be regarded as a deep
fusion model. The comparison clearly shows the effectiveness
ofthedeepfusionmethod,whichshowsbetterfeaturelearning
ability in capturing the highly non-linear relationships across
modalities. This is mainly because of the use of the multi-
layer structure of DBNs, in which multiple RBMs are stacked
to form multiple hidden layers. A RBM is a generative model
representing a probability distribution associated with input
data. Each RBM in a DBN is able to learn the joint probability
1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE
Transactions on Circuits and Systems for Video Technology
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 10
88.70
0.90
0.00
1.94
3.79
8.622.61
95.50
4.20
0.97
3.03
0.000.00
0.00
71.33
15.53
1.52
0.001.74
0.90
20.98
68.93
8.33
4.310.00
1.80
3.50
2.91
83.33
0.006.96
0.90
0.00
9.71
0.00
87.07anger disgust fear joy sadness surprise
anger
disgust
fear
joy
sadness
surprise
Fig. 6. Confusion matrix of multimodality emotion recogniti on results when
DBN performs best on the RML dataset.
90.61
1.36
3.00
0.00
1.34
4.610.94
89.55
2.00
3.24
0.45
1.844.23
2.73
79.50
0.46
9.82
8.292.35
1.82
0.50
92.13
0.45
2.300.47
3.18
5.50
0.46
83.93
3.231.41
1.36
9.50
3.70
4.02
79.72anger disgust fear joy sadness surprise
anger
disgust
fear
joy
sadness
surprise
Fig. 7. Confusion matrix of multimodality emotion recogniti on results when
DBN performs best on the eNTERFACE05 dataset.
distribution of input audio-video data. By using multiple
RBMs and the layer-wise training algorithm, DBNs can effec-
tively learn the non-lineardependenciesacross modalities, and
results in better fusion of audio-visual features. This Ô¨Ånding is
consistent with the one in a previous work [ 19]. It is desirable
t
o intuitively visualize the learned weights of our DBN model.
However, the input of our DBNs are audio-visual features,
ratherthanthesemanticimages.Thismakethelearnedweights
of DBNs hard to interpret intuitively.
DBNs also outperform the FC fusion method in [ 62],e .g.,
78.84%vs. 80.36 %on the RML dataset, 83.55 %vs. 85.97 %
on the eNTERFACE05 dataset, and 52.35 %vs. 54.57%on the
BAUM-1s dataset, respectively. This advantage might be due
tothe unsupervisedpre-traininginDBNs, whichpresentslocal
optimal weights for network initialization, whereas the initial
weights in the FC fusion method are randomly produced.
Fig.6, Fig.7and Fig.8present the classiÔ¨Åcation confusion
m
atrix on three datasets, respectively. Note that, these con-
fusion matrixes are obtained in terms of the average LOSO27.78
7.23
10.98
0.00
15.38
5.8811.11
65.11
1.83
0.00
20.00
0.0036.11
5.96
53.05
25.00
27.69
5.882.78
1.70
14.02
25.00
7.69
17.658.33
16.17
12.20
25.00
26.15
5.8813.89
3.83
7.93
25.00
3.08
64.71anger joy sadness fear disgust surprise
anger
joy
sadness
fear
disgust
surprise
Fig. 8. Confusion matrix of multimodality emotion recogniti on results when
DBN performs best on the BAUM-1s dataset.
TABLE VII
MULTIMODALITY PERFORMANCE (%)MEASURE FOR EACH EMOTION
WHENDBNGIVES AN AVERAGE ACCURACY OF 80.36%ON THERML
DATASET .
Emotion Precision RecallF-score
Anger 85.3388.7086.98
Disgust 89.7195.5092.51
Fear 80.7171.3375.73
Joy65.5368.9367.19
Sadness 91.0383.3387.01
Surprise 83.2187.0785.10
or LOSGO recognition results. It is interesting to Ô¨Ånd that
o
n the RML dataset, ‚Äújoy‚Äù and ‚Äúfear‚Äù are more difÔ¨Åcult to
be identiÔ¨Åed than the other emotions. This might be because
the audio-visual cues of ‚Äòjoy‚Äù and ‚Äúfear‚Äù are not distinct
enough. On the eNTERFACE05 dataset, ‚Äúsadness‚Äù, ‚Äúsurprise‚Äù
and ‚Äúfear‚Äù are recognized with relatively lower accuracy, i.e.,
about 80%, whereas others emotions are identiÔ¨Åed well with
accuracy of about 90 %. On the BAUM-1s dataset, the average
classiÔ¨Åcation accuracy is much lower than the ones of the
other two datasets. This shows that the spontaneous emotions
are more difÔ¨Åcult to be recognized than the acted emotions.
In addition to the classiÔ¨Åcation confusion matrix, we com-
pute precision, recall and F-score to further measure the mul-
timodality emotion recognition performance on three datasets.
TheexperimentalresultsarepresentedinTable VII,TableVIII
and Table IX, respectively. The results in these three tables
i
ndicate that the three datasets show different difÔ¨Åculties in
recognizing speciÔ¨Åc emotions. For example, it is easier to
identify ‚Äúdisgust‚Äù on the RML dataset than the other two
datasets. It is easier to identify ‚Äújoy‚Äù on the eNTERFACE05
and BAUM-1s than the RML dataset.
D. Effect of Deep Structures in the Fusion network
The structure of DBNs may heavily affect its performance
of fusingaudio-visualmodalities.To evaluatethe effectiveness
of different deep structures, we present the performance of
1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE
Transactions on Circuits and Systems for Video Technology
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 11
TABLE VIII
MULTIMODALITY PERFORMANCE (%)MEASURE FOR EACH EMOTION
WHENDBNGIVES AN AVERAGE ACCURACY OF 85.97%ON THE
ENTERFACE05 DATASET .
Emotion Precision RecallF-score
Anger 89.7890.6190.20
Disgust 91.3689.5590.45
Fear 75.6979.5077.55
Joy92.5592.1392.34
Sadness 86.7383.9385.31
Surprise 79.9579.7279.84
TABLE IX
MU
LTIMODALITY PERFORMANCE (%)MEASURE FOR EACH EMOTION
WHENDBNGIVES AN AVERAGE ACCURACY OF 54.57%ON THE
BAUM-1 S DATASET .
Emotion Precision RecallF-score
Anger 27.0627.7827.41
Joy82.0365.1172.60
Sadness 52.4953.0552.77
Fear 20.1425.0022.31
Disgust 25.6526.1525.90
Surprise 41.9364.7150.89
three DBN fusion networks: DBN-1 (8192-4096-6), DBN-
2
(8192-4096-2048-6),and DBN-3 (8192-4096-2048-1024-6).
Similarly, we also show the performance of three FC fusion
networks corresponding to DBNs: FC-1 (8192-4096-6), FC-
2 (8192-4096-2048-6), and FC-3 (8192-4096-2048-1024-6).
For these fusion networks, as done in [ 62], a dropout layer is
a
dded before the Ô¨Ånal softmax layer correspondingto emotion
categories. The dropout parameter is set to 0.3 to reduce over-
Ô¨Åtting.
TableXpresents the performance comparisons of different
s
tructuresinthefusionnetwork.FromTable X,wecanseethat
F
C-1 performs best among the three FC fusion networks. This
indicates that FC-1 is more effective than FC-2 and FC-3 to
fuse audio and visual cues. This might be because with more
layers in the FC network, the massively increasing network
parameters make the FC network prone to over-Ô¨Åtting. For
the DBN fusion network, DBN-2 slightly outperforms DBN-
3, and yields substantially better performance than DBN-1.
Due to using multiple RBMs and the effective layer-wise
training algorithm [ 21], the deeper DBN models, i .e., DBN-
2 and DBN-3, exhibit better feature fusion ability than the
1-layer DBN-1. DBN-3 degrades the performance of DBN-2
may be because DBN-3 is deeper than DBN-2, thus involves
morenetworkparameters,which are more difÔ¨Åcultto optimize
on a small-scale training dataset.
E. Comparisons with the state-of-the-art results
We compare our method with some previous works on
three datasets in Table XI. Note that these works also conduct
s
ubject-independent experiments, which are consistent with
our experimental setting. The results in Table XIindicate that
o
ur method is very competitive to the state-of-the-art results.
Specially, on the acted RML and eNTERFACE05 datasets,
our method outperforms previous works [ 5], [12], [13], [14],T
ABLE X
SUBJECT-INDEPENDENT MULTIMODALITY EMOTION RECOGNITION
ACCURACY (%)OF DIFFERENT DEEP STRUCTURES IN THE FUSION
NETWORK .
Fusion method RMLeNTERFACE05 BAUM-1s
FC-1 78.84 83.55 52.35
FC-2 77.92 82.72 51.50
FC-3 75.67 81.25 50.43
DBN-1 78.50 84.16 51.61
DBN-2 80.36 85.97 54.57
DBN-3 80.10 85.02 53.38
TABLE XI
MU
LTIMODALITY EMOTION RECOGNITION PERFORMANCE (%)
COMPARISONS WITH STATE -OF-THE-ART WORKS ON THREE DATASETS .
Datasets Refs. Accuracy
RMLSarvestani e t al. , [13]72.03
Elmadany e t al. , [37]75.00
Zhange t al. , [62]74.32
Ours 80.36
eNTERFACE05Sarvestani e t al. , [13]70.11
Mansoorizadeh e t al., [5]71.00
Bejanie t al. , [14]77.78
Zhalehpour e t al., [12]77.02
Ours 85.97
BAUM-1sZhalehpour e t al., [12]51.29
Ours 54.57
[37] by more than about 5 %. On the spontaneous BAUM-1s
dataset, we improve the performance of [ 12] from 51.29 %t o
54.57%. These compared works use hand-crafted features and
shallow fusion methods to integrate audio-visual modalities.
This thus shows the advantages of our learned features and
fusion strategy.
In addition, our method also improves our previous work
[62] from 74.32 %t o 80.36%on the RML dataset. This is
achievedbytwo improvements.First, comparedwith the CNN
models in [ 62], 3D-CNN models in this work can extract
s
patial-temporal cues from video. Second, as shown in our
experiments,the DBN fusion methodshows better multimodal
feature fusion ability than the FC fusion method in [ 62].
V
. CONCLUSIONS AND FUTUREWORK
This paper presents a new method for audio-visual emotion
recognitionwith a hybrid deep learning frameworkintegrating
CNN, 3D-CNN and DBN. The outputs of audio and visual
networks are connected with a deep DBN model to fuse
audio and visual cues. To learn a joint discriminant feature
representation, this network is trained in two stages: 1) the
audio and visual networks are initialized with the pre-trained
AlexNet and C3D-Sports-1M for Ô¨Åne-tuning, and 2) the DBN
fusion network is trained on the target emotion classiÔ¨Åcation
datasets. Experimental results on the RML, eNTERFACE05,
and BAUM-1s datasets show that our hybrid deep learning
model jointly learns a discriminative audio-visual feature rep-
resentation, which performs better than previous hand-crafted
features and fusion methods on emotion recognition tasks. Its
success guarantees further research in this direction.
1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE
Transactions on Circuits and Systems for Video Technology
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 12
Our work transforms 1-D audio signals into three channels
of Mel-spectrogram with size 64√ó64√ó3as the suitable
input of CNN. This extracted Mel-spectrogram is regarded
as a RGB image feature representation. Consequently, we
can conveniently resize it into the suitable size as the input
of the existing CNN models pre-trained on image datasets.
In this case, it is possible to Ô¨Åne-tune the pre-trained CNN
models on target emotion datasets for audio feature extraction.
Experimental results demonstrate the validity of our cross-
media Ô¨Åne-tuning scheme.
This work employs 3D-CNNs to extract emotional features
from video segments. Another commonly used solution for
video featureextraction is combiningCNN and LSTM [ 65]. A
L
STM could also be a better solution for audio-visual feature
fusion on video segments than average pooling. Therefore,
we will investigate the performance of CNN+LSTM for facial
expression recognition in our future work.
Besides, this work employs a two-stage learning strategy
to train the audio-visual networks and fusion network, respec-
tively. An end-to-endlearning strategy would be more concise
and has potential to further boost performance. Additionally,
an end-to-end recognition system could be constructed by
referring to recent LSTM based visual recognition works
[65]. Therefore, end-to-endlearning and recognition strategi es
would also be investigated in our future work.
It shouldalso be notedthat, deep modelscommonlycontain
a large number of network parameters, resulting in expensive
computational cost. It is thus meaningful to investigate how
to reduce the network parameters of deep models, e.g., deep
compression [ 66], to achieve real-time emotion recognition
w
ith deep model.
This work employs face detector developed by Viola and
Jones [57] for face detection in videos. It may fail in some
c
hallenging datasets like AFEW [ 67], where face images
s
uffer from substantial viewpoint and illumination changes.
More robust face detectors and models will be studied in our
future work. Moreover,this work aims to employ deep models
to identify discrete emotions, such as anger, disgust, fear,
joy, sadness, and surprise. It is interesting to investigate the
performance of our proposed method on dimensional emotion
recognitiontasks, such as the recognitionof depression degree
ontheAVDLCdataset[ 68].Textisanotherimportantmodality
c
haracterizing human emotion. Therefore, it is also an impor-
tant direction to employ deep models to learn audio, visual,
and textural features for multimodal emotion recognition [ 69].
RE
FERENCES
[1] M. S. Hossain, G. Muhammad, B. Song, M. M. Hassan, A. Alelaiwi,
and A. Alamri, ‚ÄúAudio‚Äìvisual emotion-aware cloud gaming framework,‚Äù
IEEE Trans. Circuits Syst. Video Technol. , vol. 25, no. 12, pp. 2105‚Äì
2118, 2015. 1
[2] R. Gupta, N. Malandrakis, B. Xiao, T. Guha, M. Van Segbroec k,
M. Black, A. Potamianos, and S. Narayanan, ‚ÄúMultimodal prediction of
affective dimensions and depression in human-computer interactions,‚Äù in
Proceedings of the 4th International Workshop on Audio/Visual Emotion
Challenge (AVEC) , Orlando, FL, USA, 2014, pp. 33‚Äì40. 1
[3] Y. Wang and L. Guan, ‚ÄúRecognizing human emotional state fr om
audiovisual signals*,‚Äù IEEE Trans. Multimedia , vol. 10, no. 5, pp. 936‚Äì
946, 2008. 1,3,6,7[4] Z. Zeng, J. Tu, B. M. Pianfetti, and T. S. Huang, ‚ÄúAudio-vis ual affective
expression recognition through multistream fused hmm,‚Äù IEEE Trans.
Multimedia , vol. 10, no. 4, pp. 570‚Äì577, 2008. 1,3
[5] M. Mansoorizadeh and N. M. Charkari, ‚ÄúMultimodal informa tion fusion
application to human emotion recognition from face and speech,‚Äù
Multimed. Tool. Appl. , vol. 49, no. 2, pp. 277‚Äì297, 2010. 1,3,8,11
[6] M. Glodek, S. Tschechne, G. Layher, M. Schels, T. Brosch, S . Scherer,
M. K¬® achele, M. Schmidt, H. Neumann, G. Palm et al., ‚ÄúMultiple
classiÔ¨Åer systems for the classiÔ¨Åcation of audio-visual emotional states,‚Äù
inAffective Computing and Intelligent Interaction (ACII) . Springer,
2011, pp. 359‚Äì368. 1
[7] M. Soleymani, M. Pantic, and T. Pun, ‚ÄúMultimodal emotion r ecognition
in response to videos,‚Äù IEEE Trans. Affect. Comput. , vol. 3, no. 2, pp.
211‚Äì223, 2012. 1
[8] J.-C. Lin, C.-H. Wu, and W.-L. Wei, ‚ÄúError weighted semi-c oupled
hidden markov model for audio-visual emotion recognition,‚Äù IEEE
Trans. Multimedia , vol. 14, no. 1, pp. 142‚Äì156, 2012. 1,3
[9] J. Wagner, E. Andre, F. Lingenfelser, and J. Kim, ‚ÄúExplori ng fusion
methods for multimodal emotion recognition with missing data,‚Äù IEEE
Trans. Affect. Comput. , vol. 2, no. 4, pp. 206‚Äì218, 2011. 1
[10] A. Metallinou, M. W¬® ollmer, A. Katsamanis, F. Eyben, B. S chuller,
and S. Narayanan, ‚ÄúContext-sensitive learning for enhanced audiovisual
emotion classiÔ¨Åcation,‚Äù IEEE Trans. Affect. Comput. , vol. 3, no. 2, pp.
184‚Äì198, 2012. 1
[11] D. Gharavian, M. Bejani, and M. Sheikhan, ‚ÄúAudio-visual emotion
recognition using fcbf feature selection method and particle swarm
optimization for fuzzy artmap neural networks,‚Äù Multimed. Tool. Appl. ,
pp. 1‚Äì22, 2016. 1
[12] S. Zhalehpour, O. Onder, Z. Akhtar, and C. E. Erdem, ‚ÄúBaum -1: A
spontaneous audio-visual face database of affective and mental states,‚Äù
IEEE Trans. Affect. Comput. , 2016.1,3,6,7,8,11
[13] R. R. Sarvestani and R. Boostani, ‚ÄúFf-skpcca: Kernel pro babilistic
canonical correlation analysis,‚Äù Appl. Intell. , pp. 1‚Äì17, 2016. 1,11
[14] M. Bejani, D. Gharavian, and N. M. Charkari, ‚ÄúAudiovisua l emotion
recognition using anova feature selection method and multi-classiÔ¨Åer
neural networks,‚Äù Neural Comput. Appl. , vol. 24, no. 2, pp. 399‚Äì412,
2014.1,8,11
[15] A. Hanjalic, ‚ÄúExtracting moods from pictures and sounds : Towards truly
personalized tv,‚Äù IEEE Signal Process. Mag. , vol. 23, no. 2, pp. 90‚Äì100,
2006.1
[16] Y. Wang, L. Guan, and A. N. Venetsanopoulos, ‚ÄúKernel cros s-modal
factor analysis for information fusion with application to bimodal
emotion recognition,‚Äù IEEE Trans. Multimedia , vol. 14, no. 3, pp. 597‚Äì
607, 2012. 1,3,9
[17] Z. Zeng, M. Pantic, G. Roisman, T. S. Huang e t al., ‚ÄúA survey of affect
recognition methods: Audio, visual, and spontaneous expressions,‚Äù IEEE
Trans. Pattern Anal. Mach. Intell. , vol. 31, no. 1, pp. 39‚Äì58, 2009. 1,3
[18] Z. Xie and L. Guan, ‚ÄúMultimodal information fusion of aud io emotion
recognition based on kernel entropy component analysis,‚Äù Int. J. Semant.
Comput., vol. 7, no. 01, pp. 25‚Äì42, 2013. 1,3
[19] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng,
‚Äú
Multimodal deep learning,‚Äù in International Conference on Machine
Learning (ICML) , 2011, pp. 689‚Äì696. 1,4,10
[20] G. E. Hinton and R. R. Salakhutdinov, ‚ÄúReducing the dimen sionality of
data with neural networks,‚Äù Science, vol. 313, no. 5786, pp. 504‚Äì507,
2006.1
[21] G. E. Hinton, S. Osindero, and Y.-W. Teh, ‚ÄúA fast learning algorithm
for deep belief nets,‚Äù Neural comput. , vol. 18, no. 7, pp. 1527‚Äì1554,
2006.1,11
[22] Y.LeCun,L.Bottou, Y.Bengio, andP.Haffner, ‚ÄúGradient -based learning
applied to document recognition,‚Äù Proc.IEEE ,vol. 86, no. 11, pp. 2278‚Äì
2324, 1998. 1
[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImagenet classiÔ¨Åcation
with deep convolutional neural networks,‚Äù in Advances in neural infor-
mation processing systems (NIPS) , 2012, pp. 1097‚Äì1105. 1,4,5
[24] G. E. Hinton, ‚ÄúTraining products of experts by minimizin g contrastive
divergence,‚Äù Neural comput. , vol. 14, no. 8, pp. 1771‚Äì1800, 2002. 1
[25] R. Salakhutdinov and G. Hinton, ‚ÄúAn efÔ¨Åcient learning pr ocedure for
deep boltzmann machines,‚Äù Neural comput. , vol. 24, no. 8, pp. 1967‚Äì
2006, 2012. 2
[26] Y. Kim, H. Lee, and E. M. Provost, ‚ÄúDeep learning for robus t feature
generation in audiovisual emotion recognition,‚Äù in IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP) ,
Vancouver, BC, 2013, pp. 3687‚Äì3691. 2,3
[27] L.Pang,S.Zhu,and C.-W.Ngo,‚ÄúDeep multimodal learning foraffective
analysis and retrieval,‚Äù IEEE Trans. Multimedia , vol. 17, no. 11, pp.
2008‚Äì2020, 2015. 2,3
1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE
Transactions on Circuits and Systems for Video Technology
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 13
[28] L. Pang and C.-W. Ngo, ‚ÄúMutlimodal learning with deep boltzmann
machine for emotion prediction in user generated videos,‚Äù in Pro-
ceedings of the 5th ACM on International Conference on Multimedia
Retrieval(ICMR) , Shanghai, China, 2015, pp. 619‚Äì622. 2,3
[29] C. Barat and C. Ducottet, ‚ÄúString representations and di stances in deep
convolutional neural networks for image classiÔ¨Åcation,‚Äù Pattern Recogn. ,
vol. 54, pp. 104‚Äì115, 2016. 2
[30] Y. Cao, Y. Chen, and D. Khosla, ‚ÄúSpiking deep convolution al neural
networks for energy-efÔ¨Åcient object recognition,‚Äù Int. J. Comput. Vis. ,
vol. 113, no. 1, pp. 54‚Äì66, 2015. 2
[31] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman, ‚ÄúReading
text in the wild with convolutional neural networks,‚Äù Int. J. Comput.
Vis., vol. 116, no. 1, pp. 1‚Äì20, 2016. 2
[32] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y. Tian, H. Li, S . Yang,
Z. Wang, C.-C. Loy et al., ‚ÄúDeepid-net: Deformable deep convolutional
neural networks for object detection,‚Äù in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) , 2015,
pp. 2403‚Äì2412. 2
[33] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Palur i, ‚ÄúLearning
spatiotemporal features with 3d convolutional networks,‚Äù in 2015 IEEE
International Conference on Computer Vision (ICCV) , Santiago, Chile,
2015, pp. 4489‚Äì4497. 2,4,5
[34] M. El Ayadi, M. S. Kamel, and F. Karray, ‚ÄúSurvey on speech e motion
recognition: Features, classiÔ¨Åcation schemes, and databases,‚Äù Pattern
Recogn., vol. 44, no. 3, pp. 572‚Äì587, 2011. 3
[35] B. Schuller, B. Vlasenko, F. Eyben, G. Rigoll, and A. Wend emuth,
‚ÄúAcoustic emotion recognition: A benchmark comparison of perfor-
mances,‚Äù in IEEE Workshop on Automatic Speech Recognition & Un-
derstanding (ASRU) , Merano, 2009, pp. 552‚Äì557. 3,8
[36] L.Gao, L.Qi, and L.Guan, ‚ÄúInformation fusion based on ke rnel entropy
component analysis in discriminative canonical correlation space with
application to audio emotion recognition,‚Äù in 2016 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP) ,
Shanghai, China, 2016, pp. 2817‚Äì2821. 3,8
[37] N. E.D.Elmadany, Y. He, and L.Guan, ‚ÄúMultiview emotion r ecognition
via multi-set locality preserving canonical correlation analysis,‚Äù in
2016 IEEE International Symposium on Circuits and Systems (ISCAS) ,
Montral, QC, Canada, 2016, pp. 590‚Äì593. 3,8,11
[38] ‚Äî‚Äî, ‚ÄúMultiview learning via deep discriminative canoni cal correlation
analysis,‚Äù in 2016 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP) , Shanghai, China, 2016, pp. 2409‚Äì
2413.3,8
[39] Y. Tian, T. Kanade, and J. F. Cohn, ‚ÄúFacial expression rec ognition,‚Äù in
Handbook of face recognition . Springer, 2011, pp. 487‚Äì519. 3
[40] X. Zhao and S. Zhang, ‚ÄúA review on facial expression recog nition:
Feature extraction and classiÔ¨Åcation,‚Äù IETE Tech. Review , vol. 33, no. 5,
pp. 505‚Äì507, 2016. 3
[41] Z. Zhang, M. Lyons, M. Schuster, and S. Akamatsu, ‚ÄúCompar ison
between geometry-based and gabor-wavelets-based facial expression
recognition using multi-layer perceptron,‚Äù in Third IEEE International
Conference on Automatic Face and Gesture Recognition , Nara, 1998,
pp. 454‚Äì459. 3
[42] G. Zhao and M. Pietikainen, ‚ÄúDynamic texture recognitio n using local
binary patterns with an application to facial expressions,‚Äù IEEE Trans.
Pattern Anal. Mach. Intell. , vol. 29, no. 6, pp. 915‚Äì928, 2007. 3
[43] A. Dhall, A. Asthana, R. Goecke, and T. Gedeon, ‚ÄúEmotion r ecognition
using phog and lpq features,‚Äù in 2011 IEEE International Conference
on Automatic Face & Gesture Recognition and Workshops (FG 2011) ,
Santa Barbara, USA, 2011, pp. 878‚Äì883. 3
[44] W. Ding, M. Xu, D. Huang, W. Lin, M. Dong, X. Yu, and H. Li,
‚Äú
Audio and face video emotion recognition in the wild using deep
neural networks and small datasets,‚Äù in Proceedings of the 18th ACM
International Conference on Multimodal Interaction (ICMI) , Tokyo,
Japan, 2016, pp. 506‚Äì513. 3,4
[45] S. Hochreiter and J. Schmidhuber, ‚ÄúLong short-term memo ry,‚ÄùNeural
comput., vol. 9, no. 8, pp. 1735‚Äì1780, 1997. 3
[46] M. Song, M. You, N. Li, and C. Chen, ‚ÄúA robust multimodal ap proach
for emotion recognition,‚Äù Neurocomput. , vol. 71, no. 10, pp. 1913‚Äì1920,
2008.3
[47] B. Schuller, R. M¬® ueller, B. H¬® oernler, A. H¬® oethker, H. Konosu, and
G. Rigoll, ‚ÄúAudiovisual recognition of spontaneous interest within
conversations,‚Äù in 9th international conference on Multimodal interfaces
(ICMI), Aichi, Japan, 2007, pp. 30‚Äì37. 3
[48] C. Busso, Z. Deng, S. Yildirim, M. Bulut, C. M. Lee, A. Kaze mzadeh,
S. Lee, U. Neumann, and S. Narayanan, ‚ÄúAnalysis of emotion recog-
nition using facial expressions, speech and multimodal information,‚Äù in6th international conference on Multimodal interfaces (ICMI) , PA,USA,
2004, pp. 205‚Äì211. 3
[49] C. Busso, S. Lee, and S. S. Narayanan, ‚ÄúUsing neutral spee ch models
for emotional speech analysis,‚Äù in Interspeech , Antwerp, Belgium, 2007,
pp. 2225‚Äì2228. 4
[50] L. He, M. Lech, N. Maddage, and N. Allen, ‚ÄúStress and emoti on
recognition using log-gabor Ô¨Ålter analysis of speech spectrograms,‚Äù in
3rd International Conference on Affective Computing and Intelligent
Interaction and Workshops (ACII) , Amsterdam, The Netherlands, 2009,
pp. 1‚Äì6. 4
[51] O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, L. Deng, G. Penn ,
and D. Yu, ‚ÄúConvolutional neural networks for speech recognition,‚Äù
IEEE/ACM Trans. Audio, Speech, Lang. Process. , vol. 22, no. 10, pp.
1533‚Äì1545, 2014. 4,8
[52] E. M. Provost, ‚ÄúIdentifying salient sub-utterance emot ion dynamics us-
ing Ô¨Çexible units and estimates of affective Ô¨Çow,‚Äù in IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP) ,
Vancouver, BC, 2013, pp. 3682‚Äì3686. 4,5,9
[53] M. W¬® ollmer, M. Kaiser, F. Eyben, B. Schuller, and G. Rigo ll, ‚ÄúLstm-
modeling of continuous emotions in an audiovisual affect recognition
framework,‚Äù Image Vis. Comput. , vol. 31, no. 2, pp. 153‚Äì163, 2013. 4,
5,9
[54] X. Huang, A. Acero, H.-W. Hon, and R. Foreword By-Reddy, S poken
language processing: A guide to theory, algorithm, and system develop-
ment. Prentice Hall PTR, 2001. 5
[55] C. Shan, S. Gong, and P. W. McOwan, ‚ÄúFacial expression rec ognition
based on local binary patterns: A comprehensive study,‚Äù Image Vis.
Comput., vol. 27, no. 6, pp. 803‚Äì816, 2009. 5
[56] X. Zhao and S. Zhang, ‚ÄúFacial expression recognition usi ng local binary
patterns and discriminant kernel locally linear embedding,‚Äù EURASIP J.
Adv. signal process. , vol. 2012, no. 1, pp. 1‚Äì9, 2012. 5
[57] P. Viola and M. J. Jones, ‚ÄúRobust real-time face detectio n,‚ÄùInt. J.
Comput. Vis. , vol. 57, no. 2, pp. 137‚Äì154, 2004. 5,12
[58] L. Zheng, Z. Bie, Y. Sun, J. Wang, C. Su, S. Wang, and Q. Tian , ‚ÄúMars:
Avideo benchmark for large-scale person re-identiÔ¨Åcation,‚Äù in European
Conference on Computer Vision (ECCV) , Amsterdam, The Netherlands,
2016, pp. 868‚Äì884. 5
[59] G. E. Hinton, S. Osindero, and Y.-W. Teh, ‚ÄúA fast learning algorithm
for deep belief nets,‚Äù Neural comput. , vol. 18, no. 7, pp. 1527‚Äì1554,
2006.6
[60] O. Martin, I. Kotsia, B. Macq, and I. Pitas, ‚ÄúThe enterfac e‚Äô05 audio-
visual emotion database,‚Äù in 22nd International Conference on Data
Engineering Workshops , Atlanta, GA, USA, 2006, pp. 8‚Äì8. 6,7
[61] B. Schuller, S. Steidl, A. Batliner, F. Burkhardt, L. Dev illers, C. A.
M¬® uller, and S. S. Narayanan, ‚ÄúThe interspeech 2010 paralinguistic
challenge.‚Äù in INTERSPEECH ,Makuhari, Chiba, Japan, 2010,pp.2794‚Äì
2797.7
[62] S. Zhang, S. Zhang, T. Huang, and W. Gao, ‚ÄúMultimodal deep con-
volutional neural network for audio-visual emotion recognition,‚Äù in
Proceedings of the 6th ACM on International Conference on Multimedia
Retrieval(ICMR) , New York, USA, 2016, pp. 281‚Äì284. 8,9,10,11
[63] J.Kittler, M.Hatef, R.P.Duin, and J.Matas, ‚ÄúOn combini ng classiÔ¨Åers,‚Äù
IEEE Trans. Pattern Anal. Mach. Intell. , vol. 20, no. 3, pp. 226‚Äì239,
1998.9
[64] Z.Sun,Q.Song,X.Zhu,H.Sun,B.Xu,and Y.Zhou,‚ÄúAnovele nsemble
method for classifying imbalanced data,‚Äù Pattern Recogn. , vol. 48, no. 5,
pp. 1623‚Äì1637, 2015. 9
[65] Y. Yan, B. Ni, Z. Song, C. Ma, Y. Yan, and X. Yang, ‚ÄúPerson re -
identiÔ¨Åcation via recurrent feature aggregation,‚Äù in European Conference
on Computer Vision (ECCV) , Amsterdam, The Netherlands, 2016, pp.
701‚Äì716. 12
[66] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and Y. Ch en,
‚ÄúCompressing neural networks with the hashing trick,‚Äù in Proceedings
of The 32nd International Conference on Machine Learning (ICML) ,
Lille, France, 2015, pp. 2285‚Äì2294. 12
[67] A.Dhall,O.RamanaMurthy,R.Goecke, J.Joshi,andT.Ged eon,‚ÄúVideo
and image based emotion recognition challenges in the wild: Emotiw
2015,‚Äù in Proceedings of the 2015 ACM on International Conference on
Multimodal Interaction (ICMI) , Seattle, WA, USA, 2015, pp. 423‚Äì426.
12
[68] M. Valstar, B. Schuller, K. Smith, F. Eyben, B. Jiang, S. B ilakhia,
S. Schnieder, R. Cowie, and M. Pantic, ‚ÄúThe continuous audio/visual
emotion and depression recognition challenge,‚Äù in The 21st ACM Inter-
national Conference on Multimedia(MM) , Barcelona, Spain, 2013. 12
[69] S. Poria, H. Peng, A. Hussain, N. Howard, and E. Cambria, ‚Äú Ensemble
application of convolutional neural networks and multiple kernel learn-
ing for multimodal sentiment analysis,‚Äù Neurocomput. , 2017.12
1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE
Transactions on Circuits and Systems for Video Technology
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 14
Shiqing Zhang r eceived the Ph.D. degree at school
of Communication and Information Engineering,
University of Electronic Science and Technology of
China, in 2012. Currently, he is a postdoctor with
the School of Electronic Engineering and Computer
Science, Peking University, Beijing, China, and also
works as an associate professor of department of
physics and electronics engineering, Taizhou Univer-
sity, China. His research interests include audio and
image processing, affective computing and pattern
recognition.
Shiliang Zhang i s currently a tenure-track Assistant
Professor in School of Electronic Engineering and
Computer Science, Peking University. He received
the Ph.D. degree in computer science from Institute
of Computing Technology, Chinese Academy of
Sciences in 2012. He was a Postdoctoral Scientist
in NEC Labs America and a Postdoctoral Research
Fellow in University of Texas at San Antonio.
Dr. Zhang‚Äôs research interests include large-scale
image retrieval and computer vision for autonomous
driving. He was awarded the National 1000 Youth
Talents Plan of China, Outstanding Doctoral Dissertation Awards from both
Chinese Academy of Sciences and Chinese Computer Federation (CCF),
President Scholarship by Chinese Academy of Sciences, NEC Laboratories
America Spot Recognition Award, and the Microsoft Research Fellowship.
He has published over 30 papers in journals and conferences including IEEE
Trans. on Pattern Analysis and Machine Intelligence, IEEE Trans. on Image
Processing, IEEE Trans. on Multimedia, ACM Multimedia, and ICCV. He is
the recipient of Top 10% Paper Award in IEEE MMSP 2011. His research
is supported by the National 1000 Youth Talents Plan and Natural Science
Foundation of China (NSFC).
Tiejun Huang T iejun Huang (M‚Äô01 ‚àíSM‚Äô12) is cur-
rently a Professor with the School of Electronic En-
gineering and Computer Science, Peking University,
Beijing, China, where he is also the Director of the
Institute for Digital Media Technology. He received
the Ph.D. degree in pattern recognition and intelli-
gent system from Huazhong (Central China) Uni-
versity of Science and Technology, Wuhan, China,
in 1998, and the masters and bachelors degree in
computer science from the Wuhan University of
Technology, Wuhan, in 1995 and 1992, respectively.
His research area includes video coding, image understanding, digital right
management, and digital library. He has authored or co-authored over 100
peer-reviewed papers and three books. He is a member of the Board of Di-
rector for Digital Media Project, the Advisory Board of the IEEE Computing
Society, and the Board of the Chinese Institute of Electronics.
Wen Gao W en Gao (M‚Äô92 ‚àíSM‚Äô05‚àíF‚Äô09) received
the Ph.D. degree in electronics engineering from the
University of Tokyo, Tokyo, Japan, in 1991.
He is currently a Professor with the School of
Electronic Engineering and Computer Science with
Peking University, Beijing, China. Before joining
Peking University, he was a Professor of Computer
Science with the Harbin Institute of Technology,
Harbin, China, from 1991 to 1995, and a Profes-
sor with the Institute of Computing Technology,
Chinese Academy of Sciences, Beijing, China. He
has authored Ô¨Åve books and more than 600 technical articles in refereed
journals and conference proceedings in image processing, video coding
and communication, pattern recognition, multimedia information retrieval,
multimodal interface, and bioinformatics.
Dr. Gao serves the editorial board for several journals, such as IEEE
TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECH-
NOLOGY, IEEE TRANSACTIONS ON MULTIMEDIA, IEEE TRANS-
ACTIONS ON AUTONOMOUS MENTAL DEVELOPMENT, EURASIP
Journal of Image Communications, and Journal of Visual Communication
and Image Representation. He chaired a number of prestigious international
conferences on multimedia and video signal processing, such as the IEEE
ICME and ACM Multimedia, and also served on the advisory and technical
committees of numerous professional organizations.
Qi TianQ i Tian (S‚Äô95 ‚àíM‚Äô96‚àíSM‚Äô03) received the
B.E. degree in electronic engineering from Tsinghua
University, Beijing, China, in 1992, the M.S. degree
in electrical and computer engineering from Drexel
University, Philadelphia, PA, USA, in 1996, and the
Ph.D. degree in electrical and computer engineering
from the University of Illinois at Urbana-Champaign
(UIUC), Champaign, IL, USA, in 2002.
He is currently a Full Professor with the Depart-
ment of Computer Science, the University of Texas
at San Antonio (UTSA), San Antonio, TX, USA.
He was a tenure-track Assistant Professor from 2002 to 2008 and a tenured
Associate Professor from 2008 to 2012 at the same institution. During 2008
and 2009, he took one-year faculty leave as Lead Researcher with the Media
Computing Group, Microsoft Research Asia, Beijing, China. He has authored
or coauthored over 310 refereed journal and conference papers. His research
interests include multimedia information retrieval, computer vision, pattern
recognition and bioinformatics.
Dr. Tian is a Member of the ACM (2004). He is the Associate Editor of the
IEEE TRANSACTIONS ON MULTIMEDIA, the IEEE TRANSACTIONS
ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, and the
Multimedia System Journal, and is an Editorial Board Member for the Journal
of Multimedia and the Journal of Machine Vision and Applications. He is a
Guest Editor for a number of journals, including the IEEE TRANSACTIONS
ON MULTIMEDIA and the Journal of Computer Vision and Image Under-
standing. Hewas the recipient of the 2014 Research Achievement Award from
the College of Science, UTSA, and the 2010 ACM Service Award. He was
coauthor of the recipient of the Best Paper Award in ACM ICMR 2015, Best
Paper Award in PCM 2013, Best Paper Award in MMM 2013, Best Paper
Award in ACM ICIMCS 2012, Top 10% Paper Award in MMSP 2011, Best
Student Paper in ICASSP 2006, a Best Student Paper Candidate in ICME
2015, and a Best Paper Candidate in PCM 2007.
"
http://ieeexplore.ieee.org/document/8658192/,"Received January 16, 2019, accepted February 18, 2019, date of publication March 5, 2019, date of current version March 26, 2019.
Digital Object Identifier 10.1 109/ACCESS.2019.2901521
Learning Affective Video Features for
Facial Expression Recognition
via Hybrid Deep Learning
SHIQING ZHANG
1, XIANZHANG PAN1, YUELI CUI1, XIAOMING ZHAO1, AND LIMEI LIU2
1Institute of Intelligent Information Processing, Taizhou University, Taizhou 318000, China
2Institute of Big Data and Internet Innovation, Hunan University of Commerce, Changsha 410205, China
Corresponding author: Shiqing Zhang (tzczsq@163.com)
This work was supported in part by the Zhejiang Provincial National Science Foundation of China under Grant LY16F020011, in part by
the Major Project for National Natural Science Foundation of China under Grant 71790615, in part by the Taizhou Science and Technology
Project under Grant 1802gy06, and in part by the Training Project of Taizhou University under Grant 2017PY026 and Grant 2018JQ003.
ABSTRACT One key challenging issues of facial expression recognition (FER) in video sequences is
to extract discriminative spatiotemporal video features from facial expression images in video sequences.
In this paper, we propose a new method of FER in video sequences via a hybrid deep learning model.
The proposed method rst employs two individual deep convolutional neural networks (CNNs), including
a spatial CNN processing static facial images and a temporal CN network processing optical ow images,
to separately learn high-level spatial and temporal features on the divided video segments. These two CNNs
are ne-tuned on target video facial expression datasets from a pre-trained CNN model. Then, the obtained
segment-level spatial and temporal features are integrated into a deep fusion network built with a deep belief
network (DBN) model. This deep fusion network is used to jointly learn discriminative spatiotemporal
features. Finally, an average pooling is performed on the learned DBN segment-level features in a video
sequence, to produce a xed-length global video feature representation. Based on the global video feature
representations, a linear support vector machine (SVM) is employed for facial expression classication tasks.
The extensive experiments on three public video-based facial expression datasets, i.e., BAUM-1s, RML,
and MMI, show the effectiveness of our proposed method, outperforming the state-of-the-arts.
INDEX TERMS Facial expression recognition, spatio-temporal features, hybrid deep learning, deep
convolutional neural networks, deep belief network.
I. INTRODUCTION
Facial expression is one of the most natural nonverbal ways
for expressing human emotions and intentions. In recent
years, automatic facial expression recognition (FER), which
aims to analyze and understand human facial behavior, has
become an increasingly active research topic in the domains
of computer vision, articial intelligence, pattern recognition,
etc. This is because FER has many potential applications
such as human emotion perception, social robotics, human-
computer interaction and healthcare [1][5].
FER methods can be divided into two categories: video
sequence-based methods (dynamic) and image-based meth-
ods (static). Most previous FER studies focus on identifying
The associate editor coordinating the review of this manuscript and
approving it for publication was Tariq Ahamed Ahanger.facial expressions from static facial images [1][4]. Although
these image-based methods can effectively derive spatial
information from still images, they cannot capture the tempo-
ral variability in consecutive frames in video sequences. As a
dynamic event, classifying facial expression from consecu-
tive frames in a video is more natural, since video sequences
provides much more information for FER than static facial
images. One key issue for video sequence-based FER meth-
ods is how to effectively encode input video sequences into
an appropriate feature representation. Currently, the main-
stream methods employ hand-designed feature representa-
tions, such as Gabor motion energy [6], Local Binary Patterns
from Three Orthogonal Planes (LBP-TOP) [7] or Local
Phase Quantization from TOP (LPQ-TOP) [8]. However,
these hand-designed feature representations are low-level to
discriminate dynamic facial expressions. Recently, the deep
VOLUME 7, 20192169-3536 
2019 IEEE. Translations and content mining are permitted for academic research only.
Personal use is also permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.32297
S. Zhang et al. : Learning Affective Video Features for FER via Hybrid Deep Learning
neural network driven feature learning representations from
data may achieve better performance without requiring
domain expertise [9][15].
Inspired by the strong feature learning ability of deep neu-
ral networks, this paper proposes a new deep neural network-
based FER method in video sequences by using a hybrid deep
learning model. Our hybrid deep learning model contains
three deep models. The rst two deep models are deep Con-
volutional Neural Networks (CNNs) [16], including a spatial
CNN network processing static facial images and a tem-
poral CNN network processing optical ow images. These
two CNNs are separately used to learn high-level spatial
features and temporal features on the divided video segments.
The third deep model is a deep fusion network built with a
Deep Belief Network (DBN) [17] model, which is trained to
jointly learn a discriminative spatio-temporal segment-level
feature representation. When nishing the joint training of
a DBN, an average-pooling is applied on all the divided video
segments to produce a xed-length global video feature rep-
resentation. Then, a linear Support Vector Machine (SVM)
is adopted to perform facial expression classication tasks in
video sequences.
It is noted that two-stream CNNs have been successfully
used for video action recognition [18]. Nevertheless, in [18],
a score-level scheme, which belongs to a shallow fusion
method, is used to merge different features produced by two-
stream CNNs. This shallow fusion method is not able to
effectively model the complicated non-linear joint distribu-
tion of multiple input modalities [19]. To tackle this issue,
it is desired to design deep fusion methods which lever-
age a deep fusion model to implement multiple meaning-
ful feature fusion operations. Since a DBN model consists
of multiple RBMs, each of which can be used to jointly
learn feature representations of multiple input modalities,
it may be feasible to use a DBN model as a deep fusion
method to integrate different features produced by two-
stream CNNs. This motivates us to develop a hybrid deep
leaning method to learn video features for facial expression
recognition in video sequences. Experiment results on three
public video-based facial expression databases, including
the BAUM-1s database [20], the RML database [21], and
the MMI database [22], are presented to demonstrate the
effectiveness of the proposed method on FER tasks in video
sequences.
The distinct features of this paper can be summarized
in two-fold: (1) We propose a hybrid deep learning model,
comprising a spatial CNN network, a temporal CNN net-
work and a deep fusion network built with a DBN model,
to apply for FER in video sequences. To the best of our knowl-
edge, it is the rst time to employ a hybrid deep learning
model to learn video features for FER in video sequences.
(2) To deeply fuse the spatial CNN features and temporal
CNN features, we employ a deep DBN model as a deep
fusion network to learn a joint discriminative spatio-temporal
segment-level feature representation for FER. Extensive
experiments are conducted on three public video-based facialexpression datasets, and experiment results demonstrate that
our method outperforms the-state-of-the-arts.
The structure of this paper is organized as follows.
Section 2 reviews the related work in brief. Section 3
describes our proposed method in detail. Experiment results
and analysis are given in Section 4. Section 5 presents the
conclusions and future work.
II. RELATED WORK
In this section, we review the recent works related to feature
extraction in FER in video sequences, which uses hand-
designed features and deep learning-based features.
A. HAND-DESIGNED FEATURE-BASED METHOD
For facial feature representation in static images, a variety
of local image descriptors, including Local Binary Pattern
(LBP) [23], Histogram of Oriented Gradient (HOG) [24],
and Scale Invariant Feature Transform (SIFT) [25] have been
widely used for FER. For dynamic expression recognition,
these typical local features have been extended and applied
to video sequences, such as LBP-TOP [7], LPQ-TOP [8],
3D-HOG [26], 3D-SIFT [27], respectively. Hayat et al. [28]
compare the performance of various dynamic descriptors
including HOG, 3D-HOG, 3D-SIFT and LBP-TOP by using
bag of features framework for video-based FER, and nd that
LBP-TOP performs best among these dynamic descriptors.
Additionally, spatio-temporal Gabor motion energy lters [6]
is presented for low-level integration of spatio-temporal
information on FER tasks.
Recently, some efforts have been conducted to develop
more powerful spatio-temporal feature extraction methods
for FER. For instance, Liu et al. [29] present an expressionlet-
based spatio-temporal manifold descriptor which shows
the superiority over traditional methods on FER tasks.
Fan and Tjahjadi [30] provide a spatio-temporal feature based
on local Zernike moment and motion history image for
dynamic FER. Yan [31] proposes a collaborative discrim-
inative multi-metric learning for FER in video sequences.
In particular, for each video sequence they rstly calculate
multiple feature descriptors such as 3D-HOG, and geometric
warp features. Then, these extracted multiple features are
employed to learn multiple distance metrics collaboratively
to obtain complementary and discriminative information for
dynamic FER.
B. DEEP LEARNING-BASED METHOD
In recent years, deep CNNs [16], [32][34], composed of
multiple convolution layers and pooling layers, have domi-
nated various computer vision tasks such as image classica-
tion, object detection and face recognition. These deep CNNs
extends the traditional CNN model [35] into a deep multi-
layered architecture which consists of ve convolution layers
followed by three max-pooling layers.
One of the major drawbacks of conventional CNNs is that
they are able to extract spatial relationships of input images,
but cannot model the temporal relationships of them in
32298 VOLUME 7, 2019
S. Zhang et al. : Learning Affective Video Features for FER via Hybrid Deep Learning
FIGURE 1. The framework of our proposed hybrid deep learning network for facial expression recognition in video sequences.
a video sequence. To solve this problem, the recently-
developed 3D-CNNs [36] may present a possible solution.
3D-CNNs can extract spatio-temporal features in a video
sequence by means of sliding over the temporal dimen-
sion of input data as well as the spatial dimension simul-
taneously. In recent years, 3D-CNNs have been used to
learn spatio-temporal expression representations from suc-
cessive frames in video sequences [12], [15]. In addi-
tion, a variant of 3D-CNNs is 3DCNN-DAP [14] used for
dynamic FER. In 3DCNN-DAP, a constraint of Deformable
Action Parts (DAP) is incorporated into the basic 3D-CNN
framework. Similar to 3DCNN-DAP, Jung et al. [10] propose
a small temporal CNN to extract temporal geometric features
from facial landmark points. Although these 3D-CNNs based
methods have achieved good performance on FER tasks in
video sequences, but they still has a drawback. That is,
these methods cannot take the deep fusion of spatio-temporal
features into account simultaneously in the procedure of
extracting them.
To tackle this problem, two-stream CNNs used for video
action recognition [18], may present a cue. However, the used
shallow fusion method in [18] based on a score-level scheme,
cannot able to effectively model the complicated non-linear
joint distribution of multiple input modalities. To make full
use of the advantages of two-stream CNNs, we design a deep
fusion network built with a deep DBN model to jointly learn
the outputs of two-stream CNNs. This is our proposed hybrid
deep learning model. Then, we apply this hybrid deep learn-
ing model for FER in video sequences. Experiment results
on three video-based facial expression databases demonstrate
the advantages of our proposed method.
III. OUR METHOD
Figure 1 shows the framework of our proposed hybrid deep
learning model. As depicted in Fig.1, our method is composedof two individual channels of input streams, i.e., a spatial
CNN network processing static frame-level cropped facial
images and a temporal CNN network processing optical
ow images produced between consecutive frames. To inte-
grate the learned spatio-temporal features represented by
the outputs of fully connected layers of these two CNNs, a
fusion network built with a deep DBN model is designed.
In detail, our method contains four key steps: (1) genera-
tion of CNN inputs (2) spatio-temporal feature learning with
CNNs (3) spatio-temporal fusion with DBNs (4) video-based
expression classication. In the followings, we present the
details about abovementioned four steps of our method.
A. GENERATION OF CNN INPUTS
Since CNNs require a xed size of input data, we divide each
video sample with different durations into a certain number
of xed-length segments as inputs of CNNs. This not only
produces appropriate inputs of CNNs, but also augments the
amount of training data to some extent.
Following in [18], the divided segment length Lis set
to be LD16 for its good performance when using the
temporal CNN network. As a result, in the latter experiments,
we divide each video sample into a xed-length segment
with LD16. To this end, when L>16 we eliminate the
rst and last ( L 16)=2 frames. Oppositely, when L<16,
we simply duplicate the rst and last (16  L)=2 frames. In this
way, we make sure that each divided segment has a length
ofLD16.
1) INPUTS OF TEMPORAL CNNs
To produce suitable inputs of temporal CNNs, we extract
optical ow images between consecutive frames in a video
sequence. Optical ow images represent the displacement
changes of corresponding positions between consecutive
frames. Following in [37], we rstly transform the values of
VOLUME 7, 2019 32299
S. Zhang et al. : Learning Affective Video Features for FER via Hybrid Deep Learning
the motion eld dx,dyinto the interval [0, 255] by
QdxjyDadxjyCb; (1)
where aD16,bD128.
Then, the transformed ow maps are conserved as an opti-
cal ow image containing three channels, which corresponds
to motionQdx,Qdyand the optical ow magnitude. In this
way, we nally produce an optical ow image with size of
2272273. It is noted that a video segment LD16 can
generate 15 optical ow images as inputs of temporal CNNs,
since two consecutive frames yield one optical ow image.
2) INPUTS OF SPATIAL CNNs
For inputs of spatial CNNs, we employ a cropped facial
image of 1501103 for each frame in a video segment,
as in [23]. In detail, a robust real-time face detector [38] is
rstly leveraged to perform face detection to crop a facial
image from each frame in a video segment. Then, in terms of
the normalized distance between two eyes, a cropped image
of 1501103 containing facial key parts, such as head,
nose, mouth, etc., is obtained from a facial image. Finally,
we resize the cropped facial image into 227 2273 as
inputs of spatial CNNs. Note that we discard the rst frame in
a video segment LD16, and employ the remaining 15 frames
as inputs of spatial CNNs. In this case, we can make sure that
the input frames of spatial CNNs in a video segment equals
to that of temporal CNNs.
B. SPATIO-TEMPORAL FEATURE LEARNING WITH CNNs
As described in Fig.1, the used spatial and temporal CNNs
have the same structure as the original VGG16 [16], which
consists of ve convolution layers (Conv1a-Conv1b, Conv2a-
Conv2b, Conv3a-Conv3b-Conv3c-, , Conv5a, Conv5b-
Conv5c), ve max-pooling layers (Pool1, Pool2, , Pool5),
and three fully connected (FC) layers (fc6, fc7, fc8). Note
that fc6 and fc7 have 4096 units, while fc8 represents a class
label vector which equals to data categories. Note that fc8 in
VGG16 corresponds to 1000 image categories.
To realize the task of spatio-temporal feature learning
with CNNs, we ne-tune the pre-trained VGG16 [16] on tar-
get video-based facial expression data. In particular, we rstly
copy the existing VGG16 parameters pre-trained on large-
scale ImageNet data to initialize the temporal CNN network
and the spatial CNN network, respectively. Then, we replace
the fc8 layer in VGG16 with a new class label vector cor-
responding to six facial expression categories used in our
experiments. Ultimately, we individually retrain these two
CNN streams by using the standard back propagation strat-
egy. Specially, we use the back propagation technique to solve
the following minimizing problem so as to update the CNN
network parameters:
min
W;NX
iD1H(softmax ( W7(aiI#));yi); (2)where Wdenotes the weights of the softmax layer for the
network parameters #belonging to spatial CNNs or tem-
poral CNNs. 7(viI#) is the 4096-D output of fc7 for input
dataai. And yiis the class label vector of the i-th segment, H
is the softmax log-loss function dened as
H(#;y)D CX
jD1yjlog(yj); (3)
where Cis the total number of facial expression categories.
Once both spatial CNNs and temporal CNNs are trained,
the 4096-D outputs of their fc7 layers represent the learned
high-level feature representations in video segments.
C. SPATIO-TEMPORAL FUSION WITH DBNs
When nishing the training of spatial CNNs and tempo-
ral CNNs, the 4096-D outputs of their fc7 layers were directly
concatenated into a total 8192-D vector as inputs of the fusion
network built with a deep DBN model [17], as illustrated
in Fig.1. This deep DBN model is used to capture highly
non-linear relationships across spatial and temporal modali-
ties, and produce a joint discriminative feature representation
for FER.
A DBN model is a multi-layered neural network struc-
ture formed by stacking a series of Restricted Boltzmann
Machines (RBMs) [39], each of which is a bipartite graph.
In Fig.1, two RBMs constituted by one visible layer and two
hidden layers, are presented as an illustration of a DBN's
structure. Here, the output layer denotes the softmax layer
for classication. One key characteristic of a DBN is that it
can employ multiple RBMs to learn a multi-layer generative
model of input data. As a result, DBNs can effectively dis-
cover the distribution properties of input data, and learn the
hierarchical feature representations of input data.
As done in [40], we use a two-step strategy to train the
DBN fusion network, as described below.
(1) An unsupervised pre-training is conducted in the
bottom-up way by means of a greedy layer-wise training
algorithm. According to the logarithm of the probability of
derivative, the weights of each RBM model is updated by
1wD""(<vihj>data <vihj>model); (4)
where""denotes the learning rate, <>represents the data
expectation. viandhjare the status of visual nodes and hidden
nodes, respectively.
(2) A supervised ne-tuning is performed to update the
network parameters with back propagation. Specially, super-
vised ne-tuning is realized by using the following loss func-
tion between input data and the reconstructed data.
L(x;x0)Dx x02
2; (5)
where xandx0separately denotes input data and the recon-
struction data,kk2
2is the L2-norm reconstruction error.
32300 VOLUME 7, 2019
S. Zhang et al. : Learning Affective Video Features for FER via Hybrid Deep Learning
D. VIDEO-BASED EXPRESSION CLASSIFICATION
After implementing the training of the DBN fusion network,
the output of its last hidden layer represents the jointly
learned discriminative spatio-temporal feature representa-
tions in video segments. Based on this learned segment-
level features of DBNs, we then apply an average-pooling
approach on all divided segments in a video sample to
produce a xed-length global video feature representation
for FER. Finally, a linear SVM classier is adopted to
perform the nal FER tasks in video sequences.
IV. EXPERIMENTS
To verify the performance of our proposed method on FER
tasks in video sequences, FER experiments are performed on
three public video-based facial expression datasets, i.e., the
BAUM-1s database [20], the RML database [21] and the
MMI database [22].
FIGURE 2. Some examples of cropped facial expression images from the
BAUM-1s dataset.
A. DATASETS
1) BAUM-1s
The original BAUM-1 is a newly-developed spontaneous
audio-visual face database of affective and mental states [20].
The BAUM-1 database contains not only the six basic facial
expressions (joy, anger, sadness, disgust, fear, surprise) as
well as boredom and contempt, but also four mental states
(unsure, thinking, concentrating, bothered). It comprises
of 1222 video samples collected from 31 Turkish persons.
Each video frame is 720 5763. Following in [20], we aim
to identify the six basic facial expressions, which forms a
small subset called the BAUM-1s dataset with 521 video
samples in total. Fig.2 gives some examples of cropped facial
expression images from the BAUM-1s dataset.
2) RML
The RML database [21] consists of 720 video samples col-
lected from 8 persons. Each video frame is 720 4803. This
database has the six basic facial expressions (angry, disgust,
fear, joy, sadness and surprise). Fig.3 shows some samples of
cropped facial expression images from the RML database.
3) MMI
The MMI database [22] consists of 2894 video samples,
out of which 213 sequences have been labeled with six
basic expressions from 30 subjects aging from 19 to 62.
FIGURE 3. Some examples of cropped facial expression images from the
RML dataset.
FIGURE 4. Some examples of cropped facial expression images from the
MML dataset.
Fig.4 provides some samples of cropped facial expression
images from the MMI database.
B. EXPERIMENT SETTINGS
When training deep neural networks, we adopt a mini-
batch size of 30. The maximum number of epochs is 300
for CNNs, and 100 for DBNs, respectively. The learning rate
is set to 0.001. To accelerate the training of deep models,
one NVIDIA GTX TITAN X GPU with 12GB memory is
employed. For all experiments we adopt subject-independent
cross-validation strategy widely used in real applications.
In particular, on the BAUM-1s and MMI database with more
than 10 subjects, Leave-One-Subject-Group-Out (LOSGO)
with ve subject groups is employed, whereas on the RML
database with less than 10 subjects, Leave-One-Subject-
Out (LOSO) is used for experiments. Finally, we report the
average recognition accuracy in all test-runs to testify the
performance of all compared methods.
It is noted that we train deep models on the divided
video segments so that the number of training data can be
augmented. In this work, on the BAUM-1s database about
7000 segments are produced from 521 video samples. Sim-
ilarily, on the RML database about 12, 000 segments are
produced from 720 video samples, whereas on the MMI
database, about 4000 segments are given from 213 video
samples.
C. RESULTS AND ANALYSIS
We rstly evaluate the effects of deep structures of DBNs
in the fusion network, since the deep structures of DBNs
may greatly affects the performance of fusing spatio-temporal
features. To verify the different structures of DBNs, we pro-
vide the performance of three different DBNs, includ-
ing DBN-1 (8192-4096-6), DBN-2 (8192-4096-2048-6),
VOLUME 7, 2019 32301
S. Zhang et al. : Learning Affective Video Features for FER via Hybrid Deep Learning
TABLE 1. Accuracy (%) of different structures of DBNS.
TABLE 2. Accuracy (%) of different learned deep features.
and DBN-3 (8192-4096-2048-1024-6). Table 1 presents the
recognition accuracy of different structures of DBNs in the
fusion network. From Table 1, we can observe that DBN-3
performs best among three different structures. In particular,
DBN-3 presents an accuracy of 55.85% on the BAUM-1s
dataset, 73.73% on the RML dataset, and 71.43% on the
MMI dataset, respectively. This demonstrates that the deeper
DBN exhibits stronger feature fusion ability based on the
used multiple RBMs. In the latter experiments, in the fusion
network we thus adopt DBN-3 as the default structure of the
used DBN for its best performance.
To verify the advantages of fusing spatio-temporal features
with DBNs, Table 2 shows the performance of four methods:
the single spatial CNN features, the single temporal CNN fea-
tures, the score-level fusion based on spatio-temporal CNN
features, and the DBN fusion based on spatio-temporal CNN
features. As shown in Table 2, we can see that the spatio-
temporal CNNCDBN features, which fuse spatio-temporal
CNN features with DBNs, outperform the other two features.
This indicates the effectiveness of fusing spatio-temporal
features by using a deep DBN. This is because DBNs are
able to effectively discover the distribution properties of input
spatio-temporal data, and learn the hierarchical feature repre-
sentations of input spatio-temporal data.
To further present the recognition performance for each
facial expression, Fig.5-7 separately show the confusion
matrix of recognition results achieved by the DBN fusion
network on these three datasets. It can be seen from Fig.5 that
on the BAUM-1s dataset only ``joy'' and ``sadness'' are
classied well with an accuracy of 88.44% and 72.39%,
respectively, whereas other four facial expressions are iden-
tied badly with an accuracy of less than 35%. The results
in Fig.6 demonstrate that on the RML dataset ``disgust'',
``sadness'' and ``surprise'' are recognized well with an
accuracy of more than 84%, whereas the remaining three
facial expressions are distinguished with an accuracy of less
than 80%. In Fig.7, we can see that ``sadness'' and ``surprise''
are distinguished with an accuracy of 100%, whereas the
others are identied with an accuracy of less than 75%.
FIGURE 5. Confusion matrix of recognition results with DBNs on the
BAUM-1s dataset.
FIGURE 6. Confusion matrix of recognition results with DBNs on the RML
dataset.
FIGURE 7. Confusion matrix of recognition results with DBNs on the MMI
dataset.
Now we directly conduct a comparison with previous
works on these three datasets. It is noted that these com-
paring works also employs subject-independent test-runs,
32302 VOLUME 7, 2019
S. Zhang et al. : Learning Affective Video Features for FER via Hybrid Deep Learning
TABLE 3. Performance (%) comparisons of the-state-of-the-arts on the
used three datasets.
similar to ours. Table 3 provides the comparisons of the state-
of-the-arts. From Table 3, it can be seen that our proposed
method signicantly outperforms the state-of-the-arts on
these three datasets. This exhibits the superiority of our pro-
posed method over other methods, including other deep mod-
els such as 3D-CNN [14], [15], and Inception-ResNet [41],
as well as hand-designed features such as LPQ [20], and
Gabor wavelets [42]. Note that 3DCNN-DAP (Deformable
Action Parts) [14], 3D-CNN [15], Inception-ResNet [41] are
popular spatio-temporal deep feature learning methods by
using the spatial and temporal convolutions simultaneously.
V. CONCLUSION
This paper proposes a hybrid deep learning model, which
consists of the spatial CNN network, the temporal CNN
network, and the DBN fusion network, to apply for FER in
video sequences. We implement our proposed method in two
stages. (1) We employ the existing VGG16 model pre-tained
on ImageNet data to individually ne-tune the spatial CNN
network and the temporal CNN network on target video-
based facial expression data. (2) To deeply fuse the learned
spatio-temporal CNN features, we train a deep DBN model to
jointly learn discriminative spatio-temporal features. Exper-
iment results on three public video-based facial expression
datasets, i.e., BAUM-1s RML, and MMI, demonstrate the
advantages of our proposed method.
In future, we will extend our work to practical applications.
For instance, it is challenging to develop a real-time FER
system based on our proposed method. In addition, it is also
interesting to explore deep compression of deep models so as
to reduce the large network parameters of deep models.
REFERENCES
[1] B. Martinez, M. F. Valstar, B. Jiang, and M. Pantic, ``Automatic analysis of
facial actions: A survey,'' IEEE Trans. Affective Comput. , to be published.
doi: 10.1109/TAFFC.2017.2731763.
[2] X. Zhao and S. Zhang, ``A review on facial expression recognition: Feature
extraction and classication,'' IETE Tech. Rev. , vol. 33, no. 5, pp. 505517,
2016.[3] C. A. Corneanu, M. O. Sim√≥n, J. F. Cohn, and S. E. Guerrero, ``Survey
on RGB, 3D, thermal, and multimodal approaches for facial expression
recognition: History, trends, and affect-related applications,'' IEEE Trans.
Pattern Anal. Mach. Intell. , vol. 38, no. 8, pp. 15481568, Aug. 2016.
[4] E. Sariyanidi, H. Gunes, and A. Cavallaro, ``Automatic analysis of facial
affect: A survey of registration, representation, and recognition,'' IEEE
Trans. Pattern Anal. Mach. Intell. , vol. 37, no. 6, pp. 11131133, Jun. 2015.
[5] G. Muhammad, M. Alsulaiman, S. U. Amin, A. Ghoneim, and
M. F. Alhamid, ``A facial-expression monitoring system for improved
healthcare in smart cities,'' IEEE Access , vol. 5, pp. 1087110881, 2017.
[6] T. Wu, M. S. Bartlett, and J. R. Movellan, ``Facial expression recognition
using Gabor motion energy lters,'' in Proc. IEEE Comput. Soc. Conf.
Comput. Vis. Pattern Recognit. Workshops , San Francisco, CA, USA,
Jun. 2010, pp. 4247.
[7] G. Zhao and M. Pietik√§inen, ``Dynamic texture recognition using local
binary patterns with an application to facial expressions,'' IEEE Trans.
Pattern Anal. Mach. Intell. , vol. 29, no. 6, pp. 915928, Jun. 2007.
[8] B. Jiang, M. F. Valstar, B. Martinez, and M. Pantic, ``A dynamic appearance
descriptor approach to facial actions temporal modeling,'' IEEE Trans.
Cybern. , vol. 44, no. 2, pp. 161174, Feb. 2014.
[9] T. Zhang, W. Zheng, Z. Cui, Y. Zong, J. Yan, and K. Yan, ``A deep neural
network-driven feature learning method for multi-view facial expression
recognition,'' IEEE Trans. Multimedia , vol. 18, no. 12, pp. 25282536,
Dec. 2016.
[10] H. Jung, S. Lee, J. Yim, S. Park, and J. Kim, ``Joint ne-tuning in deep
neural networks for facial expression recognition,'' in Proc. IEEE Int. Conf.
Comput. Vis. (ICCV) , Dec. 2015, pp. 29832991.
[11] K. Zhang, Y. Huang, Y. Du, and L. Wang, ``Facial expression recognition
based on deep evolutional spatial-temporal networks,'' IEEE Trans. Image
Process. , vol. 26, no. 9, pp. 41934203, Sep. 2017.
[12] B. Hasani and M. H. Mahoor, ``Facial expression recognition using
enhanced deep 3D convolutional neural networks,'' in Proc. IEEE Conf.
Comput. Vis. Pattern Recognit. Workshops , Jul. 2017, pp. 22782288.
[13] A. T. Lopes, E. de Aguiar, A. F. De Souza, and T. Oliveira-Santos, ``Facial
expression recognition with convolutional neural networks: Coping with
few data and the training sample order,'' Pattern Recognit. , vol. 61,
pp. 610628, Jan. 2017.
[14] M. Liu, S. Li, S. Shan, R. Wang, and X. Chen, ``Deeply learning
deformable facial action parts model for dynamic expression analysis,''
inProc. Asian Conf. Comput. Vis. (ACCV) , Singapore, 2014, pp. 143157.
[15] S. Zhang, S. Zhang, T. Huang, W. Gao, and Q. Tian, ``Learning affective
features with a hybrid deep model for audiovisual emotion recognition,''
IEEE Trans. Circuits Syst. Video Technol. , vol. 28, no. 10, pp. 30303043,
Oct. 2018.
[16] K. Simonyan and A. Zisserman, ``Very deep convolutional networks for
large-scale image recognition,'' in Proc. ICLR , San Diego, CA, USA, 2015,
pp. 114.
[17] G. E. Hinton and R. R. Salakhutdinov, ``Reducing the dimensionality of
data with neural networks,'' Science , vol. 313, no. 5786, pp. 504507,
2006.
[18] K. Simonyan and A. Zisserman, ``Two-stream convolutional networks for
action recognition in videos,'' in Proc. Int. Conf. Neural Inf. Process. Syst. ,
Montreal, QC, Canada, 2014, pp. 568576.
[19] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng, ``Multimodal
deep learning,'' in Proc. 28th Int. Conf. Mach. Learn. (ICML) , 2011,
pp. 689696.
[20] S. Zhalehpour, O. Onder, Z. Akhtar, and C. E. Erdem, ``BAUM-1: A spon-
taneous audio-visual face database of affective and mental states,'' IEEE
Trans. Affective Comput. , vol. 8, no. 3, pp. 300313, Jul./Sep. 2016.
[21] Y. Wang, L. Guan, and A. N. Venetsanopoulos, ``Kernel cross-
modal factor analysis for information fusion with application to
bimodal emotion recognition,'' IEEE Trans. Multimedia , vol. 14, no. 3,
pp. 597607, Jun. 2012.
[22] M. Pantic, M. Valstar, R. Rademaker, and L. Maat, ``Web-based database
for facial expression analysis,'' in Proc. IEEE Int. Conf. Multimedia
Expo (ICME) , Amsterdam, The Netherlands, Jul. 2005, pp. 317321.
[23] C. Shan, S. Gong, and P. W. McOwan, ``Facial expression recognition
based on local binary patterns: A comprehensive study,'' Image Vis. Com-
put., vol. 27, no. 6, pp. 803816, 2009.
[24] U. Tariq, J. Yang, and T. S. Huang, ``Multi-view facial expression recog-
nition analysis with generic sparse coding feature,'' in Proc. Eur. Conf.
Comput. Vis. (ECCV) , 2012, pp. 578588.
VOLUME 7, 2019 32303
S. Zhang et al. : Learning Affective Video Features for FER via Hybrid Deep Learning
[25] U. Tariq et al. , ``Emotion recognition from an ensemble of features,''
inProc. IEEE Int. Conf. Autom. Face Gesture Recognit. Workshops (FG) ,
Santa Barbara, CA, USA, Mar. 2011, pp. 872877.
[26] A. Klaser, M. Marsza¬™ek, and C. Schmid, ``A spatio-temporal descriptor
based on 3D-gradients,'' in Proc. 19th Brit. Mach. Vis. Conf. (BMVC) ,
vol. 275, 2008, pp. 110.
[27] P. Scovanner, S. Ali, and M. Shah, ``A 3-dimensional sift descriptor and its
application to action recognition,'' in Proc. 15th ACM Int. Conf. Multime-
dia (MM) , Augsburg, Germany, 2007, pp. 357360.
[28] M. Hayat, M. Bennamoun, and A. El-Sallam, ``Evaluation of spatiotem-
poral detectors and descriptors for facial expression recognition,'' in Proc.
5th Int. Conf. Hum. Syst. Interact. (HSI) , Perth, WA, Australia, 2012,
pp. 4347.
[29] M. Liu, S. Shan, R. Wang, and X. Chen, ``Learning expressionlets
via universal manifold model for dynamic facial expression recogni-
tion,'' IEEE Trans. Image Process. , vol. 25, no. 12, pp. 59205932,
Dec. 2016.
[30] X. Fan and T. Tjahjadi, ``A dynamic framework based on local Zernike
moment and motion history image for facial expression recognition,''
Pattern Recognit. , vol. 64, pp. 399406, Apr. 2017.
[31] H. Yan, ``Collaborative discriminative multi-metric learning for facial
expression recognition in video,'' Pattern Recognit. , vol. 75, pp. 3340,
Mar. 2018.
[32] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ``ImageNet classica-
tion with deep convolutional neural networks,'' in Proc. Adv. Neural Inf.
Process. Syst. , 2012, pp. 11061114.
[33] C. Szegedy et al., ``Going deeper with convolutions,'' in Proc. IEEE Conf.
Comput. Vis. Pattern Recognit. (CVPR) , Boston, MA, USA, Jun. 2015,
pp. 19.
[34] K. He, X. Zhang, S. Ren, and J. Sun, ``Deep residual learning for image
recognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) ,
Las Vegas, NV, USA, Jun. 2016, pp. 770778.
[35] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, ``Gradient-based learn-
ing applied to document recognition,'' Proc. IEEE , vol. 86, no. 11,
pp. 22782324, Nov. 1998.
[36] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, ``Learn-
ing spatiotemporal features with 3D convolutional networks,'' in Proc.
IEEE Int. Conf. Comput. Vis. (ICCV) , Santiago, Chile, Dec. 2015,
pp. 44894497.
[37] G. Gkioxari and J. Malik, ``Finding action tubes,'' in Proc. IEEE Conf.
Comput. Vis. Pattern Recognit. (CVPR) , Boston, MA, USA, Jun. 2015,
pp. 759768.
[38] P. Viola and M. J. Jones, ``Robust real-time face detection,'' Int. J. Comput.
Vis., vol. 57, no. 2, pp. 137154, 2004.
[39] G. E. Hinton, ``Training products of experts by minimizing contrastive
divergence,'' Neural Comput. , vol. 14, no. 8, pp. 17711800, 2002.
[40] G. E. Hinton, S. Osindero, and Y.-W. Teh, ``A fast learning algorithm for
deep belief nets,'' Neural Comput. , vol. 18, no. 7, pp. 15271554, 2006.
[41] B. Hasani and M. H. Mahoor, ``Spatio-temporal facial expression recogni-
tion using convolutional neural networks and conditional random elds,''
inProc. 12th IEEE Int. Conf. Autom. Face Gesture Recognit. (FG) ,
Washington, DC, USA, May/Jun. 2017, pp. 790795.
[42] N. El D. Elmadany, Y. He, and L. Guan, ``Multiview emotion recognition
via multi-set locality preserving canonical correlation analysis,'' in Proc.
IEEE Int. Symp. Circuits Syst. (ISCAS) , Montreal, QC, Canada, May 2016,
pp. 590593.
SHIQING ZHANG received the Ph.D. degree
from the School of Communication and Informa-
tion Engineering, University of Electronic Science
and Technology of China, in 2012. He held a
Postdoctoral position with the School of Electronic
Engineering and Computer Science, Peking Uni-
versity, Beijing, China, from 2015 to 2017. He is
currently a Professor with the Institute of Intelli-
gent Information Processing, Taizhou University,
China. He has published over 30 papers in journals
such as the IEEE T RANSACTIONS ON MULTIMEDIA and the IEEE T RANSACTIONS ON
CIRCUITS AND SYSTEMS FOR VIDEOTECHNOLOGY . His research interests include
affective computing and pattern recognition.
XIANZHANG PAN received the B.S. and M.S.
degrees in computer science from Lanzhou
Jiaotong University. He is currently a Senior
Engineer with the Institute of Intelligent Infor-
mation Processing, Taizhou University, China.
His research interests include image processing,
affective computing, and pattern recognition.
YUELI CUI received the B.S. degree in electron-
ics and communication engineering from Zhejiang
University City College, Hangzhou, in 2006, and
the M.S. degree in electronics and communica-
tion engineering from Hebei University, Baoding,
in 2009. He is currently a Lecturer with the Depart-
ment of Physics and Electronics Engineering,
Taizhou University, China. His research interests
include image processing and pattern recognition.
XIAOMING ZHAO received the B.S. degree
in mathematics from Zhejiang Normal Univer-
sity, in 1990, and the M.S. degree in software
engineering from Beihang University, in 2006.
He is currently a Professor with the Institute
of Intelligent Information Processing, Taizhou
University, China. His research interests include
image processing, machine learning, and pattern
recognition.
LIMEI LIU received the Ph.D. degree from the
School of Information Science and Engineering,
Central South University, in 2011. She held a post-
doctoral position with the Business School, Hunan
University, Changsha, China, from 2013 to 2017.
She is currently a Professor with the Institute of
Big Data and Internet Innovation, Hunan Univer-
sity of Commerce, China. Her research interests
include machine learning and pattern recognition.
32304 VOLUME 7, 2019
"
https://ieeexplore.ieee.org/document/9474949,Error
https://ieeexplore.ieee.org/document/8080244,"1057-7149 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2765830, IEEE
Transactions on Image Processing
1
Multi-Task Convolutional Neural Network for
Pose-Invariant Face Recognition
Xi Yin and Xiaoming Liu Member, IEEE
Abstract ‚ÄîThis paper explores Multi-Task Learning (MTL) for
face recognition. First, we propose a multi-task Convolutional
Neural Network (CNN) for face recognition where identity
classiÔ¨Åcation is the main task and Pose, Illumination, and
Expression (PIE) estimations are the side tasks. Second, we
develop a dynamic-weighting scheme to automatically assign the
loss weights to each side task, which solves the crucial problem
of balancing between different tasks in MTL. Third, we propose
a pose-directed multi-task CNN by grouping different poses to
learn pose-speciÔ¨Åc identity features, simultaneously across all
poses in a joint framework. Last but not least, we propose
an energy-based weight analysis method to explore how CNN-
based MTL works. We observe that the side tasks serve as
regularizations to disentangle the PIE variations from the learnt
identity features. Extensive experiments on the entire Multi-PIE
dataset demonstrate the effectiveness of the proposed approach.
To the best of our knowledge, this is the Ô¨Årst work using all data
in Multi-PIE for face recognition. Our approach is also applicable
to in-the-wild datasets for pose-invariant face recognition and
achieves comparable or better performance than state of the art
on LFW, CFP, and IJB-A datasets.
Index Terms‚Äîmulti-task learning, pose-invariant face recog-
nition, CNN, disentangled representation
I. I NTRODUCTION
FACE recognition is a challenging problem that has been
studied for decades in computer vision. The large intra-
person variations in Pose, Illumination, Expression (PIE),
and etc. will challenge any state-of-the-art face recognition
algorithms. Recent CNN-based approaches mainly focus on
exploring the effects of 3D model-based face alignment [49],
larger datasets [49], [41], or new metric learning algo-
rithms [45], [41], [32], [53] on face recognition performance.
Most existing methods consider face recognition as a single
task problem. We believe that face recognition is not an iso-
lated problem ‚Äî often tangled with other tasks. This motivates
us to explore multi-task learning for face recognition.
Multi-Task Learning (MTL) aims to learn several tasks
simultaneously to boost the performance of the main task or
all tasks. It has been successfully applied to face detection [7],
[59], face alignment [62], pedestrian detection [50], attribute
estimation [1], and so on. Despite the success of MTL in
various vision problems, there is a lack of study on MTL for
face recognition. In this paper, we study face recognition as
a multi-task problem where identity classiÔ¨Åcation is the main
task with PIE estimations being the side tasks. The goal is
to leverage the side tasks to improve the performance of the
main task, i.e., face recognition.
Xi Yin and Xiaoming Liu are with the Department of Computer Science
and Engineering, Michigan State University, East Lansing, MI 48824. Corre-
sponding author: Xiaoming Liu, liuxm@cse.msu.edu
CNN 
PIE-variant 
training data entangled  
features 
weights in 
FC layer Wd
Wp
disentangled 
features 
identity  
classification 
pose 
classification PIE-invariant  
identity features 
Fig. 1. We propose MTL for face recognition with identity classiÔ¨Åcation as
the main task and PIE classiÔ¨Åcations as the side tasks (only pose is illustrated
in this Ô¨Ågure for simplicity). A CNN framework learns entangled features from
the data. The weight matrix in the fully connected layer of the main task is
learnt to have close-to-zero values for PIE features in order to exclude PIE
variations, which results in PIE-invariant identity features for face recognition.
We answer the questions of how and why PIE estimations
can help face recognition. Regarding how, we propose a multi-
task CNN (m-CNN) framework for joint identity classiÔ¨Åcation
and PIE classiÔ¨Åcations, which learns a shared embedding.
Regarding why, we conduct an energy-based weight analysis
and observe that the side tasks serve as regularizations to
inject PIE variations into the shared embedding, which is
further disentangled into PIE-invariant identity features for
face recognition. As shown in Figure 1, a shared embedding
is learnt from PIE-variant training images through a CNN
framework. A fully connected layer is connected to the shared
features to perform classiÔ¨Åcation of each task. The shared
features entangles both identity and PIE variations, and the
weight matrix in the fully connected layer performs feature
selection for disentanglement.
One crucial problem in MTL is how to determine the
importance of each task. Prior work either treats each task
equally [57] or obtains the loss weights by greedy search [50].
It may not be fair to assume that each task contributes equally.
However, it will be very time consuming or practically impos-
sible to Ô¨Ånd the optimal weights for all side tasks via brute-
force search. Instead, we propose a dynamic-weighting scheme
where we only need to determine the overall loss weight for
the PIE estimations, and the CNN can learn to dynamically
assign a loss weight to each side task during training. This is
effective and efÔ¨Åcient as will shown in Section IV.
Since pose variation is the most challenging one among
other non-identity variations, and the proposed m-CNN al-
ready classiÔ¨Åes all images into different pose groups, we
propose to apply divide-and-conquer to the CNN learning.
SpeciÔ¨Åcally, we develop a novel pose-directed multi-task CNN
(p-CNN) where the pose labels can categorize the training data
1057-7149 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2765830, IEEE
Transactions on Image Processing
2
TABLE I
COMPARISON OF THE EXPERIMENTAL SETTINGS THAT ARE COMMONLY USED IN PRIOR WORK ON MULTI -PIE. (* T HE20IMAGES
CONSIST OF 2DUPLICATES OF NON -FLASH IMAGES AND 18FLASH IMAGES . IN TOTAL THERE ARE 19DIFFERENT ILLUMINATIONS .)
setting session pose illum exp train subjects / images gallery / probe images total references
I 4 7 1 1 200 /5;383 137 /2;600 8; 120 [4], [28]
II 1 7 20 1 100 /14;000 149 /20;711 34; 860 [65], [57]
III 1 15 20 1 150 /45;000 99/29;601 74; 700 [54]
IV 4 9 20 1 200 /138;420 137 /70;243 208; 800 [66], [51]
V 4 13 20 1 200 /199;940 137 /101;523 301; 600 [58]
ours 4 15 20* 6 200 /498;900 137 /255;163 754; 200
into three different pose groups, direct them through different
routes in the network to learn pose-speciÔ¨Åc identity features
in addition to the generic identity features. Similarly, the loss
weights for extracting these two types of features are learnt
dynamically in the CNN framework. During the testing stage,
we propose a stochastic routing scheme to fuse the generic
identity features and the pose-speciÔ¨Åc identity features for face
recognition, which is more robust to pose estimation errors.
We Ô¨Ånd this technique to be very effective for pose-invariant
face recognition especially for in-the-wild faces, where pose
classiÔ¨Åcation error is more likely to happen compared to
controlled datasets with discrete pose angles.
This work utilizes alldata in the Multi-PIE dataset [16],
i.e., faces with the full range of PIE variations, as the main
experimental dataset ‚Äî ideal for studying MTL for PIE-
invariant face recognition. To the best of our knowledge, there
is no prior face recognition work that studies the full range
of variations on Multi-PIE. We also apply our method to in-
the-wild datasets for pose-invariant face recognition. Since the
ground truth label of the side task is unavailable, we use the
estimated poses as labels for training.
In summary, we make four contributions.
We formulate face recognition as an MTL problem and
explore how it works via an energy-based weight analysis.
We propose a dynamic-weighting scheme to learn the loss
weights for each side task automatically in the CNN.
We develop a pose-directed multi-task CNN to learn
pose-speciÔ¨Åc identity features and a stochastic routing
scheme for feature fusion during the testing stage.
We perform a comprehensive and the Ô¨Årst face recogni-
tion study on the entire Multi-PIE. We achieve compa-
rable or superior performance to state-of-the-art methods
on Multi-PIE, LFW [20], CFP [42], and IJB-A [26].
II. R ELATED WORK
A. Face Recognition
Recent progress in face recognition has been mainly focused
on developing metric learning algorithms including center
loss [53], A-softmax [33], N-pair [44], etc. In this work, we
study Pose-Invariant Face Recognition (PIFR) via CNN-based
multi-task learning. Therefore, we focus our review on PIFR
and MTL-based approaches.
Pose-Invariant Face Recognition According to [11], existing
PIFR methods can be classiÔ¨Åed into four categories including:
multi-view subspace learning [27], [2], pose-invariant featureextraction [6], [41], [39], face synthesis [64], [19], [58],
and a hybrid approach of the above three [51], [57]. For
example, FF-GAN [58] incorporates a 3D Morphable Model
(3DMM) [5] into a CNN framework for face frontalization
with various loss functions. The frontalized faces can be
used to improve the face recognition performance especially
for large-pose faces. By modeling the face rotation process,
DR-GAN [51] learns both a generative and discriminative
representation from one or multiple face images of the same
subject. This representation can be used for PIFR and face
image synthesis.
Our work belongs to the second category, i.e., pose-invariant
feature extraction. Previous work in this category treats each
pose separately. For example, Masi et al. [34] propose a pose-
aware face recognition method by learning a speciÔ¨Åc model
for each type of face alignment and pose group. And the
results are fused together during the testing stage. The idea
of divide-and-conquer is similar to our work. Differently, we
learn the pose-invariant identity features for all poses jointly in
one CNN framework and propose a stochastic routing scheme
during the testing stage for feature fusion, which is more
efÔ¨Åcient and robust. Xiong et al. [54] propose a conditional
CNN for PIFR, which discovers the modality information
automatically during training. In contrast, we utilize the pose
labels as a side task to better disentangle pose variation from
the learnt identity features. Peng et al. [39] use the pose
classiÔ¨Åcation as a side task to learn a rich embedding, which is
further disentangled via pair-wise reconstruction. However, it
need to be trained on datasets with non-frontal and frontal face
pairs, which is not widely available especially for in-the-wild
scenario.
MTL for Face Recognition For MTL-based face recognition
method, Ding et al. [12] propose to transform the features
of different poses into a discriminative subspace, and the
transformations are learnt jointly for all poses with one task
for each pose. [57] develops a deep neural network to rotate
a face image. The reconstruction of the face is considered
as a side task. This multi-task framework is more effective
than the single task model without the reconstruction part.
Similar work [66], [51] have developed along this direction
to extract robust identity features and synthesize face images
simultaneously. In this work, we treat face recognition as a
multi-task problem with PIE estimations as the side tasks. It
sounds intuitive to have PIE as side tasks for face recognition,
but we are actually the Ô¨Årst to consider this and we have
explored how it works.
1057-7149 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2765830, IEEE
Transactions on Image Processing
3
B. Multi-Task Learning
Multi-task learning has been widely studied in machine
learning [15], [3] and computer vision [50], [62]. We focus
our review on different regularizations in MTL, CNN-based
MTL, and the importance of each task in MTL.
Regularizations The underlying assumption for most MTL
algorithms is that different tasks are related to each other.
Thus, a key problem is how to determine the task relatedness.
One common way is to learn shared features for different
tasks. The generalized linear model parameterizes each task
with a weight vector. The weight vectors of all tasks form a
weight matrix, which is regularized by l2;1norm [3], [37] or
trace norm [22] to encourage a low-rank matrix. For example,
Obozinski et al. [37] propose to penalize the sum of l2
norm of the blocks of weights associated with each feature
across different tasks to encourage similar sparsity patterns.
Lin et al. [30] propose to learn higher order feature interaction
without limiting to a linear model for MTL. Other work [14],
[61], [31] propose to learn the task relationship from a task
covariance matrix computed from the data.
CNN-based MTL It is natural to fuse MTL with CNN to
learn the shared features and the task-speciÔ¨Åc models. For
example, [62] proposes a deep CNN for joint face detection,
pose estimation, and landmark localization. Misra et. al. [35]
propose a cross-stitch network for MTL to learn the sharing
strategy, which is difÔ¨Åcult to scale to multiple tasks. This
requires training one model for each task and introduces
additional parameters in combining them. In [59], a task-
constrained deep network is developed for landmark detection
with facial attribute classiÔ¨Åcations as the side tasks. However,
unlike the regularizations used in the MTL formulation in the
machine learning community, there is no principled method
to analysis how MTL works in the CNN framework. In this
paper, we propose an energy-based weight analysis method to
explore how MTL works. We discover that the side tasks of
PIE estimations serve as regularizations to learn more discrim-
inative identity features that are robust to PIE variations.
Importance of Each Task It is important to determine the
loss weight for each task in MTL. The work of [57] uses
equal loss weights for face recognition and face frontalization.
Tian et al. [50] propose to obtain the loss weights of all side
tasks via greedy search within 0and1. Lettandkbe the
number of side tasks and searched values respectively. This
approach has two drawbacks. First, it is very inefÔ¨Åcient as
the computation scales to the number of tasks (complexity
tk). Second, the optimal loss weight obtained for each task
may not be jointly optimal. Further, the complexity would be
ktif searching all combinations in a brute-force way. Zhang
et al. [62] propose a task-wise early stopping to halt a task
during training when the loss no longer reduces. However, a
stopped task will never resume so its effect may disappear.
In contrast, we propose a dynamic-weighting scheme where
we only determine the overall loss weight for all side tasks
(complexity k) and let CNN learn to automatically distribute
the weights to each side task. In this case when one task is
saturated, we have observed the dynamic weights will reduce
without the need of early stopping.III. T HEPROPOSED APPROACH
In this section, we present the proposed approach by using
Multi-PIE dataset as an example and extend it to in-the-wild
datasets in the experiments. First, we propose a multi-task
CNN (m-CNN) with dynamic weights for face recognition
(the main task) and PIE estimations (the side tasks). Second,
we propose a pose-directed multi-task CNN (p-CNN) to tackle
pose variation by separating all poses into different groups and
jointly learning pose-speciÔ¨Åc identity features for each group.
A. Multi-Task CNN
We combine MTL with CNN framework by sharing some
layers between different tasks. In this work, we adapt CASIA-
Net [56] with three modiÔ¨Åcations. First, Batch Normalization
(BN) [21] is applied to accelerate the training process. Second,
the contrastive loss is excluded for simplicity. Third, the
output dimension of the fully connected layer is changed
according to different tasks. Details of the layer parameters
are shown in Figure 2. The network consists of Ô¨Åve blocks
each including two convolutional layers and a pooling layer.
BN and ReLU [36] are used after each convolutional layer.
Similar to [56], no ReLU is used after conv52 layer to learn
a compact feature representation, and a dropout layer with a
ratio of 0:4is applied after pool5 layer.
Given a training set TwithNimages and their labels:
T=fIi;yigN
i=1, where Iiis the image and yiis a vector
consisting of the identity label yd
i(main task) and the side task
labels. In our work, we consider three side tasks including pose
(yp
i), illumination (yl
i), and expression (ye
i). We eliminate the
sample index ifor clarity. As shown in Figure 2, the proposed
m-CNN extracts a high-level feature embedding x2RD1:
x=f(I;k;b;;); (1)
wheref()represents the non-linear mapping from the input
image to the shared features. kandbare the sets of Ô¨Ålters and
bias of all the convolutional layers. andare the sets of
scales and shifts in the BN layers [21]. Let =fk;b;;g
denote all parameters to be learnt to extract the features x.
The extracted features x, which is pool5 in our model,
are shared among all tasks. Suppose Wd2RDDdand
bd2RDd1are the weight matrix and bias vector in the
fully connected layer for identity classiÔ¨Åcation, where Ddis
the number of different identities in T. The generalized linear
model can be applied:
yd=Wd|x+bd: (2)
ydis fed to a softmax layer to compute the probability of
xbelonging to each subject in the training set:
softmax(yd)n=p(^yd=njx) =exp(yd
n)P
jexp(yd
j); (3)
where yd
jis thejth element in yd. Thesoftmax() function
converts the output ydto a probability distribution over all
subjects and the subscript selects the nth element. Finally, the
estimated identity ^ydis obtained via:
^yd= argmax
nsoftmax(yd)n: (4)
4
conv11:3x3/1/32 conv12: 3x3/1/64 pool1: max/2x2/2 conv21:3x3/1/64 conv22: 3x3/1/128 pool2: max/2x2/2 conv31:3x3/1/96 conv32: 3x3/1/192 pool3: max/2x2/2 conv41:3x3/1/128 conv42: 3x3/1/256 pool4: max/2x2/2 conv51:3x3/1/160 conv52: 3x3/1/320 pool5: avg/7x7/1 50x50x64 25x25x128 13x13x192 7x7x256 1x1x320 100x100x1 s-CNN:  
m-CNN:  
p-CNN:  
¬µs
¬µm
fc6_id fc6_pos 
fc6_exp fc6_illum fc: 200/13/6/19/3/2 
block 1 
block 2 
block 3 
block 4 
block 5 
fc6_left fc6_frontal fc6_right fc: 200/200/200 
pose-directed branch 
batch split 
Fig. 2. The proposed m-CNN and p-CNN for face recognition. Each block reduces the spatial dimensions and increases the feature channels. The parameter
format for the convolutional layer is: Ô¨Ålter size / stride / Ô¨Ålter number. The parameter format for the pooling layer is: method / Ô¨Ålter size / stride. The feature
dimensions after each block are shown on the bottom. The color indicates the component for each model. The dashed line represents the batch split operation
as shown in Figure 3. The layers with the stripe pattern are the identity features used in the testing stage for face recognition.
The cross-entropy loss is employed:
L(I;yd) = log(p(^yd=ydjI;;Wd;bd)): (5)
Similarly, we formulate the losses for the side tasks. Let
W=fWd;Wp;Wl;Wegrepresent the weight matrices for
identity and PIE classiÔ¨Åcations. The bias terms are eliminated
for simplicity. Given the training set T, our m-CNN aims to
minimize the combined loss of all tasks:
argmin
;WdNX
i=1L(Ii;yd
i) +pNX
i=1L(Ii;yp
i)+
lNX
i=1L(Ii;yl
i) +eNX
i=1L(Ii;ye
i);(6)
whered,p,l,econtrol the importance of each task.
It becomes a single-task model (s-CNN) when p;l;e = 0.
The loss drives the model to learn both the parameters 
for extracting the shared features and Wfor the classiÔ¨Åcation
tasks. In the testing stage, the features before the softmax layer
(yd) are used for face recognition by applying a face matching
procedure based on cosine similarity.
B. Dynamic-Weighting Scheme
In MTL, it is an open question on how to set the loss weight
for each task. Prior work either treats all tasks equally [57] or
obtains the loss weights via brute-force search [50], which
is very time-consuming especially considering the training
time for CNN models. To solve this problem, we propose a
dynamic-weighting scheme to automatically assign the loss
weights to each side task during training.
First, we set the weight for the main task to 1, i.e.d= 1.
Second, instead of Ô¨Ånding the loss weight for each task, we
Ô¨Ånd the summed loss weight for all side tasks, i.e. 's=p+
l+e, via brute-force search on a validation set. Our m-
CNN learns to allocate 'sto three side tasks. As shown in
Figure 2, we add a fully connected layer and a softmax layer
to the shared features xto learn the dynamic weights. Let
!s2RD3ands2R31denote the weight matrix and
bias vector in this fully connected layer,
s=softmax (!s|x+s); (7)wheres= [p;l;e]|are the dynamic weight percentages
for the side tasks with p+l+e= 1 andp;l;e0. So
Equation 6 becomes:
argmin
;W;!sNX
i=1L(Ii;yd
i) +'sh
pNX
i=1L(Ii;yp
i)+
lNX
i=1L(Ii;yl
i) +eNX
i=1L(Ii;ye
i)i
s:t:  p+l+e= 1;  p;l;e0(8)
The multiplications of the overall loss weight 'swith the
learnt dynamic percentage p;l;e are the dynamic loss weights
for each side task, i.e., p;l;e='sp;l;e.
We use mini-batch Stochastic Gradient Descent (SGD) to
solve the above optimization problem where the dynamic
weights are averaged over a batch of samples. Intuitively,
we expect the dynamic-weighting scheme to behave in two
different aspects in order to minimize the loss in Equation 8.
First, since our main task contribute mostly to the Ô¨Ånal loss
('s<1), the side task with the largest contribution to the
main task should have the highest weight in order to reduce
the loss of the main task. Second, our m-CNN should assign
a higher weight for an easier task with a lower loss so as to
reduce the overall loss. We have observed these effects as will
shown in the experiments.
C. Pose-Directed Multi-Task CNN
It is very challenging to learn a non-linear mapping to
estimate the correct identity from a face image with arbitrary
PIE, given the diverse variations in the data. This challenge
has been encountered in classic pattern recognition work. For
example, in order to handle pose variation, [29] proposes
to construct several face detectors where each of them is in
charge of one speciÔ¨Åc view. Such a divide-and-conquer scheme
can be applied to CNN learning because the side tasks can
‚Äúdivide‚Äù the data and allow the CNN to better ‚Äúconquer‚Äù them
by learning tailored mapping functions.
Therefore, we propose a novel task-directed multi-task CNN
where the side task labels categorize the training data into
multiple groups, and direct them to different routes in the
network. Since pose is considered as the primary challenge
in face recognition [54], [60], [66], we propose pose-directed
5
1	 4	 7	 3	
2	 5	 8	 5	
..	 ..	 ..	 ..	
3	 6	 9	 4	
1	 0	 0	 0	
2	 0	 0	 0	
..	 ..	 ..	 ..	
3	 0	 0	 0	
0	 4	 0	 3	
0	 5	 0	 5	
..	 ..	 ..	 ..	
0	 6	 0	 4	
0	 0	 7	 0	
0	 0	 8	 0	
..	 ..	 ..	 ..	
0	 0	 9	 0	
Fig. 3. Illustration of the batch split operation in p-CNN. The Ô¨Årst row shows
the input images and the second row shows a matrix representing the features
xfor each sample. After batch split, one batch of samples is separated into
three batches where each of them only consists of the samples belonging to
the speciÔ¨Åc pose group.
WlWfWr
Pose-Specific Identity Features Pose-Directed Multi-Task CNN 
Fig. 4. The proposed pose-directed multi-task CNN aims to learn pose-
speciÔ¨Åc identity features jointly for all pose groups.
multi-task CNN (p-CNN) to handle pose variation. However,
it is applicable to other variation.
As shown in Figure 2, p-CNN is built on top of m-CNN
by adding the pose-directed branch (PDB). The PDB groups
face images with similar poses to learn pose-speciÔ¨Åc identity
features via a batch split operation. We separate the training set
into three groups according to the pose labels: left proÔ¨Åle ( Gl),
frontal (Gf), and right proÔ¨Åle ( Gr). As shown in Figure 3, the
goal of the batch split is to separate a batch of N0samples
(X=fxigN0
i=1) into three batches Xl,Xf, andXr, which are
of the same size as X. During training, the ground truth pose
is used to assign a face image into the correct group. Let us
take the frontal group as an example to illustrate the batch
split operation:
Xf
i=(
xi;ifyp
i2Gf
0;otherwise;(9)
where 0denotes a vector of all zeros with the same dimension
asxi. The assignment of 0is to avoid the case when no
sample is passed into one group, the next layer will still have
valid input. As a result, Xis separated into three batches
where each batch consists of only the samples belonging to the
corresponding pose group. Each group learns a pose-speciÔ¨Åc
mapping to a joint space, resulting in three different sets of
weights:fWl;Wf;Wrg, as illustrated in Figure 4. Finally
the features from all groups are merged as the input to a
softmax layer to perform robust identity classiÔ¨Åcation jointly.
Left frontal right 
0.2 0.3 0.5 0.4 0.3 0.3 
1	Fig. 5. Blue bars are the generic identity features and purple bars are the
pose-speciÔ¨Åc features. The numbers are the probabilities of each input image
belonging to each pose group. The proposed stochastic routing in the testing
stage taking account of all pair comparisons so that it is more robust to pose
estimation errors.
Our p-CNN aims to learn two types of identity features:
Wdis the weight matrix to extract the generic identity features
that is robust to all poses; Wl;f;rare the weight matrices to
extract the pose-speciÔ¨Åc identity features that are robust within
a small pose range. Both tasks are considered as our main
tasks. Similar to the dynamic-weighting scheme in m-CNN,
we use dynamic weights to combine our main tasks as well.
The summed loss weight for these two tasks is 'm=d+g.
Let!m2RD2andm2R21denote the weight matrix
and bias vector for learning the dynamic weights,
m=softmax (!m|x+m): (10)
We havem= [d;g]|as the dynamic weights for generic
identity classiÔ¨Åcation and pose-speciÔ¨Åc identity classiÔ¨Åcation.
Finally, the loss of p-CNN is formulated as:
argmin
;W;!'mh
dNX
i=1L(Ii;yd
i) +gGX
g=1NgX
i=1L(Ii;yd
i)i
+
'sh
pNX
i=1L(Ii;yp
i) +lNX
i=1L(Ii;yl
i) +eNX
i=1L(Ii;ye
i)i
s:t:  d+g= 1;p+l+e= 1;d;g0;p;l;e0(11)
whereG= 3 is the number of pose groups and Ngis the
number of training images in the g-th group.!=f!m;!sg
is the set of parameters to learn the dynamic weights for both
the main and side tasks. We set 'm= 1.
Stochastic Routing Given a face image in the testing
stage, we can extract the generic identity features ( yd), the
pose-speciÔ¨Åc identity features ( fygg3
g=1), as well as estimate
the probabilities (fpgg3
g=1) of the input image belonging to
each pose group by aggregating the probabilities from the
pose classiÔ¨Åcation side task. As shown in Figure 5, for face
matching, we can compute the distance of the generic identity
features and the distance of the pose-speciÔ¨Åc identity features
by selecting the pose group with the largest probability (red
underline). However, the pose estimation error may cause in-
ferior feature extraction results, which is inevitable especially
for unconstrained faces.
To solve this problem, we propose a stochastic routing
scheme by taking into account of all comparisons, which is
1057-7149 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2765830, IEEE
Transactions on Image Processing
6
TABLE II
PERFORMANCE COMPARISON (%)OF SINGLE -TASK LEARNING (S-CNN), MULTI -TASK LEARNING (M-CNN) WITH ITS VARIANTS ,AND POSE -DIRECTED
MULTI -TASK LEARNING (P-CNN) ON THE ENTIRE MULTI -PIE DATASET .
model loss weights rank-1 (all / left / frontal /right) pose illum exp
s-CNN: id d= 1 75:67 /71:51 /82:21/73:29 ‚Äì ‚Äì ‚Äì
s-CNN: pos p= 1 ‚Äì 99:87 ‚Äì ‚Äì
s-CNN: exp l= 1 ‚Äì ‚Äì 96:43 ‚Äì
s-CNN: illum e= 1 ‚Äì ‚Äì ‚Äì 92:44
s-CNN: id+L2 d= 1 76:43 /73:31 /81:98/73:99 ‚Äì ‚Äì ‚Äì
m-CNN: id+pos d= 1;p= 0:1 78:06 /75:06 /82:91/76:21 99:78 ‚Äì ‚Äì
m-CNN: id+illum d= 1;l= 0:1 77:30/74:87 /82:83/74:21 ‚Äì 93:57 ‚Äì
m-CNN: id+exp d= 1;e= 0:1 77:76 /75:48 /82:32/75:48 ‚Äì ‚Äì 90:93
m-CNN: id+all d= 1;p;l;e= 0:033 77:59 /74:75 /82:99 /75:04 99:75 88:46 79:97
m-CNN: id+all (dynamic) d= 1;'s= 0:1 79:35 /76:60 /84:65 /76:82 99:81 93:40 91:47
p-CNN 'm= 1;'s= 0:1 79:55/76:14 /84:87/77:65 99:80 90:58 90:02
more robust to pose estimation errors. SpeciÔ¨Åcally, the distance
cbetween a pair of face images (I 1andI2) is computed as the
average between the distance of the generic identity features
(yd
1,yd
2) and weighted distance of the pose-speciÔ¨Åc identity
features (fyg
1g,fyg
2g):
c=1
2h(yd
1;yd
2) +1
23X
i=13X
j=1h(yi
1;yj
2)pi
1pj
2; (12)
whereh() is the cosine distance metric used to measure the
distance between two feature vectors. The proposed stochastic
routing accounts for all combinations of the pose-speciÔ¨Åc
identity features weighted by the probabilities of each combi-
nation. We treat the generic features and pose-speciÔ¨Åc features
equally, and fuse them for face recognition.
IV. E XPERIMENTS
We evaluate the proposed m-CNN and p-CNN under two
settings: (1) face identiÔ¨Åcation on Multi-PIE with PIE estima-
tions being the side tasks; (2) face veriÔ¨Åcation/identiÔ¨Åcation on
in-the-wild datasets including LFW, CFP, and IJB-A, where
pose estimation is the only side task. Further, we analyze
the effect of MTL on Multi-PIE and discover that the side
tasks regularize the network to learn a disentangled identity
representation for PIE-invariant face recognition.
A. Face IdentiÔ¨Åcation on Multi-PIE
Experimental Settings The Multi-PIE dataset consists of
754;200images of 337subjects recorded in 4sessions. Each
subject was recorded with 15different cameras where 13are at
the head height spaced at 15interval and 2are above the head.
For each camera, a subject was imaged under 19different
illuminations. In each session, a subject was captured with 2
or3expressions, resulting in a total of 6different expressions
across all sessions.
All previous work [10], [18], [12], [17], [25], [28], [54],
[55], [60], [64], [66] studies face recognition on a subset
of PIE variations on Multi-PIE. In our work, we use the
entire dataset including all PIE variations. For the two cameras
above the head, their poses are labeled as 45. The Ô¨Årst 200
subjects are used for training. The remaining 137subjects areused for testing, where one image with frontal pose, neutral
illumination, and neutral expression for each subject is selected
as the gallery set and the remaining as the probe set.
We use the landmark annotations [13] to align each face to
a canonical view of size 100100. The images are normalized
by subtracting 127:5 and dividing by 128, similar to [53]. We
use Caffe [23] with our modiÔ¨Åcations. The momentum is set to
0:9and the weight decay to 0:0005. All models are trained for
20epochs from scratch with a batch size of 4unless speciÔ¨Åed.
The learning rate starts at 0:01 and reduces at 10th, 15th, and
19th epochs with a factor of 0:1. The rank-1 identiÔ¨Åcation rate
is reported as the face recognition performance. For the side
tasks, the mean accuracy over all classes is reported.
We randomly select 20subjects from the training set to
form a validation set to Ô¨Ånd the optimal overall loss weight
for all side tasks. We obtain 's= 0:1 via brute-force search.
For p-CNN model training, we split the training set into
three groups based on the yaw angle of the image: right pro-
Ô¨Åle ( 90; 75; 60, 45), frontal ( 30, 15;0;15,
30), and left proÔ¨Åle (45,60;75;90).
Effects of MTL Table II shows the performance comparison
of single-task learning (s-CNN), multi-task learning (m-CNN),
and pose-directed multi-task learning (p-CNN) on the entire
Multi-PIE. First, we train four single-task models for identity
(id), pose (pos), illumination (illum), and expression (exp)
classiÔ¨Åcation respectively. As shown in Table II, the rank-1
identiÔ¨Åcation rate of s-CNN is only 75:67%. The performance
of the frontal pose group is much higher than those of the
proÔ¨Åle pose groups, indicating that pose variation is indeed
a big challenge for face recognition. Among all side tasks,
pose estimation is the easiest task, followed by illumination,
and expression as the most difÔ¨Åcult one. This is caused by
two potential reasons: 1) discriminating expression is more
challenging due to the non-rigid face deformation; 2) the
data distribution over different expressions is unbalanced with
insufÔ¨Åcient training data for some expressions.
Second, we train multiple m-CNN models by adding only
one side task at a time in order to evaluate the inÔ¨Çuence of
each side task. We use ‚Äúid+pos‚Äù, ‚Äúid+illum‚Äù, and ‚Äúid+exp‚Äù to
represent these variants and compare them to the performance
of adding all side tasks denoted as ‚Äúid+all‚Äù. To evaluate the
effects of the dynamic-weighting scheme, we train a model
1057-7149 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2765830, IEEE
Transactions on Image Processing
7
epoch0 5 10 15 20dynamic weights
00.020.040.060.080.1,p
,l
,e
epoch0 5 10 15 20loss
00.511.522.533.5
idd
pos
illum
exp
epoch0 5 10 15 20dynamic weights
00.20.40.60.81,d
,g
,p
,l
,e
epoch0 5 10 15 20loss
00.511.522.533.5
idd
idg
pos
illum
exp
(a)
m-CNN: dynamic weights (b) m-CNN: losses (c) p-CNN: dynamic weights (d) p-CNN: losses
Fig. 6. The learnt dynamic weights and the losses of each task for m-CNN and p-CNN models during the training process.
TABLE III
MULTI -PIE PERFORMANCE COMPARISON ON SETTING IIIOFTABLE I.
153045607590a
vg.
Fisher
Vector [43] 93:30 87 :21 80 :33 68 :71 45 :51 24 :53 66 :60
FIP20[65] 95:88
89:23 78 :89 61 :64 47 :32 34 :13 67 :87
FIP40[65] 96:30
92:98 85 :54 69 :75 49 :10 31 :37 70 :90
c-CNN [54] 95:64 92 :66 85 :09 70 :49 55 :64 41 :71 73 :54
c-CNN Forest [54] 96:97 94 :05 89 :02 74 :38 60 :66 47 :26 76 :89
s-CNN
(ours) 98:41 96 :89 85 :18 88 :71 82 :80 76 :72 88 :45
m-CNN (ours) 99:02 97 :40 89 :15 89 :75 84 :97 76 :72 90 :08
p-CNN (ours) 99:19 98:01 90:34 92:07 87:83 76:96 91:27
T
ABLE IV
MULTI -PIE PERFORMANCE COMPARISON ON SETTING VOFTABLE I.
0153045607590a
vg.[ 60;60] avg.[ 90;90]
FIP
[65] 94:3 90:7 80 :7 64 :1 45 :9 ‚Äì ‚Äì 72:9 ‚Äì
Zhu et al. [66] 95:7 92:8 83 :7 72 :9 60 :1 ‚Äì ‚Äì 79:3 ‚Äì
Yim et al. [57] 99:5 95:0 88 :5 79 :9 61 :9 ‚Äì ‚Äì 83:3 ‚Äì
DR-GAN [51] 97:0 94:0 90 :1 86 :2 83 :2 ‚Äì ‚Äì 89:2 ‚Äì
FF-GAN [58] 95:7 94:6 92 :5 89 :7 85 :2 77 :2 61 :2 91:6 85:2
s-CNN
(ours) 95:9 95:1 92 :8 91 :6 88 :9 84 :9 78 :6 92:5 89:2
m-CNN (ours) 95:4 94:5 92 :6 91 :8 88 :4 85 :3 82 :2 92:2 89:6
p-CNN (ours) 95:4 95:2 94 :3 93 :0 90 :3 87 :5 83 :9 93:5 91:1
with
Ô¨Åxed loss weights for the side tasks as: p=l=e=
's=3 = 0:033. The summation of the loss weights for all side
tasks are equal to 'sfor all m-CNN variants in Table II for a
fair comparison.
Comparing the rank-1 identiÔ¨Åcation rates of s-CNN and m-
CNNs, it is obvious that adding the side tasks is always helpful
for the main task. The improvement of face recognition is
mostly on the proÔ¨Åle faces with MTL. The m-CNN ‚Äúid+all‚Äù
with dynamic weights shows superior performance to others
not only in rank-1 identiÔ¨Åcation rate, but also in the side
task estimations. Further, the lower rank-1 identiÔ¨Åcation rate
of ‚Äúid+all‚Äù w.r.t ‚Äúid+pos‚Äù indicates that more side tasks do
not necessarily lead to better performance without properly
setting the loss weights. In contrast, the proposed dynamic-
weighting scheme effectively improves the performance to
79:35% from the Ô¨Åxed weighting of 77:59%. As will be
shown in Section IV-B, the side tasks in m-CNN help to
inject PIE variations into the shared representation, similar to
a regularization term. For example, an L2 regularization will
encourage small weights. We add L2 regularization on the
shared representation to s-CNN (‚Äúid+L2‚Äù), which improves
over s-CNN without regularization. However it is still much
worse than the proposed m-CNN.Third, we train p-CNN by adding the PDB to m-CNN
‚Äúid+all‚Äù with dynamic weights. The loss weights are 'm= 1
for the main tasks and 's= 0:1for the side tasks. The pro-
posed dynamic-weighting scheme allocates the loss weights
to both two main tasks and three side tasks. P-CNN further
improves the rank-1 identiÔ¨Åcation rate to 79:55%.
Dynamic-Weighting Scheme Figure 6 shows the dynamic
weights and losses during training for m-CNN and p-CNN.
For m-CNN, the expression classiÔ¨Åcation task has the largest
weight in the Ô¨Årst epoch because it has the highest chance to be
correct with random guess with the least number of classes.
As training goes on, pose classiÔ¨Åcation takes over because
it is the easiest task (highest accuracy in s-CNN) and also
the most helpful for face recognition (compare ‚Äúid+pos‚Äù to
‚Äúid+exp‚Äù and ‚Äúid+illum‚Äù). pstarts to decrease at the 11th
epoch when pose classiÔ¨Åcation is saturated. The increased
landelead to a reduction in the losses of expression
and illumination classiÔ¨Åcations. As we expected, the dynamic-
weighting scheme assigns a higher loss weight for the easiest
and/or the most helpful side task.
For p-CNN, the loss weights and losses for the side tasks
behave similarly to those of m-CNN. For the two main tasks,
the dynamic-weighting scheme assigns a higher loss weight to
1057-7149 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2765830, IEEE
Transactions on Image Processing
8
n0 100 200 300energy
051015
sd
spsl
se
50 100 150 20050
100
150
200250300
20 40 6010
20
30
50 100 150 20050
100
150
200
250300
-0.4-0.200.20.40.6
n100 150 200 250 300accuracy
0.720.740.760.780.8
xn
yd
(a)
(b) (c)
Fig. 7. Analysis on the effects of MTL: (a) the sorted energy vectors for all tasks; (b) visualization of the weight matrix Wallwhere the red box in the
top-left is a zoom-in view of the bottom-right; (c) the face recognition performance with varying feature dimensions.
the easier task at the moment. At the beginning, learning the
pose-speciÔ¨Åc identity features is an easier task than learning
the generic identity features. Therefore the loss weight gis
higher than d. As training goes on, dincreases as it has a
lower loss. Their losses reduce in a similar way, i.e., the error
reduction in one task will also contribute to the other.
Compare to Other Methods As shown in Table I, no
prior work uses the entire Multi-PIE for face recognition. To
compare with state of the art, we choose to use setting III and
V to evaluate our method since these are the most challenging
settings with more pose variation. The network structures and
parameter settings are kept the same as those of the full set
except that the outputs of the last fully connected layers are
changed according to the number of classes for each task. Only
pose and illumination are used as the side tasks.
The performance on setting III is shown in Table III. Our
s-CNN already outperforms c-CNN forest [54], which is an
ensemble of three c-CNN models. This is attributed to the deep
structure of CASIA-Net [56]. Moreover, m-CNN and p-CNN
further outperform s-CNN with signiÔ¨Åcant margins, especially
for non-frontal faces. We want to stress the improvement
margin between our method 91:27% and the prior work of
76:89% ‚Äî a relative error reduction of 62%.
The performance on setting V is shown in Table IV.
For fair comparison with FF-GAN [58], where the models
are Ô¨Ånetuned from pre-trained in-the-wild models, we also
Ô¨Ånetune s-CNN, m-CNN, p-CNN models from the pre-trained
models on CASIA-Webface for 10epochs. Our performance is
much better than previous work with a relative error reduction
of60%, especially on large-pose faces. The performance gap
between Table III / IV and II indicates the challenge of face
recognition under various expressions, which is less studied
than pose and illumination variations on Multi-PIE.
B. How does m-CNN work?
It is well known in both the computer vision and the
machine learning communities that learning multiple tasks
together allows each task to leverage each other and improves
the generalization ability of the model. For CNN-based MTL,
previous work [63] has found that CNN learns shared features
for facial landmark localization and attribute classiÔ¨Åcations,
e.g. smiling. This is understandable because the smiling at-tribute is related to landmark localization as it involves the
change of the mouth region. However in our case, it is not
obvious how the PIE estimations can share features with the
main task. On the contrary, it is more desirable if the learnt
identity features are disentangled from the PIE variations.
Indeed, as we will show later, the PIE estimations regularize
the CNN to learn PIE-invariant identity features.
We investigate why PIE estimations are helpful for face
recognition. The analysis is done on m-CNN model (‚Äúid+all‚Äù
with dynamic weights) in Table II. Recall that m-CNN learns
a shared embeding x2R3201. Four fully connected lay-
ers with weight matrices Wd
320200,Wp
32013,Wl
32019,
We
3206are connected to xto perform classiÔ¨Åcation of
each task (200 subjects, 13poses, 19illuminations, and 6
expressions). We analyze the importance of each dimension
inxto each task. Taking the main task as an example, we
calculate an energy vector sd2R3201whose element is
computed as:
sd
i=200X
j=1jWd
ijj: (13)
A higher value of sd
iindicates that the ith feature in xis
more important to the identity classiÔ¨Åcation task. The energy
vectors sp,sl,sefor all side tasks are computed similarly.
Each energy vector is sorted and shown in Figure 7 (a). For
each curve, we observe that the energy distributes unevenly
among all feature dimensions in x. Note that the indexes of
the feature dimension do not correspond among them since
each energy vector is sorted independently.
To compare how each feature in xcontributes to differ-
ent tasks, we concatenate the weight matrix of all tasks as
Wall
320238 = [Wd;Wp;Wl;We]and compute its energy
vector as sall. We sort the rows in Wallbased on the
descending order in energy and visualize the sorted Wallin
Figure 7 (b). The Ô¨Årst 200columns represent the sorted Wd
where most energy is distributed in the Ô¨Årst 280 feature
dimensions (rows), which are more crucial for face recognition
and less important for PIE classiÔ¨Åcations. We observe that x
are learnt to allocate a separate set of dimensions/features for
each task, as shown in the block-wise effect in the zoom-in
view. Each block shows the most essential features with high
energy for PIE classiÔ¨Åcations respectively.
1057-7149 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2765830, IEEE
Transactions on Image Processing
9
-90¬∞-75¬∞-60¬∞-45¬∞-30¬∞-15¬∞0¬∞15¬∞30¬∞45¬∞60¬∞75¬∞90¬∞percentage
00.050.10.150.20.250.30.350.40.45
(a) (b) (c) (d)
Fig. 10. (a) yaw angle distribution on CASIA-Webface; (b) average image of the right proÔ¨Åle group; (c) average image of the frontal group; (d) average
image of the left proÔ¨Åle group.
TABLE V
PERFORMANCE COMPARISON ON LFW DATASET .
Method #Net Training Set Metric Accuracy Std (% )
DeepID2 [45] 1 202;599 images of 10;177 subjects, private Joint-Bayes 95:43
DeepFace [49] 1 4:4M images of 4;030 subjects, private cosine 95:920:29
CASIANet [56] 1 494;414 images of 10;575 subjects, public cosine 96:130:30
Wang et al. [52] 1 404;992 images of 10;553 subjects, public Joint-Bayes 96:20:9
Littwin and Wolf [32] 1 404;992 images of 10;553 subjects, public Joint-Bayes 98:140:19
MultiBatch [48] 1 2:6M images of 12K subjects, private Euclidean 98:20
VGG-DeepFace [38] 1 2:6M images of 2;622 subjects, public Euclidean 98:95
Wen et al. [53] 1 0:7M images of 17;189 subjects, public cosine 99:28
FaceNet [41] 1 260 M images of 8M subjects, private L2 99:63 0:09
s-CNN (ours) 1 494;414 images of 10;575 subjects, public cosine 97:870:70
m-CNN (ours) 1 494;414 images of 10;575 subjects, public cosine 98:070:57
p-CNN (ours) 1 494;414 images of 10;575 subjects, public cosine 98:270:64
epoch0 5 10 15 20energy mean
0510152025id
pos
illum
exp
epoch0 5 10 15 20energy std
012345id
pos
illum
exp
(a) mean (b) std.
Fig. 8. The mean and standard deviation of each energy vector during the
training process.
n0 100 200 300energy
05101520
sd
spsl
se
n0 100 200 300energy
05101520sd
spsl
se
(a)'s= 0:2 (b)'s= 0:3
Fig. 9. Energy vectors of m-CNN models with different overall loss weights.
Based on the above observation, we conclude that the PIE
classiÔ¨Åcation side tasks help to inject PIE variations into the
shared features x. The weight matrix in the fully connected
layer learns to select identity features and ignore the PIE
features for PIE-invariant face recognition. To validate this
observation quantitatively, we compare two types of featuresfor face recognition: 1) xn:a subset of xwithnlargest
energies in sd, which are more crucial in modeling identity
variation; 2) yd
2001=Wd
n200|xn1+bd, which is the
multiplication of the corresponding subset of Wdandxn.
We varynfrom 100 to320 and compute the rank-1 face
identiÔ¨Åcation rate on the entire Multi-PIE testing set. The
performance is shown in Figure 7 (c). When xnis used, the
performance improves with increasing dimensions and drops
when additional dimensions are included, which are learnt to
model the PIE variations. In contrary, the identity features
ydcan eliminate the dimensions that are not helpful for
identity classiÔ¨Åcation through the weight matrix Wd, resulting
in continuously improved performance w.r.t. n.
We further analyze how the energy vectors evolve over time
during training. SpeciÔ¨Åcally, at each epoch, we compute the
energy vectors for each task. Then we compute the mean and
standard deviation of each energy vector, as shown in Figure 8.
Despite some local Ô¨Çuctuations, the overall trend is that the
mean is decreasing and standard deviation is increasing as
training goes on. This is because in the early stage of training,
the energy vectors are more evenly distributed among all
feature dimensions, which leads to the higher mean values
and lower standard deviations. In the later stage of training,
the energy vectors are shaped in a way to focus on some key
dimensions for each task, which leads to the lower mean values
and higher standard deviations.
The CNN learns to allocate a separate set of dimensions
in the shared features to each task. The total number of
dimensions assigned to each task depends on the loss weights.
Recall that we obtain the overall loss weight for the side
1057-7149 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2765830, IEEE
Transactions on Image Processing
10
tasks as's= 0:1 via brute-force search. Figure 9 shows
the energy distributions with 's= 0:2and's= 0:3, which
are compared to Figure 7 (a) where 's= 0:1. We have two
observations. First, a larger loss weight for the side tasks
leads to more dimensions being assigned to the side tasks.
Second, the energies in sdincrease in order to compensate the
fact that the dimensions assigned to the main task decrease.
Therefore, we conclude that the loss weights control the energy
distribution between different tasks.
C. Unconstrained Face Recognition
Experimental Settings We use CASIA-Webface [56] as our
training set and evaluate on LFW [20], CFP [42], and IJB-
A [26] datasets. CASIA-Webface consists of 494;414images
of10;575 subjects. LFW consists of 10folders each with
300 same-person pairs and 300 different-person pairs. Given
the saturated performance of LFW mainly due to its mostly
frontal view faces, CFP and IJB-A are introduced for large-
pose face recognition. CFP is composed of 500subjects with
10frontal and 4proÔ¨Åle images for each subject. Similar to
LFW, CFP includes 10folders, each with 350 same-person
pairs and 350 different-person pairs, for both frontal-frontal
(FF) and frontal-proÔ¨Åle (FP) veriÔ¨Åcation protocols. IJB-A
dataset includes 5;396 images and 20;412 video frames of
500subjects. It deÔ¨Ånes template-to-template matching for both
face veriÔ¨Åcation and identiÔ¨Åcation.
In order to apply the proposed m-CNN and p-CNN, we need
to have the labels for the side tasks. However, it is not easy to
manually label our training set. Instead, we only consider pose
estimation as the side task and use the estimated pose as the
label for training. We use PIFA [24] to estimate 34landmarks
and the yaw angle, which deÔ¨Ånes three groups: right proÔ¨Åle
[ 90; 30), frontal [ 30;30], and left proÔ¨Åle (30;90].
Figure 10 shows the distribution of the yaw angle estimation
and the average image of each pose group. CASIA-Webface
is biased towards frontal faces with 88% faces belonging to
the frontal pose group based on our pose estimation.
The network structures are similar to those experiments on
Multi-PIE. All models are trained from scratch for 15epochs
with a batch size of 8. The initial learning rate is set to 0:01 and
reduced at the 10th and 14th epoch with a factor of 0:1. The
other parameter settings and training process are the same as
those on Multi-PIE. We use the same pre-processing as in [56]
to align a face image. Each image is horizontally Ô¨Çipped for
data augmentation in the training set. We also generate the
mirror image of an input face in the testing stage. We use the
average cosine distance of all four comparisons between the
image pair and its mirror images for face recognition.
Performance on LFW Table V compares our face veri-
Ô¨Åcation performance with state-of-the-art methods on LFW
dataset. We follow the unrestricted with labeled outside data
protocol. Although it is well-known that an ensemble of
multiple networks can improve the performance [46], [47], we
only compare CNN-based methods with one network for fair
comparison. Our implementation of the CASIA-Net (s-CNN)
with BN achieves much better results compared to the original
performance [56]. Even with such a high baseline, m-CNNand p-CNN can still improve, achieving comparable results
with state of the art, or better results if comparing to those
methods trained with the same amount of data. Since LFW is
biased towards frontal faces, we expect the improvement of
our proposed m-CNN and p-CNN to the baseline s-CNN to
be larger if they are tested on cross-pose face veriÔ¨Åcation.
Performance on CFP Table VI shows our face veriÔ¨Åcation
performance comparison with state-of-the-art methods on CFP
dataset. For FF setting, m-CNN and p-CNN improve the
veriÔ¨Åcation rate of s-CNN slightly. This is expected, as there
is little pose variation. For FP setting, p-CNN substantially
outperforms s-CNN and prior work, reaching close-to-human
performance (94:57%). Note our accuracy of 94:39% is9%
relative error reduction of the previous state of the art [39] with
93:76%. Therefore, the proposed divide-and-conquer scheme
is very effective for in-the-wild face veriÔ¨Åcation with large
pose variation. And the proposed stochastic routing scheme
improves the robustness of the algorithm. Even with the
estimated pose serving as the ground truth pose label for MTL,
the models can still disentangle the pose variation from the
learnt identity features for pose-invariant face veriÔ¨Åcation.
Performance on IJB-A We conduct close-set face iden-
tiÔ¨Åcation and face veriÔ¨Åcation on IJB-A dataset. First, we
retrain our models after removing 26overlapped subjects
between CASIA-Webface and IJB-A. Second, we Ô¨Åne-tune
the retrained models on the IJB-A training set of each fold
for50epochs. Similar to [52], we separate all images into
‚Äúwell-aligned‚Äù and ‚Äúpoorly-aligned‚Äù faces based on the face
alignment results and the provided annotations. In the testing
stage, we only select images from the ‚Äúwell-aligned‚Äù faces for
recognition. If all images in a template are ‚Äúpoorly-aligned‚Äù
faces, we select the best aligned face among them. Table VII
shows the performance comparison on IJB-A. Similarly, we
only compare to the methods with a single model. The
proposed p-CNN achieves comparable performance in both
face veriÔ¨Åcation and identiÔ¨Åcation.
V. C ONCLUSIONS
This paper explores multi-task learning for face recognition
with PIE estimations as the side tasks. To solve the problem of
balancing each task in MTL, we propose a dynamic-weighting
scheme to automatically assign the loss weights to each side
task during the training process. This scheme is shown to as-
sign a larger loss weight to an easier side task and/or the most
helpful side task. We also propose a pose-directed multi-task
CNN to learn pose-speciÔ¨Åc identity features during training
and a stochastic routing scheme for feature fusion in the testing
stage. A comprehensive study on the entire Multi-PIE dataset
has shown the effectiveness of the proposed approach for PIE-
invariant face recognition. An in-depth weight matrix analysis
has shown why PIE estimations can help face recognition to
learn a disentangled representation.
The proposed method is applicable to in-the-wild datasets
with estimated pose serving as the label for training. However,
we do not see large improvement on LFW and IJB-A as that
on Multi-PIE. This may due to several factors. First, both m-
CNN and p-CNN rely on the pose estimation, which is limited
1057-7149 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2765830, IEEE
Transactions on Image Processing
11
TABLE VI
PERFORMANCE COMPARISON ON CFP DATASET . RESULTS REPORTED ARE THE AVERAGE STANDARD DEVIATION OVER THE 10FOLDS .
Method# Frontal-Frontal Frontal-ProÔ¨Åle
Metric (%)! Accuracy EER AUC Accuracy EER AUC
Sengupta et al. [42] 96:400:69 3:480:67 99:430:31 84:911:82 14:971:98 93:001:55
Sankarana. et al. [40] 96:930:61 2:510:81 99:680:16 89:172:35 8:850:99 97:000:53
Chen, et al. [9] 98:670:36 1:400:37 99:900:09 91:971:70 8:001:68 97:700:82
DR-GAN [51] 97:840:79 2:220:09 99:720:02 93:411:17 6:450:16 97:960:06
Peng, et al. [39] 98:67 ‚Äì ‚Äì 93:76 ‚Äì ‚Äì
Human 96:240:67 5:341:79 98:191:13 94:571:10 5:021:07 98:920:46
s-CNN (ours) 97:340:99 2:490:09 99:690:02 90:961:31 8:790:17 96:900:08
m-CNN (ours) 97:770:39 2:310:06 99:690:02 91:391:28 8:800:17 97:040:08
p-CNN (ours) 97:790:40 2:480:07 99:710:02 94:391:17 5:940:11 98:360:05
TABLE VII
PERFORMANCE COMPARISON ON IJB-A.
Method# VeriÔ¨Åcation IdentiÔ¨Åcation
Metric (% )! @FAR=0:01 @FAR=0:001 @Rank-1 @Rank-5
OpenBR [26] 23:60:9 10:41:4 24:61:1 37:50:8
GOTS [26] 40:61:4 19:80:8 44:32:1 59:52:0
Wang et al. [52] 72:93:5 51:06:1 82:22:3 93:11:4
PAM [34] 73:31:8 55:23:2 77:11:6 88:70:9
DR-GAN [51] 77:42:7 53:94:3 85:51:5 94:71:1
DCNN [8] 78:74:3 ‚Äì 85:21:8 93:71:0
s-CNN (ours) 75:63:5 52:07:0 84:31:3 93:00:9
m-CNN (ours) 75:62:8 51:64:5 84:71:0 93:40:7
p-CNN (ours) 77:52:5 53:94:2 85:81:4 93:80:9
by the state-of-the-art pose estimation methods. Second, a
large training set might diminishes the beneÔ¨Åts of multi-
task learning for unconstrained face recognition. Third, both
LFW and IJB-A have large variations other than pose such as
expression, blurring, etc. that cannot be well handled by the
proposed method. Nevertheless, for dataset like CFP where
pose variation is the major variation, we achieve state-of-the-
art performance on the frontal-to-proÔ¨Åle veriÔ¨Åcation protocol.
REFERENCES
[1] A. H. Abdulnabi, G. Wang, J. Lu, and K. Jia. Multi-task CNN model
for attribute prediction. TMM, 2015.
[2] G. Andrew, R. Arora, J. Bilmes, and K. Livescu. Deep canonical
correlation analysis. In ICML, 2013.
[3] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature
learning. Machine Learning, 2008.
[4] A. Asthana, T. K. Marks, M. J. Jones, K. H. Tieu, and M. Rohith. Fully
automatic pose-invariant face recognition via 3D pose normalization. In
ICCV, 2011.
[5] V . Blanz and T. Vetter. A morphable model for the synthesis of 3D
faces. SIGGRAPH, 1999.
[6] D. Chen, X. Cao, F. Wen, and J. Sun. Blessing of dimensionality: High-
dimensional feature and its efÔ¨Åcient compression for face veriÔ¨Åcation.
InCVPR, 2013.
[7] D. Chen, S. Ren, Y . Wei, X. Cao, and J. Sun. Joint cascade face detection
and alignment. In ECCV, 2014.
[8] J.-C. Chen, V . M. Patel, and R. Chellappa. Unconstrained face veriÔ¨Åca-
tion using deep cnn features. In WACV, 2016.
[9] J.-C. Chen, J. Zheng, V . M. Patel, and R. Chellappa. Fisher vector
encoded deep convolutional features for unconstrained face veriÔ¨Åcation.
InICIP, 2016.
[10] B. Chu, S. Romdhani, and L. Chen. 3D-aided face recognition robust
to expression and pose variations. In CVPR, 2014.
[11] C. Ding and D. Tao. A comprehensive survey on pose-invariant face
recognition. ACM Transactions on intelligent systems and technology
(TIST), 2016.
[12] C. Ding, C. Xu, and D. Tao. Multi-task pose-invariant face recognition.
TIP, 2015.[13] L. El Shafey, C. McCool, R. Wallace, and S. Marcel. A scalable
formulation of probabilistic linear discriminant analysis: Applied to face
recognition. TPAMI, 2013.
[14] H. Fei and J. Huan. Structured feature selection and task relationship
inference for multi-task learning. Knowledge and information systems,
2013.
[15] P. Gong, J. Zhou, W. Fan, and J. Ye. EfÔ¨Åcient multi-task feature learning
with calibration. In ICKDDM, 2014.
[16] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker. Multi-PIE.
IVC, 2010.
[17] H. Han, S. Shan, X. Chen, S. Lao, and W. Gao. Separability oriented
preprocessing for illumination-insensitive face recognition. In ECCV,
2012.
[18] H. Han, S. Shan, L. Qing, X. Chen, and W. Gao. Lighting aware
preprocessing for face recognition across varying illumination. In ECCV,
2010.
[19] T. Hassner, S. Harel, E. Paz, and R. Enbar. Effective face frontalization
in unconstrained images. In CVPR, 2015.
[20] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled faces
in the wild: A database for studying face recognition in unconstrained
environments. Technical report, Technical Report 07-49, University of
Massachusetts, Amherst, 2007.
[21] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network
training by reducing internal covariate shift. arXiv preprint:1502.03167,
2015.
[22] S. Ji and J. Ye. An accelerated gradient method for trace norm
minimization. In ICML, 2009.
[23] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for
fast feature embedding. In ACMMM, 2014.
[24] A. Jourabloo and X. Liu. Large-pose face alignment via CNN-based
dense 3D model Ô¨Åtting. In CVPR, 2016.
[25] M. Kan, S. Shan, H. Chang, and X. Chen. Stacked progressive auto-
encoders (SPAE) for face recognition across poses. In CVPR, 2014.
[26] B. F. Klare, B. Klein, E. Taborsky, A. Blanton, J. Cheney, K. Allen,
P. Grother, A. Mah, M. Burge, and A. K. Jain. Pushing the frontiers of
unconstrained face detection and recognition: IARPA Janus Benchmark
A. In CVPR, 2015.
[27] A. Li, S. Shan, X. Chen, and W. Gao. Maximizing intra-individual
correlations for face recognition across pose differences. In CVPR, 2009.
[28] S. Li, X. Liu, X. Chai, H. Zhang, S. Lao, and S. Shan. Morphable
displacement Ô¨Åeld based image matching for face recognition across
1057-7149 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2765830, IEEE
Transactions on Image Processing
12
pose. In ECCV, 2012.
[29] Y . Li, B. Zhang, S. Shan, X. Chen, and W. Gao. Bagging based efÔ¨Åcient
kernel Ô¨Åsher discriminant analysis for face recognition. In ICPR, 2006.
[30] K. Lin, J. Xu, I. M. Baytas, S. Ji, and J. Zhou. Multi-task feature
interaction learning. In ICKDDM, 2016.
[31] K. Lin and J. Zhou. Interactive multi-task relationship learning. In
ICDM, 2016.
[32] E. Littwin and L. Wolf. The multiverse loss for robust transfer learning.
InCVPR, 2016.
[33] W. Liu, Y . Wen, Z. Yu, M. Li, B. Raj, and L. Song. Sphereface: Deep
hypersphere embedding for face recognition. In CVPR, 2017.
[34] I. Masi, S. Rawls, G. Medioni, and P. Natarajan. Pose-aware face
recognition in the wild. In CVPR, 2016.
[35] I. Misra, A. Shrivastava, A. Gupta, and M. Hebert. Cross-stitch networks
for multi-task learning. In CVPR, 2016.
[36] V . Nair and G. E. Hinton. RectiÔ¨Åed linear units improve restricted
boltzmann machines. In ICML, 2010.
[37] G. Obozinski, B. Taskar, and M. Jordan. Multi-task feature selection.
Technical Report, Statistics Department, UC Berkeley, 2006.
[38] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In
BMVC, 2015.
[39] X. Peng, X. Yu, K. Sohn, D. N. Metaxas, and M. Chandraker.
Reconstruction-based disentanglement for pose-invariant face recogni-
tion. In ICCV, 2017.
[40] S. Sankaranarayanan, A. Alavi, C. Castillo, and R. Chellappa. Triplet
probabilistic embedding for face veriÔ¨Åcation and clustering. arXiv
preprint:1604.05417, 2016.
[41] F. Schroff, D. Kalenichenko, and J. Philbin. FaceNet: A uniÔ¨Åed
embedding for face recognition and clustering. In CVPR, 2015.
[42] S. Sengupta, J.-C. Chen, C. Castillo, V . M. Patel, R. Chellappa, and
D. W. Jacobs. Frontal to proÔ¨Åle face veriÔ¨Åcation in the wild. In WACV,
2016.
[43] K. Simonyan, O. M. Parkhi, A. Vedaldi, and A. Zisserman. Fisher vector
faces in the wild. In BMVC, 2013.
[44] K. Sohn. Improved deep metric learning with multi-class n-pair loss
objective. In NIPS, 2016.
[45] Y . Sun, Y . Chen, X. Wang, and X. Tang. Deep learning face represen-
tation by joint identiÔ¨Åcation-veriÔ¨Åcation. In NIPS, 2014.
[46] Y . Sun, D. Liang, X. Wang, and X. Tang. DeepID3: Face recognition
with very deep neural networks. arXiv preprint:1502.00873, 2015.
[47] Y . Sun, X. Wang, and X. Tang. Deeply learned face representations are
sparse, selective, and robust. In CVPR, 2015.
[48] O. Tadmor, Y . Wexler, T. Rosenwein, S. Shalev-Shwartz, and
A. Shashua. Learning a metric embedding for face recognition using
the multibatch method. In NIPS, 2016.
[49] Y . Taigman, M. Yang, M. Ranzato, and L. Wolf. DeepFace: Closing the
gap to human-level performance in face veriÔ¨Åcation. In CVPR, 2014.
[50] Y . Tian, P. Luo, X. Wang, and X. Tang. Pedestrian detection aided by
deep learning semantic tasks. In CVPR, 2015.
[51] L. Tran, X. Yin, and X. Liu. Disentangled representation learning GAN
for pose-invariant face recognition. In CVPR, 2017.
[52] D. Wang, C. Otto, and A. K. Jain. Face search at scale. TPAMI, 2016.
[53] Y . Wen, K. Zhang, Z. Li, and Y . Qiao. A discriminative feature learning
approach for deep face recognition. In ECCV, 2016.
[54] C. Xiong, X. Zhao, D. Tang, K. Jayashree, S. Yan, and T.-K. Kim.
Conditional convolutional neural network for modality-aware face recog-
nition. In ICCV, 2015.
[55] M. Yang, L. Gool, and L. Zhang. Sparse variation dictionary learning
for face recognition with a single training sample per person. In ICCV,
2013.
[56] D. Yi, Z. Lei, S. Liao, and S. Z. Li. Learning face representation from
scratch. arXiv preprint:1411.7923, 2014.
[57] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim. Rotating your
face using multi-task deep neural network. In CVPR, 2015.
[58] X. Yin, X. Yu, K. Sohn, X. Liu, and M. Chandraker. Towards large-pose
face frontalization in the wild. In ICCV, 2017.
[59] C. Zhang and Z. Zhang. Improving multiview face detection with multi-
task deep convolutional neural networks. In WACV, 2014.
[60] H. Zhang, Y . Zhang, and T. S. Huang. Pose-robust face recognition via
sparse representation. Pattern Recognition, 2013.
[61] Y . Zhang and D.-Y . Yeung. A convex formulation for learning task
relationships in multi-task learning. arXiv preprint:1203.3536, 2012.
[62] Z. Zhang, P. Luo, C. C. Loy, and X. Tang. Facial landmark detection
by deep multi-task learning. In ECCV, 2014.
[63] Z. Zhang, P. Luo, C. C. Loy, and X. Tang. Learning deep representation
for face alignment with auxiliary attributes. TPAMI, 2016.
[64] X. Zhu, Z. Lei, J. Yan, D. Yi, and S. Z. Li. High-Ô¨Ådelity pose and
expression normalization for face recognition in the wild. In CVPR,
2015.
[65] Z. Zhu, P. Luo, X. Wang, and X. Tang. Deep learning identity-preserving
face space. In ICCV, 2013.[66] Z. Zhu, P. Luo, X. Wang, and X. Tang. Multi-view perceptron: A deep
model for learning face identity and view representations. In NIPS,
2014.
Xi Yin received the B.S. degree in Electronic and
Information Science from Wuhan University, China,
in 2013. Since August 2013, she has been work-
ing toward her Ph.D. degree in the Department of
Computer Science and Engineering, Michigan State
University, USA. Her research area are computer
vision, deep learning, and image processing. Her pa-
per on multi-leaf segmentation won the Best Student
Paper Award at Winter Conference on Application
of Computer Vision (WACV) 2014.
Xiaoming Liu is an Assistant Professor at the
Department of Computer Science and Engineering
of Michigan State University. He received the Ph.D.
degree in Electrical and Computer Engineering from
Carnegie Mellon University in 2004. Before joining
MSU in Fall 2012, he was a research scientist at
General Electric (GE) Global Research. His research
interests include computer vision, machine learning,
and biometrics. As a co-author, he is a recipient
of Best Industry Related Paper Award runner-up at
ICPR 2014, Best Student Paper Award at WACV
2012 and 2014, and Best Poster Award at BMVC 2015. He has been the
Area Chair for numerous conferences, including FG, ICPR, WACV , ICIP,
and CVPR. He is the program chair of WACV 2018. He is an Associate
Editor of Neurocomputing journal. He has authored more than 100 scientiÔ¨Åc
publications, and has Ô¨Åled 22 U.S. patents.
"
https://ieeexplore.ieee.org/document/7006757,"980 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 3, MARCH 2015
Multi-task Pose-Invariant Face Recognition
Changxing Ding, Student Member, IEEE , Chang Xu, and Dacheng Tao, Fellow, IEEE
Abstract ‚Äî Face images captured in unconstrained
environments usually contain signiÔ¨Åcant pose variation,which dramatically degrades the performance of algorithmsdesigned to recognize frontal faces. This paper proposes a novelface identiÔ¨Åcation framework capable of handling the full rangeof pose variations within ¬±90¬∞ of yaw. The proposed framework
Ô¨Årst transforms the original pose-invariant face recognitionproblem into a partial frontal face recognition problem. A robustpatch-based face representation scheme is then developed torepresent the synthesized partial frontal faces. For each patch,a transformation dictionary is learnt under the proposed multi-task learning scheme. The transformation dictionary transformsthe features of different poses into a discriminative subspace.Finally, face matching is performed at patch level rather thanat the holistic level. Extensive and systematic experimentationon FERET, CMU-PIE, and Multi-PIE databases shows thatthe proposed method consistently outperforms single-task-basedbaselines as well as state-of-the-art methods for the poseproblem. We further extend the proposed algorithm for theunconstrained face veriÔ¨Åcation problem and achieve top-levelperformance on the challenging LFW data set.
Index Terms ‚Äî Pose-invariant face recognition, partial face
recognition, multi-task learning.
I. I NTRODUCTION
FACE recognition has been one of the most active research
topics in computer vision for more than three decades.
With years of effort, promising results have been achievedfor automatic face recognition, in both controlled [1] and
uncontrolled environments [2], [3]. However, face recogni-
tion remains signiÔ¨Åcantly aff ected by the wide variations of
pose, illumination, and expressi on often encountered in real-
world images. The pose problem in particular is still largely
unsolved, as argued in a recent work [4]. In this paper,
we mainly handle the identiÔ¨Åcation problem of matching an
arbitrary pose probe face with frontal gallery faces, which is
Manuscript received August 7, 2014; revised October 27, 2014; accepted
December 28, 2014. Date of publication January 12, 2015; date of current
version January 30, 2015. This work was supported by the Australian
Research Council under Grant DP-120103730, Grant DP-140102164,Grant FT-130101457, and Grant LP-140100569. The associate editor coor-dinating the review of this manuscript and approving it for publication was
Prof. Shiguang Shan. Corresponding author: Dacheng Tao.
C. Ding and D. Tao are with the Centre for Quantum Computation and
Intelligent Systems, Faculty of Engineering and Information Technology,
University of Technology at Sydney, Sydney, NSW 2007, Australia (e-mail:
changxing.ding@student.uts.edu. au; dacheng.tao@uts.edu.au).
C. Xu was with the Center of Quantum Computation and Intelligent
Systems, Faculty of Engineering and Information Technology, University of
Technology at Sydney, Sydney, NSW 2007, Australia. He is now with the
Key Laboratory of Machine Perception, School of Electronics Engineering and
Computer Science, Ministry of Education, Peking University, Beijing 100871,
China (e-mail: xuchang@pku.edu.cn).
Color versions of one or more of the Ô¨Ågures in this paper are available
online at http://ieeexplore.ieee.org.
Digital Object IdentiÔ¨Åer 10.1109/TIP.2015.2390959
Fig. 1. (a) The rigid rotation of the h ead results in self-occlusion as well
as nonlinear facial texture deformati on. (b) The pose problem is combined
with other factors, e.g., variations in expression and illumination, to affectface recognition.
the most common setting for both the research and application
of pose-invariant face recognition (PIFR) [4]‚Äì[8]. At the end
of the paper, we brieÔ¨Çy extend the proposed approach to solve
the unconstrained face veriÔ¨Åcation problem [9].
Pose variation induces drama tic appearance change in the
face image. Essentially, thi s is caused by the complex 3D
geometrical structure of the human head. As shown in Fig. 1,
the rigid rotation of the head results in self-occlusion, whichmeans that some facial texture will be invisible with variations
in pose. Even the shape and position in the image of visible
facial texture vary nonlinearly from pose to pose. The pose
problem is also usually combined with other factors, such
as variations in illumination and expression, to affect theappearance of face images. In consequence, the extent of
appearance change caused by pose variation is usually greater
than that caused by differences in identity, and the performanceof frontal face recognition algorithms degrades dramatically
when the images to be matched feature different poses.
Directly matching faces in different poses is difÔ¨Å-
cult. One intuitive solution is to conduct face synthesis
so that the two facial imag es can be compared in the
same pose. Most approaches fo llowing this idea are dedi-
cated to recovering complete frontal faces from non-frontal
faces [4], [8], [10]‚Äì[12]. However, synthesizing the entirefrontal face from proÔ¨Åle faces is difÔ¨Åcult since most facial
texture is invisible as a result of occlusion. Therefore, the
aforementioned methods tend to constrain their recognition
ability within ¬±45¬∞ of yaw variation.
Inspired by the observation that human beings can easily
recognize proÔ¨Åle faces without th e need to elaborately recover
the whole frontal face, we present a novel face representation
approach that makes full use of just the facial texture thatis occlusion-free. This representation approach is general in
1057-7149 ¬© 2015 I EEE. Personal u se is perm itted, but republication/redistri bution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
DING et al. : MULTI-TASK POSE-INV ARIANT FACE RECOGNITION 981
Fig. 2. Overview of the proposed PBPR-MtFTL framework for pose-invariant
face recognition, as applied to the recognition of arbitrary pose probe faces.
nature, which means that it applies continuously to the full
range of pose variations between ‚àí90¬∞ and +90¬∞ of yaw.
Using simple pose normalization and pre-processing opera-
tions, our approach converts the original PIFR problem into
a partial face recognition task [13], [14], in which the faceis represented by the unoccluded facial parts in a patch-based
fashion. The proposed face representation scheme is therefore
named Patch-based Partial Representation (PBPR).
Feature transformation enhances recognition ability by
transforming the features from the gallery and probe images
to a common discriminative subspace. Based on this intu-
ition, we propose a learning method called Multi-Task
Feature Transformation Learning (MtFTL). By considering thecorrelation between the transformation matrices for different
poses, MtFTL consistently achieves better performance than
its single-task based counterparts. Its advantage is particularlyevident when the size of training data is limited. The trans-
formation matrices learnt by MtFTL are highly compact as a
result of sharing most projection vectors across poses, which
additionally reduces memory cost.
We term the entire proposed framework for tackling the
pose problem PBPR-MtFTL. Under this framework, matching
an arbitrary pose probe face and frontal gallery faces involves
transformation of the extracted PBPR representation usingthe learnt MtFTL transformation dictionaries, followed by
patch-level cosine distance computation and score fusion,
as illustrated in Fig. 2. Extensive experiments on FERET,CMU-PIE, and Multi-PIE datasets indicate that superior per-
formance is consistently achieved with PBPR-MtFTL.
The remainder of the paper is organized as follows:
Section II brieÔ¨Çy reviews related works for PIFR and multi-
task learning. The proposed PBPR face representation schemeis illustrated in Section III. The m ulti-task feature transfor-
mation learning approach MtFT L is described in Section IV .
Face matching using PBPR-MtFTL is introduced in Section V .Experimental results are presented in Section VI, leading to
conclusions in Section VII.
II. R
ELATED STUDIES
A. Pose-Invariant Face Recognition
Many promising approaches have been proposed to tackle
the pose challenge in face recognition [15]. These methods can
be broadly classiÔ¨Åed into two categories: face image synthesis-
based methods and synthesis-free methods.
Most previous works rely on face image synthesis,
Face image synthesis can be accomplished with 2D or3D techniques. Using 2D techniques, Ashraf et al. [16] learnt
patch-wise warps between two i mages with the Lucas-Kanade
optimization algorithm. Chai et al. [11] proposed the
locally linear regression method for frontal face synthesis.
Liet al. [17] proved that more accurate face synthesis can
be achieved by imposing lasso or ridge regularization on
the regression function. Ho et al. [8] proposed synthesizing
the virtual frontal view using Markov Random Fields and avariant of belief propagation algorithm. Li et al. [18] proposed
the Morphable Displacement Field method for frontal face
synthesis and achieved pixel-level semantic correspondencebetween a face pair. Other 2D face synthesis methods can
be found in related facial analysis topics, e.g., face hal-
lucination [19], [20]. Methods based on 3D face models
have also been introduced because pose variation is essen-
tially caused by the 3D rigid transformation of the face.Blanz et al. [5] developed the 3D Morphable Model (3DMM)
which can be used to Ô¨Åt a 2D face image in arbitrary
poses. Face recognition can be conducted by rendering aspeciÔ¨Åed view with the 3D model or directly matching the
regression coefÔ¨Åcents; however, the optimization of 3DMM is
computationally expensive. In [21], an efÔ¨Åcient 3D modeling
method called Generic Elastic Models (GEM) was introduced.
By assuming that the depth information between individualsis not highly discriminative, GEM estimates the 3D shape
of a 2D frontal face by directly assigning generic depth
information. Arbitrary pos e face images can be rendered with
GEM for matching; however, GEM can only estimate the
3D shape for frontal faces. Asthana et al. [22] proposed
synthesizing the frontal face image by aligning and mapping
the texture of a 2D face image to a 3D generic shape model.
This method can handle pose variations within ¬±45¬∞ of yaw.
The synthesis-free approaches aim to extract pose-robust
features or to transform features from different poses into a
shared subspace. For example, [12], [23] proposed learningpose-insensitive face representations with neural networks.
Wright et al. [24] represented the face image as a histogram
of quantized patch descriptors which are expanded with theirspatial locations. Yi et al. [25] proposed the extraction of pose
adaptive features by transfor ming Gabor Ô¨Ålers according to
the pose of the face image. Of the feature transformation-
based methods, Li et al. [26] proposed employing Canonical
Correlation Analysis to project image patches under twodifferent views to a shared subspace, where the patch similarity
can be measured by the correlation of their projections. Similar
methods incorporate Partial Least Squares (PLS) [27] and
982 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 3, MARCH 2015
Generalized Multiview Analysis [28]. Recently, Kan et al. [29]
propose an efÔ¨Åcient algorithm that simultaneously learns pose-
speciÔ¨Åc transformations to a c ommon discriminative space.
This approach enables conveni ent matching between two faces
with arbitrary poses. Other methods to learn robust representa-
tions or feature transformations can be found in related areas,
e.g., scene classiÔ¨Åcation [30] and image denoising [31].
The two categories of method described above are closely
related. For example, it is shown in [12] and [23] that pose-
invariant features can be utilized for frontal-face reconstruc-
tion. It is also demonstrated that the coefÔ¨Åcients of regression
models for face synthesis [4], [5], [11], [17] can be regardedas pose-insensitive features for face matching. Our work is
related to both categories, and there is clear novelty: First, the
proposed PBPR method represents arbitrary pose face imagesfrom the perspective of partial face recognition. Second, the
MtFTL approach learns compact feature transformation for
various poses based on the principle of multi-task learning,
which is novel for PIFR. Third, the PBPR-MtFTL framework
continuously tackles the full range of pose variations from‚àí90¬∞ to +90¬∞ of yaw and obtains strong performance. In com-
parison, the recognition ability of existing methods is typically
restricted to a range of ¬±45¬∞ of yaw [4], [32].
B. Multi-Task Learning
Multi-task learning (MTL) is a machine learning technique
that learns several tasks simultaneously for better performance
by capturing the intrinsic correla tion between different tasks.
MTL implicitly increases the sample size and improves the
generalization ability for each task; hence, it is especially
beneÔ¨Åcial when the training data for the tasks is small.
While MTL has been widely applied to computer vision
tasks, e.g., visual tracking [33], action recognition [34]‚Äì[36],
and face recognition [37], [38], it is new for PIFR. Existing
approaches for PIFR ignore the correlation between the feature
transformations of different poses [26], [28]. To the best ofour knowledge, MTL for PIFR is only brieÔ¨Çy mentioned
in [39] but no detailed information is provided, and multi-view
reconstruction is targeted rather than feature transformationlearning. Nevertheless, MTL provides a principled way for us
to model the correlation between poses if we view the learning
of feature transformation for each pose as a task. MtFTL is
arguably the Ô¨Årst MTL approach that jointly learns feature
transformations for different poses and is shown to proÔ¨Åt fromthe latent inter-pose correlations.
III. F
ACE REPRESENTATION FOR THE POSE PROBLEM
Existing face representation methods tend to extract Ô¨Åxed-
length features from face images, with the underlying assump-tion that all facial components are visible in the image [13].
However, as shown in Fig. 2, this hypothesis does not hold
for a proÔ¨Åle face where there is severe self-occlusion. In this
section, we propose the Ô¨Çexible PBPR face representation
scheme, where the length of face representation is related tothe pose of the face; for example, a frontal face image will
have larger face representatio n than a proÔ¨Åle face image. This
is reasonable, since the proÔ¨Åle face provides less information
Fig. 3. Overview of the proposed PBPR face representation method. PBPR is
applied to arbitrary pose face images. The Ô¨Ånal PBPR representation is a set
of patch-level DCP features af ter dimension reduction by PCA.
for recognition. As shown in Fig. 3, PBPR is essentially
composed of three steps: face pose normalization, unoccluded
facial texture detection, and p atch-wise feature extraction.
In this section, we describe the three main components in
detail.
A. Face Pose Normalization
A standard 3D method is adopted for face pose nor-
malization [22]. The Ô¨Åve most stable facial feature points,
i.e., the centers of both eyes, the tip of the nose, and the twomouth corners, are Ô¨Årst detected automatically or manually.
For proÔ¨Åle faces (as shown in Fig. 2), the coordinates of
the occluded facial feature poi nts are estimated. Using the
orthographic projection model [40] and the detected Ô¨Åve facial
feature points, a 3D generic shape model is aligned to the 2D
face image.
1The 2D face image is then back-projected to
the 3D model, and a frontal face image is rendered with the
textured 3D model.
P r e v i o u sw o r k sr e l yo nd e n se facial feature points,
e.g., 68 points in [22] and 79 points in [4], for accurate pose
normalization. However, detecting dense facial feature pointsfor proÔ¨Åle faces is difÔ¨Åcult due to the severe self-occlusion of
the face, which in turn restricts the range of poses that these
methods can handle. In stead, only the Ô¨Åve most stable facial
feature points are utilized for pose normalization in this paper.
This greatly facilitates the realization of a fully automaticface recognition system and extends the range of poses that
can be processed. Although using sparse facial feature points
will result in larger normalization error, we highlight the
1In this step, we roughly estimate the pose of the 2D face image by the
method described in [41].
DING et al. : MULTI-TASK POSE-INV ARIANT FACE RECOGNITION 983
Fig. 4. Illustration of facial contour detection. (a) The 3D generic shape
model is projected to the 2D plane and its facial contour is detected;
(b) the region containing the facial contour of the 2D face image is estimated;
(c) candidate facial contour points; (d ) facial contour obtained by point set
registration.
power of the proposed PBPR-MtFTL framework given its low
normalization requirements.
B. Unoccluded Facial Texture Detection
Pose normalization corrects the deformation of facial texture
resulting from pose variations, but it cannot recover the texture
lost by occlusion. Rather than trying to synthesize the occludedtexture to obtain a complete frontal face [4], we propose to
make full use of the unoccluded texture only. This is inspired
by the observation that human beings can easily recognizeproÔ¨Åle faces without the need to recover the whole frontal
face. As shown in Fig. 3, the main boundary between the
occluded and unoccluded facial texture is the facial contour.
Therefore, facial contour detection is the key to identifying
the occluded facial texture.
Although there are off-the-shelf face alignment tools for
facial contour detection, they return only sparse facial contour
points and may not be reliable enough to severe occlusion,expression, and pose variations. We propose a much simpler
but effective method that makes use of the 3D generic shape
model. After aligning the 3D model and the 2D face image,
it can be projected to the 2D image plane roughly in the pose
of the 2D face. As shown in Fig. 4(a), the contour of the3D model can be easily detected. Based on the contour of the
3D model, the facial contour search of the 2D face can be
constrained within a certain region, as illustrated in Fig. 4(b).The edge points are then detected in this region by the Canny
operator [42]. To reduce imposters, only the edge points with
horizontal gradient directions are saved, with the prior that
facial contour extends in a vertical direction. Lastly, the facial
contour is obtained by a point sets registration algorithm calledCoherent Point Drift (CPD) [43]. BrieÔ¨Çy, CPD iteratively
aligns the facial contour highlighted in Fig. 4(a) to the edge
point set shown in Fig. 4(c) with afÔ¨Åne transformations. Theimposter contour points in Fig. 4(c) can gradually be detected
and ignored. The obtained facial contour is shown in Fig. 4(d).
More detection examples on unconstrained face images in theLFW dataset [9] are shown in Fig. 5.
Alongside the projection of facial texture in the Ô¨Årst step,
the detected facial contour poi nts are Ô¨Årst projected to the
3D model and then projected to the rendered frontal face
image. Since the head is approximately an ellipsoid, thefacial contour points in the frontal view are Ô¨Åtted with an
arc. As shown in Fig. 3, the arc effectively separates the
unoccluded and occluded texture in the rendered frontal image.
Fig. 5. Examples of facial contour det ection for unconstrained face images
in the LFW dataset.
In the following subsection, face representation is built using
only the detected unoccluded facial texture.
C. Patch-Based Face Representation
The area of the unoccluded facial texture in the rendered
frontal view varies with pose change, with demonstrable
Ô¨Çuctuation in the amount of effective information available forface recognition. In light of this observation, a variable-length
face representation method is proposed.
As illustrated in Fig. 3, the normalized face image is
Ô¨Årst divided into M√óNoverlapped patches. The severity
of occlusion for each patch is then evaluated based on thedetected boundary between the occluded and unoccluded facial
texture. If more than 80% of pixels in one patch fall into
the unoccluded region, then it is designated as an unoccludedpatch; otherwise, the patch is ignored due to the large area of
occlusion. Next, each of the unoccluded patches is split into
J√óJcells. A state-of-the-art local descriptor called Dual-
Cross Patterns (DCP) [2] is employed for feature extraction.
The concatenated DCP histogram feature from the J
2cells
forms the raw feature of the patch. Following [2], elements
in the DCP histogram are normalized by square root. Lastly,
Principal Component Analysis (PCA) is applied to each patchto project its feature into a subspace with dimension D,b y
which the noise is suppressed.
The set of patch-level DCP features following PCA process-
ing from all unoccluded patches forms the representation of
the face image. Note that this representation method is generalin nature, meaning that it applie s to faces with arbitrary poses.
This is a valuable property, because we do not need to apply
different algorithms to frontal and non-frontal faces, unlikesome existing approaches [8].
IV . M
ULTI -TASK FEATURE TRANSFORMATION LEARNING
The previous section has introduced the PBPR face
representation scheme, whereby face recognition can be
984 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 3, MARCH 2015
Fig. 6. Pose normalization for non-frontal images. The boundary between
unoccluded and occluded facial texture is detected by the method illustratedin Fig. 3. (a) ‚àí90¬∞‚â§yaw‚â§‚àí45¬∞; (b) ‚àí30¬∞‚â§yaw‚â§+30¬∞; (c) +45¬∞‚â§
yaw‚â§+ 90¬∞. The image quality is degraded with the increase in value of
the yaw angles, and the amount of unoccluded facial texture for recognitiondecreases.
accomplished by directly matching corresponding patch fea-
tures of two face images. In this section, we further proposethe MtFTL approach for learning transformation dictionaries,
which enable the patch features of a frontal face and a non-
frontal face to be transformed i nto a common discriminative
space to enhance recognition ability. The learning process is
patch-wise, which means a separate transformation dictionary
is learnt by MtFTL for each patch. Consequently, we obtain
M√óNtransformations dictionaries. Details of the MtFTL
approach are illustrated below.
A. Feature Transformation Learning
Three aspects are considered in the design of feature trans-
formation learning. First, as shown in Fig. 6, the normalized
images from different poses are of different image quality,therefore there will be differences in the transformations for
different poses. Second, a strong correlation exists between
the feature transformations for different poses, since they
essentially process the data of the same subjects. Third, the
amount of training data might be limited in real scenarios,because collecting multi-pose face images tends to be difÔ¨Åcult.
Ideally, the shared knowledge from different poses should be
leveraged for robust transformation learning.
These considerations call for a multi-task strategy for fea-
ture transformation learning, in which the learning for each
pose type is regarded as a task. Therefore, we propose theMtFTL approach which takes into consideration both the
correlation and difference between tasks. Instead of learning
a separate transformation matrix for each task [44], MtFTL
learns a common transformation dictionary for all the tasks.
Differences between the tasks are reÔ¨Çected by the selection ofdifferent projection vectors in the transformation dictionary.
Hence, MtFTL learns more compact feature transformations
than previous approaches [44].Before presenting the formulation of the proposed model,
several necessary notations are introduced. Let Pbe the
number of tasks, i.e., the number of pose types that are
available in the training set for the current patch. The set
{(X
t,Yt):1‚â§t‚â§P}stores the training data composed of
intra-personal and inter-personal patch pairs. Xt‚ààRD√óNtp
and Yt‚àà RD√óNtn,w h e r e Ntpand Ntnare the number of
intra-personal and inter-personal patch pairs for the tth task,
respectively. The nth column of Xtis denoted as xn
tand
xn
t=xtn‚àíx0n,w h e r e {xtn,x0n}is one intra-personal patch
pair between the pose type tand the frontal pose. Similarly,
yn
tis the nth column of Ytand yn
t=ytn‚àíy0n,w h e r e
{ytn,y0n}is one inter-personal patch pair between the pose
type tand the frontal pose. We learn the transformation
dictionary U‚àà RD√óDshared by all tasks, and the set
of vectors Œ±t‚àà{0,1}D,1‚â§t‚â§P.Œ±tselects projec-
tion vectors for task tfrom the shared dictionary U.L e t
A‚ààRD√óPbe the matrix of stacked vectors Œ±t. In addition,
At‚ààRD√óDis a diagonal matrix expanded by Œ±t, denoted
asAt=diag(Œ±t).
The loss function for task tis denoted as TU,t. It is based
on the principle that the margi n between intra-personal patch
pairs and inter-personal patch pairs should be as large aspossible.
T
U,t(Œ±t)=1
Ntp/bardblAtUTXt/bardbl2
F‚àíŒª
Ntn/bardblAtUTYt/bardbl2
F, (1)
where Œªis a regularization paramet er that weights the intra-
personal and inter-personal terms. We then formulate themulti-task learning algorithm as the optimization problem:
min
U,A1
PP/summationdisplay
t=1TU,t(Œ±t)
s.t.UTU=I. (2)
For simplicity, the above problem is relaxed to
min
U,A1
PP/summationdisplay
t=1TU,t(Œ±t)+Œº/bardblUTU‚àíI/bardbl2
F, (3)
where Œºis another regularization parameter. We set the
number of non-zero elements in Œ±tasd, and aim to optimize
Œ±tto select dmost discriminative projection vectors from U
for the tth task. The optimal value of Œº,d,a n dŒªis estimated
through cross validation.
Note that the transformation dictionary Uis learnt jointly
for all Ptasks, which enables knowledge sharing between
the tasks. We show in the experiment section that knowledge
sharing is especially important when the amount of trainingdata for the tasks is limited.
B. Iterative Optimization Algorithm
The optimization problem (3) of MtFTL is convex in U
for Ô¨Åxed Aand in Afor Ô¨Åxed U. Therefore, we solve
this problem by alternately optimizing Uand A.T h eÔ¨Å n a l
learning algorithm is summarized in Algorithm 1. The main
optimization procedure can be outlined in two steps.
DING et al. : MULTI-TASK POSE-INV ARIANT FACE RECOGNITION 985
Algorithm 1 Multi-Task Feature Transformation Learning
Learning A: With Ô¨Åxed U, the optimization problems for
each task decouple. For the tth task, the optimal Œ±tis obtained:
min
Œ±t‚ààRDTU,t(Œ±t), (4)
TU,t(Œ±t)=tr/parenleftbigg1
NtpAtUTXtXT
tUA t‚àíŒª
NtnAtUTYtYT
tUA t/parenrightbigg
=tr/braceleftbigg
At/parenleftbigg1
NtpUTXtXT
tU‚àíŒª
NtnUTYtYT
tU/parenrightbigg
At/bracerightbigg
=Œ±T
tBtŒ±t, (5)
where tr(¬∑)represents the trace of a matrix; and Btis a
diagonal matrix by directly copying the diagonal elements
from the matrix1
NtpUTXtXT
tU‚àíŒª
NtnUTYtYT
tU. Since the
role of Œ±tis to select dmost discriminative projection vectors
for the tth pose type, the elements in Œ±tthat correspond to d
smallest diagonal elements in Btare set as 1 while the other
elements in Œ±tare set as 0.
Learning U: The shared transformation dictionary Ucou-
ples all the tasks. In this step, Uis updated efÔ¨Åciently via
the limited-memory BFGS (LBFGS) algorithm. The choice of
LBFGS algorithm here is due to both its high efÔ¨Åciency and
low memory requirement.
While Ais Ô¨Åxed, the optimization problem (3) reduces to
min
U‚ààRD√óDTA(U), (6)
TA(U)=1
PP/summationdisplay
t=1/parenleftbigg1
Ntp/bardblAtUTXt/bardbl2
F‚àíŒª
Ntn/bardblAtUTYt/bardbl2
F/parenrightbigg
+Œº/bardblUTU‚àíI/bardbl2
F. (7)
The derivative of TA(U)with respect to Uis
‚àÇTA(U)
‚àÇU=2
PP/summationdisplay
t=1/parenleftbigg1
NtpXtXT
tUA tAt‚àíŒª
NtnYtYT
tUA tAt/parenrightbigg
+4Œº/parenleftbig
UUTU‚àíU/parenrightbig
. (8)
With the provided formula for calculating TA(U)and
‚àÇTA(U)
‚àÇU, the optimization problem can be readily solved with
the LBFGS algorithm [45].
Initialization: The transformation matrix Uis simply ini-
tialized with a random matrix whose elements are drawn from
the standard uniform distribution on the open interval (0,1).In the experiment section, we show that even the randomly
initialized Uachieves promising performance.
Stopping criterion: The iterative optimization process stops
when the Frobenius norms of both /Delta1Uand/Delta1Aare below
/epsilon1=10‚àí3,w h e r e /Delta1Uand/Delta1Aare the difference matrices
between two successive iterations for Uand A, respectively.
C. Theoretical Analysis
In this subsection, we study the robustness and general-
ization error of the proposed MtFTL algorithm. The detailed
proof can be found in the Appendix. All through the theoretical
analysis, we consider the loss function for face patch feature
xŒ∏at non-frontal pose Œ∏as
/lscript(A,U,x,Œ∏)=/bardblAŒ∏UT(xŒ∏‚àíx0)/bardbl, (9)
whose maximum value is assumed to be B.
1) Robustness Analysis: If two corresponding face patch
features of two images are from the same subject, then their
associated losses are close. This property is formalized as‚Äúrobustness‚Äù in [46], and the precise deÔ¨Ånition is given below:
DeÔ¨Ånition 1: An algorithm Ais(K,/epsilon1(¬∑))robust, for K ‚ààN
and/epsilon1(¬∑):Z‚Üí R,i ft h es a m p l e Zcan be partitioned into K
disjoint sets, denoted as {C
i}K
i=1, so that the following holds
for all s‚ààZ, given the loss function /lscript(As,z)of the algorithm
Astrained on s:
‚àÄs‚ààs,‚àÄz‚ààZ,‚àÄi=1,¬∑¬∑¬∑,K:
if s,z‚ààCi,then|/lscript(As,s)‚àí/lscript(As,z)|‚â§/epsilon1(s).
Given two face patch features sand zof the same
subject from different poses Œ∏sandŒ∏z,i f/bardbls‚àíz/bardbl‚â§ Œ≥
and|Œ∏s‚àíŒ∏z|‚â§/Delta1Œ∏, we suggest that these two face patch
features are close. We assume that for one subject, the
difference between any of its non-frontal face patch feature
xŒ∏and its frontal face patch feature x0can be bounded by
/bardblxŒ∏‚àíx0/bardbl‚â§Œ≥0. Since matrix AŒ∏at pose Œ∏is a sparse diagonal
matrix that has dnon-zero elements, we have /bardblAŒ∏/bardbl‚â§‚àö
d.
Also, we restrict /bardblAŒ∏1‚àíAŒ∏2/bardbl‚â§/Omega1/Delta1Œ∏for any two matrices
AŒ∏1andAŒ∏2at different poses. A face recognition algorithm is
said to be robust if the corresponding face patch features from
images of the same subject have close losses. This robustness
can be measured by the following theorem.
Theorem 1: Example z is in space Z‚äÇRD,w h i c hc a nb e
partitioned into K disjoint sets and denoted as {Ci=1}K
i=1.
Given the algorithm A{A,U:z‚Üí Rd}, we have for
anys‚äÇZ,
|/lscript(As,z)‚àí/lscript(As,s)|‚â§‚àö
dŒ≥+/Omega1/Delta1Œ∏Œ≥0
‚àÄi,j=1,¬∑¬∑¬∑,K:s‚ààCiand z ‚ààCj.
Hence Ais(K,‚àö
dŒ≥+/Omega1/Delta1Œ∏Œ≥0)-robust.
Robustness is a fundamental property which ensures that
a learning algorithm performs well. Since the sparse diag-
onal matrices Asand Az, which select projection vectors
from the transformation dictionary U, are learned from
face patches of different poses, they cannot be identical.According to Theorem 1, it is instructive to suggest that
the robustness of the algorithm will be improved for the
face patches at close poses if their feature transformations
986 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 3, MARCH 2015
have more shared elements, that is, encouraging /Omega1/Delta1Œ∏to be
small.
2) Generalization Analysis: Based on the robustness analy-
sis, we show a PAC generalization bound for the algorithm,
i.e., the difference between the expected error L(As)and
the empirical error Lemp(As). We begin by presenting a
concentration inequality [47] that helps to derive the bound.
Proposition 1: Let (|N1|,¬∑¬∑¬∑,|NK|)be an IID multinomial
random variable with parameters n and (Œ≤(C1),¬∑¬∑¬∑,Œ≤(CK)).
By the Breteganolle-Huber-Carol inequality we have
Pr{/summationtextK
i=1|Ni
n‚àíŒ≤(Ci)|‚â•Œ∂}‚â§ 2Kexp(‚àínŒ∂2
2), hence with
probability at least 1‚àíŒ¥,
K/summationdisplay
i=1|Ni
n‚àíŒ≤(Ci)|‚â§/radicalbigg
2Kln 2+2l n(1/Œ¥)
n.
The generalization error bound is presented in the following
theorem.
Theorem 2: If the algorithm Ais(K,/epsilon1(¬∑))-robust and the
training sample sis composed of n examples {si}n
i=1,w h i c h
are generated from Œ≤, then for any Œ¥>0, with the probability
at least 1‚àíŒ¥we have,
|L(As)‚àíLemp(As)|‚â§/epsilon1(s)+B/radicalbigg
2Kln 2+2l n(1/Œ¥)
n.
By combining the results of Theorem 1 and Theorem 2, we
can easily illustrate the genera lization error of the proposed
algorithm. Exploiting the shared information of face patches
from different poses can strengthen the robustness of thealgorithm and then improve the generalization error.
V. F
ACE MATCHING WITHPBPR-MtFTL
In this section, the face matching problem is addressed
based on the proposed PBPR-MtFTL framework. It is assumed
that/braceleftbig/parenleftbigUi,Ai/parenrightbig:1‚â§i‚â§MN/bracerightbig, i.e., the set of patch-wise
transformation dictionaries an d selection matrices, has been
learnt by MtFTL.
Suppose we are matching a probe face image xtof
pose type tto a frontal gallery face image x0.I ti sa l s o
assumed that there are Kunoccluded patches for xt. Without
loss of generality, we denote the sets of features for the
Kpatches as {xt1,xt2,¬∑¬∑¬∑,xtK}and{x01,x02,¬∑¬∑¬∑,x0K}for
xtandx0, respectively. First, the features of each patch pair
{(xtk,x0k):1‚â§k‚â§K}are projected into the discriminative
space using the learnt Ukand Ak.
ÀÜxtk=Ak
t(Uk)Txtk
ÀÜx0k=Ak
t(Uk)Tx0k, (10)
where Ak
tis the diagonal matrix expanded by the tth column
ofAk. Then, the cosine metric is utilized to calculate the
similarity of each patch pair and the similarity scores of all
Kpatch pairs are fused by the sum rule.
s(xt,x0)=1
KK/summationdisplay
k=1ÀÜxT
tkÀÜx0k
/bardblÀÜxtk/bardbl/bardblÀÜx0k/bardbl, (11)
where sis the similarity score between the probe image xt
and the gallery image x0. Lastly, the nearest neighbor (NN)
classiÔ¨Åer is adopted for face identiÔ¨Åcation.Although the adopted matching scheme is simple compared
to existing methods [17], [26], it is still expected that the
proposed PBPR-MtFTL framework will achieve stronger per-
formance, since the recognition ability of PBPR-MtFTL hasbeen enhanced by exploiting the correlation between poses.
VI. E
XPERIMENTAL EVA L UAT I O N
In this section, extensive experiments are conduct to present
the effectiveness of PBPR-MtFTL. We mainly conduct iden-
tiÔ¨Åcation experiments on the three most popular databases
for the pose problem, i.e., CMU-PIE [48], FERET [49], andMulti-PIE [50]. These experiments are to recognize a subject
across pose variations with a single enrolled frontal face
image. At the end of this section, we slightly modify theproposed framework to deal with the unconstrained face veri-
Ô¨Åcation problem, and conduct experiments on the challenging
LFW dataset [9].
The CMU-PIE [48] and FERET [49] datasets incorporate
multi-pose images of 68 and 200 subjects, respectively. Forthe two databases, we adopt the same protocols as previous
works [7], [18] that exclude both illumination and expression
variations [51]. The Multi-PIE [50] database contains imagesof 337 subjects, each of which is captured in up to four
recording sessions. Images in each session cover 15 view
points and 20 illumination cond itions. As there is no uniÔ¨Åed
protocol for the pose problem on Multi-PIE, we adopt the three
most popular protocols in the literature [17], [22], [39].
Eight sets of experiments are conducted. First, the per-
formance of PBPR-MtFTL is brieÔ¨Çy compared with previ-
ous works for PIFR on CMU-PIE and FERET. Next, theMtFTL approach is compared with its single-task baselines
on Multi-PIE to justify the signiÔ¨Åcance of MTL for the pose
problem. Then, considering that the pose problem is often
combined with other factors, we evaluate the performance
of PBPR-MtFTL in three different settings, i.e., combinedvariations of pose and illumination, combined variations of
pose and recording session, and combined variations of pose,
illumination, and recording session. We also test the sensitivityof PBPR-MtFTL to the value of model parameters and face
alignment errors. Lastly, we slightly modify the proposed
approach to deal with the unconstrained face veriÔ¨Åcation prob-
lem and present experimental results on the LFW database.
All images in this paper are normalized as follows. The
mean shape of the Basel Face Model (BFM) [52] is adopted
as the 3D generic shape model. The Ô¨Åve facial feature points
are manually labeled in the Ô¨Årst six experiments and auto-matically detected in the last two experiments.
2After the
pose normalization step described in Section III, the face
images are cropped and resized to 156 √ó130 pixels, as shown
in Fig. 6. The patch size M√óNis set at 26 √ó24 pixels, with
50% overlap between nearby patches. The number of cells
J√óJwithin each patch is set at 2 √ó2. For the Ô¨Årst seven
experiments, images are further photometrically normalized
using a simple operator [1], with the two parameters œÉ1andœÉ2
2Coordinates of manually labeled facial feature points for Multi-PIE are
provided by Zhu et al. [12]; we use an off-the-shelf tool [53] for automaticfacial feature point detection.
DING et al. : MULTI-TASK POSE-INV ARIANT FACE RECOGNITION 987
TABLE I
MODEL PARAMETERS ESTIMATED ON THE VALIDA TION
SUBSETS FOR DIFFERENT DATABASES
set at 1.4 and 2.0. The two parameters RinandRexfor DCP are
set at[3,7]. For the last experiment, we omit the photometric
normalization step since it slightly degrades the performance
of PBPR-MtFTL on the View 1 data of LFW. Three-scale DCP
features are extracted and con catenated, with th e parameters
set at [2,4],[4,8],a n d[6,12], respectively.
For each identiÔ¨Åcation experime nt, the subjects in the train-
ing data are randomly divided into two subsets, one for model
training and the other for validation. The two subsets are of
equal size. The optimal values of model parameters Œº,d,a n dŒª
are estimated on the validation subset and applied to the
test data. For simplicity, the value of Œºacross the models
of all patches keeps consistent and this applies to dandŒª.
The random division of the training data is repeated Ô¨Åve
times, and the mean rank-1 identiÔ¨Åcation rates on the testdata are reported. The estimated model parameters for different
databases are tabulated in Table I, where the superscripts ‚Äú ‚àó
1‚Äù,
‚Äú‚àó2‚Äù, and ‚Äú ‚àó3‚Äù stand for one of the three protocols adopted
for the Multi-PIE database, respectively.
A. Comparison on CMU-PIE and FERET
All 68 CMU-PIE subjects with neutral expression and
normal illumination at 11 different poses are employed. Note
that pose type C31 and C25 are with hybrid yaw and pitchvariations. The 68 frontal images are utilized as gallery
images and all the rest are used as probes. Following previous
works [18], [57], we train the MtFTL model with randomlyselected 50 subjects in Multi-PIE database since there are
only 68 subjects in CMU-PIE. For FERET, all 200 subjects
at 9 different poses are incorporated. Images of the Ô¨Årst
100 subjects consist the training data and the rest 100 subjects
are used for testing.
As shown in Table II and III, the proposed PBPR-MtFTL
approach outperforms the other methods. But the advantage of
PBPR-MtFTL is not well exhib ited, because the performance
of existing methods has nearly reached the saturation point on
the two databases. Therefore, we focus on the larger and more
challenging Multi-PIE database in the following experiments.
B. Comparison With Single-Task Baselines
In this experiment, we aim to justify the importance of
MTL for the pose problem. The proposed MtFTL algorithm is
compared with three single-task baselines: (a) the Linear Dis-
criminative Analysis (LDA) a pproach, which learns a singleLDA model for all poses; (b) the single-task feature trans-
formation learning (StFTL) approach, which learns a single
feature transformation for all poses. StFTL is equal to the
Discriminative Locality Alignment (DLA) model [58]; (c) themultiple independent feature transformation learning (MiFTL)
approach, which independen tly learns a DLA model for each
pose. Unlike LDA and StFTL, MtFTL and MiFTL learn posespeciÔ¨Åc feature transformations. The main difference between
MtFTL and MiFTL is that MiFTL learns the transformation for
each pose independently, while MtFTL learns compact trans-
formations simultaneously and beneÔ¨Åts from the correlation of
different poses.
The protocol deÔ¨Åned in [17] is employed. This protocol
covers 249 subjects in Session 1, in which images with neutral
expression under 20 illumination conditions are involved. TheÔ¨Årst 100 subjects (Subject ID 001 to 100) are used for training
and the remaining 149 subjects (Subject ID 101 to 250) are
used for testing. The gallery set is composed of 149 frontal
images (Pose ID 051) with the illumination ID 07. The probe
sets cover 20 illumination conditions of the same subjects.In Fig. 7, we present the performance of the four algorithms
with varied size of training data on the three most challenging
poses. The number of subjects (S) utilized for model learningis gradually increased from 20 to the maximum number 50.
It is shown in Fig. 7 that MtFTL consistently outperforms
the baselines under all settings. SpeciÔ¨Åcally, MtFTL signiÔ¨Å-
cantly outperforms StFTL and LDA, which means that learn-
ing pose speciÔ¨Åc feature transformations is necessary. MtFTLoutperforms MiFTL while learni ng much more compact trans-
formations. This proves that MTL is helpful for enhancing
the ability to recognize non-frontal faces. The advantage ofMtFTL is more evident when the amount of training data is
limited, which indicates that knowledge sharing among related
tasks is important for better generalization ability.
C. Recognition Across Pose and Illumination
In this subsection, the performance of the PBPR-MtFTL
framework is compared with existing algorithms under the set-
ting of the combined variations of pose and illumination. The
adopted protocol is the same as the previous experiment [17].
The experimental results are shown in Table IV and Fig. 8.In general, face recognition across combined variations of
pose and illumination is a difÔ¨Åcult problem. However, it
is clear that the proposed method outperforms existingapproaches [12], [17] with a large margin, even though only
half training data is employed to train the MtFTL model.
It is worth noting that the algorithm proposed in [17]
also employs photometric normalization, and that all three
approaches employ manually labeled facial feature points.Notably, we employ exactly the same facial feature point
coordinates as the method followed in [12].
D. Recognition Across Pose and Recording Session
This experiment is to test the performance of algorithms
under the combined variations of pose and recording session.
The protocol described in [22] is followed. This protocol
covers all 337 subjects across the four recording sessions.
988 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 3, MARCH 2015
TABLE II
PERFORMANCE COMPARISON WITHSTATE -OF-THE-ARTPIFR M ETHODS ON CMU-PIE
TABLE III
PERFORMANCE COMPARISON WITHSTATE -OF-THE-ARTPIFR M ETHODS ON FERET
Fig. 7. Performance comparison of MtFTL and the three single-task baselines on the Multi-PIE database with varying numbers of training subjects.
(a) yaw =¬±90¬∞; (b) yaw =¬±75¬∞; (c) yaw =¬±60¬∞.
TABLE IV
RANK -1 I DENTIFICATION RATES ON COMBINED VARIATIONS OF POSE AND ILLUMINATION ON MULTI -PIE
Only images with neutral expression and frontal illumina-
tion are employed. Images of the Ô¨Årst 200 subjects (SubjectID 001 to 200) are used for training, and images of the
remaining 137 subjects (Subject ID 201 to 346) are employed
for testing. The frontal imag es from the earliest recording
sessions for the testing subjects are collected as the gallery
set (137 images in total). The non-frontal images of the
testing subjects construct fourteen probe sets. The comparisons
between our approach and the state-of-the-art methods are
presented in Table V and Fig. 9. We observe that:
1) In general, the performance of all the algorithms is
good when the pose value of the probe images is small.
While high performance is achieved by all methods onthe probe sets 130, 140, 050, and 041, our method
achieves perfect identiÔ¨Åcation rates on all four probesets.
2) There is a substantial drop in performance for existing
methods on the probe sets 080 and 190, where the yawangles are ¬±45¬∞. PBPR-MtFTL performs signiÔ¨Åcantly
better than the other methods on both probe sets,
indicating that it is more robust to large pose variations.
3) While most existing methods can only handle yaw
angle variations within [‚àí45¬∞,+45¬∞], the proposed
method can tackle the full range of yaw angle variation.
Fig. 9 shows that high performance is achieved even
when the yaw angle approaches ¬±75¬∞.
DING et al. : MULTI-TASK POSE-INV ARIANT FACE RECOGNITION 989
TABLE V
RANK -1 I DENTIFICATION RATES ON COMBINED VARIATIONS OF POSE AND RECORDING SESSION ON MULTI -PIE
Fig. 8. Performance comparison on combined variations of pose and illumi-
nation. The probe sets 081 and 191 are with hybrid yaw and pitch variations.
The other probe sets contain only yaw variations from ‚àí90¬∞ to +90¬∞.
Fig. 9. Performance comparison of different methods on combined variations
of pose and recording session.
E. Recognition Across Pose, Illumination,
and Recording Session
To examine the robustness of the proposed algorithm under
more challenging conditions, a new protocol speciÔ¨Åed in [39]
is employed. This protocol extends the original protocolFig. 10. Performance comparison of different methods on combined
variations of pose, illumination, and recording session.
designed in [22] by incorporating all 20 illumination types,
while the other settings remain the same. Therefore, the gallery
set is exactly the same as [22], while the number of probe
images is 20 times more than th at in [22]. The performance
of the proposed method, compared with the state-of-the-art
approaches, is presented in Table VI and Fig. 10. All methodsin Table VI employ manually labeled facial feature points.
We make the following observations:
1) PBPR-MtFTL signiÔ¨Åcantly outperforms the other three
approaches across all probe sets. This result isconsistent with those observed in the previous two
experiments.
2) Among the four approaches, PBPR-MtFTL is the only
one that can handle full range of pose variations, and
its performance degrades gracefully across wide pose
variations including ¬±60¬∞.
F . Parameter Evaluation for MtFTL
In the above experiments, the optimal value of model
parameters Œº,d,a n dŒªis estimated on the validation subsets.
In this experiment, the impact of their value on the perfor-mance of MtFTL is investigated. The same protocol [17] as the
second experiment is followed. Also, the rank-1 identiÔ¨Åcation
rates on the three most challenging poses are reported.
990 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 3, MARCH 2015
TABLE VI
RANK -1 I DENTIFICATION RATES ON COMBINED VARIATIONS OF POSE,ILLUMINATION ,AND RECORDING SESSION ON MULTI -PIE
Fig. 11. InÔ¨Çuence of the parameters Œº,d,a n dŒªto the performance of MtFTL. (a) evaluation against the value of Œºwhile dandŒªare set at 200 and 0.5,
respectively; (b) evaluation against the value of dwhile ŒºandŒªare set at 0.1 and 0.5, respectively; (c) evaluation against the value of Œªwhile Œºanddare
set at 0.1 and 200, respectively.
The performance of the MtFTL approach for different value
ofŒº,d,a n d Œªis shown in Fig. 11. Their optimal value is
around 0.1, 200, and 0.5, respectively. The experimental results
also indicate that the performance of MtFTL is robust to the
Ô¨Çuctuation of parameter value.
G. Performance in the Fully-Automatic Mode
The performance of the presented PBPR-MtFTL framework
is related to the accuracy of the facial feature detection and
pose estimation algorithms. The previous experiments aresemi-automatic (SA), i.e., the facial feature points are labeled
manually and it is assumed that the probe image poses are
known. In this experiment, the PBPR-MtFTL framework isrun in the fully-automatic (FA) mode. We leave the manu-
ally labeled facial feature points for the gallery and training
images intact. This is reasonabl e since the labeling work could
be conducted ofÔ¨Çine. For all probe images, the Ô¨Åve facial
feature points are automatica lly detected. Since existing face
alignment tools cannot reliably detect facial feature points for
proÔ¨Åle or half-proÔ¨Åle faces, we limit the yaw range of the
probe images to within ¬±45¬∞ in this experiment. For pose
estimation, we compare the unoccluded region of each probe
image with those of a set of training images whose poses are
known. The pose of the probe image is assigned to that of thetraining image whose unoccluded region is the most similar.
The same protocol [17] as used in the second experiment
is adopted. The performance of PBPR-MtFTL in the SA and
FA modes is compared in Fig. 12. There is a minor drop in
performance under the FA mode. In fact, the performance dropis mainly caused by the failure of face detection, whose failure
rates on the six probe sets are 3.66%, 1.95%, 1.21%, 1.51%,
1.98%, and 3.72%, respectively. Besides, when consideredFig. 12. Performance comparison of the proposed PBPR-MtFTL framework
in the SA and FA modes. In the FA mode, both facial feature point detection
and pose estimation are completely au tomatic. Note that the identiÔ¨Åcation
error in the FA mode incorporates the failure in face detection.
along with the results shown in Fig. 8, the performance of
PBPR-MtFTL in the FA mode is still considerably better thanthe state-of-the-art methods.
H. Extension to Unconstrained Face VeriÔ¨Åcation
In this experiment, we slightly modify the proposed
approach to tackle the unconstrained face veriÔ¨Åcationproblem, and present experimental results on the LFW
database [9].
In Section IV , we assume that the tth task of MtFTL
is to learn the feature transformation between the tth non-
frontal pose type and the frontal pose. As shown in Fig. 13,image pairs deÔ¨Åned in LFW m ay contain no frontal pose
image. Therefore, we add tasks in the model that learn the
feature transformation between every possible pair of poses.
DING et al. : MULTI-TASK POSE-INV ARIANT FACE RECOGNITION 991
Fig. 13. Many image pairs deÔ¨Åned in LFW contain no frontal faces. The Ô¨Årst
line shows the Ô¨Årst images in the image pairs, while the second line showsthe second images in the image pairs.
TABLE VII
P
ERFORMANCE COMPARISON ON LFW W ITHSTATE -OF-THE-ART
METHODS BASED ON SINGLE FACE REPRESENTATION
Another characteristic of LFW is that it has very few proÔ¨Åle
face images. To make sure that each pose type incorporates
sufÔ¨Åcient training data, we quantize the pose space into three
types, i.e., left proÔ¨Åle (LP, yaw< ‚àí10¬∞), frontal pose
(FP, ‚àí10¬∞ ‚â§ yaw‚â§+ 10¬∞), and right proÔ¨Åle
(RP, yaw> +10¬∞). Therefore, there are six tasks in total,
i.e., LP-LP, LP-FP, LP-RP, FP-FP, FP-RP, and RP-RP. Besides,
the weight of each task in Eq. 3 is modiÔ¨Åed to be proportionalto the number of training sam ples in each task, since the
training samples are far from balanced among the tasks.
For each pair of faces in LFW, both images are normalized
as described in Section III. Occlusion detection is conducted
for non-frontal face images. Feat ures are extracted only from
the patches that are un-occluded in both images. The features
are transformed with the learnt MtFTL model, as described
in Section V . Similarity scores between all un-occluded patchpairs are averaged as the similarity score of the face image
pair. Since MtFTL explicitly employs the image labels, the
proposed method falls in the paradigm of ‚ÄúUnrestricted, Label-Free Outside Data‚Äù [9]. We conduct performance comparison
with the state-of-the-arts in Table VII. To promote perfor-
mance, most existing methods designed for the LFW challenge
fuse multiple face representations, e.g., employing several
descriptors [60] or mirroring the face image [61]. In compar-ison, this work is not targeted at the LFW challenge and we
employ only a single face representation. For fair comparison,
we report the best performance of all approaches in Table VIIachieved with a single face representation.
3
3The performance of [61] is obtained using the code and data released by
the authors, while the performance of the other approaches is directly citedfrom the original papers.The Ô¨Årst Ô¨Åve approaches in Table VII adopt metric learn-
ing based classiÔ¨Åers, and PBPR-MtFTL achieves signiÔ¨Åcantly
better performance than the oth er approaches. Recently, gen-
erative model based classiÔ¨Åers have been introduced to theLFW challenge. We then replace the MtFTL model with
the Probabilistic Linear Discriminative Model (PLDA) [66].
The dimension of the PLDA subspace is set at 100.With generative model based classiÔ¨Åers, the high-dim LBP
approach [59] achieves a slightly higher accuracy than our
approach. However, this approach relies on dense facial feature
detection. We emphasize here that only the 5 most stable facial
feature points are required by our method. This makes ouralgorithm easier to use in practical applications.
VII. C
ONCLUSION
Face recognition across pose is a challenging task because
of the signiÔ¨Åcant appearance ch ange caused by pose variations.
We handle this problem from two aspects. First, we proposethe PBPR face representation scheme that makes use of
the unoccluded face textures only. PBPR can be applied to
face images in arbitrary pos e, which is a great advantage
over existing methods. Second, we present the MtFTL model
for learning compact feature transformations by utilizing thecorrelation between poses. Clear advantage is shown compared
to single-task based methods. To the best of our knowledge,
this is the Ô¨Årst time that MTL has been formally applied tothe PIFR problem. As the proposed PBPR-MtFTL framework
effectively utilizes all the uno ccluded face texture and the
correlation between different poses, very encouraging results
for face identiÔ¨Åcation in all th ree popular multi-pose databases
are achieved. We also slightly modify the proposed approach totackle the unconstrained face ve riÔ¨Åcation problem, and achieve
top level performance on the challenging LFW database.
A
PPENDIX A
PROOF OF THEOREM 1
Proof: We can partition Zinto Kdisjoint sets, so that if
two face patch features sandzare close, then
/bardbls‚àíz/bardbl‚â§Œ≥and |Œ∏s‚àíŒ∏z|‚â§/Delta1Œ∏. (12)
By arranging the loss functions so that the Ô¨Årst loss is always
larger than the second one, we therefore have
|/lscript(As,U,s)‚àí/lscript(Az,U,z)|
=/bardblAsUT(s‚àíx0)/bardbl‚àí/bardbl AzUT(z‚àíx0)/bardbl
=/bardblAsUT(s+z‚àíz‚àíx0)/bardbl‚àí/bardbl AzUT(z‚àíx0)/bardbl
‚â§/bardblAsUT(s‚àíz)/bardbl+/bardbl AsUT(z‚àíx0)/bardbl‚àí/bardbl AzUT(z‚àíx0)/bardbl
‚â§/bardblAsUT(s‚àíz)/bardbl+/bardbl(As‚àíAz+Az)UT(z‚àíx0)/bardbl
‚àí/bardblAzUT(z‚àíx0)/bardbl
‚â§/bardblAsUT(s‚àíz)/bardbl+/bardbl(As‚àíAz)UT(z‚àíx0)/bardbl
‚â§/bardblAs/bardbl/bardbls‚àíz/bardbl+/bardbl As‚àíAz/bardbl/bardblz‚àíx0/bardbl
‚â§‚àö
dŒ≥+/Omega1/Delta1Œ∏Œ≥0,
which completes the proof.

992 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 3, MARCH 2015
APPENDIX B
PROOF OF THEOREM 2
Proof: Let Nibe the set of index of points of sthat
fall into Ci.(|N1|,¬∑¬∑¬∑,|NK|)is an IID random variable with
parameters nand(Œ≤(C1),¬∑¬∑¬∑,Œ≤(CK)).W eh a v e
/vextendsingle/vextendsingleL(As)‚àíLemp(As)/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleK/summationdisplay
i=1Ez‚àºŒ≤(/lscript(As,z)|z‚ààCi)Œ≤(Ci)‚àí1
nn/summationdisplay
i=1/lscript(As,si)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
‚â§/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleK/summationdisplay
i=1Ez‚àºŒ≤(/lscript(As,z)|z‚ààCi)Nj
n‚àí1
nn/summationdisplay
i=1/lscript(As,si)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleK/summationdisplay
i=1Ez‚àºŒ≤(/lscript(As,z)|z‚ààCi)Œ≤(Ci)
‚àíK/summationdisplay
i=1Ez‚àºŒ≤(/lscript(As,z)|z‚ààCi)Nj
n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
‚â§/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nK/summationdisplay
i=1/summationdisplay
j‚ààNimax
z‚ààCi|/lscript(As,sj)‚àí/lscript(As,z)|/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglemax
z‚ààZ|/lscript(As,z)|K/summationdisplay
i=1||Ni|
n‚àíŒ≤(Ci)|/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
‚â§/epsilon1(s)+BK/summationdisplay
i=1|Ni
n‚àíŒ≤(Ci)|
‚â§/epsilon1(s)+B/radicalbigg
2Kln 2+2l n(1/Œ¥)
n.
The Ô¨Årst inequality is due to the triangle inequality, and
the second inequality is because of/summationtextK
i=1Œ≤(Ci)=1a n d/summationtextK
i=1Nj
n=1. Finally, the last inequality is the application
of Proposition 1.
ACKNOWLEDGMENT
The authors would like to thank the associate editor
Prof. Shiguang Shan and the three anonymous reviewers
for their careful reading and va luable remarks, which have
contributed to improving the quality of the paper. The authorswould also like to thank Prof. Thomas Vetter in the Department
of Computer Science, the University of Basel to provide the
BFM model.
R
EFERENCES
[1] X. Tan and B. Triggs, ‚ÄúEnhanced local texture feature sets for face
recognition under difÔ¨Åcult lighting conditions,‚Äù IEEE Trans. Image
Process. , vol. 19, no. 6, pp. 1635‚Äì1650, Jun. 2010.
[2] C. Ding, J. Choi, D. Tao, and L. S. Davis. (2014). ‚ÄúMulti-directional
multi-level dual-cross patterns for robust face recognition.‚Äù [Online].
Available: http://arxiv.org/abs/1401.5311
[3] M. Gunther et al. , ‚ÄúThe 2013 face recognition evaluation in mobile
environment,‚Äù in Proc. IEEE/IAPR Int. Conf. Biometrics , Jun. 2013,
pp. 1‚Äì7.
[4] R. Abiantun, U. Prabhu, and M. Savvi des, ‚ÄúSparse feature extraction for
pose-tolerant face recognition,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. ,
vol. 36, no. 10, pp. 2061‚Äì2073, Oct. 2014.
[5] V . Blanz and T. Vetter, ‚ÄúFace recognition based on Ô¨Åtting a 3D
morphable model,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 25,
no. 9, pp. 1063‚Äì1074, Sep. 2003.
[6] S. J. D. Prince, J. Warrell, J. H. Elder, and F. M. Felisberti, ‚ÄúTied factor
analysis for face recognition across large pose differences,‚Äù IEEE Trans.
Pattern Anal. Mach. Intell. , vol. 30, no. 6, pp. 970‚Äì984, Jun. 2008.[7] S. R. Arashloo and J. Kittler, ‚ÄúEnergy normalization for pose-invariant
face recognition based on MRF model image matching,‚Äù IEEE Trans.
Pattern Anal. Mach. Intell. , vol. 33, no. 6, pp. 1274‚Äì1280, Jun. 2011.
[8] H. T. Ho and R. Chellappa, ‚ÄúPose-invariant face recognition using
Markov random Ô¨Åelds,‚Äù IEEE Trans. Image Process. , vol. 22, no. 4,
pp. 1573‚Äì1584, Apr. 2013.
[9] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller,
‚ÄúLabeled faces in the wild: A database for studying face
recognition in unconstrained environments,‚Äù Dept. Comput. Sci.,Univ. Massachusetts Amherst, Am herst, MA, USA, Tech. Rep. 07-49,
Oct. 2007.
[10] V . Blanz, P. Grother, P. J. Phillips, and T. Vetter, ‚ÄúFace recognition based
on frontal views generated from non-frontal images,‚Äù in Proc. IEEE
Comput. Soc. Conf. Comput. Vis. Pattern Recognit. , vol. 2. Jun. 2005,
pp. 454‚Äì461.
[11] X. Chai, S. Shan, X. Chen, and W. Gao, ‚ÄúLocally linear regression for
pose-invariant face recognition,‚Äù IEEE Trans. Image Process. , vol. 16,
no. 7, pp. 1716‚Äì1725, Jul. 2007.
[12] Z. Zhu, P. Luo, X. Wang, and X. Tang, ‚ÄúDeep learning identity-
preserving face space,‚Äù in Proc. IEEE Int. Conf. Comput. Vis. , Dec. 2013,
pp. 113‚Äì120.
[13] S. Liao, A. K. Jain, and S. Z. Li, ‚ÄúPartial face recognition: Alignment-
free approach,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 35, no. 5,
pp. 1193‚Äì1205, May 2013.
[14] R. Weng, J. Lu, J. Hu, G. Yang, and Y .-P. Tan, ‚ÄúRobust feature set
matching for partial face recognition,‚Äù in Proc. IEEE Int. Conf. Comput.
Vis., Dec. 2013, pp. 601‚Äì608.
[15] X. Zhang and Y . Gao, ‚ÄúFace recognition across pose: A review,‚Äù Pattern
Recognit. , vol. 42, no. 11, pp. 2876‚Äì2896, 2009.
[16] A. B. Ashraf, S. Lucey, and T. Ch en, ‚ÄúLearning patch correspondences
for improved viewpoint invariant face recognition,‚Äù in Proc. IEEE Conf.
Comput. Vis. Pattern Recognit. , Jun. 2008, pp. 1‚Äì8.
[17] A. Li, S. Shan, and W. Gao, ‚ÄúCoupled bias‚Äìvariance tradeoff for cross-
pose face recognition,‚Äù IEEE Trans. Image Process. , vol. 21, no. 1,
pp. 305‚Äì315, Jan. 2012.
[18] S. Li, X. Liu, X. Chai, H. Zhang, S. Lao, and S. Shan, ‚ÄúMorphable
displacement Ô¨Åeld based image mat ching for face recognition across
pose,‚Äù in Proc. 12th Eur. Conf. Comput. Vis. , 2012, pp. 102‚Äì115.
[19] N. Wang, D. Tao, X. Gao, X. Li, and J. Li, ‚ÄúA comprehensive survey
to face hallucination,‚Äù Int. J. Comput. Vis. , vol. 106, no. 1, pp. 9‚Äì30,
2014.
[20] N. Wang, D. Tao, X. Gao, X. Li, and J. Li, ‚ÄúTransductive face sketch-
photo synthesis,‚Äù IEEE Trans. Neural Netw. Learn. Syst. , vol. 24, no. 9,
pp. 1364‚Äì1376, Sep. 2013.
[21] U. Prabhu, J. Heo, and M. Savvides, ‚ÄúUnconstrained pose-invariant face
recognition using 3D generic elastic models,‚Äù IEEE Trans. Pattern Anal.
Mach. Intell. , vol. 33, no. 10, pp. 1952‚Äì1961, Oct. 2011.
[22] A. Asthana, T. K. Marks, M. J. Jones, K. H. Tieu, and M. Rohith, ‚ÄúFully
automatic pose-invariant face recognition via 3D pose normalization,‚Äù
inProc. IEEE Int. Conf. Comput. Vis. , Nov. 2011, pp. 937‚Äì944.
[23] Y . Zhang, M. Shao, E. K. Wong, and Y . Fu, ‚ÄúRandom faces guided sparse
many-to-one encoder for pose-invariant face recognition,‚Äù in Proc. IEEE
Int. Conf. Comput. Vis. , Dec. 2013, pp. 2416‚Äì2423.
[24] J. Wright and G. Hua, ‚ÄúImplicit elastic matching with random projec-
tions for pose-variant face recognition,‚Äù in Proc. IEEE Conf. Comput.
Vis. Pattern Recognit. , Jun. 2009, pp. 1502‚Äì1509.
[25] D. Yi, Z. Lei, and S. Z. Li, ‚ÄúTowards pose robust face recogni-
tion,‚Äù in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , Jun. 2013,
pp. 3539‚Äì3545.
[26] A. Li, S. Shan, X. Chen, and W. Ga o, ‚ÄúMaximizing intra-individual
correlations for face recognition across pose differences,‚Äù in Proc. IEEE
Conf. Comput. Vis. Pattern Recognit. , Jun. 2009, pp. 605‚Äì611.
[27] A. Sharma and D. W. Jacobs, ‚ÄúBypassing synthesis: PLS for face
recognition with pose, low-resolution and sketch,‚Äù in Proc. IEEE Conf.
Comput. Vis. Pattern Recognit. , Jun. 2011, pp. 593‚Äì600.
[28] A. Sharma, A. Kumar, H. Daume, and D. W. Jacobs, ‚ÄúGeneralized
multiview analysis: A discriminative latent space,‚Äù in Proc. IEEE Conf.
Comput. Vis. Pattern Recognit. , Jun. 2012, pp. 2160‚Äì2167.
[29] M. Kan, S. Shan, H. Zhang, S. Lao, and X. Chen, ‚ÄúMulti-view
discriminant analysis,‚Äù in Proc. 12th Eur. Conf. Comput. Vis. , 2012,
pp. 808‚Äì821.
[30] L. Zhang, X. Zhen, and L. Shao, ‚ÄúLearning object-to-class kernels
for scene classiÔ¨Åcation,‚Äù IEEE Trans. Image Process. , vol. 23, no. 8,
pp. 3241‚Äì3253, Aug. 2014.
[31] R. Yan, L. Shao, and Y . Liu, ‚ÄúNonlo cal hierarchical dictionary learning
using wavelets for image denoising,‚Äù IEEE Trans. Image Process. ,
vol. 22, no. 12, pp. 4689‚Äì4698, Dec. 2013.
DING et al. : MULTI-TASK POSE-INV ARIANT FACE RECOGNITION 993
[32] M. Kan, S. Shan, H. Chang, and X. Chen, ‚ÄúStacked progressive auto-
encoders (SPAE) for face recognition across poses,‚Äù in Proc. IEEE Conf.
Comput. Vis. Pattern Recognit. , Jun. 2014, pp. 1883‚Äì1890.
[33] Z. Hong, X. Mei, D. Prokhorov, and D. Tao, ‚ÄúTracking via robust multi-
task multi-view joint sparse representation,‚Äù in Proc. IEEE Int. Conf.
Comput. Vis. , Dec. 2013, pp. 649‚Äì656.
[34] B. Mahasseni and S. Todorovic, ‚ÄúLatent multitask learning for view-
invariant action recognition,‚Äù in Proc. IEEE Int. Conf. Comput. Vis. ,
Dec. 2013, pp. 3128‚Äì3135.
[35] L. Liu, L. Shao, and P. Rockett, ‚ÄúBoosted key-frame selection and
correlated pyramidal motion-feature representation for human actionrecognition,‚Äù Pattern Recognit. , vol. 46, no. 7, pp. 1810‚Äì1818, 2013.
[36] F. Zhu and L. Shao, ‚ÄúWeakly-supervised cross-domain dictionary learn-
ing for visual recognition,‚Äù Int. J. Comput. Vis. , vol. 109, nos. 1‚Äì2,
pp. 42‚Äì59, 2014.
[37] D. Masip, √Å. Lapedriza, and J. Vitri√†, ‚ÄúMultitask learning: An applica-
tion to incremental face recognition,‚Äù in Proc. Int. Conf. Comput. Vis.
Appl. , 2008, pp. 585‚Äì590.
[38] D. Masip, √Å. Lapedriza, and J. Vitri√†, ‚ÄúMultitask learning applied to face
recognition,‚Äù in Proc. 1st Spanish Workshop Biometrics , 2007, pp. 1‚Äì8.
[39] Z. Zhu, P. Luo, X. Wang, and X. Tang, ‚ÄúMulti-view perceptron: A deep
model for learning face identity and view representations,‚Äù in Advances
in Neural Information Processing Systems 27 . Red Hook, NY , USA:
Curran Associates, 2014, pp. 217‚Äì225.
[40] Z.-L. Sun, K.-M. Lam, and Q.-W. Gao, ‚ÄúDepth estimation of face images
using the nonlinear least-squares model,‚Äù IEEE Trans. Image Process. ,
vol. 22, no. 1, pp. 17‚Äì30, Jan. 2013.
[41] A. M. Bruckstein, R. J. Holt, T. S. Huang, and A. N. Netravali,
‚ÄúOptimum Ô¨Åducials under weak perspective projection,‚Äù Int. J. Comput.
Vis., vol. 35, no. 3, pp. 223‚Äì244, 1999.
[42] J. Canny, ‚ÄúA computationa l approach to edge detection,‚Äù IEEE Trans.
Pattern Anal. Mach. Intell. , vol. PAMI-8, no. 6, pp. 679‚Äì698, Nov. 1986.
[43] A. Myronenko and X. Song, ‚ÄúPoint set r egistration: Coherent point drift,‚Äù
IEEE Trans. Pattern Anal. Mach. Intell. , vol. 32, no. 12, pp. 2262‚Äì2275,
Dec. 2010.
[44] L. Ma, X. Yang, and D. Tao, ‚ÄúPerson re-identiÔ¨Åcation over camera
networks using multi-task distance metric learning,‚Äù IEEE Trans. Image
Process. , vol. 23, no. 8, pp. 3656‚Äì3670, Aug. 2014.
[45] M. Schmidt. The minFunc Package . [Online]. Available: http://www.cs.
ubc.ca/schmidtm/Software/minFunc.html, accessed Jul. 2014.
[46] H. Xu and S. Mannor, ‚ÄúRobustness and generalization,‚Äù Mach. Learn. ,
vol. 86, no. 3, pp. 391‚Äì423, 2012.
[47] A. W. van der Vaart and J. A. Wellner, Weak Convergence .N e wY o r k ,
NY , USA: Springer-Verlag, 1996.
[48] T. Sim, S. Baker, and M. Bsat, ‚ÄúThe CMU pose, illumination, and
expression database,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 25,
no. 12, pp. 1615‚Äì1618, Dec. 2003.
[49] P. J. Phillips, H. Moon, S. A. Rizvi, and P. J. Rauss, ‚ÄúThe FERET
evaluation methodology for face-recognition algorithms,‚Äù IEEE Trans.
Pattern Anal. Mach. Intell. , vol. 22, no. 10, pp. 1090‚Äì1104, Oct. 2000.
[50] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker, ‚ÄúMulti-PIE,‚Äù
Image Vis. Comput. , vol. 28, no. 5, pp. 807‚Äì813, 2010.
[51] L. Liu, L. Shao, and X. Li, ‚ÄúEvolutionary compact embedding for large-
scale image classiÔ¨Åcation,‚Äù Inf. Sci. , pp. 1‚Äì15, Jul. 2014.
[52] P. Paysan, R. Knothe, B. Amberg, S. Romdhani, and T. Vetter, ‚ÄúA 3D
face model for pose and illumination invariant face recognition,‚Äù in Proc.
6th IEEE Int. Conf. AVSS , Sep. 2009, pp. 296‚Äì301.
[53] Y . Sun, X. Wang, and X. Tang, ‚ÄúDeep convolutional network cascade
for facial point detection,‚Äù in Proc. IEEE Conf. Comput. Vis. Pattern
Recognit. , Jun. 2013, pp. 3476‚Äì3483.
[54] S. R. Arashloo, J. Kittler, and W. J. Christmas, ‚ÄúPose-invariant face
recognition by matching on multi-resolution MRFs linked by super-
coupling transform,‚Äù Comput. Vis. Image Understand. , vol. 115, no. 7,
pp. 1073‚Äì1083, 2011.
[55] S. R. Arashloo and J. Kittler, ‚ÄúFast pose invariant face recognition using
super coupled multiresolution Markov random Ô¨Åelds on a GPU,‚Äù Pattern
Recognit. Lett. , vol. 48, pp. 49‚Äì59, Oct. 2014.
[56] S. R. Arashloo and J. Kittler, ‚ÄúEfÔ¨Åcient processing of MRFs for
unconstrained-pose face recognition,‚Äù in Proc. IEEE 6th Int. Conf.
Biometrics, Theory, Appl., Syst. , Sep./Oct. 2013, pp. 1‚Äì8.
[57] S. Li, X. Liu, X. Chai, H. Zhang, S. Lao, and S. Shan, ‚ÄúMaximal
likelihood correspondence estimation for face recognition across pose,‚ÄùIEEE Trans. Image Process. , vol. 23, no. 10, pp. 4587‚Äì4600, Oct. 2014.
[58] T. Zhang, D. Tao, X. Li, and J. Yang, ‚ÄúPatch alignment for dimen-
sionality reduction,‚Äù IEEE Trans. Knowl. Data Eng. , vol. 21, no. 9,
pp. 1299‚Äì1313, Sep. 2009.[59] D. Chen, X. Cao, F. Wen, and J. Sun, ‚ÄúBlessing of dimensionality:
High-dimensional feature and its ef Ô¨Åcient compression for face veriÔ¨Å-
cation,‚Äù in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , Jun. 2013,
pp. 3025‚Äì3032.
[60] H. Li, G. Hua, Z. Lin, J. Brandt, and J. Yang, ‚ÄúProbabilistic elastic
matching for pose variant face veriÔ¨Åcation,‚Äù in Proc. IEEE Conf.
Comput. Vis. Pattern Recognit. , Jun. 2013, pp. 3499‚Äì3506.
[61] K. Simonyan, O. M. Parkhi, A. Vedaldi, and A. Zisserman, ‚ÄúFisher
vector faces in the wild,‚Äù in Proc. Brit. Mach. Vis. Conf. , 2013, vol. 1,
no. 2, pp. 1‚Äì12.
[62] Y . Taigman, L. Wolf, and T. Hassner, ‚ÄúMultiple one-shots for utilizing
class label information,‚Äù in Proc. Brit. Mach. Vis. Conf. , 2009, pp. 1‚Äì12.
[63] Q. Cao, Y . Ying, and P. Li, ‚ÄúSimilar ity metric learning for face recogni-
tion,‚Äù in Proc. IEEE Int. Conf. Comput. Vis. , Dec. 2013, pp. 2408‚Äì2415.
[64] O. Barkan, J. Weill, L. Wolf, and H. Aronowitz, ‚ÄúFast high dimensional
vector multiplication face recognition,‚Äù in Proc. IEEE Int. Conf. Comput.
Vis., Dec. 2013, pp. 1960‚Äì1967.
[65] P. Li, Y . Fu, U. Mohammed, J. H. Elder, and S. J. D. Prince, ‚ÄúProba-
bilistic models for inference about identity,‚Äù IEEE Trans. Pattern Anal.
Mach. Intell. , vol. 34, no. 1, pp. 144‚Äì157, Jan. 2012.
[66] S. J. D. Prince and J. H. Elder, ‚ÄúProbabilistic linear discriminant analysis
for inferences about identity,‚Äù in Proc. IEEE 11th Int. Conf. Comput. Vis. ,
Oct. 2007, pp. 1‚Äì8.
Changxing Ding received the B.E. degree in
automation from the Shandong University ofScience and Technology, Qingdao, China, and the
M.E. degree in control science and engineering from
the Harbin Institute of Technology, Harbin, China,in 2009 and 2011, respectively. He is currently pur-
suing the Ph.D. degree with the Centre for Quantum
Computation and Intelligent Systems and the Fac-ulty of Engineering and Information Technology,University of Technology at Sydney, Sydney, NSW,
Australia. His research interests include computer
vision, machine learning, and in particular, on face recognition.
Chang Xu received the B.E. degree from Tianjin
University, Tianjin, China, in 2011. He is currentlypursuing the Ph.D. degree with the Key Labora-
tory of Machine Perception (Ministry of Educa-
tion), Peking University, Beijing, China. He was aResearch Intern with the Knowledge Mining Group,
Microsoft Research Asia, Beijing, and a Research
Assistant with the Center of Quantum Computa-tion and Intelligent Systems and the Faculty ofEngineering and Information Technology, University
of Technology at Sydney, Sydney, NSW, Australia.
He received the best student paper award in ACM ICIMCS 2013. His researchinterests lie primarily in machine learning, multimedia search, and computer
vision.
Dacheng Tao (F‚Äô15) iis currently a Professor of
Computer Science with the Centre for QuantumComputation and Intelligent Systems and the Faculty
of Engineering and Information Technology, Univer-
sity of Technology at Sydney, Sydney, NSW, Aus-tralia. He mainly applies statistics and mathematicsto data analytics, and his research interests spread
across computer vision, data science, image process-
ing, machine learning, neural networks, and videosurveillance. His research results have expounded
in 1 monograph and over 100 publications at pres-
tigious journals and prominent conferences, such as the IEEE T
RANSAC -
TIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE , the IEEE
TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE ,
the IEEE T RANSACTIONS ON IMAGE PROCESSING ,t h e Journal of Machine
Learning Research ,t h e International Journal of Computer Vision ,t h eW o r k -
shop on Neural Information Processing Systems, the International Conference
on Machine Learning, the Conference on Computer Vision and Pattern
Recognition, the International Confer ence on Computer Vision, the European
Conference on Computer Vision, the I nternational Conference on ArtiÔ¨Åcial
Intelligence and Statistics, and the International Conference on Data Mining,
and the ACM Conference on Knowledge Discovery and Data Mining, with
several best paper awards, such as the Best Theory/Algorithm Paper Runner-
Up Award in the IEEE ICDM‚Äô07, the best student paper award in the IEEE
ICDM‚Äô13, and the 2014 ICDM 10-Year Highest Paper Award.
"
https://ieeexplore.ieee.org/document/4813445,"978-1-4244-2154-1/08/$25.00 ¬©2008 IE  
Abstract 
 
The ability to handle multi-view  facial expressions is 
important for computers to understand affective behavior under less constrained envir onment. However, most of 
existing methods for facial  expression recognition are 
based on the near-frontal view face data, which are likely to fail in the non-frontal facial expression analysis. In this paper, we conduct an investiga tion on analyzing multi-view 
facial expressions. Three local patch descriptors (HoG, LBP, and SIFT) are used to extract facial features, which are the inputs to a nearest-neighbor indexing method that identifies facial expressions. We also investigate the influence of feature dimension reductions (PCA, LDA, and LPP) and classifier fusion on the recognition performance. We test our approaches on multi- view data generated from 
BU-3DFE 3D facial expression database that includes 100 subjects with 6 emotions and 4 intensity levels. Our extensive person-independent experiments suggest that the SIFT descriptor outperforms HoG and LBP, and LPP outperforms PCA and LDA in this application. But the classifier fusion does not show a significant advantage over SIFT-only classifier.  
1. Introduction 
Analysis of human facial expressions has attracted 
increasing attention from res earchers, because facial 
expression plays an important role in human social life [3], [15]. Automated human facial expression can benefit multiple research fields. On the one hand, it can provide researchers in social scie nce (psychology, psychiatry, 
education, etc.) a powerful inst rument to analyze data of 
human affective behavior. On the other hand, it can contribute to the paradigm of human-centered human computer interfaces that shoul d be proactive and able to 
understand human behavior.  
The research of machine understanding of facial 
expressions has witnessed significant progress in the past years [15]. Automatic detection of the six basic emotions in posed and controlled displays can be done with reasonably high accuracy. However, detec ting these expressions or any expression of human affective behavior in less 
constrained settings is still a very challenging problem. Specifically, most of the exis ting efforts toward facial 
expression recognition focus on the images/sequences of near-frontal-view faces, which are not able to handle the arbitrary head movement of spontaneous affective behavior in a realistic interactive setting.  One main reason behind this fact is that the existing and frequently used databases (e.g., Cohn-Kanade database [6]) in facial expression research community typically  captured the frontal-view 
facial displays.  
A recent database named BU-3DFE database collected 
by Yin et al. [14] represents the initial efforts toward building readily accessible da tabase of 3D facial 
expressions. Access to this database motivates us to explore some interesting and open issues about multi-view 
facial expressions, which have been precluded in the previous studies that focused on frontal-view expressions. 
In this paper, we focus on the investigation of multi-view 
facial expression recognition. We develop methods to recognize facial expressions under multiple view angles. 
We apply three local patch descriptor (Local Binary Pattern (LBP) [9], Histograms of Oriented Gradients (HoG) [4]  and Scale Invariant Feature Transform (SIFT) [8]) to 
characterize facial expressions , and also investigate the 
influence of feature dimension reductions (PCA, LDA, LPP [5]) and classifier fusion on the recognition performance. We test our approaches on the BU-3DFE data of 100 subjects with 6 emotions and 4 intensity levels, and 5 yaw rotation view angles. Our extensive person-independent experiment suggests that the SIFT descriptor performs better than HoG and LBP, LPP performs better than PCA and LDA in this application. The contributions of this paper include: 1) we explored the automatic multi-view facial expression recognition, to 
which very few efforts are found to date; 2) we applied the advanced techniques of feature extraction (HoG, LBP, SIFT) for this application; 3) we investigated the classifier fusion in order to improve the performance. 
2. Related work 
Research on machine understanding of facial 
expressions has attracted much attention from researchers in multiple disciplines, including computer science,  
Multi-View Facial Expression Recognition 
 
1Yuxiao Hu, 1Zhihong Zeng, 2Lijun Yin, 2Xiaozhou Wei, 1Xi Zhou and 1Thomas S. Huang 
   1University of Illinois at Urbana-Champaign           2State University of New York at Binghamton 
    Urbana, IL 61801, USA                                            Binghamton, NY 13902, USA 
  {hu3,zhzeng,xizhou2,huang}@ifp.uiuc.edu       {lijun,xwei}@cs.binghamton.edu  
 
linguistics, psychology, psychiatry, education, 
neuroscience, etc. Although most of the existing methods are based on deliberately displayed facial expressions [15], an increasing number of studies have been reported to recognize spontaneous facial expressions [1], [11], [12]. This trend of the field inspires us to explore some challenging and largely unexplored area, including the research of multi-view facial expression analysis.  
Most of existing efforts in this field, including studies 
both on deliberately displayed facial expressions and on spontaneous facial expressions, has been focused on recognition of facial expre ssions in near-frontal-view 
recordings. The human behavior in less constraint environment, e.g., arbitrary head movement which results in non-front-view facial expressions, challenges these existing methods.  
An exemplar exception is the study of Pantic and Patras 
[10], who explored automatic analysis of facial expressions from the profile view of the f ace. Recently, Yin et al. [14], 
Wang et al. [13] and Chang et al. [2] used 3D expression data for facial expression recognition. In particular, the study of [13] analyzed the influence of view change on recognition performance of facial expression, based on the classifier trained on frontal-view faces. As shown by the 
experimental results in [13], the classifier performed poorly to recognize facial expression undergoing a large view variation.  
Compared with the previous studies mentioned above, 
this paper has the following significant progresses: Firstly, we used the data of 100 subjects, which is much larger than the amount used in the previous studies; Secondly, we used images of all four levels of  intensity (from low to high) 
rather than two levels in [13] for the experiment; Thirdly, we developed algorithms for the performance study with a wider range of viewing angles; Finally, we applied three advanced feature extraction methods (HoG, LBP, and SIFT) and investigated the combinati on of these classifiers based 
on different features. 
3. BU-3DFE data and preprocessing 
Although there have been some 2D face expression 
databases accessible for facial  expression research, the 
efforts toward building readily  accessible database for 3D 
expression analysis has just emerged [15]. To our best knowledge, the BU-3DFE database [13] is the only publicly available emotion database that contains 3D range data of six prototypical facial expressions. 
In BU_3DFE 3D facial expression database, there are 
100 subjects who participated in face scans, including undergraduates, graduates and faculties from State University of New York at Binghamton. The resulting database consists of about  60% female and 40% male 
subjects with a variety of ethnic/racial ancestries.  
Each subject in the database performed seven expressions, captured by a 3D face scanner. With the 
exception of the neutral expression, each of the six prototypic expressions ( happiness, disgust, fear, anger, 
surprise, and sadness ) includes four levels of intensity. 
Figure 1 shows some samples in the BU-3DFE database. 
 
 
Figure 1. Samples of expressi ons in the BU-3DFE database 
 
In our study, we generate 64*64 multi-view images of 
facial expressions from the av ailable 3D data in BU-3DFE 
database. The data in our experiment includes 100 subjects, 6 emotions (excluding neutral) with four levels of intensities, and 5 yaw angles (0, 30, 45, 60, and 90 degrees), illustrated in Figure 2.  
 
 
 
Figure 2: The examples of multi-view facial expressions 
4. Features 
Most of the existing facial expression recognizers 
employ various pattern recognition approaches, which are primarily based on 2D facial features [15]. The commonly used facial features are either geometric features such as the 
shapes of the facial components (eyes, mouth, etc.) [10], [11] or appearance features re presenting the facial texture 
(e.g., Gabor wavelets [1][13]). Wang et al. [13] proposed to use 3D geometric curvature features to recognize the 3D 
Angry 
Disgust 
Fear  Sad  
Surprise Happy 
 
facial expressions. 
Recently, many features ha ve been proposed in the 
computer vision field. In particular, Local Binary Pattern (LBP) [9], Histograms of Oriented Gradients (HoG) [4]  and Scale Invariant Feature Transform (SIFT) [8] have  been successfully applied fo r texture classification, 
segmentation, image retrieval, object and face recognition, etc. These features are able to  characterize the local region, 
and in the meantime they are robust to a certain degree to changes in image transformation and tolerant to occlusion and segmentation noise. In this paper, we propose to explore these features for our facial expression analysis.  
HoG [4] is to describe local object appearances and 
shapes by distribution of local intensity gradients or edge directions, without the consideration of the corresponding gradient or edge position. During its implementation, an image is divided into a number of small blocks. In each block, a local 1-D histogram of gradient directions or edge orientations is calculated. As a result, a HoG descriptor is represented by the combin ed histogram entries.  
LBP [9] is to describe the pixels of an image by 
thresholding the neighborhood of each pixel with the value 
of the center point and using these binary numbers to construct a label. Then the histogram of the labels over a region can be used as the texture descriptor.  
SIFT [8] is also a local desc riptor of an image which is 
implemented by two steps: different-of-Gaussian (DoG) filters are first used to localize key points with the local scale-space maxima of DoG, and then each of these key 
points is used to generate a descriptor represented by a 3D histogram of gradient locations and orientations. This method transforms an image into a set of local descriptors, each of which is robust to image translation, scaling, and rotation and to some degree invariant to illumination changes and 3D projection due to the quantization of gradient locations and orientaitons. 
In recent years, computer vi sion research has witnessed a 
growing interest in subspace analysis techniques. Before 
we utilize any classification technique, it is beneficial to first perform dimensionality reduction to project an image into a low dimensional f eature space, due to the 
consideration of learnability and computational efficiency.  
Locality Preserving Projection (LPP) [5] is a linear 
mapping which is obtained by finding the optimal linear approximations to the eige n-functions of the Laplace 
Beltrami operator on the manifold [5]. Different from the nonlinear manifold learning techniques, LPP could be simply applied to any new data point to locate it in the reduced representation manifold subspace, which is suitable for the classi fication application. 
Some traditional subspace methods such as the PCA and 
LDA aim to preserve the global structure. However, in many real world applicati ons, especially the facial 
expression recognition, the local structure could be more important. Different from PCA and LDA, LPP finds an 
embedding that preserves local information, and obtains a subspace that best detects the e ssential manifold structure.  
5. Classification 
The variation of facial styl es (e.g., different facial 
geometry and textures associated with persons‚Äô identity), facial expressions (e.g., defo rmation of facial surfaces), 
and head poses (e.g., different viewing angles with respect to a camera) brings the big variation of the resulting facial images, thus challenging the current methods for analysis of facial expressions, which are mainly based on the nearly-frontal-view face image.  In our study, we focus on 
analyzing the factors of facial expressions and viewing angles while different facial styles (person identity) are considered as the w ithin-class variance.  
Suppose we have an image 
x that captured a certain 
human facial expression (emotion) ) ,...,2,1( m iCi=  at 
certain view angle (pose) ) ,...,2,1( n jDj= . The goal of 
multi-view facial expression recognition is to determine what emotion the test image represents, and at what pose the face is with respect to th e camera. This problem can be 
formulated in the terms of probability theory as follows,  
)(),|(),()|,(xpDCxpDCPxDCPj i j i
j i=       (1) 
In our experimental data, the prior ),(j iDCp is an 
evenly distribution probability. Thus, we use maximum likelihood criterion to predict the emotion and view angle of the face with resp ect to the camera.  
We propose the following two schemes to recognize the 
emotion expression and view angle (pose) from an input face image: view+emotion cascade classification and 
view*emotion composite classification. And the classifier we use in this study is Nearest Neighbor (NN) which shows robust in many realistic applications. 
5.1. Pose+Emotion cascade classification 
The view+emotion cascade cl assification divides the 
task of multi-view facial e xpression recognition into two 
sequential steps, as illustrated in Figure 3. We first use an view classifier to recogni ze the view of this face (0
0, 300, 
450, 600, or 900), then use the view-dependent emotion 
classifier to estimate the emo tion that the image represents 
(happy, angry, disgust, fear, sad, and surprise ). This 
classification scheme can be described by Equation (2). 
)|(),|( )|,( xDpxDCp xDCPj j i j i=              (2) 
During the learning stage, we use all training data to 
build a 5-class view classifier which output the view estimation (0
0, 300, 450, 600, or 900) of the face. In this step, 
the difference of the facial e xpression is regarded as the 
within-class variance.  A set of view-dependent emotion 
 
classifiers are learned by us ing training images that are 
corresponding to certain view angles. For example, the 300 
emotion classifier in Figur e 3 is a 6-class emotion 
recognizer, which is built on th e face images captured at 
300 view angle. 
 
 
Figure 3: The View+Emotion cascade classification, including a 
view classifier and a set of view-dependent emotion classifiers.  
5.2. View*emotion composite classification 
The second solution for multi- view facial expression 
recognition is to build a cla ssifier that estimates the 
emotion and facial views simultaneously.. In the other words, the each possible pair of emotions and poses is considered as a class. Considering 6 emotions (happy, angry, disgust, fear, sad, and surprise) and 5 viewing angles 
(0
0, 300, 450, 600, or 900), the goal is to build a 30-class 
classifier. During the learning stage, we use all training 
data with emotion and view labels to build the emotion*view classifier. 
5.3. Classifier fusion 
Classifier fusion is an ac tive area in machine learning 
and pattern recognition field. Many studies have demonstrated the advantage of  combination of classifiers 
over individual classifiers due to uncorrelated errors from different classifiers [7]. 
Our nearest neighbor classifiers output the similarity 
(range 0~1) between a test sa mple and each class according 
to its nearest distance to diffe rent class samples, so the 
results from different classifiers can be combined based on these similarities.  In this study, we are very interested in the 
combination of classifiers, each of which is built on different feature descriptors. Specifically, HoG, LBP and SIFT characterize the different  aspects of image patches, 
providing complementary information about facial expressions. They could produce different classification errors which can be reduced to certain degrees by the 
classifier fusion technique. T hus, the combined classifier 
could generate a better performance than the individual classifiers. 6. Experiments 
In our experiment, we apply 5-run two-fold 
cross-validation person-indepe ndent scheme to evaluate 
our approaches for multi-view facial expression recognition. In details, we randomly divided 100 subjects in BU-3DFE database into 2 groups without overlap. Each group includes 50 subjects.  For this test, all the data in one group are used as the test data, and the data of the remaining group are used as training samples. Then we switch the groups for training and testing. This experiment is repeated 5 times, each time dividing the subjects randomly. The statistics of our experimental data are 100 subjects, 6 emotions with 4 intensity levels, 5 view angles (0
0, 300, 450, 600, or 900), illustrated in Table 1. So the total 
number of images is 12000. We use a histogram equalization to reduce the influence of the skin color variation due to the di fferent human races.   
 
Table 1. Statistics of data 
Subject Emotion Intensity Angle Total images
100 6 4 5 12000 
 
We first used the technique (SIFT, HoG, LBP) on 
aligned multi-view images described in Section 4 to extract facial features. The extracted features on key facial points 
are concatenated to form a long vector as the input of our classifiers.  
We also investigate the influence of various dimension 
reduction methods on our emotion classification. We use PCA, LDA, and LPP to project the original features into 
low dimension spaces, where the low-dimensional features are used as input to a nearest-neighbor indexing method that identifies view and facial expressions. For each method, we use the optimal facial expression subspace which has the best recognition result with respect to the number of dimensions.   
Table 2, 3, 4 and 5 report the view-dependent 
experimental results of facial expression recognition, based on raw appearance intensity, HoG, LBP, and SIFT descriptors, respectively. Each  row in these tables shows 
the error rates of different di mension reductions (original, 
LPP, PCA, LDA) with respect to a certain yaw rotation angle (0, 30, 45, 60 and 90 degrees). ‚ÄúOriginal‚Äù means the original space without any dime nsion reduction.  The last 
row is the average performance of these classifiers in the 
multi-view emotion recogniti on experiment.  The best 
average performance result with the lowest error rate in 
each table is highlighted  by the bold font.  
Table 2 shows that based on the raw appearance intensity, 
the classification in the orig inal space achieves the best 
performance with 57.16% error rate. Table 3, 4 and 5 demonstrate that based on HoG, LBP and SIFT descriptors, the classification in the LPP space has the best performance with error rates of 32.618%, 35.764%, and 26.942%, 00 Emotion classifier 
300 Emotion classifier 
450 Emotion classifier 
600 Emotion classifier 
900 Emotion classifier x
  
   
View 
classifier 00 
300 
450 
600 
900 
 
respectively. All of them are much better than the 
classification based on the ra w appearance intensity. The 
experiments suggest that SIFT+LPP has the best performance with 26.942% average error rate, and achieves 
the lowest error rate ( 26.13 %) at 30
0 and highest error rate 
(28.55% ) at the profile view (900). 
 
Table 2: Error rates of view-dep endent emotion recognition based 
on raw appearance intensity. 
Angle Original  LPP PCA LDA 
0 0.5458 0.5623 0.5467 0.5763
30 0.5498 0.6198 0.5488 0.6282
45 0.5637 0.6545 0.561 0.7128
60 0.5848 0.6962 0.585 0.695
90 0.6143 0.7707 0.6175 0.7835
Average 0.5716 0.6607 0.5718 0.6791
 
In our experiment, the view recognition is pretty easy 
because we use data with five view angles (00, 300, 450, 600, 
or 900) from frontal view to profile view. The interval of 
view angles is larger than or equal 150 so the image 
differences between different vi ew angles are considerable. 
Our experiments show less than  1% error rates with raw 
appearance feature and HoG features. In our future work, we will add images with smaller angle interval. 
Table 6 is the results of view-dependent emotion 
recognition, based on the combination of SIFT+LPP, HoG+LPP, and LBP+LPP classi fication. Specifically, we 
use the following combining rules in PRTools (www.prtools.org) to make th e final recognition decision:  
Prodc: product combining classifier Meanc: averaging co mbining classifier 
Median: median combining classifier Maxc: maximum combining classifier Minc: minimum combining classifier Votec: voting comb ining classifier 
Table 6 shows that the rank of combining classifiers with 
respect to average recognition performance from the best to 
the worst is 
Prodc > meanc > minc > median > maxc >votec However, even the best combination (prodc) with error 
rate (25.54%) does not show significant advantage over SIFT-only classification whose error rate is 26.942% 
Table 8 show the confusion matrix of average 
recognition results, based on the prodc combination of SIFT+LPP, HoG+LPP and LBP+L PP. It demonstrates that 
the rank of emotions with  respect to the average 
recognition performance from the best to the worst is  
Surprise > Happy > Angry > Sad > Disgust > Fear In our experiment of view*emotion composite 
recognition where a 30-class classifier is built to simultaneously recognize view and emotion, the results based on HoG features are shown in Table 7. These results are a little worse than the co rresponding results in Table 3 
based on view+emotion cascade classification scheme. The view*emotion composite classification methods based on 
LBP and SIFT features demand too much memory that are 
beyond our current computer  capability. So we gave up 
using LBP and SIFT features for this view*emotion composite recognition experiment.  
Table 3: Error rates of view-dep endent emotion recognition based 
on HoG features. 
Angle Original  LPP PCA LDA 
0 0.5302 0.3208 0.529 0.4147
30 0.5373 0.3155 0.5363 0.4004
45 0.5573 0.3234 0.5599 0.3867
60 0.5607 0.3198 0.5575 0.3716
90 0.5468 0.3514 0.5468 0.366
Average 0.54646 0.32618 0.5459 0.38788
 
Table 4: Error rates of view-dep endent emotion recognition based 
on LBP features. 
Angle Original  LPP PCA LDA 
0 0.5013 0.3567 0.5048 0.4636
30 0.5132 0.3488 0.515 0.4538
45 0.5277 0.3633 0.5274 0.5957
60 0.515 0.3482 0.5156 0.4908
90 0.5484 0.3712 0.5495 0.6059
Average 0.52112 0.35764 0.52246 0.52196
 
Table 5: Error rates of view-dep endent emotion recognition based 
on SIFT features. 
Angle Original  LPP PCA LDA 
0 0.4368 0.2724 0.4393 0.4084
30 0.4319 0.2613 0.4327 0.4041
45 0.4454 0.2665 0.4423 0.5518
60 0.4464 0.2614 0.4479 0.4229
90 0.4384 0.2855 0.4389 0.481
Average 0.43978 0.26942 0.44022 0.45364
 
Table 6: Error rates of view -dependent emotion, based on 
combination of SIFT+LPP, HoG+LPP and LBP+LPP classifiers 
Angle prodc meanc median maxc minc votec 
0 0.2641 0.2677 0.2757 0.278 0.2741 0.2849
30 0.2553 0.2555 0.2639 0.2683 0.265 0.2757
45 0.2626 0.2657 0.2772 0.2713 0.2717 0.2882
60 0.2657 0.2704 0.2718 0.2877 0.2746 0.2808
90 0.2796 0.2823 0.2954 0.2907 0.2907 0.3092
Ave0.2654 0.2683 0.2768 0.2792 0.2752 0.2877
 Table 7: Error rates of view*e motion composite classification, 
based on HoG features 
Original  LPP PCA LDA 
0.5788 0.3673 0.57097 0.48417 
7. Conclusion 
Developing a facial expression recognition system to be 
capable of handling non-frontal-view facial expressions is important to advance th e research of machine 
understanding of spontaneous facial expressions. However, little work has been done to investigate such an issue 
 
systematically in the past.  Most of existing methods are 
based on data of near-frontal-v iew facial expression, which 
are conceivable to fail in affec tive behavior analysis in less 
constraint interactive environment. 
In this paper, we investigated multi-view facial 
expression recognition, with a goal to improve performance by taking into account the influence of viewing angles of facial images. We applied three advanced local patch descriptors (HoG, LBP and SIFT) to characterize the facial expressi ons. We investigated feature 
dimension reduction (LPP, PCA and LDA) and the classifier fusion for this application. Our extensive person-independent experime nt based on 100 subjects in 
the BU-3DFE database shows that the SIFT descriptor performs better than HoG and LBP, LPP performs better than PCA and LDA. Nevertheless, classifier fusion does not show significant improvement in our experiment. How to build an optimal combination is worthy further investigation.  
In our experiment, the view recognition is pretty easy, 
because the large interval of viewing angles (e.g., greater 
than or equal to 15
0 in our current study) causes obvious 
image differences between different view angles. In our future work, we will add images with smaller angle intervals in order to further explore the view recognition problem. 
References 
[1] Bartlett, M.S., Littlewort, G., Fra nk, M., Lainscsek, C., Fasel, 
I., and Movellan, J.(2005), R ecognizing Facial Expression: 
Machine Learning and Applicati on to Spontaneous Behavior, 
IEEE International Conference on Computer Vision and Pattern Recognition, 568-573 
[2] Chang Y, Vieira M, Turk M,  and Velho L (2005). Automatic 
3D facial expression analysis in videos. Analysis and Modelling of Faces and Gestures, 3723, 293-307. 
[3] Cohn, J.F. (2006), Foundations of  Human Computing: Facial 
Expression and Emotion, Int. Conf. on Multimodal Interfaces, 233-238 [4] Dalal, N. and Triggs, B. ( 2005). Histograms of oriented 
gradients for human detection. IEEE International Conference on Computer Vision and Pattern Recognition. 
[5] He, X., Yan, S., Hu, Y., and Zhang, H, Learning a Locality 
Preserving Subspace for Visual Recognition, Int. Conf. on Computer Vision, 2003 
[6] Kanade, T., Cohn, J., and Tian, Y. (2000), Comprehensive 
Database for Facial Expression Analysis, Int. Conf. on Face and Gesture Recognition, 46-53 
[7] Kuncheva, L.I. (2004), Combin ing Pattern Classifier: Meth
ods and Algorithms, John Wiley and Sons, 2004 
[8] Lowe, D.G. (1999). Object Recognition from Local 
Scale-Invariant Features, Int‚Äôl Conf. on Computer Vision.   
[9] Ojala, T., Pietikainen, M.  and Maenpaa, I. (2002). 
Multi-resolution gray-scale and rotation invariant texture classification with local binary patterns. IEEE Trans. on Pattern Analysis and Machine Intelligence, 24: 971-987, 20 
[10] Pantic, M., and Patras, I. (2006). Dynamics of facial 
expression: recognition of facial  actions and their temporal 
segments form face profile image sequences. IEEE Trans. Systems, Man and Cybernetic s‚ÄìPart B, Vol. 36, No.2, 
433-449 
[11] Sebe, N., Lew, M.S., Cohen, I., Sun, Y., Gevers, T., Huang, 
T.S.(2004), Authentic Facial Expression Analysis, Int. Conf. on Automatic Face and Gesture Recognition. 
[12] Valstar, M., Pantic, M., Ambada r, Z., and Cohn, J.F. (2006). 
Spontaneous vs. Posed Facial Behavior: Automatic Analysis 
of Brow Actions. Int. Conf. on Multimedia Interfaces. 162 ‚Äê
170  
[13] Wang, J., Yin, L., Wei, X., a nd Sun, Y. (2006). 3D Facial 
Expression Recognition Based on Primitive Surface Feature Distribution. IEEE Conference on Computer Vision and Pattern Recognition, 2:1399-1406 
[14] Yin, L., Wei, X., Sun, Y., Wang,  J., Rosato, M. J. (2006).  A 
3D facial expression database for facial behavior research. 
Int. Conf. on Automatic Face and Gesture Recognition,  211- 216 
[15] Zeng, Z., Pantic, M., Roisman, G.I. and Huang, T.S. (2007). 
A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions. Int‚Äôl Conf. Multimodal Interfaces, 126-133. 
 
Table 8: confusion matrix of average recognition base on the pr odc combination of SIFT+LPP, LB P+LPP and HoG+LPP on 5 views  
Recognized   
Accuracy (%) Angry Disgust Fear Happy Sad Surprise
Angry 73.47 5.56 3.87 0.78 15.38 0.94
Disgust 10.15 71.00 7.74 3.67 4.05 3.39
Fear 6.96 9.21 55.40 14.53 7.65 6.25
Happy 1.67 3.13 11.61 81.74 1.13 0.72
Sad 19.65 2.08 5.69 0.92 71.40 0.26Ground 
truth 
Surprise 1.30 2.31 5.49 1.48 1.69 87.73
 
"
https://ieeexplore.ieee.org/document/7944639,"1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE
Transactions on Multimedia
1
Multimodal 2D+3D Facial Expression Recognition
with Deep Fusion Convolutional Neural Network
Huibin Li, Student Member, IEEE, Jian Sun, Member, IEEE, Zongben Xu,
Member, IEEE, and Liming Chen, Member, IEEE
Abstract ‚ÄîThis paper presents a novel and efÔ¨Åcient Deep
Fusion Convolutional Neural Network (DF-CNN) for multi-modal
2D+3D Facial Expression Recognition (FER). DF-CNN comprises
a feature extraction subnet, a feature fusion subnet and a softmax
layer. In particular, each textured 3D face scan is represented as
six types of 2D facial attribute maps (i.e., geometry map, three
normal maps, curvature map, and texture map), all of which are
jointly fed into DF-CNN for feature learning and fusion learning,
resulting in a highly concentrated facial representation (32-
dimensional). Expression prediction is performed by two ways: 1)
learning linear SVM classiÔ¨Åers using the 32-dimensional fused
deep features; 2) directly performing softmax prediction using
the 6-dimensional expression probability vectors. Different from
existing 3D FER methods, DF-CNN combines feature learning
and fusion learning into a single end-to-end training framework.
To demonstrate the effectiveness of DF-CNN, we conducted
comprehensive experiments to compare the performance of DF-
CNN with handcrafted features, pre-trained deep features, Ô¨Åne-
tuned deep features, and state-of-the-art methods on three 3D
face datasets (i.e., BU-3DFE Subset I, BU-3DFE Subset II, and
Bosphorus Subset). In all cases, DF-CNN consistently achieved
the best results. To the best of our knowledge, this is the Ô¨Årst
work of introducing deep CNN to 3D FER and deep learning
based feature-level fusion for multi-modal 2D+3D FER.
Index Terms‚ÄîFacial expression recognition, deep fusion con-
volutional neural network, multimodal, textured 3D face scan.
I. I NTRODUCTION
FACIAL expressions, as a form of nonverbal communica-
tion, and a primary means of conveying social information
among humans, are ideal for human emotion measuremen-
t, computation, and interpretation. Therefore, machine-based
automatic facial expression recognition (FER) has a wide
range of applications in human-computer interaction, facial
animation, entertainment, and psychology study [3], [27], [43],
[58], etc. It has been extensively investigated over the past
decades in the Ô¨Åelds of multimedia, affective computing, and
computer vision [6], [12], [37], [40].
Existing FER methods generally can be classiÔ¨Åed from three
perspectives, namely the data modality, expression granular-
ity, and temporal dynamics [12], [37], [40]. From the Ô¨Årst
perspective, they are classiÔ¨Åed into: 2D FER (which uses 2D
face images), 3D FER (which uses 3D face shape models),
H. Li, J. Sun, and Z. Xu are with the Institute for Information and System
Sciences, School of Mathematics and Statistics, Xi‚Äôan Jiaotong University,
Xi‚Äôan, 710049, China. E-mail: fhuibinli, jiansun, zbxug@mail.xjtu.edu.cn,
L. Chen is with the LIRIS UMR 5205, Department of Mathematics
and Informatics, Ecole Centrale de Lyon, Lyon, 69134, France. E-mail:
liming.chen@ec-lyon.fr.
Manuscript received xxx, xxx; revised xxx, xxx.and 2D+3D multi-modal FER (which uses both 2D and 3D
face data). From the second perspective, they are divided into:
1) recognition of prototypical facial expressios (i.e., anger,
disgust, fear, happiness, sadness and surprise), 2) detection and
recognition of facial Action Units (AU, e.g., brow raiser, lip
tightener, and mouth stretch). From the third perspective, they
are categorized into static (still images) or dynamic (image
sequences) FER [40], [68]. In this paper, we focus on the
problem of recognizing the six prototypical facial expressions
using multi-modal 2D and 3D static face data (i.e., textured
3D face scans).
In the literature of FER, the majority of methods are
based on 2D face images or videos (e.g., [5], [6], [8], [18],
[37], [49], [55], [56], [58], [61], [62]). Despite signiÔ¨Åcant
advances have been achieved, 2D methods still fail to solve the
challenging problems of illumination and pose variations [37].
Designing FER systems using infrared facial images is a
beneÔ¨Åcial attempt to solve the illumination issue [54], [55].
But infrared images are usually fail to capture subtle facial
deformations, e.g., skin wrinkles [18], and also sensitive to
the effect of wearing glasses, which is often occur in uncon-
trolled condition. With the fast development of 3D imaging
and scanning technologies, FER using 3D face scans has
attracted more and more attentions [12], [13], [16], [40]. This
is mainly due to that 3D face scans are naturally robust
to lighting and pose variations. Moreover, 3D facial shape
deformations caused by facial muscle movements contain
important cues to distinguish different expressions. To meet
the requirements of real applications, FER based on multi-
modality data (e.g., visual and audio [49], visible and infrared
face images [54], [55]), especially using both 2D face images
and 3D face models [16], [24] [42], [50], is becoming a
promising research direction due to that there exist large
complementarity among different modalities.
This paper is a new attempt along this promising direc-
tion, which dedicates to exploring multi-modal 2D+3D FER
method by combing the advantages of both 2D and 3D face
data. The main challenges of such combination involve the
following two issues: 1) how to Ô¨Ånd a uniÔ¨Åed framework
to generate discriminative facial representations for both
2D and 3D face data? 2) how to optimally combine the
facial representations of 2D and 3D face data for expression
prediction? As illustrated in Table I, handcrafted features
such as HOG [7], LBP [65], and Gabor [64] have been
widely used for facial representations in 2D FER. Similarly,
these handcrafted features have also been widely employed
in 3D FER, which are used to describe 3D facial shape
1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE
Transactions on Multimedia
2
TABLE I
MOTIVATIONS :A FEW EXAMPLES OF CURRENT FER RESEARCH .
Handcrafted
features Learned
Features
2D
FERHOG:
Hu et al. [7] Deep
CNN: Yu and Zhang [60]
LBP:
Zhao et al. [65] DBN:
Kahou et al. [19]
Gabor:
Zhang et al. [64] Auto-Encoder:
Rifai et al. [39]
3D
FERDepth-SIFT
: Berretti et al. [1]
Normal-LBP:
Li et al. [23] Learned
feature for 3D FER?
Curv
ature-HOG: Lemaire et al. [22]
2D+3D
FERHandcrafted
feature-level fusion
Handcrafted
score-level fusion Learning-based
fusion for 2D+3D FER?
Sa
vran et al. [42], Li et al. [24]
information by coding different types of geometric maps like
depth-SIFT [1], normal-LBP [23], and curvature-HOG [22].
Recently, with the signiÔ¨Åcant breakthrough of deep learning,
such kind of handcrafted features have been proven to be
suboptimal. Thanks to the continuous updating and releasing
of large 2D expression datasets (e.g., Acted Facial Expressions
in the Wild (AFEW) [10] and Static Facial Expressions in
the Wild (SFEW) [9]), leaning facial representations using
deep learning is becoming the mainstream in 2D FER. For
example, following the Emotion Recognition in the Wild
(EmotiW) Grand Challenge, a large number of deep learning
based approaches, such as deep convolutional neural network
(CNN) [60], deep belief network (DBN) [19], and auto-
encoder [39] have been successfully used in 2D FER as shown
at the right side of Table I.
However, to the best of our knowledge, deep learning has
never been used to learn 3D facial representations in 3D FER.
This motivates us to Ô¨Åll this gap although a very limited
number of 3D face scans with expression labels are available.
Inspired by the fact that the off-the-shelf pre-trained deep CNN
models have surprising and consistent good generalization
ability for various visual recognition tasks [11], [38], A
promising way is using transfer learning method that Ô¨Åne tunes
a pre-trained deep CNN model using as many as possible 3D
face data.
Deep CNN can provide a uniÔ¨Åed framework to learn facial
representations for both 2D and 3D face data. Then, how to
Ô¨Ånd a strategy to optimally combine these learned 2D and 3D
facial representations is becoming the key issue. As illustrated
in Table I, the suboptimal handcrafted feature-level fusion
and score-level fusion are widely used in current multi-modal
2D+3D FER methods. The importance weights of 2D and 3D
facial features have not be well explored. This motivates us to
design a learning-base fusion strategy, i.e., a novel deep fusion
network, which can automatically learn sophisticated fusion
weights of 2D and 3D facial representations for multi-modal
2D+3D FER. Overall, this paper presents a uniÔ¨Åed end-to-
end learning framework (i.e., Deep Fusion CNN or DF-CNN),
which can deals with both feature learning and fusion learning
for multi-modal 2D+3D FER. Therefore, the main novelties
and contributions of this paper can be summarized as follows:
This is the Ô¨Årst work of introducing deep CNN to 3D
FER and using learned features to describe 3D facial
expressions. To overcome the issue that training 3D faces
are far from enough, we propose to use multiple types offacial attribute maps to learn facial representations by Ô¨Åne
tuning pre-trained deep CNN models trained from large-
scale image dataset for generic visual tasks.
This paper proposes to use a deep fusion net (i.e., a
learning-based feature-level fusion) to learn the optimal
combination weights of 2D and 3D facial representations
for multi-modal 2D+3D FER. This is totally different
from the suboptimal handcrafted feature-level fusion and
score-level fusion used in existing 2D+3D FER.
This paper presents a Deep Fusion CNN, which combines
feature learning and fusion learning into a uniÔ¨Åed end-
to-end training framework, and consistently outperforms
the handcrafted features, pre-trained deep features, Ô¨Åne-
tuned deep features, and state-of-the-art 3D FER methods
on three 3D face datasets.
The remainder of this paper is organized as follows. Related
works for 2D, 3D and 2D+3D FER are introduced in Sec-
tion II. Section III gives an overview of the proposed approach.
Section IV introduces the computational details of generating
different facial attribute maps. Section V describes our DF-
CNN in detail, involving net architecture, training strategy, and
visualization. Experimental results are shown in Section VI,
and Section VII concludes the paper.
II. R ELATED WORKS
A. Related works on 3D and 2D+3D FER
Current 3D FER approaches are mainly model-based or
feature-based [12]. Model-based methods generally employ
dense rigid registration and non-rigid Ô¨Åtting techniques to get
the one-to-one point correspondence among face scans. This
generates a generic expression deformable model, which can
be used to Ô¨Åt unknown face scans, and the Ô¨Åtting param-
eters are Ô¨Ånally used as expression features. For example,
Mpiperis et al. [35] proposed to build a novel bilinear facial
deformable model to characterize the behaviors of facial non-
rigid deformations. Given a new 3D face model, its expression
and identity parameters can be estimated using the well-
trained bilinear model. These parameters are then used as
expression features and fed into the Maximum Likelihood
classiÔ¨Åer for expression prediction. Similarly, Gong et al. [15]
suggested to learn a model to decompose the shape of an
expressive face into a neutral-style basic facial shape com-
ponent (BFSC) and an expression shape component (ESC).
The ESC is then used to design expression features. Zhao et
al.[66] proposed to build a statistical facial feature model
(SFAM) for automatic facial landmarking, both 3D shape
and 2D texture features are extracted around these landmarks
for expression recognition. Feature-based methods generally
extract local expression features around facial landmarks based
on surface geometric attributes or differential quantities. For
example, 3D landmark distances [44], [45], [46], [47], local
surface patch distances [24] [32] [33], geometry and normal
maps [36], conformal images [63], surface normal [26] and
curvatures [26], [53] are some popular features use for 3D
FER. As a typical local feature-based method, Maalej et
al.[32] [33] proposed to extract local surface patches around
70 facial landmarks in the 3D mesh. These patches were
1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE
Transactions on Multimedia
3
then parameterized by a set of closed iso-level curves at the
landmarks. The distance between two patches was computed
by the geodesic distance of deforming their corresponding iso-
level curves in the Riemannian shape analysis space. Finally,
multi-boosting and support vector machines (SVM) classiÔ¨Åers
were used to classify the six prototypical facial expressions.
By combing the advantages of both feature-based and model-
based methods, Zhen et al. [67], [68] proposed to study
3D FER problem from the perspective of facial muscular
movement model. Their method Ô¨Årst automatically segments
3D face shapes into several facial regions according to the
muscular movement model. Then, each region is described by
a set of geometric features. The weights of different regions
are learned by genetic algorithm, and SVM classiÔ¨Åer with
score-level fusion is used for expression prediction. Savran
et al. [42] utilized multi-modal 2D+3D face data for facial
AU detection. They found that 3D data generally perform
better than 2D data, especially for lower AUs. Moreover,
the fusion of two modalities can improve the detection rates
from 93.5% (2D) and 95.4% (3D) to 97.1% (2D+3D). Li
et al. [24] proposed a fully automatic multi-modal 2D+3D
feature based FER approach. Both 2D texture descriptors
and 3D geometry descriptors are used to describe the ap-
pearances and geometric deformations of local facial patches
around automatically detected 2D and 3D facial landmarks.
The complementarity between 2D descriptors, 3D descriptors,
and 2D+3D descriptors are demonstrated in their experiments
based on both feature-level and score-level fusion strategies of
the SVM classiÔ¨Åer.
The main weakness of model-based methods lie in that they
require to establish dense correspondence among face scans,
which is still a challenging issue. Moreover, time consuming
procedures like dense 3D face registration and model Ô¨Åtting are
usually indispensable in practice. Feature-based methods gen-
erally perform better than model-based ones. However, their
performances are largely dependent on the accuracy of 3D
facial landmarking, which is also a challenging task [12]. FER
based on 2D+3D multi-modal data is becoming a promising
research direction due to that there exist large complementarity
among different modalities. Giving a complete survey for
3D FER is out the scope of this paper, readers are strongly
suggested to refer to the comprehensive survey [40] for the
issues of 3D and 4D face acquisition, dense correspondence,
alignment, tracking, available databases, as well as the details
of feature extraction, selection, classiÔ¨Åcation, and temporal
modeling for static and dynamic 3D facial expression recog-
nition.
B. Related works on 2D FER
Rifai et al. [39] designed a multi-scale contractive convolu-
tional network to learn hierarchical expression features which
are robust to the variations of factors like pose, identity, mor-
phology of the face. Tang [48] demonstrated the advantages
of replacing the softmax loss function of a deep CNN by
a linear SVM loss for 2D FER. Liu et al. [30] proposed a
uniÔ¨Åed Boosted Deep Belief Network framework to iteratively
optimizing the expression training process of feature learning,feature selection, and classiÔ¨Åer construction. Burkert et al. [2]
proposed a convolutional neural network (CNN) architecture
for 2D FER and claimed that it outperforms the earlier
proposed CNN based approaches. Liu et al. [29] designed a
3D CNN incorporating a deformable parts learning component
for dynamic expression analysis. The authors also proposed
the action unit inspired deep networks for 2D FER [28].
Khorrami et al. [20] showed both qualitatively and quanti-
tatively that CNNs can learn facial action units when doing
expression recognition, and their method achieved state-of-the-
art performance on the extended Cohn-Kanade (CK+) and the
Toronto Face Dataset (TFD). Kahou et al. [19] developed a
deep learning approach for emotion recognition in video. Their
method respectively trained a CNN for video and a deep belief
net for audio. ‚ÄúBag of moutm‚Äù features are also extracted to
further improve the performance. To fusion different models,
the ensemble weights are determined with random search. The
idea of ensemble multiple deep models has also been used in
Kim et al. [21]. This work trained 216 deep CNNs by varying
network architectures, input normalization, and weight initial-
ization and by adopting several learning strategies. Then, the
valid-accuracy-based exponentially-weighted decision fusion
method was proposed to ensemble different CNNs.
The work by Yu and Zhang [60] is probably the most related
work to ours. This method proposed to independently train
multiple differently initialized CNNs and output their training
responses. To combine multiple CNN models, they proposed
to learn the ensemble weighs of the network responses by
minimizing the log likelihood loss or hinge loss. Despite
with the same spirit of fusing deep models, our proposed
learning strategy differs from [60] signiÔ¨Åcantly. First, they
trained multiple CNNs by varying the network initialization,
while we only need to train a single CNN for different facial
attribute maps. As shown in our experiments (Section VI-D),
this kind of single network training can largely reduce both
computate time and memory consumption, while still preserve
the accuracy. Second, their method learned different weights
for different networks, thus corresponding to a learning-
based score-level fusion strategy, while ours corresponds to
a learning-based feature-level fusion strategy.
III. O VERVIEW OF THE PROPOSED APPROACH
Figure 1 illustrates the pipeline of the proposed DF-CNN
approach for 2D+3D FER. Given a set of preprocessed tex-
tured 3D face scans with different expressions, each of which
is Ô¨Årst represented as six types of 2D facial attribute maps
(see Section IV), including geometry map (3D coordinates),
three normal component maps (normal vectors), normalized
curvature map (principle curvatures), and texture map. Then,
these six facial attribute maps of each textured 3D face scan
are jointly fed into the feature extraction subnet (repetitions of
convolution, ReLU, and pooling layers) with sharing param-
eters, resulting in several hundreds of multi-channel feature
maps. All these feature maps are fed into the following feature
fusion subnet (including a reshape and two feature fusion
layers), leading to a highly concentrated facial representation
(32-dimensional fused deep feature). Finally, the softmax-loss
1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE
Transactions on Multimedia
4
Fig. 1. Pipeline of the proposed deep fusion CNN (DF-CNN) based multi-modal 2D+3D FER approach. Each textured 3D face scan is represented as six
types of 2D facial geometric and photometric attribute maps (i.e., 3D coordinates based geometry map, normal vectors based normal maps, principle curvatures
based curvature map, and texture map). These attribute maps are jointly fed into the feature extraction subnet of DF-CNN with sharing parameters, generating
hundreds of multi-channel feature maps. All these feature maps are then fed into the feature fusion subnet (including a reshape and two fusion layers) of
DF-CNN, resulting in a highly concentrated facial representation (32-dimensional fused deep feature). Finally, the softmax-loss layer is followed for network
training (see section V-A for details). The Ô¨Ånal expression label prediction is performed by two ways: learning linear SVM classiÔ¨Åers using the 32-dimensional
fused deep features or directly performing softmax prediction based on the 6-dimensional probability vectors.
or softmax layer is followed for network training or expression
prediction (see Section V-A for details).
For DF-CNN training, considering that there are very limit-
ed numbers of textured 3D face scans with expression labels,
the feature extraction subnet is initialized using the off-the-
shelf convolutional layers of a pre-trained deep model (e.g.,
vgg-net-m). This kind of pre-trained deep models have been
proven to have a good generalization ability for generic visual
recognition tasks [11], [38]. The feature fusion subnet is
randomly initialized, and the whole net is trained by the back-
prorogation algorithm using the softmax-loss function and the
stochastic gradient descent (SGD) algorithm.
For DF-CNN testing, six facial attribute maps of each
textured 3D face scan are jointly fed into the feature extraction
and feature fusion subnets, generating a highly concentrat-
ed facial representation (32-dimensional fused deep feature).
This deep feature is further transformed into a 6-dimensional
expression probability vector by the Ô¨Ånal softmax layer. Ex-
pression label prediction is preformed by training linear SVM
classiÔ¨Åers using the 32-dimensional fused deep features (i.e.,
DF-CNN svm) or directly performing softmax prediction based
on the 6-dimensional probability vectors (i.e., DF-CNN softmax ).
IV. A TTRIBUTE MAPS OF A TEXTURED 3D FACE
To comprehensively describe the geometric and photometric
attributes of a textured 3D face scan, six types of 2D fa-
cial attribute maps, namely the geometry map, texture map,
three normal maps, as well as normalized curvature map are
employed. Given a raw textured 3D face scan, we Ô¨Årst run
the preprocessing pipeline algorithm (see section VI-A) to
generate a 2D texture map Itand a geometry map Ig. The
coordinates information of each geometry map are then used to
estimate the surface normals and curvatures, resulting in three
normal component maps Ix
n,Iy
n, andIz
n, and one normalized
curvature (i.e. shape index) map Ic. Finally, a textured 3D face
scanIcan be described by six types of 2D facial attributemaps:I=fIg;Ix
n;Iy
n;Iz
n;Ic;Itg, as shown in Fig. 2. The
details for generation of normal maps and curvature map are
introduced as follows.
A. Normal maps
Given a normalized facial geometry map Igrepresented by
amn3matrix:
Ig= [pij(x;y;z )]mn = [pijk]mnfx;y;zg; (1)
wherepij(x;y;z ) = (pijx;pijy;pijz)T;(1im;1j
n;i;j2Z)represents the 3D coordinates of point pij:Let its
unit normal vector matrix (m n3) be
In= [n(pij(x;y;z ))]mn = [nijk]mnfx;y;zg; (2)
wheren(pij(x;y;z )) = (nijx;nijy;nijz)T;(1im;1
jn;i;j2Z)denotes the unit normal vector of pij:In this
paper, we utilize the local plane Ô¨Åtting method [17] to estimate
In. That is to say, for each point pij2Ig, its normal vector
n(pij)can be estimated as the normal vector of the following
local Ô¨Åtted plane:
Sij:nijxqijx+nijyqijy+nijzqijz=d; (3)
where (qijx;qijy;qijz)Trepresents any point within the local
neighborhood of point pijandd=nijxpijx+nijypijy+
nijzpijz:In this work, a neighborhood of 55window is
used. To simplify, each normal component in equation (2) can
be represented by an mnmatrix:
In=8
><
>:Ix
n= [nx
ij]mn;
Iy
n= [ny
ij]mn;
Iz
n= [nz
ij]mn:(4)
wherek(nx
ij;ny
ij;nz
ij)Tk2= 1:
1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE
Transactions on Multimedia
5
Fig. 2. Illustration of the six types of 2D geometric and photometric facial
attribute maps of six textured 3D face scans (subject F0001 in the BU-
3DFE dataset) with six prototypical facial expressions (i.e., anger, disgust,
fear, happiness, sadness, and surprise). The left hand column shows: the
geometry maps, texture maps, and curvature maps, and the three normal maps
(components x,y, andz) are shown at the right hand column.
B. Curvature map
Similar to the local plane Ô¨Åtting method used for normal
estimation, we explored the local cubic Ô¨Åtting method [14] to
estimate the principle curvatures. This method assumes that the
local geometry of a surface is approximated by a cubic surface
patch. For robustly solving the local Ô¨Åtting problem, both the
3D coordinates and the normal vectors of the neighboring
points of the point pij2Igto be estimated are used. That
is, we are Ô¨Åtting the following equations:
8
><
>:z(x;y) =a
2x2+bxy+c
2y2+dx3+ex2y+fxy2+gy3
zx=ax+by+ 3dx2+ 2exy +fy2
zy=bx+cy+ 3gy2+ 2fxy +ex2:
(5)
These equations can be solved by the least squares regression,
and the shape operator Scan be computed as:
S=0
@a b
b c1
A:
Then, the eignvalues of Sgive the two principle curvatures
1and2at pointpij2Ig. The normalized curvatures (i.e.,
shape index value) at this point is deÔ¨Åned by:
1
2 1
arctan1+2
1 2
: (6)
Figure 2 shows six types of 2D geometric and photometric
facial attribute maps of six textured face scans with six
prototypical facial expressions of subject F0001 in the BU-
3DFE database.
V. D EEPFUSION CONVOLUTIONAL NEURAL NETWORK
This section Ô¨Årst describes the architecture and training
details of DF-CNN. To intuitively highlight the discrimina-
tive ability of DF-CNN, both the highly concentrated 32-
dimensional fused deep features and the expression-speciÔ¨Åc
saliency maps are visualized.
A. DF-CNN: Architecture and Training
The architecture of DF-CNN is formed by a feature extrac-
tion subnet, a feature fusion subnet, and a softmax layer. The
feature extraction subnet is used to generate hierarchical and
over-completed facial representations (i.e., feature maps) foreach type of attribute maps. And the feature fusion subnet is
used to combine hundreds of feature maps from different types
of attribute maps into a highly concentrated deep feature. The
main building blocks of feature extraction subnet include the
convolutional layers and ReLU nonlinearity, while the reshape
layer and fusion layers are main components of feature fusion
subnet. The details of these components are introduced as
follows:
Convolutional Layer and ReLU Nonlinearity. A con-
volutional layer transforms a 3D volume of activation maps
(i.e., feature maps) to another through a set of learnable 3D
Ô¨Ålters. In particular, input a volume of activation maps of
the previous layer Yl 12RWl 1Hl 1Dl 1, andKl3D
Ô¨ÅltersfWl
kgKl
k=1, each with size Wl
fHl
fDl 1, it outputs
a 3D volume of activation maps Yl2RWlHlDlat layer
l. Let the convolutional stride be S, and the amount of zero
padding beP, then we have Wl= (Wl 1 Wl
f+2P )=S+1,
Hl= (Hl 1 Hl
f+ 2P )=S+ 1, andDl=Kl. Thek-th 2D
activation mapYl
kis denoted by
Yl
k='(Wl
kYl 1+bl
k); (7)
wherebl
k2Rdenotes the bias term of k-th Ô¨ÅlterWl
k,is
the convolution operator, and 'is the rectiÔ¨Åed linear units
(ReLU):'(x) = max(0 ;x).
Reshape Layer. This layer is used to concatenate all the
3D volumes of activation maps produced from all types of 2D
facial attribute maps. Suppose DF-CNN has Lconvolutional
layers in total, and acts on Ndifferent types of facial attribute
maps, then the reshape layer operation is deÔ¨Åned as:
YL
Re=Reshape(fYL(Ii)gN
i=1) =[YL(I1)j;;jYL(IN)]
2RWLHL(K LN);
(8)
whereIiisi-th type of facial attribute maps, and the notation
[j;;j]denotes the concatenation of 3D matrices along
feature channel dimension.
Feature Channel Fusion Layer. This is a fully connected
layer, which is used to fuse all the activation volumes extracted
from all types of facial attribute maps in feature channel
dimension. Let this feature channel fusion layer be the (L+1)-
th layer, and its input be the output of the reshape layer YL
Re,
which is fully connected with KL+1 3D Ô¨ÅltersfWL+1
kgKL+1
k=1,
each with size 11(KLN), then the output of this
layer isYL+12RWL+1HL+1DL+1. HereWL+1 =WL,
HL+1=HL, andDL+1=KL+1. Thek-th 2D activation
mapYL+1
kis denoted by
YL+1
k='(WL+1
kYL
Re+bL+1
k); (9)
wherebL+1
k2Rdenotes the bias term of k-th Ô¨ÅlterWL+1
k.
That is to say, to achieve an activation volume YL+1with
much smaller number of feature channels, the number of Ô¨Ålters
KL+1 should be much smaller than the number of feature
channels in the previous activation volume YL
Re.
Spatial Dimension Fusion Layer. This is also a fully
connected layer, which is used to fuse the activation volume
YL+1in the height-width spatial dimension. Let this fusion
layer be the (L+ 2) -th layer, and its input be the output
volume of feature channel fusion layer YL+1, which is fully
1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE
Transactions on Multimedia
6
Fig. 3. The architecture of the proposed deep fusion convolutional neural network (DF-CNN). Six types of facial attribute maps of a textured 3D face model
are jointly fed into Ô¨Åve feature convolutional layers (convolution + ReLU + Pooling), a reshape layer, a feature channel fusion layer, a spatial dimension
fusion layer, and a Ô¨Ånal softmax layer. The sizes and numbers of input data, feature maps and Ô¨Ålters are listed for each layer.
connected with KL+2 3D Ô¨ÅltersfWL+2
kgKL+2
k=1, each with size
ofWL+2
k2RWL+1HL+1KL+1, then the output activation
feature of this layer is YL+22R11K L+2. Thek-th 2D
valueYL+2
kis denoted by
YL+2
k='(WL+2
kYL+1+bL+2
k); (10)
wherebL+2
k2Rdenotes the bias term of k-th Ô¨ÅlterWL+2
k.
Softmax Layer. GivenKpossible expression classes, the
softmax layer has Knodes denoted by pi, wherei=
1;2;;K.pispeciÔ¨Åes a discrete probability distribution
of expressions, therefore,PK
i=1pi= 1. LetYL+2be the
output of spatial dimension fusion layer, and fWL+3
k2
R11KL+2gK
k=1beKweights fully connecting spatial dime-
sion fusion layer to softmax layer. Then the total input into a
softmax layer, denoted by YL+3, is
YL+3
k=WL+3
kYL+2+bL+3
k2R; (11)
then we have
pi=exp(YL+3
k)P6
jexp(YL+3
j): (12)
The predicted expression class ^iwould be
^i= arg maxipi: (13)
In practice, considering that there are very limited numbers
of 3D face scans with expression labels, we use the con-
volutional architecture and parameters of a pre-trained deep
CNN model to build and initialize the convolutional layers.
In particular, we choose vgg-net-m [4] as the pre-trained deep
model since it performs well and involves moderate amount of
parameters. In principle, other pre-trained deep CNN models
or newly designed deep CNN models are also possible to be
used if enough numbers of training samples are available. The
parameters of fusion layers and softmax layer are randomly
initialized. The detailed architecture of DF-CNN, including
the sizes and numbers of Ô¨Ålters and activation maps for each
layer, is illustrated in Fig. 3.
As shown in Figure 3, DF-CNN comprises Ô¨Åve convolu-
tional layers, a reshape layer, two fusion layers, and a softmax
layer. Moreover, ReLU neuron is used after all convolutional
layers and feature fusion layers. The max pooling layer is used
following the Ô¨Årst, second, and the Ô¨Åfth convolutional layers.
And Local Response Normalization (LRN) layer is used before
the Ô¨Årst and second pooling layers.
Each 2D facial attribute map is converted to color scale
and resized to 2242243, and then all six types of
attribute maps of each textured 3D face scan are jointly fedinto feature extraction subnet of DF-CNN, generating six
activation volumes, each with size of 66512. These six
activation volumes are concatenated and reshaped into size
of663;072 by reshape layer (Reshape6 in Fig. 3).
The reshaped activation volumes are fused by the following
feature channel fusion layer (Fusion7 in Fig. 3), resulting in
an activation volume with size of 6632. This activation
volume is further fused by spatial dimension fusion layer
(Fusion8 in Fig. 3), generating a highly concentrated facial
representation (i.e., 32-dimensional fused deep feature). This
fusion layer is followed by another fully connected layer,
which outputs a 6-dimensional expression probability vector.
Finally, a softmax loss layer is used to train all the parameters
of DF-CNN based on the back-propagation algorithm.
During training, the weight decay parameter is set to 5e-
4. The learning rate and momentum parameters are set to
1e-4 and 0.9, respectively. The open source implementation
MatConvNet1is used to build DF-CNN. During testing, ex-
pression label of a textured 3D face scan is predicted by
two ways: 1) training linear SVM classiÔ¨Åers using the highly
concentrated 32-dimensional deep features (i.e., DF-CNN svm).
2) performing softmax prediction based on the 6-dimensional
vectors of expression probabilities (i.e., DF-CNN softmax ).
B. DF-CNN: Deep Feature Visualization
To have an intuitive impression and gain insight into the
discriminative ability of DF-CNN, we visualize both the
‚Äúlow-level‚Äù and ‚Äúhigh-level‚Äù deep features extracted from the
Ô¨Årst convolution layer and the last fusion layer of DF-CNN,
respectively. Figure 3 shows that there are totally 96 3D
Ô¨Ålters in the Ô¨Årst convolution layer, thus we can generate
96 ‚Äúlow-level‚Äù feature maps for each type of facial attribute
maps. Figure 4 illustrates 11 typical feature maps for each
type of facial attribute maps of a textured 3D face scan with
happiness expression. From this Ô¨Ågure, we can see that diverse
feature maps can be extracted from DF-CNN using different
Ô¨Ålters and different attribute maps. Moreover, each feature
map looks similar to conventional gradient-like facial maps
extracted from the shadow handcrafted features (e.g., LBP and
Gabor face maps in [25]). Such a large number of feature
maps can comprehensively capture various expression-related
facial shape or texture deformations, of course with very high
dimensions. Therefore, how to combine such a large number
of over-completed and redundant deep representations into a
single compact facial representation becomes the key issue to
1http://www.vlfeat.org/matconvnet/
1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE
Transactions on Multimedia
7
Fig. 4. Visualization of 11 typical feature maps of geometric and photometric facial attribute maps extracted from the Ô¨Årst convolution layer of DF-CNN.
From top to bottom are the feature maps for the geometry map, texture map, curvature map, and normal maps with components x,y, andz.
  
Happiness
Surprise
Fear
Anger
Disgust
Sadness
  
Fear
Happiness
Anger
Sadness
Disgust
Surprise
  
Sadness
Surprise
Happiness
Fear
Anger
Disgust
(a)
Handcrafted feature (b) Pre-trained deep feature (c) Fused deep feature by DF-CNN
Fig.
5. Comparison of the clustering structures of t-SNE based 2-dimensional embedding of the handcrafted feature (i.e., Gabor), pre-trained deep feature
(i.e., vgg-net-m-conv5), and 32-dimensional fused deep feature learned by DF-CNN associated with six prototypical facial expressions.
be solved. Fortunately, DF-CNN is designed to handle this
problem, and providing us a high-level, low-dimensional, and
high discriminative facial representation.
To highlight the high discriminative property DF-CNN, Fig-
ure 5 visualizes the clustering structures of t-SNE [51] based
2-dimensional embedding of handcrafted feature, pre-trained
deep feature, and 32-dimensional fused deep feature associated
with six prototypical facial expressions. In particular, the same
features (i.e., Gabor, vgg-net-m-conv5, and 32-dimensional
fused deep feature) are used as those in Section VI-B. Notice
that for Gabor and vgg-net-m-conv5, the features of different
attribute maps are concatenated together (i.e., feature-level
fusion) to generate a single high-dimensional representation of
each textured 3D face scan. The feature dimensions of Gabor
andvgg-net-m-conv5 are 40,320 (6 attribute maps, each one is
described as a 6,720-dimensional Gabor feature) and 110,592
(6 attribute maps, each one is described as feature maps with
size 66512 ), respectively. Figure 5 shows that the
32-dimensional fused deep feature has an obvious clustering
structure for different expression categories, while other two
types of features demonstrate large category-wised overlap-
ping. This clearly indicates that the 32-dimensional fused deep
features learned by DF-CNN has more discriminative powerto distinguish different expressions than handcrafted feature
Gabor and pre-trained deep feature vgg-net-m.
C. DF-CNN: Saliency Map Visualization
Since different facial expressions relate to different ways of
local facial shape deformations, the importance weights of d-
ifferent facial parts are generally quite different for expression
predicting as shown in [31], [69]. In this section, we show
that the importance weights can be revealed by pixel-level
expression related saliency maps of DF-CNN.
To this end, we visualize the importance of each pixel for
its Ô¨Ånal discrimination ability of different facial expressions.
For example, for ‚Äúhappiness‚Äù, we visualize the saliency map
for a textured 3D face by the importance of each image
pixel contributing to the Ô¨Ånal discrimination of ‚Äúhappiness‚Äù.
To compute the saliency map of a textured 3D face scan
I=fIg;Ix
n;Iy
n;Iz
n;Ic;Itgw.r.t. an expression indexed by
e, we construct a score function for assigning this face to
expressioneby:
S(Ije;) =weTf(I;); (14)
where is the set of learned parameters (i.e., Ô¨Ålters and
biases) of DF-CNN, f(I;)is the 32-dimensional fused
1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE
Transactions on Multimedia
8
Fig. 6. Visualization of the DF-CNN based facial expression saliency maps.
From top to bottom rows: saliency maps for anger, disgust, fear, happiness,
sadness and surprise. The less important pixels are shown in dark blue.
deep feature of I, andweis the weight of a trained SVM
classiÔ¨Åer for expression eusingf(I;). Obviously, the high-
er value ofweTf(I;)implies higher conÔ¨Ådence in labeling
this textured 3D face as expression e. We next compute the
gradient of score function in Eqn. (14) w.r.t. the input pixels:
G(xjI ;e;) =X
I2IweT@f(I;)
@I(x)(15)
wherexdenotes any pixel of an attribute map.@f(I;)
@I(x)is
the gradient of fused deep feature w.r.t. the attribute map I
at pixelx, which can be computed by the back-propagation
algorithm of DF-CNN from the spatial dimension fusion layer.
Its absolute value jG(xjI ;e;)j measures the importance
of pixelxin labeling Ias expression e. We call this term
computed over all pixels of all facial attribute maps of a
textured 3D face scan as saliency map.
Figure 6 visualizes some examples of saliency maps for
different expressions. The saliency map is re-scaled to [0,
1]. We visualize it by fusing the face texture map with a
dark blue background using the saliency map as weights. The
less important pixels are shown in dark blue in these maps.
We observe some interesting phenomena from these maps.
First, mouth is the most salient facial part for discriminating
all these expressions of interest, particularly for sadness and
surprise. Second, the distributions of those salient maps for all
expressions are approximately consistent with the patterns of
facial shape deformations, which may spread over the whole
faces with different importance. These observations indicate
that the proposed DF-CNN can provide a discriminative facial
representation and can distinguish facial expressions using the
discriminative facial parts.
Fig. 7. Samples of 2D texture maps of BU-3DFE database with different
genders, ethnicities, ages, expressions (from left to right, anger, disgust, fear,
happiness, sadness and surprise) and levels of expression intensity (from top
to bottom: level 1 to level 4).
VI. E XPERIMENTAL EVALUATION
To evaluate the effectiveness of DF-CNN for multi-modal
2D+3D FER, we will compare its performance with popular
handcrafted features, pre-trained deep features, Ô¨Åne-tuned deep
features, and state-of-the-art methods over three expression
subsets of two 3D face datasets (i.e., BU-3DFE and Bospho-
rus). Finally, we will discuss the issues of feature extraction
with or without parameter sharing, effectiveness of learning-
based fusion, and optimality of linear SVM based expression
prediction.
A. Databases and Preprocessing
BU-3DFE Database. The BU-3DFE (Binghamton Uni-
versity 3D Facial Expression) Database [59] has been the
benchmarking for static 3D FER [12]. It includes 100 subjects
(56 females and 44 males), with age ranging from 18 to 70
years old, and with a variety of racial ancestries (e.g., White,
Black, East-Asian). Each subject has 25 samples of seven
expressions: one sample for neutral, and other 24 samples for
six prototypical expressions (anger, disgust, fear, happiness,
sadness, and surprise), each includes four levels of intensity
(see Fig. 7). As a result, this database consists of 2,500
2D texture images and 2,500 geometric shape models. To
fairly compare DF-CNN with state-of-the-art methods, and to
validate the effectiveness of DF-CNN for samples with lower
levels of expression intensity, the following two subsets are
used.
BU-3DFE Subset I. This subset is the standard dataset
used for 3D FER. It contains 1,200 2D and 3D face
pairs (i.e., 7,200 2D facial attribute maps) of 100 subjects
with 6 prototypical expressions and two higher levels of
expression intensity.
BU-3DFE Subset II. This subset includes all samples
of BU-3DFE except the 100 neutral samples. It contains
2,400 2D and 3D face pairs (i.e., 14,400 2D facial
1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE
Transactions on Multimedia
9
attribute maps) of 100 subjects with 6 prototypical ex-
pressions of four levels of intensity. To our knowledge,
the samples with lower levels of expression intensity have
not been used for 3D FER.
Bosphorus 3D Face Database. The Bosphorus 3D Face
Database [41] has been widely used for 3D face recognition
under adverse conditions, 3D facial action unit detection, 3D
facial landmarking, etc. It contains 105 subjects and 4,666
pairs of 3D face models and 2D face images with different
action units, facial expressions, poses and occlusions. In
this dataset, there are totally 65 subjects performing the six
prototypical expressions with near frontal view. Each person
has only one 2D (or 3D) sample for each expression, resulting
in 390 2D and 3D face pairs. To better partition, we use the
following subset for experimental evaluations.
Bosphorus Subset. It contains 360 2D and 3D face pairs
(i.e., 2,160 facial attribute maps) of 60 subjects with 6
prototypical expressions.
Preprocessing. We performed similar preprocessing for both
BU-3DFE subsets and Bosphorus subset. First, we used the It-
erative Closest Point algorithm for 3D face registration. Then,
we performed nose detection, face cropping, re-sampling,
and projection procedures using the 3D face normalization
method proposed in [34]. Finally, we achieved the normalized
2D range images (i.e., geometry maps) with x,y, andz
coordinates. Once we have geometry maps, other geometric
facial attribute maps can be estimated according to the method
introduced in Section IV. The 2D texture maps of BU-3DFE
dataset are generated by projecting 3D texture images with
linear interpolation. Samples of preprocessed facial attribute
maps of BU-3DFE database are shown in Fig. 2. And Figure 8
illustrates some samples of 2D texture images of Bosphorus
subset.
B. Evaluation and comparison on BU-3DFE Subset I
Experimental Protocol. This experimental protocol is Ô¨Årst-
ly used in [15] and has been proven to be more stable than the
one used in [53]. In this protocol, 60 subjects, each with 12
samples (i.e., 6 prototypical expressions with two higher levels
of intensity) are randomly selected from the BU-3DFE subset
I. That is to say, 720 textured 3D face scans (i.e., 4,320 2D
facial attribute maps) are used. To achieve stable results, 1,000
times random and independent 54-versus-6-subject-partition
experiments (1,000 times train and test sessions in total) are
performed. For each partition, 648 textured 3D face scans of
54 subjects are used for training and 72 textured 3D face
scans of 6 subjects are used for testing. Different partitions are
independently trained and tested, and the average expression
recognition accuracy of all the 1,000 test sessions across all 6
prototypical expressions are reported for the Ô¨Ånal evaluation.
In particular, we use the remaining 40 subjects (i.e., 2,880
2D facial attribute maps) of BU-3DFE Subset I to train our
DF-CNN. Once DF-CNN is trained, it is then used to extract
the 32-dimensional fused deep features of the other 60 subject-
s. These fused deep features are then used to train linear SVM
classiÔ¨Åers for expression prediction using above 1,000 times
54-versus-6-subject-partition experiments (i.e., DF-CNN svm).TABLE II
COMPARISON OF THE AVERAGE ACCURACIES WITH HANDCRAFTED
FEATURES ON BU-3DFE S UBSET I.
Method Ig Ix
n Iy
n Iz
n Ic It All
MS-LBP 76.47 76.77 77.87 76.41 77.70 71.65 81.74
dense-SIFT 80.29 79.97 82.35 80.95 80.28 75.56 83.16
HOG 81.89 82.09 80.58 81.81 77.95 78.11 83.74
Gabor 77.95 78.80 81.97 81.10 81.65 80.36 84.72
DF-CNN svm - - - - - - 86.86
DF-CNN softmax - - - - - - 86.20
Alternatively, expression labels of the other 60 subjects are
also predicted directly by the softmax layer of the trained
DF-CNN (i.e., DF-CNN Softmax ). It‚Äôs important to note that
result of this one-time prediction is very close to (86.20% vs.
86.25%) the one achieved by predicting expression label using
maximum value of the 6-dimensional expression probabilities
with the same 1,000 times 54-versus-6 experimental protocol.
1)Comparison with handcrafted features: This para-
graph compares the performance of DF-CNN with the ones
achieved by using handcrafted features. Four classical hand-
crafted image features: MS-LBP, dense-SIFT, HOG, and Ga-
bor, which have been proven to be quite efÔ¨Åcient for both 2D
and 3D facial expression analysis, are employed for compar-
isons. Please refer to [25], [52], [22], and [25], respectively for
the implementations of these features. When used for multi-
modal 2D+3D FER, these features are Ô¨Årst extracted from each
type of facial attribute maps, then respectively fed into linear
SVM2classiÔ¨Åer with default parameter of C. To achieve Ô¨Ånal
results, score-level fusion of SVM scores with sum rule is
used.
Table II shows the average expression recognition accu-
racies across all six expressions of four handcrafted fea-
tures, and the proposed DF-CNN on BU-3DFE subset I.
From Table II, we can conclude that: 1) Gabor and HOG
generally perform better than dense-SIFT and MS-LBP. In
particular, Gabor achieves the highest fusion accuracy of
84.72%, which outperforms HOG, dense-SIFT, and MS-LBP
by 0.98%, 1.56%, and 2.98%, respectively. 2) For different
facial attribute maps, normal maps (Ix
n,Iy
n, andIz
n) generally
perform better than others, and the fusion of all six attribute
maps (i.e., All) achieves the best performance. These results
indicate that different facial attribute maps indeed contain large
complementary information for multi-modal 2D+3D FER. 3)
DF-CNN svmand DF-CNN softmax achieves similar and much
better results (86.86% vs. 86.20%) than handcrafted features.
2)Comparison with pre-trained deep features: This
paragraph compares the performance of DF-CNN with the
ones achieved by using deep features extracted from three deep
models (i.e., caffe-alex, vgg-net-m, and vgg-net-16 ) pre-trained
on the ImageNet database [4]. Notice that the convolutional
layers of vgg-net-m is used to initialize our DF-CNN. Similar
to the case of handcrafted features, each type of facial attribute
maps are separately fed into these pre-trained models to
2http://www.csie.ntu.edu.tw/ cjlin/liblinear/
1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE
Transactions on Multimedia
10
TABLE III
COMPARISON OF THE AVERAGE ACCURACIES WITH PRE -TRAINED DEEP
FEATURES ON BU-3DFE S UBSET I.
Method Ig Ix
n Iy
n Iz
n Ic ItAll
caffe-alex-conv5 77.53 78.87 81.50 78.71 80.83 81.40 83.74
vgg-net-m-conv5 80.38 80.37 81.68 81.23 79.23 82.14 84.22
vgg-net-16-conv5-3 81.72 78.55 83.06 81.25 76.95 78.46 83.78
caffe-alex-full7 68.64 73.43 76.64 75.72 74.52 74.45 82.56
vgg-net-m-full7 73.34 74.99 77.51 76.77 68.81 70.93 81.56
vgg-net-16-full7 76.71 72.22 73.87 74.61 64.35 67.03 82.45
DF-CNN svm - - - - - - 86.86
DF-CNN softmax - - - - - - 86.20
TABLE IV
COMPARISON OF THE AVERAGE CONFUSION MATRICES WITH GABOR AND
PRE-TRAINED DEEP FEATURE FOR ALL FACIAL ATTRIBUTE MAPS ON
BU-3DFE S UBSET I.
Gabor (average accuracy = 84.72)
% AN DI FE HA SA SU
AN 85.53 1.67 0.93 0 11.88 0
DI 1.63 84.48 6.19 4.35 0 3.36
FE 3.56 6.24 65.98 12.70 4.34 7.18
HA 0 0.83 3.03 96.14 0 0
SA 18.70 0 1.20 0.95 79.15 0
SU 0 1.26 1.67 0 0.04 97.03
vgg-net-m-conv5 (average accuracy = 84.22)
% AN DI FE HA SA SU
AN 86.96 1.68 0.83 0 10.53 0
DI 1.88 80.43 8.47 4.29 0 4.93
FE 3.23 9.53 66.41 12.83 2.10 5.91
HA 0 0.25 3.48 96.27 0 0
SA 19.82 0 2.83 0.39 76.96 0
SU 0 0.04 1.67 0 0.01 98.28
DF-CNN svm(average accuracy = 86.86)
% AN DI FE HA SA SU
AN 82.08 3.60 2.42 0 11.90 0
DI 3.27 84.94 5.70 2.50 0 3.59
FE 1.84 5.28 79.24 8.33 0.81 4.50
HA 0 0 3.74 96.26 0 0
SA 12.63 0.10 5.56 0.53 81.18 0
SU 0 0.07 1.67 0 0.83 97.43
extract deep features, and then linear SVM classiÔ¨Åers are
trained for expression classiÔ¨Åcation. The Ô¨Ånal fusion results
are achieved by performing score-level fusion of SVM scores
with sum rule. For comparisons, deep features extracted from
the 5th convolutional layer (net-conv5) and the penultimate
fully connected layer (net-full7 ) are used for each type of facial
attribute maps.
Table III shows the average expression recognition accura-
cies of pre-trained deep features and DF-CNN on BU-3DFETABLE V
COMPARISON OF THE AVERAGE ACCURACIES WITH FINE -TUNED DEEP
FEATURES ON BU-3DFE S UBSET I.
Method Ig Ix
n Iy
n Iz
n Ic ItAll
caffe-alex-ft-full7 svm 79.44 79.84 80.51 79.50 79.46 80.83 84.05
vgg-net-m-ft-full7 svm 79.68 82.85 82.15 80.30 82.01 81.62 84.85
vgg-net-16-ft-full7 svm 80.21 82.30 82.04 80.43 80.87 84.10 86.01
caffe-alex-ft softmax 78.19 80.96 81.94 78.75 78.89 80.83 83.61
vgg-net-m-ft softmax 78.33 83.06 82.78 81.11 81.11 80.42 85.00
vgg-net-16-ft softmax 78.33 82.08 80.69 79.19 79.31 84.17 85.14
DF-CNN svm - - - - - - 86.86
DF-CNN softmax - - - - - - 86.20
subset I. From Table III, we can Ô¨Ånd that: 1) Different pre-
trained deep features have different superiorities associated
with different facial attribute maps. For example, vgg-net-
16-conv5-3 achieves the best score for Iy
n, while vgg-net-m-
conv5 performs best for Ix
n. 2) For the fusion scores, vgg-
net-m-conv5 andcaffe-alex-full7 achieve slightly better results
than others among pre-trained deep features. 3) The deep
features extracted from convolutional layers (i.e., conv5) of
pre-trained deep models generally perform much better than
the ones extracted from fully connected layers (i.e., full7). 4)
Our method achieves consistently better results than all pre-
trained deep features. Notice that the dimension of vgg-net-m-
conv5 for one type of facial attribute maps is 18,432, which
is much higher than the 32-dimensional fused deep feature
produced by DF-CNN.
Table IV compares the average confusion matrices achieved
by Gabor feature, vgg-net-m-conv5 and DF-CNN svm. It can be
seen that DF-CNN svmoutperforms Gabor for all expressions
except anger (with a difference of 3.45%). It is worth noting
that DF-CNN svmhas more powerful discriminative ability
to distinguish fear expression, promoting the accuracy upto
13.26% and 12.83% for Gabor feature and vgg-net-m-conv5.
3)Comparison with Ô¨Åne-tuned deep models: To further
demonstrate the effectiveness of DF-CNN, we also compared
it with Ô¨Åne-tuned deep models. The same pre-trained deep
models: caffe-alex, vgg-net-m, and vgg-net-16 are used for
Ô¨Åne-tuning. For each pre-trained deep model, we keep the
net architecture of all layers and parameters unchanged except
the Ô¨Ånal fully connected layer. In particular, since we have six
expression classes, the Ô¨Ålter weight with size of 114096
1000 is changed to 1140966and randomly initialized,
and the corresponding 1000-dimensional bias vector is also
replaced by a 6-dimensional zero vector. Then, we separately
Ô¨Åne-tune the pre-trained deep models using different facial
attribute maps, resulting in six Ô¨Åne-tuned deep models for each
pre-trained deep model. Finally, testing data associated with
each kind of attribute maps are fed into the corresponding
Ô¨Åne-tuned deep model for feature extraction. Similar to DF-
CNN, expression prediction for each kind of attribute maps
is achieved by the following two ways: 1) learning linear
SVM classiÔ¨Åers using the 4,096-dimensional Ô¨Åne-tuned deep
features (e.g., vgg-net-m-ft-full7 svm); 2) performing softmax
1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE
Transactions on Multimedia
11
TABLE VI
COMPARISON OF THE AVERAGE CONFUSION MATRICES WITH FINE -TUNED
DEEP MODEL FOR ALL FACIAL ATTRIBUTE MAPS ON BU-3DFE S UBSET I.
vgg-net-m-ft-full7 svm(average accuracy = 84.85)
% AN DI FE HA SA SU
AN 81.48 1.92 0.83 0 15.77 0
DI 1.95 81.91 8.58 3.14 0 4.42
FE 3.42 6.96 73.51 11.57 1.99 2.56
HA 0 0.77 3.48 95.74 0 0
SA 15.88 0.19 4.18 0 79.75 0
SU 0 0.83 1.67 0 0.78 96.72
vgg-net-16-ft-full7 svm(average accuracy = 86.01)
% AN DI FE HA SA SU
AN 86.19 2.52 0.83 0 10.45 0
DI 1.97 82.00 9.59 2.27 0 4.17
FE 2.73 7.42 74.38 12.99 0.79 1.68
HA 0 0.83 3.47 95.69 0 0
SA 16.21 0 4.29 0 79.50 0
SU 0 0 1.67 0 0.06 98.27
DF-CNN svm(average accuracy = 86.86)
% AN DI FE HA SA SU
AN 82.08 3.60 2.42 0 11.90 0
DI 3.27 84.94 5.70 2.50 0 3.59
FE 1.84 5.28 79.24 8.33 0.81 4.50
HA 0 0 3.74 96.26 0 0
SA 12.63 0.10 5.56 0.53 81.18 0
SU 0 0.07 1.67 0 0.83 97.43
prediction using the 6-dimensional Ô¨Åne-tuned deep features of
expression probabilities (e.g., vgg-net-m-ft softmax ). To achieve
fusion results, score-level fusion with sum rule are used for
both cases.
Table V shows the average expression recognition accura-
cies of Ô¨Åne-tuned deep features and DF-CNN on BU-3DFE
subset I. From Table V, we can Ô¨Ånd that: 1) Fusion of
multiple facial attribute maps can also signiÔ¨Åcantly improve
the accuracies for all Ô¨Åne-tuned deep features. 2) Fine-tuned
deep feature vgg-net-16 achieves signiÔ¨Åcantly better results for
texture maps, and also achieves the highest accuracies (86.01%
and 85.14%) for both two prediction ways. This conclusion of
deeper net performs better is consistent with the one in [4]. 3)
Our DF-CNN initialized by vgg-net-m still achieves the best
results. It‚Äôs necessary to compare the results of Table III and
Table V. It‚Äôs easy to Ô¨Ånd that signiÔ¨Åcant improvements have
been achieved from pre-trained to Ô¨Åne-tuned deep features,
particularly for the case of 4096-dimensional deep features
extracted from the penultimate fully connected layer (i.e.,
full7). For example, the improvements are upto 16.52% for
curvature maps and 17.07% for texture maps when considering
the pre-trained and Ô¨Åne-tuned vgg-net-16-full7.
Table VI compares the average confusion matrices achieved
by two Ô¨Åne-tuned deep features: vgg-net-m-ft-full7 svm,vgg-net-
16-ft-full7 svm, and our DF-CNN svm. It‚Äôs not difÔ¨Åcult to see thatDF-CNN svmachieves consistent better results than vgg-net-m-
ft-full7 svmfor all six expressions. It even achieves better results
than vgg-net-16-ft-full7 svm, which is Ô¨Åne-tuned from a much
deeper pre-trained deep model. In particular, the superiority
for fear expression is upto 4.86% .
4)Comparison with other methods: To comprehensively
evaluate the effectiveness of DF-CNN, we compared it with
18 state-of-the-art methods on BU-3DFE subset I. To give a
thoroughly analysis, four aspects, including the data modal-
ity, expression feature, expression classiÔ¨Åer, and recognition
accuracy are compared in Table VII.
1)For data modality, we can see that all previous methods
reported their results using only 3D data exception of [24]
and [66]. It is worth noting that Li et al. [24] proposed
a local feature-based multimodal 2D+3D FER method, and
studied the complementarity between 2D and 3D features.
However, their fusion results were produced by handcrafted
feature-level and score-level fusion schemes. In contrast, our
method can automatically combine different 3D geometric and
2D photometric maps into a single 32-dimensional fused deep
feature.
2)For expression feature, one way is directly building
histograms of surface geometric quantities, such as coordinates
(e.g., [66], [67]), normals (e.g., [24], [26], [67]), and curvatures
(e.g., [24], [26], [53], [66], [67]). Another way is extracting
popular handcrafted features (e.g., HOG, SIFT, LBP, DWT)
from depth maps (e.g., [1], [15], [36], [57]), normal maps (e.g.,
[23], [36], [57], [22]), or curvature maps (e.g., [36], [57], [63]).
As mentioned in our introduction section, all these state-of-
the-art works for 3D-FER are based on handcrafted expression
features. In contrast, our method can learn highly concentrated
and discriminative facial representation (only 32-dimensional)
from six types of facial attribute maps.
3)For expression classiÔ¨Åer, SVM (e.g., [1], [15], [26]) is the
most popular classiÔ¨Åer compared with others such as Neural
Networks (NN), Maximal Likelihood (ML), Bayesian Belief
Net (BBN), multi-boosting, and Sparse Representation-based
ClassiÔ¨Åer (SRC). It is worth noting that a majority of methods
are based on SVM classiÔ¨Åer with non-linear RBF kernel (e.g.,
[26], [57], [63], [67]) or using multiple kernel learning [23] to
combine multiple high-dimensional features (e.g., normal-LBP
in [23]), while our results are based on linear SVM classiÔ¨Åer
with default parameter.
4)For recognition accuracy, beneÔ¨Åting from the end-to-
end training framework of DF-CNN, the fused deep features
produced by DF-CNN have strong discriminative ability to
distinguish different expressions. In particular, our method
(DF-CNN svm) achieves the highest accuracy of 86.86% com-
pared with all state-of-the-art methods using the same (or
very similar [1]) experimental protocol. Notice that both the
experimental protocol used in our paper [15] and the similar
one used in [1] have been proved stable since the scores are
achieved by averaging 100 times independent 10-fold cross-
validation tests. It should be pointed out that directly compar-
isons of the two accuracy columns in Table VII are far from
fair since the results listed in the second column were achieved
based on an unstable experimental protocol (i.e., 10-fold or
20-fold cross-validation) Ô¨Årstly used in [53]. For example, the
1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE
Transactions on Multimedia
12
TABLE VII
COMPARISON OF EXPRESSION FEATURES ,CLASSIFIERS ,AND ACCURACIES
WITH THE STATE -OF-THE-ART ON BU-3DFE S UBSET I (N OTICE THAT
THE ACCURACIES IN THE LEFT COLUMN ARE ACHIEVED BY AVERAGING
100 ROUND INDEPENDENT 10-FOLD CROSS -VALIDATION TESTS ,WHILE
THE ONES IN THE RIGHT COLUMN ARE ACHIEVED BY AVERAGING ONLY
ONE OR TWO ROUND 10-FOLD CROSS -VALIDATION TESTS ).
Methods Data Feature ClassiÔ¨Åer Accuracy
Wang et al. [53] 3D curvatures/hist. LDA 61.79 83.60
Soyel et al. [44] 3D points/distance NN 67.52 91.30
Soyel et al. [45] 3D points/distance NN - 93.72
Tang et al. [46] 3D points/distance LDA 74.51 95.10
Tang et al. [47] 3D slopes, distance SVM - 87.10
Mpiperis [35] 3D deformable model ML - 90.50
Gong et al. [15] 3D depth/PAC SVM 76.22 -
Berretti et al. [1] 3D depth/SIFT SVM 77.54 -
Maalej et al. [33] 3D facial curvesmuiti-
boosting-98.81
92.75
Li et al. [26] 3D normals, curv./hist. SVM 82.01 -
Li et al. [23] 3D normals/LBP MKL 80.14 -
Lemaire [22] 3D curvature/HOG SVM 76.61 -
Ocegueda [36] 3Dcoordinates, normals
curvatures/DWTLogistic
Reg.- 90.40
Zeng et al. [63] 3D curvatures/LBP SRC 70.93 -
Zhen et al. [67] 3Dcoordinates, normals,
shape indexSVM 84.50 -
Yang et al. [57] 3Ddepth, normals,
curv./scatteringSVM 84.80 -
Zhao et al. [66] 2D+3Dintensity,coordinates,
shape index/LBPBBN - 82.30
Li et al. [24] 2D+3DmeshHOG/SIFT
meshHOS/HSOGSVM 86.32 -
DF-CNN svm 2D+3D 32-D deep feature SVM 86.86 -
DF-CNN softmax 2D+3D 6-D deep feature Softmax 86.20 -
accuracy of [33] is reduced from 98.81% to 92.75% when
using 20-fold instead of 10-fold cross-validation. As produced
by Gong et al. [15], the accuracies of [53], [44], [46] were
dropped signiÔ¨Åcantly (more than 20%) when using a more
stable experimental protocol. Overall, different from state-
of-the-art methods, the proposed DF-CNN combines feature
learning and fusion learning into a single end-to-end training
framework, and achieves the best accuracy for multimodal
2D+3D FER under the more stable experimental protocol.
C. Evaluation and comparison on other datasets
This section will show more experimental results evaluated
on BU-3DFE subset II and Bosphorus subset.
Experimental Protocol. To get more training data and to
reduce the effect of data bias for DF-CNN training, we used
the standard 10-fold cross-validation (10 train and test ses-
sions) experimental setting. That is, different DF-CNNs should
be trained for different sessions, and the average recognition
accuracies of 10 different DF-CNNs across all six prototypical
expressions are reported for evaluations and comparisons. InTABLE VIII
COMPARISON OF THE AVERAGE ACCURACIES WITH HANDCRAFTED
FEATURES ,PRE-TRAINED DEEP FEATURES ,AND FINE -TUNED DEEP
FEATURES ON BU-3DFE S UBSET II.
Method Ig Ix
n Iy
n Iz
n Ic ItAll
MS-LBP 73.50 74.58 73.54 73.21 73.37 66.08 77.75
dense-SIFT 76.25 75.79 77.42 76.58 75.88 71.79 79.42
HOG 76.25 76.88 76.29 77.75 76.29 72.04 79.71
Gabor 73.04 75.00 78.29 76.42 76.33 75.86 80.00
vgg-net-m-conv5 76.17 75.04 76.92 76.54 75.54 76.42 79.75
vgg-net-m-full7 70.21 69.71 72.67 70.67 67.00 66.83 77.38
vgg-net-m-ft-full7 svm 75.17 76.62 77.08 75.83 78.12 78.67 81.08
vgg-net-m-ft softmax 74.62 75.33 76.96 75.79 77.88 78.54 80.71
DF-CNN svm - - - - - - 81.04
DF-CNN softmax - - - - - - 81.33
particular, for BU-3DFE subset II, 100 subjects are randomly
divided into 10 subsets, and for each session, 12,960 attribute
maps of 90 subjects are used for training and the remaining
1,440 attribute maps of 10 subjects are used for testing.
Similarly, for Bosphorus subset, 60 subjects are randomly
divided into 10 subsets, and for each session, 1,944 attribute
maps of 54 subjects are used for training and the remaining
216 attribute maps of 6 subjects are used for testing.
1)Results on BU-3DFE subset II: Table VIII reports
the performance comparisons of the proposed DF-CNN with
handcrafted features, pre-trained deep features, and Ô¨Åne-tuned
deep features on BU-3DFE Subset II. From this table, we can
conclude that: 1) As before, Gabor feature still achieves the
best results among handcrafted features. It even slightly out-
performs the pre-trained deep feature vgg-net-m-conv5 (80%
vs. 79.75%). 2) Fine-tuned deep features achieve signiÔ¨Åcantly
better results than pre-trained deep features, e.g., 81.08% for
vgg-net-m-ft-full7 vs. 77.38% for vgg-net-m-full7. 3) Our DF-
CNN based methods achieve comparable (81.04% vs. 81.08%)
or slightly better (81.33% vs. 81.08%) results compared with
Ô¨Åne-tuned deep features. It is worth noting that for each train
and test session, DF-CNN only needs to train a single CNN
for both feature learning and feature fusion, while Ô¨Åne-tuned
deep feature based method needs to respectively train different
deep models for different types of facial attribute maps, and
respectively extract Ô¨Åne-tuned deep features from different
deep models and combine all scores by hand. This leads to
much more consumptions of training time and parameter space
compared with DF-CNN. Notice that the results of two BU-
3DFE subsets clearly indicate that the samples with lower
levels of expression intensity are indeed much more difÔ¨Åcult
to be recognized than the higher level ones.
2)Results on Bosphorus subset: Table IX reports the
performance comparisons of the proposed DF-CNN with
handcrafted features, pre-trained deep features, and Ô¨Åne-tuned
deep features on Bosphorus subset. Similar to the conclusions
achieved on BU-3DFE subset I and subset II, we have: 1)
Gabor feature achieves the highest accuracy of 77.50% among
handcrafted features. 2) Fine-tuned deep feature (i.e., vgg-
1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE
Transactions on Multimedia
13
Fig. 8. Six pairs of 2D texture images with fear and surprise expressions of
Bosphorus database. It‚Äôs not easy even for humans to distinguish these fear
and surprise pairs illustrated in this Ô¨Ågure.
TABLE IX
COMPARISON OF THE AVERAGE ACCURACIES WITH HANDCRAFTED
FEATURES PRE -TRAINED DEEP FEATURES ,AND FINE -TUNED DEEP
FEATURES ON BOSPHORUS SUBSET .
Method Ig Ix
n Iy
n Iz
n Ic ItAll
MS-LBP 71.11 69.44 70.56 66.67 62.78 62.50 73.33
dense-SIFT 70.28 73.89 72.78 73.89 72.50 65.56 76.39
HOG 72.50 74.22 73.89 74.72 71.94 71.94 77.22
Gabor 67.78 73.61 75.83 71.61 75.56 70.56 77.50
vgg-net-m-conv5 71.94 72.50 73.61 71.67 72.78 73.06 79.72
vgg-net-m-full7 61.11 63.33 63.89 65.83 60.56 61.94 75.56
vgg-net-m-ft-full7 svm 71.67 72.78 74.72 76.11 71.94 73.61 79.17
vgg-net-m-ft softmax 71.39 72.78 75.28 75.00 73.33 73.61 79.72
DF-CNN svm - - - - - - 80.28
DF-CNN softmax - - - - - - 80.00
net-m-ft-full7 ) also signiÔ¨Åcantly outperforms the pre-trained
one (i.e., vgg-net-m-full7 ). Note that although the pre-trained
deep feature vgg-net-m-conv5 achieves the same accuracy of
79.72% as the Ô¨Åne-tuned deep feature vgg-net-m-ft softmax , the
feature dimension is much higher (18; 4326vs. 6). 3) Our
DF-CNN based methods achieve slightly better results com-
pared with the Ô¨Åne-tuned deep features. Overall, Bosphorus
subset is the most difÔ¨Åcult dataset among the three subsets
used in this paper.
3)Comparison with other methods: To compare the
performance of the proposed DF-CNN with other methods
on BU-3DFE subset II and Bosphorus subset, we reproduced
TABLE X
COMPARISON WITH THE STATE -OF-THE-ART ON THE BU-3DFE S UBSET
IIAND BOSPHORUS SUBSET .
Method BU-3DFE Subset II Bosphorus subset
Li et al. (2012) [23] 78.50 75.83
Li et al. (2015) [24] 80.42 79.72
Yang et al. (2015) [57] 80.46 77.50
DF-CNN svm 81.04 80.28
DF-CNN softmax 81.33 80.00three state-of-the-art methods (i.e., [23], [24], and [57]) on
these two datasets using the same experimental protocol (i.e.,
10-fold cross-validation with the same subjects for training
and testing in each train and test session) as DF-CNN. In
particular, [23] and [24] are two of our previous methods.
Results of [57] were reproduced using the code shared by the
authors. Notice that multiple kernel learning was used in [23],
non-linear SVM was used in [24] and [57] for expression
prediction, respectively. For fair comparison, the non-linear
SVM was replaced by linear SVM classiÔ¨Åer, and the sum rule
based score-level fusion was used for [24] and [57].
Table X reports the performance comparisons of DF-CNN
with state-of-the-art methods [23], [24] and [57] on both BU-
3DFE subset II and Bosphorus subset. From this table, we
can see that method [23] achieves the lowest accuracy on
both subsets. Methods [24] and [57] achieve very similar
results (80.42% vs. 80.46%) on BU-3DFE subset II, while
method [24] performs better by 2.22% on Bosphorus subset.
Our DF-CNN achieves the best results on both two sub-
sets. Similar to the case on BU-3DFE subset I, DF-CNN
has signiÔ¨Åcant superiority to distinguish fear expression. For
example, on the Bosphorus subset, DF-CNN svmachieves an
average recognition rate of 65% for fear expression, which is
much higher than the results of 36.67%, 51.67%, and 43.33%
achieved by [23], [24], and [57], respectively. It is worth noting
that distinguishing the samples of Bosphorus subset with fear
expression and surprise expression is a very difÔ¨Åcult task even
for humans as illustrated in Fig. 8. From this Ô¨Ågure, we can
see that there only exist very subtle differences between fear
and surprise pairs of the same person.
Overall, the proposed DF-CNN uniÔ¨Åes feature learning and
fusion learning into a single end-to-end training framework,
and performs better than handcrafted features, pre-trained deep
features, Ô¨Åne-tuned deep features, and state-of-the-art methods,
resulting in a good generalization ability on BU-3DFE subset
II and Bosphorus subset for multimodal 2D+3D FER.
D. Discussion
To further validate the effectiveness of DF-CNN, three
issues: feature extraction with or without parameter sharing,
effectiveness of learning-based fusion, and optimality of lin-
ear SVM based expression prediction are discussed in this
paragraph. Noting that all the following discussions are based
on BU-3DFE subset I and the corresponding experimental
protocol introduced in section VI-B.
1)Feature extraction with or without parameter shar-
ing: As shown in Fig. 1, the CNN parameters are shared
for different types of facial attribute maps in the feature
extraction subnet of DF-CNN. Alternatively, different attribute
maps can also been separately fed into different CNNs for
feature fusion, then adding the following feature fusion and
expression prediction layers. Clearly, the latter one (namely
DF-CNNa) needs to learn more parameters and thus perhaps
performs better. In Table XI, we compared the parameter
quantity, compute time, and accuracy between DF-CNN and
DF-CNNa. We can see that, comparing with DF-CNN, DF-
CNNahas much more parameters (50M vs. 300M) and thus
runs more slowly (7.4 Hz vs. 4.1 Hz). However, DF-CNN still
1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE
Transactions on Multimedia
14
achieves slightly better results than DF-CNNa. This might be
due to that we used very limited number of training samples to
train DF-CNN and DF-CNNa. Therefore, we guess that if one
has sufÔ¨Åcient training samples available, DF-CNNastill has a
large potential to outperform DF-CNN in general but needs to
learn more parameters, and to take more training time.
TABLE XI
COMPARISON OF DF-CNN AND DF-CNNa(WITHOUT PARAMETER
SHARING ,I.E.,DIFFERENT ATTRIBUTE MAPS CORRESPONDING TO
DIFFERENT FEATURE EXTRACTION SUBNETS )ONBU-3DFE SUBSET I.
Method Parameter Time/epoch Accuracy
DF-CNN svm '50 MB 7.4 Hz 86.86
DF-CNN softmax '50 MB 7.4 Hz 86.20
DF-CNNa
svm '300 MB 4.1 Hz 86.48
DF-CNNa
softmax'300 MB 4.1 Hz 85.97
2)Effectiveness of learning-based fusion: To show the
effectiveness of learning-based fusion, we compared DF-CNN
with two popular classiÔ¨Åers: linear SVM with score-level
fusion and multiple kernel learning (MKL) with kernel-level
fusion. Deep CNN features DF-CNN-in-conv5 andDF-CNN-
ft-conv5 are respectively extracted from the feature extraction
subnet of DF-CNN with initialized and Ô¨Åne-tuned CNN pa-
rameters. From Table XII, we can see that MKL with kernel-
level fusion achieves better results than liner SVM with score-
level fusion in both cases. Our Ô¨Åne-tuned DF-CNN, which
combines feature learning and fusion learning in a single end-
to-end training framework, achieves signiÔ¨Åcant better results
than liner SVM and MKL.
Moreover, to see the effect of feature fusion subnet, we Ô¨Åxed
all the initialized CNN parameters of the feature extraction
subnet, and only learned the parameters of the following
feature fusion subnet. This is equivalent to learn hierarchical
fusion weights to combine the high-dimensional pre-trained
deep features. From Table XII, we can see that this kind
of pure fusion learning-based DF-CNN can achieve slightly
better results (84.79% vs. 84.22%) than linear SVM with
handcrafted score-level fusion, but signiÔ¨Åcantly worse than
the proposed DF-CNN. This indicates that the combination
of feature learning and fusion learning into a single end-to-
end training framework is very important for the proposed
DF-CNN.
TABLE XII
COMPARISON OF THE LEARNING BASED FUSION STRATEGY (DF-CNN)
WITH OTHERS ON BU- SUBSET I.
Feature and
fusionlinear SVM
(score-level)MKL
(kernel)DF-CNN svm
(learning-based)DF-CNN softmax
(learning-based)
DF-CNN-in-
conv584.22 85.07 84.79 83.90
DF-CNN-ft-
conv584.17 85.73 86.86 86.20
3)Optimality of Linear SVM based prediction: To
validate the optimality of using linear SVM classiÔ¨Åer for
expression prediction, we compared it with Ô¨Åve popular clas-siÔ¨Åers: logistic regression, k-Nearest Neighbor, naive bayes,
random forests, and rbf-kernel SVM. All experiments were
carried out on BU-3DFE subset I based on the 1,000 times 54-
vs-6 experimental setting, and using the 32-dimensional fused
deep features produced by DF-CNN. The hyper-parameters of
these classiÔ¨Åers (e.g., the value of kin k-Nearest Neighbor,
number of trees in random forests, and in rbf-kernel SVM)
were carefully selected by cross-validation on the training set
of each train session. In contrast, the parameter Cin linear
SVM was set to be the default value 1 for all 1,000 times
train sessions.
Table XIII reports the comparison results. We can see that:
1) All classiÔ¨Åers achieve comparable results except logistic re-
gression, which indicates again that the 32-dimensional fused
deep feature is very discriminative. 2) Among all classiÔ¨Åers,
linear SVM has obvious advantages in both accuracy and
speed (without parameter tuning). Therefore, linear SVM is
generally the best candidate classiÔ¨Åer for expression prediction
using fused deep features produced by DF-CNN.
TABLE XIII
COMPARISON OF DIFFERENT CLASSIFIERS OVER THE 32-DIMENSIONAL
DEEP FEATURES EXTRACTED FROM DF-CNN ONBU-3DFE SUBSET I.
ClassiÔ¨Åer Logistic
Regres.k-Nearest
NeighborNaive
BayesRandom
Forestskernel
SVMlinear
SVM
Accuracy (%) 81.03 85.84 85.90 85.18 86.76 86.86
Finally, it is worth noting that we have also studied the
issue of optimal dimension for the fused deep feature produced
by DF-CNN. Our experimental results indicate that the 32-
dimensional fused deep feature can achieve slightly better
results than both 16-dimensional and 64-dimensional fused
deep features.
VII. C ONCLUSION AND FUTURE WORK
This paper presents a novel deep fusion convolution neu-
ral network (DF-CNN) for subject-independent multi-modal
2D+3D FER. DF-CNN comprises a feature extraction subnet,
a feature fusion subnet, and a softmax-loss layer. Each textured
3D face scan is Ô¨Årstly represented as six types of facial
attribute maps, all of which are then jointly fed into DF-CNN
for feature extraction and feature fusion, resulting in a highly
concentrated facial representation. Expression prediction is
performed by two ways: 1) learning linear SVM classiÔ¨Åers
using the 32-dimensional fused deep features; 2) directly
performing softmax prediction using the 6-dimensional ex-
pression probabilities. Different from existing methods for 3D
FER, DF-CNN combines feature learning and fusion learning
into a single end-to-end training framework. To demonstrate
the effectiveness of DF-CNN, we conducted comprehensive
experiments to compare the performance of DF-CNN with
handcrafted features, pre-trained deep features, Ô¨Åne-tuned deep
features, and the state-of-the-art methods on three subsets of
two popular 3D face datasets (i.e., BU-3DFE and Bosphorus).
In all cases, DF-CNN consistently achieves the best results.
Both visualization and quantiÔ¨Åcation results indicate that the
32-dimensional fused deep feature of DF-CNN has strong dis-
criminative ability to distinguish different facial expressions.
1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE
Transactions on Multimedia
15
In the future, some other issues of DF-CNN such as how to
choose the optimal pre-trained deep CNN for initialization, and
the optimal loss function for training will be studied. More-
over, we will also study to extend current DF-CNN frame-
work to multi-modal 2D+3D video based facial expression
recognition, or other multi-modal facial emotion prediction
problems such as action unit detection and expression intensity
estimation.
ACKNOWLEDGMENT
Huibin Li was supported in part by the NSFC under grant
11401464, Chinese Postdoctoral Science Foundation under
grant 2014M560785, and International Exchange Founda-
tion of China NSFC and United Kingdom RS under grant
61711530242. Jian Sun was supported in part by the NSFC
under grants 61472313 and 11622106. Liming Chen was sup-
ported in part by the French Research Agency, l‚ÄôAgence Na-
tionale de Recherche (ANR), through the Jemime project (N
contract ANR-13-CORD-0004-02) and the Biofence project
(Ncontract ANR-13-INSE-0004-02) and the PUF 4D Vision
project funded by the Partner University Foundation.
REFERENCES
[1] S. Berretti, A. Bimbo, P. Pala, B. Amor, and M. Daoudi. A set of
selected sift features for 3d facial expression recognition. In 20th Int.
Conference on Pattern Recognition, pages 4125‚Äì4128, 2010.
[2] P. Burkert, F. Trier, M. Z. Afzal, A. Dengel, and M. Liwicki. Dex-
pression: Deep convolutional neural network for expression recognition.
CoRR, abs/1509.05371, 2015.
[3] R. A. Calix, S. A. Mallepudi, B. Chen, and G. M. Knapp. Emotion
recognition in text for 3-d facial expression rendering. IEEE Transac-
tions on Multimedia, 12(6):544‚Äì551, Oct 2010.
[4] K. ChatÔ¨Åeld, K. Simonyan, A. Vedaldi, and A. Zisserman. Return of
the devil in the details: Delving deep into convolutional nets. In British
Machine Vision Conference, 2014.
[5] W. S. Chu, F. de la Torre, and J. Cohn. Selective transfer machine for
personalized facial expression analysis. IEEE Transactions on Pattern
Analysis and Machine Intelligence, PP(99):1‚Äì1, 2016.
[6] C. A. Corneanu, M. Oliu, J. F. Cohn, and S. Escalera. Survey on rgb, 3d,
thermal, and multimodal approaches for facial expression recognition:
History, trends, and affect-related applications. IEEE Transactions on
Pattern Analysis and Machine Intelligence, PP(99):1‚Äì1, 2016.
[7] M. Dahmane and J. Meunier. Emotion recognition using dynamic
grid-based hog features. In Automatic Face Gesture Recognition and
Workshops (FG 2011), 2011 IEEE International Conference on, pages
884‚Äì888, March 2011.
[8] M. Dahmane and J. Meunier. Prototype-based modeling for facial
expression analysis. IEEE Transactions on Multimedia, 16(6):1574‚Äì
1584, Oct 2014.
[9] A. Dhall, R. Goecke, S. Lucey, and T. Gedeon. Static facial expression
analysis in tough conditions: Data, evaluation protocol and benchmark.
InComputer Vision Workshops (ICCV Workshops), 2011 IEEE Interna-
tional Conference on, pages 2106‚Äì2112, Nov 2011.
[10] A. Dhall, S. Member, S. Lucey, and T. Gedeon. Collecting large, richly
annotated facial-expression databases from movies, 2012.
[11] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and
T. Darrell. Decaf: A deep convolutional activation feature for generic
visual recognition, 2014.
[12] T. Fang, X. Zhao, O. Ocegueda, S. Shah, and I. Kakadiaris. 3d facial
expression recognition: A perspective on promises and challenges. In
IEEE International Conference on Automatic Face Gesture Recognition
and Workshops, pages 603‚Äì610, 2011.
[13] T. Fang, X. Zhao, O. Ocegueda, S. K. Shah, and I. A. Kakadiaris. 3d/4d
facial expression analysis: An advanced annotated face model approach.
Image and Vision Computing, 30(10):738 ‚Äì 749, 2012.
[14] J. Goldfeather and V . Interrante. A novel cubic-order algorithm for ap-
proximating principal direction vectors. ACM Transactions on Graphics
(TOG), 23(1):45‚Äì63, 2004.[15] B. Gong, Y . Wang, J. Liu, and X. Tang. Automatic facial expression
recognition on a single 3d face by exploring shape deformation. In
Proceedings of the 17th ACM international conference on Multimedia,
pages 569‚Äì572, 2009.
[16] M. Hayat and M. Bennamoun. An automatic framework for textured
3d video-based facial expression recognition. IEEE Transactions on
Affective Computing, 5(3):301‚Äì313, July 2014.
[17] R. Hoffman and A. K. Jain. Segmentation and classiÔ¨Åcation of
range images. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 9(5):608‚Äì620, 1987.
[18] Y . Huang, Y . Li, and N. Fan. Robust symbolic dual-view facial
expression recognition with skin wrinkles: Local versus global approach.
IEEE Transactions on Multimedia, 12(6):536‚Äì543, Oct 2010.
[19] S. E. Kahou, X. Bouthillier, P. Lamblin, C. Gulcehre, V . Michal-
ski, K. Konda, S. Jean, P. Froumenty, Y . Dauphin, N. Boulanger-
Lewandowski, and et al. Emonets: Multimodal deep learning approaches
for emotion recognition in video. Journal on Multimodal User Inter-
faces, 10(2):99‚Äì111, 2016.
[20] P. Khorrami, T. L. Paine, and T. S. Huang. Do deep neural networks
learn facial action units when doing expression recognition? CoRR,
abs/1510.02969, 2015.
[21] B.-K. Kim, H. Lee, J. Roh, and S.-Y . Lee. Hierarchical committee
of deep cnns with exponentially-weighted decision fusion for static
facial expression recognition. In Proceedings of the 2015 ACM on
International Conference on Multimodal Interaction, ICMI ‚Äô15, pages
427‚Äì434, 2015.
[22] P. Lemaire, L. Chen, M. Ardabilian, and M. Daoudi. Fully automatic
3d facial expression recognition using differential mean curvature maps
and histograms of oriented gradients. In Workshop 3D Face Biometrics,
IEEE Automatic Facial and Gesture Recognition, FG‚Äô13, 2013.
[23] H. Li, L. Chen, D. Huang, Y . Wang, and J.-M. Morvan. 3d facial
expression recognition via multiple kernel learning of multi-scale local
normal patterns. In 21st International Conference on Pattern Recogni-
tion (ICPR), pages 2577‚Äì2580, 2012.
[24] H. Li, H. Ding, D. Huang, Y . Wang, X. Zhao, J.-M. Morvand, and
L. Chen. An efÔ¨Åcient multimodal 2d + 3d feature-based approach to
automatic facial expression recognition. Computer Vision and Image
Understanding, 140:83 ‚Äì 92, 2015.
[25] H. Li, D. Huang, L. Chen, and Y . Wang. A group of facial normal
descriptors for recognizing 3d identical twins. In IEEE Fifth Inter-
national Conference on Biometrics: Theory, Applications and Systems,
pages 271‚Äì277, 2012.
[26] H. Li, J.-M. Morvan, and L. Chen. 3d facial expression recognition based
on histograms of surface differential quantities. Advances Concepts for
Intelligent Vision Systems, pages 483‚Äì494, 2011.
[27] K. Li, Q. Dai, R. Wang, Y . Liu, F. Xu, and J. Wang. A data-driven
approach for facial expression retargeting in video. IEEE Transactions
on Multimedia, 16(2):299‚Äì310, Feb 2014.
[28] M. Liu, S. Li, S. Shan, and X. Chen. Au-inspired deep networks for
facial expression feature learning. Neurocomputing, 159:126 ‚Äì 136,
2015.
[29] M. Liu, S. Li, S. Shan, R. Wang, and X. Chen. Deeply Learning De-
formable Facial Action Parts Model for Dynamic Expression Analysis,
pages 143‚Äì157. Springer International Publishing, Cham, 2015.
[30] P. Liu, S. Han, Z. Meng, and Y . Tong. Facial expression recognition
via a boosted deep belief network. In Computer Vision and Pattern
Recognition (CVPR), 2014 IEEE Conference on, pages 1805‚Äì1812,
2014.
[31] P. Liu, J. T. Zhou, I. W.-H. Tsang, Z. Meng, S. Han, and Y . Tong. ECCV
2014: 13th European Conference, Zurich, Switzerland, September 6-12,
2014, Proceedings, Part IV, chapter Feature Disentangling Machine -
A Novel Approach of Feature Selection and Disentangling in Facial
Expression Analysis, pages 151‚Äì166. Springer International Publishing,
Cham, 2014.
[32] A. Maalej, B. B. Amor, M. Daoudi, A. Srivastava, and S. Berretti.
Local 3d shape analysis for facial expression recognition. In 2010 20th
International Conference on Pattern Recognition, pages 4129‚Äì4132, Aug
2010.
[33] A. Maalej, B. B. Amor, M. Daoudi, A. Srivastava, and S. Berretti. Shape
analysis of local facial patches for 3d facial expression recognition.
Pattern Recognition, 44(8):1581 ‚Äì 1589, 2011.
[34] A. Mian, M. Bennamoun, and R. Owens. Automatic 3d face detection,
normalization and recognition. In In 3DPVT, pages 735‚Äì742, 2006.
[35] I. Mpiperis, S. Malassiotis, and M. Strintzis. Bilinear models for 3-d face
and facial expression recognition. IEEE Transactions on Information
Forensics and Security, 3(3):498 ‚Äì511, sept. 2008.
[36] O. Ocegueda, T. Fang, S. K. Shah, and I. A. Kakadiaris. Expressive maps
for 3d facial expression recognition. In IEEE International Conference
on Computer Vision Workshops(ICCV), pages 1270‚Äì1275, 2011.
1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE
Transactions on Multimedia
16
[37] M. Pantic and L. J. M. Rothkrantz. Automatic analysis of facial
expressions: The state of the art. IEEE Trans. Pattern Anal. Mach.
Intell., 22(12):1424‚Äì1445, 2000.
[38] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson. CNN
features off-the-shelf: an astounding baseline for recognition. CoRR,
abs/1403.6382, 2014.
[39] S. Rifai, Y . Bengio, A. Courville, P. Vincent, and M. Mirza. Disentan-
gling factors of variation for facial expression recognition. In Computer
Vision C ECCV 2012, volume 7577 of Lecture Notes in Computer
Science, pages 808‚Äì822. 2012.
[40] G. Sandbach, S. Zafeiriou, M. Pantic, and L. Yin. Static and dynamic
3d facial expression recognition: A comprehensive survey. Image and
Vision Computing, 2012. in press.
[41] A. Savran, N. Aly ¬®uz, H. Dibeklio Àòglu, O. C ¬∏ eliktutan, B. G ¬®okberk,
B. Sankur, and L. Akarun. Bosphorus Database for 3D Face Analysis,
pages 47‚Äì56. Springer Berlin Heidelberg, Berlin, Heidelberg, 2008.
[42] A. Savran, B. Sankur, and M. T. Bilge. Facial action unit detection:
3d versus 2d modality. In IEEE Computer Society Conference on
Computer Vision and Pattern Recognition Workshops (CVPRW), pages
71‚Äì78, 2010.
[43] M. Song, Z. Dong, C. Theobalt, H. Wang, Z. Liu, and H. P. Seidel. A
generic framework for efÔ¨Åcient 2-d and 3-d facial expression analogy.
IEEE Transactions on Multimedia, 9(7):1384‚Äì1395, Nov 2007.
[44] H. Soyel and H. Demirel. Facial expression recognition using 3d facial
feature distances. In Image Analysis and Recognition , volume 4633 of
Lecture Notes in Computer Science, pages 831‚Äì838, 2007.
[45] H. Soyel and H. Demirel. 3d facial expression recognition with
geometrically localized facial features. In 23rd International Symposium
on Computer and Information Sciences (ISCIS), pages 1 ‚Äì4, 2008.
[46] H. Tang and T. Huang. 3d facial expression recognition based on
automatically selected features. In IEEE Computer Society Conference
on Computer Vision and Pattern Recognition Workshops(CVPRW), pages
1‚Äì8, 2008.
[47] H. Tang and T. Huang. 3d facial expression recognition based on
properties of line segments connecting facial feature points. In 8th IEEE
International Conference on Automatic Face Gesture Recognition(FG),
pages 1‚Äì6, 2008.
[48] Y . Tang. Deep learning using support vector machines. CoRR,
abs/1306.0239, 2013.
[49] A. Tawari and M. M. Trivedi. Face expression recognition by cross
modal data association. IEEE Transactions on Multimedia , 15(7):1543‚Äì
1552, Nov 2013.
[50] F. Tsalakanidou and S. Malassiotis. Real-time 2d+3d facial action and
expression recognition. Pattern Recognition, 43(5):1763‚Äì1775, 2010.
[51] L. van der Maaten and G. Hinton. Visualizing data using t-sne, 2008.
[52] A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library of
computer vision algorithms, 2008.
[53] J. Wang, L. Yin, X. Wei, and Y . Sun. 3d facial expression recognition
based on primitive surface feature distribution. In IEEE Conference on
Computer Vision and Pattern(CVPR), pages 1399‚Äì1406, 2006.
[54] S. Wang, Z. Liu, S. Lv, Y . Lv, G. Wu, P. Peng, F. Chen, and X. Wang.
A natural visible and infrared facial expression database for expression
recognition and emotion inference. IEEE Transactions on Multimedia,
12(7), Nov 2010.
[55] S. Wang, Z. Liu, Z. Wang, G. Wu, P. Shen, S. He, and X. Wang.
Analyses of a multimodal spontaneous facial expression database. IEEE
Transactions on Affective Computing, 4(1):34‚Äì46, Jan 2013.
[56] C. H. Wu, W. L. Wei, J. C. Lin, and W. Y . Lee. Speaking effect removal
on emotion recognition from facial expressions based on eigenface
conversion. IEEE Transactions on Multimedia, 15(8):1732‚Äì1744, Dec
2013.
[57] X. Yang, D. Huang, Y . Wang, and L. Chen. Automatic 3d facial ex-
pression recognition using geometric scattering representation. In IEEE
International Conference on Automatic Face and Gesture Recognition,
2015.
[58] M. Yeasin, B. Bullot, and R. Sharma. Recognition of facial expressions
and measurement of levels of interest from video. IEEE Transactions
on Multimedia, 8(3):500‚Äì508, June 2006.
[59] L. Yin, X. Wei, Y . Sun, J. Wang, and M. J. Rosato. A 3d facial expression
database for facial behavior research. IEEE International Conference on
Automatic Face and Gesture Recognition, pages 211‚Äì216, 2006.
[60] Z. Yu and C. Zhang. Image based static facial expression recognition
with multiple deep network learning. IEEE Institute of Electrical and
Electronics Engineers, November 2015.
[61] S. Zafeiriou and I. Pitas. Discriminant graph structures for facial
expression recognition. IEEE Transactions on Multimedia, 10(8):1528‚Äì
1540, Dec 2008.
[62] G. Zen, L. Porzi, E. Sangineto, E. Ricci, and N. Sebe. Learning per-
sonalized models for facial expression analysis and gesture recognition.IEEE Transactions on Multimedia, 18(4):775‚Äì788, April 2016.
[63] W. Zeng, H. Li, L. Chen, J.-M. Morvan, and X. D. Gu. An automatic
3d expression recognition framework based on sparse representation
of conformal images. In 10th IEEE International Conference and
Workshops on Automatic Face and Gesture Recognition (FG), pages
1‚Äì8, 2013.
[64] Z. Zhang, M. Lyons, M. Schuster, and S. Akamatsu. Comparison
between geometry-based and gabor-wavelets-based facial expression
recognition using multi-layer perceptron. In FG, pages 454‚Äì459, 1998.
[65] G. Zhao and M. Pietikainen. Dynamic texture recognition using
local binary patterns with an application to facial expressions. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 29(6):915‚Äì
928, June 2007.
[66] X. Zhao, D. Huang, E. Dellandrea, and L. Chen. Automatic 3d facial
expression recognition based on a bayesian belief net and a statistical
facial feature model. International Conference on Pattern Recognition
(ICPR), pages 3724‚Äì3727, 2010.
[67] Q. Zhen, D. Huang, Y . Wang, and L. Chen. Muscular movement
model based automatic 3d facial expression recognition. In MultiMedia
Modeling,Lecture Notes in Computer Science , volume 8935, pages 522‚Äì
533, 2015.
[68] Q. Zhen, D. Huang, Y . Wang, and L. Chen. Muscular movement model
based automatic 3d/4d facial expression recognition. IEEE Transactions
on Multimedia, PP(99):1‚Äì1, 2016.
[69] L. Zhong, Q. Liu, P. Yang, J. Huang, and D. N. Metaxas. Learning mul-
tiscale active facial patches for expression analysis. IEEE Transactions
on Cybernetics, 45(8):1499‚Äì1510, 2015.
Huibin Li received his Bachelor and Master de-
grees both in mathematics from Shaanxi Normal
University and Xian Jiaotong University, in 2006
and 2009, respectively. And he received Ph.D degree
in mathematics and computer science in 2013 from
Universit ¬¥e de Lyon, CNRS, Ecole Centrale de Lyon,
LIRIS, Lyon, France. He is currently an assistant
professor in the school of mathematics and statistics,
Xi‚Äôan Jiaotong University. His research interests
include 3D shape analysis, 3D face recognition,
3D facial expression analysis, discrete differential
geometry, geometric data analysis, modeling and learning.
Jian Sun received Ph.D. in applied mathematics
from Xian Jiaotong University. He worked as a
visiting student in Microsoft Research Asia (Nov.
2005 - March 2008), a post-doctoral researcher in
university of central Ô¨Çorida in USA (August 2009 -
April 2010), and a post-doctoral researcher in willow
team of cole Normale Sup ¬¥erieure de Paris / INRIA
(Sept. 2012 - August 2014). He now serves as a
professor in the school of mathematics and statistics
of Xian Jiaotong University. His research interests
are in the mathematics &machine learning-based
approaches for image processing / recognition, and medical image analysis.
Zongben Xu received his Ph.D. degree in mathemat-
ics from Xian Jiaotong University, China, in 1987.
He now serves as the Chief Scientist of National
Basic Research Program of China (973 Project), and
Director of the Institute for Information and System
Sciences of the university. He is owner of the Na-
tional Natural Science Award of China in 2007, and
winner of CSIAM Su Buchin Applied Mathematics
Prize in 2008. He delivered a 45 minute talk on
the International Congress of Mathematicians 2010.
He was elected as member of Chinese Academy of
Science in 2011. His current research interests include intelligent information
processing and applied mathematics.
Liming Chen was awarded a joint BSc degree
in Mathematics and Computer Science from the
University of Nantes in 1984. He obtained a Master‚Äôs
degree in 1986 and a PhD in computer science from
the University of Paris 6 in 1989. He Ô¨Årst served as
associate professor at the Universit ¬¥e de Technologie
de Compi `egne, then joined Ecole Centrale de Lyon
as Professor in 1998, where he leads an advanced
research team in multimedia computing and pattern
recognition. He has been Head of the department of
Mathematics and Computer science from 2007. His
current research interests include multimedia processing, discrete differential
geometry and statistical learning, with applications in particular to 2D/3D face
analysis and recognition, image and video analysis and categorization.
"
https://ieeexplore.ieee.org/document/8576656,Error
https://ieeexplore.ieee.org/document/8545853/,"Patch-Gated CNN for Occlusion-aware Facial
Expression Recognition
Y ong Li1,2, Jiabei Zeng1, Shiguang Shan1,2,3and Xilin Chen1,2
1Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS),
Institute of Computing Technology, CAS, Beijing 100190, China
2University of Chinese Academy of Sciences, Beijing 100049, China
3CAS Center for Excellence in Brain Science and Intelligence Technology
{yong.li, jiabei.zeng }@vipl.ict.ac.cn, {sgshan, xlchen }@ict.ac.cn
Abstract ‚ÄîFacial expression recognition in the wild is challeng-
ing due to various un-constrained conditions. Although existing
facial expression classiÔ¨Åers have been almost perfect on analyzing
constrained frontal faces, they fail to perform well on partiallyoccluded faces that are common in the wild. In this paper,
we propose an end-to-end trainable Patch-Gated Convolution
Neutral Network (PG-CNN) that can automatically percept the
occluded region of the face and focus on the most discriminative
un-occluded regions. To determine the possible regions of intereston the face, PG-CNN decomposes an intermediate feature map
into several patches according to the positions of related facial
landmarks. Then, via a proposed Patch-Gated Unit, PG-CNNreweighs each patch by the unobstructed-ness or importance that
is computed from the patch itself. The proposed PG-CNN is eval-
uated on two largest in-the-wild facial expression datasets (RAF-DB and AffectNet) and their modiÔ¨Åcations with synthesized facial
occlusions. Experimental results show that PG-CNN improves the
recognition accuracy on both the original faces and faces withsynthesized occlusions. Visualization results demonstrate that,
compared with the CNN without Patch-Gated Unit, PG-CNN
is capable of shifting the attention from the occluded patch to
other related but unobstructed ones. Experiments also show that
PG-CNN outperforms other state-of-the-art methods on severalwidely used in-the-lab facial expression datasets under the cross-
dataset evaluation protocol.
I. I NTRODUCTION
Facial expression recognition (FER) has received signiÔ¨Åcant
interest from computer scientists and psychologists over recentdecades, as it holds promise to an abundance of applications,such as human computer interaction, affect analysis, andmental health assessment. Although many facial expressionrecognition systems have been proposed and implemented,majority of them are builded on images captured in controlledenvironment, such as on CK+ [1], MMI [2], Oulu-CASIA[3], and other lab-collected datasets. The controlled faces arefrontal and without any occlusions. The FER systems that
perform perfectly on the lab-collected datasets, is highly pos-
sible to perform poorly when recognizing human‚Äôs expressionsunder natural and un-controlled conditions.
To Ô¨Åll the gap between the recognition accuracy on the
controlled faces and un-controlled faces, researchers makeefforts on collecting large-scale facial expression datasets inthe wild ( [4], [5]). Despite the usage of data from the wild,facial expression recognition is still challenging due to theexistence of partially occluded faces. It it non-trivial to address1. Input image 2. Convolutional Feature Extraction 
3. Occlusion perc eption for each 
region of interest  4. Classification neutral 
anger 
disgust 
fear 
happy 
sad 
surprise 
Attention Net 
local feature 
PG-Unit 
Attention Net 
local feature 
PG-Unit 
‚Ä¶ ‚Ä¶ ‚Ä¶ 0.80 
 0.82 0.85 0.83 0.70 0.75 Patches 
unobstructed-ness 
0.95 0.90 0.21 0.20 0.03 0.07 ‚Ä¶ 
‚Ä¶ 
Fig. 1. Illustration of the proposed PG-CNN for occlusion-aware facial
expression recognition. During Part 3, PG-CNN extracts 24 regions of interest
from the intermediate feature maps. For each region, a speciÔ¨Åc Patch GatedUnit (PG-Unit) is learnt to reweigh the local representation according to the
region‚Äôs ‚Äú unobstructed-ness ‚Äù (to what extent the patch is occluded). Then,
the weighted representations are concatenated and passed to the classiÔ¨Åcation
part.
the occlusion issue because occlusion varies in the occluders
and their positions. The occlusion may caused by hair, glasses,scarf, breathing mask, hands, arms, food, and other objects that
could be placed in front of the face in daily life. These objects
may block the eyes, mouth, part of the cheek, and any otherpart of the face. The variability of occlusion cannot be fullycovered by restricted amounts of data and will inevitably leadthe recognition accuracy to decrease.
To address the issue of occlusion, we propose a Patch-
Gated Convolution Neutral Network (PG-CNN), mimicing theway that human recognize the facial expression. Intuitively,human recognize the facial expression based on certain patchesof the face. When some regions of the face are blocked
(e.g., the lower left cheek), human may judge the expression
according to the symmetric part of face (e.g., the lower rightcheek), or other highly related facial region (e.g., regionaround the eyes or mouth). Inspired by the intuition, PG-CNN automatically percepts the blocked facial patch and pays
attentions mainly to the unblocked and informative patches.
Fig. 1 illustrates the main idea of PG-CNN. The patches ofinterest are cropped from the last convolution feature mapsaccording to the positions of the related facial landmarks. Foreach patch, a Patch-Gated Unit (PG-Unit) is learned to reweighthe patch‚Äôs local representation by its unobstructed-ness that iscomputed from the patch itself. As can be seen in Fig. 1, thelast four visualized patches are blocked and thus they have lowunobstructed-ness ( Œ±
p). Then, the weighted representations are2018 24th International Conference on Pattern Recognition (ICPR)
Beijing, China, August 20-24, 2018
978-1-5386-3788-3/18/$31.00 ¬©2018 IEEE 2209
concatenated and used in the classiÔ¨Åcation part.
The contributions of this work are summarized as follows:
1) We propose a PG-CNN to recognize facial expressions
with partially occluded faces. PG-CNN can automati-cally percept the occluded region of the face and focuson the most informative and un-blocked regions. To thebest of our knowledge, PG-CNN is the Ô¨Årst end-to-endtrainable framework that addresses occlusions in facialexpression recognition.
2) Visualized results show that PG-Unit (the crucial part of
PG-CNN) is effective in perceiving the occluded facial
patch. PG-Unit is capable to learn a low weight for the
blocked patch and a high weight for an unblocked andinformative one.
3) Experimental results demonstrate the advantages of the
proposed PG-CNN over other state-of-the-art methodson two large in-the-wild facial expression datasets andseveral popular in-the-lab datasets, under settings witheither partially occluded or non-occluded faces.
II. R
ELA TED WORK
We review the previous work considering two aspects that
are related to ours, i.e., the similar tasks (facial analysis withoccluded faces) and related techniques (attention mechanism).
A. Methods towards facial occlusions
For facial analysis tasks, occlusion is one of the inherent
challenges in the real world FER and other facial analysistasks, e.g., facial recognition, age estimate, gender classiÔ¨Åca-tion, etc. Previous approaches that address facial occlusionscan be classiÔ¨Åed into two categories: holistic-based or part-based methods.
Holistic-based approaches treat the face as a whole and do
not explicitly divide the face into sub-regions. They usuallyimprove the robustness of the features through designatedregularization, e.g., L
1-norm [6]. This idea is also suitable for
non-facial occlusions, for example, Elad et al. [7] proposed to
mutually re-weight L1regularization in an end-to-end frame-
work to deal with arbitrary occlusion in object recognition.Another holistic way is to reconstruct a complete face from theoccluded one( [8], [9]). These reconstruction based methodsrely on the training data with varied occlusion conditions.Specially, Irene et al. [10] analysed how partial occlusion
affects FER performance in detail.
Part-based methods explicitly divide the face into several
overlapped or non-overlapped segmentations. To determine thepatches on the face, existing works either divide the facialimage into several uniform parts( [11], [12]), or get the patchesaround the facial landmarks( [13], [14]), or get the patches by asampling strategy [15], or explicitly detect the occluders( [16],[17]). Then, the part-based methods detect and compensatethe missing part ( [18], [19]), or re-weight the occluded andnon-occluded patches differently( [13], [17]), or ignore theoccluded part( [15], [16]). We adopt the way of the part-based methods because they successfully incorporate the priorsinformation of the structure of human faces and have a betterinterpretation. The proposed PG-CNN is end-to-end trainable.
It learns occlusion patterns from data and encodes them withmodel weights. Therefore, it is preferable to handle arbitrarykind of occluder at any position in front of the face.
B. CNN with attention
Recently, attention models have been successfully applied
in many computer vision tasks, including Ô¨Åne-grained imagerecognition [20], image caption [21], visual question answer-ing [22], person re-identiÔ¨Åcation [23], etc. Usually attentioncan be modeled as a region sequence in an image. AnRNN/LSTM model is adopted to predict the next attentionregion based on current attention region‚Äôs location with visualfeatures.
Moreover, zheng et al. [20] adopted channel grouping sub-
network to cluster convolutional feature maps into groups ac-cording to peak responses of maps, which do not need part an-notations but is not suitable for FER under occlusion. For false
responses caused by occluders will inevitably disturb channels
clustering. Zhao et al. [23] estimated multiple 2-dimensionalattention maps, they have equal spatial size of convolutionalfeature maps to weight. This approach is straightforward butdo not take occlusion patterns into consideration.
Attention models allow for salient features to dynamically
come to forefront as needed. This is especially beneÔ¨Åcial whenthere is some occlusion or clutter in an image. They also helpinterpret the results by visualizing where the model attends tofor certain tasks. Compare with existing attention models, Ourapproach adopts facial landmarks for region decomposition,which is straightforward and easily implemented. Meanwhile,PG-CNN adopts CNN based Patch-Gated Unit for occlusionperception, guiding the model to shift attention to informativeas well as unblocked facial patches.
III. P
ROPOSED METHOD
A. Method overview
We propose a Patch-Gated CNN (PG-CNN) for facial
expression recognition with partially occlusions. To address
the occlusion issue, PG-CNN is end-to-end trainable with twokey schemes: region decomposition and occlusion perception.
Figure 2 illustrates the framework of the proposed PG-CNN.
As can be seen in Fig . 2 , the network takes input as a facial
image. The image is fed into VGG net and is represented assome feature maps. Then, PG-CNN decomposes the feature
maps of the whole face to 24 sub-feature-maps for 24 localpatches. Each local patch is encoded as a weighted vectorof local feature by a Patch-Gated Unit (PG-Unit). PG-Unit
computes the weight of each patch by an Attention Net,considering its obstructed-ness (to what extent the patch is oc-cluded). Finally, the weighted local features are concatenated
and serve as a representation of the occluded face. Three fully
connected layers are followed to assign the face to one of theemotional categories. PG-CNN is optimized by minimizingthe soft-max loss.
Below, we present the details of the two key schemes, region
decomposition and occlusion perception, in PG-CNN.2210
Patch 1 
Patch 24 512x6x6 1536 
1024 
Attention net 512x6x6 512x6x6 64 
Attention net 
pooling 512x3x3 128x3x3 64 1 512x6x6 VGG16 Net 
3xHxW 
512x28x28 PG-Unit Region decomposition 
PG-Unit Soft-max 
loss 
sigmoid 
Fig. 2. Framework of the proposed PG-CNN. PG-CNN takes a facial image as input and encodes the image with VGG-16 Net. The feature maps from the
last convolution layer ( conv 42in VGG [24]) are cropped into 24 local patches through a region decomposition scheme. Each patch is then processed by a
Patch-Gated Unit (PG-Unit). PG-Unit encodes a patch by a vector-shaped feature and estimates how informative the patch is through an Attention net. T he
soft-max loss is attached at the end. Parameters in the overall network are learned by minimizing the soft-max loss.
Fig. 3. Region decomposition of the face. The left Ô¨Ågure shows the selected
landmarks (green dots with numbers), around which the patches in right Ô¨Ågure
are cropped. We select 24 points in total, covering the region on or around
each subject‚Äôs eyebrows, eyes, nose, mouth, and cheek.
B. Region decomposition
Facial expression is distinguished in speciÔ¨Åc facial regions,
because the expressions are facial activities invoked by setsof muscle motions. Localizing and encoding the expression-related parts is of beneÔ¨Åt to recognize facial expression [12].Additionally, dividing the face into multiple local patcheshelps to Ô¨Ånd the position of occlusions [13].
To Ô¨Ånd the typical facial parts that related to expression, we
extract the patches according to the positions of each subject‚Äôsfacial landmarks. Fig. 3 shows the selection of facial patches.
We Ô¨Årst detect 68 facial landmark points by the method in[25] and then, based on the detected 68 points, we select orre-compute 24 points that cover the informative region of theface, including the two eyes, nose, mouth, cheek, and dimple.The selected patches are deÔ¨Åned as the regions taking each ofthe 24 points as the center. It is noteworthy that face alignmentmethod in [25] is robust to occlusions, which is important forprecise region decomposition.
As can be seen in the overall framework (Fig. 2), the patch
decomposition operation is conducted on the feature map fromconvolution layers rather than from the original image. This isbecause sharing some convolutional operations can decreasethe model size and enlarge the receptive Ô¨Åelds of subsequentneurons. Based on the 512√ó28√ó28feature maps as well asthe 24 local region centers, we get a total of 24 local regions,
each with a size of 512√ó6√ó6.
C. Occlusion perception with PG-Unit
We embed the PG-Unit in the PG-CNN to automatically
percept the blocked facial patch and pay attentions mainly tothe unblocked and informative patches. The detailed structureof PG-Unit is illustrated in the blue dashed rectangle in Fig. 2.In each patch-speciÔ¨Åc PG-Unit, the cropped local feature mapsare fed to two convolution layers without decreasing the spatialresolution, so as to preserve more information when learningregion speciÔ¨Åc patterns. Then, the last 512√ó6√ó6feature
maps are processed in two branches. The Ô¨Årst branch encodesthe input feature maps as the vector-shaped local feature. Thesecond branch consist an attention net that estimates a scaler
weight to denote the importance of the local patch. The local
feature is then weighted by the computed weight.
Mathematically speaking, let us suppose p
ithe input 512√ó
6√ó6feature map of the i-th patch. Therefore, the i-th PG-Unit
takes the feature map pias the input and outputs its weighted
featureœÜi. We formulate PG-Unit as:
œÜi=Ii(Àúpi)‚äôœà(Àúpi), (1)
whereÀúpi=ÀúœÜ(pi)is the last 512√ó6√ó6feature maps ahead
of the two branches. PG-Unit estimates patch i‚Äôs importance
or ‚Äòunobstructed-ness‚Äô as Œ±i=Ii(Àúpi)and then uses Œ±i
to weight the local feature œài=œà(Àúpi).œà(Àúpi)is a vector
that represents the un-weighted feature. ‚äôdenotes production.
Œ±i=Ii(Àúpi)is a scaler that represent the patch i‚Äôs importance
or ‚Äòunobstructed-ness‚Äô (to what extent the patch is occluded).I(¬∑)means the operations in the attention net, consisting
a pooling operation, one convolution operations, two innerproductions, and a sigmoid activation. The sigmoid activationforces the output Œ±
iranges in [0,1], where1indicates the
most salient unobstructed patch and 0indicates the completely
blocked patch.
In PG-Unit, each patch is weighted differently according to
its occlusion conditions or importance. Through the end-to-end2211
Fig. 4. Examples of the synthesized occluded facial images from RAF-DB
dataset. The occluders are various in color, shape, and positions.
training of the overall PG-CNN, PG-Units can automatically
learn low weights for the occluded parts and high weights forthe unblocked and discriminative parts.
IV . E
XPERIMENT
In this section, we present the experimental evaluations of
PG-CNN. Then, we compared our method with the state-of-the-art FER methods and methods with attention mechanism.Finally, we provide an ablation analysis of the proposed PG-CNN.
A. Experimental setup
1) Datasets: We evaluated the methods on both in-the-
wild datasets (RAF-DB [4] and AffectNet [5]) and in-the-lab datasets(CK+ [1], MMI [2], and Oulu-CASIA [3]). RAF-
DB contains 30,000 facial images annotated with basic or
compound expressions by 40 trained human coders. In ourexperiment, only images with basic emotions were used,including 12,271 images as training data and 3,068 imagesas test data. AffectNet is the largest database with annotated
facial emotions. It contains about 400,000 images manuallyannotated for the presence of seven discrete facial expressionsand the intensity of valence and arousal. We only used the oneswith neutral and 6 basic emotions, containing 280,000 trainingsamples and 3,500 test samples. The Extended Cohn-Kanade
database (CK+) contains 593 video sequences recorded from
123 subjects. we selected the Ô¨Årst and Ô¨Ånal frame of eachsequence as neutral and target expressions, which results in634 images. MMI database includes more than 30 subjects
of both genders (44% female), ranging in age from 19 to 62.There are 79 sequences of each subject. Each begin and endwith neutral facial expression. We extracted the neutral and
peak frames from each sequence, resulting in 7348 images.Oulu-CASIA dataset contains six prototypic expressions from
80 people between 23 to 58 years old. We selected peak andneutral frames from sequences captured in normal illumina-tion, which results in 9431 images.
2) Synthesis of occluded images: It seems unlikely that
any reasonable sized set of training images would serve todensely probe the space of possible occlusions. We tackle theproblem by manually collecting about 4k images as masksfor generating occluders. These mask images were collectedTABLE I
TEST ACCURACY (%) ONRAF-DB AND AFFECT NET.(clean :ORIGINAL
IMAGES .occ. :SYNTHETICALLY OCCLUDED IMAGES .)
Methods RAF-DB(clean/occ.) AffectNet(clean/occ.)
VGG-16 [24] 80.96/75.26 51.11/46.48
DLP-CNN [23] 80.89/76.29 54.47/51.07
P-CNN 81.64/76.09 53.9/50.32
PG-CNN (proposed) 83.27/78.05 55.33/52.47
from search engine using more than 50 keywords, such as
beer, bread, wall, hand, hair, hat, book, cabinet, computer,
cup et al. All the items were selected due to their highfrequency of occurence as obstructions in facial images. SinceBenitez et al. [26] veriÔ¨Åed that small local occluders take noaffects on current FER algorithms, we heuristically restrainoccluder size Ssatisfying S‚àà[96,128] , which is smaller
or equal to half size of expression images. Fig. 4 shows
some occluded examples derived from RAF-DB dataset. TheseartiÔ¨Åcial synthesised images are various in occlusion patternsand can better reÔ¨Çect occluder distribution in wild condition.
3) Implementation details: We implemented PG-CNN us-
ing Caffe deep learning framework [27]. We adopted VGG-16 [24] as base for PG-CNN due to its simple structureand excellent performance in object classiÔ¨Åcation. We onlychoose the Ô¨Årst nine convolution layers as the feature mapfor region decomposition then attached 24 PG-Units. Thepre-trained model based on ImageNet dataset was used forinitializing the model. For each dataset, Both train and testcorpus are mixed with occluded images with a ratio of 1:1.We adopt a batch-based stochastic gradient descent method tooptimize the model. The base learning rate was set as 0.001and was reduced by polynomial policy with gamma of 0.1.The momentum was set as 0.9 and the weight decay was setas 0.0005. The training of models was completed on a Titan-XGPU with 12GB memory. During the training stage, we setthe actual batch size as 128 and the maximum iterations as50K. It took about 1.5 days to Ô¨Ånish optimizing the model.
4) Evaluation metric: All the datasets are mixed with
their modiÔ¨Åcations with synthesized facial occlusions with 1:1ratio. We report FER performance on both non-occluded andoccluded images. For both occluded and non-occluded FER
scenarios we use the overall accuracy on seven facial expres-sion categories(i e. six prototypical plus neutral category) as a
performance metric. Both cross-dataset evaluation and 10-foldevaluation within dataset are used in our experiments.
B. Comparison with state of arts
1) Comparison with other attention models: We compare
PG-CNN with DLP-CNN [23]. DLP-CNN estimates Kspatial
maps for attention parts generation. The hyper-parameter Kis
Ô¨Åne-tuned to the best in out experiments. Table I reports theresults of PG-CNN and DLP-CNN on RAF-DB and AffectNetdatabases. PG-CNN outperforms DLP-CNN on non-occludedimages because the patch-based model can better reÔ¨Çect subtlemuscle motions than the model with global attention. PG-CNNexceeds DLP-CNN on occluded datasets with the help of PG-Unit, which encodes occlusion patterns in model weights andenable model attend to unblocked & distinctive patches. From2212
TABLE II
10- FOLD TEST ACCURACY (%) ONCK+ DA TASET WITH SYNTHETIC
OCCLUSIONS . (R8, R16, R24 DENOTE THE SIZE OF THE OCCLUSION AS
8√ó8,16√ó16,24√ó24.THE FULL -IMAGE SIZE IS 48√ó48.)
Occlusion PG-CNN‚àóPG-CNN WLS-RF [28] RGBT [15]
non-occlusion 90.37 97.03 94.3 94.4
R8 89.74 96.58 92.2 92.0
R16 87.22 95.70 86.4 82.0
R24 83.91 92.86 74.8 62.5
eyes occluded 85.02 96.50 87.9 88.0
mouth occluded 82.96 93.92 72.7 30.3
‚àódenotes cross-dataset test accuracy on CK+ by the PG-CNN trained on
AffectNet.
RAF-DB to AffectNet database, the performance gap becomes
narrowed because signiÔ¨Åcant increase in training data.
2) Comparison with other methods handling FER with oc-
clusion: We compare PG-CNN with state-of-the-arts methods
WLS-RF [28] and RGBT [15]. WLS-RF adopted multiplyweighted random forests and RGBT converted a set of Gaborbased part-face templates into template match distance featuresfor FER with occlusion. We followed the same occlusionprotocol of WLS-RF and RGBT and evaluated performanceon model trained by AffectNet dataset.
Table II show the comparisons. The overall performance
of PG-CNN is signiÔ¨Åcantly better than that of WLS-RFand RGBT. Specially, PG-CNN suffers 4.30% performancedegradation under random occlusion with R24 pattern, whileeyes or mouth occlusion has little impact on PG-CNN. Theproceeds of PG-CNN are due to PG-Unit as well as largeamount of training data in AffectNet database.
PG-CNN
‚àóin Table II shows that without training on CK+
dataset, PG-CNN‚àócan achieve comparable performance com-
pared with WLS-RF and RGBT.
3) Cross database evaluation: We evaluated the general-
ization ability of PG-CNN under the cross-dataset evaluation
protocol. In our experiments, PG-CNN was trained on RAF-
DB or AffectNet dataset and evaluated on CK+, MMI, Oulu-CASIA dataset with or without synthetic occlusions. Table IIIshows the results compared with other FER methods. Amongthe compared experiments, [29] adopted an inception basedCNN and provided the average cross-database recognition
accuracy. [30] and [31] reported the highest cross-database
results, which were both trained on MMI and evaluated onCK+ or vice versa. PG-CNN(A) exceeds [29]‚Äì[31] by at least40.7% and 3.02% on CK+ and MMI dataset respectively.It suggests that PG-CNN(A) can generalize better than PG-CNN(R) due to a larger amount of training data.
C. Ablation analysis
We conducted ablation analysis to Ô¨Ågure out how PG-CNN
boosts performance on FER with occlusion task.
1) CNN VS P-CNN: We compared VGG-16 and P-CNN
(PG-CNN without PG-Unit) to verify beneÔ¨Åt of region de-composition. As listed in Table I, P-CNN exceeds VGG-16on both original and occluded images. The promotions of P-CNN suggest that globally encoded representation has fallenbehind in reÔ¨Çecting subtle muscle motions compared withlocally learned patterns.TABLE III
CROSS DA TASET EV ALUA TION (ACCURACY %) ON IN -THE -LAB DA TASETS
(clean :ORIGINAL IMAGES .occ. :SYNTHESIZED OCCLUDED IMAGES .)
method CK+(clean/occ.) MMI(clean/occ.) Oulu-CASIA(clean/occ.)
[29] 64.2 / ‚àí 55.6 / ‚àí‚àí /‚àí
[30] 60.8 / ‚àí 60.3 / ‚àí‚àí /‚àí
[31] 61.2 / ‚àí 66.9 / ‚àí‚àí /‚àí
P-CNN(R) 79.81 / 76.02 57.02 / 53.70 49.83 / 46.98
P-CNN(A) 89.27 / 85.33 66.94 / 61.26 54.77 / 51.05
PG-CNN(R) 80.28 / 79.49 55.61 / 53.44 50.04 / 47.15
PG-CNN(A) 90.38 /86.27 68.92 /63.94 57.93 /54.18
A denotes models trained on AffectNet dataset.
R denotes models trained on RAF-DB dataset.
(a) Expression images and  
corresponding occluded 
version 
(b) Attention maps of (a),  
       derived from P-CNN. 
 Left:  maps of original images.  Right: maps of occluded images (c) Attention maps of (a), 
       derived from PG-CNN. 
Left:  maps of original images. Right: maps of occluded images neutral anger disgust fear happy sad surprise 
Fig. 5. Attention maps of several test images and their modiÔ¨Åcations with
artiÔ¨Åcial facial occlusions. Each image denotes one of seven basic expressions.
A deep white corresponds to high attention and a deep dark to no attention
at all. Better viewed in color and zoom in.
2) P-CNN VS PG-CNN: We compared P-CNN and PG-
CNN to verify beneÔ¨Åt of PG-Unit. As displayed in Table I,total improvements of PG-CNN on RAF-DB and AffectNetdatasets are 1.99%, 2.58% and 3.65%, 4.27% respectively.This is because PG-Unit enables the model to attend to mostrelated local patches, and shift attention to other related localparts when original ones are occluded. Similar performanceimprovements can be found in Table III, where PG-CNN
outperforms P-CNN on nearly all datasets except for MMI.
We visualized the attention map of PG-CNN and P-CNN
using the method in [32]. Simonyan et al. [32] derives aattention map by computing the gradient of the class scorewith respect to the input image. As can be seen in Fig. 5, P-CNN relies on almost the whole face region, while PG-CNN2213
(a) Image (b) Attention 
maps of (a),  
derived from P-
CNN 
neutral disgust fear 
sad surprise 
(c) Attention 
maps of (a), 
derived from 
PG-CNN (d) Image (e) Attention 
maps of (d), 
derived from P-
CNN (f) Attention 
maps of (d), 
derived from 
PG-CNN happy 
Fig. 6. Attention maps of several test images results of test images with real
occlusion in RAF-DB. Better viewed in color and zoom in.
attends to local discriminative patches. This can decrease the
probability that an occluder take effect in PG-CNN. Moreover,PG-CNN responses weakly to an occluder and shifts attentionfrom the occluded patch(e.g., right eye in the subÔ¨Ågures fordisgust ) to other related but unobstructed one(e.g., left eye).
Fig. 6 displays images with real occluders picked from testset in RAF-DB and AffectNet corpus. PG-CNN performs asconsistently as on artiÔ¨Åcial occluders. Take angry category for
instance, we observe only PG-CNN attends to subjects‚Äô nose,which is a strongly discriminative patch for anger.
V. C
ONCLUSION
This work presents a Patch-Gated CNN for facial expres-
sion recognition under occlusion. PG-CNN consists of regiondecomposition, Patch Gated Unit for robust facial expres-sion recognition. Experiments under intra and cross databaseevaluation protocols demonstrated PG-CNN outperforms otherstate-of-the-art methods. Ablation analyses show PG-CNN iscapable of shifting attention from occluded patch to otherrelated ones. For future work, we will study how to generateattention parts in face without landmarks, as PG-CNN relies onrobust face detection and facial landmark localization modules.
A
CKNOWLEDGMENT
This work was partially supported by National Key R&D
Program of China under contracts NO.2017YFB1002802,Natural Science Foundation of China (grants 61702481 and61702486), and External Cooperation Program of CAS (grantGJHZ1843). Thanks also to Xin Liu for discussions.
R
EFERENCES
[1] P . Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews,
‚ÄúThe extended cohn-kanade dataset (ck+): A complete dataset for actionunit and emotion-speciÔ¨Åed expression,‚Äù in CVPRW, 2010 .
[2] M. Pantic, M. V alstar, R. Rademaker, and L. Maat, ‚ÄúWeb-based database
for facial expression analysis,‚Äù in ICME, 2005 .
[3] G. Zhao, X. Huang, M. Taini, S. Z. Li, and M. Pietik ¬®aInen, ‚ÄúFacial
expression recognition from near-infrared videos,‚Äù Image & Vision
Computing , vol. 29, no. 9, pp. 607‚Äì619, 2011.
[4] S. Li, W. Deng, and J. Du, ‚ÄúReliable crowdsourcing and deep locality-
preserving learning for expression recognition in the wild,‚Äù in CVPR,
2017 .[5] A. Mollahosseini, B. Hasani, and M. H. Mahoor, ‚ÄúAffectnet: A database
for facial expression, valence, and arousal computing in the wild,‚Äù arXiv
preprint arXiv:1708.03985 , 2017.
[6] J. Wright, A. Y . Yang, A. Ganesh, S. Sastry, and Y . Ma, ‚ÄúRobust face
recognition via sparse representation,‚Äù IEEE TPAMI , vol. 31, no. 2, pp.
210‚Äì227, 2009.
[7] E. Osherov and M. Lindenbaum, ‚ÄúIncreasing cnn robustness to occlu-
sions by reducing Ô¨Ålter support,‚Äù in CVPR, 2017 .
[8] M. Ranzato, J. Susskind, V . Mnih, and G. E. Hinton, ‚ÄúOn deep generative
models with applications to recognition,‚Äù in CVPR, 2011 .
[9] X. Mao, Y . Xue, Z. Li, K. Huang, and S. Lv, ‚ÄúRobust facial expression
recognition based on rpca and adaboost,‚Äù in WIAMIS, 2009 .
[10] I. Kotsia, I. Buciu, and I. Pitas, ‚ÄúAn analysis of facial expression
recognition under partial facial image occlusion,‚Äù Image and Vision
Computing , vol. 26, no. 7, pp. 1052‚Äì1067, 2008.
[11] A. M. Martinez, ‚ÄúRecognizing imprecisely localized, partially occluded,
and expression variant faces from a single sample per class,‚Äù IEEE
TPAMI , vol. 24, no. 6, pp. 748‚Äì763, 2002.
[12] L. Zhong, Q. Liu, P . Yang, B. Liu, J. Huang, and D. N. Metaxas,
‚ÄúLearning active facial patches for expression analysis,‚Äù in CVPR, 2012 .
[13] A. Dapogny, K. Bailly, and S. Dubuisson, ‚ÄúConÔ¨Ådence-weighted local
expression predictions for occlusion handling in expression recognition
and action unit detection,‚Äù IJCV , pp. 1‚Äì17, 2017.
[14] W. Li, F. Abitahi, and Z. Zhu, ‚ÄúAction unit detection with region
adaptation, multi-labeling learning and optimal temporal fusing,‚Äù in
CVPR, 2017 .
[15] L. Zhang, D. Tjondronegoro, and V . Chandran, ‚ÄúRandom gabor based
templates for facial expression recognition in images with facial occlu-
sion,‚Äù Neurocomputing , vol. 145, pp. 451‚Äì464, 2014.
[16] R. Min, A. Hadid, and J. L. Dugelay, ‚ÄúImproving the recognition of
faces occluded by facial accessories,‚Äù in Automatic Face & Gesture
Recognition and Workshops, 2011 .
[17] X. Huang, G. Zhao, W. Zheng, and M. Pietikinen, ‚ÄúTowards a dynamic
expression recognition system under facial occlusion,‚Äù Pattern Recogni-
tion Letters , vol. 33, no. 16, pp. 2181‚Äì2191, 2012.
[18] H. Towner and M. Slater, ‚ÄúReconstruction and recognition of occluded
facial expressions using pca,‚Äù in ACII, 2007 .
[19] Y . Deng, D. Li, X. Xie, K. Lam, and Q. Dai, ‚ÄúPartially occluded face
completion and recognition,‚Äù in ICIP , 2009 .
[20] H. Zheng, J. Fu, T. Mei, and J. Luo, ‚ÄúLearning multi-attention convo-
lutional neural network for Ô¨Åne-grained image recognition,‚Äù in ICCV ,
2017 .
[21] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel,
and Y . Bengio, ‚ÄúShow, attend and tell: Neural image caption generation
with visual attention,‚Äù in ICML, 2015 .
[22] C. Zhu, Y . Zhao, S. Huang, K. Tu, and Y . Ma, ‚ÄúStructured attentions
for visual question answering,‚Äù in
CVPR, 2017 .
[23] L. Zhao and W. Zhuang, Jingdong, ‚ÄúPart-aligned network for person
re-identiÔ¨Åcation,‚Äù in ICCV , 2017 .
[24] K. Simonyan and A. Zisserman, ‚ÄúV ery deep convolutional networks for
large-scale image recognition,‚Äù arXiv preprint arXiv:1409.1556 , 2014.
[25] J. Zhang, M. Kan, S. Shan, and X. Chen, ‚ÄúOcclusion-free face alignment:
deep regression networks coupled with de-corrupt autoencoders,‚Äù inCVPR, 2016 .
[26] C. F. Benitez-Quiroz, R. Srinivasan, Q. Feng, Y . Wang, and A. M.
Martinez, ‚ÄúEmotionet challenge: Recognition of facial expressions ofemotion in the wild,‚Äù arXiv preprint arXiv:1703.01210 , 2017.
[27] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell, ‚ÄúCaffe: Convolutional architecture for
fast feature embedding,‚Äù in ACM Multimedia, 2014 .
[28] A. Dapogny, K. Bailly, and S. Dubuisson, ‚ÄúConÔ¨Ådence-weighted local
expression predictions for occlusion handling in expression recognition
and action unit detection,‚Äù IJCV , pp. 1‚Äì17, 2016.
[29] A. Mollahosseini, D. Chan, and M. H. Mahoor, ‚ÄúGoing deeper in facial
expression recognition using deep neural networks,‚Äù in WACV , 2016 .
[30] C. Mayer, M. Eggers, and B. Radig, ‚ÄúCross-database evaluation for facial
expression recognition,‚Äù Pattern recognition and image analysis , vol. 24,
no. 1, pp. 124‚Äì132, 2014.
[31] X. Zhang, M. H. Mahoor, and S. M. Mavadati, ‚ÄúFacial expression
recognition using {l}
{p}-norm mkl multiclass-svm,‚Äù Machine Vision
and Applications , vol. 26, no. 4, pp. 467‚Äì483, 2015.
[32] K. Simonyan, A. V edaldi, and A. Zisserman, ‚ÄúDeep inside convolutional
networks: Visualising image classiÔ¨Åcation models and saliency maps,‚Äù
arXiv preprint arXiv:1312.6034 , 2013.2214
"
http://ieeexplore.ieee.org/document/9143068/,"Received July 1, 2020, accepted July 9, 2020, date of publication July 17, 2020, date of current version July 29, 2020.
Digital Object Identifier 10.1 109/ACCESS.2020.3010018
Pyramid With Super Resolution for In-The-Wild
Facial Expression Recognition
THANH-HUNG VO
 , GUEE-SANG LEE
 , (Member, IEEE),
HYUNG-JEONG YANG
 , (Member, IEEE), AND SOO-HYUNG KIM
 , (Member, IEEE)
Department of Electronic Computer Engineering, Chonnam National University, Gwangju 61186, South Korea
Corresponding author: Soo-Hyung Kim (shkim@jnu.ac.kr)
This work was supported by the National Research Foundation of Korea (NRF) funded by the Ministry of Education through the Basic
Science Research Program under Grant NRF-2018R1D1A3A03000947 and Grant NRF-2020R1A4A1019191.
ABSTRACT Facial Expression Recognition (FER) is a challenging task that improves natural
human-computer interaction. This paper focuses on automatic FER on a single in-the-wild (ITW) image.
ITW images suffer real problems of pose, direction, and input resolution. In this study, we propose a
pyramid with super-resolution (PSR) network architecture to solve the ITW FER task. We also introduce
a prior distribution label smoothing (PDLS) loss function that applies the additional prior knowledge of the
confusion about each expression in the FER task. Experiments on the three most popular ITW FER datasets
showed that our approach outperforms all the state-of-the-art methods.
INDEX TERMS Emotion recognition, image resolution, human computer interaction.
I. INTRODUCTION
Non-verbal communication plays an essential role in
person-person communication. These non-verbal signals can
add clues, additional information, and meaning to spo-
ken (verbal) communication. Some studies estimate that
around 60% to 80% of communication is non-verbal [1].
These signals include facial expressions, eye contact, voice
tone and pitch, gestures, and physical distance, of which
facial expression is the most popular input for analysis. The
facial expression recognition (FER) task aims to recognize
the emotion from the facial image.
In psychology and computer vision, emotion can be
divided into two kinds of model: discrete and dimensional
continuous [2][4]. The dimensional continuous model
focuses on arousal and valence, which values from -1.0 to
1.0, whereas the discrete emotion theory classies a few core
emotions such as happy, sad, angry, neutral, surprise, disgust,
fear and contempt. In our study, we attempted discrete emo-
tion recognition .
Ekman and Friesen developed a Facial Action Coding Sys-
tem (FACS) to analyze human facial movements [5]. How-
ever, this scheme needs trained humans and is extensively
time consuming. The recent advances of successful machine
The associate editor coordinating the review of this manuscript and
approving it for publication was Waleed Alsabhan
 .learning in computer vision could help simplify and automate
those processes. The scope of our study is automatic facial
expression recognition, where emotional expression is in the
discrete model.
Many studies use traditional image processing and
machine learning for the FER task. Shan et al. used
local statistical features, termed Local Binary Patterns,
for person-independent facial expression recognition [6].
Ma and Khorasani used one-hidden-layer feed forward
neural network on a two-dimensional discrete cosine
transform [7]. Lien et al. combined facial feature point track-
ing, dense ow tracking, and gradient component detec-
tion to detect FACS and calculate emotion [8]. In [9],
Zhang et al. extracted scale-invariant feature transform and
used the deep neural network (DNN) as the classier. Aleksic
and Katsaggelos used hidden Markov models for automatic
FER [10].
Recently, deep learning (DL) has signicantly affected
many elds, such as image, voice, and natural language
processing. In the Boosted Deep Belief Network [14] intro-
duced by Liu et al. , multiple deep belief networks learned
feature representation from patches of image and some of
them were selected to boost. In [15], Liu et al. ensembled
three convolutional neural networks (CNN) subnets and con-
catenated the outputs to predict the nal results. Huang [16]
used a custom residual block of the ResNet architecture
131988This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/VOLUME 8, 2020
T.-H. Vo et al. : PSR for ITW FER
and late fusion to combine the results from the VGG and
the ResNet models. Zeng et al. extracted image histogram
of oriented gradients and passed them through deep sparse
autoencoders to classify them [17]. Tozadore et al. grouped
emotions into several groups to help CNN classify with better
accuracy [18].
Despite these successes of in-the-lab datasets, the rise of
the in-the-wild (ITW) dataset in recent years has raised new
challenges for researchers. When in-the-lab datasets were
collected under control, the data were clean, accurate, and
uniform. In contrast, ITW datasets are noisy, inaccurate, and
variant. We outline the following two observations about ITW
datasets for the FER task.
Observation 1: The images size of the ITW datasets
varies . While the size of in-the-lab datasets images is con-
trolled and nearly constant, ITW dataset images have various
sizes from too small to large. Figure 1 shows the image
size distribution of the RAF-DB [11], [12] (Fig. 1a) and the
AffectNet [13] dataset (Fig. 1b). These two selected datasets
are the most popular ITW datasets for the FER task. Because
of the differences in width and length, the average of the
two is considered as the size of the image. In both datasets,
the small images occur more frequently and this frequency
FIGURE 1. The image size distribution of the RAF-DB [11], [12] and
AffectNet [13] datasets.decreases with increasing size. The mean and variance of the
image size in the RAF-DB are 193 and 144, which is a bit
large. The AffectNet dataset has larger image sizes, ranging
from 130 pixels to more than 2000 pixels. In the graph,
we round all images larger than 2000 pixels to the xed value
of 1000 pixels. Similar to the RAD-DB dataset, the number
of image decreases when the size of the image increases. The
third most popular ITW datasets for the FER task is the FER C
dataset [19] extended from the FER2013 [20]. It also faces
the different-image-size problem. Unfortunately, the original
image size information was omitted when the author of the
FER data published. Most of the studies in this eld does
not consider the image-size problem. They simply resized
all images to the same size, e.g. 128 128 or 224224.
The rst reason is due to the DL framework itself, because
in the batch mode, each batch must have the same input
shape. Implementing different input sizes at the same time
takes more effort, and is complicated and computationally
inefcient. While CNN architecture was successful for many
image classication tasks, it is based on the assumption that
despite the resizing of the images the network could learn to
distinguish by itself. Nearest-neighbor interpolation, bilinear,
and bicubic algorithms are popular techniques to scale image
sizes.
Observation 2: The CNNs are usually sensitized with
input image size. While CNN was very successful for many
tasks related to image classication and segmentation, this
architecture suffers from several weaknesses. One of CNN's
weaknesses is the sensitivity to the size of the input image.
Zooming is one of the data augmentation techniques that
attempts to address this problem. The selected zooming scale
in most of the experiments ranged from 0.9 to 1.2 because
values outside this range degraded and damaged the network.
With global pooling , CNN networks could support different
input sizes, and the size incremental technique was used to
train the networks more quickly and gives coverage easier.
Despite the improvement offered by this process, the network
remains sensitive to the input size. Therefore, the network
trained with this input size works poorly with the same images
but on a different scale. Figure 2 shows the training and
validation loss for VGG16 when training with the RAF-DB
and the FERCin different scales: 50 50, 100100,
150150 and back to 5050 again in RAF-DB and 48 48,
9696, 192192 and again 4848 in the FERCfor every
20 epochs in the sequence. We use weights transfer from the
ImageNet [21], and then, we freeze the whole CNN architec-
ture except the fully connected layers. The freeze steps were
trained in 20 epochs at the smallest input image size. At the
point of image size change (epoch 41, 61, 81), the loss of
both training and validation set a signicant increase. At the
epoch 81, although the input size returns to the size 48 48
that was used to train to the network before, the loss value still
increases because of the characteristics of convolution. The
convolution layer uses a kernel (size 3 3, 55, or similar) to
scan the ``pixel'' in the previous layer. Then, even though the
image is the same but in a different scale, the next convolution
VOLUME 8, 2020 131989
T.-H. Vo et al. : PSR for ITW FER
FIGURE 2. The loss value for the training and validation during the
training process as the input size changed for the RAF-DB and the FER C
(VGG16 architecture [22]).
layers learns very different features; therefore, increasing the
kernel size does not help here.
While currently, the super-resolution (SR) step was in
the pre-processing for input, it could be a part of the DL
architecture. SR approaches may be better than the older
algorithms such as nearest-neighbor interpolation, bilinear,
and bicubic to solve the small-image-size problem. The SR
task is used to make the larger image from a low-resolution
image while trying to ll the lost pixels and to avoid the
pixels becoming blurred. From a low-resolution image, e.g.
sizeWH, the SR task is used to make the larger image
kWkHwhere k2, with the aim of making the
new image as clear as possible. While down-scaling the
image from high-resolution to low-resolution is an easy task,
the reverse direction is not. The missing pixels that are
lost from low-resolution need to be recovered. Some recent
research has focused on this problem. Dong et al. intro-
duced the Super-Resolution Convolutional Neural Network
(SRCNN), a deep CNN model that works on low-resolution
and high-resolution feature maps and nally generated a
high-resolution image [23]. The SRCNN is lightweight and
outperforms the bicubic interpolation. Very Deep Super Res-
olution (VDSR) has a similar structure as the SRCNN but
is more in-depth [24]. In [25], Shi et al. makes the efcient
sub-pixel convolutional neural network (ESPCN) that out-
performs the SRCNN. ESPCN improves SRCNN by dealing
with the feature maps at low-resolution and upsampling to thenal image. Ledig et al. used resblocks to build SRResNet
in [26]. Lim et al. proposed enhanced deep super-resolution
network (EDSR) [27]. The EDSR is a modied version of
the SRResNet that removes all batch normalization layers to
reduce computing by 40% while improving the efciency.
They also designed a multi-scale network from the base block
with good results. Hu et al. published a Cascaded Multi-Scale
Cross network that includes a sequence of the cascaded
sub-networks [28]. In recent years, the network for the SR has
deepened, and the accuracy has been improved more. While
the SRCNN is lightweight but low accuracy, the EDSR needs
more computing but generates better results.
Our study has two highlight contributions. Firstly, we pro-
pose a Pyramid with Super-Resolution (PSR) network archi-
tecture to deal with the different-image-size problem for the
ITW FER task. Our approach aims to view an image on sev-
eral scales and uses SR for up-scaling. With many small-size-
image problems in real-world FER datasets, the SR improves
the network performance. Viewing the image on many scales
also helps the network to learn not only at a small local but
also at the global view of input. We also discuss the loss
function and apply it to the FER task where the distribution
of confusion labels are known and can be used.
The rest of this paper is organized as follows. We explain
our proposed methods in section II and introduce the
prior distribution label smoothing (PDLS) loss function in
section III. Dataset information is presented in section IV.
Section V describes the experimental results and discussion.
Finally, we conclude our study in section VI.
II. PYRAMID WITH SUPER-RESOLUTION (PSR) NETWORK
We deal with the various image-size problems by using
a pyramid architecture, which is termed as the PSR net-
work. Figure 3 shows the overall PSR network architecture.
There are six blocks in our approach, including spatial trans-
former network (STN), scaling, low-level feature extractor,
high-level feature extractor, fully connected, and the nal
concatenation block. STN is a simulator of an afne transfor-
mation in a 2D image. The STN is used for face alignment.
The scaling block is the main block, the fundamental idea of
our approach. The details about this block are explained in
the next subsection. After the scaling block, there are several
internal outputs, each of which is one image of the original
input, but in different scales, and hence has different sizes.
Low and high-level feature extractors are two usual parts
in most of the CNN. The fully connected block includes
several fully connected layers and dropout layers. Finally,
we combine all branch outputs with a late fusion technique.
A. THE SPATIAL TRANSFORMER NETWORK (STN) BLOCK
The STN was introduced by Jaderberg et al. [29] and
Daiet al. [30]. The main idea of STN is to align the input
by learning the transformer. This block is comprised of three
parts: localization net, grid generator, and a sampler [29]. The
localization net has several convolution layers, and nally,
a fully connected layer to output , whereis a matrix size
131990 VOLUME 8, 2020
T.-H. Vo et al. : PSR for ITW FER
FIGURE 3. Overall network architecture.
of 23, a representation of an afne transform in a 2D
image. The grid generator then accepts and makes a grid,
and nally, the sampler uses this grid and generates the output
image. The output image is from the input image with rotate,
scale, and transforms operators. The input and output of this
block are images with the same size and the same number of
channels.
Different from in-the-lab images, ITW images are very
different from the head pose direction. We add the STN block
to help the network learn to align the face and make it easier
to recognize.
Our implementation details follow the previously pub-
lished paper [29]. Table 1 shows the details of the internal
layers of this block. For the convolution layers, the parameters
are the input channel, output channel, kernel size, and stride.
The kernel size and stride are needed for the maxpool2d layer.
For the linear layer, only two parameters are needed: the
number of input nodes and that of output nodes. After the
localization, the feature map is attened and passed through
the fully connected part. Our algorithm calculates the size
of the feature map dynamically based on the input size. So,
the block is adaptive to different sizes of the input images.
TABLE 1. The details of STN block.
B. THE SCALING BLOCK
The scaling block is the leading block in our architecture. The
main idea of this block is to view the input image on different
scale from small to large. Belong to that, super-resolution was
used to upscale the image size. As in many CNNs, to ensurethe efciency of memory and computing, the input images
are kept at the same size. And to use the best information
from the input images, they are passed to the network at the
largest attainable size. The input size may be limited by the
computational limit and based on each dataset. While passing
the same size images, as in the rst observation, many of them
were in low-resolution and were up-scaled by using some
traditional algorithm. However, our approach down-scales
them and then up-scales them again using the SR technique.
This block is to view the overall context in the low-resolution
images, along with the high-resolution image to consider the
original features.
In the scaling block the network branches to three or more
sub-networks. All sub-networks work with the same input
image but on a different scale. The latest branch received
the original input images, which had the highest resolution
for the network. Due to the computational limit, most studies
in the eld of image classication use the input image from
100 to maximum 312. For the larger input size, the higher
resolution does not improve the performance. For the batch
mode, all images were resized to the central size before being
passed through to the network. The larger image size is then
down-scaled, and the smaller images needed to be up-scaled.
We call the original input size WH. This process of scale
input is the traditional algorithm, such as Nearest-neighbor
interpolation, bilinear, and bicubic. While the down-scaled
image is safe, the up-scaling from small size images to the
larger size using the traditional algorithm is complicated and
inaccurate. Our approach tends to overcome this issue. The
rst branch is applied to the lowest resolution image, which
was down-scaled from the original input by the simple oper-
ator using mean pooling to implement. We declare the value
stepandkstep for the step scale value between two neighbors.
By the limit of DL, stepis set to 2. A large kstep can be used,
but due to the computational limitation, we restrict kstep to
only 1 or 2. The size of image for the rst branch is
W
2kstepH
2kstep
VOLUME 8, 2020 131991
T.-H. Vo et al. : PSR for ITW FER
Between the rst and the last branch, there are kstep of
SR branches, each of which is a SR block with the scale size
of 2;4;8;:::from the lowest resolution image from the rst
branch. The size of ithSR is given by equation 1.
W
2kstep iH
2kstep i(1)
In case kD1, there is only one SR branch in the scaling
block, and the output size is the same as the original input
size. In case kD2, there are two SR branches, which
have the sizes of [ W=2,H=2] and [ W,H]. Our setup always
ensures that the last SR part has the same size as the original
input size. For the SR task, we use the EDSR architecture
introduced by Lim et al. [27].
By learning how to resample the image size, we assume
that this block can add useful information to this particular
task, and thereby increases the accuracy of the prediction
model.
C. LOW AND HIGH-LEVEL FEATURE EXTRACTOR
Typically, low and high-level feature extractors are combined
in a base network architecture. We choose VGG16 [22] as
the base network because this network is still used as the
base of many recent network for the FER task [31][33].
From the base network, VGG16 [22], we separated into two
parts for two levels of input. The low-level feature extractor
receives the images as input and generates the feature map
corresponding to the data. This block works at low level
of features, e.g., edge, corner, and so on. The high-level
feature extractor receives the feature map from the low-level
part and makes a more in-depth, high-level features for the
input.
While the input is passed through both extractors in this
order, we separated them as two to share across branches.
As in the second observation, we know that the CNNs are
very sensitized with the input size, and here, each branch has
different input sizes. The low-level features for each branch
are quite different and cannot be shared because sharing
low-level layers damages the network. The high-level feature
block is in another environment. At this level, a high-level
feature needs to be learned and is less dependent on the size
of the input. Then the weight of this block can be shared
across branches. The shared weights also act in a similar way
to multi-task learning where the combination helps each task
obtain better results.
The position of the cutting point denotes pos, which is the
position of the convolution layers in the base network, where
we separate the two parts. A lower posvalue means that all
branches share the weights in most of the internal layers,
while the highest value of posseparates all branches. From
the second observation, we assume that the low posvalue
degrades the network. Since the base network is VGG16,
which has 12 convolution layers, the cutting position pos
should be in 0 12, which is the position of corresponding
convolution layers. We analyze the effect of the cutting point
(theposvalue) in the experiments.D. FULLY CONNECTED BLOCK AND CONCATENATION
BLOCK
The fully connected block includes two fully connected layers
(Linear, FC) and several additional layers. The output feature
from the high-level block then passes through this block to get
the vector to represent the score for each label. Depending
on the experiment, we use either seven or eight emotions,
and then the output vector sizes are set to seven or eight,
respectively. We also use BatchNorm1d for the last feature
map, and two dropout layers with pvalues of 0:25 and 0:5 for
the rst and the second FC layers, respectively. The ReLU
activation function was applied after the rst FC layer.
Similar to the high-level feature extractor block, the fully
connected block was also shared among branches.
All branches were fused with the weighted late fusion
strategy. The weight of each branch has been determined
according to the contribution to the nal score of the whole
network.
III. THE PRIOR DISTRIBUTION LABEL
SMOOTHING (PDLS) LOSS FUNCTION
FER for basic emotion is a classication problem, where each
input image is classied into one of seven or eight classes.
Softmax Cross-Entropy is the most popular loss function for
classication tasks. The cross entropy (CE) loss function is
given in equation 2.
CED X
c2Ctclog((zc)) (2)
where :
CE: cross entropy
C: set of classes (labels)
tc: the distribution value of the label cin the ground truth
whereP
c2CtcD1
(zc): softmax function for zc
zc: raw value score for class cfrom the model
In the real world, it is difcult to get the ground truth
distribution for the labels for each sample; therefore, the all
in one assumption was used in most cases. In the ideal case,
the sample belongs to one and only one class; therefore,
the one-hot vector is widely used in the classication task
for labeling, so that equation 2 becomes the simple case of
 log((zk)), where tcD0 for all c2Cbut the correct label
k(tkD1). Then, all parts except the label kare omitted.
The Label Smoothing (LS) loss function has been intro-
duced in other studies [34], [35], and [36]. The formula for LS
is given as equation 3. The main idea here is the contribution
of all incorrect labels. The parameter was set around 0 :9,
meaning that the contribution for other labels is very small;
e.g., for FER task,jCjD8, then the weight for each of them is
0:1=80:0125 and for the correct label is 0 :9125. Although
the weight of the incorrect labels is small, the LS has been
used successfully in many classication tasks. The advantage
of the LS over CE with one-hot is that all label scores pre-
dicted by the model are activated. Then the backpropagation
131992 VOLUME 8, 2020
T.-H. Vo et al. : PSR for ITW FER
TABLE 2. The prior distribution of the emotions on the FER task.
process can learn not only how to increase the score for the
correct label but also how to decrease for the incorrect ones.
LSD log((zk))C X
c2Clog((zc))(1 )
jCj(3)
where :
jCj: size of label set
: parameter control the weight for each part
In the LS loss function, all labels except the correct one
are given equal, i.e., they have a small role and are all equal.
LS can be used extensively in many tasks when there is no
information about the distribution. However, in many tasks
like FER, for a particular correct label, the confusion to other
classes are not uniform. The FER task has two advantages: the
number of labels is small, just seven or eight and, more impor-
tantly, we know that for the particular label, the confusion
for some specic classes is higher than others. For example,
the correct label fearis very likely to be confused with sur-
prise than with disgust . Another example is the disgust facial,
which can easily be mistaken as neutral , orsadness than
anger orfear. If we have this prior knowledge, the smoothing
part should not be a uniform distribution. So, we proposed
an extended version of LS with additional prior knowledge
of the label's confusion call PDLS. The PDLS loss function
was given by two parts: the one-hot and the prior distribution,
as shown in equation 4.
PDLSD X
c2C(tcCdkc(1 ))log((zc)) (4)
where :
is a parameter to control the weight of one-hot and
distribution.
dkcthe prior distribution for the correct label kand the
confusion label c.
All notations in equation 4 are similar to those in equation 2
and 3. The dkcvalue is the new operand in this formula, and it
replaced the uniform distribution1
jCjin the LS loss function.
Thedmatrix has the following properties V
sizeDjCjjCjX
c2CdkcD1;8k2C
argmax (dk1;dk2;:::; dkjCj)Dk;8k2C
The most important part is how to calculate the dkc. Using
Barsoum et al. [19], when correcting the labels for theFER2013 dataset [20], the authors of FER Calso provided the
labels distribution information for every sample. In FER C,
each sample was labeled by ten people, who need to classify
each image to eight basic classes plus two additional classes,
unknown andnon-face . While the correct label's distribution
for each sample is difcult to obtain, we assumed that the
method for making the FER Cis a good approximation for
the ground truth distribution. For each sample s2S,Sis
the FERCdataset, we have the approximate distribution ads.
Since unknown and non-face images are omitted, we only use
information for eight basic emotions, denote by E. Then ads
is a vector in R8when 8 is the sizejEj, andPadsD1.
Equation 5 is to calculate the average distribution for each
ground truth emotion k. In this calculation, we used only the
training set in the FER C.
dkDP
s2Skads
jSkj(5)
where :
dk: the average distribution of the label k,dk2R8
jSkj: the size of the subset Sk,SkS, where the ground
truth emotion is k.[k2ESkDS.
The nal prior distribution dkifor the FER task is provided
in table 2. Each row in the table is dk, and kis one in eight
emotion labels. The columns are the confusion labels, and
there are also eight emotion labels. E.g. dneutral;sadnessD
0:114 means when image in neutral , there is 11.4% chance
to confuse it as sadness . The number in the main diagonal is
always higher than 0 :5 that represents the distribution for its
own emotion. The happiness emotion is very clear and easy
to detect: dhappiness;happinessD0:918, whereas fearand the
disgust are difcult to detect and easy to be confused.
IV. DATASETS
There are three popular ITW datasets for the FER task,
including the FERC[19], RAF-DB [11], [12] and Affect-
Net [13] datasets. In this study, the experiments are con-
ducted with all of them. The eight discrete emotions for
the classication are neutral ,happiness ,surprise ,sadness ,
anger ,disgust ,fearandcontempt . Some previous datasets
and studies used seven of them as they excluded contempt
because it is difcult and rare in the real world. The details
for each dataset are given below.
FERCdataset. The FERCdataset [19] is the rst
ITW dataset among them. The original version is the
VOLUME 8, 2020 131993
T.-H. Vo et al. : PSR for ITW FER
TABLE 3. Number of images in training/testing/validation subsets of the FER C, RAF-DB, and AffectNet datasets.
FER2013 [20] by Goodfellow et al. , released for the ICML
2013 Workshop on Challenges in Representation Learning.
But as the labeling accuracy of the FER2013 dataset is not
reliable, Barsoum et al. reassigned the labels [19]. Ten people
assigned manually the basic emotion for each image in the
FER2013 dataset. The subset of the original images was
excluded if it is classied as unknown ornon-face . The nal
emotion label was assigned based on the voting from the ten
people. The number of people voting for each emotion for
each image was given, which was then used to calculate the
approximate distribution of the emotion over that image.
The dataset includes all the images, each of which has one
person's face aligned. The dataset images were collected from
the Internet by querying many related expression keywords.
There are many kinds of face in the real-world environment,
and their pose and rotation make them more challenging to
recognize. The images were aligned and centered, and they
were scaled slightly differently. All images are low-resolution
and in grayscale with a size of 48 48 pixels. Each corre-
sponding label for each image is also given. The eight basic
emotions are used in this dataset.
Table 3 and gure 4 show the distribution of train, test
and validation on the FER Cdataset. The number of neutral
images is highest, 9,030 on the train set, and 1,102 on the test
set. The disgust emotion has the lowest number of images:
only 107 on train and 15 on test. The contempt emotion has
a similar number of images with disgust : only 115 on train
and 13 on test. Disgust ,contempt andfearhave few images,
compared with the other ve emotions. This is normal in
natural communication where people are usually in neutral
andhappy state and only rarely experience disgust ,contempt
FIGURE 4. The FER Cdata distribution of train/test/valid.orfear. Figure 4 shows that the distribution of emotions on
training, testing, and validation on the FER Care similar.
RAF-DB dataset . Shan Li, Weihong Deng, and Jun-
Ping Du provided the Real-world Affective Faces Database
(RAF-DB) for emotion recognition [11], [12]. The dataset
contains about 30,000 images downloaded from the Internet.
About 40 trained annotators labeled carefully the image. The
dataset has two parts: the single-label subset (basic emotions)
and the two-tab subset (compound emotions). We used the
single-label subset with seven classes of basic emotions. This
subset has 12,271 samples in the training set and 3,068 in the
test set. The number of samples for each emotion is given in
table 3. Notably, the RAF-DB dataset does not include the
contempt expression. Figure 1 shows that images sizes in the
RAF-DB vary from tiny to large, which makes it difcult for
the DL model to deal with.
AffectNet dataset . The AffectNet [13] is the largest
dataset for the FER task. The dataset contains more than one
million images queried from the Internet by using related
expression keywords. There are about 450,000 images man-
ually annotated by trained persons. It also includes train,
validation, and test sets. The test set has not yet been pub-
lished, so most previous studies used validation set as the
test set [13], [37][40]. Because the contempt emotion is
rare in the natural world, some studies [40] used only seven
emotions while other studies [13], [38], [39] analyzed all
eight emotions. Another study used both eight and seven
expressions [37]. Therefore, to compare our results with the
previous studies, we performed experiments with both eight
classes and seven classes.
Table 3 shows the number of samples for each emotion
class on each subset train, validation, and test on the FER C,
RAF-DB, and AffectNet datasets. The name they use for
labels are a little different but can be mapped to the eight
basic emotions as the emotion column. The FER Chas three
separate subsets for training, validation, and testing, while
two others have only two subsets. The AffectNet dataset
has not published the testing subset, so as for most of the
studies in this dataset, the validation is taken as the testing
subset, and the validation subset during the training process
should be randomly selected from the training subset. Similar
to the RAF-DB, the training subset is randomly separated
and then applied to get the training and validation subsets.
Only AffectNet exhibits balanced validation (as the testing),
131994 VOLUME 8, 2020
T.-H. Vo et al. : PSR for ITW FER
while the FERCand RAF-DB are highly unbalanced. Both
the FERCand AffectNet datasets have eight emotions labels,
and the RAF-DB has only seven emotion classes without
contempt emotion expression.
Figure 5 gives some sample images for each class from the
three datasets. In this gure, each column presents one emo-
tion expression. The images in the rst two rows (gure 5a)
is from the FERCdataset, gure 5b is from RAF-DB, and
the rest (gure 5c) are from AffectNet. The last column of
RAF-DB is empty because the RAF-DB dataset has seven
emotions without contempt expression.
FIGURE 5. Sample images from the (a) FER C, (b) RAF-DB and
(c) AffectNet datasets.
V. EXPERIMENTS AND RESULTS
This section reports our experiments and results. Subsec-
tion V-A gives the experimental setup. Results are shown in
subsection V-B. Finally, subsection V-C presents the discus-
sion about our approach and limitations.
A. EXPERIMENTAL SETUP
For all experiments, Fastai [41] and PyTorch [42] were used.
Those toolboxes make DL experiments easier, with many
build-in classes, functions, and also pre-trained models to
reuse.
In DL, the network initialization has a signicant impact on
the training process. Commonly, weights are initially random.
Having a good initialization strategy helps the networks to
learn better and more smoothly. In our case, we carefully
initialize the network weights. The STN block was set to
identical transformation. The SR layers were initialized from
previously published pre-trained model [27]. The base net-
work, VGG16, was trained with different scale input images.
The model weights were then saved and reloaded to our
architecture. The careful initialization step has several advan-
tages. It is easier to train the network, gives quicker network
coverage, and makes a more stable network, leading to fewer
variants.We use Adam optimization algorithm [43] with an adap-
tive learning rate using The One Cycle Policy suggested by
Smith [44]. The learning rates were set to 1e-3 for some later
layers of the network, and 1e-4 for the STN block. The lower
learning rate for STN with the transformation aims to keep
this bock with little change.
The validation set is used to optimize the hyper-parameters,
and then we collected the best models. The hyper-parameters
for all our experiments include the learning rate and the
number of epoch where the network gets the best result.
Those models were used to evaluate the test set. We applied
Test Time Augmentation on the test step. Eight randomly
rotated, zoomed images are generated from each image and
then passed through the model to get the raw score to predict.
The nal raw score is the average of their outputs.
For basic emotions recognition, several metrics are used to
evaluate the results. The rst and most widely used metric is
accuracy , orweighted accuracy (WA), which is the number
of correct answers divided by the total number of the test
samples. But, when the number of samples for each class is
highly unbalanced, WA may have poor performance, partic-
ularly FER task, because the emotions in the real world are
usually unbalanced. Some emotions such as neutral, happy,
or sad are more common than disgust, fear, or contempt. In
this case, unweighted accuracy (UA) should be considered for
the additional evaluation of the system. The UA metric is an
unbiased version of WA. The UA is calculated by the average
of the accuracy of each class. For comparison with other
studies, both WA and UA are adopted in the experiments.
All experiments were run on Ubuntu 18.04, 32G RAM,
GeForce RTX 2080 Ti GPU with 11G GPU RAM.
B. EXPERIMENTAL RESULTS
We report the experimental results for the RAF-DB, FER C,
and AffectNet datasets.
1) RAF-DB DATASET
Table 4 gives the results for the RAF-DB dataset. In previous
studies, the methods in [38], [39], [45] report results in WA
metric, and others [46], [47] report UA metric. We report and
compare with previous ndings in both WA and UA metrics.
Our approach produces signicantly better results than the
recent studies on both metrics. For WA, we get 88.98%, which
is improved by more than 2% in absolute terms or 2.4%
relatively, compared to Wang et al. [39]. In the UA metric,
our approach is 4.05% better in absolute terms compared to
[46] or 5.28% relatively.
Figure 6 shows the confusion matrix for the RAF-DB. It is
shown that the model gives very good accuracy for happiness
andneutral , but the results for disgust andfear are only
54% and 59%, respectively. Disgust images were predicted
asneutral by 17% and fearwas predicted as surprise by 16%.
2) FERCDATASET
Table 5 shows the experimental results on the FER Ctest set.
The highest accuracy is from the PSR model, which achieved
VOLUME 8, 2020 131995
T.-H. Vo et al. : PSR for ITW FER
FIGURE 6. The confusion matrix on the test set for the RAF-DB, FER Cand AffectNet datasets.
TABLE 4. RAF-DB accuracy comparison (%).
89.75%. Compared to the best previous result in the literature
by Albanie et al. [48], our approach is improved by 0.65%.
The average accuracy for our proposed architecture is
69.54% and F1 score (macro) is 74.88% . The low accuracyTABLE 5. FERCaccuracy comparison (%).
ondisgust andfearmakes the F1 score and average accuracy
far lower than the average. Future work should consider
focusing on increasing the number of sample of disgust and
fearto improve the accuracy for these two expressions.
131996 VOLUME 8, 2020
T.-H. Vo et al. : PSR for ITW FER
FIGURE 7. Cumulative accuracy by size on the test set of RAF-DB dataset with the VGG16 (base-line) and the PSR architecture.
Figure 6c shows the confusions matrix on the test set of
the PSR architecture: happiness has the highest accuracy
of 96%, followed by neutral ,surprise andanger . All four
expressions had accuracy above 90%. The lowest accuracy
was for contempt , 23% accuracy. Due to the lack of contempt
images, the model could not learn to distinguish it from neu-
tral,anger , orsadness . Some emotions have high likelihood
of wrong classication: fearpredicted as surprise by 37%,
disgust classied as anger by 33% and sadness classied as
neutral by 22%. These high levels of confusion are typical in
the real world because even for humans, it can be difcult to
distinguish these pairs of emotions.
3) AffectNet DATASET
We compared both eight and seven classes in the AffectNet
dataset. Table 6 shows the results in classication accuracy
(WA). In the classication of eight emotions, our model
archived the accuracy of 60.68%, outperformed the current
state-of-the-art 59.58% achieved by Georgescu et al. [37].
In the seven-emotion task, our model archived the accuracy
of 63.77%, slightly improved relative to the current highest
one at 63.31% [37]. Figure 6b and gure 6d present the confu-
sion matrix for the AffectNet in the seven-class task and eight-
class task, respectively. The happy expression has the highest
detection rate in both cases, followed by the fearemotion.
Surprise, anger, and disgust have a similar performance in
both cases. In the eight-expression task, contempt has the
lowest performance just at 49%.
Figure 7 shows the cumulative accuracy according to the
size of the original image on the base-line network and the
PSR architecture. The PSR was run with the three branchesTABLE 6. AffectNet accuracy comparison (%).
[1;2;1] and the cutting point at the sixth convolution layer,
with the original input size of 100 pixels. The image size
ranged from 23 pixels to about 1200 pixels. Because the
large images were resized to a xed size at 100 pixels,
we consider only those images smaller than 100 pixels to see
how our approach is affected. We omitted the rst twenty
points because they are unstable to calculate the accuracy.
The gure shows that initially, with tiny image sizes less
than 40 pixels, both base-line and PSR are unstable. But after
40 pixels, the PSR architecture is improved and works better
than the base-line network. The PSR maintained this trend to
the end of the dataset because, in our approach, we added the
super-resolution module with double size for a small image
in one of the three branches, and another branch for half size
100=2D50 improved the recognition accuracy.
Figure 8 shows the accuracy discrepancy by size between
PSR and VGG16 on the test set of the RAF-DB dataset. The
blue points are raw values, and yellow ones are the smoothed
version. The accuracy discrepancy represents the speed of
the improvement of the PSR over the baseline network. It is
clear that the improvement had the highest speed when the
VOLUME 8, 2020 131997
T.-H. Vo et al. : PSR for ITW FER
FIGURE 8. The discrepancy of accuracy by size on the test set of the
RAF-DB dataset between PSR and baseline.
TABLE 7. Analysis of the effectiveness of each block on the RAF-DB (%).
original image size ranged from 40 pixels to about 55 pix-
els; it slows down when the size reached between 55 pixels
and 75 pixels, and it becomes lower for 75-85 pixels. After
85 pixels, the improvement continues but at a slow speed.
Notably, in the experiments for RAF-DB, the original input
size is 100 pixels resolution, then 50 pixels is half of the input
size.
4) THE EFFECTIVENESS OF EACH BLOCK
Table 7 shows a comparison between some variations of the
PSR on the RAF-DB dataset. The second row presents the
result of the PSR without the STN block, which means that
there are only the pyramid structure on top of the baseline
network with three branches ( kstepD2). It is clear that on
both WA and UA metrics, this network architecture gets better
results than the VGG16. The improvements are signicant
in both cases of metrics, 2.73% for WA and 3.30% for UA.
This implies that our pyramid with SR has an important role.
When adding STN block to make the full PSR architecture,
we can get a little improvement, about 0.36% in WA and
0.81% in WA metrics. We analyzed the effectiveness of the
super-resolution reconstruction module by breaking down the
PSR without the STN block to three separate branches to see
the contribution of each to the nal fusion. Figure 9 shows the
accuracy of each separated branch and also the fusion of them
on the PSR architecture. As expected, the small size branch
got the lowest accuracy, and the fusion gets the best one when
combining all three branches. The SR branch and original
input size use the same scale input size, one is SR from
the haft size and another is the original input size. Although
using the same scale size, the SR branch performs better
FIGURE 9. The accuracy by original image size on each branch of the PSR
without the STN block on the RAF-DB test-set.
FIGURE 10. The boxplots of performance with different cutting points
(accuracy).
than the original size branch. The discrepancy between SR
and original input size branch is large for the small images,
and it decreases as the size increase. The results clearly
reconrm that the SR branch helps the network improve the
performance when the original image size is small.
Figure 10 shows the performance on the RAF-DB dataset
by the cutting point posof the convolution layers from
VGG16. The network exhibits the lowest performance at the
point posD0, indicating that all the convolution layers are
shared. The accuracy increases as the posvalue increases
but this improvement ceases after the particular cutting point
atposD5. After the fth layer cutting, the accuracy
remains stable around the particular value. This result sup-
ports the second observation, i.e. the CNN is sensitized with
the input size. Sharing some early convolution layers causes
the network to crash. On the other hand, the deeper layers can
be shared because the former convolution layers are learning
the low-level features, and the later convolution layers are
working on more abstract, high-level features.
5) THE SENSITIVITY OF THE NETWORK TO THE DIFFERENT
INPUT IMAGE SIZE
Figure 11 shows the comparison between PSR and
VGG16 about the sensitivity when changing the input image
131998 VOLUME 8, 2020
T.-H. Vo et al. : PSR for ITW FER
FIGURE 11. Visualization of training loss of the PSR and baseline during
the training process of the RAF-DB when changing the original input size
in the sequence 50, 100, 150, and back to 50 pixels again.
TABLE 8. The loss function comparison (accuracy %).
size on the RAF-DB dataset. The training process is similar
as in gure 2a with the rst 20 freeze steps were omitted.
The changing points are in epoch 20, 40, and 60. The graph
exhibits that the PSR is less sensitive than the baseline. After
the changing point, the loss of PSR architecture is slightly
increasing. But the VGG16 has a large increase of loss values.
The results conrm that our approach has the robustness
for the ITW FER task where the original image size varies,
although the CNNs usually sensitized to the input image.
6) THE COMPARISON OF THE THREE DIFFERENT LOSS
FUNCTIONS
Table 8 compares three loss functions, including the CE,
LS and PDLS on the RAF-DB dataset. For each type of
loss function, we conducted on experiment on the baseline
architecture, VGG16, and our proposed network architecture.
In both cases of the VGG16 and PSR network architecture,
the CE loss function gets the lowest accuracy. For the baseline
network, the LS is slightly better than PDLS, by 0.12%. For
the PSR architecture, however, the PDLS is slightly better
than LS with a margin of 0.42%.
Figure 12 shows some sample images from the RAF-DB
dataset that PSR predict the correct emotions while the base-
line network gave the incorrect emotions. All three images are
in the low-resolution which the size ranged between 45 and
56 pixels.
C. DISCUSSION
The experiments have demonstrated the signicant improve-
ment of our approach in FER task on all the three datasets.
Compared to the base network, VGG16, our pyramid archi-
tecture with additional SR block and late fusion greatly
improves the performance. On the RAF-DB dataset, our
accuracy is better by about 2% in WA metric and 4.05% in
FIGURE 12. Sample images in low-resolution in the RAF-DB dataset
where PSR recognizes better than the baseline network.
UA metric, compared to the state-of-the-art results. The most
substantial improvement in accuracy has been obtained on the
RAF-DB dataset. On the AffectNet dataset, PSR improves the
accuracy by 1.01% and 0.46%, compared to the best previous
study, respectively. Although the given input is in a small
size (4848) as the FERCdataset, our PSR model gener-
ated better results. Among the three datasets, the RAF-DB
exhibited the most improvement because the RAF-DB has
many image sizes from 23-100. The AffectNet dataset shows
less improvement. For the FER Cdataset, the dataset includes
the resized and cropped version of images; using the original
version, if it were available, PSR would give better results.
Notable, the different accuracy discrepancies versus the sec-
ond best algorithm in tables 4, 5, and 6 might be due to
each of these tables having a different set of algorithms..
Overall, the pyramid with SR has a signicant improvement
for the FER task on the ITW dataset. The SR branch helps
the network performance on the low-resolution image and
then combining with other branches makes the whole network
better. The STN block also has some improvement.
As in the second observation, the DL networks are sen-
sitized with the image input size, and the low-level block
in each branch is very different. The result shown in
gure 10 supports our assumption. When the posvalue is
decreased, indicated that more layers are shared, including
some low-level convolution layers, the network is degraded.
When the posvalue is increased, indicating that the low-level
features are less shared, the network exhibits better results.
Due to the trade-off between the performance and the com-
puting cost in real practice, the results in gure 10 are useful
for selecting the cutting point.
The experimental results in table 8 reconrmed that LS
loss function is better than CE as in many previous studies
[34][36]. Both the LS and PDLS have better performance
compared to CE, and in the case of the PSR architecture,
PDLS shows a signicant boost. The PDLS loss function
gave a slight improvement over the original LS function in
the FER task, but it varies case by case, it depends on the
network architecture. In the case of VGG16, the experiments
show that the PDLS is nearly equal to LS, which suggests
that future improvements should be needed. The results from
PSR model suggests that either LS or PDLS is good for the
loss function in the FER task, instead of CE.
Despite the signicant improvements presented in our
study, some limitations warrant further research. The rst is
VOLUME 8, 2020 131999
T.-H. Vo et al. : PSR for ITW FER
the step of the scale-up from the lowest resolution. The pyra-
mid architecture has viewed the input on several scales, but a
step is an integer number larger than one, and 2 is a starting
value. But, the double scale is still a tremendous value. While
the scale 1:2 is a good point for most of the augmentation
techniques, and1
1:2(0:83) in the reverse case, we suggest
that the scale step should be 1 :22D1:44, or approximated
as 1:5. For the traditional algorithm, a decimal scaling value
is possible, but it cannot be used for the DL approaches.
The second weakness is the baseline network architecture.
Although several network architectures, more reliable than
VGG16, such as ResNet [49] and SENet [50] have been
reported, we chose the VGG16 as the base network. Although
our approach is general, we can apply many kinds of CNN,
and the re-implementation is needed for each base network.
Our approach is not a simple module, so extra efforts must be
taken to implement case by case. The innovation of another
architecture is left for future work.
VI. CONCLUSION
In this study, we addressed the various different-image-size
problem in the FER task for ITW datasets, where the original
input image size varies. Although the CNNs could work on
the image with a small rotate and scale, they are worthless
when the scale is enormous. The main contribution of this
study is the development of a pyramid network architec-
ture with several branches, each of which works on one
level of input scale. The proposed network is based on the
VGG16 model, but it can be extended to another baseline net-
work architecture. In the PSR architecture, the SR method is
applied for up-scaling the low-resolution input. Experiments
on three ITW FER datasets show that our proposed method
outperforms all the current state-of-the-art methods.
REFERENCES
[1] A. Mehrabian, Nonverbal Communication . New Brunswick, NJ, USA:
Aldine Transaction, 1972.
[2] P. Ekman, ``Are there basic emotions?'' Psychol. Rev. , vol. 99, no. 3,
pp. 550553, 1992.
[3] P. Ekman, ``Basic emotions.,'' in Handbook Cognition Emotion . New York,
NY, USA: Wiley, 1999, pp. 4560.
[4] J. A. Russell, ``A circumplex model of affect.,'' J. Personality Social
Psychol. , vol. 39, no. 6, pp. 11611178, 1980.
[5] P. Ekman and W. Friesen, Facial Action Coding System , vol. 1.
Mountain View, CA, USA: Consulting Psychologists Press, 1978.
[6] C. Shan, S. Gong, and P. W. McOwan, ``Facial expression recognition
based on local binary patterns: A comprehensive study,'' Image Vis. Com-
put., vol. 27, no. 6, pp. 803816, May 2009.
[7] L. Ma and K. Khorasani, ``Facial expression recognition using con-
structive feedforward neural networks,'' IEEE Trans. Syst., Man,
Cybern. B. Cybern. , vol. 34, no. 3, pp. 15881595, Jun. 2004.
[8] J. J. Lien, T. Kanade, J. F. Cohn, and C.-C. Li, ``Automated facial expres-
sion recognition based on FACS action units,'' in Proc. 3rd IEEE Int. Conf.
Autom. Face Gesture Recognit. , 1998, pp. 390395.
[9] T. Zhang, W. Zheng, Z. Cui, Y. Zong, J. Yan, and K. Yan, ``A deep neural
network-driven feature learning method for multi-view facial expression
recognition,'' IEEE Trans. Multimedia , vol. 18, no. 12, pp. 25282536,
Dec. 2016.
[10] P. S. Aleksic and A. K. Katsaggelos, ``Automatic facial expression recog-
nition using facial animation parameters and multistream HMMs,'' IEEE
Trans. Inf. Forensics Security , vol. 1, no. 1, pp. 311, Mar. 2006.[11] S. Li and W. Deng, ``Reliable crowdsourcing and deep locality-preserving
learning for unconstrained facial expression recognition,'' IEEE Trans.
Image Process. , vol. 28, no. 1, pp. 356370, Jan. 2019.
[12] S. Li, W. Deng, and J. P. Du, ``Reliable crowdsourcing and deep locality-
preserving learning for expression recognition in the wild,'' in Proc. 30th
IEEE Conf. Comput. Vis. Pattern Recognit. , Oct. 2017, pp. 25842593.
[13] A. Mollahosseini, B. Hasani, and M. H. Mahoor, ``AffectNet: A database
for facial expression, valence, and arousal computing in the wild,'' IEEE
Trans. Affect. Comput. , vol. 10, no. 1, pp. 1831, Jan. 2019.
[14] P. Liu, S. Han, Z. Meng, and Y. Tong, ``Facial expression recognition via
a boosted deep belief network,'' in Proc. IEEE Conf. Comput. Vis. Pattern
Recognit. , Jun. 2014, pp. 18051812.
[15] K. Liu, M. Zhang, and Z. Pan, ``Facial expression recognition with CNN
ensemble,'' in Proc. Int. Conf. Cyberworlds (CW) , Sep. 2016, pp. 163166.
[16] C. Huang, ``Combining convolutional neural networks for emotion recog-
nition,'' in Proc. IEEE MIT Undergraduate Res. Technol. Conf. (URTC) ,
Nov. 2017, pp. 14.
[17] N. Zeng, H. Zhang, B. Song, W. Liu, Y. Li, and A. M. Dobaie, ``Facial
expression recognition via learning deep sparse autoencoders,'' Neurocom-
puting , vol. 273, pp. 643649, Jan. 2018.
[18] D. C. Tozadore, C. M. Ranieri, G. V. Nardari, R. A. F. Romero, and
V. C. Guizilini, ``Effects of emotion grouping for recognition in human-
robot interactions,'' in Proc. 7th Brazilian Conf. Intell. Syst. (BRACIS) ,
Oct. 2018, pp. 438443.
[19] E. Barsoum, C. Zhang, C. C. Ferrer, and Z. Zhang, ``Training deep
networks for facial expression recognition with crowd-sourced label dis-
tribution,'' in Proc. 18th ACM Int. Conf. Multimodal Interact. , 2016,
pp. 279283.
[20] I. J. Goodfellow, ``Challenges in representation learning: A report on three
machine learning contests,'' Neural Netw. , vol. 64, pp. 5963, Apr. 2015.
[21] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ``ImageNet:
A large-scale hierarchical image database,'' in Proc. IEEE Conf. Comput.
Vis. Pattern Recognit. , Jun. 2009, pp. 248255.
[22] K. Simonyan and A. Zisserman, ``Very deep convolutional networks for
large-scale image recognition,'' 2014, arXiv:1409.1556 . [Online]. Avail-
able: http://arxiv.org/abs/1409.1556
[23] C. Dong, ``Learning a deep convolutional network for image super-
resolution,'' in Computer Vision , vol. 8692, D. Fleet, Ed. Cham, Switzer-
land: Springer, 2014, pp. 184199.
[24] J. Kim, J. K. Lee, and K. M. Lee, ``Accurate image super-resolution using
very deep convolutional networks,'' in Proc. IEEE Conf. Comput. Vis.
Pattern Recognit. (CVPR) , Jun. 2016, pp. 16461654.
[25] W. Shi, J. Caballero, F. Huszar, J. Totz, A. P. Aitken, R. Bishop, D. Rueck-
ert, and Z. Wang, ``Real-time single image and video super-resolution
using an efcient sub-pixel convolutional neural network,'' in Proc. IEEE
Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2016, pp. 18741883.
[26] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta,
A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi, ``Photo-realistic
single image super-resolution using a generative adversarial network,''
inProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jul. 2017,
pp. 46814690.
[27] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee, ``Enhanced deep residual
networks for single image super-resolution,'' in Proc. IEEE Conf. Comput.
Vis. Pattern Recognit. Workshops (CVPRW) , Jul. 2017, pp. 11321140.
[28] Y. Hu, X. Gao, J. Li, Y. Huang, and H. Wang, ``Single image
super-resolution via cascaded multi-scale cross network,'' 2018,
arXiv:1802.08808 . [Online]. Available: http://arxiv.org/abs/1802.08808
[29] M. Jaderberg, K. Simonyan, A. Zisserman, others, and K. Kavukcuoglu,
``Spatial transformer networks,'' in Proc. Adv. Neural Inf. Process. Syst. ,
2015, pp. 20172025.
[30] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei, ``Deformable
convolutional networks,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) ,
Oct. 2017, pp. 764773.
[31] M. Hu, H. Wang, X. Wang, J. Yang, and R. Wang, ``Video facial emo-
tion recognition based on local enhanced motion history image and
CNN-CTSLSTM networks,'' J. Vis. Commun. Image Represent. , vol. 59,
pp. 176185, Feb. 2019.
[32] S. Li, W. Zheng, Y. Zong, C. Lu, C. Tang, X. Jiang, J. Liu, and W. Xia, ``Bi-
modality fusion for emotion recognition in the wild,'' in Proc. Int. Conf.
Multimodal Interact. , Oct. 2019, pp. 589594.
[33] A. Sepas-Moghaddam, A. Etemad, F. Pereira, and P. L. Correia, ``Facial
emotion recognition using light eld images with deep attention-based
bidirectional LSTM,'' in Proc. ICASSP - IEEE Int. Conf. Acoust., Speech
Signal Process. (ICASSP) , May 2020, pp. 33673371.
132000 VOLUME 8, 2020
T.-H. Vo et al. : PSR for ITW FER
[34] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, ``Rethinking
the inception architecture for computer vision,'' in Proc. IEEE Conf.
Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2016, pp. 28182826.
[35] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, ``Learning transferable
architectures for scalable image recognition,'' in Proc. IEEE/CVF Conf.
Comput. Vis. Pattern Recognit. , Jun. 2018, pp. 86978710.
[36] R. M√ºller, S. Kornblith, and G. Hinton, ``When does label smoothing
help?'' in Proc. NIPS , 2019, pp. 46944703.
[37] M.-I. Georgescu, R. T. Ionescu, and M. Popescu, ``Local learning with deep
and handcrafted features for facial expression recognition,'' IEEE Access ,
vol. 7, pp. 6482764836, 2018.
[38] J. Zeng, S. Shan, and X. Chen, ``Facial expression recognition with incon-
sistently annotated datasets,'' in Proc. Eur. Conf. Comput. Vis. , vol. 11217,
2018, pp. 227243.
[39] K. Wang, X. Peng, J. Yang, D. Meng, and Y. Qiao, ``Region attention
networks for pose and occlusion robust facial expression recognition,''
IEEE Trans. Image Process. , vol. 29, pp. 40574069, 2020.
[40] W. Hua, F. Dai, L. Huang, J. Xiong, and G. Gui, ``HERO: Human emotions
recognition for realizing intelligent Internet of Things,'' IEEE Access ,
vol. 7, pp. 2432124332, 2019.
[41] J. Howard. (2018). Fastai . [Online]. Available: https://github.
com/fastai/fastai
[42] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,
A. Desmaison, L. Antiga, and A. Lerer, ``Automatic differentiation in
PyTorch,'' in Proc. NIPS , 2017, pp. 14.
[43] D. P. Kingma and J. Ba, ``Adam: A method for stochastic
optimization,'' 2014, arXiv:1412.6980 . [Online]. Available:
http://arxiv.org/abs/1412.6980
[44] L. N. Smith, ``Cyclical learning rates for training neural networks,''
inProc. IEEE Winter Conf. Appl. Comput. Vis. (WACV) , Mar. 2017,
pp. 464472.
[45] J. Cai, Z. Meng, A. S. Khan, Z. Li, J. O'Reilly, and Y. Tong,
``Probabilistic attribute tree in convolutional neural networks for facial
expression recognition,'' 2018, arXiv:1812.07067 . [Online]. Available:
http://arxiv.org/abs/1812.07067
[46] Y. Fan, J. C. Lam, and V. O. Li, ``Multi-region ensemble convolutional
neural network for facial expression recognition,'' in Articial Neural
Networks and Machine Learning (Lecture Notes in Computer Science),
vol. 11139. Berlin, Germany: Springer, 2018, pp. 8494.
[47] F. Lin, R. Hong, W. Zhou, and H. Li, ``Facial expression recognition with
data augmentation and compact feature learning,'' in Proc. 25th IEEE Int.
Conf. Image Process. (ICIP) , Oct. 2018, pp. 19571961.
[48] S. Albanie, A. Nagrani, A. Vedaldi, and A. Zisserman, ``Emotion recog-
nition in speech using cross-modal transfer in the wild,'' in Proc. ACM
Multimedia Conf. Multimedia Conf. , 2018, pp. 292301.
[49] K. He, X. Zhang, S. Ren, and J. Sun, ``Deep residual learning for image
recognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) ,
Jun. 2016, pp. 770778.
[50] J. Hu, L. Shen, S. Albanie, G. Sun, and E. Wu, ``Squeeze-and-excitation
networks,'' IEEE Trans. Pattern Anal. Mach. Intell. , vol. 42, no. 8,
pp. 20112023, Aug. 2020.
THANH-HUNG VO received the B.Eng. degree
from the Ho Chi Minh City University of Technol-
ogy, in 2010, and the M.Eng. degree from Vietnam
National University, Vietnam, in 2013, all in com-
puter science. He is currently pursuing the Ph.D.
degree with the Pattern Recognition Laboratory,
School of Electronics and Computer Engineering,
Chonnam National University, South Korea. Since
2011, he has been working as a Lecturer with the
Ho Chi Minh City University of Technology. His
research interests include natural language processing, speech, and computer
vision applying machine learning, and deep learning techniques.
GUEE-SANG LEE (Member, IEEE) received the
B.S. degree in electrical engineering and the
M.S. degree in computer engineering from Seoul
National University, South Korea, in 1980 and
1982, respectively, and the Ph.D. degree in com-
puter science from Pennsylvania State Univer-
sity, in 1991. He is currently a Professor with
the Department of Electronics and Computer
Engineering, Chonnam National University, South
Korea. His primary research interests include
image processing, computer vision, and video technology
HYUNG-JEONG YANG (Member, IEEE) received
the B.S., M.S., and Ph.D. degrees from Chonbuk
National University, South Korea. She is currently
a Professor with the Department of Electronics and
Computer Engineering, Chonnam National Uni-
versity, Gwangju, South Korea. Her main research
interests include multimedia data mining, medical
data analysis, social network service data mining,
and video data understanding.
SOO-HYUNG KIM (Member, IEEE) received the
B.S. degree in computer engineering from Seoul
National University, in 1986, and the M.S. and
Ph.D. degrees in computer science from the Korea
Advanced Institute of Science and Technology,
in 1988 and 1993, respectively. Since 1997, he has
been a Professor with the School of Electronics
and Computer Engineering, Chonnam National
University, South Korea. His research interests
include pattern recognition, document image pro-
cessing, medical image processing, and deep learning applications.
VOLUME 8, 2020 132001
"
https://ieeexplore.ieee.org/document/8895215,"Real Time Emotion Recognition from Facial 
Expressions Using CNN Architecture  
 
Mehmet Akif OZDEMIR1,Berkay ELAGOZ1, AysegulALAYBEYOGLU2, Reza SADIGHZADEH3and Aydin AKAN1 
1Department of Biomedical Engineering, 2Department of Computer Engineering, 3Business Administration  
Izmir Katip Celebi University  
Izmir, Turkey  
makif.ozdemir@ikc.edu.tr , berkayelagoz@gmail.com , aysegul.alaybeyoglu@ikc.edu.tr , riza@taimaksan.com , aydin.akan@ikc.edu.tr  
 
 
 
Abstract‚ÄîEmotion is an important topic in different fields 
such as biomedical engineering, psychology, neuroscience and 
health . Emotion recognition could be useful for diagnosis of brain 
and psychological disorders. In recent ye ars, deep learning has 
progressed much in the field of image classification. In this study, 
we proposed a Convolutional Neural Network (CNN ) based 
LeNet architecture for facial expression recognition. First of all, 
we merged 3 dataset s (JAFFE, KDEF and our  custom dataset). 
Then we trained our LeNet architecture for emotion states 
classification. In this study, we achieved  accuracy of 96.43% and 
validation accuracy of 91.81% for classification of 7 different 
emotions through facial expressions.  
Keywords ‚ÄîConvolutional Neural Network;  Deep Learning; 
Emotion Recognition; Facial Expressions , Real Time Detection . 
I. INTRODUCTION  
Although there are many studies in the literature on 
emotion, there is no common or singular definition in the 
literature  about emotion [1]. Emotion is the appearance or 
reflection of a feeling. Distinct from feeling, emotion can be 
either real or sham. For example, feeling of pain can directly 
represent the feeling. But emotions are not felt exactly. 
Emotions present inner situations psychologically [2, 3] . 
Emotion is an important, complex and extensive research 
topic in the fields of biomedical engineering  [4], psychology  
[5], neuroscience  [6] and health  [7]. Emotion detection is an 
important research area in biomedical engineering. Studies in 
this area focus on predicting human emotion and computer -
assisted diagnosis of psychological disorders. There are 
different methods in literature to detect emotional states such 
as electroencephalography (EEG ), galvanic  skin response 
(GSR), speech analysis,  facial expression, multimodal, visual 
scanning behavior  [8-10]. 
In recent years, with the popularization of deep learning, 
great progress has been made in image classification. 
Convolutional neural networks (CNNs) is an artificial neural 
network type that proposed by Yann  LeChun in 1988  [11]. 
Convolutional neural networks are one of the most popular 
deep learning architectures  for image classification, 
recognition, and segmentation.  
Convolutional neural networks built like a human brain 
with artificial neurons and consist of hierarchical multiply hidden layers. These artificial neurons take input from image, 
multiply weight, add bias and then apply activation function. 
So that, artifi cial neurons can be used in image classification, 
recognition, and segmentation by perform simple 
convolutions. By feeding the convolutional neural network 
with more data (huge amount of data), a better and highly 
accurate deep learning model can be achiev ed. 
Deep learning based facial expression recognition is one of 
these methods to detect emotion state (e.g., anger, fear, 
neutral, happiness, disgust, sadness and surprise ) of human. 
This method aims to detect facial expressions automatically to  
identify e motional state with high accuracy. In this method, 
labeled facial images from facial expression dataset are sent to 
CNN and CNN is trained by these images. Then, proposed 
CNN model makes  a determination which facial expression is 
performed.  
Chang et al. u sed CNN model based on ResNet to extract 
feature from Fer2013 and CK+ dataset. Proposed complexity 
perception classification algorithm (CPC) was applied with 
different classifiers (Softmax, LinearSVM, and 
RandomForest). CNN+Softmax with CPC has achieved 
71.35% and 98.78% recognition accuracies for Fer2013 and 
CK+ respectively  [12]. 
Clawson et al. proposed two human centric CNN 
architecture for facial expression recognitio n on CK+ dataset. 
CNN A consists of 1 convolutional layer and 1 max pooling 
layer. CNN B consists of 2 convolutional layers and 2 max 
pooling layers. These architectures trained with 0.0001 initial 
learning rate, 300 epochs  and 10 batch size. According to 
results, proposed model has achieved 93.3% accuracy on CK+ 
images  [13]. 
Nguyen et al. propos ed multi -level18 -layer  CNN model 
similar to VGG. These model does not take only high-level  
features also takes mid-level  features. Plain CNN model has 
reached 69.21% accuracy and proposed multi -level  CNN 
model has reached 73.03% accuracy on Fer2013 dataset  [14].  
Cao et al. proposed CNN model with K -means clustering 
idea and SVM classifier which has  achieved 80.29% accuracy. 
K-means clustering model determines initial value of the 
convolution ke rnel of CNN. SVM layers takes features from 
trained CNN model to classify Fer2013 images  [15]. 
978-1-7281 -2420 -9/19/$31.00 ¬©2019 IEEE  
Ahmed et al. merged different facial expression datasets 
which are CK, CK+, Fer2013, the MUG facial expression 
database, KDEF, AKDEF, and KinFaceW -I/II. Data 
augmentation was applied merged dataset. Proposed CNN 
model consists of 3 convolutional layers with 32, 64,  128 
filters and kernel size are 3x3.  According to results, proposed 
model has reached 96.24% accuracy  [16]. 
Christou et al. proposed 13 layer CNN model that used on 
Fer2013 dataset and achieved 91.12% accuracy on validation 
dataset  [17]. 
Sajjanhar et al. worked on CK+, JAFFE and FACES 
datasets. They trained and used pre -trained CNN models such 
as Inception -V3, VGG -16, VGG -19 and VGG -Face. 
According to results, highest accuracy (97.16%) was obtained 
with VGG -19 model  on FACES dataset  [18]. 
Chen et al. proposed two -stage framework based on 
Difference Convolutional Neural Network (DCNN) that 
trained with CK+ and BU -4DFE datasets. Results showed 
proposed model achieved 95.4% accuracy on CK+ dataset and 
77.4% on BU -4DFE  [19]. 
In this study, we proposed CNN based LeNet architecture 
for facial expression recognition to estimate emotion states of 
human. We merged 3 different datasets  (KDEF, JAFFE and 
our custom dataset). Then, proposed LeNet architecture was 
trained with final dataset for classification of 7 emotion states 
(happy, sad, surprised, angry, disgust, afraid and neutral). The 
aim of the study is to obtain deep learning mode l that achieve  
higher accuracy rate for  emotion recognition through  facial 
expression.  
II. METHODS  
A. Facial Expression Dataset  
There are many open accesses  facial expression dataset in 
literature. We used 3 facial expression datasets . These are 
JAFFE, KDEF and o ur custom dataset . 
JAFFE dataset contains 213 images with 7 facial 
expressions (happy, sad, surprised, angry, disgust, afraid and 
neutral). These images were taken from 10 Japanese female 
models  [20]. An example of images from JAFFE dataset are 
shown in Fig. 1.  
KDEF dataset contains 4900 images with 7 facial 
expressions (happy, sad, surprised, angry, disgust, afraid and 
neutral). Participants are 35 males and 35 females. Dataset 
contains 5 different angles. We used only straight position in 
this study  [21]. An example of images from KDEF dataset are 
shown in Fig. 2.  
 
Fig.1. Example of images from JAFFE facial expression dataset.    
Fig.2. Example of images from KDEF facial expression dataset.  
 Our custom dataset contains 140 images with 7 facial 
expressions (happy, sad, surprised, angry, disgust, afraid and 
neutral). Participants are 1 male and 1 female. Each facial 
expression was expressed 10 times by 1 participant.  
B. Image Preprocessing  
 Containing approximately equal numb ers of face images 
which is seven different facial expressions  were different 
resolutions, because of there were 3 different databases. 
Therefore, first of all, the face circumference was detected 
using the Haar Cascade  library from the pictures. Then, thes e 
detected rectangular facial expressions were clipped and 
recorded to the same size. Also, the pixel values in the images 
were converted to gray images size of  64x64 to be placed in 
neural networks. This process was done to avoid unnecessary 
density in th e neur al networks.  
C. Convolutional Neural  Network Architecture  
With the proposed CNN architecture, it is aimed to educate 
the pixel values in the rectangular region containing facial 
expressions quickly and functionally and to make quick queries 
with the dee p artificial neur al network model formed . The 
proposed CNN structure is summarized in Fig. 3.  The network 
mimics the LeNet  structure used in classification of 2D facial 
expression data and includes the two convolutional layers, two 
max-pooling layers, and one fully connected layer. The 
convolutional layers with kernel size of 2x2 are stacked 
together which are followed by max -pooling layer with kernel 
size of 2x2 and stride of 2. After all operations of convolutional 
layers and max -pooling layers, each frame feeds to the fully 
connected layers and prediction of frames was processed with 
Softmax classifier as seven different facial emotional state.  
D. Network Training  
In training of network, test size determined as 25%. Batch 
size ha s been set as 32 and epoch number was found as 500 to 
converge parameters of network. Learning rate defined as 10-3. 
All k ernel size defined as 2x2 with stride of 2 for convolutional 
layers and max -pooling layers respectively. Number of 
convolutional layer s represented as 16  and 32 respectively.  
Summary of proposed CNN architecture is shown in Table  I. 
E. Real Time Testing  
 After training of proposed CNN architecture, the trained 
model was tested in real time.  First of all , human faces were 
detected with the Haar Cascade  library within 30 images per 
second of the computer camera.  After that , the detected 
images were sent to the model and the classes they belong to 
were queried.  As a result of the predictions, the possibility of 
belonging to which class the facial expression was shown on a 

 
Fig. 3. Propos ed CNN model diagram for  facial emotio n recognition . 
separate screen and the emotion in which class was higher was 
overwritten on the Haar Cascade  frame. This process was 
performed on every 30 frames that occurred every second of 
the camera image obtained in real tim e. 
TABLE I. SUMMARY  OF PROPOSED  CNN  ARCHITECTURE  
Layer (type)  Output Shape  Param #  
conv2d_1 (Conv2D)  (None, 64, 64, 20) 520 
activation_1 (Activation)  (None, 64, 64, 20) 0 
max_pooling2d_1 (MaxPooling2)  (None, 32, 64, 20) 0 
conv2d_2 (Conv2D)  (None, 32, 32, 50) 25050  
activation_2 (Activation)  (None, 32, 32, 50) 0 
max_pooling2d_2 (MaxPooling2)  (None, 16, 16, 50) 0 
flatten_1 (Flatten)  (None, 12800 ) 0 
dense_1 (Dense)  (None, 500) 6400500  
activation_ 3 (Activation)  (None, 500) 0 
dense_2 (Dense)  (None, 7) 3507  
activation_ 4 (Activation)  (None, 7) 0 
Total params: 6,429,577    
Trainable params: 6,429,577    
Non-trainable params: 0    
None    
*Conv2D: 2D Convolutional Layer , Maxpooling2: 2D Max pooling Layer  III. RESULT AND DISCUSSION  
In this study, Keras  and TensorFlow  libraries were used 
for training LeNet CNN architecture and prediction of 
emotion states with proposed deep learning model. Intel I7 
8300 CPU was used for all experiments and training custom 
dataset. Proposed LeNet CNN model was set wi th mentioned 
parameters. Fig. 4 . shows performance metrics (training 
accuracy and  training  loss, validation accuracy and validation 
loss) of proposed architecture during training and testing. 
According to experiment results, training loss was found 
0.0887;  training accuracy was found 96.43%; validation loss 
was found 0.2725 and validation accuracy was found 91.81%. 
When we look at our results, we get better results than 
mentioned studies in introduction section [9,11,12,14,16].  
 
Fig. 4. Performance metric s of proposed architecture.  

 According to Fig. 5 . confusion matrix, proposed LeNet 
model more accurate at prediction of surprised, fear, neutral 
emotion states and less accurate at prediction of sad emotion 
state.  
 
Fig. 5. Confusion matrix of propoesed architecture . 
IV. CONCLUSION  
 This paper proposed a low cost and functionality method 
for real time classification seven different emotions by facial 
expression based on LeNet CNN architecture. In this study, 
facial expression pictures, which can be said has a  small 
number, were successfully trained in CNN and achieved high 
classification accuracy . Using the Haar Cascade  library, the 
effect of unimportant pixels which is outside facial 
expressions was reduced.  In addition, single -depth placement 
of the pixels i n the pictures to networks did not only result in 
loss of success rate, but also reduced training time and number 
of networks.  Using a custom database has provided higher 
validation and test accuracy  than training in existing 
databases.  The real -time test model has the functionality to 
query each image that occurs in every second . 
 Emotion estimation from facial expressions is the area of 
interest of many researchers in the literature. It is hoped that 
this study will be a source of studies that will help i n the early 
detection of diseases from facial expressions and also studies 
of consumer behavior analysis.  
ACKNOWLEDGMENT  
 This work was supported by Scientific Research Projects 
Coordinatorship of Izmir Katip Celebi University.  Project 
number: 2019 -√ñNAP -M√úMF -0003 . REFERENCES  
[1] M. Cabanac, ""What is emotion?,"" Behavioural proc esses, vol. 60, 
pp. 69 -83, 2002.  
[2] R. Roberts, ""What an Emotion Is: a Sketch,"" The Philosophical 
Review, vol. 97, 1988.  
[3] E. Shouse, ""Feeling, emotion, affect,"" M/c journal, vol. 8, no. 6, p. 
26, 2005.  
[4] J. Zhao, X. Mao, and L. Chen, ""Speech emotion recognition using 
deep 1D & 2D CNN LSTM networks,"" Biomedical Signal 
Processing and Control, vol. 47, pp. 312 -323, 2019.  
[5] J. M. B. Fugate, A. J. O'Hare, and W. S. Emmanuel, ""Emotion 
words: Facing change,"" Journal of Experimental Social 
Psychology, vol. 79, pp. 264 -274, 2018.  
[6] J. P. Powers and K. S. LaBar, ""Regulating emotion through 
distancing: A taxonomy, neur ocognitive model, and supporting 
meta -analysis,"" Neuroscience & Biobehavioral Reviews, vol. 96, 
pp. 155 -173, 2019.  
[7] R. B. Lopez and B. T. Denny, ""Negative affect mediates the 
relationship between use of emotion regulation strategies and 
general health in college -aged students,"" Personality and Individual 
Differences, vol. 151, p. 109529, 2019.  
[8] S. Albanie, A. Nagrani, A. Vedaldi, and A. J. a. p. a. Zisserman, 
""Emotion recognition in speech using cross -modal transfer in the 
wild,"" pp. 292 -301, 2018.  
[9] K.-Y. Huang, C. -H. Wu, Q. -B. Hong, M. -H. Su, and Y. -H. Chen, 
""Speech Emotion Recognition Using Deep Neural Network 
Considering Verbal and Nonverbal Speech Sounds,"" in ICASSP 
2019 -2019 IEEE International Conference on Acoustics, Sp eech 
and Signal Processing (ICASSP), 2019, pp. 5866 -5870: IEEE.  
[10] M. Degirmenci, M. A. Ozdemir, R. Sadighzadeh, and A. Akan, 
""Emotion Recognition from EEG Signals by Using Empirical 
Mode Decomposition,"" in 2018 Medical Technologies National 
Congress (TI PTEKNO), 2018, pp. 1 -4. 
[11] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, ""Gradient -Based 
Learning Applied to Document Recognition,"" Proceedings of the 
IEEE , vol. 86, pp. 2278 -2324, 1998.  
[12] T. Chang, G. Wen, Y. Hu, and J. Ma, ""Facial Expression  
Recognition Based on Complexity Perception Classification 
Algorithm,"" arXiv e -prints, Accessed on: February 01, 2018  
Available: 
https://ui.adsabs.harvard.edu/abs/2018arXiv180300185C  
[13] K. Clawson, L. Delicato, and C. Bowerman, ""Human Centric 
Facial Expr ession Recognition ,"" 2018.  
[14] H.-D. Nguyen, S. Yeom, G. -S. Lee, H. -J. Yang, I. Na, and S. H. 
Kim, ""Facial Emotion Recognition Using an Ensemble of Multi -
Level Convolutional Neural Networks,"" International Journal of 
Pattern Recognition and Artificial Int elligence,  2018.  
[15] T. Cao and M. Li, ""Facial Expression Recognition Algorithm 
Based on the Combination of CNN and K -Means,"" presented at the 
Proceedings of the 2019 11th International Conference on Machine 
Learning and Computing, Zhuhai, China, 20 19.  
[16] T. Ahmed, S. Hossain, M. Hossain, R. Islam, and K. Andersson, 
""Facial Expression Recognition using Convolutional Neural 
Network with Data Augmentation ,"" pp. 1 -17, 2019.  
[17] N. Christou and N. Kanojiya, ""Human Facial Expression 
Recognition with Convolution  Neural Networks,"" Singapore, 2019, 
pp. 539 -545: Springer Singapore.  
[18] A. Sajjanhar, Z. Wu, and Q. Wen, ""Deep learning models for facial 
expression recognition,"" in 2018 Digital Image Computing: 
Techniques and Applications (DICTA), 2018, pp. 1 -6: IEEE.  
[19] J. Chen, Y. Lv, R. Xu, and C. Xu, ""Automatic social signal 
analysis: Facial expression recognition using difference 
convolution neural network,"" Journal of Parallel and Distributed 
Computing, vol. 131, pp. 97 -102, 2019.  
[20] M. Lyons, S. A kamatsu, M. Kamachi, and J. Gyoba, ""Coding 
Facial Expressions with Gabor Wavelets ,"" pp. 200 -205, 1998 . 
[21] D. Lundqvist, A. Flykt, and A. √ñhman, ""The Karolinska directed 
emotional faces (KDEF),"" CD ROM from Department of Clinical 
Neuroscience, Psychology section, Karolinska Institutet, vol. 91, p. 
630, 1998.  

"
https://ieeexplore.ieee.org/document/8014981,"Recognition of Affect in the wild using Deep Neural Networks
Dimitrios Kollias‚ãÜMihalis A. Nicolaou‚Ä†Irene Kotsia1,2Guoying Zhao3
Stefanos Zafeiriou‚ãÜ,3
‚ãÜDepartment of Computing, Imperial College London, UK
‚Ä†Department of Computing, Goldsmiths, University of London, UK
3Center for Machine Vision and Signal Analysis, University of Oulu, Finland
1School of Science and Technology, International Hellenic University, Greece
2Department of Computer Science, Middlesex University, UK
‚ãÜ{s.zafeiriou, dimitrios.kollias15 }@imperial.ac.uk,‚Ä†m.nicolaou@gold.ac.uk
Abstract
In this paper we utilize the Ô¨Årst large-scale ‚Äùin-the-wild‚Äù
(Aff-Wild) database, which is annotated in terms of the
valence-arousal dimensions, to train and test an end-to-end
deep neural architecture for the estimation of continuous
emotion dimensions based on visual cues. The proposed
architecture is based on jointly training convolutional
(CNN) and recurrent neural network (RNN) layers, thus
exploiting both the invariant properties of convolutional
features, while also modelling temporal dynamics that
arise in human behaviour via the recurrent layers. V arious
pre-trained networks are used as starting structures which
are subsequently appropriately Ô¨Åne-tuned to the Aff-Wild
database. Obtained results show premise for the utilization
of deep architectures for the visual analysis of human
behaviour in terms of continuous emotion dimensions and
analysis of different types of affect.
1. Introduction
Behavioral modeling and analysis constitute a crucial as-
pect of Human Computer Interaction. Emotion recognition
is a key issue, dealing with multimodal patterns, such as
facial expressions, head pose, hand and body gestures, lin-
guistic and paralinguistic acoustic cues, as well as physi-
ological data [ 19][5][18]. However, building machines
which are able to recognize human emotions is a very chal-
lenging problem. This is due to the fact that the emotion
patterns are complex, time-varying, user and context de-
pendent, especially when considering uncontrolled environ-
ments, i.e., in-the-wild.
Currently deep neural network architectures are the
method of choise for learning-based computer vision,
speech recognition and natural language processing tasks.They have also achieved great performances in emotion
recognition challenges and contests [ 23][14][13][8].
Moreover, end-to-end architectures, i.e., networks designed
- trained, tested and subsequently used - as whole systems,
accepting the raw input data and learning to produce the de-
sired outputs, seem very promising for implementing plat-
forms that can reach the market and be easily used by cus-
tomers and users.
In this paper, we make a considerable effort to go beyond
current practices in facial behaviour analysis, by training
models on large scale data gathered in ‚Äùin-the-wild‚Äù, that is
in entirely uncontrolled conditions. In more detail, we uti-
lize the Ô¨Årst, annotated in terms of continuous emotion di-
mensions, large scale ‚Äùin-the-wild‚Äù database of facial affect,
i.e. the Aff-Wild database [ 27]. Exploiting the abundance
of data available in video-sharing websites, the database is
enriched with spontaneous behaviours (such as subjects re-
acting to an unexpected development in a movie or a series,
a disturbing clip, etc.). The database contains more than 30
hours of video, and around 200 subjects.
Given the Aff-Wild data, we show that it is possible to
build upon the recent breakthroughs in deep learning and
propose, the Ô¨Årst, to the best of our knowledge, end-to-end
trainable system for valence and arousal estimation using
‚Äùin-the-wild‚Äù visual data1.
In the rest of the paper, we Ô¨Årst describe brieÔ¨Çy the Aff-
Wild database (Section 2), afterwards we present the pro-
posed end-to-end deep CNN and CNN-RNN architectures
(Section 3) and then the experimental results (Section 4).
Finally, conclusions and future work are presented in Sec-
1An end-to-end trainable Convolutional Neural Network (CNN) plus
Recurrent Neural Network (RNN) for valence and arousal estimation from
speech has been recently proposed in [ 25]. Furthermore, the recent method
in [12] combines CNN with RNN (but independently trained) for valence
and arousal in the A VEC data that have been captured in controlled condi-
tions [ 22].
2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops
2160-7516/17 $31.00 ¬© 2017 IEEE
DOI 10.1109/CVPRW.2017.2471972

Figure 1: Annotated valence and arousal (Person A)
tion 5.
2. The Aff-Wild Database
Training and testing of the deep neural architectures
has been performed using the Aff-Wild database [ 27]; this
database shows spontaneous facial behaviors in arbitrary
recording conditions, which should be analyzed so as to
detect the valence and arousal emotion parameters. The
database contains 298 videos of 200 people in total.
Figures 1,2show two characteristic sequences of facial
images, taken from different videos of Aff-Wild. They in-
clude the respective video frame numbers and the valence
and arousal annotations for each of them. A visual repre-
sentation of the valence and arousal values is also depicted
on the 2-D emotion space, showing the change in the reac-
tions/behavior of the person among these time instances of
the video. Time evolution is indicated, by using a larger size
for the more recent frames and a smaller size for the older
ones.
Figure 3shows the histogram of valence and arousal an-
notations. It can be seen that the amount of annotated pos-
itive reactions, corresponding to positive valence values, is
larger than that of negative ones. Similarly, the amount of
annotated ‚Äôevident‚Äô reactions, with positive arousal values,
is larger than the less ‚Äôevident‚Äô, or hidden ones, with nega-
tive arousal values. We examine in more detail this issue in
the experimental section of the paper.
2.1. Database Pre-processing
The whole database contains more than 1,180,000
frames (around 900K for training and 300K for testing).
From each frame, we detected faces using the method de-
Figure 2: Annotated valence and arousal (Person B)
Figure 3: Histogram of Annotations
scribed in [ 15] and cropped the faces. Next, we applied the
best performing method of [ 3] in order to track 68 facial
landmark. Since, many of the current pre-trained networks,
such as VGG series of networks, operate on images with
resolution of 224√ó224√ó3we have always resized the
facial images to this resolution.
In order to have a more balanced dataset for training,
we performed data augmentation, mainly through oversam-
pling by re-sampling and duplicating [ 16] some data from
the Aff-Wild database. To be more precise, we re-sampled
data that had negative valence and arousal values, as well
1973
as positive valence and negative arousal values. As a conse-
quence, the training set consisted of about 43% of positive
valence and arousal values, 24% of negative valence and
positive arousal values, 19% of positive valence and nega-
tive arousal values and 14% of negative valence and arousal
values.
3. The End-to-End Deep Neural Architectures
We have developed end-to-end architectures, i.e., archi-
tectures that trained all-together, accepting raw data colour
images, learn to produce 2-D predictions of valence and
arousal.
In particular, we have evaluated the following architec-
tures:
(1) An architecture based on the structure of the ResNet
L50 network [ 9].
(2) An architecture based on the structure of the VGG
Face network [ 20].
(3) An architecture based on the structure of the VGG-16
network [ 24].
We also considered two different approaches (a) an only
frame based approach where only CNNs are trained and (b)
CNN plus RNN end-to-end approaches that can exploit the
dynamic information of the video data. In both settings, we
present experimental results based on the following scenar-
ios:
(1) The network is applied directly on cropped facial video
frames of the generated database, trained to produce
both valence and arousal (V , A) predictions.
(2) The network is trained on both the facial appearance
video frames, as well as the facial landmarks corre-
sponding to the same frame.
Regarding the CNN-RNN architecture, we utilize Long
Short Term Memory (LSTM) [ 10] and Gated Recurrent
Unit (GRU) [ 4] layers, stacked on top of the last fully con-
nected layer. The RNN-LSTM/GRU consists of one or two
hidden layers, along with the output layer that provides the
Ô¨Ånal 2-D emotion predictions. We note that all deep learn-
ing architectures have been implemented in the TensorÔ¨Çow
platform [ 1].
For training, we utilize the Adam optimizer, that pro-
vides slightly better overall performance in comparison to
other methods, such as the stochastic gradient descent. Fur-
thermore, the utilized loss functions for evaluation and
training include the Concordance Correlation CoefÔ¨Åcient
(CCC) and Mean Squared Error (MSE). We primarily fo-
cus on optimizing the CCC, since it can provide better in-
sight on whether the prediction follows the structure of theground truth annotation. In more detail, the CCC is deÔ¨Åned
as
œÅc=2sxy
s2x+s2y+( ¬Øx‚àí¬Øy)2(1)
wheresxandsyare the variances of the predicted and
ground truth values respectively, ¬Øxand¬Øyare the correspond-
ing mean values, while sxandsyare the respective covari-
ance values.
Regarding initialization, in our experiments we trained
the proposed deep architectures by either (i) randomly ini-
tializing the weight values, or (ii) using pre-trained weights
from networks having been pre-trained on large databases,
such as the ImageNet [ 6]. For the second approach we used
transfer learning [ 17], especially of the convolutional and
pooling part of the pre-trained networks. In more detail,
we utilized the ResNet L50 and VGG-16 networks, which
have been pre-trained for object detection tasks, along with
VGG-Face, which has been pre-trained for face recognition
tasks. In all experiments, the VGG-Face provided much
better results, so, in the following we focus on the transfer
learning methodology with weight initialization using the
VGG-Face network; this has been pre-trained on the Face-
V alue dataset [ 2]. It should be noted that when utilizing
pre-trained networks, we experimented based on two ap-
proaches: either performing Ô¨Åne-tuning, i.e., training the
entire architecture with a relatively small learning rate, or
freezing the pre-trained part of the architecture and retrain-
ing the rest (i.e., the fully connected layers of the CNN,
as well as the hidden layers of the RNN). In general, the
procedure of freezing a part of the network and Ô¨Åne-tuning
[11] the rest can be deemed very useful, in particular when
the given dataset is incremented with more videos. This in-
creases the Ô¨Çexibility of the architecture, as Ô¨Åne-tuning can
be performed by simply considering only the new videos.
Training was performed on a single TITAN X (Pascal) GPU
and the training time was around 5 days.
3.1. Implementing the CNN Architectures
In the following we provide speciÔ¨Åc information on the
selected structure and parameters in the used end-to-end
neural architectures, with reference to the results obtained
for each case in our experimental study.
Extensive testing and evaluation has been performed by
selecting different network parameter values, including (1)
the number of neurons in the CNN fully connected layers,
(2) the batch size used for network parameter updating, (3)
the value of the learning rate and the strategy for reducing it
during training (e.g. exponential decay in Ô¨Åxed number of
epochs), (4) the weight decay parameter value, and Ô¨Ånally
(5) the dropout probability value.
With respect to parameter selection in the CNN archi-
tectures, we used a batch size in the range 10 ‚àí100 and an
initial learning rate value in the range 0.0001 ‚àí0.01 with ex-
1974
ponential decay. The best results have been obtained with
batch size 80 and initial learning rate 0.001. The dropout
probability value was 0.5, the decay steps of the exponen-
tial decay of the learning rate were 400, while the learning
rate decay factor was 0.97. The number of neurons per layer
per CNN type is described in the next subsections.
3.1.1 CNN architecture based on ResNet L50
Table 1shows the conÔ¨Åguration of the CNN architecture
based on ResNet L50. This is composed of 9 blocks.
For each convolutional layer the parameters are denoted
as (channels, kernel, stride) and for the max pooling layer
as (kernel, stride). The bottleneck modules are deÔ¨Åned as
in [9]. For the fully connected layers, Table 1shows the
respective number of units. Using three Fully-Connected
(FC) layers was found to provide best results.
Table 1: Architecture for CNN network based on ResNet
L50
block 1 1√óconv layer (64,7√ó7,2√ó2)
batch norm layer
1√ómax pooling (3√ó3,2√ó2)
block 2 3√óbottleneck [(64, 1√ó1),
(64,3√ó3),
(256, 1√ó1)]
block 3 4√óbottleneck [(128, 1√ó1),
(128, 3√ó3),
(512, 1√ó1)]
block 4 6√óbottleneck [(256, 1√ó1),
(256, 3√ó3),
(1024, 1√ó1)]
block 5 6√óbottleneck [(512, 1√ó1),
(512, 3√ó3),
(2048, 1√ó1)]
block 6 1√óaverage pooling
block 7 fully connected 1 1500
dropout layer
block 8 fully connected 2 256
dropout layer
block 9 fully connected 3 2
Table 1refers to our second scenario where both the out-
puts of the last pooling layer of the CNN, as well as the 68
landmark 2-D positions ( 68√ó2values) were provided as
inputs to the Ô¨Årst of the three fully connected (FC) layers of
the architecture. In the contrary, in scenario (1), the outputs
of the last pooling layer of the CNN were the only inputs of
the fully connected layer of our architecture. In this case,
the architecture included only two fully connected layers,
i.e., the 1st and 3rd fully connected ones.
In general, we used 1000 ‚àí1500 units in the Ô¨Årst FC
layer and 200 ‚àí500 units in the second FC layer. The lastlayer consisted of 2 output units, providing the (V , A) pre-
dictions. A linear activation function was used in this last
FC layer, providing the Ô¨Ånal estimates. All units in the other
FC layers were equipped with the rectiÔ¨Åcation (ReLU) non-
linearity.
3.1.2 CNN architecture based on VGG-Face/VGG-16
Table 2shows the conÔ¨Åguration of the CNN architecture
based on VGG-Face or VGG-16. It is also composed of 9
blocks. For each convolutional layer the parameters are de-
noted as (channels, kernel, stride) and for the max pooling
layer as (kernel, stride). Table 2shows the respective num-
ber of units of each fully connected layer. Using four fully
connected layers was found to provide best results.
Table 2: Architecture for CNN network based on VGG-
Face/VGG-16
block 1 2√óconv layer (64,3√ó3,1√ó1)
1√ómax pooling (2√ó2,2√ó2)
block 2 2√óconv layer (128, 3√ó3,1√ó1)
1√ómax pooling (2√ó2,2√ó2)
block 3 3√óconv layer (256, 3√ó3,1√ó1)
1√ómax pooling (2√ó2,2√ó2)
block 4 3√óconv layer (512, 3√ó3,1√ó1)
1√ómax pooling (2√ó2,2√ó2)
block 5 3√óconv layer (512, 3√ó3,1√ó1)
1√ómax pooling (2√ó2,2√ó2)
block 6 fully connected 1 4096
dropout layer
block 7 fully connected 2 4096
dropout layer
block 8 fully connected 3 2622
dropout layer
block 9 fully connected 4 2
Table 2also refers to the second scenario. In this case,
however, best results were obtained, when the 68 landmark
2-D positions ( 68√ó2values) were provided, together with
the outputs of the Ô¨Årst FC layer of the CNN, as inputs to
the second of the four FC layers of the architecture. In sce-
nario 1, the outputs of the Ô¨Årst FC layer of the CNN were
the only inputs to the second fully connected layer of our
architecture. In this case, the architecture included only 3
FC layers, i.e., the 1st, 2nd and 4th FC layers. A linear acti-
vation function was used in the last FC layer, providing the
Ô¨Ånal estimates. All units in the rest FC layers were equipped
with the rectiÔ¨Åcation (ReLU) non-linearity.
3.2. Implementing the CNN-RNN architectures
When developing the CNN-RNN architecture, the RNN
part was fed with the outputs of either the Ô¨Årst, or the second
1975
fully connected layer of the respective CNN network. The
structure of the RNN, which we examined, consisted of one
or two hidden layers, with 100 -150 units, following either
the LSTM neuron model allowing peephole connections, or
the GRU neuron model. Using one fully connected layer in
the CNN part and two hidden layers in the RNN part was
found to provide the best results.
Table 3shows the conÔ¨Åguration of the CNN-RNN ar-
chitecture. The CNN part of this architecture is based on
the convolutional and pooling layers of the CNN architec-
tures described above (in subsection 3.1). It is followed by
a fully connected layer. Note that in the case of the second
scenario, both the outputs of the last pooling layer of the
CNN, as well as the 68 landmark 2-D positions ( 68√ó2val-
ues) were provided as inputs to this fully connected layer.
For the RNN and fully connected layers, Table 3shows the
respective number of units.
Table 3: Architecture for CNN-RNN network based on con-
volution and pooling layers of previously described CNN
architectures
block 1 CNN‚Äôs conv & pooling parts
block 2 fully connected 1 4096
dropout layer
block 3 RNN layer 1 128
dropout layer
block 4 RNN layer 2 128
dropout layer
block 5 fully connected 2 2
Long evaluation has been performed by selecting differ-
ent network parameter values. These parameters included:
the batch size used for network parameter updating; the
value of the learning rate and the strategy for reducing it
during training (e.g. exponential decay in Ô¨Åxed number
of epochs); the weight decay parameter value; the dropout
probability value. Final selection of these parameters was
similar to the CNN cases, apart from the batch size which
was selected in the range 100 (‚âà3 seconds) - 300 (‚âà9 sec-
onds). Best results have been obtained with batch size 100 .
4. Experimental Results
In the following, we provide the main outcomes of the
experimental study, illustrating the above-described cases
and scenarios. In all experiments training and validation
was performed in the training set of the Aff-Wild database,
while testing was performed in the test set of Aff-Wild. The
Ô¨Årst approach we tried was based on extracting SIFT fea-
tures [ 26] from the facial region and then using an Support
V ector Regression (SVR) [ 7] for valence and arousal esti-
mation. For training and testing the SVRs, we utilized the
scikit-learn library [ 21]. Obtained results were very poor. Inparticular, in all cases, the obtained CCC values were very
low, while very low variance was present in the correspond-
ing predictions (we do not present the performance of SVR
in order not to clutter the results).
4.1. Only-CNN architectures
Table 4summarizes the obtained CCC and MSE values
on the test set of Aff-Wild using each of the three afore-
mentioned CNN structures as pre-trained networks. The
best results have been obtained using the VGG Face pre-
trained CNN for initialization as shown in Table 4. There-
fore, we focus on utilizing this conÔ¨Åguration for the results
presented in the rest of this section. Moreover, Table 5
shows that there is a signiÔ¨Åcant improvement in the per-
formance, when we also use the 68 2-D landmark positions
as input data (case with landmarks). It should be also noted
that we have examined the following scenarios (a) having
one network for joined estimation of valence and arousal
and (b) estimation of the values of valence and arousal us-
ing two different networks (one for valence and one for
arousal). Slightly better results were obtained on the lat-
ter case; so, this architecture is being used in the following
results.
Furthermore, we have trained the networks with two dif-
ferent annotations. The Ô¨Årst is the annotation provided by
the Aff-Wild database, which is the average over some an-
notators (please see the Aff-Wild[ 27]). The second is the
annotation produced by only one annotator (the one with the
highest correlation to the landmarks). Annotations coming
from a single annotator are generally less smooth than av-
erage over annotators. Hence, they are more difÔ¨Åcult to be
learned. The results are summarized in Table 6.A s i t w a s
expected it is better to train over the annotation provided by
Aff-Wild[ 27].
Table 4: CCC and MSE evaluation of valence & arousal
predictions provided by the CNN architecture when using 3
different pre-trained networks for initialization
CCC MSE
V alence Arousal V alence Arousal
VGG Face 0.46 0.35 0.10 0.09
VGG-16 0.40 0.30 0.13 0.11
ResNet-50 0.33 0.24 0.16 0.13
4.2. CNN plus RNN architectures
Let us now turn to the application of CNN plus RNN
end-to-end neural architecture on Aff-Wild. We Ô¨Årst per-
form a comparison between two different units that can be
used in an RNN network, i.e. an LSTM vs GRU. Table
9summarises the CCC and MSE values when using LSTM
and GRU. It can be seen that best results have been obtained
1976
Table 5: CCC and MSE evaluation of valence & arousal
predictions provided by the best CNN network (based on
VGG Face), with/without landmarks
With Landmarks Without Landmarks
V alence Arousal V alence Arousal
CCC 0.46 0.35 0.38 0.31
MSE 0.10 0.09 0.14 0.11
Table 6: CCC and MSE evaluation of valence & arousal
predictions provided by the best CNN network (based on
VGG Face), using either one annotators values or the mean
of annotators values
1 Annotator Mean of Annotators
V alence Arousal V alence Arousal
CCC 0.35 0.25 0.46 0.35
MSE 0.18 0.14 0.10 0.09
Table 7: Comparison of best CCC and MSE values of va-
lence & arousal provided by best CNN and CNN-RNN ar-
chitectures
CCC MSE
V alence Arousal V alence Arousal
CNN 0.46 0.35 0.10 0.09
CNN-RNN 0.57 0.43 0.08 0.06
Table 8: Effect of Changing Number of Hidden Units &
Hidden Layers for CCC valence & arousal values in the
CNN-RNN architecture
1 Hidden Layer 2 Hidden Layers
Hidden Units V alence Arousal V alence Arousal
100 0.40 0.33 0.47 0.40
128 0.49 0.40 0.57 0.43
150 0.44 0.37 0.50 0.41
when the GRU model was used. All results reported in the
following are, therefore, based on the GRU model. Table
7shows the improvement in the CCC and MSE values ob-
tained when using the best CNN-RNN end-to-end neural ar-
chitecture compared to the best only-CNN one. In particu-
lar, we compare networks that take as input facial landmarks
and are based on the pre-trained VGG Face network. It can
be seen that this improvement is about 24% in valence esti-
mation and about 23% in arousal estimation, which clearly
indicates the ability of the CNN-RNN architecture to better
capture the dynamic phenomenon.
We have tested various numbers of hidden layers and
hidden units per layer when training and testing the CNN-
RNN network. Some characteristic selections and the cor-
Figure 4: Predictions vs Ground Truth for valence for a part of
a video
Figure 5: Predictions vs Ground Truth for arousal for a part of
a video
responding CNN-RNN performances are shown in Table 8.
Table 9: CCC and MSE evaluation of valence & arousal
predictions provided by the best CNN-GRU and CNN-
LSTM architectures that had same network conÔ¨Ågurations
(2 hidden layers with 128 units each)
CCC MSE
V alence Arousal V alence Arousal
CNN-GRU 0.57 0.43 0.08 0.06
CNN-LSTM 0.49 0.38 0.10 0.09
In Figures 4and 5, we qualitatively illustrate some of the
obtained results, comparing a segment of the obtained va-
lence/arousal predictions compared to the ground truth val-
ues, in over 6000 consecutive frames of test data.
1977
5. Conclusions and future work
In this paper, we present the design, implementation
and testing of deep learning architectures for the problem
of analysing human behaviour utilizing continuous dimen-
sional emotions. In more detail, we present, to the best
of our knowledge, the Ô¨Årst such architecture that is trained
on hundred thousands of data, gathered ‚Äùin-the-wild‚Äù (i.e.,
in entirely uncontrolled conditions) and annotated in terms
of continuous emotion dimensions. It should be empha-
sized that a major challenge in facial expression and emo-
tion recognition lies in the large variability of spontaneous
expressions and emotions, arising in uncontrolled environ-
ments. This prevents pre-trained models and classiÔ¨Åers to
be successfully utilized in new settings and unseen datasets.
In the current paper, our focus has been on experiments in-
vestigating the ability of the proposed deep CNN-RNN ar-
chitectures to provide accurate predictions of the 2D emo-
tion labels in a variety of scenarios, as well as with cross-
database experiments. Presented results are very encourag-
ing, and illustrate the ability of the presented architectures
to predict the values of continuous emotion dimensions on
data gathered ‚Äùin-the-wild‚Äù. Planned future work lies in
extending the analysis to simultaneously interpret the be-
haviour of multiple subjects appearing in videos, as well
as to further extend the derived representations obtained by
the CNN-RNN architectures for subject and setting speciÔ¨Åc
adaptation.
6. Acknowledgments
The work of Stefanos Zafeiriou has been partially
funded by the FiDiPro program of Tekes (project num-
ber: 1849/31/2015). The work of Dimitris Kollias was
funded by a Teaching Fellowship of Imperial College Lon-
don. We would like also to acknowledge the contribution of
the Y outube users that gave us the permission to use their
videos (especially Zalzar and Eddie from The1stTake).
References
[1] M. Abadi, A. Agarwal, P . Barham, E. Brevdo, Z. Chen,
C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghe-
mawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y . Jia,
R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man ¬¥e,
R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,
J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P . Tucker,
V . V anhoucke, V . V asudevan, F. Vi ¬¥egas, O. Vinyals, P . War-
den, M. Wattenberg, M. Wicke, Y . Y u, and X. Zheng. Tensor-
Flow: Large-scale machine learning on heterogeneous sys-
tems, 2015. Software available from tensorÔ¨Çow.org.
[2] S. Albanie and A. V edaldi. Learning grimaces by watching
tv.arXiv preprint arXiv:1610.02255 , 2016.
[3] G. G. Chrysos, E. Antonakos, P . Snape, A. Asthana, and
S. Zafeiriou. A comprehensive performance evaluationof deformable face tracking‚Äù in-the-wild‚Äù. arXiv preprint
arXiv:1603.06015 , 2016.
[4] J. Chung, C. Gulcehre, K. Cho, and Y . Bengio. Empirical
evaluation of gated recurrent neural networks on sequence
modeling. arXiv preprint arXiv:1412.3555 , 2014.
[5] C. Corneanu, M. Oliu, J. Cohn, and S. Escalera. Survey on
rgb, 3d, thermal, and multimodal approaches for facial ex-
pression recognition: History, trends, and affect-related ap-
plications. IEEE transactions on pattern analysis and ma-
chine intelligence , 2016.
[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database.
InComputer Vision and Pattern Recognition, 2009. CVPR
2009. IEEE Conference on , pages 248‚Äì255. IEEE, 2009.
[7] H. Drucker, C. J. Burges, L. Kaufman, A. Smola, V . V ap-
nik, et al. Support vector regression machines. Advances in
neural information processing systems , 9:155‚Äì161, 1997.
[8] I. Goodfellow, Y . Bengio, and A. Courville. Deep Learning .
MIT Press, 2016. http://www.deeplearningbook.
org .
[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition , pages
770‚Äì778, 2016.
[10] S. Hochreiter and J. Schmidhuber. Long short-term memory.
Neural computation , 9(8):1735‚Äì1780, 1997.
[11] H. Jung, S. Lee, J. Yim, S. Park, and J. Kim. Joint Ô¨Åne-tuning
in deep neural networks for facial expression recognition. In
Proceedings of the IEEE International Conference on Com-
puter Vision , pages 2983‚Äì2991, 2015.
[12] P . Khorrami, T. Le Paine, K. Brady, C. Dagli, and T. S.
Huang. How deep neural networks can improve emotion
recognition on video data. In Image Processing (ICIP), 2016
IEEE International Conference on , pages 619‚Äì623. IEEE,
2016.
[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
classiÔ¨Åcation with deep convolutional neural networks. In
Advances in neural information processing systems , pages
1097‚Äì1105, 2012.
[14] Y . LeCun, Y . Bengio, and G. Hinton. Deep learning. Nature ,
521(7553):436‚Äì444, 2015.
[15] M. Mathias, R. Benenson, M. Pedersoli, and L. V an Gool.
Face detection without bells and whistles. In European Con-
ference on Computer Vision , pages 720‚Äì735. Springer, 2014.
[16] A. More. Survey of resampling techniques for improving
classiÔ¨Åcation performance in unbalanced datasets. arXiv
preprint arXiv:1608.06048 , 2016.
[17] H.-W. Ng, V . D. Nguyen, V . V onikakis, and S. Winkler.
Deep learning for emotion recognition on small datasets us-
ing transfer learning. In Proceedings of the 2015 ACM on
International Conference on Multimodal Interaction , pages
443‚Äì449. ACM, 2015.
[18] M. A. Nicolaou, H. Gunes, and M. Pantic. Continuous pre-
diction of spontaneous affect from multiple cues and modal-
ities in valence-arousal space. Affective Computing, IEEE
Transactions on , 2(2):92‚Äì105, 2011.
1978
[19] M. Pantic and L. J. Rothkrantz. Automatic analysis of fa-
cial expressions: The state of the art. Pattern Analysis and
Machine Intelligence, IEEE Transactions on , 22(12):1424‚Äì
1445, 2000.
[20] O. M. Parkhi, A. V edaldi, and A. Zisserman. Deep face
recognition. In BMVC , volume 1, page 6, 2015.
[21] F. Pedregosa, G. V aroquaux, A. Gramfort, V . Michel,
B. Thirion, O. Grisel, M. Blondel, P . Prettenhofer, R. Weiss,
V . Dubourg, J. V anderplas, A. Passos, D. Cournapeau,
M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Ma-
chine learning in Python. Journal of Machine Learning Re-
search , 12:2825‚Äì2830, 2011.
[22] F. Ringeval, B. Schuller, M. V alstar, R. Cowie, and M. Pan-
tic. Avec 2015: The 5th international audio/visual emotion
challenge and workshop. In Proceedings of the 23rd ACM
international conference on Multimedia , pages 1335‚Äì1336.
ACM, 2015.
[23] J. Schmidhuber. Deep learning in neural networks: An
overview. Neural Networks , 61:85‚Äì117, 2015.
[24] K. Simonyan and A. Zisserman. V ery deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556 , 2014.
[25] G. Trigeorgis, F. Ringeval, R. Brueckner, E. Marchi, M. A.
Nicolaou, B. Schuller, and S. Zafeiriou. Adieu features?
end-to-end speech emotion recognition using a deep convo-
lutional recurrent network. In Acoustics, Speech and Signal
Processing (ICASSP), 2016 IEEE International Conference
on, pages 5200‚Äì5204. IEEE, 2016.
[26] A. V edaldi and B. Fulkerson. Vlfeat: An open and portable
library of computer vision algorithms. In Proceedings of the
18th ACM international conference on Multimedia , pages
1469‚Äì1472. ACM, 2010.
[27] S. Zafeiriou, D. Kollias, M. Nicolaou, A. Papaioannou,
G. Zhao, and I. Kotsia. Aff-wild: V alence and arousal ‚Äòin-
the-wild‚Äô challenge. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition Workshop ,
2017.
1979
"
https://ieeexplore.ieee.org/document/908962,"Recognizing Action Units for
Facial Expression Analysis
Ying-li Tian, Member ,IEEE , Takeo Kanade, Fellow ,IEEE , and Jeffrey F. Cohn, Member ,IEEE
Abstract √êMost automatic expression analysis systems attempt to recognize a small set of prototypic expressions, such as
happiness, anger, surprise, and fear. Such prototypic expressions, however, occur rather infrequently. Human emotions and intentionsare more often communicated by changes in one or a few discrete facial features. In this paper, we develop an Automatic Face
Analysis (AFA) system to analyze facial expressions based on both permanent facial features (brows, eyes, mouth) and transient facial
features (deepening of facial furrows) in a nearly frontal-view face image sequence. The AFA system recognizes fine-grained changes
in facial expression into action units (AUs) of the Facial Action Coding System (FACS), instead of a few prototypic expressions.
Multistate face and facial component models are proposed for tracking and modeling the various facial features, including lips, eyes,
brows, cheeks, and furrows. During tracking, detailed parametric descriptions of the facial features are extracted. With these
parameters as the inputs, a group of action units (neutral expression, six upper face AUs and 10 lower face AUs) are recognized
whether they occur alone or in combinations. The system has achieved average recognition rates of 96.4 percent (95.4 percent if
neutral expressions are excluded) for upper face AUs and 96.7 percent (95.6 percent with neutral expressions excluded) for lower face
AUs. The generalizability of the system has been tested by using independent image databases collected and FACS-coded for
ground-truth by different research teams.
Index Terms √êComputer vision, multistate face and facial component models, facial expression analysis, facial action coding system,
action units, AU combinations, neural network.
√¶
1I NTRODUCTION
FACIAL expression is one of the most powerful, natural, and
immediate means for human beings to communicate their
emotions and intentions. The face can express emotion sooner
than people verbalize or even realize their feelings. In the past
decade, much progress has been made to build computersystems to understand and use this natural form of human
communication [4], [3], [8], [10], [16], [18], [24], [26], [28], [32],
[37], [38], [36], [40]. Most such systems attempt to recognize asmall set of prototypic emotional expressions, i.e., joy,
surprise, anger, sadness, fear, and disgust. This practice may
follow from the work of Darwin [9] and more recently Ekmanand Friesen [13], Friesen [12], and Izard et al. [19] whoproposed that basic emotions have corresponding prototypic
facial expressions. In everyday life, however, such prototypic
expressions occur relatively infrequently. Instead, emotionmore often is communicated by subtle changes in one or a few
discrete facial features, such as a tightening of the lips in anger
or obliquely lowering the lip corners in sadness [7]. Change inisolated features, especially in the area of the eyebrows
or eyelids, is typical of paralinguistic displays; for
instance, raising the brows signals greeting [11]. To capturesuch subtlety of human emotion and paralinguisticcommunication, automated recognition of fine-grained
changes in facial expression is needed.1.1 Facial Action Coding System
Ekman and Friesen [14] developed the Facial Action Coding
System (FACS) for describing facial expressions by actionunits (AUs). Of 44 FACS AUs that they defined, 30 AUs areanatomically related to the contractions of specific facialmuscles: 12 are for upper face, and 18 are for lower face. AUscan occur either singly or in combination. When AUs occur incombination they may be additive , in which the combination
does not change the appearance of the constituent AUs, ornonadditive, in which the appearance of the constituents doeschange. Although the number of atomic action units isrelatively small, more than 7,000 different AU combinationshave been observed [30]. FACS provides the descriptivepower necessary to describe the details of facial expression.
Commonly occurring AUs and some of the additive and
nonadditive AU combinations are shown in Tables 1 and 2.As an example of a nonadditive effect, AU 4 appearsdifferently depending on whether it occurs alone or incombination with AU 1 (as in AU 1¬á4). When AU 4 occurs
alone, the brows are drawn together and lowered. In
AU1¬á4, the brows are drawn together but are raised due
to the action of AU 1. AU 1¬á2is another example of
nonadditive combinations. When AU 2 occurs alone, it notonly raises the outer brow, but also often pulls up the innerbrow which results in a very similar appearance toAU1¬á2. These effects of the nonadditive AU combinations
increase the difficulties of AU recognition.
1.2 Automated Facial Expression Analysis
Most approaches to automated facial expression analysis sofar attempt to recognize a small set of prototypic emotionalexpressions. Suwa et al. [31] presented an early attempt toanalyze facial expressions by tracking the motion of20 identified spots on an image sequence. Essa andIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 23, NO. 2, FEBRUARY 2001 97
.Y.-l. Tian and T. Kanade are with the Robotics Institute, Carnegie Mellon
University, Pittsburgh, PA 15213. E-mail: {yltian, tk}@cs.cmu.edu.
.J.F. Cohn is with the Robotics Institute, Carnegie Mellon University,
Pittsburgh, and the Department of Psychology, University of Pittsburgh,Pittsburgh, PA 15260. E-mail: jeffcohn@pitt.edu.
Manuscript received 26 Apr. 2000; revised 5 Oct. 2000; accepted 14 Oct.
2000.
Recommended for acceptance by K.W. Bowyer.For information on obtaining reprints of this article, please send e-mail to:tpami@computer.org, and reference IEEECS Log Number 112006.
0162-8828/01/$10.00 √ü2001 IEEE
Pentland [16] developed a dynamic parametric model based
on a 3D geometric mesh face model to recognize fiveprototypic expressions. Mase [26] manually selected facialregions that corresponded to facial muscles and computedmotion within these regions using optical flow. The workby Yacoob and Davis [37] used optical flow like Mase'swork, but tracked the motion of the surface regions of facialfeatures (brows, eyes, nose, and mouth) instead of that ofthe underlying muscle groups. Zhang [40] investigated theuse of two types of facial features: the geometric positions of34 fiducial points on a face and a set of multiscale,multiorientation Gabor wavelet coefficients at these pointsfor facial expression recognition.
Automatic recognition of FACS action units (AU) is a
difficult problem, and relatively little work has beenreported. AUs have no quantitative definitions and, as noted,
can appear in complex combinations. Mase [26] and Essa [16]
described patterns of optical flow that corresponded toseveral AUs, but did not attempt to recognize them. Bartlettet al. [2] and Donato et al. [10] reported some of the mostextensive experimental results of upper and lower face AUrecognition. They both used image sequences that were freeof head motion, manually aligned faces using three coordi-nates, rotated the images so that the eyes were in horizontal,scaled the images and, finally, cropped a window of6090pixels. Their system was trained and tested using
the leave-one-out cross-validation procedure, and the meanclassification accuracy was calculated across all of the testcases. Bartlett et al. [2] recognized six single upper face AUs(AU 1, AU 2, AU 4, AU 5, AU 6, and AU 7) but no AUsoccurring in combinations. They achieved 90.9 percentaccuracy by combining holistic spatial analysis and opticalflow with local feature analysis in a hybrid system. Donatoet al. [10] compared several techniques for recognizing actionunits. These techniques included optical flow, principal
component analysis, independent component analysis, local
feature analysis, and Gabor wavelet representation. The best
performances were obtained by using Gabor waveletrepresentation and independent component analysis withwhich a 95.5 percent average recognition rate was reported
for six single upper face AUs (AU 1, AU 2, AU 4, AU 5, AU 6,
and AU 7) and two lower face AUs and four AU combina-tions (AU 17, AU 18, AU 9¬á25,A U 10¬á25,A U 16¬á25,
AU 20¬á25). For analysis purpose, they treated each
combination as if it were a separate new AU.
The authors' group has developed a few versions of the
facial expression analysis system. Cohn et al. [8] and
Lien et al. [24] used dense-flow, feature-point tracking,
and edge extraction to recognize four upper face AUs andtwo combinations (AU 4, AU 5, AU 6, AU 7, AU 1¬á2,
and AU 1¬á4) and four lower face AUs and five
combinations (AU 12, AU 25, AU 26, AU 27, AU 12¬á25,
AU 20¬á2516,A U 15¬á17,A U 17¬á23¬á24,a n d
AU 9¬á1725). Again, each AU combination was
regarded as a separate new AU. The average recognition
rate ranged from 80 percent to 92 percent depending on
the method used and AUs recognized.
These previous versions have several limitations:
1. They require manual marking of 38 to 52 feature
points around face landmarks in the initial input
frame. A more automated system is desirable.
2. The initial input image is aligned with a standard
face image by affine transformation, which assumesthat any rigid head motion is in-plane.
3. The extraction of dense flow is relatively slow, which
limits its usefulness for large databases and real-time
applications.98 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 23, NO. 2, FEBRUARY 2001
TABLE 1
Upper Face Action Units and Some Combinations

4. Lip and eye feature tracking is not reliable because
of the aperture problem and when features undergoa large amount of change in appearance, such asopen to tightly closed mouth or eyes.
5. While they used three separate feature extraction
modules, they were not integrated for the purpose ofAU recognition. By integrating their outputs, it islikely that even higher accuracy could be achieved.
6. A separate hidden Markov model is necessary for
each single AU and each AU combination. Because
FACS consists of 44 AUs and potential combinations
numbering in the thousands, a more efficientapproach will be needed.
The current AFA system addresses many of these
limitations:1. Degree of manual preprocessing is reduced by using
automatic face detection [29]. Templates of face
components are quickly adjusted in the first frame
and then tracked automatically.
2. No image alignment is necessary, and in-plane and
limited out-of-plane head motion can be handled.
3. To decrease processing time, the system uses a more
efficient facial feature tracker instead of a computa-tionally intensive dense-flow extractor. Processingnow requires less than 1 second per frame pair.
4. To increase the robustness and accuracy of the
feature extraction, multistate face-component mod-els are devised. Facial feature tracking can cope witha large change of appearance and limited out-of-plane head motion.TIAN ET AL.: RECOGNIZING ACTION UNITS FOR FACIAL EXPRESSION ANALYSIS 99
TABLE 2
Lower Face Action Units and Some Combinations
Single AU 23 and AU 24 are not included in this table because our database happens to contain only occurences of their combination, but not
individual ones.
5. Extracted features are represented and normalized
based on an explicit face model that is invariant toimage scale and in-plane head motion.
6. More AUs are recognized and they are recognized
whether they occur alone or in combinations. Insteadof one HMM for each AU or AU combination, thecurrent system employs two Artificial Neural Net-works (one for the upper face and one for thelower face) for AU recognition. It recognizes 16 of the30 AUs that have a specific anatomic basis and
occur frequently in emotion and paralinguistic
communication.
2M ULTISTATE FEATURE -BASED AU R ECOGNITION
An automated facial expression analysis system must solve
two problems: facial feature extraction and facial expression
classification. In this paper, we describe our multistatefeature-based AU recognition system, which explicitly
analyzes appearance changes in localized facial features in
a nearly frontal image sequence. Since each AU is
associated with a specific set of facial muscles, we believe
that accurate geometrical modeling and tracking of facialfeatures will lead to better recognition results. Furthermore,
the knowledge of exact facial feature positions could be
useful for the area-based [37], holistic analysis [2], and
optical-flow-based [24] classifiers.
Fig. 1 depicts the overall structure of the AFA system.
Given an image sequence, the region of the face and
approximate location of individual face features are
detected automatically in the initial frame [29]. Thecontours of the face features and components then are
adjusted manually in the initial frame. Both permanent
(e.g., brows, eyes, lips) and transient (lines and furrows)
face feature changes are automatically detected and tracked
in the image sequence. Informed by FACS AUs, we groupthe facial features into separate collections of feature
parameters because the facial actions in the upper and
lower face are relatively independent for AU recognition
[14]. In the upper face, 15 parameters describe shape,
motion, eye state, motion of brow and cheek, and furrows.
In the lower face, nine parameters describe shape, motion,
lip state, and furrows. These parameters are geometricallynormalized to compensate for image scale and in-plane
head motion.
The facial feature parameters are fed to two neural-
network-based classifiers. One recognizes six upper faceAUs (AU 1, AU 2, AU 4, AU 5, AU 6, AU 7) and
NEUTRAL , and the other recognizes 10 lower face AUs
(AU 9, AU 10, AU 12, AU 15, AU 17, AU 20, AU 25, AU 26,AU 27, AU 23¬á24) andNEUTRAL . These classifiers are
trained to respond to the designated AUs whether theyoccur singly or in combination. When AUs occur incombination, multiple output nodes could be excited. Forthe upper face, we have achieved an average recognitionrate of 96.4 percent for 50 sample sequences of 14 subjectsperforming seven AUs (including NEUTRAL ) singly or in
combination. For the lower face, our system has achieved anaverage recognition rate of 96.7 percent for 63 sample
sequences of 32 subjects performing 11 AUs (including
NEUTRAL ) singly or in combination. The generalizability
of AFA has been tested further on an independent databaserecorded under different conditions and ground-truthcoded by an independent laboratory. A 93.3 percentaverage recognition rate has been achieved for 122 samplesequences of 21 subjects for neutral expression and 16 AUswhether they occurred individually or in combinations.
3F ACIAL FEATURE EXTRACTION
Contraction of the facial muscles produces changes in thedirection and magnitude of the motion on the skin surfaceand in the appearance of permanent and transient facialfeatures. Examples of permanent features are the lips, eyes,and any furrows that have become permanent with age.Transient features include facial lines and furrows that arenot present at rest but appear with facial expressions. Even
in a frontal face, the appearance and location of the facial
features can change dramatically. For example, the eyeslook qualitatively different when open and closed. Differentcomponents require different extraction and detectionmethods. Multistate models of facial components havebeen introduced to detect and track both transient andpermanent features in an image sequence.
3.1 Multistate Face Component Models
To detect and track changes of facial components in nearfrontal images, we develop multistate facial componentmodels. The models are illustrated in Table 3, whichincludes both permanent (i.e., lips, eyes, brows, and cheeks)and transient components (i.e., furrows). A three-state lipmodel describes lip state: open, closed, and tightly closed. A
two-state model (open or closed) is used for each of the
eyes. Each brow and cheek has a one-state model. Transientfacial features, such as nasolabial furrows, have two states:present and absent.
3.2 Permanent Features
Lips: A three-state lip model represents open, closed, and
tightly closed lips. A different lip contour template isprepared for each lip state. The open and closed lipcontours are modeled by two parabolic arcs, which aredescribed by six parameters: the lip center position (xc, yc),the lip shape ( h1,h2, andw), and the lip orientation ( ). For100 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 23, NO. 2, FEBRUARY 2001
Fig. 1. Feature-based automatic facial action analysis (AFA) system.
tightly closed lips, the dark mouth line connecting the lip
corners represents the position, orientation, and shape.
Tracking of lip features uses color, shape, and motion. In
the first frame, the approximate position of the lip templateis detected automatically. Then, it is adjusted manually bymoving four key points. A Gaussian mixture modelrepresents the color distribution of the pixels inside of thelip template [27]. The details of our lip tracking algorithmhave been presented in [33].
Eyes: Most eye trackers developed so far are for open
eyes and simply track the eye locations [23], [39]. Torecognize facial AUs, however, we need to detect whetherthe eyes are open or closed, the degree of eye opening, andthe location and radius of the iris. For an open eye, the eyetemplate (Table 3), is composed of a circle with threeparameters¬Öx
0;y0;r¬Üto model the iris and two parabolic
arcs with six parameters ¬Öxc;yc;h1;h2;w;¬Üto model the
boundaries of the eye. This template is the same as Yuille's[39] except for the two points located at the center of the
whites of the eyes. For a closed eye, the template is reduced
to four parameters: two for the position of each of the eyecorners.
The open-eye template is adjusted manually in the first
frame by moving six points for each eye. We found that theouter corners are more difficult to track than the innercorners; for this reason, the inner corners of the eyes are
tracked first. The outer corners then are located on the line
that connects the inner corners at a distance of the eye widthas estimated in the first frame.TIAN ET AL.: RECOGNIZING ACTION UNITS FOR FACIAL EXPRESSION ANALYSIS 101
TABLE 3
Multistate Facial Component Models of a Frontal Face

The iris provides important information about the eye
state. Part of the iris is normally visible if the eye is open.
Intensity and edge information are used to detect the iris.We have observed that the eyelid edge is noisy even in a
good quality image. However, the lower part of the iris is
almost always visible and its edge is relatively clear if theeye is open. Thus, we use a half circle mask to filter the irisedge (Fig. 2). The radius of the iris circle template r
0is
determined in the first frame, since it is stable except for
large out-of-plane head motion. The radius of the circle isincreased or decreased slightly ( r) fromr
0so that it can
vary between minimum radius ¬Ör0√ør¬Üand maximum
radius¬Ör0¬ár¬Ü. The system determines that the iris is
found when the following two conditions are satisfied: One
is that the edges in the mask are at their maximum. The
other is that the change in the average intensity is less than athreshold. Once the iris is located, the eye is determined to
be open and the iris center is the iris mask center ¬Öx
0;y0¬Ü.
The eyelid contours then are tracked. For a closed eye, a lineconnecting the inner and outer corners of the eye is used as
the eye boundary. The details of our eye-tracking algorithm
have been presented in [34].
Brow and cheek: Features in the brow and cheek areas
are also important for expression analysis. Each left or right
brow has one model√êa triangular template with six
parameters¬Öx1;y1¬Ü,¬Öx2;y2¬Ü, and¬Öx3;y3¬Ü. Also, each cheek
has a similar six parameter downward triangular template
model. Both brow and cheek templates are tracked using
the Lucas-Kanade algorithm [25].
3.3 Transient Features
In addition to permanent features that move and change
their shape and positions, facial motion also produces
transient features that provide crucial information forrecognition of certain AUs. Wrinkles and furrows appear
perpendicular to the direction of the motion of the activated
muscles. Contraction of the corrugator muscle, for instance,
produces vertical furrows between the brows, which is
coded in FACS as AU 4, while contraction of the medial
portion of the frontalis muscle (AU 1) causes horizontal
wrinkling in the center of the forehead.
Some of these transient features may become permanent
with age. Permanent crow's-feet wrinkles around the
outside corners of the eyes, which are characteristic of
AU 6, are common in adults but not in children. When
wrinkles and furrows become permanent, contraction of the
corresponding muscles produces only changes in their
appearance, such as deepening or lengthening. The pre-
sence or absence of the furrows in a face image can be
determined by edge feature analysis [22], [24], or by eigen-
image analysis [21], [35]. Terzopoulos and Waters [32]detected the nasolabial furrows for driving a face animator,
but with artificial markers. Kwon and Lobo [22] detectedfurrows using snakes to classify pictures of people intodifferent age groups. Our previous system [24] detectedhorizontal, vertical, and diagonal edges using a complex
face template.
In our current system, we detect wrinkles in the
nasolabial region, the nasal root, and the areas lateral to
the outer corners of the eyes (Fig. 3). These areas are located
using the tracked locations of the corresponding permanentfeatures. We classify each of the wrinkles into one of twostates: present and absent. Compared with the neutralframe, the wrinkle state is classified as present if wrinklesappear, deepen, or lengthen. Otherwise, it is absent.
We use a Canny edge detector to quantify the amount
and orientation of furrows [6]. For nasal root wrinkles andcrow's-feet wrinkles, we compare the number of edge pixelsEin the wrinkle areas of the current frame with the number
of edge pixels E
0of the first frame. If the ratio E=E 0is larger
than a threshold, the furrows are determined to be present.Otherwise, the furrows are absent. For nasolabial furrows,the existence of vertical to diagonal connected edges is usedfor classification. If the connected edge pixels are largerthan a threshold, the nasolabial furrow is determined to bepresent and is modeled as a line. The orientation of the
furrow is represented as the angle between the furrow line
and line connecting the eye inner corners. This anglechanges according to different AUs. For example, thenasolabial furrow angle of AU 9 or AU 10 is larger thanthat of AU 12.
3.4 Examples of Feature Extraction
Permanent Features: Fig. 4 shows the results of tracking
permanent features for the same subject with different
expressions. In Figs. 4a, 4b, and 4d, the lips are tracked asthey change in state from open to closed and tightly closed.
The iris position and eye boundaries are tracked while the
eye changes from widely opened to tightly closed and blink(Figs. 4b, 4c, and 4d). Notice that the semicircular iris model
tracks the iris even when the iris is only partially visible.
Figs. 5 and 6 show examples of tracking in subjects whovary in age, sex, skin color, and in amount of out-plane
head motion. Difficulty occurs in eye tracking when the eye
becomes extremely narrow. For example, in Fig. 5a, theleft eye in the last image is mistakenly determined to be
closed because the iris was too small to be detected. In
these examples, face size varies between 8090and102 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 23, NO. 2, FEBRUARY 2001
Fig. 2. Half circle iris mask. ¬Öx0;y0¬Üis the iris center, r0is the iris radius
r1is the minimum radius of the mask, and r2is the maximum radius of
the mask.
Fig. 3. The areas for nasolabial furrows, nasal root, and outer eye
corners.
200220 pixels. For display purpose, images have been
cropped to reduce space. Additional results can be found athttp://www.cs.cmu.edu/~face.
Transient Features: Fig. 7 shows the results of nasolabial
furrow detection for different subjects and AUs. Thenasolabial furrow angles systematically vary between AU 9and AU 12 (Fig. 7a and 7b). For some images, the nasolabialfurrow is detected only on one side. In the first image ofFig. 7d, only the left nasolabial furrow exists, and it iscorrectly detected. In the middle image of Fig. 7b, the rightnasolabial furrow is missed because the length of thedetected edges is less than threshold. The results of nasalroot and crow's-feet wrinkle detection are shown in Fig. 8.Generally, the crow's-feet wrinkles are present for AU 6,and the nasal root wrinkles appear for AU 9.4F ACIAL FEATURE REPRESENTATION AND AU
RECOGNITION by N EURAL NETWORKS
We transform the extracted features into a set of parameters
for AU recognition. We first define a face coordinate
system. Because the inner corners of the eyes are most
reliably detected and their relative position is unaffected by
muscle contraction, we define the x- a x i sa st h el i n e
connecting two inner corners of eyes and the y-axis as
perpendicular to it. We split the facial features into twogroups (upper face and lower face) of parameters because
facial actions in the upper face have little interaction with
facial motion in lower face and vice versa [14].
Upper Face Features: We represent the upper face features
by 15 parameters, which are defined in Table 4. Of these,
12 parameters describe the motion and shape of the eyes,TIAN ET AL.: RECOGNIZING ACTION UNITS FOR FACIAL EXPRESSION ANALYSIS 103
Fig. 4. Permanent feature tracking results for different expressions of same subject. Note appearance changes in eye and mouth states. In this and
the following figures, images have been cropped for display purpose. Face size varies between 8090and200220pixels.
brows, and cheeks, two parameters describe the state of the
crow's-feet wrinkles, and one parameter describes the
distance between the brows. To remove the effects of
variation in planar head motion and scale between image
sequences in face size, all parameters are computed as ratiosof their current values to that in the initial frame. Fig. 9 shows
the coordinate system and the parameter definitions.
Lower Face Features: Nine parameters represent the lower
face features ( Table 5 and Fig. 10). Of these, six parameters
describe lip shape, state and motion, and three describe thefurrows in the nasolabial and nasal root regions. These
parameters are normalized by using the ratios of the current
feature values to that of the neutral frame.AU Recognition by Neural Networks: We use three-
layer neural networks with one hidden layer to recognizeAUs by a standard back-propagation method [29]. Separatenetworks are used for the upper- and lower face. For
AU recognition in the upper face, the inputs are the
15 parameters shown in Table 4. The outputs are the sixsingle AUs (AU 1, AU 2, AU 4, AU 5, AU 6, and AU7) andNEUTRAL . In the lower face, the inputs are the seven
parameters shown in Table 5 and the outputs are 10 single
AUs ( AU 9, AU 10, AU 12, AU 15, AU 17, AU 20, AU23¬á24, AU 25, AU 26, and AU 27) and NEUTRAL . These
networks are trained to respond to the designated AUs
whether they occur singly or in combination. When AUs
occur in combination, multiple output nodes are excited.104 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 23, NO. 2, FEBRUARY 2001
Fig. 5. Permanent feature tracking results for different subjects.
5E XPERIMENTAL EVALUATIONS
We conducted three experiments to evaluate the performance
of our system. The first is AU recognition in the upper facewhen image data contain only single AUs. The second isAU recognition in the upper and lower face when image datacontain both single AUs and combinations. The thirdexperiment evaluates the generalizability of our system byusing completely disjointed databases for training andtesting, while image data contain both single AUs andcombinations. Finally, we compared the performance of oursystem with that of other AU recognition systems.
5.1 Facial Expression Image Databases
Two databases were used to evaluate our system: the Cohn-
Kanade AU-Coded Face Expression Image Database [20]
and Ekman-Hager Facial Action Exemplars [15].
Cohn-Kanade AU-Coded Face Expression Image Data-
base: We have been developing a large-scale database for
promoting quantitative study of facial expression analysis[20]. The database currently contains a recording of thefacial behavior of 210 adults who are 18 to 50 years old,
69 percent female and 31 percent male, and 81 percent
Caucasian, 13 percent African, and 6 percent other groups.Over 90 percent of the subjects had no prior experience inFACS. Subjects were instructed by an experimenter toperform single AUs and AU combinations. Subjects' facial
behavior was recorded in an observation room. Imagesequences with in-plane and limited out-of-plane motionwere included.
The image sequences began with a neutral face and were
digitized into 640480 pixel arrays with either 8-bit
gray-scale or 24-bit color values. To date, 1,917 imagesequences of 182 subjects have been FACS coded by certified
FACS coders for either the entire sequence or target AUs.
Approximately 15 percent of these sequences were coded bytwo independent certified FACS coders to validate theaccuracy of the coding. Interobserver agreement was quanti-fied with coefficient kappa, which is the proportion of
agreement above what would be expected to occur by chance
[17]. The mean kappas for interobserver agreement were 0.82for target AUs and 0.75 for frame-by-frame coding.
Ekman-Hager Facial Action Exemplars: This database
was provided by P. Ekman at the Human Interaction
Laboratory, University of California, San Francisco, and
contains images that were collected by Hager, Methvin, andIrwin. Bartlett et al. [2] and Donato et al. [10] used thisdatabase to train and test their AU recognition systems. TheEkman-Hager database includes 24 Caucasian subjects
(12 males and 12 females). Each image sequence consists
of six to eight frames that were sampled from a longerimage sequence. Image sequences begin with a neutralTIAN ET AL.: RECOGNIZING ACTION UNITS FOR FACIAL EXPRESSION ANALYSIS 105
Fig. 6. Permanent feature tracking results with head motions. (a) Head yaw. (b) Head pitch. (c) Head up and left with background motion.
expression (or weak facial actions) and end with stronger
facial actions. AUs were coded for each frame. Sequences
containing rigid head motion detectable by a human
observer were excluded. Some of the image sequencescontain large lighting changes between frames and we
normalized intensity to keep the average intensity constant
throughout the image sequence.
5.2 Upper Face AU Recognition for Image Data
Containing Only Single AUs
In the first experiment, we used a neural network-based
recognizer having the structure shown in Fig. 11. The inputsto the network were the upper face feature parametersshown in Table 4. The outputs were the same set of sixsingle AUs (AU 1, AU 2, AU 4, AU 5, AU 6, AU 7); these arethe same set that were used by Bartlett and Donato. In
addition, we included an output node for NEUTRAL . The
output node that showed the highest value was interpreted
as the recognized AU. We tested various numbers of hiddenunits and found that six hidden units gave the best
performance.
From the Ekman-Hager database, we selected image
sequences in which only a single AU occurred in the upperface. 99 image sequences from 23 subjects met this criterion.
These 99 image sequences we used are the superset of the
80 image sequences used by Bartlett and Donato. The initialand final two frames in each image sequence were used. As
shown in Table 6, the image sequences were assigned to
training and testing sets in two ways. In S1, the sequences106 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 23, NO. 2, FEBRUARY 2001
Fig. 7. Nasolabial furrow detection results.
were randomly selected, so the same subject was allowed to
appear in both training and testing sets. In S2, no subject
could appear in both training and testing sets; testing wasperformed done with novel faces.
Table 7 shows the recognition results with the S1testing
set. The average recognition rate was 88.5 percent whensamples of NEUTRAL were excluded (Recognizing neutral
faces is easier), and 92.3 percent when samples ofNEUTRAL were included. For the S2test set (i.e., novel
faces), the recognition rate remained virtually identical:
89.4 percent ( NEUTRAL exclusive) and 92.9 percent
(NEUTRAL inclusive), which is shown in Table 8.TIAN ET AL.: RECOGNIZING ACTION UNITS FOR FACIAL EXPRESSION ANALYSIS 107
Fig. 8. Nasal root and crow's-feet wrinkle detection. For the left image of (a), (b), and (c), crow's-feet wrinkles are present. For the right image of ( a),
(b), and (c), the nasal root wrinkles appear.
TABLE 4
Upper Face Feature Representation for AU Recognition

5.3 Upper and Lower Face AU Recognition for
Image Sequences Containing Both Single AUs
and Combinations
Because AUs can occur either singly or in combinations, an
AU recognition system must have the ability to recognize
them however they occur. All previous AU recognition
systems [2], [10], [24] were trained and tested on single AUs
only. In these systems, even when AU combinations wereincluded, each combination was treated as if it were a
separate AU. Because potential AU combinations number
in the thousands, this method of separately treating
AU combinations is impractical. In our second experiment,
we trained a neural network to recognize AUs singly and incombinations by allowing multiple output units of the
networks to fire when the input consists of AU combinations.
Upper Face AUs: The neural network-based recognition
system for AU combination is shown in Fig. 12. Thenetwork has a similar structure to that used in Experiment 1,where the output nodes correspond to six single AUs plusNEUTRAL . However, the network for recognizing AU
combinations is trained so that when an AU combination ispresented, multiple output nodes that correspond to thecomponent AUs are excited. In training, all of the outputnodes that correspond to the input AU components are setto have the same value. For example, when a training inputis AU 1¬á2¬á4, the output values are trained to be 1.0 for
AU 1, AU 2, and AU 4; 0.0 for the remaining AUs and
NEUTRAL . At the runtime, AUs whose output nodes
show values higher than the threshold are considered to berecognized.
A total of 236 image sequences of 23 subjects from the
Ekman-Hager database (99 image sequences containingonly single AUs and 137 image sequences containingAU combinations) were used for recognition of AUs inthe upper face. We split them into training (186 sequences)108 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 23, NO. 2, FEBRUARY 2001
Fig. 9. Upper face features. hl¬Öhl1¬áhl2¬Üandhr¬Öhr1¬áhr2¬Üare the
height of left eye and right eye; Dis the distance between brows; cland
crare the motion of left cheek and right cheek. bliandbriare the motion
of the inner part of left brow and right brow. bloandbroare the motion of
the outer part of left brow and right brow. flandfrare the left and right
crow's-feet wrinkle areas.
TABLE 5
Lower Face Feature Representation for AUs Recognition
Fig. 10. Lower face features. h1andh2are the top and bottom lip
heights;wis the lip width; Dleftis the distance between the left lip corner
and eye inner corners line; Drightis the distance between the right lip
corner and eye inner corners line; n1is the nasal root area.
Fig. 11. Neural network-based recognizer for single AUs in the upper
face. The inputs are the feature parameters, and the output is one labelout of six single AUs and NEUTRAL .
and testing (50 sequences) sets by subjects (9 subjects for
training and 14 subjects for testing) to ensure that the samesubjects did not appear in both training and testing. Testing,
therefore, was done with ¬™novel faces.¬∫ From experiments,we have found that it was necessary to increase the number
of hidden units from six to 12 to obtain optimized
performance.
Because input sequences could contain one or more AUs,
several outcomes were possible. Correct denotes that the
recognized results were completely identical to the input
samples.Partiallycorrect denotes that some, but not all of
the AUs were recognized ( MissingAUs ) or that AUs that
did not occur were misrecognized in addition to the one(s)
that did (ExtraAUs ). If none of the AUs that occurred were
recognized, the result was Incorrect .
Using (1) and (2), we calculated recognition- and false-
alarm rates for input samples and input AU components,
respectively. Human FACS coders typically use the latter to
calculate percentage agreement. We believe, however, thatTIAN ET AL.: RECOGNIZING ACTION UNITS FOR FACIAL EXPRESSION ANALYSIS 109
TABLE 6
Details of Training and Testing Data from Ekman-Hager Database that Are Used for Single AU Recognition in the Upper Face
InS1, some subjects appear in both training and testing sets. In S2, no subject appears in both training and testing sets.
TABLE 7
AU Recognition for Single AUs on S1Training and Testing Sets in Experiment 1
A same subject could appear in both training and testing sets. The numbers in bold are results excluding NEUTRAL .
TABLE 8
AU Recognition for Single AUs on S2Train and Testing Sets in Experiment 1
No subject appears in both training and testing sets. The numbers in bold are results excluding NEUTRAL .
Fig. 12. Neural network-based recognizer for AU combinations in the
upper face.
the recognition rates based on input samples are the more
conservative measures.
Recognitionrate ¬à
Totalnumberof correctlyrecognized samples
Totalnumberof samplesbased on input samples
Totalnumberof correctlyrecognized AUs
Totalnumberof AUsbased on AU components(
¬Ö1¬Ü
Falsealarmrate ¬à
Totalnumberof recognized sampleswithextraAUs
Totalnumberof samplesbased on input samples
Totalnumberof extra AUs
Totalnumberof AUsbased on AU components :¬Ö2¬Ü
Table 9 shows a summary of the AU combination
recognition results of 50 test image sequences of 14 subjects
from the Ekman-Hager database. For input samples, we
achieved average recognition and false alarm rates of88 percent and 6.7 percent, respectively, when NEUTRAL
was included, and 82 percent and 12 percent, respectively,
whenNEUTRAL was excluded. AU component-wise, an
average recognition rate of 96.4 percent and a false alarm rateof 6.3 percent were achieved when NEUTRAL was included
and a recognition rate of 95.4 percent and a false alarm rate of8.2 percent was obtained when NEUTRAL was excluded.
Recognition rates in Experiment 2 were slightly higher
than those in Experiment 1. There are two possible reasons:One is that in the neural network used in Experiment 2,
multiple output nodes could be excited to allow for recogni-
tion of AUsoccurring in combinations. Another reason maybethat a larger training data set was used in Experiment 2.
Lower Face AUs: The same structure of the neural
network-based recognition scheme, as shown in Fig. 12,was used, except that the input feature parameters and the
output component AUs now are those for the lower face. The
inputs were the lower face feature parameters shown in Table5. The outputs of the neural network were the 11 single AUs(AU 9, AU 10, AU 12, AU 15, AU 17, AU 20, AU 25, AU 26,AU 27, AU 23¬á24, andNEUTRAL ) (see Table 2). Note that
AU23¬á24is modeled as a single unit, instead of as AU 23
and AU 24 separately, because they almost always occurred
together in our data. Use of 12 hidden units achieved the bestperformance in this experiment.110 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 23, NO. 2, FEBRUARY 2001
TABLE 9
Upper Face AU Recognition with AU Combinations in Experiment 2
The numbers in bold are results excluding NEUTRAL . TheMissingAUs column shows the AUs that are missed. The ExtraAUs column lists the
extra AUs that are misrecognized. The recognized AU with ¬™*¬∫ indicates that it includes both MissingAUs andExtraAUs .
A total of 463 image sequences from the Cohn-Kanade
AU-Coded Face Expression Image Database were used for
lower face AU recognition. Of these, 400 image sequences
were used as the training data and 63 sequences were usedas the testing data. The test data set included 10 single AUs,NEUTRAL , and 11 AU combinations (such as AU 12¬á25,
AU15¬á17¬á23,A U 9¬á17¬á23¬á24, and AU 17¬á20¬á26)
from 32 subjects; none of these subjects appeared in trainingdata set. Some of the image sequences contained limitedplanar and out-of-plane head motions.
Table 10 shows a summary of the AU recognition results
for the lower face when image sequences contain both
single AUs and AU combinations. As above, we report the
recognition and false alarm rates based on both the numberof input samples and the number of AU components (see(1) and (2)). With respect to the input samples, an average
recognition rate of 95.8 percent was achieved with a false
alarm rate of 4.2 percent when NEUTRAL was included
and a recognition rate of 93.7 percent and a false alarm rateof 6.4 percent when NEUTRAL was excluded. With respect
to AU components, an average recognition rate of 96.7 per-
cent was achieved with a false alarm rate of 2.9 percent
whenNEUTRAL was included, and a recognition rate of
95.6 percent with a false alarm rate of 3.9 percent wasobtained when NEUTRAL was excluded.
Major Causes of the Misidentifications: Most of the
misidentifications come from confusions between similar
AUs: AU1 and AU2, AU6 and AU7, and AU25 and AU26.
The confusions between AU 1 and AU 2 were caused by thestrong correlation between them. The action of AU 2, whichTIAN ET AL.: RECOGNIZING ACTION UNITS FOR FACIAL EXPRESSION ANALYSIS 111
TABLE 10
Lower Face AU Recognition Results in Experiment 2

raises the outer portion of the brow, tends to pull the inner
brow up as well (see Table 1). Both AU 6 and AU 7 raise the
lower eyelids and are often confused by human AU coders
as well [8]. All the mistakes of AU 26 were due to confusionwith AU 25. AU 25 and AU 26 contain parted lips but differonly with respect to motion of the jaw, but jaw motion was
not detected or used in the current system.
5.4 Generalizability between Databases
To evaluate the generalizability of our system, we trainedthe system on one database and tested it on another
independent image database that was collected and FACScoded for ground-truth by a different research team. Onewas Cohn-Kanade database and the other was the Ekman-
Hager database. This procedure ensured a more rigorous
test of generalizability than more usual methods whichdivide a single database into training and testing sets.Table 11 summarizes the generalizability of our system.
For upper face AU recognition, the network was trained
on 186 image sequences of nine subjects from the Ekman-
Hager database and tested on 72 image sequences of sevensubjects from the Cohn-Kanade database. Of the 72 imagesequences, 55 consisted of single AUs (AU 1, AU 2, AU 4,AU 5, AU 6, and AU 7) and the others contained
AU combinations such as AU 1¬á2,A U 1¬á2¬á4, and
AU6¬á7. We achieved a recognition rate of 93.2 percent
and a false alarm of 2 percent (when samples of NEUTRAL
were included), which is only slightly (3-4 percent) lower
than the case when the Ekman-Hager database was used for
both training and testing.
For lower face AU recognition, the network was trained
on 400 image sequences of 46 subjects from the Cohn-Kanade database and tested on 50 image sequences of
14 subjects from the Ekman-Hager database. Of the 50 image
sequences, half contained AU combinations, such asAU 10¬á17,A U 10¬á25,A U 12¬á25,A U 15¬á17, and
AU20¬á25. No instances of AU 23¬á24were available in
the Ekman-Hager database. We achieved a recognition rate
of 93.4 percent (when samples of NEUTRAL were
included). These results were again only slightly lowerthan those of using the same database. The system showedhigh generalizability.
5.5 Comparison with Other AU Recognition
Systems
We compare the current AFA system's performance with
that of Cohn et al. [8], Lien et al. [24], Bartlett et al. [2], andDonato et al. [10]. The comparisons are summarized in
Table 12. When performing comparison of recognition
results in general, it is important to keep in mind
differences in experimental procedures between systems.For example, scoring methods may be either by dividing thedata set into training and testing sets [8], [24] or by using a
leave-one-out cross-validation procedure [2], [10]. Even
when the same data set is used, the particular AUs thatwere recognized or the specific image sequence that wereused for evaluation are not necessarily the same. Therefore,minor differences in recognition rates between systems are
not meaningful.
In Table 12, the systems were compared along several
characteristics: feature extraction methods, recognition rates,treatment of AU combinations, AUs recognized, and data-bases used. The terms ¬™old faces¬∫ and ¬™novel faces¬∫ in the
third column requires some explanation. ¬™Old faces¬∫ means
that in obtaining the recognition rates, some subjects appearin both training and testing sets. ¬™Novel faces¬∫ means nosame subject appears in both training and testing sets; this is
obviously a little more difficult case than ¬™Old faces.¬∫ In the
fourth column, the terms ¬™ No,¬∫ ¬™Yes=Yes ,¬∫ and ¬™Yes=No ¬∫
are used to describe how the AU combinations are treated.¬™No¬∫ means that no AU combination was recognized.
¬™Yes=Yes ¬∫ means that AU combinations were recognized
and AUs in combination were recognizable individually.
¬™Yes=No ¬∫ means that AU combinations were recognized but
each AU combination was treated as if it were a separate newAU. Our current AFA system, while being able to recognize a
larger number of AUs and AU combinations, shows the best
or near the best recognition rates even for the tests with ¬™novelfaces¬∫ or in tests where independent different databases areused for training and testing.
6C ONCLUSION
Automatically recognizing facial expressions is important tounderstand human emotion and paralinguistic communica-tion, to design multimodal user interfaces, and to relate
applications, such as human identification. The facial action
coding system (FACS) developed by Ekman and Friesen[14] is considered to be one of the best and acceptedfoundations for recognizing facial expressions. Our feature-
based automatic face analysis (AFA) system has shown
improvement in AU recognition over previous systems.
It has been reported [2], [5], [40] that holistic template-
based methods (including image decomposition with image112 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 23, NO. 2, FEBRUARY 2001
TABLE 11
Generalizability to Independent Databases
The numbers in bold are results from independent databases.
kernels such as Gabors, Eigenfaces, and Independent
Component Images) outperform explicit parameterization
of facial features. Our comparison indicates that a feature-
based method performs just as well as the best holistic
template-based method and in more complex data. It may
be premature to conclude that one or the other approach issuperior. Recovering FACS-AUs from video using auto-
matic computer vision techniques is not an easy task and
numerous challenges remain [20]. We feel that furtherefforts will be required for combining both approaches in
order to achieve the optimal performance, and that tests
with a substantially large database are called for [1].ACKNOWLEDGMENTS
The authors would like to thank Paul Ekman, at the Human
Interaction Laboratory, University of California, San Fran-
cisco for providing the Ekman-Hager database. The authors
also thank Zara Ambadar, Bethany Peters, and Michelle
Lemenager for processing the images. The authors appreci-
ate the helpful comments and suggestions of Marian
Bartlett, Simon Baker, Karen Schmidt, and anonymous
reviewers. This work was supported by the National
Institute of Mental Health grant R01 MH51435.TIAN ET AL.: RECOGNIZING ACTION UNITS FOR FACIAL EXPRESSION ANALYSIS 113
TABLE 12
Comparison with Other AU Recognition Systems
In the fourth column, ¬™ No¬∫ means that no AU combination was recognized. ¬™ Yes=Yes ¬∫ means that AU combinations were recognized and AUs in
combination were recognizable individually. ¬™ Yes=No ¬∫ means that AU combinations were recognized but each AU combination was treated as if it
were a separate new AU.
REFERENCES
[1] Facial Expression Coding Project, cooperation and competition
between Carnegie Mellon Univ. and Univ. of California, SanDiego, unpublished, 2000.
[2] M. Bartlett, J. Hager, P. Ekman, and T. Sejnowski, ¬™Measuring
Facial Expressions by Computer Image Analysis,¬∫ Psychophysiol-
ogy, vol. 36, pp. 253-264, 1999.
[3] M.J. Black and Y. Yacoob, ¬™Tracking and Recognizing Rigid and
Nonrigid Facial Motions Using Local Parametric Models of ImageMotion,¬∫ Proc. Int'l Conf. Computer Vision, pp. 374-381, 1995.
[4] M.J. Black and Y. Yacoob, ¬™Recognizing Facial Expressions in
Image Sequences Using Local Parameterized Models of Image
Motion,¬∫ Int'l J. Computer Vision, vol. 25, no. 1, pp. 23-48, Oct. 1997.
[5] R. Brunelli and T. Poggio, ¬™Face Recognition: Features versus
Templates,¬∫ IEEE Trans. Pattern Analysis and Machine Intelligence,
vol. 15, no. 10, pp. 1042-1052, Oct. 1993.
[6] J. Canny, ¬™A Computational Approach to Edge Detection,¬∫ IEEE
Trans. Pattern Analysis Machine Intelligence, vol. 8, no. 6, June 1986.
[7] J.M. Carroll and J. Russell, ¬™Facial Expression in Hollywood's
Portrayal of Emotion,¬∫ J. Personality and Social Psychology, vol. 72,
pp. 164-176, 1997.
[8] J.F. Cohn, A.J. Zlochower, J. Lien, and T. Kanade, ¬™Automated
Face Analysis by Feature Point Tracking has High Concurrent
Validity with Manual Faces Coding,¬∫ Psychophysiology, vol. 36,
pp. 35-43, 1999.
[9] C. Darwin, The Expression of Emotions in Man and Animals, John
Murray, reprinted by Univ. of Chicago Press, 1965, 1872.
[10] G. Donato, M.S. Bartlett, J.C. Hager, P. Ekman, and T.J. Sejnowski,
¬™Classifying Facial Actions,¬∫ IEEE Trans. Pattern Analysis and
Machine Intelligence, vol. 21, no. 10, pp. 974-989, Oct. 1999.
[11] I. Eibl-Eihesfeldt, Human Ethology. New York: Aldine de Gruvter,
1989.
[12] P. Ekman, ¬™Facial Expression and Emotion,¬∫ Am. Psychologist,
vol. 48, pp. 384-392, 1993.
[13] P. Ekman and W.V. Friesen, Pictures of Facial Affect. Palo Alto,
Calif.: Consulting Psychologist, 1976.
[14] P. Ekman and W.V. Friesen, The Facial Action Coding System: A
Technique for The Measurement of Facial Movement. San Francisco:
Consulting Psychologists Press, 1978.
[15] P. Ekman, J. Hager, C.H. Methvin, and W. Irwin, ¬™Ekman-Hager
Facial Action Exemplars,¬∫ unpublished data, Human InteractionLaboratory, Univ. of California, San Francisco.
[16] I.A. Essa and A.P. Pentland, ¬™Coding, Analysis, Interpretation,
and Recognition of Facial Expressions,¬∫ IEEE Trans. Pattern
Analysis and Machine Intelligence, vol. 19, no. 7, pp. 757-763, July
1997.
[17] J. Fleiss, Statistical Methods for Rates and Proportions. New York:
Wiley, 1981.
[18] K. Fukui and O. Yamaguchi, ¬™Facial Feature Point Extraction
Method Based on Combination of Shape Extraction and PatternMatching,¬∫ Systems and Computers in Japan, vol. 29, no. 6, pp. 49-58,
1998.
[19] C. Izard, L. Dougherty, and E.A. Hembree, ¬™A System for
Identifying Affect Expressions by Holistic Judgments,¬∫ unpub-lished manuscript, Univ. of Delaware, 1983.
[20] T. Kanade, J. Cohn, and Y. Tian, ¬™Comprehensive Database for
Facial Expression Analysis,¬∫ Proc. Int'l Conf. Face and Gesture
Recognition, pp. 46-53, Mar. 2000.
[21] M. Kirby and L. Sirovich, ¬™Application of the k-l Procedure for the
Characterization of Human Faces,¬∫ IEEE Trans. Pattern Analysis
and Machine Intelligence, vol. 12, no. 1, pp. 103-108, Jan. 1990.
[22] Y. Kwon and N. Lobo, ¬™Age Classification from Facial Images,¬∫
Proc. IEEE Conf. Computer Vision and Pattern Recognition, pp. 762-
767, 1994.
[23] K. Lam and H. Yan, ¬™Locating and Extracting the Eye in Human
Face Images,¬∫ Pattern Recognition, vol. 29, no. 5, pp. 771-779, 1996.
[24] J.-J.J. Lien, T. Kanade, J.F. Cohn, and C.C. Li, ¬™Detection, Tracking,
and Classification of Action Units in Facial Expression,¬∫ J. Robotics
and Autonomous System, vol. 31, pp. 131-146, 2000.
[25] B. Lucas and T. Kanade, ¬™An Interative Image Registration
Technique with an Application in Stereo Vision,¬∫ Proc. Seventh
Int'l Joint Conf. Artificial Intelligence, pp. 674-679, 1981.
[26] K. Mase, ¬™Recognition of Facial Expression from Optical Flow,¬∫
IEICE Trans., vol. E74, no. 10, pp. 3474-3483, Oct. 1991.
[27] R.R. Rao, ¬™Audio-Visual Interaction in Multimedia,¬∫ PhD thesis,
Electrical Eng., Georgia Inst. of Technology, 1998.[28] M. Rosenblum, Y. Yacoob, and L.S. Davis, ¬™Human Expression
Recognition from Motion Using a Radial Basis Function NetworkArchtecture,¬∫ IEEE Trans. Neural Network, vol. 7, no. 5, pp. 1121-
1138, 1996.
[29] H.A. Rowley, S. Baluja, and T. Kanade, ¬™Neural Network-Based
Face Detection,¬∫ IEEE Trans. Pattern Analysis and Machine
intelligence, vol. 20, no. 1, pp. 23-38, Jan. 1998.
[30] K. Scherer and P. Ekman, Handbook of Methods in Nonverbal
Behavior Research. Cambridge, UK: Cambridge Univ. Press, 1982.
[31] M. Suwa, N. Sugie, and K. Fujimora, ¬™A Preliminary Note on
Pattern Recognition of Human Emotional Expression,¬∫ Proc. Int'l
Joint Conf. Pattern Recognition, pp. 408-410, 1978.
[32] D. Terzopoulos and K. Waters, ¬™Analysis of Facial Images Using
Physical and Anatomical Models,¬∫ Proc. IEEE Int'l Conf. Computer
Vision, pp. 727-732, 1990.
[33] Y. Tian, T. Kanade, and J. Cohn, ¬™Robust Lip Tracking by
Combining Shape, Color, and Motion,¬∫ Proc. Asian Conf. Computer
Vision, pp. 1040-1045, 2000.
[34] Y. Tian, T. Kanade, and J. Cohn, ¬™Dual-State Parametric Eye
Tracking,¬∫ Proc. Int'l Conf. Face and Gesture Recognition, pp. 110-
115, Mar. 2000.
[35] M. Turk and A. Pentland, ¬™Face Recognition Using Eigenfaces,¬∫
Proc. IEEE Conf. Computer Vision and Pattern Recognition, pp. 586-
591, 1991.
[36] Y. Yacoob and M.J. Black, ¬™Parameterized Modeling and
Recognition of Activities,¬∫ Proc. of the Sixth Int'l Conf. Computer
Vision, pp. 120-127, 1998.
[37] Y. Yacoob and L.S. Davis, ¬™Recognizing Human Facial Expression
from Long Image Sequences Using Optical Flow,¬∫ IEEE Trans.
Pattern Analysis and Machine Intelligence, vol. 18, no. 6, pp. 636-642,
June 1996.
[38] Y. Yacoob, H. Lam, and L. Davis, ¬™Recognizing Face Showing
Expressions,¬∫ Proc. Int'l Workshop Automatic Face and Gesture
Recognition, 1995.
[39] A. Yuille, P. Haallinan, and D.S. Cohen, ¬™Feature Extraction from
Faces Using Deformable Templates,¬∫ Int'l J. Computer Vision, vol. 8,
no. 2, pp. 99-111, 1992.
[40] Z. Zhang, ¬™Feature-Based Facial Expression Recognition: Sensi-
tivity Analysis and Experiments with a Multilayer Perceptron,¬∫Int'l J. Pattern Recognition and Artificial Intelligence, vol. 13, no. 6,
pp. 893-911, 1999.
Ying-li Tian received the BS and MS degrees in
optical engineering from TianJin University,China, in 1987 and 1990, and the PhD degreein electrical engineering from the Chinese
University of Hong Kong, in 1996. After holding
a faculty position at National Laboratory ofPattern Recognition, Chinese Academy ofSciences, Beijing, she joined Carnegie MellonUniversity in 1998, where she is currently apostdoctoral fellow of the Robotics Institute. Her
current research focuses on a wide range of computer vision problems
from photometric modeling and shape from shading, to human
identification, 3D reconstruction, motion analysis, and facial expressionanalysis. She is a member of the IEEE.114 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 23, NO. 2, FEBRUARY 2001

Takeo Kanade received the Doctoral degree in
electrical engineering from Kyoto University,Japan, in 1974. After holding a faculty positionat Department of Information Science, KyotoUniversity, he joined Carnegie Mellon University
in 1980. He is currently U.A. Helen Whitaker
University Professor of computer science anddirector of the Robotics Institute. Dr. Kanade hasworked in multiple areas of robotics: computervision, manipulators, autonomous mobile robots,
and sensors. He has written more than 200 technical papers and reports
in these areas, as well as more than 10 patents. He has been the
principal investigator of a dozen major vision and robotics projects atCarnegie Mellon. Dr. Kanade has been elected to the National Academyof Engineering. He is a fellow of the IEEE, the ACM, a founding fellow ofthe American Association of Artificial Intelligence, and the foundingeditor of the International Journal of Computer Vision . He has received
several awards including the C&C Award, the Joseph Engelberger
Award, JARA Award, Otto Franc Award, Yokogawa Prize, and Marr
Prize Award. Dr. Kanade has served the government, industry, and as auniversity advisory or on consultant committees, including Aeronauticsand Space Engineering Board (ASEB) of the National ResearchCouncil, NASA's Advanced Technology Advisory Committee, and theAdvisory Board of Canadian Institute for Advanced Research.Jeffrey F. Cohn received the PhD degree in
clinical psychology from the University of Mas-sachusetts at Amherst in 1983. He is anassociate professor of psychology and psychia-try at the University of Pittsburgh and an adjunct
faculty member at the Robotics Institute, Carne-
gie Mellon University. Dr. Cohn's primary re-search interests are emotion and paralinguisticcommunication, developmental psychopathol-
ogy, face image and prosodic analysis, and human-computer interac-tion. He has published more than 60 peer reviewed articles and
conference proceedings on these topics. He is member of the IEEE.
TIAN ET AL.: RECOGNIZING ACTION UNITS FOR FACIAL EXPRESSION ANALYSIS 115

"
https://ieeexplore.ieee.org/document/1467492/1000,"Recognizing Facial Expression: Machine Learning and Application to
Spontaneous Behavior
Marian Stewart Bartlett1, Gwen Littlewort1,M a r kF r a n k2, Claudia Lainscsek1,
Ian Fasel1, Javier Movellan1
Institute for Neural Computation, University of California, San Diego
2Rutgers University, New Brunswick, NJ
mbartlett@ucsd.edu
Abstract
W e present a systematic comparison of machine learning
methods applied to the problem of fully automatic recogni-tion of facial expressions. W e report results on a series of
experiments comparing recognition engines, including Ad-
aBoost, support vector machines, linear discriminant anal-
ysis. W e also explored feature selection techniques, includ-
ing the use of AdaBoost for feature selection prior to clas-siÔ¨Åcation by SVM or LDA. Best results were obtained by
selecting a subset of Gabor Ô¨Ålters using AdaBoost followed
by classiÔ¨Åcation with Support V ector Machines. The system
operates in real-time, and obtained 93% correct general-
ization to novel subjects for a 7-way forced choice on the
Cohn-Kanade expression dataset. The outputs of the clas-siÔ¨Åers change smoothly as a function of time and thus can
be used to measure facial expression dynamics. W e applied
the system to to fully automated recognition of facial ac-
tions (F ACS). The present system classiÔ¨Åes 17 action units,
whether they occur singly or in combination with other ac-
tions, with a mean accuracy of 94.8%. W e present prelimi-nary results for applying this system to spontaneous facial
expressions.
1 Introduction
We present results on a user independent fully automatic
system for real time recognition of basic emotional expres-
sions from video. The system automatically detects frontal
faces in the video stream and codes each frame with respect
to 7 dimensions: Neutral, anger, disgust, fear, joy, sadness,
surprise. A second version of the system detects 17 actionunits of the Facial Action Coding System (FACS). We con-
ducted empirical investigations of machine learning meth-
ods applied to this problem, including comparison of recog-nition engines and feature selection techniques. Best results
were obtained by selecting a subset of Gabor Ô¨Ålters using
AdaBoost and then training Support V ector Machines onthe outputs of the Ô¨Ålters selected by AdaBoost. The com-
bination of AdaBoost and SVM‚Äôs enhanced both speed andaccuracy of the system. The system presented here is fully
automatic and operates in real-time. We present prelimi-
nary results for recognizing spontaneous expressions in an
interview setting.
2 Facial Expression Data
The facial expression system was trained and tested on
Cohn and Kanade‚Äôs DFA T-504 dataset [7]. This dataset con-sists of 100 university students ranging in age from 18 to
30 years. 65% were female, 15% were African-American,
and 3% were Asian or Latino. Videos were recoded in ana-
log S-video using a camera located directly in front of the
subject. Subjects were instructed by an experimenter to per-
form a series of 23 facial expressions. Subjects began eachdisplay with a neutral face. Before performing each display,
an experimenter described and model ed the desired display.
Image sequences from neutral to target display were digi-
tized into 640 by 480 pixel arrays with 8-bit precision for
grayscale values. For our study, we selected the 313 se-
quences from the dataset that were labeled as one of the 6
basic emotions. The sequences came from 90 subjects, with
1 to 6 emotions per subject. The Ô¨Årst and last frames (neu-tral and peak) were used as training images and for testing
generalization to new subjects, for a total of 626 examples.
The trained classiÔ¨Åers were later applied to the entire se-quence.
2.1 Real-time Face Detection
We developed a real-time face detection system that em-
ploys boosting techniques in a generative framework [5]
and extends work by [17]. Enhancements to [17] include
employing Gentleboost instead of Adaboost, smart featuresearch, and a novel cascade training procedure, combined
in a generative framework. Source code for the face detec-
tor is freely available at http://kolmogorov.sourceforge.net.
Accuracy on the CMU-MIT dataset, a standard public data
set for benchmarking frontal face detection systems, is 90%
detections and 1/million false alarms, which is state-of-the-
art accuracy. The CMU test set has unconstrained lightingand background. With controlled lighting and background,
such as the facial expression data employed here, detection
accuracy is much higher. The system presently operates at24 frames/second on a 3 GHz Pentium IV for 320x240 im-
ages.
All faces in the DFA T-504 dataset were successfully de-
tected. The automatically located faces were rescaled to
48x48 pixels. The typical distance between the centers ofthe eyes was roughly 24 pixels. No further registration was
performed. The images were converted into a Gabor mag-
nitude representation, using a bank of Gabor Ô¨Ålters at 8 ori-entations and 9 spatial frequencies (2:32 pixels per cycle at
1/2 octave steps) (See [9] and [10]).
3 ClassiÔ¨Åcation of Full Expressions
3.1 Support V ector Machines
We Ô¨Årst examined facial expression classiÔ¨Åcation based
on support vector machines (SVM‚Äôs). SVM‚Äôs are well
suited to this task because the high dimensionality of theGabor representation O( 10
5) does not affect training time,
which depends only on the number of training examples
O(102). The system performed a 7-way forced choice be-
tween the following emotion categories: Happiness, sad-
ness, surprise, disgust, fear, anger, neutral. Methods for
multiclass decisions with SVM‚Äôs were investigated in [10].
Here, the seven-way forced choice was performed in two
stages. In stage I, support vector machines performed bi-nary decision tasks using one-versus-all partitioning of the
data, where each SVM discriminated one emotion from ev-
erything else. Stage II converted the representation pro-duced by the Ô¨Årst stage into a probability distribution over
the seven expression categories. This was achieved by pass-
ing the 7 SVM outputs through a softmax competition.
Generalization to novel subjects was tested using leave-
one-subject-out cross-validation, in which all images of the
test subject were excluded from training. Linear, polyno-mial, and radial basis function (RBF) kernels with Lapla-
cian, and Gaussian basis functions were explored. Lin-
ear and RBF kernels employing a unit-width Gaussian per-formed best, and are presented here. Results are given in
Table 1.
3.2 Adaboost
SVM performance was next compared to Adaboost for
emotion classiÔ¨Åcation. The features employed for the Ad-
aboost emotion classiÔ¨Åer were the individual Gabor Ô¨Ålters.
This gave 9x8x48x48= 165,888 possible features. A subsetof these features was chosen using Adaboost. On each train-
ing round, the Gabor feature with the best expression clas-
siÔ¨Åcation performance for the current boosting distributionwas chosen. The performance measure was a weighted sum
of errors on a binary classiÔ¨Åcation t ask, where the weighting
distribution (boosting) was updated at every step to reÔ¨Çect
how well each training vector was classiÔ¨Åed.
Adaboost training continued until the classiÔ¨Åer output
distributions for the positive and negative samples were
completely separated by a gap proportional to the widths
of the two distributions. The union of all features selected
for each of the 7 emotion classiÔ¨Åers resulted in a total of
900 features.
ClassiÔ¨Åcation results are given in Table 1. The general-
ization performance with Adaboost was comparable to lin-
ear SVM performance. Adaboost had a substantial speed
advantage. There was a 180-fold reduction in the number
of Gabor Ô¨Ålters used. Because the system employed a sub-
set of Ô¨Ålter outputs at speciÔ¨Åc image locations the convo-lutions were calculated in pixel space rather than Fourier
space which reduced the speed advantage, but it neverthe-
less resulted in a speed beneÔ¨Åt of over 3 times faster thanthe linear SVM.
3.3 Linear Discriminant Analysis
A previous successful approach to basic emotion recog-
nition used Linear Discriminant Analysis (LDA) to classify
Gabor representations of images [11]. While LDA may be
optimal when the class distributions are Gaussian, SVM‚Äôsmay be more effective when the class distributions are not
Gaussian. Table 1 compares LDA with SVM‚Äôs and Ad-
aboost. A small ridge term was used in LDA.
The performance results for LDA were dramatically
lower than SVMs. Performance with LDA improved by ad-
justing the decision threshold for each emotion so as to bal-ance the number of false detects and false negatives. This
form of threshold adjustment is commonly employed with
LDA classiÔ¨Åers, but it uses post-hoc information, whereasthe SVM performance was without post-hoc information.
Even with the threshold adjustment, the linear SVM per-
formed signiÔ¨Åcantly better than LDA. (See Tables 1 and 2.)
3.4 Feature selection using PCA
Many approaches to LDA a lso employ PCA to perform
feature selection prior to classiÔ¨Åcation. For each classi-Ô¨Åer we searched for the number of PCA components which
gave maximum LDA performance, which was typically 40
to 70 components. The PCA step resulted in a substantial
improvement. The combination of PCA and threshold ad-
justment gave performance accuracy of 80.7% for the 7-alternative forced choice, which was comparable to other
LDA results in the literature [11]. Nevertheless, the lin-
ear SVM outperformed LDA even with the combination of
PCA and threshold adjustment. SVM performance on the
PCA representation was signiÔ¨Åcantly reduced, indicating an
incompatibility between PCA and SVM‚Äôs for the problem.
3.5 Feature selection by Adaboost
Adaboost is not only a fast classiÔ¨Åer, it is also a feature
selection technique. An advantage of feature selection byAdaboost is that features are selected contingent on the fea-
tures that have already been selected. In feature selection by
Adaboost, each Gabor Ô¨Ålter is a treated as a weak classiÔ¨Åer.
Adaboost picks the best of those classiÔ¨Åers, and then boosts
the weights on the examples to weight the errors more. Thenext Ô¨Ålter is selected as the one that gives the best perfor-
mance on the errors of the previous Ô¨Ålter. At each step, the
chosen Ô¨Ålter can be shown to be uncorrelated with the out-put of the previous Ô¨Ålters [6, 15].
We explored training SVM and LDA classiÔ¨Åers on the
features selected by Adaboost. Here, the classiÔ¨Åers were
t r a i n e do nt h e continuous outputs of the selected Gabor
features, in contrast to the Adaboost classiÔ¨Åer which em-
ployed thresholded outputs. Adaboost was used to select
900 features from 9x8x48x48=165888 possible Gabor fea-
tures, which were then classiÔ¨Åed by the SVM or LDA.
The results are shown in Table 1 and 2. Best performance
was obtained with the combination of Adaboost and SVM‚Äôs.
We informally call these combined classiÔ¨Åers AdaSVM. We
informally call these combined classiÔ¨Åers AdaSVM. The re-sults are shown in Table 1. AdaSVM‚Äôs outperformed both
Adaboost (z=2 .1,p =0 .2)and SVM‚Äôs (z=2 .6,p <
.01),w h e r e zis the Z-statistic for comparing success rates
of Bernoulli random variables, and pis probability that the
two performances come from the same distribution. The re-sult of 93.3% accuracy for a user-independent 7-alternative
forced choice was encouraging given that previously pub-
lished results on this database were 81-83% accuracy (e.g.
[2]). AdaSVM‚Äôs also carried a substantial speed advantage
over SVM‚Äôs. The nonlinear AdaSVM was over 400 times
faster than the nonlinear SVM.
Regarding LDA, feature selection with Adaboost gave
better performance than feature selection by PCA and re-
duced the difference in performance between LDA and
SVM‚Äôs. Nevertheless, SVM‚Äôs continued to outperform
LDA.
Table 1. Leave-one-out generalization performance of Ad-
aboost,SVM‚Äôs and AdaSVM‚Äôs. AdaSVM: Feature selection by
AdaBoost followed by classiÔ¨Åcation with SVM‚Äôs. LDA pca:L i n -
ear Discriminant analysis with feature selection based on principle
component analysis, as commonly implemented in the literature.
Kernel
 Adaboost SVM AdaSVM LDA pca
Linear
 90.1 88.0 93.3 80.7
RBF
 89.1 93.3Table 2. Comparing SVM performance to LDA with different fea-
ture selection techniques. The two classiÔ¨Åers are compared with
no feature selection, with feature selection by PCA, and feature
selection by Adaboost.
LDA SVM (linear)
Feature selection
None
 44.4 88.0
PCA
 80.7 75.5
Adaboost
 88.2 93.3
4 Application to Spontaneous Behavior
In order to objectively capture the richness and complex-
ity of facial expressions, behavioral scientists have found it
necessary to develop objective coding standards. The facialaction coding system (FACS) [4] is the most objective and
comprehensive coding system in the behavioral sciences.
A human coder decomposes facial expressions in terms of46 component movements, which roughly correspond to
the 44 facial muscles. Several research groups have rec-
ognized the importance of automatically recognizing FACS[3, 16, 14, 8]. Here we apply the system described above to
the problem of fully automated facial action coding.
4.1 Spontaneous Expression Database
Our collaborators at Rutgers University have collected
a dataset of spontaneous facial behavior consisting of 100subjects participating in a ‚Äôfalse opinion‚Äô paradigm. In this
paradigm, subjects Ô¨Årst Ô¨Åll out a questionnaire regarding
their opinions about a social or political issue. Subjects arethen asked to either tell the truth or take the opposite opinion
on an issue where they rated strong feelings, and convince
an interviewer they are telling the truth. This paradigm hasbeen shown to elicit a wide range of emotional expressions
as well as speech-related facial expressions. This dataset
is particularly challenging both because of speech-related
mouth movements, and also because of out-of-plane head
rotations which tend to be present during discourse.
Two minutes of each subject‚Äôs behavior is being FACS
coded by two certiÔ¨Åed FACS coders. FACS codes include
the apex frame as well as the onset and offset frame foreach action unit (AU). Here we present preliminary results
for a system trained on two large datasets of FACS-coded
posed expressions, and tested on the spontaneous expres-sion database.
4.2 FACS Training
The system was trained on FACS-coded images from 2
datasets. The Ô¨Årst dataset was the Cohn Kanade dataset,
which contains FACS scores by two certiÔ¨Åed FACS coders
in addition to the basic emotion labels. The second dataset
consisted of directed facial actions collected by Hager and
Ekman. (See [3].) The combined dataset contained 2568
training examples from 119 subjects. As above, the sys-tem was fully automated. Automatic eye detection [5] was
employed to align the eyes in each image. Images were
scaled to 192x192, passed through a bank of Gabor Ô¨Åltersat 8 orientations and 7 spatial frequencies (4:32 pixels per
cyc). Output magnitudes were then passed to nonlinear sup-
port vector machines using RBF kernels. No feature selec-
tion was performed, although we plan to evaluate feature
selection by AdaBoost in the near future.
Separate support vector machines, one for each AU,
were trained to perform context-independent recognition.
In context-independent recognition, the system detects the
presence of a given AU regardless of the co-occurring AU‚Äôs.
Positive examples consisted of the last frame of each se-quence which contained the expression apex. Negative ex-
amples consisted of all apex frames that did not contain the
target AU plus neutral images obtained from the Ô¨Årst frameof each sequence, for a total of 2568-N negative examples
for each AU.
4.3 Generalization Performance Within Dataset
We Ô¨Årst report performance for generalization to
novel subjects within the Cohn-Kanade and Ekman-Hager
databases. Generalization to new subjects was tested us-
ing leave-one-subject-out cross-validation. The results areshown in Table 3. All system outputs above threshold were
treated as detections. Performance was evaluated for thresh-
olds of 0 in the SVM, and then evaluated again for the opti-
mal threshold that maximized percent correct.
The system obtained a mean of 94.8% agreement with
human FACS labels. System outputs for full image se-
quences of test subjects are shown in Figure 1. Althougheach individual image is separately processed and classi-
Ô¨Åed, the outputs change smoothly as a function of expres-
sion magnitude in the successive frames of each sequence,enabling applications for measuring the magnitude and dy-
namics of facial expressions.
Figure 1. Automated FACS measurements for full image se-
quences. Shown are 4 subjects from the Cohn-Kanade dataset pos-
ing disgust containing AU‚Äôs 4,7 and 9. These are test sequences
not used for training.
Over 7000 action unit combinations have been reportedTable 3. Performance for fully automatic recognition of 17 facial
actions, generalization to novel subjects in the Cohn-Kanade and
Ekman-Hager databases. N: Total number of positive examples. P:
Percent agreement with Human FACS codes (positive and negative
examples classed correctly). P opt: Same with optimal threshold.
FA, Hit: Hit and false alarm rates with optimal threshold.
AU Name N P P opt FA Hit
1 Inn. brow raise 409 90.3 92.9 0.4 71.3
2 Out. brow raise 315 91.8 92.8 1.6 62.6
4 Brow lower 412 82.7 86.8 6.9 41.05 Upper lid raise 286 91.2 92.9 2.1 61.9
6 Cheek raise 278 92.8 93.5 1.4 70.1
7 Lower lid tight 403 85.7 88.5 4.6 52.1
9 Nose wrinkle 68 98.7 98.8 0.04 85.3
10 Lip Raise 50 97.7 98.1 13.9 26.012 Lip crnr. pull 196 97.8 98.0 0.04 93.4
15 Lip crnr. depr. 100 97.0 97.2 1.0 72.0
17 Chin raise 203 87.0 92.8 7.0 40.420 Lip stretch 99 94.4 96.2 6.6 41.4
23 Lip tighten 57 97.0 97.9 11.0 36.8
24 Lip press 49 98.4 98.5 1.7 61.225 Lips part 376 89.7 91.2 2.2 64.9
26 Jaw drop 86 96.7 97.1 5.9 45.3
27 Mouth stretch 81 99.2 99.2 0.04 97.5
Mean 93.4 94.8 3.9 60.2
in the psychology literature, and the problem of how to han-
dle recognition of action unit combinations has received
considerable discussion (e.g. [16, 13]). Here we address
recognition of combinations by training a data-driven sys-
tem to detect a given action regardless of whether it appearssingly or in combination with other actions (context inde-
pendent recognition). A strength of data-driven systems is
that they learn the variations due to combinations, and theyalso learn the most likely contexts of an action. Nonlin-
ear support vector machines have the added advantage of
being able to handle multimodal data distributions which
can arise with action combinations.
1It is an open question
whether building classiÔ¨Åers for speciÔ¨Åc combinations im-
proves recognition performance, and that is a topic of futurework.
4.4 Generalization to Spontaneous Expressions
The system described in Section 4.2 was then tested on
the spontaneous expression database. Preliminary results
1when the class of kernel is well matched to the problem. The distribu-
tion of facial expression data is not well known, and this question requiresempirical study. Several labs in addition to ours have found a range of RBF
kernels to be effective for face classiÔ¨Åcation tasks.
are presented for 12 subjects. This data contained a total of
1689 labeled events, consisting of 33 distinct action units,
16 of which were AU‚Äôs for which we had trained classiÔ¨Åers.
The face detector operates for frontal faces of ¬±10 deg ,
whereas unconstrained head movements during discourse
can rotate outside that range. Face detections were accepted
if the face box was greater than 150 pixels width, both eyeswere detected with positive position, and the distance be-
tween the eyes was >40 pixels. This resulted in faces found
for 95% of the video frames. All detected faces were passed
to the AU recognition system.
a
b
Figure 2. Sample system outputs for a 10-second segment con-
taining a brow-raise (FACS code 1+2). System output is shown
for AU 1 (left) and AU 2 (right). Human codes are overlayed for
comparison (onset, apex, offset).
Here we present benchmark performance of the basic
frame-by-frame system on the video data. Figure 2 shows
sample system outputs for one subject, and performance is
shown in Table 4. Performance was assessed several ways.
First, we assessed overall percent correct for each actionunit on a frame-by-frame basis, where system outputs that
were above threshold inside the onset and offset interval
indicated by the human FACS codes, and below thresholdoutside that interval were considered correct. This gave an
overall accuracy of 90.5% correct across AU‚Äôs.
2
Next an interval analysis was performed which measured
percent correct detections on intervals of length I. Here we
present performance for intervals of length 21 (10 on either
side of the apex), but performance was stable for a range of
choices of I. A target AU was treated as present if at least
6/21 frames were above threshold. An SVM threshold of
2Overall percent correct can give high numbers since the AU‚Äôs are
present for a small percentage of frames.Table 4. Recognition of spontaneous facial actions. AU: Action
unit number. N: Total number of testing examples. Dur.: Mean
duration of the AU in frames. P: percent correct over all frames;
Hit apex : Hit rate for AU apex frame. P ‚àÜ: Percent correct for
interval analysis (see text). FA, Hit: Hit and false alarm rates for
interval analysis.
AU
 ND u r . P P ‚àÜ FA Hit
1
 166 30 84 81 17 48
2
 138 23 88 79 20 55
4
 33 23 93 78 22 55
5
 34 26 98 80 20 33
6
 56 112 91 86 13 79
7
 48 78 83 76 22 33
9
 2 12 100 79 21 100
10
 53 69 95 76 23 29
12
 112 102 86 84 11 58
15
 73 18 98 80 19 40
17
 88 39 93 78 20 48
20
 8 8 99 80 20 18
23
 29 46 94 79 21 36
24
 66 27 92 77 22 17
25
 131 65 65 74 21 34
26
 105 55 92 73 23 27
Mean
 90.5 78.8 19.7 44.4
1 standard deviation above the mean was employed. Neg-
ative examples consisted of the remaining 2 minute video
stream for each subject, outside the FACS coded onset and
offset intervals for the target AU, parsed into intervals of
21 frames. Mean percent correct for the interval analysiswas 79%, with hit and false alarm rates of 44% and 20%
respectively.
5 Conclusions
We presented a systematic comparison of machine learn-
ing methods applied to the problem of fully automaticrecognition of facial expressions, including AdaBoost, sup-
port vector machines, and linear discriminant analysis, as
well as feature selection methods. Best results were ob-tained by selecting a subset of Gabor Ô¨Ålters using AdaBoost
and then training Support V ector Machines on the outputs
of the Ô¨Ålters selected by AdaBoost. The combination of
Adaboost and SVM‚Äôs enhanced both speed and accuracy of
the system. The full system operates in real time. Face de-
tection runs at 24 frames/second in 320x240 images on a 3
GHz Pentium IV . The expression recognition step operates
in less than 10 msec.
The generalization performance to new subjects for
recognition of full facial expressions of emotion in a 7-way
forced choice was 93.3%, which is the best performance
reported so far on this publicly available dataset. Our re-
sults suggest that user independent, fully automatic real
time coding of facial expressions in the continuous video
stream is an achievable goal with present computer power,at least for applications in which frontal views can be as-
sumed.
The machine-learning based system presented here can
be applied to recognition of any facial expression dimen-
sion given a training dataset. Here we applied the sys-
tem to fully automated facial action coding, and obtained
a mean agreement rate of 94.8% for 17 AU‚Äôs from the Fa-
cial Action Coding System. The outputs of the expressionclassiÔ¨Åers change smoothly as a function of time, provid-
ing information about expression dynamics that was previ-
ously intractable by hand coding. The system is fully auto-mated, and performance rates are similar to or better than
other systems tested on this dataset that employed varying
levels of manual registration. The approach to automaticFACS coding presented here, in addition to being fully au-
tomated, also differs from approaches such as [13] and [16]
in that instead of designing special purpose image featuresfor each facial action, we explore general purpose learning
mechanisms for data-driven facial expression classiÔ¨Åcation.
The approach detects not only changes in position of fea-
ture points, but also changes in image texture such as those
created by wrinkles, bulges, and changes in feature shapes.
Here we presented preliminary r esults for the perfor-
mance of the system on spontaneous expressions. The sys-
tem was able to detect facial actions in this database de-
spite the presence of speech, out-of-plane head movements
that occur during discourse, and the fact that many of the
action units occurred in combination. These results pro-vide a benchmark for frame-by-frame analysis by a system
trained for frontal views. The output sequence contains in-
formation about dynamics that can be exploited for decid-ing the presence of a facial action [1]. Future work will
explore these dynamics, and compare improvement to the
benchmark provided here. The accuracy of automated fa-
cial expression measurement may also be considerably im-
proved by 3D alignment of faces. Moreover, information
about head movement dynamics is an important component
of nonverbal behavior, and is measured in FACS. Members
of this group have developed techniques for automaticallyestimating 3D head pose in a generative model [12] and for
aligning face images in 3D. These techniques will be inte-
grated into future versions of our system.
Acknowledgments. Support for this work was provided
by NSF-ITR IIS-0220141, California Digital Media Inno-
vation Program DiMI 01-10130, and NRL Advanced Infor-
mation Technology 55-05-03.
References
[1] M.S. Bartlett, G. Littlewort, B. Braathen, T.J. Sejnowski,
and J.R. Movellan. A protot ype for automatic recognitionof spontaneous facial actions. In S. Becker, S. Thrun, and
K. Obermayer, editors, Advances in Neural Information Pro-
cessing Systems , volume 15, pages 1271‚Äì1278, Cambridge,
MA, 2003. MIT Press.
[2] I. Cohen, N. Sebe, F. Cozman, M. Cirelo, and T. Huang.
Learning baysian network classiÔ¨Åers for facial expression
recognition using both lab eled and unlabeled data. Computer
Vision and Pattern Recognition. , 2003.
[3] G. Donato, M. Bartlett, J. Hager, P. Ekman, and T. Se-
jnowski. Classifying facial actions. IEEE Transactions on
Pattern Analysis and Machine Intelligence , 21(10):974‚Äì989,
1999.
[4] P. Ekman and W. Friesen. Facial Action Coding System: A
Technique for the Measurement of Facial Movement . Con-
sulting Psychologists Press, Palo Alto, CA, 1978.
[5] I. R. Fasel, B. Fortenberry, and J. R. Movellan. GBoost: A
generative framework for boosting with applications to real-
time eye coding. Computer Vision and Image Understand-
ing, in press.
[6] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic
regression: a statistical view of boosting, 1998.
[7] T. Kanade, J.F. Cohn, and Y . Tian. Comprehensive database
for facial expression analysis. In Proceedings of the fourth
IEEE International conference on automatic face and ges-ture recognition (FG‚Äô00) , pages 46‚Äì53, Grenoble, France,
2000.
[8] A. Kapoor, Y . Qi, and R.W.Picard. Fully automatic upper
facial action recognition. IEEE International Workshop on
Analysis and Modeling of Faces and Gestures. , 2003.
[9] M. Lades, J. V orbr¬® uggen, J. Buhmann, J. Lange, W. Konen,
C. von der Malsburg, and R. W ¬® urtz. Distortion invariant
object recognition in the dynamic link architecture. IEEE
Transactions on Computers , 42(3):300‚Äì311, 1993.
[10] G. Littlewort, M.S. Bartlett, I. Fasel, J. Susskind, and J.R.
Movellan. Dynamics of facial expression extracted automat-
ically from video. In IEEE Conference on Computer Vision
and Pattern Recognition, Workshop on Face Processing in
Video , 2004.
[11] M. Lyons, J. Budynek, A. Plante, and S. Akamatsu. Classify-
ing facial attributes using a 2-d gabor wavelet representation
and discriminant analysis. In Proceedings of the 4th interna-
tional conference on automatic face and gesture recognition ,
pages 202‚Äì207, 2000.
[12] T. K. Marks, J. Hershey, J. Cooper Roddey, and J. R. Movel-
lan. 3d tracking of morphable objects using condition-ally gaussian nonlinear Ô¨Ålters. Computer Vision and Image
Understanding , under review. See also CVPR04 workshop:
Generative-Model Based Vision.
[13] M. Pantic and J.M. Rothkrantz. Automatic analysis of fa-
cial expressions: State of the art. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence , 22(12):1424‚Äì1445,
2000.
[14] M. Pantic and J.M. Rothkrantz. Facial action recognition
for facial expression analysis from static face images. IEEE
Transactions on Systems, Man and Cybernetics , 34(3):1449‚Äì
1461, 2004.
[15] R. E. Schapire. A brief introduction to boosting. In IJCAI ,
pages 1401‚Äì1406, 1999.
[16] Y .L. Tian, T. Kanade, and J.F. Cohn. Recognizing ac-
tion units for facial expression analysis. IEEE Transactions
on Pattern Analysis and Machine Intelligence , 23:97‚Äì116,
2001.
[17] Paul Viola and Michael Jones. Robust real-time object detec-
tion. Technical Report CRL 20001/01, Cambridge Research-
Laboratory, 2001.
"
https://ieeexplore.ieee.org/document/8099760,"Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression
Recognition in the Wild
Shan Li, Weihong Deng, and JunPing Du
Beijing University of Posts and Telecommunications
{ls1995, whDeng, junpingd }@bupt.edu.cn
Abstract
Past research on facial expressions have used relative-
ly limited datasets, which makes it unclear whether cur-
rent methods can be employed in real world. In this pa-
per , we present a novel database, RAF-DB , which contains
about 30000 facial images from thousands of individuals.
Each image has been individually labeled about 40 times,
then EM algorithm was used to Ô¨Ålter out unreliable label-
s. Crowdsourcing reveals that real-world faces often ex-
press compound emotions, or even mixture ones. F or all
we know, RAF-DB is the Ô¨Årst database that contains com-
pound expressions in the wild. Our cross-database study
shows that the action units of basic emotions in RAF-DB are
much more diverse than, or even deviate from, those of lab-
controlled ones. To address this problem, we propose a new
DLP-CNN (Deep Locality-Preserving CNN) method, which
aims to enhance the discriminative power of deep features
by preserving the locality closeness while maximizing the
inter-class scatters. The benchmark experiments on the 7-
class basic expressions and 11-class compound expression-
s, as well as the additional experiments on SFEW and CK+
databases, show that the proposed DLP-CNN outperforms
the state-of-the-art handcrafted features and deep learning
based methods for the expression recognition in the wild.
1. Introduction
Millions of images are being uploaded every day by user-
s from different events and social gatherings. There is an
increasing interest in designing systems capable of under-
standing human manifestations of emotional attributes and
affective displays. To automatic learn the affective state of
face images from the Internet, large annotated databases are
required. However, the complexity of annotations of emo-
tion categories has hindered the collection of large annotat-
ed databases. On the other side, popular AU coding [12]
requires speciÔ¨Åc expertise to take months to learn and be
perfected, hence, alternative solutions are needed. And dueto the cultural difference in the way of perceiving facial e-
motion [13], it is difÔ¨Åcult for psychologists to deÔ¨Åne deÔ¨Ånite
prototypical AUs for each facial expressions. Therefore, it
is also worth to study the emotion of social images from the
judgments of a large common population, besides from the
professional knowledge of a few experts.
In this paper, we propose to study the common ex-
pression perception by a reliable crowdsourcing approach.
SpeciÔ¨Åcally, our well-trained annotators are asked to label
face images with one of the seven basic categories [11],
and each face is annotated enough times independently, i.e.
about 40 times in our experiment. Then, the noisy labels
are Ô¨Åltered by an EM based reliability evaluation algorithm,
through which each image can be represented reliably by a
7-dimensional emotion probability vector. By analyzing 1.2
million labels of 29672 great-diverse facial images down-
loaded from the Internet, these Real-world Affective Faces
(RAF)1are naturally categorized into two types: basic ex-
pression with single-modal distribution and compound e-
motions with bimodal distribution, an observation support-
ing a recent ground-breaking Ô¨Ånding in the lab-controlled
condition [10]. To the best of our knowledge, the real-
world expression database RAF-DB is the Ô¨Årst large-scale
database providing the labels of common expression per-
ception and compound emotions in unconstrained environ-
ment.
The cross-database experiment and AU analysis on
RAF-DB indicates that AUs of real-world expressions are
much more diverse than, or even deviate from, those of
lab-controlled ones guided by psychologists. To address
this ambiguity of unconstrained emotion, we further pro-
pose a novel Deep Locality-preserving CNN (DLP-CNN).
Inspired by [17], we develop a practical back-propagation
algorithm which creates a locality preserving loss (LP loss)
aiming to pull the locally neighboring faces of the same
class together. Jointly trained with the classical softmax
loss which forces different classes to stay apart, locality p-
reserving loss drives the intra-class local clusters of each
1http://whdeng.cn/RAF/model1.html
2017 IEEE Conference on Computer Vision and Pattern Recognition
1063-6919/17 $31.00 ¬© 2017 IEEE
DOI 10.1109/CVPR.2017.2772584

/g94/g437/g396/g393/g396/g349/g400/g286/g282
/g38/g286/g258/g396/g296/g437/g367
/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g349/g400/g336/g437/g400/g410/g286/g282
/g44/g258/g393/g393/g455
/g94/g258/g282
/;#23#23#23#23/g374/g336/g396/g455
/g69/g286/g437/g410/g396/g258/g367
(a) DCNN without LP loss
/g94/g437/g396/g393/g396/g349/g400/g286/g282
/g38/g286/g258/g396/g296/g437/g367
/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g349/g400/g336/g437/g400/g410/g286/g282
/g44/g258/g393/g393/g455
/g94/g258/g282
/;#23#23#23#23/g374/g336/g396/g455
/g69/g286/g437/g410/g396/g258/g367
(b) DLP-CNN
/g94/g437/g396/g393/g396/g349/g400/g286/g282
/g38/g286/g258/g396/g296/g437/g367
/g44/g258/g393/g393/g455
/g94/g258/g282
/;#23#23#23#23/g374/g336/g396/g455
(c) AmpliÔ¨Åed face images in color from SubÔ¨Ågure (b)
Figure 1. The distribution of deeply learned features in (a) ‚ÄúDCNN without LP loss‚Äù and (b) ‚ÄúDLP-CNN‚Äù. As can be seen, locality preserv-
ing loss layer helps the network to learn features with more discrimination. Moreover, it can be clearly seen that non-neutral expressions
which have obvious intensity variations, such as Happiness, Sadness, Fear, Surprise and Anger, change the intensity continuously and
smoothly, from low to high, from center to periphery. And images with Disgust label, which is the most confused expression, are assem-
bled in the middle. With the neighborhood preserving character of DLP-CNN, the deep feature seems to be able to capture the intrinsic
expression manifold structure to a large extent. Best viewed in color.
class to become compact, and thus the discriminative pow-
er of the deeply learned features can be highly enhanced.
Moreover, locally neighboring faces tend to share similar e-
motion intensity by using DLP-CNN, which can derive the
discriminative deep feature with smooth emotion intensity
transition. Figure 1 (b) shows the resulting 2-dimensional
deep features learnt from our DLP-CNN model, where we
attach example face images with various intensity in differ-
ent expression classes.
Extensive experiments on RAF-DB and other related
databases show that the proposed DLP-CNN outperform-
s other state-of-the-art methods. Moreover, the activation
features trained on RAF-DB can be re-purposed to new
databases with small-sample training data, suggesting that
the DLP-CNN is a powerful tool to handle the cross-culture
problem on perception of emotion (POE).
2. Related Work
2.1. Expression image datasets
Facial expression recognition largely relies on well-
deÔ¨Åned databases, however, several limitations exist.
Many available databases were produced in tightly con-
trolled environments without diversity on subjects and con-
ditions. Subjects in them were taught to act expressions in
a uniform way. Besides, the majority of current databas-
es only include six basic categories or less. However, im-
ages captured in real-life scenarios often present complex,
compound or even ambiguous emotions rather than simple
and prototypical ones [3]. What‚Äôs more, labelers in these
databases are too few, which would reduce the reliability
and validity of the emotion labels.
We then focus on discussing image databases with spon-
taneous expressions. SFEW 2.0 [7] contains 700 images
extracted from movies, and images were labelled by twoindependent labelers. The database covers unconstrained
facial expressions, varied head poses, large age range, oc-
clusions, varied focus, different resolution of face. FER-
2013 [16] contains 35887 images collected and labelled us-
ing the Google image search API. Cropped images are pro-
vided in 48 √ó48 pixels and converted to grayscale. BP4D-
Spontaneous [47] contains plenty of images from 41 sub-
jects revealing a range of spontaneous expressions elicit-
ed through eight tasks. However, the database organiza-
tion were lab-controlled. AM-FED [30] is collected in real
world with sufÔ¨Åcient samples, however, without speciÔ¨Åcal
emotion labels, it‚Äôs more suited for researches on AUs. E-
motioNet [1] is a large database of one million facial expres-
sion images in the wild created by an automatic AU detec-
tion algorithm. Unlike these databases, RAF-DB simulta-
neously satisÔ¨Åes multiple requirements: sufÔ¨Åcient data, var-
ious environments, group perceiving on facial expressions
and data labels with the least noise.
2.2. The framework for expression recognition
Facial expression analysis can be generally divided into
three main parts [14]: face aquisition, facial feature extrac-
tion and facial expression classiÔ¨Åcation.
In face aquisition stage, an automatic face detector is
used to locate faces in complex scenes. Feature points are
then used to crop and align faces into a uniÔ¨Åed template
by geometric transformations. For facial feature extrac-
tion, previous methods can be generally categorized into t-
wo groups: Appearance-based methods [29] and AU-based
methods [42]. The former uses common feature extraction
methods such as LBP [38], Haar [44]. The latter recog-
nizes expression by detecting AUs. Feature classiÔ¨Åcation is
performed in the last stage. The commonly used methods
include SVM, nearest neighbor, LDA, DBN and decision-
level fusion on these classiÔ¨Åers [46]. The extracted facial
2585
expression information is either classiÔ¨Åed as a set of facial
actions or a particular basic emotion [34]. Most focus onthe latter and is based on Ekman‚Äôs theory of six basic emo-tions [12]. Indeed, without making additional assumptionsabout how to determine what action units constitute an ex-pression, there can be no exact deÔ¨Ånition for the expressioncategory. The basic emotional expressions is therefore notuniversal enough to generalize expressions displayed on hu-man face [37].
2.3. Deep learning for expression recognition
Recently, deep learning algorithms have been applied
to visual object recognition, face veriÔ¨Åcation and detec-tion, image classiÔ¨Åcation and many other problems, whichachieve state-of-the-art results. So far, there have been afew deep neural networks used in facial expression recogni-tion due to the lack of sufÔ¨Åcient training samples. In ICM-L 2013 competition [16], the winner [41] was based onDeep Convolutional Neural Network (DCNN) plus SVM.In EmotiW 2013 competition [6], the winner [19] combinedmodality speciÔ¨Åc deep neural network models. In EmotiW2015 [8], more competitors have tried deep learning meth-ods: transfer learning was used to solve the problem ofsmall database in [32], hierarchical committee of multi-column DCNNs in [20] gained the best result on SFEWdatabase, LBP features combined with DCNNs structurewere proposed in [22]. In [24], AU-aware Deep Networks(AUDN) was proposed to learn features with the interpreta-tion of facial AUs. In [31], a DCNN with inception layerswas proposed to gain comparable results.
3. Real-world Expression Database: RAF-DB
3.1. Creating RAF-DB
Data collection. At the very beginning, the images‚Äô
URLs collected from Flickr were fed into an automat-ic open-source downloader to download images in batch-es. Considering that the results returned by Flickr‚Äôs im-age search API were in well-structured XML format, fromwhich the URLs can be easily parsed, we then used a setof keywords (for example: smile, giggle, cry, rage, scared,frightened, terriÔ¨Åed, shocked, astonished, disgust, expres-sionless) to pick out images that were related with the sixbasic emotions plus the neutral emotion. At last, a to-tal of 29672 real-world facial images are presented in ourdatabase. Figure 2 shows the pipeline of data collection.
Database annotation. Annotating nearly 30000 images
of expression is an extremely difÔ¨Åcult and time-consumingtask. Considering the compounded property of real-worldexpressions, multiple views of images‚Äô expression stateshould be collected from different labelers. We thereforeemployed 315 annotators (students and staffs from univer-sities) who have been instructed with one-hour tutorial of
Figure 2. Overview of construction and annotation of RAF-DB.
psychological knowledge on emotion for an online facialexpression annotation assignment, where they were askedto classify the image into the most apparent one from sevenclasses. We developed a website for RAF-DB annotation,which shows each image with exclusive attribute options.Images were randomly and equally assigned to each label-er, ensuring that there were no direct correlation among theimages labeled by one person. And each image was assuredto be labeled by about 40 independent labelers. After that,a multi-label annotation result is obtained for each image,i.e., a seven dimensional vector that each dimension indi-cates the votes of relevant emotion.
Metadata. The data is provided with precise locations
and size of the face region, as well as the manually locatedÔ¨Åve landmark points (the central of two eyes, the tips of thenose and two corners of the mouth) on the face. Besides, anautomatic landmark annotation mode without manual labelis included: 37 landmarks were picked out from the annota-tion results provided by Face++ API [18]. We also manual-ly annotated the basic attributes (gender, age (5 ranges) andrace) of all RAF faces. In summary, subjects in our databaserange in age from 0 to 70 years old. They are 52% female,43% male, and 5% remains unsure. For racial distribution,there are 77% Caucasian, 8% African-American, and 15%Asian. The pose of each image, including pitch, yaw androll parameters, is computed from the manually labeled lo-cations of the Ô¨Åve facial landmarks.
Reliability estimation. Due to subjectivity and varied
expertise of labelers and wide ranging levels of images‚Äô d-ifÔ¨Åculty, there were some disagreements among annotators.To get rid of noisy labels, motivated by [45], a ExpectationMaximization (EM) framework was used to assess each la-beler‚Äôs reliability.
LetD={(x
j,yj,t1
j,t2j,...,tRj)}nj=1denote a set of n la-
beled inputs, where yjis the gold standard label (hidden
variable) for the jth samplesxj,tij‚àà{1,2,3,4,5,6,7}is
the corresponding label given by the ithannotator. The cor-
rect probability of tijare formulated as a sigmoid function:
p(tij=yj|Œ±i,Œ≤j)=( 1 + e x p ( ‚àíŒ±iŒ≤j))‚àí1, where 1/Œ≤ jis
the difÔ¨Åculty of the jth images,Œ±iis the reliability of ith
annotators.
Our goal is to optimize the log-likelihood of the given
2586
/g44/g258/g393/g393/g349/g367/g455/;#23#23#23/g94/g437/g396/g393/g396/g349/g400/g286/g282
/g1010/g1013/g1011/;#23#23#23/g894/g1005/g1011/g856/g1009 œµ/g1081/g895
/g44/g258/g393/g393/g349/g367/g455/;#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g349/g400/g336/g437/g400/g410/g286/g282
/g1006/g1010/g1010/;#23#23#23/g894/g1010/g856/g1011/g1005/g1081/g895
/g94/g258/g282/g367/g455/;#23#23#23/g38/g286/g258/g396/g296/g437/g367
/g1005/g1006/g1013/;#23#23#23/g894/g1007/g856/g1006œ≤/g1081/g895
/g94/g258/g282/g367/g455/;#23#23#23/g258/g374/g336/g396/g455
/g1005/g1010œØ/;#23#23#23/g894/g1008/g856/g1005œ≠/g1081/g895
/g94/g258/g282/g367/g455/;#23#23#23/g94/g437/g396/g393/g396/g349/g400/g286/g282
/g1012/g1010/;#23#23#23/g894/g1006/g856/g1005/g1011/g1081/g895
/g94/g258/g282/g367/g455/;#23#23#23/g282/g349/g400/g336/g437/g400/g410/g286/g282
/g1011/g1007/g1012/;#23#23#23/g894/g1005/g1012/g856/g1010 œØ/g1081/g895
/g38/g286/g258/g396/g296/g437/g367/g367/g455/;#23#23#23/g258/g374/g336/g396/g455
/g1005/g1009/g1004/;#23#23#23/g894/g1007/g856/g1011œµ/g1081/g895
/g38/g286/g258/g396/g296/g437/g367/g367/g455/;#23#23#23/g400/g437/g396/g393/g396/g349/g400/g286/g282 
/g1009/g1010œ¨/;#23#23#23/g894/g1005/g1008/g856/g1005œØ/g1081/g895
/g38/g286/g258/g396/g296/g437/g367/g367/g455/;#23#23#23/g282/g349/g400/g336/g437/g400/g410/g286/g282
/g1012/;#23#23#23/g894/g1004/g856/g1006/g1004/g1081/g895
/;#23#23#23#23/g374/g336/g396/g349/g367/g455/;#23#23#23/g400/g437/g396/g393/g396/g349/g400/g286/g282
/g1005/g1011/g1010/;#23#23#23/g894/g1008/g856/g1008/g1008/g1081/g895
/;#23#23#23#23/g374/g336/g396/g349/g367/g455/;#23#23#23/g282/g349/g400/g336/g437/g400/g410/g286/g282
/g1012/g1008œ≠/g894/g1006/g1005/g856/g1006œØ/g1081/g895
/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g349/g400/g336/g437/g400/g410/g286/g282/g367/g455/;#23#23#23/g400/g437/g396/g393/g396/g349/g400/g286/g282 
/g1005œ∞œ¥/g894/g1007/g856/g1011œ∞/g1081/g895/g94/g437/g396/g393/g396/g349/g400/g286/g282
/g1005/g1010/g1005/g1013/;#23#23#23/g894/g1005/g1004/g856/g1009/g1009/g1081/g895
/g38/g286/g258/g396/g296/g437/g367
/g1007/g1009/g1009/;#23#23#23/g894/g1006/g856/g1007/g1005/g1081/g895
/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g349/g400/g336/g437/g400/g410/g286/g282
/g1012/g1011/g1011/;#23#23#23/g894/g1009/g856/g1011/g1006/g1081/g895
/g44/g258/g393/g393/g455
/g1009/g1013/g1009/g1011/;#23#23#23/g894/g1007/g1012/g856/g1012/g1008/g1081/g895
/g94/g258/g282
/g1006/g1008/g1010/g1004/;#23#23#23/g894/g1005/g1010/g856/g1004/g1008/g1081/g895
/;#23#23#23#23/g374/g336/g396/g455
/g1012/g1010/g1011/;#23#23#23/g894/g1009/g856/g1010/g1009/g1081/g895
Figure 3. Examples of six-class basic emotions and twelve-class compound emotions from RAF-DB. Detailed data distribution of RAF-DB
has been attached to each expression classes.
labels:
max
Œ≤>0l(Œ±,Œ≤)=/summationdisplay
jlnp(t|Œ±,Œ≤)=/summationdisplay
jln/summationdisplay
yp(t,y|Œ±,Œ≤)
=/summationdisplay
jln/summationdisplay
yQj(y)p(t,y|Œ±,Œ≤)
Qj(y)
‚â•/summationdisplay
j/summationdisplay
yQj(y)l np(t,y|Œ±,Œ≤)
Qj(y)
whereQj(y)is a certain distribution of hidden variable y,
Qj(yj)=p(tj,yj|Œ±,Œ≤)
/summationtext
yp(tj,yj|Œ±,Œ≤)=p(tj,yj|Œ±,Œ≤)
p(tj|Œ±,Œ≤)=p(yj|tj,Œ±,Œ≤ )
After revision, 285 annotators‚Äô labels have been remained
and Cronbach‚Äôs Alpha score of all labels is 0.966.
Subset Partitions. LetGj={g1,g2,...,g 7}denotes
the 7-dimensional ground truth of the jth image, where
gk=R/summationtext
i=1Œ±i1ti
j=k(Œ±imeans the ith annotators reliabili-
ty.1Ais an indicator function that evaluates to ‚Äú1‚Äù if the
Boolean expression A is true and ‚Äú0‚Äù otherwise.), and label
k‚àà{1,2,3,4,5,6,7}refer to surprise, fear, disgust, hap-
piness, sadness, anger and neutral, respectively. We then
divided RAF-DB into different subsets according to the 7-
dimensional ground truth. For Single-label Subset, we Ô¨Årst
calculated the mean distribution value gmean =7/summationtext
k=1gk/7
for each image, then picked out label k w.r.t. gk>g mean
as the valid label. Images who have single valid label are
classiÔ¨Åed into Single-label Subset. For Two-tab Subset, the
partition rule is similar. The only difference is that we took
out images with neutral label before partition. Figure 3 ex-
hibits speciÔ¨Åc samples of 6-class basic emotions and 12-
class compound emotions.
3.2. CK+ and RAF Cross-Database Study
We then conducted a CK+ [26] and RAF cross-database
study to explore the speciÔ¨Åc difference between expression-Algorithm 1 Label reliability estimation algorithm.
Input: Training set D={(xj,t1
j,t2
j,...,tR
j)}n
j=1
Output: Each annotator‚Äôs reliability Œ±‚àó
i
Initialize:
‚àÄj=1,...,n , initialize the true label yjusing majority voting
Œ≤j:=‚àíR/summationtext
i=1p(ti
j)l np(ti
j),Œ±i:= 1 ,
The initial value of Œ≤jis image j‚Äôs entropy. The higher the en-
tropy, the more uncertain the image.
Repeat:
E-step:
Qj(yj): =/productdisplay
ip(yj|tj,Œ±i,Œ≤j)
M-step:
Œ±i:= arg max
Œ±i/summationdisplay
j/summationdisplay
yjQj(yj)l np(tj,yj|Œ±i,Œ≤j)
Qj(yj)
We also optimize Œ≤jalong with Œ±iduring M-step. However, the
goal is to get each labeler‚Äôs reliability, so we didn‚Äôt include it in
this step. For optimization, we take a derivative with respect to
Œ≤jandŒ±irespectively.
Until convergence
s of real-world affective face and the lab-controlled posed
face guided by psychologist. Here, ‚Äúcross-database‚Äù mean-
s we use all of the images from one database for training
and the images from the other for testing. In order to elimi-
nate the bias caused by different training size, the single-tab
subset of RAF-DB has been sub-sampled for experiment to
balance the size of two databases.
To ensure the generalization capabilities of the classiÔ¨Åer-
s, we applied support vector machine for classiÔ¨Åcation and
tried HOG descriptor [5] for representation. SpeciÔ¨Åcally,
original images were Ô¨Årst aligned to the size of 100 √ó100.
Then, we got a 4000-dimensional HOG feature vector per
aligned image. Finally, SVM with RBF kernel implemented
2587
98.2
36.5
0.3
0.0
7.4
11.80.0
16.4
0.0
0.0
0.8
5.20.3
24.3
76.4
0.2
30.2
61.61.0
6.3
6.8
99.7
0.0
0.60.3
3.9
4.4
0.1
61.2
1.60.1
12.5
12.1
0.0
0.4
19.2
Sur Fea Dis Hap Sad AngSur
Fea
Dis
Hap
Sad
Ang
(a) RAF‚àí ‚ÜíCK+73.7
20.5
10.1
5.8
4.6
1.74.5
14.9
4.2
4.6
11.5
0.03.3
11.8
56.0
20.6
28.1
47.80.3
22.0
0.5
62.7
2.8
0.78.0
30.5
6.5
0.4
18.9
42.810.1
0.3
22.7
6.0
34.1
7.0
Sur Fea Dis Hap Sad AngSur
Fea
Dis
Hap
Sad
Ang
(b) CK+ ‚àí ‚ÜíRAF
Figure 4. Confusion matrixes for cross-database experiments using
HOG features. The true labels (training data) are on the vertical
axis, the predicted labels (test data) are on the horizontal axis.
by LibSVM [4] was applied for classiÔ¨Åcation. Parameters
were optimized using grid search.
We then performed a cross-database experiment based
on six-class expression. Multiclass support vector machine
(mSVM) and confusion matrix were used as the classiÔ¨Åca-
tion method and the assessment criteria respectively. Fig-
ure 4 shows the results of this experiment.
Analyzing the diagonal of these two matrixes, we can
see that surprise, happiness and disgust are the top three
that have the highest recognition rates in both cases. This
result is in line with many single database tests based on
CK+, such as [26], [35] and [38]. After calculating the
average of the diagonals, Matrix I was detected with 62%
accuracy while Matrix II with only 39%, which indicates
that data collected from real world is more multiple and ef-
fective than lab-controlled one. This is particularly evident
in the expression of sadness, then happiness and surprise.
Besides, anger and disgust are usually confused with each
other in both cases, which conforms to the survey in [2].
In order to explain the phenomena above, a more de-
tailed research must be conducted to Ô¨Ånd out the speciÔ¨Åcal
differences of each expression between these two databas-
es. Therefore, a facial action coding system (FACS) anal-
ysis has been employed. FACS was Ô¨Årst presented in [12],
where the changes on facial behaviors are described by a
set of action units (AUs). AUs of sub-sampled images in
RAF-DB were Ô¨Årst labeled by our FACS coders. We then
quantitatively analyzed the AU presence for different emo-
tions in CK+ and RAF. Some examples from CK+ and RAF
are shown in Figure 5. Besides, probabilities of AUs‚Äô oc-
currence for each expression from sub-sampled images in
RAF-DB have been shown in Table 1.
4. Deep Locality-Preserving Feature Learning
Besides the ‚Äúin-the-wild‚Äù difÔ¨Åculties such as variable
lighting, poses and occlusions, real-world affective faces
at least pose two challenges that demand new algorithm-
/;#23#23#23#23/g104/g1006/g1008/;#23#23#23#23/g104/g1008
/;#23#23#23#23/g104/g1011
/;#23#23#23#23/g104/g1005/g1011
/;#23#23#23#23/g104/g1005/g853/g1008
/;#23#23#23#23/g104/g1011
/;#23#23#23#23/g104/g1006/g1004/g853
/;#23#23#23#23/g104/g1006/g1009
/;#23#23#23#23/g104/g1010
/;#23#23#23#23/g104/g1005/g1006
/;#23#23#23#23/g104/g1006/g1009
/;#23#23#23#23/g104/g1013
/;#23#23#23#23/g104/g1005/g1011/;#23#23#23#23/g104/g1008
/;#23#23#23#23/g104/g1011
/;#23#23#23#23/g104/g1005/g1009
/;#23#23#23#23/g104/g1005/g1011/;#23#23#23#23/g104/g1005/g853/g1008
/;#23#23#23#23/g104/g1005/g853/g1006
/;#23#23#23#23/g104/g1009
/;#23#23#23#23/g104/g1006/g1009/g853
/;#23#23#23#23/g104/g1006/g1011
/;#23#23#23#23/g104/g1005/g853/g1006
/;#23#23#23#23/g104/g1009
/;#23#23#23#23/g104/g1005/g853/g1006
/;#23#23#23#23/g104/g1009
/;#23#23#23#23/g104/g1006/g1009
 /;#23#23#23#23/g104/g1006/g1009/g853
/;#23#23#23#23/g104/g1006/g1010/;#23#23#23#23/g104/g1005/g853/g1006
 /;#23#23#23#23/g104/g1005/g853/g1006
/;#23#23#23#23/g104/g1006/g1010
/;#23#23#23#23/g104/g1008
/;#23#23#23#23/g104/g1009/;#23#23#23#23/g104/g1005/g853/g1008
/;#23#23#23#23/g104/g1011
/;#23#23#23#23/g104/g1006/g1010
/;#23#23#23#23/g104/g1006/g1011/;#23#23#23#23/g104/g1005/g853/g1008/;#23#23#23#23/g104/g1005
/;#23#23#23#23/g104/g1009
/;#23#23#23#23/g104/g1006/g1010
/;#23#23#23#23/g104/g1006/g1011/;#23#23#23#23/g104/g1006/g1004/g853
/;#23#23#23#23/g104/g1006/g1009
/;#23#23#23#23/g104/g1005/g1006/;#23#23#23#23/g104/g1010
/;#23#23#23#23/g104/g1006/g1009/;#23#23#23#23/g104/g1005/g1006/;#23#23#23#23/g104/g1010
/;#23#23#23#23/g104/g1006/g1010
 /;#23#23#23#23/g104/g1006/g1010/;#23#23#23#23/g104/g1005/g1006/;#23#23#23#23/g104/g1010
/;#23#23#23#23/g104/g1005/g1006/;#23#23#23#23/g104/g1010
/;#23#23#23#23/g104/g1005/g1011
/;#23#23#23#23/g104/g1008
/;#23#23#23#23/g104/g1011
/;#23#23#23#23/g104/g1006/g1008
/;#23#23#23#23/g104/g1009
/;#23#23#23#23/g104/g1005/g1004
/;#23#23#23#23/g104/g1006/g1009
/;#23#23#23#23/g104/g1006/g1010/;#23#23#23#23/g104/g1006/g1009
/;#23#23#23#23/g104/g1013/;#23#23#23#23/g104/g1011
/;#23#23#23#23/g104/g1005/g1004
/;#23#23#23#23/g104/g1006/g1010
/;#23#23#23#23/g104/g1006/g1011/;#23#23#23#23/g104/g1008
/;#23#23#23#23/g104/g1011
/;#23#23#23#23/g104/g1005/g853/g1008
/;#23#23#23#23/g104/g1005/g1009
/;#23#23#23#23/g104/g1005/g1009
/;#23#23#23#23/g104/g1005/g1011
/;#23#23#23#23/g104/g1011
/;#23#23#23#23/g104/g1006/g1009
/;#23#23#23#23/g104/g1005/g1011
/;#23#23#23#23/g104/g1005/g853/g1008
/;#23#23#23#23/g104/g1006/g1009/;#23#23#23#23/g104/g1005/g1004
/;#23#23#23#23/g104/g1008
/;#23#23#23#23/g104/g1011
/;#23#23#23#23/g104/g1013
/;#23#23#23#23/g104/g1005/g1011
/;#23#23#23#23/g104/g1009
/;#23#23#23#23/g104/g1005/g1004
/;#23#23#23#23/g104/g1008
/;#23#23#23#23/g104/g1009
/;#23#23#23#23/g104/g1005/g1004
/g90/;#23#23#23#23/g38 /;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g60/g1085/g94/g437/g396/g393/g396/g349/g400/g286
/g94/g258/g282/g374/g286/g400/g400/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g349/g400/g336/g437/g400/g410/;#23#23#23#23/g374/g336/g286/g396/g38/g286/g258/g396/g58/g381/g455
/;#23#23#23#23/g104/g1006/g1004/g853
/;#23#23#23#23/g104/g1006/g1009/;#23#23#23#23/g104/g1011
Figure 5. Comparison of six basic emotions from CK+ and RAF.
It‚Äôs evident that expression AUs in RAF are more diverse than
those in CK+.
Table 1. Probabilities of AUs‚Äô occurrence for each expression in
RAF-DB
(%) AU1 AU2 AU4 AU5 AU6 AU7 AU9 AU10 AU12 AU15 AU 17 AU20 AU25 AU 26 AU27
Sur 97 97 84 98 53‚àó
Fea 78 42 74 79 50 30‚àó61‚àó43‚àó
Dis 51 34‚àó89‚àó82 26 55‚àó
Hap 98 85 97 23
Sad 88 84 21‚àó54 49‚àó
Ang 9672‚àó94 36 87 79‚àó72‚àó
The empty data indicates the probability is less than 10%
An asterisk(*) indicates the AU‚Äôs probability is quite different from CK+‚Äôs (at least 40% disparity).
s to address. First, as indicated by our cross-database s-
tudy, real world expression may associate with various AU
combinations that require classiÔ¨Åcation algorithms to model
the multi-modality distribution of each emotion in the fea-
ture space. Second, as suggested by our crowdsourcing re-
sults, a large amount of real-world affective faces express
compound, or even multiple emotions. So traditional hand-
engineered representations which perform well on the lab-
controlled databases are no longer suitable for expression
recognition tasks in the wild.
Nowadays, DCNN has been proved to outperform hand-
crafted features on lager-scale visual recognition tasks. N-
evertheless, conventional DCNN uses only the softmax loss
layer to supervise the training process. The softmax layer
helps keeping the deeply learned features of different class-
es separable, however, still remains serious intra-class vari-
ation. On the contrary, facial expressions in real world show
signiÔ¨Åcant intra-class difference on account of varied occlu-
sions, illuminations, resolutions and head positions. What‚Äôs
more, individual variation can also lead to big difference for
the same category expression, for example, laugh v.s. smile.
Hence, we proposed a novel DLP-CNN to address the am-
biguity and multi-modality of real-world facial expression-
s. In DLP-CNN, we added a new supervised layer on the
fundamental architecture shown in Table 2, namely locali-
ty preserving loss (LP loss), to improve the discrimination
ability of the deep features.
The basic idea is to preserve the locality of each sample
xiand make the local neighborhoods within each class as
2588
Table 2. The conÔ¨Åguration parameters in the fundamental architecture (baseDCNN).
Layer
Type1 2 3 4 5 6 7 8 91 01 11 2 1 3 1 4 1 5 1 6 1 7 1 8
Conv ReLu MPool Conv ReLu MPool Conv ReLu Conv ReLu MPool Conv ReLu Conv ReLu FC ReLu FC
Kernel 3 - 2 3 - 2 3 - 3 - 2 3 - 3 - -
output 64 - - 96 - - 128 - 128 - - 256 - 256 - 2000 - 7
Stride 1 1 2 1 1 2 1111 2 1111 1
P a d 10 0 10 0 1010 0 1010 0
compact as possible. To formulate our goal:
min
W/summationdisplay
i,jSij||xi‚àíxj||2
2 (1)
whereWis the network parameters, and the matrix Sis a
similarity matrix. The deep feature x‚ààRddenotes Deep
Convolutional activation features (DeCaf) [9] taken from
the Ô¨Ånal hidden layer, i.e., just before the softmax layer that
produces the class prediction. A possible way of deÔ¨Åning S
is as follows.
Sij=‚éß
‚é®
‚é©1,xjis among knearest neighbors of xi
orxiis among knearest neighbors of xj
0,otherwise(2)
wherexiandxjbelong to the same class of expression, k
deÔ¨Ånes the size of the local neighborhood.
This formulation effectively characterizes the intra-class
local scatters. Note that xishould be updated as the itera-
tive optimization of the CNN. To compute the summation of
the pairwise distance, we need to take the entire training set
in each iteration, which is inefÔ¨Åcient to implement. To ad-
dress this difÔ¨Åculty, we do the approximation by searching
theknearest neighbors for each sample xi, and the locality
preserving loss function of xiis deÔ¨Åned as follow:
Llp=1
2n/summationdisplay
i=1||xi‚àí1
k/summationdisplay
x‚ààNk{xi}x||2
2 (3)
whereNk{xi}denotes the ensemble of the knearest neigh-
bors of sample xiwith the same class.
The gradients of Llpwith respect to xiis computed as:
‚àÇLlp
‚àÇxi=xi‚àí1
k/summationdisplay
x‚ààNk{xi}x (4)
In this manner, we can perform the update based on mini-
batch. Note that, the recently proposed center loss [43] can
be considered as a special case of the locality preserving
loss, ifk=nc‚àí1(ncis the number of the training samples
in class c to which xibelong). While center loss simply
pulls the samples to a single centroid, the proposed locality
preserving loss is more Ô¨Çexible especially when the class
conditional distribution is multi-modal.
We then adopt the joint supervision of softmax loss
which characterizes the global scatter and the locality p-
reserving loss which characterizes the local scatters within
class, to train the CNNs for discriminative feature learning.The objective function is formulated as follow: L=
Ls+ŒªLlp, whereLsdenotes the softmax loss and Llpde-
notes the locality preserving loss. The hyper parameter Œªis
used to balance the two loss functions. Algorithm 2 sum-
marizes the learning process in the deep locality preserving
CNN.
Algorithm 2 Optimization algorithm of DLP-CNN.
Input: Training data {xi}n
i=1,
n is the size of mini-batch
Output: Network layer parameters W
Initialize: t=0
Network learning rate Œº, hyper parameter Œª, Network layer pa-
rameters W, softmax loss parameters Œ∏, neighboring nodes k.
Repeat:
1:t=t+1
2: Computer the center of k-nearest neighbor for xi:
Ct
i=1
k/summationtextn
j=1xt
jSt
ij
3: Update the softmax loss parameters:
Œ∏t+1=Œ∏t‚àíŒºt‚àÇLt
s
‚àÇŒ∏t
4: Update the backpropagation error:
‚àÇLt
‚àÇxt
i=‚àÇLt
s
‚àÇxt
i+Œª‚àÇLt
lp
‚àÇxt
i5: Computer the network layer parameters:
Wt+1=Wt‚àíŒºt‚àÇLt
‚àÇWt=Wt‚àíŒºt/summationtextn
i=1‚àÇLt
‚àÇxt
i‚àÇxt
i
‚àÇWt
Until convergence
5. Baseline System
To facilitate translating the research from laboratory en-
vironments to the real world, we performed two challenging
benchmark experiments on RAF-DB: 7-class basic expres-
sion classiÔ¨Åcation and 11-class compound expression clas-
siÔ¨Åcation, and presented afÔ¨Åliated baseline algorithms and
performances. We also conducted comparative experiments
on two small and popular datasets, CK+ and JAFFE [28].
We followed up the experimental setup in cross-database
experiments, and tried LBP [33], HOG [5] and Gabor [23]
features. The LBP descriptor applied the 59-bin LBPu2
8,2
operator, and then concatenated the histograms from 10 √ó10
pixel cells, generating a 5,900 dimensional feature vector.
The HOG feature used this shape-based segmentation di-
viding the image into 10 √ó10 pixel blocks of four 5 √ó5 pixel
cells with no overlapping. By setting 10 bins for each his-
tograms, we extract a 4000-dimensional HOG feature vec-
tor for each image. For Gabor wavelet, we used a bank of
2589
Table 3. Basic expression class performance comparison of CK+,
JAFFE and RAF along with Compound expression performance
of RAF, based on LBP , HOG and Gabor descriptors, and SVM,
LDA+kNN classiÔ¨Åcation. The metric is the mean diagonal value
of the confusion matrix.
basic compound
CK+ JAFFE RAF RAF
mSVMLBP 88.92 78.81 55.98 28.84
HOG 90.50 84.76 58.45 33.65
Gabor 91.98 88.95 65.12 35.76
LDALBP 85.84 77.74 50.97 22.89
HOG 91.77 80.12 51.36 24.01
Gabor 92.33 83.45 56.93 23.81
40 Gabor Ô¨Ålters at Ô¨Åve spatial scales and eight orientation-
s. The downsample image‚Äôs size was set to 10*10, yielding
4000-dimensional features.
In order to objectively measure the performance for the
followers entries, we split the dataset into a training set
and a test set with the idea of Ô¨Åve-fold cross-validation,
which means the size of training set is Ô¨Åve times larger than
test set, and expressions in both sets have a near-identical
distribution. Considering expressions in the wild have
imbalanced distribution, the accuracy metric which is
especially sensitive to bias and no longer effective for
imbalanced data [15], is no longer used in RAF. Instead,
we use the mean diagonal value of the confusion matrix as
the ultima metric.
Basic emotions . In this experiment, seven basic
emotion classes were detected using the whole 15339
images from the single-label subset. The best classiÔ¨Åcation
accuracy (output by SVM) was 72.71% for LBP , 74.35%
for HOG, and 77.28% for Gabor. Results declined to
55.98%, 58.45% and 65.12% respectively when using the
mean diagonal value of the confusion matrix as metric. To
assess the reliability of the basic emotion labels, we also
assigned a uniform random label to each sample, which
we call a naive emotion detector. And the best result for
the naive classiÔ¨Åer was 16.07% when using Gabor feature,
which is much lower than the former value.
For comparison, we employed the same methods on
CK+ with person-independent 5-fold cross-validation and
JAFFE with leave-one-subject-out strategy. The results
shown in Table 3 certify that expressions in real world
are more difÔ¨Åcult for recognition and the current common
methods which perform well on the existing databases
cannot solve the expression recognition problem in the
challenging real-world condition.
To evaluate effectiveness of different classiÔ¨Åers, we have
also trained LDA with nearest neighbor (NN) classiÔ¨Åcation.
We found that LDA+NN were inferior to mSVM obviouslywhen training on RAF, a extremely large database. Nev-
ertheless, it performed better when training on small-scale
datasets (CK+ and JAFFE), even outperformed mSVM in
some cases. Concrete results can be viewed in Table 3.
Compound emotions. For compound emotions clas-
siÔ¨Åcation, we got rid of fearfully disgusted emotion as it‚Äôs
too few, leaving 11 classes of compound emotion, 3954
in total. The best classiÔ¨Åcation accuracy (output by SVM)
was 45.51% for LBP , 51.89% for HOG, and 53.54% for
Gabor. Results declined to 28.84%, 33.65% and 35.76%
respectively when using the mean diagonal value of the
confusion matrix as metric. Again, to demonstrate the
reliability of the compound emotion labels, we computed
the baseline for the naive emotion detector, which declined
to 5.79% when using Gabor feature.
As expected, the overall performance dropped sig-
niÔ¨Åcantly when more expressions are involved for
classiÔ¨Åcation. The signiÔ¨Åcantly lower results compared to
that of basic emotions indicate that compound emotions
are more difÔ¨Åcult to detect and new methods should be
invented to solve this problem. Besides the multi-modality,
lack of training samples of compound expressions from
real world is another great technical challenge.
6. Deep Learning System
Nowadays, deep learning has been applied to lager-scale
visual recognition tasks and perform exceedingly well with
lager amounts of training data. However, fully-supervised
deep models are easy to be overÔ¨Åtting on facial expression
recognition task due to the insufÔ¨Åcient training samples for
the model learning. Therefore, most deep learning frame-
works employed on facial expression recognition [22, 32,
36] are base on pre-trained models. These pre-trained mod-
els, such as VGG network [40] and AlexNet [21], are ini-
tially designed for face recognition, which are short of dis-
crimination ability of expression characteristic. So in this
paper, we directly trained our deep learning system on the
big enough self-collected database RAF from scratch, with-
out using other databases.
When conducting experiments, we followed the same
dataset partition standards, image processing methods and
classiÔ¨Åcation methods as in the baseline system. Related
researches [9, 39] have proved that well-trained deep con-
volutional network can work as a feature extraction tool
with generalization ability for the classiÔ¨Åcation task. Fol-
lowing up this idea, we Ô¨Årst trained each DCNNs for basic
emotion recognition task, and then directly used the already
trained DCNN models to extract deep features for both ba-
sic and compound expressions. 2000-dimensional deep fea-
tures learnt from raw data were extracted from the penulti-
mate fully connected layer of the DCNNs and then classi-
Ô¨Åed by SVM.
2590
Table 4. Expression recognition performance of different DCNNs on RAF. The metric is the mean diagonal value of the confusion matrix.
basic compound
Anger Disgust Fear Happiness Sadness Surprise Neutral Average Average
mSVMVGG 68.52 27.50 35.13 85.32 64.85 66.32 59.88 58.22 31.63
AlexNet 58.64 21.87 39.19 86.16 60.88 62.31 60.15 55.60 28.22
baseDCNN 70.99 52.50 50.00 92.91 77.82 79.64 83.09 72.42 40.17
center loss 68.52 53.13 54.05 93.08 78.45 79.63 83.24 72.87 39.97
DLP-CNN 71.60 52.15 62.16 92.83 80.13 81.16 80.29 74.20 44.55
LDAVGG 66.05 25.00 37.84 73.08 51.46 53.49 47.21 50.59 16.27
AlexNet 43.83 27.50 37.84 75.78 39.33 61.70 48.53 47.79 15.56
baseDCNN 66.05 47.50 51.35 89.45 74.27 76.90 77.50 69.00 28.23
center loss 64.81 49.38 54.05 92.41 74.90 76.29 77.21 69.86 27.33
DLP-CNN 77.51 55.41 52.50 90.21 73.64 74.07 73.53 70.98 32.29
From the results in Table 4, we have the following obser-
vations. First, DCNNs which achieve quite reasonable re-
sults for large-scale image recognition setting, such as VGG
network and AlexNet, are not efÔ¨Åcient for facial expression
recognition. Second, all of the deep features learnt on RAF-
DB outperform the unlearned features used in the baseline
system by a signiÔ¨Åcant margin, which indicates that deep
learning architecture is more robust and applicable for both
basic and compound expression. At last, our new locali-
ty preserving loss model achieves better performance than
the based one and the center loss one. Note that, the center
loss, which efÔ¨Åciently converges unimodal class, can help
enhance the network performance on basic emotion, but it
fails on compound emotion. This shows the advantage of
the locality preserving loss on multi-modal facial expres-
sion recognition, including both basic and compound one.
To see the generalization ability of our well-trained DLP-
CNN model on other databases, we then employed it to di-
rectly extract Ô¨Åxed-length feature of CK+ and SFEW 2.0
without Ô¨Ånetune. For the lab-controlled databases CK+, we
followed the experimental principle in the baseline system.
For the real-world database SFEW 2.0, we followed the rule
in EmotiW 2015 [8], and the ‚ÄúSFEW best‚Äù is the result of
the single best model used in the winner [20] of EmotiW
2015. Note that, in [20], the Authors trained their model
with extra data from SFEW. From the comparison results
in Table 5, we can see that our network can also achieve
comparable or even better performance than other state-of-
the-art methods, not only for RAF, but also other databases.
This indicates that our proposed network can be used as an
efÔ¨Åcient and effective feature extraction tool for facial ex-
pression databases, without a signiÔ¨Åcant amount of time to
execute in traditional DCNNs.
7. Conclusions and Future Work
The main contribution of this paper is presenting a novel
optimized algorithm for crowdsourcing and a new locali-Table 5. Comparison results of DLP-CNN and other state-of-the-
art deep learning methods on CK+ and SFEW 2.0.
AUDN
[25]FP+SAE
[27][31]SFEW best
[20]DLP-CNN
(without Ô¨Ånetune)
CK+ 93.70 91.11 93.2 ‚Äì 95.78
SFEW 2.0 30.14 ‚Äì 47.7 52.5 51.05
ty preserving loss layer for deep learning, based on a real-
world publicly available facial expression database RAF-
DB. The optimized algorithm helps to keep the best anno-
tated results from labelers. The new DCNN can learn more
discriminative feature for expression recognition task. The
RAF-DB contains, 1) 29672 real-world images labeled for
different expressions, age range, gender and posture fea-
ture, 2) a 7-dimensional expression distribution vector for
each image, 3) two different subsets: single-label subset,
including seven classes of basic emotions; two-tab subset,
including twelve classes of compound emotions, 4) loca-
tions of Ô¨Åve manually accurate detect landmark points, 5)
baseline classiÔ¨Åer outputs for basic emotions and compound
emotions. We hope that the release of this database will en-
courage more researches on the effect of real-world expres-
sion distribution or detection and be a useful benchmark re-
source for researchers to compare the validity of their facial
expression analysis algorithms in challenge conditions.
8. Acknowledgments
This work was partially supported by the Na-
tional Natural Science Foundation of China (Project
6157306861471048, 61375031 and 61532006), Beijing
Nova Program under Grant No. Z161100004916088, the
Fundamental Research Funds for the Central Universities
under Grant No. 2014ZD03-01, and the Program for New
Century Excellent Talents in University(NCET-13-0683).
2591
References
[1] C. F. Benitez-Quiroz, R. Srinivasan, and A. M. Martinez. E-
motionet: An accurate, real-time algorithm for the automatic
annotation of a million facial expressions in the wild. In
Proceedings of IEEE International Conference on Comput-
er Vision & Pattern Recognition (CVPR16), Las V egas, NV ,
USA , 2016.
[2] V . Bettadapura. Face expression recognition and analysis:
the state of the art. arXiv preprint arXiv:1203.6722 , 2012.
[3] J. C. Borod. The neuropsychology of emotion . Oxford Uni-
versity Press New Y ork, 2000.
[4] C.-C. Chang and C.-J. Lin. LIBSVM: A library for sup-
port vector machines. ACM Transactions on Intelligen-
t Systems and Technology , 2:27:1‚Äì27:27, 2011. Soft-
ware available at http://www.csie.ntu.edu.tw/
Àúcjlin/libsvm .
[5] N. Dalal and B. Triggs. Histograms of oriented gradients for
human detection. In Computer Vision and Pattern Recogni-
tion, 2005. CVPR 2005. IEEE Computer Society Conference
on, volume 1, pages 886‚Äì893. IEEE, 2005.
[6] A. Dhall, R. Goecke, J. Joshi, M. Wagner, and T. Gedeon.
Emotion recognition in the wild challenge 2013. In Pro-
ceedings of the 15th ACM on International conference on
multimodal interaction , pages 509‚Äì516. ACM, 2013.
[7] A. Dhall, R. Goecke, S. Lucey, and T. Gedeon. Static fa-
cial expression analysis in tough conditions: Data, evalua-
tion protocol and benchmark. In Computer Vision Workshops
(ICCV Workshops), 2011 IEEE International Conference on ,
pages 2106‚Äì2112. IEEE, 2011.
[8] A. Dhall, O. Ramana Murthy, R. Goecke, J. Joshi, and
T. Gedeon. Video and image based emotion recognition chal-
lenges in the wild: Emotiw 2015. In Proceedings of the 2015
ACM on International Conference on Multimodal Interac-
tion , pages 423‚Äì426. ACM, 2015.
[9] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang,
E. Tzeng, and T. Darrell. Decaf: A deep convolutional acti-
vation feature for generic visual recognition. In ICML , pages
647‚Äì655, 2014.
[10] S. Du, Y . Tao, and A. M. Martinez. Compound facial expres-
sions of emotion. Proceedings of the National Academy of
Sciences , 111(15):E1454‚ÄìE1462, 2014.
[11] P . Ekman. Facial expression and emotion. American psy-
chologist , 48(4):384, 1993.
[12] P . Ekman and W. V . Friesen. Facial action coding system.
1977.
[13] P . Ekman, W. V . Friesen, M. O‚ÄôSullivan, A. Chan,
I. Diacoyanni-Tarlatzis, K. Heider, R. Krause, W. A.
LeCompte, T. Pitcairn, P . E. Ricci-Bitti, et al. Universals and
cultural differences in the judgments of facial expressions
of emotion. Journal of personality and social psychology ,
53(4):712, 1987.
[14] B. Fasel and J. Luettin. Automatic facial expression analysis:
a survey. Pattern recognition , 36(1):259‚Äì275, 2003.
[15] C. Ferri, J. Hern ¬¥andez-Orallo, and R. Modroiu. An experi-
mental comparison of performance measures for classiÔ¨Åca-
tion. Pattern Recognition Letters , 30(1):27‚Äì38, 2009.[16] I. J. Goodfellow, D. Erhan, P . L. Carrier, A. Courville,
M. Mirza, B. Hamner, W. Cukierski, Y . Tang, D. Thaler, D.-
H. Lee, et al. Challenges in representation learning: A report
on three machine learning contests. In Neural information
processing , pages 117‚Äì124. Springer, 2013.
[17] X. He and P . Niyogi. Locality preserving projections. In
NIPS , volume 16, 2003.
[18] M. Inc. Face++ research toolkit. www.faceplusplus.com,
Dec. 2013.
[19] S. E. Kahou, C. Pal, X. Bouthillier, P . Froumenty,
C¬∏.G ¬®ulc ¬∏ehre, R. Memisevic, P . Vincent, A. Courville, Y . Ben-
gio, R. C. Ferrari, et al. Combining modality speciÔ¨Åc deep
neural networks for emotion recognition in video. In Pro-
ceedings of the 15th ACM on International conference on
multimodal interaction , pages 543‚Äì550. ACM, 2013.
[20] B.-K. Kim, J. Roh, S.-Y . Dong, and S.-Y . Lee. Hierarchical
committee of deep convolutional neural networks for robust
facial expression recognition. Journal on Multimodal User
Interfaces , pages 1‚Äì17, 2016.
[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
classiÔ¨Åcation with deep convolutional neural networks. In
Advances in neural information processing systems , pages
1097‚Äì1105, 2012.
[22] G. Levi and T. Hassner. Emotion recognition in the wild via
convolutional neural networks and mapped binary pattern-
s. In Proceedings of the 2015 ACM on International Con-
ference on Multimodal Interaction , pages 503‚Äì510. ACM,
2015.
[23] C. Liu and H. Wechsler. Gabor feature based classiÔ¨Åca-
tion using the enhanced Ô¨Åsher linear discriminant model for
face recognition. Image processing, IEEE Transactions on ,
11(4):467‚Äì476, 2002.
[24] M. Liu, S. Li, S. Shan, and X. Chen. Au-aware deep net-
works for facial expression recognition. In Automatic Face
and Gesture Recognition (FG), 2013 10th IEEE Internation-
al Conference and Workshops on , pages 1‚Äì6. IEEE, 2013.
[25] M. Liu, S. Li, S. Shan, and X. Chen. Au-inspired deep net-
works for facial expression feature learning. Neurocomput-
ing, 159:126‚Äì136, 2015.
[26] P . Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and
I. Matthews. The extended cohn-kanade dataset (ck+): A
complete dataset for action unit and emotion-speciÔ¨Åed ex-
pression. In Computer Vision and Pattern Recognition Work-
shops (CVPRW), 2010 IEEE Computer Society Conference
on, pages 94‚Äì101. IEEE, 2010.
[27] Y . Lv, Z. Feng, and C. Xu. Facial expression recognition via
deep learning. In Smart Computing (SMARTCOMP), 2014
International Conference on , pages 303‚Äì308. IEEE, 2014.
[28] M. J. Lyons, S. Akamatsu, M. Kamachi, J. Gyoba, and
J. Budynek. The japanese female facial expression (jaffe)
database. 1998.
[29] M. J. Lyons, J. Budynek, and S. Akamatsu. Automatic clas-
siÔ¨Åcation of single facial images. IEEE Transactions on
Pattern Analysis & Machine Intelligence , (12):1357‚Äì1362,
1999.
[30] D. McDuff, R. El Kaliouby, T. Senechal, M. Amr, J. F. Cohn,
and R. Picard. Affectiva-mit facial expression dataset (am-
2592
fed): Naturalistic and spontaneous facial expressions collect-
ed in-the-wild. In Computer Vision and Pattern Recogni-
tion Workshops (CVPRW), 2013 IEEE Conference on , pages
881‚Äì888. IEEE, 2013.
[31] A. Mollahosseini, D. Chan, and M. H. Mahoor. Going deeper
in facial expression recognition using deep neural networks.
In2016 IEEE Winter Conference on Applications of Com-
puter Vision (WACV) , pages 1‚Äì10. IEEE, 2016.
[32] H.-W. Ng, V . D. Nguyen, V . V onikakis, and S. Winkler.
Deep learning for emotion recognition on small datasets us-
ing transfer learning. In Proceedings of the 2015 ACM on
International Conference on Multimodal Interaction , pages
443‚Äì449. ACM, 2015.
[33] T. Ojala, M. Pietik ¬®ainen, and T. M ¬®aenp ¬®a¬®a. Multiresolution
gray-scale and rotation invariant texture classiÔ¨Åcation with
local binary patterns. Pattern Analysis and Machine Intelli-
gence, IEEE Transactions on , 24(7):971‚Äì987, 2002.
[34] M. Pantic and L. J. M. Rothkrantz. Automatic analysis of
facial expressions: the state of the art. IEEE Transactions
on Pattern Analysis and Machine Intelligence , 22(12):1424‚Äì
1445, Dec 2000.
[35] M. Pard `as and A. Bonafonte. Facial animation param-
eters extraction and expression recognition using hidden
markov models. Signal Processing: Image Communication ,
17(9):675‚Äì688, 2002.
[36] X. Peng, Z. Xia, L. Li, and X. Feng. Towards facial expres-
sion recognition in the wild: A new database and deep recog-
nition system. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition Workshops , pages
93‚Äì99, 2016.
[37] J. A. Russell. Is there universal recognition of emotion from
facial expressions? a review of the cross-cultural studies.
Psychological bulletin , 115(1):102, 1994.
[38] C. Shan, S. Gong, and P . W. McOwan. Facial expression
recognition based on local binary patterns: A comprehensive
study. Image and Vision Computing , 27(6):803‚Äì816, 2009.
[39] A. Sharif Razavian, H. Azizpour, J. Sullivan, and S. Carls-
son. Cnn features off-the-shelf: an astounding baseline for
recognition. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition Workshops , pages 806‚Äì
813, 2014.
[40] K. Simonyan and A. Zisserman. V ery deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556 , 2014.
[41] Y . Tang. Deep learning using linear support vector machines.
arXiv preprint arXiv:1306.0239 , 2013.
[42] Y . I. Tian, T. Kanade, and J. F. Cohn. Recognizing action u-
nits for facial expression analysis. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence , 23(2):97‚Äì115, Feb
2001.
[43] Y . Wen, K. Zhang, Z. Li, and Y . Qiao. A discrimina-
tive feature learning approach for deep face recognition. In
European Conference on Computer Vision , pages 499‚Äì515.
Springer, 2016.
[44] J. Whitehill and C. W. Omlin. Haar features for facs au
recognition. In Automatic Face and Gesture Recognition,
2006. FGR 2006. 7th International Conference on , page 5‚Äìp-
p. IEEE, IEEE, 2006.[45] J. Whitehill, T.-f. Wu, J. Bergsma, J. R. Movellan, and P . L.
Ruvolo. Whose vote should count more: Optimal integration
of labels from labelers of unknown expertise. In Advances
in neural information processing systems , pages 2035‚Äì2043,
2009.
[46] Z. Zeng, M. Pantic, G. I. Roisman, and T. S. Huang. A sur-
vey of affect recognition methods: Audio, visual, and spon-
taneous expressions. Pattern Analysis and Machine Intelli-
gence, IEEE Transactions on , 31(1):39‚Äì58, 2009.
[47] X. Zhang, L. Yin, J. F. Cohn, S. Canavan, M. Reale,
A. Horowitz, P . Liu, and J. M. Girard. Bp4d-spontaneous:
ahigh-resolution spontaneous 3d dynamic facial expression
database. Image and Vision Computing , 32(10):692‚Äì706,
2014.
2593
"
https://ieeexplore.ieee.org/document/7124463,"2108 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. 10, NO. 10, OCTOBER 2015
Single Sample Face Recognition via Learning Deep
Supervised Autoencoders
Shenghua Gao, Yuting Zhang, Kui Jia, Jiwen Lu, Member, IEEE , and Yingying Zhang
Abstract ‚Äî This paper targets learning robust image
representation for single training sample per person facerecognition. Motivated by the success of deep learning in imagerepresentation, we propose a supervised autoencoder, which isa new type of building block for deep architectures. There aretwo features distinct our supervised autoencoder from standardautoencoder. First, we enforce the faces with variants to bemapped with the canonical face of the person, for example,frontal face with neutral expression and normal illumination;Second, we enforce features corresponding to the same person tobe similar. As a result, our supervised autoencoder extracts thefeatures which are robust to variances in illumination, expression,occlusion, and pose, and facilitates the face recognition. We stacksuch supervised autoencoders to get the deep architecture and useit for extracting features in image representation. Experimentalresults on the AR, Extended Yale B, CMU-PIE, and Multi-PIEdata sets demonstrate that by coupling with the commonly usedsparse representation-based classiÔ¨Åcation, our stacked supervisedautoencoders-based face representation signiÔ¨Åcantly outperformsthe commonly used image representations in single sampleper person face recognition, and it achieves higher recognitionaccuracy compared with other deep learning models, includingthe deep Lambertian network, in spite of much less trainingdata and without any domain information. Moreover, supervisedautoencoder can also be used for face veriÔ¨Åcation, which furtherdemonstrates its effectiveness for face representation.
Index Terms ‚Äî Single training sample per person, face
recognition, supervised auto-encoder, deep architecture.
I. I NTRODUCTION
SINGLE sample per pers on (SSPP) face recogni-
tion [1], [2]1is a very important research topic in
computer vision because of its potential applications in
many realistic scenarios like passport identiÔ¨Åcation, gate
ID identiÔ¨Åcation, video surveillance, etc.H o w e v e r ,a ss h o w n
in Fig. 1, there is only one training image (gallery image)
in SSPP, and the faces to be recognized may contain lots of
Manuscript received December 9, 2014; revised March 16, 2015; accepted
May 30, 2015. Date of publication June 16, 2015; date of current version
August 6, 2015. This work was supported by the Shanghai Pujiang Program
under Grant 15PJ1405700. The associate e ditor coordinating the review of
this manuscript and approving it for pub lication was Prof. Patrizio Campisi.
S. Gao and Y . Zhang are with ShanghaiTech University, Shanghai 200444,
China (e-mail: gaoshh@shanghaitech.edu.cn; zyt@zju.edu.cn).
K. Jia is with the University of Macau, Macau 999078, China (e-mail:
kuijia@gmail.com).
J. Lu is with Advanced Digital Sciences Center, Singapore 138632 (e-mail:
jiwen.lu@adsc.com.sg).
Y . Zhang is with Zhejiang University, Hangzhou 310027, China (e-mail:
zhangyy2@shanghaitech.edu.cn).
Color versions of one or more of the Ô¨Ågures in this paper are available
online at http://ieeexplore.ieee.org.
Digital Object IdentiÔ¨Åer 10.1109/TIFS.2015.2446438
1SSPP is one speciÔ¨Åc task of one shot learning [3]
Fig. 1. Samples of gallery images (the Ô¨Årst column) and probe images (the
rest faces) on the AR, Extended Yale B, CMU-PIE, and Multi-PIE datasets.
(a) The AR dataset. (b) The Extende d Yale B dataset. (c) The CMU-PIE
dataset. (d) The Multi-PIE dataset.
variances in, for example, illumination, expression, occlusion,
pose, etc. Therefore, SSPP face recognition is an extremely
challenging task.
Because of the presence of such challenging intra-class
variances, a robust face representation which can overcome
the effect of these variances is extremely desirable, and will
greatly facilitate SSPP face recognition. However, restricted
by having only one tr aining sample for each person, the
commonly used subspace analysis based face representationmethods, like Eigenfaces [4] and Fisherfaces [5], are no longer
suitable nor applicable to SSPP. To seek a good SSPP image
representation, lots of endeavors have been made whichachieve some good performance under certain settings. For
example, with the help of manually generated virtual faces [6]
or an external dataset [1], traditional face representation
methods can be extended to the SSPP scenario. However,
the performance of these methods is still not unsatisfactoryfor challenging real data. After representing each face
with a feature vector, a classiÔ¨Åcation technique, like Nearest
Neighbor, sparse representation based classiÔ¨Åcation (SRC) [7],can be used to predict the labels of the probe images.
Deep neural networks have demonstrated their great
successes in image representation [8], [9], and their
fundamental ingredient is the training of a nonlinear feature
extractor at each layer [10]‚Äì[12]. After the layer-wise trainingof each building block and the bu ilding of a deep architecture,
the output of the network is used for the image representation
in the subsequent task. As a typical building block in deepneural networks, Denoising Auto-Encoder [11] extracts the
1556-6013 ¬© 2015 I EEE. Personal u se is perm itted, but republication/redistri bution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
GAO et al. : SINGLE SAMPLE FACE RECOGNITION VIA LEARNING DEEP SUPERVISED AUTOENCODERS 2109
features through a deterministic nonlinear mapping, and it
is robust to the noises of the data. Image representations
based on the denoising auto-encoder have shown good
performance in many tasks, like object recognition, digitrecognition, etc. Motivated by the success of denoising
auto-encoder based deep neural networks, and driven by the
SSPP face recognition, we propose a supervised auto-encoderto build the deep neural network. We Ô¨Årst treat the faces with
all type of variants (for example, illumination, expression,
occlusion, or poses) as images contaminated by noises. With
a supervised auto-encoder, we can recover the face without
the variant, meanwhile, such a supervised auto-encoderalso extracts robust features for image representation in
SSPP scenario, i.e., the featur es corresponding to the same
person should have the same (ideally) or similar features,and such features should be stable to the intra-class variances
which commonly exist in the SSPP scenario.
The contributions of this paper are two-fold. Firstly,
we propose a new type of building block ‚Äì supervised
auto-encoder, for building the deep neural network. Differentfrom standard Auto-Encoder, on the one hand, we enforce all
the faces with variances to be mapped with the canonical face
of the person, for example, frontal face with neutral expressionand normal illumination. Such strategy helps remove the
variances in face recognition. On the other hand, by imposing
the similarity preservation constraints on the extracted
features, the supervised auto-encoder makes the features
corresponding to the same person similar, therefore it extractsmore robust features for face representation. Secondly, by
leveraging the supervised auto-encoder, robust features can be
extracted for image representation in SSPP face recognition,therefore improves the recognition accuracy of SSPP.
The rest of the paper is organized as follows: we will
review the related work, including the commonly used image
representation methods for SSPP scenario as well as the
auto-encoder and its variants in Section II. In Section III,we will introduce our supervised auto-encoder, and discuss
its application in SSPP. We will experimentally evaluate the
proposed technique and its parameters in Section IV, andconclude our work in Section V.
II. R
ELATED WORK
A. Work Related to SSPP Face Representation
Though subspace analysis based methods are usually
adopted for effective and efÔ¨Åcient face representation in
general face recognition, they are no longer suitable norapplicable to the SSPP scenario. On the one hand, because
of the limited number of gallery images and uncertainty
of the variances between probe images and gallery image,
it is not easy to estimate the data distribution and get the
proper projection matrix for unsupervised methods, like
Eigenfaces [4], 2DPCA [13], etc. To make these unsupervised
methods more suitable for SSPP , by taking advantage of
some virtual faces generated by d ifferent methods, Projection-
Combined Principal Component Analysis ((PC)
2A) [14],
Enhanced (PC)2A( E ( P C )2A) [15], etc., have been proposed.
On the other hand, to make FLDA [5] based imagerepresentation able to be used in the SSPP case where the
intra-class variance is impo ssible to be directly estimated
because only one training sample is provided for each person,
virtual samples are usually generated by using small per-turbation [16], transformation [17], SVD decomposition [6],
or subimages generated by di viding each image into small
patches [18]. Moreover, the intra-class variances can alsobe estimated from a generic dataset [1], [19], where each
subject contains images with variances in pose, expression,
illumination, occlusion, etc. Recently, with the emergence
of deep learning technique, Tang et al. propose a Deep
Lambertian Network (DLN) [20] which combines DeepBelief Nets [10] with Lambertian reÔ¨Çection assumption.
Therefore DLN extracts illumination invariant features for
face representation and it also shows good performance underSSPP setting, but it is not able to handle other variances,
like expressions, poses, occlusions, etc., which restricts its
application in real world problems.
B. Work Related to Auto-Encoder
Auto-encoder which is also termed as autoassociator or
Diabolo network, is one of the commonly used building
blocks in deep neural networks. It contains two modules.
(i) A encoder maps the input xto the hidden nodes
through some deterministic mapping function f:h=f(x).
(ii) A decoder maps the hidden nodes back to the original
input space through another deterministic mapping function
g:x
/prime=g(h). For real-valued input, by minimizing the recon-
struction error /bardblx‚àíg(f(x))/bardbl2
2, the parameters of encoder and
decoder can be learnt. Then the output of the hidden layer is
used as the feature for image representation. It has been shown
that such a nonlinear auto-encoder is different from PCA [21],
and it has been proven that ‚Äútraining an auto-encoder tominimize reconstruction error amounts to maximizing a lower
bound on the mutual information between input and the
learnt representation‚Äù [11] . To further boos t the ability of
auto-encoder for image representation in building deep
networks, Vincent et al. [11] propose a denoising auto-encoder
which enhances its generalization by training with locally
corrupted inputs. Rafai et al. enhance the robustness of
auto-encoder to noises by adding the Jacobian [22],or Jacobian and Hessian at the same time [23], into the
objective of basic auto-encoder. Zou et al. [24] use the pooling
operation after the Reconstr uction Independent Component
Analysis [25] encoding process, and enforce the pooled
features to be similar for instances with the same class label.
These extensions improve the performance of auto-encoder
based neural networks for image representation in object and
digit recognition.
C. Deep Learning Based Face VeriÔ¨Åcation Systems
In [26], a Siamese Networks is proposed for face veriÔ¨Åcation
and such SN is based on Convolutional Neural Network
in [26]. In the experiment, we have tried different settingsand use the one with the bes t performance. As the SN is
proposed for face veriÔ¨Åcation, to do the face recognition,
we run face veriÔ¨Åcation over all pairs of faces in the
2110 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. 10, NO. 10, OCTOBER 2015
test gallery set and choose the pair that‚Äôs most similar.
In this way, we can predict the label of the probe. Recently,
Taigman et al. also propose a Convolutional Neural Networks
based face veriÔ¨Åcation system [27], which signiÔ¨Åcantlyoutperforms the existing hand-crafted features based systems
on the Labeled Face in the Wild (LFW) database. Similarly,
Fan et al. also use another variant of convolutional neural
network which is termed as Pyramid CNN, for face
veriÔ¨Åcation, and also achieve similar performance on the
LFW database. But these works are proposed for face
veriÔ¨Åcation, where a pair of f aces are given and the algorithm
identiÔ¨Åes whether the two faces belonging to the same personor not (a random guess is 50%). Our task solves face recogni-
tion in which, given a test image , it tries to identify precisely
the correct person among many. Random guess gives oneover the number of subjects. Besides the tasks to be solved
are different, the network architecture of our paper is also
different with these all existing works. Moreover, our work
is also closely related to the work of Zhu et al. [28], [29].
In [28], a supervised training is deployed to train a networkconsisting of encoding layer and pooling layer. In [29],
multiple CNN networks trained on different regions of face
and a regression layer are trained to solve the face veriÔ¨Åcationtask. Similar to these work, our work also makes faces of
the same person be represented similarly, but we are based
on different architectures. Moreover, different from [28], [29],
features learnt by our method is not designed for some speciÔ¨Åc
task. As shown later, our model can also be used for facerecognition or face veriÔ¨Åcation.
III. S
UPERVISED AUTO-ENCODER FOR
SSPP F ACE REPRESENTATION
The motivation of our supervised auto-encoder for
SSPP face representation comes from the denoising auto-
encoder [11]. In this section, we will Ô¨Årst revisit the denoising
auto-encoder. Then we will propose our supervised
auto-encoder, its formulation, its optimization, its differences
with denoising auto-encoder, and its application in SSPP facerepresentation.
A. A Revisit of Denoising Auto-Encoder
A denoising auto-encoder tries to reconstruct the clean
input data by using the manually corrupted version of it.Mathematically, denote the input as x. In denoising
auto-encoder, input xis Ô¨Årst corrupted by some pre-
deÔ¨Åned noise, for example, Additive Gaussian noise(Àúx|x‚àºN(x,œÉ
2I)), masking noise (a fraction of xis forced
to 0), or salt-and-pepper noise (a fraction of xis forced to
be 0 or 1). Such a corrupted Àúxis used as the input of the
encoder h=f(Àúx)=sf(WÀúx+bf). Then the output of
encoder his input into the decoder ÀÜx=g(h)=sg(W/primeh+bg).
Here sfandsgare predeÔ¨Åned activation functions of encoder
and decoder respectively, which can be sigmoid functions,
hyperbolic tangent functions, or rectiÔ¨Åer functions [30], etc.
W‚ààRdh√ódxandbf‚ààRdhare the parameters of encoder, and
W/prime‚ààRdx√ódhand bg‚ààRdxare the parameters of decoder.
dxanddhare the dimensionality of input data and the numberof hidden nodes respectively. Based on the above deÔ¨Ånitions,
the objective of denoising auto-encoder is given as follows:
min
W,W/prime,bf,bg/summationdisplay
x‚ààXL(x,ÀÜx) (1)
Here Lis the reconstruction error, typically squared error
L(x,ÀÜx)=/bardblx‚àíÀÜx/bardbl2for real-valued inputs. After learning
fand g, the output of clean input ( f(x))i su s e da st h e
input of the next layer. By training such denoising auto-
encoder layer by layer, stacked denoising auto-encoders are
built. Experimental results show that stacked denoising auto-
encoders greatly improve the gen eralization performance of
the neural network. Even when the fraction of corrupted pixels(corrupted by zero masking noises) reaches up to 55%, the
recognition accuracy is still better or comparable with that of
a network trained without corruptions.
B. Supervised Auto-Encoder
In real applications, like passport or Gate ID identiÔ¨Åcation,
the only training sample (gallery image) for each person is
usually a frontal face with frontal/uniform lighting, neutral
expression, and no occlusion. However, the test faces
(probe images) are usually accompanied by variances in
illumination, occlusion, expression, pose, etc.Compared to the
denoising auto-encoder, these gallery images can be seen as
clean data and these probe images can be seen as corrupteddata. For robust face recognition, we desire to learn the
features which are robust to these variances. The success
of denoising auto-encoder convinces us of the possibility to
learn such features. Then the problem becomes: How do we
learn a mapping function which captures the discriminativestructures of the faces of different persons , while staying
robust to the possible variances of these faces? Once such
a function is learnt, robust features can be extracted forimage presentation, with an expected improvement to the
performance of SSPP face recognition.
Given a set of data which contain the gallery images (clean
data), probe images (corrupted data) as well as their labels,
we use them to train a deep neural network for featureextraction. We denote each probe image in this dataset as Àúx
i,
and its corresponding gallery image as xi(i=1,..., N).
It is desirable that xiandÀúxishould be represented similarly.
Therefore the following formulation is proposed (following the
work [11], [22], only the tied weights case is explored in this
paper, i.e., W/prime=WT):
min
W,bf,bg1
N/summationdisplay
i/parenleftbig
/bardblxi‚àíg(f(Àúxi))/bardbl2
2+Œª/bardblf(xi)‚àíf(Àúxi)/bardbl2
2/parenrightbig
+Œ±/parenleftbig
KL(œÅx||œÅ0)+KL(œÅÀúx||œÅ0)/parenrightbig
(2)
where
œÅx=1
N/summationdisplay
i1
2(f(xi)+1),
œÅÀúx=1
N/summationdisplay
i1
2(f(Àúxi)+1),
KL(œÅ||œÅ0)=/summationdisplay
j/parenleftbig
œÅ0log(œÅ0
œÅj)+(1‚àíœÅ0)log(1‚àíœÅ0
1‚àíœÅj)/parenrightbig
.(3)
GAO et al. : SINGLE SAMPLE FACE RECOGNITION VIA LEARNING DEEP SUPERVISED AUTOENCODERS 2111
Fig. 2. Architecture of Staked Supervised Auto-Enc oders. The left Ô¨Ågure: The basic supervised auto -encoder, which is comprised of the clean/‚Äúcorru pted‚Äù
faces, there features (hidden layer), as well as the reconstructed clean face by using the ‚Äúcorrupted face‚Äù. The middle Ô¨Ågure: The output of previous h idden
layer is used as the input to train the next supervis ed auto-encoder. We repeat such training several times until the desired number of hidden layers is reached.
In this paper, only two hidden layers are used. The right Ô¨Ågure: Once the netwo rk is trained, given any input face, the output of the last hidden layer is u sed
as the feature for image representation.
In this paper, the activation functions used are the hyperbolic
tangent, i.e., h= f(x)= tanh(Wx+bf),a n d
g(h)=tanh(WTh+bg).
We list the properties of the supervised auto-encoder as
follows:
1) The Ô¨Årst term in equation (2) is the reconstruction
error. It means that though gallery images contain somevariances, after passing through the encoder and the
decoder, they will be repaired. In this way, our learnt
model is robust to the variances of expression, occlusion,
pose, etc., which are quite different to the noises in the
Denoising Auto-encoder.
2) The second term in equation (2) is the similarity
preservation term. Since the output of the hidden layer
is used as the feature, f(x
i)and f(Àúxi)correspond to
the features of the same person. It is desirable that
they should be the same (ideally) or similar. Such a
constraint enforces the learning of a nonlinear mapping
robust to the variances that commonly appear in SSPP.
3) The third and the fourth term, the Kullback-Leiber
divergence (KL divergence) terms in equation (2)
introduce sparsity in the hidden layer. Work from
biological studies shows that the percentage of theactivated neurons of human brain at the same time is
around 1% to 4% [31]. Therefore the sparsity constraint
on the activation of the hidden layer is commonly used
in the auto-encoder based neural networks and results
show that sparse auto-encoder often achieves betterperformance [32] than that trained without the sparsity
constraint. Since the activation function we used is the
hyperbolic tangent, its output is between ‚àí1 and 1,
w h e r et h ev a l u eo f ‚àí1 is regarded as non-activated [33].
Therefore we map the output of the encoder to the
range (0,1) Ô¨Årst in equation (3). Here œÅ
xandœÅÀúxare the
mapped average activations of the clean data and
the corrupted data respectively. By choosing a small œÅ0,
the KL divergence regularizer enforces that only a few
fraction of neurons are activated [33], [34]. Following
the work [33], [34], we also set œÅ0to 0.05.
4) Weighting the Ô¨Årst and the second term with1
N
(Nis the total number of training samples) helps
balance the contributions of the the Ô¨Årst two terms andthe last terms in the optimization. Otherwise we may
need to tune Œ±on different datasets because the training
samples for different datasets may be different.
5) We aim at learning an featur e extractor to represent
faces corresponding to the same person similarly,
but [26] and [35] focus on learning a distance metric
in the last layer of their res pective DNN architectures.
Our work is different from [26] and [35] in terms of
network architecture and application.
6) Since the labels of the faces are used while training this
building block, we term our proposed formulation as the
Supervised Auto-Encoder (SAE) . We illustrate the idea
of such supervised auto-encoder in Fig 2 (the left Ô¨Ågure).
C. Optimization of Supervised Auto-Encoder
The optimization method is very important for good
performance of deep neural networks. Following the
work [36], [37], the entries of Ware randomly sampled
from the uniform distribution between [‚àí/radicalBig
6
dh+dx,/radicalBig
6
dh+dx],
and bfand bgare initialized with zero vectors. It is worth
noting that such normalized initialization is very important
for the good performance of Auto-Encoder based method‚Äúpresumably because the layer -to-layer transformations
maintain magnitudes of activations (Ô¨Çowing upward) and
gradients (Ô¨Çowing backward) [36]‚Äù. Then the Limited memoryBroydenCFletcherCGoldfarbCShanno (L-BFGS) algorithm
is used for learning the parameters because of its faster
convergence rate and better performance compared to
stochastic gradient descent methods and conjugate
gradient [38] methods.
As the computational cost of the similarity preservation
term (the 2
ndterm in equation (2)) and the calculation of its
gradient with respect to the unknown parameters is almost thesame with that of the reconstruction error term, the overall
computational complexity is still O(d
x√ódh)[22] in our
supervised auto-encoder.
D. Stacking Supervised Auto-Encoders
to Build Deep Architecture
Deep neural networks demonstrate better performance for
image representation than shallow neural networks [32].
2112 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. 10, NO. 10, OCTOBER 2015
Therefore we also train the supervised auto-encoder in a
layer-wise manner and get the deep architecture. Here we
term such an architecture as the Stacked Supervised
Auto-Encoders (SSAE) . After learning the activation function
of the encoder in previous layer, it is applied to the clean
data and corrupted data respectively, and the outputs serve
as the clean input data and corrupted input data to train thesupervised auto-encoder in the next layer. Once the whole
network is trained, the output of the highest layer will be
used as the feature for image representation. We illustrate this
learning strategy in Fig. 2.
E. The Differences Between Stacked Supervised
Auto-Encoders and Stacked Denoising Auto-Encoders
Our stacked auto-encoders are different from the stacked
denoising auto-encoders in the following three aspects:
1) Input: Denoising auto-encoder is an unsupervised feature
learning method, and the corrupted data are generated by
manually corrupting the clean data with predeÔ¨Åned noises.
Therefore the trained model is robust to these predeÔ¨Ånednoises. However, in our supervised denoising auto-encoder, the
corrupted data are the photos ta ken under different settings,
and they are physically meaningful data. By enforcing thereconstructed probe image to be similar to its gallery image,
we can overcome the possible variances which appear
in SSPP face recognition, and w hich are quite different
from the noises in denoising auto-encoder. This is why our
supervised auto-encoder needs the labels of data to train thenetwork.
2) Formulation: The similarity preservation term is used to
further emphasize the desired property of SSPP image
representation. Though we enforce the reconstructed probe
image to be similar to the gallery image, it cannot beguaranteed that the extracted features of them are similar
because the variances in pose, expression, illumination, etc.
can make the probe and the gallery quite different. But inSSPP face recognition, it is desirable that the images of the
same person have similar representation. To this end, the
similarity preservation term is added to the objective of SAE
in equation (2). Such term further makes the extracted features
be robust to the variances in face recognition. As shownin Section IV-F, this term greatly improves the recognition
accuracy, especially for the cases with large intra-class
variances.
3) Building a Deep Architecture: In stacked denoising
auto-encoders, once the activation function of the encoder inprevious layer is trained, it maps the clean data to the hidden
layer, and the outputs serve as the clean input data of next
layer SAE. Then the noises which are generated in the sameway with that in the previous layer are added on the clean
data to serve as the corrupted data. As claimed in [34], since
the features in the hidden layer are no longer in the same
feature space with the input data, the meaning of applying the
noises generated in the same way with that in previous layerto the output of the hidden layer is no longer clear. Similar
to [34], we apply the activation function of the encoder to
both the clean data and the corrupted data, and their outputsserve as the clean and corrupted input data of the next layer.
Hence this way of stacking the basic supervised auto-encoders
is more natural for build ing a deep architecture.
IV . E
XPERIMENTS
In this section, we will experimentally evaluate the
stacked supervised auto-encoder for extracting the features in
SSPP face recognition task on Extended Yale B, CMU-PIE,
and AR datasets. Important parameters will also be exper-imentally evaluated. Besides face recognition, we also use
stacked supervised auto-encoder for face veriÔ¨Åcation on the
LFW dataset.
A. Experimental Setup
We use the Extended Yale B, AR, and CMU-PIE, and
Multi-PIE datasets for evaluation of the effectiveness of the
proposed SSAE model. All the images are gray-scale images
and are manually aligned.
2The image size is 32 √ó32. Thus the
dimensionality of the input vect or is 1024. Each input feature
is normalized with the /lscript2normalization. We set Œª=Œª0√ódx
dh.
Further, we set Œª0=10 on the CMU-PIE and Multi-PIE
dataset, and set Œª0=5 on the AR and the Extended Yale
B datasets. The reason for this is that the variances are more
signiÔ¨Åcant on the CMU-PIE and Multi-PIE dataset than that
of the AR or the Extended Yale B datasets. The weightcorresponding to the KL divergency term ( Œ±)i sÔ¨Å x e dt o
be 10
‚àí4. Following the work [11], we Ô¨Åx the number of
the hidden nodes in all the hidden layers to be 2048. In our
experiments, we found that two hidden layers already give
a sufÔ¨Åciently good performance. After extracting featureswith stacked supervised auto-encoders for face representation,
following the work [7], sparse representation based classiÔ¨Åca-
tion is used for face recognition. The features are normalizedto make their /lscript
2norms equal to 1 before sparse coding.
1) Baselines: We compare our Stacked Supervised
Auto-Encoders (SSAE) with th e following work because of
their close relationships. It is also worth noting that all the
comparisons are based on the sam e training/test set, and the
same generic data if they are used.
1) Sparse Representation for ClassiÔ¨Åcation (SRC) [7] with
raw pixels;
2) LBP feature followed by SRC;
3) Collaborative Representation for ClassiÔ¨Åcation
(CRC) [39].
4) AGL [1];
5) One-Shot Similarity Kernel (OSS) [40]. The generic
data are used as the negative data in OSS, i.e., we use
the same generic/training/testing split for both OSS and
our SAE.
2In all our experiments, faces are cropped based on manually labeled
landmark points, and aligned based la ndmark points. We also tested our work
with the faces cropped with the OpenCV face detector on the AR dataset,
but the performance of our work on such data is less than 50% on AR.One possible reason for such poor performance is that we don‚Äôt have enough
data to train a deep network to be robust to the misalignment. The combination
of our work with automatic face alignment method is our future work alongthis direction.
GAO et al. : SINGLE SAMPLE FACE RECOGNITION VIA LEARNING DEEP SUPERVISED AUTOENCODERS 2113
6) Denoising Auto-Encoder (DAE) [11] with 10% masking
noises followed by SRC;
7) ModiÔ¨Åed Denoising Auto-Encoder (MDAE).
We propose to use the reconstruction error term onlyin equation (2) ( Œª=Œ±=0), and term such baseline
method as ModiÔ¨Åed Denoising Auto-Encoder (MDAE);
8) Siamese network (SN) [26].
3
In all these baseline methods, 1-3 correspond to the verypopular sparse coding related methods. 4-5 are speciallydesigned for SSPP, and 6-8 are most related deep learning
methods.
4
B. Dataset Description
The CMU-PIE dataset [42] contains 41,368 images
of 68 subjects. For each subject, the images are taken under
13 different poses, 4 different illumination conditions, and4 different expressions. For each subject, we use the face
images taken with the frontal pose, neutral expression, and
normal lighting condition as the galleries, and use the rest of
the images taken with the poses C27, C29, C07, C05, C09 as
probes. We use images of 20 subjects to learn the SSAE, anduse the remaining 48 subjects for evaluation.
The AR dataset [43] contains over 4,000 frontal faces taken
from 126 subjects (70 men and 56 women) in two differentsessions, and the images contain variances in occlusion
(sunglasses or scarves), expression (neutral expression, smile,
angry, scream), and illumination. Some images contains both
occlusion and illumination variances. In our experiments,
20 subjects from session 1 are used as the generic set fortraining the SSAE, and another 80 subjects also from session 1
are used for evaluation.
The Extended Yale B dataset [44] contains 38 categories.
For each subject, we use the frontal faces whose light source
direction with respect to the camera axis is 0 degree azimuth
(‚ÄòA+000‚Äô) and 0 degree elevation (‚ÄòE +00‚Äô) as gallery
images, and use the rest of the images with different lighting
conditions as the probe images. Following the work of deepLambertian networks (DLN) [20], 28 categories are used to
train the SSAE and the remaining 10 categories which are
from the original Yale B dataset are used for evaluation.
The Multi-PIE dataset [45] contain images of 337 persons
taken under the four sessions over the span of 5 months.
3We tried several different network architectures in order to get the best
experimental result for the Siamese network. Surprisingly, the one of the
best performers is the two-layer netwo rk which take a fully connected layer
(with 500 hidden units and the tanh non- linearity) as the Ô¨Årst layer and the
Siamese as the second one. The origin al deep architecture proposed in [26]
(i.e. ‚Äúthe basic architecture is C1-S2-C3-S4-C5-F6‚Äù) doesn‚Äôt work as well as
this simpler ones (With the architecture listed in [1], the accuracy of SiameseNetwork on AR is below 60%). Probabl y, the limited size of our generic
training set make it difÔ¨Åcult to train a good convolutional neural network
with deep architecture. Also as the image we used are well-aligned which is
different from [26], the convolutional l ayers in the original architecture might
become redundant.
4Because the codes of deepface [27] and Pyramid CNN [41] are not
available, and the implementation and preprocessing involves lots of tricks,
say very sophisticated alignment met hod and lots of outside data required
for network training, here we don‚Äôt compare our method with these works.
Another reason for not comparing with [27] and [41] is that the task solved
by these methods is face veriÔ¨Åcation, but our work solves the face veriÔ¨Åcationtask.TABLE I
P
ERFORMANCE COMPARISON BETWEEN DIFFERENT METHODS ON THE
EXTENDED YALE BAND THE AR D ATASETS (%)
TABLE II
PERFORMANCE COMPARISON BETWEEN DIFFERENT METHODS ON THE
CMU-PIE D ATAS ET (%)
TABLE III
PERFORMANCE COMPARISON BETWEEN DIFFERENT METHODS ON THE
MULTI -PIE D ATAS ET (%)
For each persons, images are taken under 15 different view
angles and 19 different illuminations while displaying different
facial expressions. In our experiments, only the images withthe neutral expression, near frontal pose (0¬∞, 15¬∞, ‚àí15¬∞), and
different illuminations are used. For each person, the frontal
face with neutral expression, frontal illumination is used asthe gallery and all the images are used as the probe. The
249 persons in session 1 are used as the evaluation, and the Ô¨Årst
time appearance images corresponding to the other 88 persons
which only appear in session 2-4 are used to train the
network.
C. Performance Evaluation
The performance of different methods on the AR, Extended
Yale B, CMU-PIE, and Multi-PIE datasets is listed in Table I,Table II, and Table III. We can see that our SSAE outperforms
all the rest of the methods including those specially designed
methods for SSPP image representation, and it achieves the
2114 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. 10, NO. 10, OCTOBER 2015
best performance, which proves the effectiveness of SSAE for
extracting robust features for face representation. Needless to
say, SSAE based face represen tation can also be combined
with ESRC if another set of labeled data are provided.
1) SAE vs. Hand-Crafted Features: The improvement of the
features learnt from our method over the popular LBP features
is between about 10% (C27) and 31% (C29) on theCMU-PIE dataset, with larger improvements for subsets of
larger poses. On the AR dataset and the Extended Yale B
dataset where there is little pose variation, the improvement
of our method over LBP is still around 5%. Our method
also outperforms ESRC, which introduces a pre-trainedintra-class variance dictionary to extend the SRC method
to the SSPP case, and the improvement is very evident for
cases of large pose variance (e.g., CMU-PIE C07, C09, C29,Multi-PIE 15, ‚àí15). Compared with ESRC, another advantage
of our method is speed. ESRC relies on the intra-class variance
dictionary to achieve good performance, and the size of such
dictionary is usually big. Such a big dictionary will increase
computational costs, but in our method, the dictionary sizein the sparse coding is the same with the number of gallery
faces. Thus our method is signiÔ¨Åcantly faster than ESRC
in testing. For example, ESRC costs about 4.63 seconds tosolve sparse coding for each probe face on Extended Yale B,
while our method can solve the sparse coding in about
1.8√ó10
‚àí3seconds. Here the number of atoms in intra-class
variance dictionary is 1746 on the Extended Yale B. Therefore
the total atoms in the dictionary is 1756 for ESRC. In ourmethod, the number of atoms in the dictionary of SRC is
only 10. All the methods are based on Matlab
implementations, and run on a Windows Server (64bit)with a 2.13GHz CPU and 16GB RAM. It is worth noting
that as the intra-class variance dictionary is very large on
Multi-PIE, the optimi zation is very slow, therefore we don‚Äôt
include its performance on the Multi-PIE dataset.
2) SAE vs. Other DNNs: Compared with DAE, the improve-
ment of our method is over 30% on all the datasets. The reason
for the poor performance of DAE f or SSPP face representation
is that the training of DAE is unsupervised, and zero maskingnoise is used to train the DAE. Therefore it is natural
that the trained DAE cannot handle the variances in poses,
expressions, etc. Different from DAE, MDAE enforces the
reconstructed faces of the probe images, which contain
the variances in expression, pose, etc, to be well aligned
to their gallery images (frontal faces), therefore its perfor-
mance is better than DAE, but it is still inferior to that
of stacked SAE, especially for the cases with variance inpose and expression on AR and CMU-PIE. In contrast,
besides using more appropriate reconstruction error term,
our SAE also enforces extracted features with the sameperson to be similar. Therefore, SAE can overcome these
variances in SSPP and is more suitable for extracting features
for face recognition. Moreover, on the Extended Yale B
dataset, the performance of our method (83.97% when
the number of hidden nodes is 1024, and 82.22% whenthe number of hidden nodes is 2048) is better than the
81% obtained by Deep Lambertian Network (DLN) [20],
which is specially designed for removing the illuminationeffect in SSPP face recogn ition [20]. In addition to the
28 categories from the Extended Yale B, DLN also uses the
Toronto Face Database, which is a very large dataset to train
the network. Although we use much less data to train the SAEand it does not use any domain information about illumination
model as in DLN, performance of SAE is still better.
5
In addition, our SAE also outperforms SN on all the datasets.
These experiments clearly demonstrate the superiority of SAE
for recognition tasks over other DNN building blocks such as
DAE and DLN.
3) More Observations: We notice that as the poses of
probe images change, the performance drops notably onthe CMU-PIE. For example, the recognition accuracy drops
at least 10.83% (C09) compared with that of the frontal
face (C27) pose. Interestingly, we also notice that the perfor-
mance of C07 (look up) and C09 (look down) is slightly better
than C05 (look right) and C29 (look left). A possible reasonis that the missing parts of face are larger for C05 and C29
than that in C07 and C09 (please refer to Fig. 1). Similar
observations can also be found on the Multi-PIE datasetin Table III where the recognition accuracy also drops by
around 30% if the poses are non-frontal.
Moreover, some probe images and their reconstructed
images on the AR dataset are also shown in Fig. 3. We can
see our method can remove the illumination, and recoverthe neutral face from the faces with different expressions.
For the faces with occlusion, our method can also simulate
the faces without occlusion, but compared with illuminationand expression, it is more difÔ¨Åcult to recover the face from
the occluded face because too much information is lost.
Such results are natural because human can infer the faces
with normal illumination and neutral expression from the
experience (For the deep neural networks, the experience islearnt from the generic set). But it is also almost impossible
for our human to infer the occluded face parts because too
much information is missing.
Moreover, some probe images and their reconstructed
images on the AR dataset are also shown in Fig. 3. We can
see our method can remove the illumination, and recover
the neutral face from the faces with different expressions.
For the faces with occlusion, our method can also simulatethe faces without occlusion, but compared with illumination
and expression, it is more difÔ¨Åcult to recover the face from
the occluded face because too much information is lost.Such results are natural because human can infer the faces
with normal illumination and neutral expression from the
experience (For the deep neural networks, the experience islearnt from the generic set). But it is also almost impossible
for our human to infer the occluded face parts because too
much information is missing.
5Because the results on the AR and CMU-PIE datasets and the codes for
DLN are not available for comparison, we don‚Äôt report its performance on
the AR and CMU-PIE datasets. But it is unlikely that DLN works well on
the AR and CMU-PIE as it is not designe d to handle other variations in the
data such as poses, occlusions, expressions, etc. Moreover, the state-of-the-art
performance on Extended Yale B has reached 93.6% in terms of accuracy
under the SSPP setting in [46]. But the e xperimental setup in [46] is a little
different from that in our paper.
GAO et al. : SINGLE SAMPLE FACE RECOGNITION VIA LEARNING DEEP SUPERVISED AUTOENCODERS 2115
Fig. 3. A comparison between the original images and the reconstructed ones on the AR dataset. (a): Corrupted faces used for training the network.
(b): Corrupted faces used for evaluation. (c): Reconstructed faces of (a). (d): Reconstructed faces of (b).
Fig. 4. The Effect of the Similarity Preservation Term on the CMU-PIE
Dataset (%).
D. Evaluation of Similarity Preservation Term
The similarity preservation term, the second term in
equation (2), is very important in the formulation of SAE. Here
we list the performance of differ ent networks trained with the
formulation with and without this term in Fig. 4. It can be
seen that this term improves the recognition by 3% for thebenign frontal face case (C27), about 10% for C09 and C07,
and about 29% for C05 and C29. We can see that improvement
in performance increases with the increase of the poses (themisalignment of faces is larger for C05 and C29 than that
in C07 and C09). This validates that the similarity preservation
term indeed enhances the robustness of face representation,
especially for pose variations.
E. The Effect of Deep Architecture
As an example, we show the performance of the SSAE
with different layers on the Multi-PIE dataset in Fig. 5. We cansee that 2-layer SSAE network outperforms the single layer
network, which demonstrates the effectiveness of the deep
architecture.Fig. 5. The effect of SSAE with different depth on the Multi-PIE dataset.
F . Parameter Evaluation
1) Number of Hidden Nodes in Hidden Layers: We change
the number of hidden nodes from 512 to 4096 on the
Extended Yale B, and show their performancein Fig. 6 (top left). Results show that the recognition
accuracy is higher when the number of hidden nodes
is 1024 or 2048, which is the same or larger than the
dimensionality of the data. The reason for this is that more
hidden nodes usually increase the expressiveness of theauto-encoder [32]. We also not e that too many hidden nodes
actually decreases the accuracy. A possible reason is that
the data used for learning the network in our setting isnot enough (only images of 28 subjects are used to learn
the parameters), which may limit stability of the learnt
network.
2) Weight of KL Terms ( Œ±):We plot the performance with
different Œ±in Fig. 6 (top right). The poor performance when
Œ±is too small (10
‚àí6)proves the importance of the sparsity
term. But if we impose too large weight on Œ±(10‚àí2), more
hidden nodes will hibernate for a given input, which affects
2116 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. 10, NO. 10, OCTOBER 2015
Fig. 6. The effect of different parameters in supervised auto-encoder on the
Extended Yale B dataset.
the sensitiveness of the model to the input, and may make
faces of different persons be represented similarly. This may
be the reason that for a value of 102, our model has the lowest
performance. So properly setting Œ±is a requirement for the
good performance of our model. Similarly phenomenon also
happens in sparse representation based face recognition [47],
too small or too large sparsity both will reduce the recognition
accuracy.
3) Weight of Similarity Preservation Term ( Œª):We plot the
performance of SSAE with different Œªin Fig. 6 (bottom). The
poor performance with smaller Œª0(0.1 or 1) also demonstrates
the importance of the similarity preservation term. But if Œªis
too big, the learnt network might become less discriminative
for different subjects as it enforces too strongly the similarityof the learnt features for diverse samples. That leads to a drop
in performance for very large Œª. Moreover, from the plot,
we see that good performance can be obtained for a fairlylarge range of Œª.
G. The Comparison of Different Activation Functions
Besides hyperbolic tangent, we also evaluate the perfor-
mance of SSAE with other activation functions, including
sigmoid and RectiÔ¨Åed Linear Units (ReLU) [48]. To achievethe sparsity in the hidden layer, different strategies are used
for different activation functions. SpeciÔ¨Åcally, if the activation
function is sigmoid, the objective function is rewritten
as follows:
min
W,bf,bg1
N/summationdisplay
i/parenleftbig
/bardblxi‚àíg(f(Àúxi))/bardbl2
2+Œª/bardblf(xi)‚àíf(Àúxi)/bardbl2
2/parenrightbig
+Œ±/parenleftbig
KL(œÅx||œÅ0)+KL(œÅÀúx||œÅ0)/parenrightbig
(4)
where
œÅx=1
N/summationdisplay
if(xi),TABLE IV
PERFORMANCE COMPARISON WITHDIFFERENT
ACTIV A TION FUNCTIONS (%)
TABLE V
PERFORMANCE COMPARISON BETWEEN DIFFERENT METHODS
ON THE LFW D ATASETS .( % )
œÅÀúx=1
N/summationdisplay
if(Àúxi),
KL(œÅ||œÅ0)=/summationdisplay
j/parenleftbig
œÅ0log(œÅ0
œÅj)+(1‚àíœÅ0)log(1‚àíœÅ0
1‚àíœÅj)/parenrightbig
.(5)
If the activation function is ReLU, the objective function is
rewritten as follows:
min
W,bf,bg1
N/summationdisplay
i/parenleftbig
/bardblxi‚àíg(f(Àúxi))/bardbl2
2+Œª/bardblf(xi)‚àíf(Àúxi)/bardbl2
2/parenrightbig
+Œ±/parenleftbig
/bardblf(xi)/bardbl1+/bardblf(Àúxi)/bardbl1/parenrightbig
(6)
We list the performance of our SAE based on different
activations on the AR dataset in Table IV. We can see that
hyperbolic tangent usually achieves the best performance.It is worth noting that ReLU is usually used for Convolu-
tional Neural Networks (CNN) and demonstrate good perfor-
mance for image classiÔ¨Åcation. But many tricks, includingthe momentum, weight decay, early stopping, are used to
optimize the objective function in CNN. In our objective
function optimization, we simply use the L-BFGS to opti-
mize the objective function. Many existing works [38], [49]
have shown that different optimization methods will greatlyaffect the performance of deep neural networks. Maybe more
advanced optimization method and more tricks help improve
the performance of ReLU.
H. Extension: SSAE for Face VeriÔ¨Åcation
Besides face recognition, our model can also be used
for face veriÔ¨Åcation. Specially, we tested our work on the
LFW dataset under the constrained without outside dataprotocol. The performance of different methods under such
protocol is listed in Table V . Interestingly, though our
work is designed for learning features to make faces of the
same person represented similarly, the performance of our
model for face veriÔ¨Åcation is not bad. By learning moresophisticated distance metric [50] with our face representation
simultaneously, the performance of our method on LFW
probably can be further boosted.
GAO et al. : SINGLE SAMPLE FACE RECOGNITION VIA LEARNING DEEP SUPERVISED AUTOENCODERS 2117
V. C ONCLUSION AND FUTURE WORK
In this paper, we propose a supervised auto-encoder,
and use it to build deep neural network architecture for
extracting robust features for SSPP face representation.
By introducing a similarity preservation term, our supervised
auto-encoder enforces faces corresponding to the same person
to be represented similarly. Experimental results on the AR,Extended Yale B, and CMU-PIE datasets demonstrate clear
superiority of this module over other conventional modules
such as DAE or DLN.
In view of the size of images and training sets, we restrict
the image size to be 32 √ó32, and only images of a handful
of subjects are used to train the network. For example, only20 subjects are used on the CMU -PIE and AR datasets, and
only 28 subjects on the Extended Yale B dataset. Obviously
more training samples will improve the stability of the learnt
network and larger images will improve the face recognition
accuracy [57].
R
EFERENCES
[1] Y . Su, S. Shan, X. Chen, and W. Gao, ‚ÄúAdaptive generic learning for
face recognition from a single sample per person,‚Äù in Proc. IEEE Conf.
Comput. Vis. Pattern Recognit. , Jun. 2010, pp. 2699‚Äì2706.
[2] X. Tan, S. Chen, Z.-H. Zhou, and F. Zhang, ‚ÄúFace recognition from a
single image per person: A survey,‚Äù Pattern Recognit. , vol. 39, no. 9,
pp. 1725‚Äì1745, 2006.
[3] L. Fei-Fei, R. Fergus, and P. Perona, ‚ÄúOne-shot learning of object
categories,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 28, no. 4,
pp. 594‚Äì611, Apr. 2006.
[4] M. A. Turk and A. P. Pentland, ‚ÄúFace recognition using eigenfaces,‚Äù
inProc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. ,
Jun. 1991, pp. 586‚Äì591.
[5] P. N. Belhumeur, J. P. Hespanha, and D. J. Kriegman, ‚ÄúEigenfaces vs.
Fisherfaces: Recognition using class speciÔ¨Åc linear projection,‚Äù IEEE
Trans. Pattern Anal. Mach. Intell. , vol. 19, no. 7, pp. 711‚Äì720,
Jul. 1997.
[6] Q.-X. Gao, L. Zhang, and D. Zhang, ‚ÄúFace recognition using FLDA
with single training image per person,‚Äù Appl. Math. Comput. , vol. 205,
no. 2, pp. 726‚Äì734, 2008.
[7] J. Wright, A. Y . Yang, A. Ganesh, S. S. Sastry, and Y . Ma, ‚ÄúRobust face
recognition via sparse representation,‚Äù IEEE Trans. Pattern Anal. Mach.
Intell. , vol. 31, no. 2, pp. 210‚Äì227, Feb. 2009.
[8] Q. V . Le et al. , ‚ÄúBuilding high-level features using large scale unsuper-
vised learning,‚Äù in Proc. 29th Int. Conf. Mach. Learn. , 2012, pp. 81‚Äì88.
[9] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImageNet classiÔ¨Åcation
with deep convolutional neural networks,‚Äù in Proc. Adv. Neural Inf.
Process. Syst. , 2012, pp. 1106‚Äì1114.
[10] G. E. Hinton, S. Osindero, and Y . -W. Teh, ‚ÄúA fast learning algorithm
for deep belief nets,‚Äù Neural Comput. , vol. 18, no. 7, pp. 1527‚Äì1554,
2006.
[11] P. Vincent, H. Larochelle, I. Laj oie, Y . Bengio, and P.-A. Manzagol,
‚ÄúStacked denoising autoencoders: L earning useful representations in a
deep network with a local denoising criterion,‚Äù J. Mach. Learn. Res. ,
vol. 11, no. 5, pp. 3371‚Äì3408, 2010.
[12] D. Erhan, Y . Bengio, A. Courville, P.-A. Manzagol, P. Vincent, and
S. Bengio, ‚ÄúWhy does unsupervised pr e-training help deep learning?‚Äù
J. Mach. Learn. Res. , vol. 11, pp. 625‚Äì660, Feb. 2010.
[13] J. Yang, D. Zhang, A. F. Frangi, and J.-Y . Yang, ‚ÄúTwo-dimensional
PCA: A new approach to appearance-based face representation andrecognition,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 21, no. 1,
pp. 131‚Äì137, Jan. 2004.
[14] J. Wu and Z.-H. Zhou, ‚ÄúFace recognition with one training image
per person,‚Äù Pattern Recognit. Lett. , vol. 23, no. 14, pp. 1711‚Äì1719,
2002.
[15] S. Chen, D. Zhang, and Z.-H. Zhou, ‚ÄúEnhanced (PC)2A for face
recognition with one training image per person,‚Äù Pattern Recognit. Lett. ,
vol. 25, no. 10, pp. 1173‚Äì1181, 2004.
[16] A. M. Martinez, ‚ÄúRecognizing impr ecisely localized, partially occluded,
and expression variant faces from a single sample per class,‚Äù IEEE Trans.
Pattern Anal. Mach. Intell. , vol. 24, no. 6, pp. 748‚Äì763, Jun. 2002.[17] S. Shan, B. Cao, W. Gao, and D. Zhao, ‚ÄúExtended Fisherface for face
recognition from a single example image per person,‚Äù in Proc. IEEE
Int. Symp. Circuits Syst. , May 2002, pp. II-81‚ÄìII-84.
[18] S. Chen, J. Liu, and Z.-H. Zhou, ‚ÄúMaking FLDA applicable to face
recognition with one sample per person,‚Äù Pattern Recognit. , vol. 37,
no. 7, pp. 1553‚Äì1555, 2004.
[19] T.-K. Kim and J. Kittler, ‚ÄúLocally linear discriminant analysis for
multimodally distributed classes for face recognition with a single
model image,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 27, no. 3,
pp. 318‚Äì327, Mar. 2005.
[20] Y . Tang, R. Salakhutdinov, and G. E. Hinton, ‚ÄúDeep Lambertian net-
works,‚Äù in Proc. 29th Int. Conf. Mach. Learn. , 2012, pp. 1623‚Äì1630.
[21] N. Japkowicz, S. J. Hanson, and M. A. Gluck, ‚ÄúNonlinear autoasso-
ciation is not equivalent to PCA,‚Äù Neural Comput. , vol. 12, no. 3,
pp. 531‚Äì545, 2000.
[22] S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y . Bengio, ‚ÄúContractive
auto-encoders: Explicit invariance during feature extraction,‚Äù in Proc.
28th Int. Conf. Mach. Learn. , 2011, pp. 833‚Äì840.
[23] S. Rifai et al. , ‚ÄúHigher order contractive auto-encoder,‚Äù in Proc. Eur.
Conf. Mach. Learn. , 2011, pp. 645‚Äì660.
[24] W. Y . Zou, A. Y . Ng, S. Zhu, and K. Yu, ‚ÄúDeep learning of invariant
features via simulated Ô¨Åxations in video,‚Äù in Proc. Adv. Neural Inf.
Process. Syst. , 2012, pp. 3203‚Äì3211.
[25] Q. V . Le, A. Karpenko, J. Ngiam, and A. Y . Ng, ‚ÄúICA with recon-
struction cost for efÔ¨Åcient overcomplete feature learning,‚Äù in Proc. Adv.
Neural Inf. Process. Syst. , 2011.
[26] S. Chopra, R. Hadsell, and Y . LeCun, ‚ÄúLearning a similarity met-
ric discriminatively, with application to face veriÔ¨Åcation,‚Äù in Proc.
IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. , Jun. 2005,
pp. 539‚Äì546.
[27] Y . Taigman, M. Yang, M. Ranzato, and L. Wolf, ‚ÄúDeepface: Closing
the gap to human-level performance in face veriÔ¨Åcation,‚Äù in Proc. IEEE
Conf. Comput. Vis. Pattern Recognit. , Jun. 2014, pp. 1701‚Äì1708.
[28] Z. Zhu, P. Luo, X. Wang, and X. Tang, ‚ÄúDeep learning identity-
preserving face space,‚Äù in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) ,
Dec. 2013, pp. 113‚Äì120.
[29] Z. Zhu, P. Luo, X. Wang, and X. Tang. (2014). ‚ÄúRecover canonical-
view faces in the wild with deep neural networks.‚Äù [Online]. Available:http://arxiv.org/abs/1404.3543
[30] V . Nair and G. E. Hinton, ‚ÄúRectiÔ¨Åed linear units improve restricted
Boltzmann machines,‚Äù in Proc. 27th Int. Conf. Mach. Learn. , 2010,
pp. 807‚Äì814.
[31] P. Lennie, ‚ÄúThe cost of cortical computation,‚Äù Current Biol. , vol. 13,
no. 6, pp. 493‚Äì497, 2003.
[32] Y . Bengio, ‚ÄúLearning deep architectures for AI,‚Äù Found. Trends Mach.
Learn. , vol. 2, no. 1, pp. 1‚Äì127, 2009.
[33] A. Ng, ‚ÄúSparse autoencoder,‚Äù Stanford Univ., Stanford, CA, USA,
Lecture Notes CS294A, 2011.
[34] J. Xie, L. Xu, and E. Chen, ‚ÄúImage denoising and inpainting with
deep neural networks,‚Äù in Proc. Adv. Neural Inf. Process. Syst. , 2012,
pp. 350‚Äì358.
[35] R. Salakhutdinov and G. E. Hint on, ‚ÄúLearning a nonlinear embedding
by preserving class neighbourhood structure,‚Äù in Proc. Int. Conf. Artif.
Intell. Statist.
, 2007, pp. 412‚Äì419.
[36] X. Glorot and Y . Bengio, ‚ÄúUndersta nding the difÔ¨Åculty of training deep
feedforward neural networks,‚Äù in Proc. Int. Conf. Artif. Intell. Statist. ,
2010, pp. 249‚Äì256.
[37] Y . Bengio, ‚ÄúPractical recommendati ons for gradient-based training of
deep architectures,‚Äù in Neural Networks: Tricks of the Trade . Berlin,
Germany: Springer-Verlag, 2012, pp. 437‚Äì478.
[38] Q. V . Le, J. Ngiam, A. Coates, A. Lahiri, B. Prochnow, and A. Y . Ng,
‚ÄúOn optimization methods for deep learning,‚Äù in Proc. 28th Int. Conf.
Mach. Learn. , 2011, pp. 265‚Äì272.
[39] L. Zhang, M. Yang, and X. Feng, ‚ÄúSpa rse representation or collaborative
representation: Which helps face recognition?‚Äù in Proc. IEEE Int. Conf.
Comput. Vis. , Nov. 2011, pp. 471‚Äì478.
[40] L. Wolf, T. Hassner, and Y . Taigman, ‚ÄúThe one-shot similarity kernel,‚Äù in
Proc. IEEE 12th Int. Conf. Comput. Vis. , Sep./Oct. 2009, pp. 897‚Äì902.
[41] H. Fan, Z. Cao, Y . Jiang, Q. Yin, and C. Doudou. (2014). ‚ÄúLearning deep
face representation.‚Äù [Online]. Available: http://arxiv.org/abs/1403.2802
[42] T. Sim, S. Baker, and M. Bsat, ‚ÄúThe CMU pose, illumination, and
expression (PIE) database,‚Äù in Proc. 5th IEEE Int. Conf. FG , May 2002,
pp. 46‚Äì51.
[43] A. Mart√≠nez and R. Benavente, ‚ÄúThe AR face database,‚Äù
CVC Tech. Rep. #24, Jun. 1998.
2118 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. 10, NO. 10, OCTOBER 2015
[44] A. S. Georghiades, P. N. Belhumeur, and D. Kriegman, ‚ÄúFrom few to
many: Illumination cone models for face recognition under variablelighting and pose,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 23,
no. 6, pp. 643‚Äì660, Jun. 2001.
[45] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker, ‚ÄúMulti-PIE,‚Äù
Image Vis. Comput. , vol. 28, no. 5, pp. 807‚Äì813, 2010.
[46] Y . Tang, R. Salakhutdinov, and G. E. Hinton, ‚ÄúTensor analyzers,‚Äù in
Proc. 30th Int. Conf. Mach. Learn. , 2013, pp. 163‚Äì171.
[47] S. Gao, I. W.-H. Tsang, and L.-T. Chia, ‚ÄúSparse representation with
kernels,‚Äù IEEE Trans. Image Process. , vol. 22, no. 2, pp. 423‚Äì434,
Feb. 2013.
[48] X. Glorot, A. Bordes, and Y . Be ngio, ‚ÄúDeep sparse rectiÔ¨Åer neural
networks,‚Äù in Proc. 14th Int. Conf. Artif. Intell. Statist. , 2011,
pp. 315‚Äì323.
[49] G. E. Hinton and R. R. Salakhutdi nov, ‚ÄúReducing the dimensionality of
data with neural networks,‚Äù Science , vol. 313, no. 5786, pp. 504‚Äì507,
2006.
[50] J. Hu, J. Lu, and Y .-P. Tan, ‚ÄúDiscriminative deep metric learning for
face veriÔ¨Åcation in the wild,‚Äù in Proc. IEEE Conf. Comput. Vis. Pattern
Recognit. (CVPR) , Jun. 2014, pp. 1875‚Äì1882.
[51] N. Pinto, J. J. DiCarlo, and D. D. Cox, ‚ÄúHow far can you get with
a modern face recognition test set using only simple features?‚Äù in
Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2009,
pp. 2591‚Äì2598.
[52] H. Li, G. Hua, Z. Lin, J. Brandt, and J. Yang, ‚ÄúProbabilistic elastic
matching for pose variant face veriÔ¨Åcation,‚Äù in Proc. IEEE Conf.
Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2013, pp. 3499‚Äì3506.
[53] S. R. Arashloo and J. Kittler, ‚ÄúEfÔ¨Åcient processing of MRFs for
unconstrained-pose face recognition,‚Äù in Proc. IEEE 6th Int. Conf.
Biometrics, Theory, Appl. Syst. (BTAS) , Sep./Oct. 2013, pp. 1‚Äì8.
[54] K. Simonyan, O. M. Parkhi, A. Vedaldi, and A. Zisserman, ‚ÄúFisher
vector faces in the wild,‚Äù in Proc. Brit. Mach. Vis. Conf. , 2013,
pp. 8.1‚Äì8.12.
[55] H. Li, G. Hua, X. Shen, Z. Lin, an d J. Brandt, ‚ÄúEigen-PEP for video face
recognition,‚Äù in Computer Vision (Lecture Notes in Computer Science),
vol. 9005. Berlin, Germany: Springer-Verlag, 2015, pp. 17‚Äì33.
[56] S. R. Arashloo and J. Kittler, ‚ÄúClass-speciÔ¨Åc kernel fusion of multiple
descriptors for face veriÔ¨Åcation using multiscale binarised statisticalimage features,‚Äù IEEE Trans. Inf. Forensics Security , vol. 9, no. 12,
pp. 2100‚Äì2109, Dec. 2014.
[57] M. D. Zeiler and R. Fergus, ‚Äú Visualizing and understanding
convolutional networks,‚Äù in Computer Vision (Lecture Notes
in Computer Science), vol. 8689. New York, NY , USA:
Springer-Verlag, 2014, pp. 818‚Äì833.
Shenghua Gao received the B.E. (Hons.) degree
from the University of Science and Technologyof China, in 2008, and the Ph.D. degree from
Nanyang Technological University, in 2012. He is
currently an Assistant Professor with ShanghaiTechUniversity, China. From 2012 to 2014, he was
a Research Scientist with the Advanced Digital
Sciences Center, Singapore. He has authored over30 papers on object and face recognition related top-ics in many international conferences and journals,
including the IEEE T
RANSACTIONS ON PATTERN
ANALYSIS AND MACHINE INTELLIGENCE ,t h e International Journal of
Computer Vision , the IEEE T RANSACTIONS ON IMAGE PROCESSING ,t h e
IEEE T RANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS ,
the IEEE T RANSACTIONS ON MULTIMEDIA , the IEEE T RANSACTIONS ON
CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY ,Computer Vision and
Pattern Recognition , and European Conference on Computer Vision. His
research interests include computer vision and machine learning. He received
the Microsoft Research Fellowship in 2010 and the ACM Shanghai YoungScientist Award in 2015.
Yuting Zhang received the B.E. degree in computer
science from Zhejiang University, in 2009, wherehe is currently pursuing the Ph.D. degree with
the Department of Computer Science, advised
by G. Pan. He is currently a Visiting Student withthe Department of Electronic Engineering andComputer Science, University of Michigan, USA.
He was also a Junior Research Assistant with the
Advanced Digital Sciences Center, Singapore, andthe University of Illinois at Urbana‚ÄìChampaign
in 2012. His research interests include machine
learning, computer vision, and in partic ular, application of deep graph model.
Kui Jia received the B.Eng. degree in marine
engineering from Northwestern Polytechnic
University, China, in 2001, the M.Eng. degreein electrical and computer engineering from theNational University of Singapore, in 2003, and the
Ph.D. degree in computer science from Queen Mary,
University of London, London, U.K., in 2007. He iscurrently a Visiting Assistant Professor with the
University of Macau, Macau, China. He also holds
a Research Scientist position with the AdvancedDigital Sciences Center, Singapore. His research
interests are computer vision, machine learning, and image processing.
Jiwen Lu (S‚Äô10‚ÄìM‚Äô11) received the B.Eng. degree
in mechanical engineering and the M.Eng. degree inelectrical engineering from the Xi‚Äôan University of
Technology, Xi‚Äôan, China, and the Ph.D. degree in
electrical engineering from Nanyang TechnologicalUniversity, Singapore. He is currently a ResearchScientist with the Advanced Digital Sciences Cen-
ter, Singapore. His research interests include com-
puter vision, pattern recognition, and machine learn-ing.
He has authored or coauthored over 100 scientiÔ¨Åc
papers in these areas, with 23 papers published in IEEE Transactions journals(IEEE T
RANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTEL -
LIGENCE , the IEEE T RANSACTIONS ON IMAGE PROCESSING , the IEEE
TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY , the IEEE
TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY ,
and IEEE T RANSACTIONS ON MULTIMEDIA ), and 12 papers published in
top-tier computer vision conferences (ICCV/CVPR/ECCV). He serves as an
Area Chair for ICME 2015 and ICB 2015, and a Special Session Chair forVCIP 2015. He was a recipient of the Firs t-Prize National Scholarship and
the National Outstanding Student Awar d from the Ministry of Education of
China in 2002 and 2003, the Best Student Paper Award from the PREMIA of
Singapore in 2012, and the Top 10% Best Paper Award from MMSP 2014.Recently, he has given tutorials at some conferences, such as CVPR 2015,
FG 2015, ACCV 2014, ICME 2014, and IJCB 2014.
Yingying Zhang received the B.E. degree from
Xi‚Äôan Jiaotong University, China, in 2014. He is
currently pursuing the master‚Äôs degree withShanghaiTech University, China. His research
interests are computer vision and machine learning.
"
https://ieeexplore.ieee.org/document/7961822,Nothing found
https://ieeexplore.ieee.org/document/6130508,Nothing found
https://ieeexplore.ieee.org/document/5543262,Nothing found
https://ieeexplore.ieee.org/document/5771374,Nothing found
https://ieeexplore.ieee.org/document/4459336,Nothing found
https://ieeexplore.ieee.org/document/5653653,Nothing found
