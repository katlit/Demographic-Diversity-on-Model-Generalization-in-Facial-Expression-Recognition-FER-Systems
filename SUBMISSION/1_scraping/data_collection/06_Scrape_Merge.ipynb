{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8a45001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afb292dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fer_datasets = [\"CK+\", \"MMI\", \"JAFFE\", \"TFD\", \"FER-2013\", \"AFEW7.0\", \n",
    "    \"SFEW2.0\", \"Multi-PIE\", \"BU-3DFE\", \"Oulu-CASIA\", \n",
    "    \"RaFD\", \"KDEF\", \"EmotioNet\", \"RAF-DB\", \"AffectNet\", \"ExpW\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bcc3528",
   "metadata": {},
   "outputs": [],
   "source": [
    "fer_datasets_LONG = [\n",
    "    \"Extended Cohn-Kanade\", \n",
    "    'MMI Facial Expression',\n",
    "    \"Japanese Female Facial Expression\", \n",
    "    \"Toronto Face Database\",\n",
    "    \"Binghamton University 3D Facial Expression\",\n",
    "    \"Oulu-CASIA\",\n",
    "    \"Radboud Faces Database\",\n",
    "    \"Karolinska Directed Emotional Faces\",\n",
    "    \"Acted Facial Expressions In The Wild\",\n",
    "    \"Static Facial Expression in the Wild\",\n",
    "    \"CMU Multi-PIE\",\n",
    "    \"Affective Faces Database\",\n",
    "    \"Expression in-the-Wild\", \n",
    "    \"Facial Expression Recognition 2013\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d0064f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\"facial expression recognition\", \"FER\", \"deep learning\", \"machine learning\", \"classification\", \"classifier\", \"neural network\", \"CNN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2c52af9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df1: (1482, 36)\n",
      "Shape of df2: (1789, 36)\n",
      "Shape of df3: (2890, 36)\n",
      "Shape of df4: (1066, 36)\n",
      "Shape of df5: (2157, 36)\n",
      "Shape of merged_df: (9384, 36)\n"
     ]
    }
   ],
   "source": [
    "# Define the file paths for the two datasets\n",
    "file_path_1 = 'scholarly_papers_combined_extended.csv'\n",
    "file_path_2 = 'scholarly_papers_combined_extended_2.csv'\n",
    "file_path_3 = 'scholarly_papers_combined_extended_3.csv'\n",
    "file_path_4 = 'scholarly_papers_combined_extended_4.csv'\n",
    "file_path_5 = \"scholarly_papers_combined_extended_FER2013.csv\"\n",
    "\n",
    "# Load the two CSV files into DataFrames\n",
    "df1 = pd.read_csv(file_path_1)\n",
    "df2 = pd.read_csv(file_path_2)\n",
    "df3 = pd.read_csv(file_path_3)\n",
    "df4 = pd.read_csv(file_path_4)\n",
    "df5 = pd.read_csv(file_path_5)\n",
    "\n",
    "\n",
    "# Print the shapes of the individual DataFrames\n",
    "print(f\"Shape of df1: {df1.shape}\")\n",
    "print(f\"Shape of df2: {df2.shape}\")\n",
    "print(f\"Shape of df3: {df3.shape}\")\n",
    "print(f\"Shape of df4: {df4.shape}\")\n",
    "print(f\"Shape of df5: {df5.shape}\")\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "merged_df = pd.concat([df1, df2, df3, df4, df5], ignore_index=True)\n",
    "\n",
    "# Print the shape of the merged DataFrame\n",
    "print(f\"Shape of merged_df: {merged_df.shape}\")\n",
    "\n",
    "#merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0051faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to check if any dataset is in the 'Dataset' column\n",
    "def extract_dataset(row):\n",
    "    for dataset in fer_datasets_LONG:\n",
    "        if dataset.lower() in str(row).lower():\n",
    "            return dataset\n",
    "    return None\n",
    "\n",
    "# Create a function to check if any topic is in the 'Abstract' or 'Title' column\n",
    "def extract_topic(row):\n",
    "    for topic in topics:\n",
    "        if topic.lower() in str(row).lower():\n",
    "            return topic\n",
    "    return None\n",
    "\n",
    "merged_df['Detected_Dataset'] = merged_df['Dataset'].apply(extract_dataset)\n",
    "merged_df['Detected_Topic'] = merged_df['Dataset'].apply(extract_topic)\n",
    "merged_df = merged_df.drop(columns=['Dataset'])\n",
    "#merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dd49262",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8486, 37)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicate rows based on all columns\n",
    "duplicate_rows = merged_df[merged_df.duplicated()]\n",
    "duplicate_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5fb74f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263, 37) (9384, 37)\n"
     ]
    }
   ],
   "source": [
    "df = merged_df.copy()\n",
    "\n",
    "# Function to merge unique datasets or topics\n",
    "def merge_unique_entries(entries):\n",
    "    # Remove None or NaN values and return unique values joined by a comma\n",
    "    return ', '.join(sorted(set([entry for entry in entries if pd.notna(entry)])))\n",
    "\n",
    "# Group by 'Title' and aggregate the 'Detected Dataset', 'Detected Topic' columns, and 'Mentions_' columns\n",
    "df_clean = df.groupby('Title').agg({\n",
    "    'Authors': 'first',  # Keep the first occurrence for non-duplicate columns\n",
    "    'Year': 'first',\n",
    "    'Cited By': 'first',\n",
    "    'Detected_Dataset': merge_unique_entries,  # Merge and remove duplicates in 'Detected Dataset'\n",
    "    'Detected_Topic': merge_unique_entries,    # Merge and remove duplicates in 'Detected Topic'\n",
    "    'Abstract': 'first',  # Keep the first occurrence for 'Abstract'\n",
    "    'DOI': 'first',       # Keep the first occurrence for 'DOI'\n",
    "    'Journal': 'first',   # Keep the first occurrence for 'Journal'\n",
    "    'URL': 'first',       # Keep the first occurrence for 'URL'\n",
    "    \n",
    "    # For 'Mentions_' columns, use the 'any()' function to keep 'True' if any row has 'True'\n",
    "    'Mentions_Accuracy': 'any',\n",
    "    'Mentions_F1': 'any',\n",
    "    'Mentions_Precision': 'any',\n",
    "    'Mentions_Recall': 'any',\n",
    "    'Mentions_Auc': 'any',\n",
    "    'Mentions_Roc': 'any',\n",
    "    'Mentions_Sensitivity': 'any',\n",
    "    'Mentions_Specificity': 'any',\n",
    "    'Mentions_Confusion_matrix': 'any',\n",
    "    'Mentions_Loss_function': 'any',\n",
    "    'Mentions_Cross-entropy': 'any',\n",
    "    'Mentions_Mean_squared_error': 'any',\n",
    "    'Mentions_Overfitting': 'any',\n",
    "    'Mentions_Underfitting': 'any',\n",
    "    'Mentions_Cross-validation': 'any',\n",
    "    'Mentions_Training_time': 'any',\n",
    "    'Mentions_Inference_time': 'any',\n",
    "    'Mentions_Statistical_significance': 'any',\n",
    "    'Mentions_P-value': 'any',\n",
    "    'Mentions_T-test': 'any',\n",
    "    'Mentions_Anova': 'any',\n",
    "    'Mentions_Correlation': 'any',\n",
    "    'Mentions_Regression': 'any',\n",
    "    'Mentions_Baseline_comparison': 'any',\n",
    "    'Mentions_Mae': 'any',\n",
    "    'Mentions_Rmse': 'any',\n",
    "    'Mentions_Bias': 'any'\n",
    "}).reset_index()\n",
    "\n",
    "# Display the shape of the cleaned and original dataframes\n",
    "print(df_clean.shape, merged_df.shape)\n",
    "\n",
    "# Checking for missing values\n",
    "missing_values = df_clean.isnull().sum()\n",
    "#print(missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37cb6862",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv('../Scrapes_ALL.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
