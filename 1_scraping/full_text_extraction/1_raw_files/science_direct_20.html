<html lang="en-US" class="toolbar-stuck"><head>
      <meta name="citation_pii" content="S1319157818303379">
<meta name="citation_issn" content="1319-1578">
<meta name="citation_volume" content="33">
<meta name="citation_lastpage" content="628">
<meta name="citation_issue" content="6">
<meta name="citation_publisher" content="Elsevier">
<meta name="citation_firstpage" content="619">
<meta name="citation_fulltext_world_readable" content="">
<meta name="citation_journal_title" content="Journal of King Saud University - Computer and Information Sciences">
<meta name="citation_type" content="JOUR">
<meta name="citation_doi" content="10.1016/j.jksuci.2018.09.002">
<meta name="dc.identifier" content="10.1016/j.jksuci.2018.09.002">
<meta name="citation_article_type" content="Review article">
<meta property="og:description" content="Human Face expression Recognition is one of the most powerful and challenging tasks in social communication. Generally, face expressions are natural a…">
<meta property="og:image" content="https://ars.els-cdn.com/content/image/1-s2.0-S1319157821X00067-cov150h.gif">
<meta name="citation_title" content="A Survey on Human Face Expression Recognition Techniques">
<meta property="og:title" content="A Survey on Human Face Expression Recognition Techniques">
<meta name="citation_publication_date" content="2021/07/01">
<meta name="citation_online_date" content="2018/09/05">
<meta name="robots" content="INDEX,FOLLOW,NOARCHIVE,NOCACHE,NOODP,NOYDIR">
      <title>A Survey on Human Face Expression Recognition Techniques - ScienceDirect</title>
      <link rel="canonical" href="https://www.sciencedirect.com/science/article/pii/S1319157818303379">
      <meta name="tdm-reservation" content="1">
      <meta name="tdm-policy" content="https://www.elsevier.com/tdm/tdmrep-policy.json">
      <meta property="og:type" content="article">
      <meta name="viewport" content="initial-scale=1">
      <meta name="SDTech" content="Proudly brought to you by the SD Technology team">
      <script async="" src="https://cdn.pendo.io/agent/static/d6c1d995-bc7e-4e53-77f1-2ea4ecbb9565/pendo.js"></script><script type="text/javascript">(function newRelicBrowserProSPA() {
  ;
  window.NREUM || (NREUM = {});
  NREUM.init = {
    privacy: {
      cookies_enabled: false
    },
    ajax: {
      deny_list: ["bam-cell.nr-data.net"]
    }
  };
  ;
  NREUM.loader_config = {
    accountID: "2128461",
    trustKey: "2038175",
    agentID: "1118783207",
    licenseKey: "7ac4127487",
    applicationID: "814813181"
  };
  ;
  NREUM.info = {
    beacon: "bam.nr-data.net",
    errorBeacon: "bam.nr-data.net",
    licenseKey: "7ac4127487",
    applicationID: "814813181",
    sa: 1
  };
  ; /*! For license information please see nr-loader-spa-1.238.0.min.js.LICENSE.txt */
  (() => {
    "use strict";

    var e,
      t,
      r = {
        5763: (e, t, r) => {
          r.d(t, {
            P_: () => f,
            Mt: () => p,
            C5: () => s,
            DL: () => v,
            OP: () => T,
            lF: () => D,
            Yu: () => y,
            Dg: () => h,
            CX: () => c,
            GE: () => b,
            sU: () => _
          });
          var n = r(8632),
            i = r(9567);
          const o = {
              beacon: n.ce.beacon,
              errorBeacon: n.ce.errorBeacon,
              licenseKey: void 0,
              applicationID: void 0,
              sa: void 0,
              queueTime: void 0,
              applicationTime: void 0,
              ttGuid: void 0,
              user: void 0,
              account: void 0,
              product: void 0,
              extra: void 0,
              jsAttributes: {},
              userAttributes: void 0,
              atts: void 0,
              transactionName: void 0,
              tNamePlain: void 0
            },
            a = {};
          function s(e) {
            if (!e) throw new Error("All info objects require an agent identifier!");
            if (!a[e]) throw new Error("Info for ".concat(e, " was never set"));
            return a[e];
          }
          function c(e, t) {
            if (!e) throw new Error("All info objects require an agent identifier!");
            a[e] = (0, i.D)(t, o), (0, n.Qy)(e, a[e], "info");
          }
          var u = r(7056);
          const d = () => {
              const e = {
                blockSelector: "[data-nr-block]",
                maskInputOptions: {
                  password: !0
                }
              };
              return {
                allow_bfcache: !0,
                privacy: {
                  cookies_enabled: !0
                },
                ajax: {
                  deny_list: void 0,
                  block_internal: !0,
                  enabled: !0,
                  harvestTimeSeconds: 10
                },
                distributed_tracing: {
                  enabled: void 0,
                  exclude_newrelic_header: void 0,
                  cors_use_newrelic_header: void 0,
                  cors_use_tracecontext_headers: void 0,
                  allowed_origins: void 0
                },
                session: {
                  domain: void 0,
                  expiresMs: u.oD,
                  inactiveMs: u.Hb
                },
                ssl: void 0,
                obfuscate: void 0,
                jserrors: {
                  enabled: !0,
                  harvestTimeSeconds: 10
                },
                metrics: {
                  enabled: !0
                },
                page_action: {
                  enabled: !0,
                  harvestTimeSeconds: 30
                },
                page_view_event: {
                  enabled: !0
                },
                page_view_timing: {
                  enabled: !0,
                  harvestTimeSeconds: 30,
                  long_task: !1
                },
                session_trace: {
                  enabled: !0,
                  harvestTimeSeconds: 10
                },
                harvest: {
                  tooManyRequestsDelay: 60
                },
                session_replay: {
                  enabled: !1,
                  harvestTimeSeconds: 60,
                  sampleRate: .1,
                  errorSampleRate: .1,
                  maskTextSelector: "*",
                  maskAllInputs: !0,
                  get blockClass() {
                    return "nr-block";
                  },
                  get ignoreClass() {
                    return "nr-ignore";
                  },
                  get maskTextClass() {
                    return "nr-mask";
                  },
                  get blockSelector() {
                    return e.blockSelector;
                  },
                  set blockSelector(t) {
                    e.blockSelector += ",".concat(t);
                  },
                  get maskInputOptions() {
                    return e.maskInputOptions;
                  },
                  set maskInputOptions(t) {
                    e.maskInputOptions = {
                      ...t,
                      password: !0
                    };
                  }
                },
                spa: {
                  enabled: !0,
                  harvestTimeSeconds: 10
                }
              };
            },
            l = {};
          function f(e) {
            if (!e) throw new Error("All configuration objects require an agent identifier!");
            if (!l[e]) throw new Error("Configuration for ".concat(e, " was never set"));
            return l[e];
          }
          function h(e, t) {
            if (!e) throw new Error("All configuration objects require an agent identifier!");
            l[e] = (0, i.D)(t, d()), (0, n.Qy)(e, l[e], "config");
          }
          function p(e, t) {
            if (!e) throw new Error("All configuration objects require an agent identifier!");
            var r = f(e);
            if (r) {
              for (var n = t.split("."), i = 0; i < n.length - 1; i++) if ("object" != typeof (r = r[n[i]])) return;
              r = r[n[n.length - 1]];
            }
            return r;
          }
          const g = {
              accountID: void 0,
              trustKey: void 0,
              agentID: void 0,
              licenseKey: void 0,
              applicationID: void 0,
              xpid: void 0
            },
            m = {};
          function v(e) {
            if (!e) throw new Error("All loader-config objects require an agent identifier!");
            if (!m[e]) throw new Error("LoaderConfig for ".concat(e, " was never set"));
            return m[e];
          }
          function b(e, t) {
            if (!e) throw new Error("All loader-config objects require an agent identifier!");
            m[e] = (0, i.D)(t, g), (0, n.Qy)(e, m[e], "loader_config");
          }
          const y = (0, n.mF)().o;
          var w = r(385),
            A = r(6818);
          const x = {
              buildEnv: A.Re,
              bytesSent: {},
              queryBytesSent: {},
              customTransaction: void 0,
              disabled: !1,
              distMethod: A.gF,
              isolatedBacklog: !1,
              loaderType: void 0,
              maxBytes: 3e4,
              offset: Math.floor(w._A?.performance?.timeOrigin || w._A?.performance?.timing?.navigationStart || Date.now()),
              onerror: void 0,
              origin: "" + w._A.location,
              ptid: void 0,
              releaseIds: {},
              session: void 0,
              xhrWrappable: "function" == typeof w._A.XMLHttpRequest?.prototype?.addEventListener,
              version: A.q4,
              denyList: void 0
            },
            E = {};
          function T(e) {
            if (!e) throw new Error("All runtime objects require an agent identifier!");
            if (!E[e]) throw new Error("Runtime for ".concat(e, " was never set"));
            return E[e];
          }
          function _(e, t) {
            if (!e) throw new Error("All runtime objects require an agent identifier!");
            E[e] = (0, i.D)(t, x), (0, n.Qy)(e, E[e], "runtime");
          }
          function D(e) {
            return function (e) {
              try {
                const t = s(e);
                return !!t.licenseKey && !!t.errorBeacon && !!t.applicationID;
              } catch (e) {
                return !1;
              }
            }(e);
          }
        },
        9567: (e, t, r) => {
          r.d(t, {
            D: () => i
          });
          var n = r(50);
          function i(e, t) {
            try {
              if (!e || "object" != typeof e) return (0, n.Z)("Setting a Configurable requires an object as input");
              if (!t || "object" != typeof t) return (0, n.Z)("Setting a Configurable requires a model to set its initial properties");
              const r = Object.create(Object.getPrototypeOf(t), Object.getOwnPropertyDescriptors(t)),
                o = 0 === Object.keys(r).length ? e : r;
              for (let a in o) if (void 0 !== e[a]) try {
                "object" == typeof e[a] && "object" == typeof t[a] ? r[a] = i(e[a], t[a]) : r[a] = e[a];
              } catch (e) {
                (0, n.Z)("An error occurred while setting a property of a Configurable", e);
              }
              return r;
            } catch (e) {
              (0, n.Z)("An error occured while setting a Configurable", e);
            }
          }
        },
        6818: (e, t, r) => {
          r.d(t, {
            Re: () => i,
            gF: () => o,
            q4: () => n
          });
          const n = "1.238.0",
            i = "PROD",
            o = "CDN";
        },
        385: (e, t, r) => {
          r.d(t, {
            FN: () => a,
            IF: () => u,
            Nk: () => l,
            Tt: () => s,
            _A: () => o,
            il: () => n,
            pL: () => c,
            v6: () => i,
            w1: () => d
          });
          const n = "undefined" != typeof window && !!window.document,
            i = "undefined" != typeof WorkerGlobalScope && ("undefined" != typeof self && self instanceof WorkerGlobalScope && self.navigator instanceof WorkerNavigator || "undefined" != typeof globalThis && globalThis instanceof WorkerGlobalScope && globalThis.navigator instanceof WorkerNavigator),
            o = n ? window : "undefined" != typeof WorkerGlobalScope && ("undefined" != typeof self && self instanceof WorkerGlobalScope && self || "undefined" != typeof globalThis && globalThis instanceof WorkerGlobalScope && globalThis),
            a = "" + o?.location,
            s = /iPad|iPhone|iPod/.test(navigator.userAgent),
            c = s && "undefined" == typeof SharedWorker,
            u = (() => {
              const e = navigator.userAgent.match(/Firefox[/\s](\d+\.\d+)/);
              return Array.isArray(e) && e.length >= 2 ? +e[1] : 0;
            })(),
            d = Boolean(n && window.document.documentMode),
            l = !!navigator.sendBeacon;
        },
        1117: (e, t, r) => {
          r.d(t, {
            w: () => o
          });
          var n = r(50);
          const i = {
            agentIdentifier: "",
            ee: void 0
          };
          class o {
            constructor(e) {
              try {
                if ("object" != typeof e) return (0, n.Z)("shared context requires an object as input");
                this.sharedContext = {}, Object.assign(this.sharedContext, i), Object.entries(e).forEach(e => {
                  let [t, r] = e;
                  Object.keys(i).includes(t) && (this.sharedContext[t] = r);
                });
              } catch (e) {
                (0, n.Z)("An error occured while setting SharedContext", e);
              }
            }
          }
        },
        8e3: (e, t, r) => {
          r.d(t, {
            L: () => d,
            R: () => c
          });
          var n = r(8325),
            i = r(1284),
            o = r(4322),
            a = r(3325);
          const s = {};
          function c(e, t) {
            const r = {
              staged: !1,
              priority: a.p[t] || 0
            };
            u(e), s[e].get(t) || s[e].set(t, r);
          }
          function u(e) {
            e && (s[e] || (s[e] = new Map()));
          }
          function d() {
            let e = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : "",
              t = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : "feature";
            if (u(e), !e || !s[e].get(t)) return a(t);
            s[e].get(t).staged = !0;
            const r = [...s[e]];
            function a(t) {
              const r = e ? n.ee.get(e) : n.ee,
                a = o.X.handlers;
              if (r.backlog && a) {
                var s = r.backlog[t],
                  c = a[t];
                if (c) {
                  for (var u = 0; s && u < s.length; ++u) l(s[u], c);
                  (0, i.D)(c, function (e, t) {
                    (0, i.D)(t, function (t, r) {
                      r[0].on(e, r[1]);
                    });
                  });
                }
                delete a[t], r.backlog[t] = null, r.emit("drain-" + t, []);
              }
            }
            r.every(e => {
              let [t, r] = e;
              return r.staged;
            }) && (r.sort((e, t) => e[1].priority - t[1].priority), r.forEach(e => {
              let [t] = e;
              a(t);
            }));
          }
          function l(e, t) {
            var r = e[1];
            (0, i.D)(t[r], function (t, r) {
              var n = e[0];
              if (r[0] === n) {
                var i = r[1],
                  o = e[3],
                  a = e[2];
                i.apply(o, a);
              }
            });
          }
        },
        8325: (e, t, r) => {
          r.d(t, {
            A: () => c,
            ee: () => u
          });
          var n = r(8632),
            i = r(2210),
            o = r(5763);
          class a {
            constructor(e) {
              this.contextId = e;
            }
          }
          var s = r(3117);
          const c = "nr@context:".concat(s.a),
            u = function e(t, r) {
              var n = {},
                s = {},
                d = {},
                f = !1;
              try {
                f = 16 === r.length && (0, o.OP)(r).isolatedBacklog;
              } catch (e) {}
              var h = {
                on: g,
                addEventListener: g,
                removeEventListener: function (e, t) {
                  var r = n[e];
                  if (!r) return;
                  for (var i = 0; i < r.length; i++) r[i] === t && r.splice(i, 1);
                },
                emit: function (e, r, n, i, o) {
                  !1 !== o && (o = !0);
                  if (u.aborted && !i) return;
                  t && o && t.emit(e, r, n);
                  for (var a = p(n), c = m(e), d = c.length, l = 0; l < d; l++) c[l].apply(a, r);
                  var f = b()[s[e]];
                  f && f.push([h, e, r, a]);
                  return a;
                },
                get: v,
                listeners: m,
                context: p,
                buffer: function (e, t) {
                  const r = b();
                  if (t = t || "feature", h.aborted) return;
                  Object.entries(e || {}).forEach(e => {
                    let [n, i] = e;
                    s[i] = t, t in r || (r[t] = []);
                  });
                },
                abort: l,
                aborted: !1,
                isBuffering: function (e) {
                  return !!b()[s[e]];
                },
                debugId: r,
                backlog: f ? {} : t && "object" == typeof t.backlog ? t.backlog : {}
              };
              return h;
              function p(e) {
                return e && e instanceof a ? e : e ? (0, i.X)(e, c, () => new a(c)) : new a(c);
              }
              function g(e, t) {
                n[e] = m(e).concat(t);
              }
              function m(e) {
                return n[e] || [];
              }
              function v(t) {
                return d[t] = d[t] || e(h, t);
              }
              function b() {
                return h.backlog;
              }
            }(void 0, "globalEE"),
            d = (0, n.fP)();
          function l() {
            u.aborted = !0, u.backlog = {};
          }
          d.ee || (d.ee = u);
        },
        5546: (e, t, r) => {
          r.d(t, {
            E: () => n,
            p: () => i
          });
          var n = r(8325).ee.get("handle");
          function i(e, t, r, i, o) {
            o ? (o.buffer([e], i), o.emit(e, t, r)) : (n.buffer([e], i), n.emit(e, t, r));
          }
        },
        4322: (e, t, r) => {
          r.d(t, {
            X: () => o
          });
          var n = r(5546);
          o.on = a;
          var i = o.handlers = {};
          function o(e, t, r, o) {
            a(o || n.E, i, e, t, r);
          }
          function a(e, t, r, i, o) {
            o || (o = "feature"), e || (e = n.E);
            var a = t[o] = t[o] || {};
            (a[r] = a[r] || []).push([e, i]);
          }
        },
        3239: (e, t, r) => {
          r.d(t, {
            bP: () => s,
            iz: () => c,
            m$: () => a
          });
          var n = r(385);
          let i = !1,
            o = !1;
          try {
            const e = {
              get passive() {
                return i = !0, !1;
              },
              get signal() {
                return o = !0, !1;
              }
            };
            n._A.addEventListener("test", null, e), n._A.removeEventListener("test", null, e);
          } catch (e) {}
          function a(e, t) {
            return i || o ? {
              capture: !!e,
              passive: i,
              signal: t
            } : !!e;
          }
          function s(e, t) {
            let r = arguments.length > 2 && void 0 !== arguments[2] && arguments[2],
              n = arguments.length > 3 ? arguments[3] : void 0;
            window.addEventListener(e, t, a(r, n));
          }
          function c(e, t) {
            let r = arguments.length > 2 && void 0 !== arguments[2] && arguments[2],
              n = arguments.length > 3 ? arguments[3] : void 0;
            document.addEventListener(e, t, a(r, n));
          }
        },
        3117: (e, t, r) => {
          r.d(t, {
            a: () => n
          });
          const n = (0, r(4402).Rl)();
        },
        4402: (e, t, r) => {
          r.d(t, {
            Ht: () => u,
            M: () => c,
            Rl: () => a,
            ky: () => s
          });
          var n = r(385);
          const i = "xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx";
          function o(e, t) {
            return e ? 15 & e[t] : 16 * Math.random() | 0;
          }
          function a() {
            const e = n._A?.crypto || n._A?.msCrypto;
            let t,
              r = 0;
            return e && e.getRandomValues && (t = e.getRandomValues(new Uint8Array(31))), i.split("").map(e => "x" === e ? o(t, ++r).toString(16) : "y" === e ? (3 & o() | 8).toString(16) : e).join("");
          }
          function s(e) {
            const t = n._A?.crypto || n._A?.msCrypto;
            let r,
              i = 0;
            t && t.getRandomValues && (r = t.getRandomValues(new Uint8Array(31)));
            const a = [];
            for (var s = 0; s < e; s++) a.push(o(r, ++i).toString(16));
            return a.join("");
          }
          function c() {
            return s(16);
          }
          function u() {
            return s(32);
          }
        },
        7056: (e, t, r) => {
          r.d(t, {
            Bq: () => n,
            Hb: () => o,
            oD: () => i
          });
          const n = "NRBA",
            i = 144e5,
            o = 18e5;
        },
        7894: (e, t, r) => {
          function n() {
            return Math.round(performance.now());
          }
          r.d(t, {
            z: () => n
          });
        },
        7243: (e, t, r) => {
          r.d(t, {
            e: () => o
          });
          var n = r(385),
            i = {};
          function o(e) {
            if (e in i) return i[e];
            if (0 === (e || "").indexOf("data:")) return {
              protocol: "data"
            };
            let t;
            var r = n._A?.location,
              o = {};
            if (n.il) t = document.createElement("a"), t.href = e;else try {
              t = new URL(e, r.href);
            } catch (e) {
              return o;
            }
            o.port = t.port;
            var a = t.href.split("://");
            !o.port && a[1] && (o.port = a[1].split("/")[0].split("@").pop().split(":")[1]), o.port && "0" !== o.port || (o.port = "https" === a[0] ? "443" : "80"), o.hostname = t.hostname || r.hostname, o.pathname = t.pathname, o.protocol = a[0], "/" !== o.pathname.charAt(0) && (o.pathname = "/" + o.pathname);
            var s = !t.protocol || ":" === t.protocol || t.protocol === r.protocol,
              c = t.hostname === r.hostname && t.port === r.port;
            return o.sameOrigin = s && (!t.hostname || c), "/" === o.pathname && (i[e] = o), o;
          }
        },
        50: (e, t, r) => {
          function n(e, t) {
            "function" == typeof console.warn && (console.warn("New Relic: ".concat(e)), t && console.warn(t));
          }
          r.d(t, {
            Z: () => n
          });
        },
        2587: (e, t, r) => {
          r.d(t, {
            N: () => c,
            T: () => u
          });
          var n = r(8325),
            i = r(5546),
            o = r(8e3),
            a = r(3325);
          const s = {
            stn: [a.D.sessionTrace],
            err: [a.D.jserrors, a.D.metrics],
            ins: [a.D.pageAction],
            spa: [a.D.spa],
            sr: [a.D.sessionReplay, a.D.sessionTrace]
          };
          function c(e, t) {
            const r = n.ee.get(t);
            e && "object" == typeof e && (Object.entries(e).forEach(e => {
              let [t, n] = e;
              void 0 === u[t] && (s[t] ? s[t].forEach(e => {
                n ? (0, i.p)("feat-" + t, [], void 0, e, r) : (0, i.p)("block-" + t, [], void 0, e, r), (0, i.p)("rumresp-" + t, [Boolean(n)], void 0, e, r);
              }) : n && (0, i.p)("feat-" + t, [], void 0, void 0, r), u[t] = Boolean(n));
            }), Object.keys(s).forEach(e => {
              void 0 === u[e] && (s[e]?.forEach(t => (0, i.p)("rumresp-" + e, [!1], void 0, t, r)), u[e] = !1);
            }), (0, o.L)(t, a.D.pageViewEvent));
          }
          const u = {};
        },
        2210: (e, t, r) => {
          r.d(t, {
            X: () => i
          });
          var n = Object.prototype.hasOwnProperty;
          function i(e, t, r) {
            if (n.call(e, t)) return e[t];
            var i = r();
            if (Object.defineProperty && Object.keys) try {
              return Object.defineProperty(e, t, {
                value: i,
                writable: !0,
                enumerable: !1
              }), i;
            } catch (e) {}
            return e[t] = i, i;
          }
        },
        1284: (e, t, r) => {
          r.d(t, {
            D: () => n
          });
          const n = (e, t) => Object.entries(e || {}).map(e => {
            let [r, n] = e;
            return t(r, n);
          });
        },
        4351: (e, t, r) => {
          r.d(t, {
            P: () => o
          });
          var n = r(8325);
          const i = () => {
            const e = new WeakSet();
            return (t, r) => {
              if ("object" == typeof r && null !== r) {
                if (e.has(r)) return;
                e.add(r);
              }
              return r;
            };
          };
          function o(e) {
            try {
              return JSON.stringify(e, i());
            } catch (e) {
              try {
                n.ee.emit("internal-error", [e]);
              } catch (e) {}
            }
          }
        },
        3960: (e, t, r) => {
          r.d(t, {
            K: () => a,
            b: () => o
          });
          var n = r(3239);
          function i() {
            return "undefined" == typeof document || "complete" === document.readyState;
          }
          function o(e, t) {
            if (i()) return e();
            (0, n.bP)("load", e, t);
          }
          function a(e) {
            if (i()) return e();
            (0, n.iz)("DOMContentLoaded", e);
          }
        },
        8632: (e, t, r) => {
          r.d(t, {
            EZ: () => u,
            Qy: () => c,
            ce: () => o,
            fP: () => a,
            gG: () => d,
            mF: () => s
          });
          var n = r(7894),
            i = r(385);
          const o = {
            beacon: "bam.nr-data.net",
            errorBeacon: "bam.nr-data.net"
          };
          function a() {
            return i._A.NREUM || (i._A.NREUM = {}), void 0 === i._A.newrelic && (i._A.newrelic = i._A.NREUM), i._A.NREUM;
          }
          function s() {
            let e = a();
            return e.o || (e.o = {
              ST: i._A.setTimeout,
              SI: i._A.setImmediate,
              CT: i._A.clearTimeout,
              XHR: i._A.XMLHttpRequest,
              REQ: i._A.Request,
              EV: i._A.Event,
              PR: i._A.Promise,
              MO: i._A.MutationObserver,
              FETCH: i._A.fetch
            }), e;
          }
          function c(e, t, r) {
            let i = a();
            const o = i.initializedAgents || {},
              s = o[e] || {};
            return Object.keys(s).length || (s.initializedAt = {
              ms: (0, n.z)(),
              date: new Date()
            }), i.initializedAgents = {
              ...o,
              [e]: {
                ...s,
                [r]: t
              }
            }, i;
          }
          function u(e, t) {
            a()[e] = t;
          }
          function d() {
            return function () {
              let e = a();
              const t = e.info || {};
              e.info = {
                beacon: o.beacon,
                errorBeacon: o.errorBeacon,
                ...t
              };
            }(), function () {
              let e = a();
              const t = e.init || {};
              e.init = {
                ...t
              };
            }(), s(), function () {
              let e = a();
              const t = e.loader_config || {};
              e.loader_config = {
                ...t
              };
            }(), a();
          }
        },
        7956: (e, t, r) => {
          r.d(t, {
            N: () => i
          });
          var n = r(3239);
          function i(e) {
            let t = arguments.length > 1 && void 0 !== arguments[1] && arguments[1],
              r = arguments.length > 2 ? arguments[2] : void 0,
              i = arguments.length > 3 ? arguments[3] : void 0;
            return void (0, n.iz)("visibilitychange", function () {
              if (t) return void ("hidden" == document.visibilityState && e());
              e(document.visibilityState);
            }, r, i);
          }
        },
        1214: (e, t, r) => {
          r.d(t, {
            em: () => b,
            u5: () => j,
            QU: () => O,
            _L: () => I,
            Gm: () => H,
            Lg: () => L,
            BV: () => G,
            Kf: () => K
          });
          var n = r(8325),
            i = r(3117);
          const o = "nr@original:".concat(i.a);
          var a = Object.prototype.hasOwnProperty,
            s = !1;
          function c(e, t) {
            return e || (e = n.ee), r.inPlace = function (e, t, n, i, o) {
              n || (n = "");
              const a = "-" === n.charAt(0);
              for (let s = 0; s < t.length; s++) {
                const c = t[s],
                  u = e[c];
                d(u) || (e[c] = r(u, a ? c + n : n, i, c, o));
              }
            }, r.flag = o, r;
            function r(t, r, n, s, c) {
              return d(t) ? t : (r || (r = ""), nrWrapper[o] = t, function (e, t, r) {
                if (Object.defineProperty && Object.keys) try {
                  return Object.keys(e).forEach(function (r) {
                    Object.defineProperty(t, r, {
                      get: function () {
                        return e[r];
                      },
                      set: function (t) {
                        return e[r] = t, t;
                      }
                    });
                  }), t;
                } catch (e) {
                  u([e], r);
                }
                for (var n in e) a.call(e, n) && (t[n] = e[n]);
              }(t, nrWrapper, e), nrWrapper);
              function nrWrapper() {
                var o, a, d, l;
                try {
                  a = this, o = [...arguments], d = "function" == typeof n ? n(o, a) : n || {};
                } catch (t) {
                  u([t, "", [o, a, s], d], e);
                }
                i(r + "start", [o, a, s], d, c);
                try {
                  return l = t.apply(a, o);
                } catch (e) {
                  throw i(r + "err", [o, a, e], d, c), e;
                } finally {
                  i(r + "end", [o, a, l], d, c);
                }
              }
            }
            function i(r, n, i, o) {
              if (!s || t) {
                var a = s;
                s = !0;
                try {
                  e.emit(r, n, i, t, o);
                } catch (t) {
                  u([t, r, n, i], e);
                }
                s = a;
              }
            }
          }
          function u(e, t) {
            t || (t = n.ee);
            try {
              t.emit("internal-error", e);
            } catch (e) {}
          }
          function d(e) {
            return !(e && e instanceof Function && e.apply && !e[o]);
          }
          var l = r(2210),
            f = r(385);
          const h = {},
            p = f._A.XMLHttpRequest,
            g = "addEventListener",
            m = "removeEventListener",
            v = "nr@wrapped:".concat(n.A);
          function b(e) {
            var t = function (e) {
              return (e || n.ee).get("events");
            }(e);
            if (h[t.debugId]++) return t;
            h[t.debugId] = 1;
            var r = c(t, !0);
            function i(e) {
              r.inPlace(e, [g, m], "-", o);
            }
            function o(e, t) {
              return e[1];
            }
            return "getPrototypeOf" in Object && (f.il && y(document, i), y(f._A, i), y(p.prototype, i)), t.on(g + "-start", function (e, t) {
              var n = e[1];
              if (null !== n && ("function" == typeof n || "object" == typeof n)) {
                var i = (0, l.X)(n, v, function () {
                  var e = {
                    object: function () {
                      if ("function" != typeof n.handleEvent) return;
                      return n.handleEvent.apply(n, arguments);
                    },
                    function: n
                  }[typeof n];
                  return e ? r(e, "fn-", null, e.name || "anonymous") : n;
                });
                this.wrapped = e[1] = i;
              }
            }), t.on(m + "-start", function (e) {
              e[1] = this.wrapped || e[1];
            }), t;
          }
          function y(e, t) {
            let r = e;
            for (; "object" == typeof r && !Object.prototype.hasOwnProperty.call(r, g);) r = Object.getPrototypeOf(r);
            for (var n = arguments.length, i = new Array(n > 2 ? n - 2 : 0), o = 2; o < n; o++) i[o - 2] = arguments[o];
            r && t(r, ...i);
          }
          var w = "fetch-",
            A = w + "body-",
            x = ["arrayBuffer", "blob", "json", "text", "formData"],
            E = f._A.Request,
            T = f._A.Response,
            _ = "prototype";
          const D = {};
          function j(e) {
            const t = function (e) {
              return (e || n.ee).get("fetch");
            }(e);
            if (!(E && T && f._A.fetch)) return t;
            if (D[t.debugId]++) return t;
            function r(e, r, i) {
              var o = e[r];
              "function" == typeof o && (e[r] = function () {
                var e,
                  r = [...arguments],
                  a = {};
                t.emit(i + "before-start", [r], a), a[n.A] && a[n.A].dt && (e = a[n.A].dt);
                var s = o.apply(this, r);
                return t.emit(i + "start", [r, e], s), s.then(function (e) {
                  return t.emit(i + "end", [null, e], s), e;
                }, function (e) {
                  throw t.emit(i + "end", [e], s), e;
                });
              });
            }
            return D[t.debugId] = 1, x.forEach(e => {
              r(E[_], e, A), r(T[_], e, A);
            }), r(f._A, "fetch", w), t.on(w + "end", function (e, r) {
              var n = this;
              if (r) {
                var i = r.headers.get("content-length");
                null !== i && (n.rxSize = i), t.emit(w + "done", [null, r], n);
              } else t.emit(w + "done", [e], n);
            }), t;
          }
          const C = {},
            N = ["pushState", "replaceState"];
          function O(e) {
            const t = function (e) {
              return (e || n.ee).get("history");
            }(e);
            return !f.il || C[t.debugId]++ || (C[t.debugId] = 1, c(t).inPlace(window.history, N, "-")), t;
          }
          var S = r(3239);
          const P = {},
            R = ["appendChild", "insertBefore", "replaceChild"];
          function I(e) {
            const t = function (e) {
              return (e || n.ee).get("jsonp");
            }(e);
            if (!f.il || P[t.debugId]) return t;
            P[t.debugId] = !0;
            var r = c(t),
              i = /[?&](?:callback|cb)=([^&#]+)/,
              o = /(.*)\.([^.]+)/,
              a = /^(\w+)(\.|$)(.*)$/;
            function s(e, t) {
              if (!e) return t;
              const r = e.match(a),
                n = r[1];
              return s(r[3], t[n]);
            }
            return r.inPlace(Node.prototype, R, "dom-"), t.on("dom-start", function (e) {
              !function (e) {
                if (!e || "string" != typeof e.nodeName || "script" !== e.nodeName.toLowerCase()) return;
                if ("function" != typeof e.addEventListener) return;
                var n = (a = e.src, c = a.match(i), c ? c[1] : null);
                var a, c;
                if (!n) return;
                var u = function (e) {
                  var t = e.match(o);
                  if (t && t.length >= 3) return {
                    key: t[2],
                    parent: s(t[1], window)
                  };
                  return {
                    key: e,
                    parent: window
                  };
                }(n);
                if ("function" != typeof u.parent[u.key]) return;
                var d = {};
                function l() {
                  t.emit("jsonp-end", [], d), e.removeEventListener("load", l, (0, S.m$)(!1)), e.removeEventListener("error", f, (0, S.m$)(!1));
                }
                function f() {
                  t.emit("jsonp-error", [], d), t.emit("jsonp-end", [], d), e.removeEventListener("load", l, (0, S.m$)(!1)), e.removeEventListener("error", f, (0, S.m$)(!1));
                }
                r.inPlace(u.parent, [u.key], "cb-", d), e.addEventListener("load", l, (0, S.m$)(!1)), e.addEventListener("error", f, (0, S.m$)(!1)), t.emit("new-jsonp", [e.src], d);
              }(e[0]);
            }), t;
          }
          const k = {};
          function H(e) {
            const t = function (e) {
              return (e || n.ee).get("mutation");
            }(e);
            if (!f.il || k[t.debugId]) return t;
            k[t.debugId] = !0;
            var r = c(t),
              i = f._A.MutationObserver;
            return i && (window.MutationObserver = function (e) {
              return this instanceof i ? new i(r(e, "fn-")) : i.apply(this, arguments);
            }, MutationObserver.prototype = i.prototype), t;
          }
          const z = {};
          function L(e) {
            const t = function (e) {
              return (e || n.ee).get("promise");
            }(e);
            if (z[t.debugId]) return t;
            z[t.debugId] = !0;
            var r = t.context,
              i = c(t),
              a = f._A.Promise;
            return a && function () {
              function e(r) {
                var n = t.context(),
                  o = i(r, "executor-", n, null, !1);
                const s = Reflect.construct(a, [o], e);
                return t.context(s).getCtx = function () {
                  return n;
                }, s;
              }
              f._A.Promise = e, Object.defineProperty(e, "name", {
                value: "Promise"
              }), e.toString = function () {
                return a.toString();
              }, Object.setPrototypeOf(e, a), ["all", "race"].forEach(function (r) {
                const n = a[r];
                e[r] = function (e) {
                  let i = !1;
                  [...(e || [])].forEach(e => {
                    this.resolve(e).then(a("all" === r), a(!1));
                  });
                  const o = n.apply(this, arguments);
                  return o;
                  function a(e) {
                    return function () {
                      t.emit("propagate", [null, !i], o, !1, !1), i = i || !e;
                    };
                  }
                };
              }), ["resolve", "reject"].forEach(function (r) {
                const n = a[r];
                e[r] = function (e) {
                  const r = n.apply(this, arguments);
                  return e !== r && t.emit("propagate", [e, !0], r, !1, !1), r;
                };
              }), e.prototype = a.prototype;
              const n = a.prototype.then;
              a.prototype.then = function () {
                var e = this,
                  o = r(e);
                o.promise = e;
                for (var a = arguments.length, s = new Array(a), c = 0; c < a; c++) s[c] = arguments[c];
                s[0] = i(s[0], "cb-", o, null, !1), s[1] = i(s[1], "cb-", o, null, !1);
                const u = n.apply(this, s);
                return o.nextPromise = u, t.emit("propagate", [e, !0], u, !1, !1), u;
              }, a.prototype.then[o] = n, t.on("executor-start", function (e) {
                e[0] = i(e[0], "resolve-", this, null, !1), e[1] = i(e[1], "resolve-", this, null, !1);
              }), t.on("executor-err", function (e, t, r) {
                e[1](r);
              }), t.on("cb-end", function (e, r, n) {
                t.emit("propagate", [n, !0], this.nextPromise, !1, !1);
              }), t.on("propagate", function (e, r, n) {
                this.getCtx && !r || (this.getCtx = function () {
                  if (e instanceof Promise) var r = t.context(e);
                  return r && r.getCtx ? r.getCtx() : this;
                });
              });
            }(), t;
          }
          const M = {},
            B = "setTimeout",
            F = "setInterval",
            U = "clearTimeout",
            q = "-start",
            Z = "-",
            V = [B, "setImmediate", F, U, "clearImmediate"];
          function G(e) {
            const t = function (e) {
              return (e || n.ee).get("timer");
            }(e);
            if (M[t.debugId]++) return t;
            M[t.debugId] = 1;
            var r = c(t);
            return r.inPlace(f._A, V.slice(0, 2), B + Z), r.inPlace(f._A, V.slice(2, 3), F + Z), r.inPlace(f._A, V.slice(3), U + Z), t.on(F + q, function (e, t, n) {
              e[0] = r(e[0], "fn-", null, n);
            }), t.on(B + q, function (e, t, n) {
              this.method = n, this.timerDuration = isNaN(e[1]) ? 0 : +e[1], e[0] = r(e[0], "fn-", this, n);
            }), t;
          }
          var W = r(50);
          const X = {},
            Q = ["open", "send"];
          function K(e) {
            var t = e || n.ee;
            const r = function (e) {
              return (e || n.ee).get("xhr");
            }(t);
            if (X[r.debugId]++) return r;
            X[r.debugId] = 1, b(t);
            var i = c(r),
              o = f._A.XMLHttpRequest,
              a = f._A.MutationObserver,
              s = f._A.Promise,
              u = f._A.setInterval,
              d = "readystatechange",
              l = ["onload", "onerror", "onabort", "onloadstart", "onloadend", "onprogress", "ontimeout"],
              h = [],
              p = f._A.XMLHttpRequest = function (e) {
                const t = new o(e),
                  n = r.context(t);
                try {
                  r.emit("new-xhr", [t], n), t.addEventListener(d, (a = n, function () {
                    var e = this;
                    e.readyState > 3 && !a.resolved && (a.resolved = !0, r.emit("xhr-resolved", [], e)), i.inPlace(e, l, "fn-", A);
                  }), (0, S.m$)(!1));
                } catch (e) {
                  (0, W.Z)("An error occurred while intercepting XHR", e);
                  try {
                    r.emit("internal-error", [e]);
                  } catch (e) {}
                }
                var a;
                return t;
              };
            function g(e, t) {
              i.inPlace(t, ["onreadystatechange"], "fn-", A);
            }
            if (function (e, t) {
              for (var r in e) t[r] = e[r];
            }(o, p), p.prototype = o.prototype, i.inPlace(p.prototype, Q, "-xhr-", A), r.on("send-xhr-start", function (e, t) {
              g(e, t), function (e) {
                h.push(e), a && (m ? m.then(w) : u ? u(w) : (v = -v, y.data = v));
              }(t);
            }), r.on("open-xhr-start", g), a) {
              var m = s && s.resolve();
              if (!u && !s) {
                var v = 1,
                  y = document.createTextNode(v);
                new a(w).observe(y, {
                  characterData: !0
                });
              }
            } else t.on("fn-end", function (e) {
              e[0] && e[0].type === d || w();
            });
            function w() {
              for (var e = 0; e < h.length; e++) g(0, h[e]);
              h.length && (h = []);
            }
            function A(e, t) {
              return t;
            }
            return r;
          }
        },
        7825: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.ajax;
        },
        6660: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.jserrors;
        },
        3081: (e, t, r) => {
          r.d(t, {
            gF: () => o,
            mY: () => i,
            t9: () => n,
            vz: () => s,
            xS: () => a
          });
          const n = r(3325).D.metrics,
            i = "sm",
            o = "cm",
            a = "storeSupportabilityMetrics",
            s = "storeEventMetrics";
        },
        4649: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.pageAction;
        },
        7633: (e, t, r) => {
          r.d(t, {
            Dz: () => i,
            OJ: () => a,
            qw: () => o,
            t9: () => n
          });
          const n = r(3325).D.pageViewEvent,
            i = "firstbyte",
            o = "domcontent",
            a = "windowload";
        },
        9251: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.pageViewTiming;
        },
        3614: (e, t, r) => {
          r.d(t, {
            BST_RESOURCE: () => i,
            END: () => s,
            FEATURE_NAME: () => n,
            FN_END: () => u,
            FN_START: () => c,
            PUSH_STATE: () => d,
            RESOURCE: () => o,
            START: () => a
          });
          const n = r(3325).D.sessionTrace,
            i = "bstResource",
            o = "resource",
            a = "-start",
            s = "-end",
            c = "fn" + a,
            u = "fn" + s,
            d = "pushState";
        },
        7836: (e, t, r) => {
          r.d(t, {
            BODY: () => x,
            CB_END: () => E,
            CB_START: () => u,
            END: () => A,
            FEATURE_NAME: () => i,
            FETCH: () => _,
            FETCH_BODY: () => v,
            FETCH_DONE: () => m,
            FETCH_START: () => g,
            FN_END: () => c,
            FN_START: () => s,
            INTERACTION: () => f,
            INTERACTION_API: () => d,
            INTERACTION_EVENTS: () => o,
            JSONP_END: () => b,
            JSONP_NODE: () => p,
            JS_TIME: () => T,
            MAX_TIMER_BUDGET: () => a,
            REMAINING: () => l,
            SPA_NODE: () => h,
            START: () => w,
            originalSetTimeout: () => y
          });
          var n = r(5763);
          const i = r(3325).D.spa,
            o = ["click", "submit", "keypress", "keydown", "keyup", "change"],
            a = 999,
            s = "fn-start",
            c = "fn-end",
            u = "cb-start",
            d = "api-ixn-",
            l = "remaining",
            f = "interaction",
            h = "spaNode",
            p = "jsonpNode",
            g = "fetch-start",
            m = "fetch-done",
            v = "fetch-body-",
            b = "jsonp-end",
            y = n.Yu.ST,
            w = "-start",
            A = "-end",
            x = "-body",
            E = "cb" + A,
            T = "jsTime",
            _ = "fetch";
        },
        5938: (e, t, r) => {
          r.d(t, {
            W: () => o
          });
          var n = r(5763),
            i = r(8325);
          class o {
            constructor(e, t, r) {
              this.agentIdentifier = e, this.aggregator = t, this.ee = i.ee.get(e, (0, n.OP)(this.agentIdentifier).isolatedBacklog), this.featureName = r, this.blocked = !1;
            }
          }
        },
        9144: (e, t, r) => {
          r.d(t, {
            j: () => m
          });
          var n = r(3325),
            i = r(5763),
            o = r(5546),
            a = r(8325),
            s = r(7894),
            c = r(8e3),
            u = r(3960),
            d = r(385),
            l = r(50),
            f = r(3081),
            h = r(8632);
          function p() {
            const e = (0, h.gG)();
            ["setErrorHandler", "finished", "addToTrace", "inlineHit", "addRelease", "addPageAction", "setCurrentRouteName", "setPageViewName", "setCustomAttribute", "interaction", "noticeError", "setUserId", "setApplicationVersion"].forEach(t => {
              e[t] = function () {
                for (var r = arguments.length, n = new Array(r), i = 0; i < r; i++) n[i] = arguments[i];
                return function (t) {
                  for (var r = arguments.length, n = new Array(r > 1 ? r - 1 : 0), i = 1; i < r; i++) n[i - 1] = arguments[i];
                  let o = [];
                  return Object.values(e.initializedAgents).forEach(e => {
                    e.exposed && e.api[t] && o.push(e.api[t](...n));
                  }), o.length > 1 ? o : o[0];
                }(t, ...n);
              };
            });
          }
          var g = r(2587);
          function m(e) {
            let t = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : {},
              m = arguments.length > 2 ? arguments[2] : void 0,
              v = arguments.length > 3 ? arguments[3] : void 0,
              {
                init: b,
                info: y,
                loader_config: w,
                runtime: A = {
                  loaderType: m
                },
                exposed: x = !0
              } = t;
            const E = (0, h.gG)();
            y || (b = E.init, y = E.info, w = E.loader_config), (0, i.Dg)(e, b || {}), (0, i.GE)(e, w || {}), y.jsAttributes ??= {}, d.v6 && (y.jsAttributes.isWorker = !0), (0, i.CX)(e, y);
            const T = (0, i.P_)(e);
            A.denyList = [...(T.ajax?.deny_list || []), ...(T.ajax?.block_internal ? [y.beacon, y.errorBeacon] : [])], (0, i.sU)(e, A), p();
            const _ = function (e, t) {
              t || (0, c.R)(e, "api");
              const h = {};
              var p = a.ee.get(e),
                g = p.get("tracer"),
                m = "api-",
                v = m + "ixn-";
              function b(t, r, n, o) {
                const a = (0, i.C5)(e);
                return null === r ? delete a.jsAttributes[t] : (0, i.CX)(e, {
                  ...a,
                  jsAttributes: {
                    ...a.jsAttributes,
                    [t]: r
                  }
                }), A(m, n, !0, o || null === r ? "session" : void 0)(t, r);
              }
              function y() {}
              ["setErrorHandler", "finished", "addToTrace", "inlineHit", "addRelease"].forEach(e => h[e] = A(m, e, !0, "api")), h.addPageAction = A(m, "addPageAction", !0, n.D.pageAction), h.setCurrentRouteName = A(m, "routeName", !0, n.D.spa), h.setPageViewName = function (t, r) {
                if ("string" == typeof t) return "/" !== t.charAt(0) && (t = "/" + t), (0, i.OP)(e).customTransaction = (r || "http://custom.transaction") + t, A(m, "setPageViewName", !0)();
              }, h.setCustomAttribute = function (e, t) {
                let r = arguments.length > 2 && void 0 !== arguments[2] && arguments[2];
                if ("string" == typeof e) {
                  if (["string", "number"].includes(typeof t) || null === t) return b(e, t, "setCustomAttribute", r);
                  (0, l.Z)("Failed to execute setCustomAttribute.\nNon-null value must be a string or number type, but a type of <".concat(typeof t, "> was provided."));
                } else (0, l.Z)("Failed to execute setCustomAttribute.\nName must be a string type, but a type of <".concat(typeof e, "> was provided."));
              }, h.setUserId = function (e) {
                if ("string" == typeof e || null === e) return b("enduser.id", e, "setUserId", !0);
                (0, l.Z)("Failed to execute setUserId.\nNon-null value must be a string type, but a type of <".concat(typeof e, "> was provided."));
              }, h.setApplicationVersion = function (e) {
                if ("string" == typeof e || null === e) return b("application.version", e, "setApplicationVersion", !1);
                (0, l.Z)("Failed to execute setApplicationVersion. Expected <String | null>, but got <".concat(typeof e, ">."));
              }, h.interaction = function () {
                return new y().get();
              };
              var w = y.prototype = {
                createTracer: function (e, t) {
                  var r = {},
                    i = this,
                    a = "function" == typeof t;
                  return (0, o.p)(v + "tracer", [(0, s.z)(), e, r], i, n.D.spa, p), function () {
                    if (g.emit((a ? "" : "no-") + "fn-start", [(0, s.z)(), i, a], r), a) try {
                      return t.apply(this, arguments);
                    } catch (e) {
                      throw g.emit("fn-err", [arguments, this, e], r), e;
                    } finally {
                      g.emit("fn-end", [(0, s.z)()], r);
                    }
                  };
                }
              };
              function A(e, t, r, i) {
                return function () {
                  return (0, o.p)(f.xS, ["API/" + t + "/called"], void 0, n.D.metrics, p), i && (0, o.p)(e + t, [(0, s.z)(), ...arguments], r ? null : this, i, p), r ? void 0 : this;
                };
              }
              function x() {
                r.e(111).then(r.bind(r, 7438)).then(t => {
                  let {
                    setAPI: r
                  } = t;
                  r(e), (0, c.L)(e, "api");
                }).catch(() => (0, l.Z)("Downloading runtime APIs failed..."));
              }
              return ["actionText", "setName", "setAttribute", "save", "ignore", "onEnd", "getContext", "end", "get"].forEach(e => {
                w[e] = A(v, e, void 0, n.D.spa);
              }), h.noticeError = function (e, t) {
                "string" == typeof e && (e = new Error(e)), (0, o.p)(f.xS, ["API/noticeError/called"], void 0, n.D.metrics, p), (0, o.p)("err", [e, (0, s.z)(), !1, t], void 0, n.D.jserrors, p);
              }, d.il ? (0, u.b)(() => x(), !0) : x(), h;
            }(e, v);
            return (0, h.Qy)(e, _, "api"), (0, h.Qy)(e, x, "exposed"), (0, h.EZ)("activatedFeatures", g.T), _;
          }
        },
        3325: (e, t, r) => {
          r.d(t, {
            D: () => n,
            p: () => i
          });
          const n = {
              ajax: "ajax",
              jserrors: "jserrors",
              metrics: "metrics",
              pageAction: "page_action",
              pageViewEvent: "page_view_event",
              pageViewTiming: "page_view_timing",
              sessionReplay: "session_replay",
              sessionTrace: "session_trace",
              spa: "spa"
            },
            i = {
              [n.pageViewEvent]: 1,
              [n.pageViewTiming]: 2,
              [n.metrics]: 3,
              [n.jserrors]: 4,
              [n.ajax]: 5,
              [n.sessionTrace]: 6,
              [n.pageAction]: 7,
              [n.spa]: 8,
              [n.sessionReplay]: 9
            };
        }
      },
      n = {};
    function i(e) {
      var t = n[e];
      if (void 0 !== t) return t.exports;
      var o = n[e] = {
        exports: {}
      };
      return r[e](o, o.exports, i), o.exports;
    }
    i.m = r, i.d = (e, t) => {
      for (var r in t) i.o(t, r) && !i.o(e, r) && Object.defineProperty(e, r, {
        enumerable: !0,
        get: t[r]
      });
    }, i.f = {}, i.e = e => Promise.all(Object.keys(i.f).reduce((t, r) => (i.f[r](e, t), t), [])), i.u = e => "nr-spa.1097a448-1.238.0.min.js", i.o = (e, t) => Object.prototype.hasOwnProperty.call(e, t), e = {}, t = "NRBA-1.238.0.PROD:", i.l = (r, n, o, a) => {
      if (e[r]) e[r].push(n);else {
        var s, c;
        if (void 0 !== o) for (var u = document.getElementsByTagName("script"), d = 0; d < u.length; d++) {
          var l = u[d];
          if (l.getAttribute("src") == r || l.getAttribute("data-webpack") == t + o) {
            s = l;
            break;
          }
        }
        s || (c = !0, (s = document.createElement("script")).charset = "utf-8", s.timeout = 120, i.nc && s.setAttribute("nonce", i.nc), s.setAttribute("data-webpack", t + o), s.src = r), e[r] = [n];
        var f = (t, n) => {
            s.onerror = s.onload = null, clearTimeout(h);
            var i = e[r];
            if (delete e[r], s.parentNode && s.parentNode.removeChild(s), i && i.forEach(e => e(n)), t) return t(n);
          },
          h = setTimeout(f.bind(null, void 0, {
            type: "timeout",
            target: s
          }), 12e4);
        s.onerror = f.bind(null, s.onerror), s.onload = f.bind(null, s.onload), c && document.head.appendChild(s);
      }
    }, i.r = e => {
      "undefined" != typeof Symbol && Symbol.toStringTag && Object.defineProperty(e, Symbol.toStringTag, {
        value: "Module"
      }), Object.defineProperty(e, "__esModule", {
        value: !0
      });
    }, i.p = "https://js-agent.newrelic.com/", (() => {
      var e = {
        801: 0,
        92: 0
      };
      i.f.j = (t, r) => {
        var n = i.o(e, t) ? e[t] : void 0;
        if (0 !== n) if (n) r.push(n[2]);else {
          var o = new Promise((r, i) => n = e[t] = [r, i]);
          r.push(n[2] = o);
          var a = i.p + i.u(t),
            s = new Error();
          i.l(a, r => {
            if (i.o(e, t) && (0 !== (n = e[t]) && (e[t] = void 0), n)) {
              var o = r && ("load" === r.type ? "missing" : r.type),
                a = r && r.target && r.target.src;
              s.message = "Loading chunk " + t + " failed.\n(" + o + ": " + a + ")", s.name = "ChunkLoadError", s.type = o, s.request = a, n[1](s);
            }
          }, "chunk-" + t, t);
        }
      };
      var t = (t, r) => {
          var n,
            o,
            [a, s, c] = r,
            u = 0;
          if (a.some(t => 0 !== e[t])) {
            for (n in s) i.o(s, n) && (i.m[n] = s[n]);
            if (c) c(i);
          }
          for (t && t(r); u < a.length; u++) o = a[u], i.o(e, o) && e[o] && e[o][0](), e[o] = 0;
        },
        r = self["webpackChunk:NRBA-1.238.0.PROD"] = self["webpackChunk:NRBA-1.238.0.PROD"] || [];
      r.forEach(t.bind(null, 0)), r.push = t.bind(null, r.push.bind(r));
    })(), (() => {
      var e = i(50);
      class t {
        addPageAction(t, r) {
          (0, e.Z)("Call to agent api addPageAction failed. The session trace feature is not currently initialized.");
        }
        setPageViewName(t, r) {
          (0, e.Z)("Call to agent api setPageViewName failed. The page view feature is not currently initialized.");
        }
        setCustomAttribute(t, r, n) {
          (0, e.Z)("Call to agent api setCustomAttribute failed. The js errors feature is not currently initialized.");
        }
        noticeError(t, r) {
          (0, e.Z)("Call to agent api noticeError failed. The js errors feature is not currently initialized.");
        }
        setUserId(t) {
          (0, e.Z)("Call to agent api setUserId failed. The js errors feature is not currently initialized.");
        }
        setApplicationVersion(t) {
          (0, e.Z)("Call to agent api setApplicationVersion failed. The agent is not currently initialized.");
        }
        setErrorHandler(t) {
          (0, e.Z)("Call to agent api setErrorHandler failed. The js errors feature is not currently initialized.");
        }
        finished(t) {
          (0, e.Z)("Call to agent api finished failed. The page action feature is not currently initialized.");
        }
        addRelease(t, r) {
          (0, e.Z)("Call to agent api addRelease failed. The agent is not currently initialized.");
        }
      }
      var r = i(3325),
        n = i(5763);
      const o = Object.values(r.D);
      function a(e) {
        const t = {};
        return o.forEach(r => {
          t[r] = function (e, t) {
            return !1 !== (0, n.Mt)(t, "".concat(e, ".enabled"));
          }(r, e);
        }), t;
      }
      var s = i(9144);
      var c = i(5546),
        u = i(385),
        d = i(8e3),
        l = i(5938),
        f = i(3960);
      class h extends l.W {
        constructor(e, t, r) {
          let n = !(arguments.length > 3 && void 0 !== arguments[3]) || arguments[3];
          super(e, t, r), this.auto = n, this.abortHandler, this.featAggregate, this.onAggregateImported, n && (0, d.R)(e, r);
        }
        importAggregator() {
          let t = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : {};
          if (this.featAggregate || !this.auto) return;
          const r = u.il && !0 === (0, n.Mt)(this.agentIdentifier, "privacy.cookies_enabled");
          let o;
          this.onAggregateImported = new Promise(e => {
            o = e;
          });
          const a = async () => {
            let n;
            try {
              if (r) {
                const {
                  setupAgentSession: e
                } = await i.e(111).then(i.bind(i, 3228));
                n = e(this.agentIdentifier);
              }
            } catch (t) {
              (0, e.Z)("A problem occurred when starting up session manager. This page will not start or extend any session.", t);
            }
            try {
              if (!this.shouldImportAgg(this.featureName, n)) return (0, d.L)(this.agentIdentifier, this.featureName), void o(!1);
              const {
                  lazyFeatureLoader: e
                } = await i.e(111).then(i.bind(i, 8582)),
                {
                  Aggregate: r
                } = await e(this.featureName, "aggregate");
              this.featAggregate = new r(this.agentIdentifier, this.aggregator, t), o(!0);
            } catch (t) {
              (0, e.Z)("Downloading and initializing ".concat(this.featureName, " failed..."), t), this.abortHandler?.(), o(!1);
            }
          };
          u.il ? (0, f.b)(() => a(), !0) : a();
        }
        shouldImportAgg(e, t) {
          return e !== r.D.sessionReplay || !!n.Yu.MO && !1 !== (0, n.Mt)(this.agentIdentifier, "session_trace.enabled") && (!!t?.isNew || !!t?.state.sessionReplay);
        }
      }
      var p = i(7633),
        g = i(7894);
      class m extends h {
        static featureName = p.t9;
        constructor(e, t) {
          let i = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          if (super(e, t, p.t9, i), ("undefined" == typeof PerformanceNavigationTiming || u.Tt) && "undefined" != typeof PerformanceTiming) {
            const t = (0, n.OP)(e);
            t[p.Dz] = Math.max(Date.now() - t.offset, 0), (0, f.K)(() => t[p.qw] = Math.max((0, g.z)() - t[p.Dz], 0)), (0, f.b)(() => {
              const e = (0, g.z)();
              t[p.OJ] = Math.max(e - t[p.Dz], 0), (0, c.p)("timing", ["load", e], void 0, r.D.pageViewTiming, this.ee);
            });
          }
          this.importAggregator();
        }
      }
      var v = i(1117),
        b = i(1284);
      class y extends v.w {
        constructor(e) {
          super(e), this.aggregatedData = {};
        }
        store(e, t, r, n, i) {
          var o = this.getBucket(e, t, r, i);
          return o.metrics = function (e, t) {
            t || (t = {
              count: 0
            });
            return t.count += 1, (0, b.D)(e, function (e, r) {
              t[e] = w(r, t[e]);
            }), t;
          }(n, o.metrics), o;
        }
        merge(e, t, r, n, i) {
          var o = this.getBucket(e, t, n, i);
          if (o.metrics) {
            var a = o.metrics;
            a.count += r.count, (0, b.D)(r, function (e, t) {
              if ("count" !== e) {
                var n = a[e],
                  i = r[e];
                i && !i.c ? a[e] = w(i.t, n) : a[e] = function (e, t) {
                  if (!t) return e;
                  t.c || (t = A(t.t));
                  return t.min = Math.min(e.min, t.min), t.max = Math.max(e.max, t.max), t.t += e.t, t.sos += e.sos, t.c += e.c, t;
                }(i, a[e]);
              }
            });
          } else o.metrics = r;
        }
        storeMetric(e, t, r, n) {
          var i = this.getBucket(e, t, r);
          return i.stats = w(n, i.stats), i;
        }
        getBucket(e, t, r, n) {
          this.aggregatedData[e] || (this.aggregatedData[e] = {});
          var i = this.aggregatedData[e][t];
          return i || (i = this.aggregatedData[e][t] = {
            params: r || {}
          }, n && (i.custom = n)), i;
        }
        get(e, t) {
          return t ? this.aggregatedData[e] && this.aggregatedData[e][t] : this.aggregatedData[e];
        }
        take(e) {
          for (var t = {}, r = "", n = !1, i = 0; i < e.length; i++) t[r = e[i]] = x(this.aggregatedData[r]), t[r].length && (n = !0), delete this.aggregatedData[r];
          return n ? t : null;
        }
      }
      function w(e, t) {
        return null == e ? function (e) {
          e ? e.c++ : e = {
            c: 1
          };
          return e;
        }(t) : t ? (t.c || (t = A(t.t)), t.c += 1, t.t += e, t.sos += e * e, e > t.max && (t.max = e), e < t.min && (t.min = e), t) : {
          t: e
        };
      }
      function A(e) {
        return {
          t: e,
          min: e,
          max: e,
          sos: e * e,
          c: 1
        };
      }
      function x(e) {
        return "object" != typeof e ? [] : (0, b.D)(e, E);
      }
      function E(e, t) {
        return t;
      }
      var T = i(8632),
        _ = i(4402),
        D = i(4351);
      var j = i(7956),
        C = i(3239),
        N = i(9251);
      class O extends h {
        static featureName = N.t;
        constructor(e, t) {
          let r = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, N.t, r), u.il && ((0, n.OP)(e).initHidden = Boolean("hidden" === document.visibilityState), (0, j.N)(() => (0, c.p)("docHidden", [(0, g.z)()], void 0, N.t, this.ee), !0), (0, C.bP)("pagehide", () => (0, c.p)("winPagehide", [(0, g.z)()], void 0, N.t, this.ee)), this.importAggregator());
        }
      }
      var S = i(3081);
      class P extends h {
        static featureName = S.t9;
        constructor(e, t) {
          let r = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, S.t9, r), this.importAggregator();
        }
      }
      var R = i(6660);
      class I {
        constructor(e, t, r, n) {
          this.name = "UncaughtError", this.message = e, this.sourceURL = t, this.line = r, this.column = n;
        }
      }
      class k extends h {
        static featureName = R.t;
        #e = new Set();
        constructor(e, t) {
          let n = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, R.t, n);
          try {
            this.removeOnAbort = new AbortController();
          } catch (e) {}
          this.ee.on("fn-err", (e, t, n) => {
            this.abortHandler && !this.#e.has(n) && (this.#e.add(n), (0, c.p)("err", [this.#t(n), (0, g.z)()], void 0, r.D.jserrors, this.ee));
          }), this.ee.on("internal-error", e => {
            this.abortHandler && (0, c.p)("ierr", [this.#t(e), (0, g.z)(), !0], void 0, r.D.jserrors, this.ee);
          }), u._A.addEventListener("unhandledrejection", e => {
            this.abortHandler && (0, c.p)("err", [this.#r(e), (0, g.z)(), !1, {
              unhandledPromiseRejection: 1
            }], void 0, r.D.jserrors, this.ee);
          }, (0, C.m$)(!1, this.removeOnAbort?.signal)), u._A.addEventListener("error", e => {
            this.abortHandler && (this.#e.has(e.error) ? this.#e.delete(e.error) : (0, c.p)("err", [this.#n(e), (0, g.z)()], void 0, r.D.jserrors, this.ee));
          }, (0, C.m$)(!1, this.removeOnAbort?.signal)), this.abortHandler = this.#i, this.importAggregator();
        }
        #i() {
          this.removeOnAbort?.abort(), this.#e.clear(), this.abortHandler = void 0;
        }
        #t(e) {
          return e instanceof Error ? e : void 0 !== e?.message ? new I(e.message, e.filename || e.sourceURL, e.lineno || e.line, e.colno || e.col) : new I("string" == typeof e ? e : (0, D.P)(e));
        }
        #r(e) {
          let t = "Unhandled Promise Rejection: ";
          if (e?.reason instanceof Error) try {
            return e.reason.message = t + e.reason.message, e.reason;
          } catch (t) {
            return e.reason;
          }
          if (void 0 === e.reason) return new I(t);
          const r = this.#t(e.reason);
          return r.message = t + r.message, r;
        }
        #n(e) {
          return e.error instanceof Error ? e.error : new I(e.message, e.filename, e.lineno, e.colno);
        }
      }
      var H = i(2210);
      let z = 1;
      const L = "nr@id";
      function M(e) {
        const t = typeof e;
        return !e || "object" !== t && "function" !== t ? -1 : e === u._A ? 0 : (0, H.X)(e, L, function () {
          return z++;
        });
      }
      function B(e) {
        if ("string" == typeof e && e.length) return e.length;
        if ("object" == typeof e) {
          if ("undefined" != typeof ArrayBuffer && e instanceof ArrayBuffer && e.byteLength) return e.byteLength;
          if ("undefined" != typeof Blob && e instanceof Blob && e.size) return e.size;
          if (!("undefined" != typeof FormData && e instanceof FormData)) try {
            return (0, D.P)(e).length;
          } catch (e) {
            return;
          }
        }
      }
      var F = i(1214),
        U = i(7243);
      class q {
        constructor(e) {
          this.agentIdentifier = e;
        }
        generateTracePayload(e) {
          if (!this.shouldGenerateTrace(e)) return null;
          var t = (0, n.DL)(this.agentIdentifier);
          if (!t) return null;
          var r = (t.accountID || "").toString() || null,
            i = (t.agentID || "").toString() || null,
            o = (t.trustKey || "").toString() || null;
          if (!r || !i) return null;
          var a = (0, _.M)(),
            s = (0, _.Ht)(),
            c = Date.now(),
            u = {
              spanId: a,
              traceId: s,
              timestamp: c
            };
          return (e.sameOrigin || this.isAllowedOrigin(e) && this.useTraceContextHeadersForCors()) && (u.traceContextParentHeader = this.generateTraceContextParentHeader(a, s), u.traceContextStateHeader = this.generateTraceContextStateHeader(a, c, r, i, o)), (e.sameOrigin && !this.excludeNewrelicHeader() || !e.sameOrigin && this.isAllowedOrigin(e) && this.useNewrelicHeaderForCors()) && (u.newrelicHeader = this.generateTraceHeader(a, s, c, r, i, o)), u;
        }
        generateTraceContextParentHeader(e, t) {
          return "00-" + t + "-" + e + "-01";
        }
        generateTraceContextStateHeader(e, t, r, n, i) {
          return i + "@nr=0-1-" + r + "-" + n + "-" + e + "----" + t;
        }
        generateTraceHeader(e, t, r, n, i, o) {
          if (!("function" == typeof u._A?.btoa)) return null;
          var a = {
            v: [0, 1],
            d: {
              ty: "Browser",
              ac: n,
              ap: i,
              id: e,
              tr: t,
              ti: r
            }
          };
          return o && n !== o && (a.d.tk = o), btoa((0, D.P)(a));
        }
        shouldGenerateTrace(e) {
          return this.isDtEnabled() && this.isAllowedOrigin(e);
        }
        isAllowedOrigin(e) {
          var t = !1,
            r = {};
          if ((0, n.Mt)(this.agentIdentifier, "distributed_tracing") && (r = (0, n.P_)(this.agentIdentifier).distributed_tracing), e.sameOrigin) t = !0;else if (r.allowed_origins instanceof Array) for (var i = 0; i < r.allowed_origins.length; i++) {
            var o = (0, U.e)(r.allowed_origins[i]);
            if (e.hostname === o.hostname && e.protocol === o.protocol && e.port === o.port) {
              t = !0;
              break;
            }
          }
          return t;
        }
        isDtEnabled() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !!e.enabled;
        }
        excludeNewrelicHeader() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !!e.exclude_newrelic_header;
        }
        useNewrelicHeaderForCors() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !1 !== e.cors_use_newrelic_header;
        }
        useTraceContextHeadersForCors() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !!e.cors_use_tracecontext_headers;
        }
      }
      var Z = i(7825),
        V = ["load", "error", "abort", "timeout"],
        G = V.length,
        W = n.Yu.REQ,
        X = n.Yu.XHR;
      class Q extends h {
        static featureName = Z.t;
        constructor(e, t) {
          let i = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, Z.t, i), (0, n.OP)(e).xhrWrappable && (this.dt = new q(e), this.handler = (e, t, r, n) => (0, c.p)(e, t, r, n, this.ee), (0, F.u5)(this.ee), (0, F.Kf)(this.ee), function (e, t, i, o) {
            function a(e) {
              var t = this;
              t.totalCbs = 0, t.called = 0, t.cbTime = 0, t.end = E, t.ended = !1, t.xhrGuids = {}, t.lastSize = null, t.loadCaptureCalled = !1, t.params = this.params || {}, t.metrics = this.metrics || {}, e.addEventListener("load", function (r) {
                _(t, e);
              }, (0, C.m$)(!1)), u.IF || e.addEventListener("progress", function (e) {
                t.lastSize = e.loaded;
              }, (0, C.m$)(!1));
            }
            function s(e) {
              this.params = {
                method: e[0]
              }, T(this, e[1]), this.metrics = {};
            }
            function c(t, r) {
              var i = (0, n.DL)(e);
              i.xpid && this.sameOrigin && r.setRequestHeader("X-NewRelic-ID", i.xpid);
              var a = o.generateTracePayload(this.parsedOrigin);
              if (a) {
                var s = !1;
                a.newrelicHeader && (r.setRequestHeader("newrelic", a.newrelicHeader), s = !0), a.traceContextParentHeader && (r.setRequestHeader("traceparent", a.traceContextParentHeader), a.traceContextStateHeader && r.setRequestHeader("tracestate", a.traceContextStateHeader), s = !0), s && (this.dt = a);
              }
            }
            function d(e, r) {
              var n = this.metrics,
                i = e[0],
                o = this;
              if (n && i) {
                var a = B(i);
                a && (n.txSize = a);
              }
              this.startTime = (0, g.z)(), this.listener = function (e) {
                try {
                  "abort" !== e.type || o.loadCaptureCalled || (o.params.aborted = !0), ("load" !== e.type || o.called === o.totalCbs && (o.onloadCalled || "function" != typeof r.onload) && "function" == typeof o.end) && o.end(r);
                } catch (e) {
                  try {
                    t.emit("internal-error", [e]);
                  } catch (e) {}
                }
              };
              for (var s = 0; s < G; s++) r.addEventListener(V[s], this.listener, (0, C.m$)(!1));
            }
            function l(e, t, r) {
              this.cbTime += e, t ? this.onloadCalled = !0 : this.called += 1, this.called !== this.totalCbs || !this.onloadCalled && "function" == typeof r.onload || "function" != typeof this.end || this.end(r);
            }
            function f(e, t) {
              var r = "" + M(e) + !!t;
              this.xhrGuids && !this.xhrGuids[r] && (this.xhrGuids[r] = !0, this.totalCbs += 1);
            }
            function h(e, t) {
              var r = "" + M(e) + !!t;
              this.xhrGuids && this.xhrGuids[r] && (delete this.xhrGuids[r], this.totalCbs -= 1);
            }
            function p() {
              this.endTime = (0, g.z)();
            }
            function m(e, r) {
              r instanceof X && "load" === e[0] && t.emit("xhr-load-added", [e[1], e[2]], r);
            }
            function v(e, r) {
              r instanceof X && "load" === e[0] && t.emit("xhr-load-removed", [e[1], e[2]], r);
            }
            function b(e, t, r) {
              t instanceof X && ("onload" === r && (this.onload = !0), ("load" === (e[0] && e[0].type) || this.onload) && (this.xhrCbStart = (0, g.z)()));
            }
            function y(e, r) {
              this.xhrCbStart && t.emit("xhr-cb-time", [(0, g.z)() - this.xhrCbStart, this.onload, r], r);
            }
            function w(e) {
              var t,
                r = e[1] || {};
              if ("string" == typeof e[0] ? 0 === (t = e[0]).length && u.il && (t = "" + u._A.location.href) : e[0] && e[0].url ? t = e[0].url : u._A?.URL && e[0] && e[0] instanceof URL ? t = e[0].href : "function" == typeof e[0].toString && (t = e[0].toString()), "string" == typeof t && 0 !== t.length) {
                t && (this.parsedOrigin = (0, U.e)(t), this.sameOrigin = this.parsedOrigin.sameOrigin);
                var n = o.generateTracePayload(this.parsedOrigin);
                if (n && (n.newrelicHeader || n.traceContextParentHeader)) if (e[0] && e[0].headers) s(e[0].headers, n) && (this.dt = n);else {
                  var i = {};
                  for (var a in r) i[a] = r[a];
                  i.headers = new Headers(r.headers || {}), s(i.headers, n) && (this.dt = n), e.length > 1 ? e[1] = i : e.push(i);
                }
              }
              function s(e, t) {
                var r = !1;
                return t.newrelicHeader && (e.set("newrelic", t.newrelicHeader), r = !0), t.traceContextParentHeader && (e.set("traceparent", t.traceContextParentHeader), t.traceContextStateHeader && e.set("tracestate", t.traceContextStateHeader), r = !0), r;
              }
            }
            function A(e, t) {
              this.params = {}, this.metrics = {}, this.startTime = (0, g.z)(), this.dt = t, e.length >= 1 && (this.target = e[0]), e.length >= 2 && (this.opts = e[1]);
              var r,
                n = this.opts || {},
                i = this.target;
              "string" == typeof i ? r = i : "object" == typeof i && i instanceof W ? r = i.url : u._A?.URL && "object" == typeof i && i instanceof URL && (r = i.href), T(this, r);
              var o = ("" + (i && i instanceof W && i.method || n.method || "GET")).toUpperCase();
              this.params.method = o, this.txSize = B(n.body) || 0;
            }
            function x(e, t) {
              var n;
              this.endTime = (0, g.z)(), this.params || (this.params = {}), this.params.status = t ? t.status : 0, "string" == typeof this.rxSize && this.rxSize.length > 0 && (n = +this.rxSize);
              var o = {
                txSize: this.txSize,
                rxSize: n,
                duration: (0, g.z)() - this.startTime
              };
              i("xhr", [this.params, o, this.startTime, this.endTime, "fetch"], this, r.D.ajax);
            }
            function E(e) {
              var t = this.params,
                n = this.metrics;
              if (!this.ended) {
                this.ended = !0;
                for (var o = 0; o < G; o++) e.removeEventListener(V[o], this.listener, !1);
                t.aborted || (n.duration = (0, g.z)() - this.startTime, this.loadCaptureCalled || 4 !== e.readyState ? null == t.status && (t.status = 0) : _(this, e), n.cbTime = this.cbTime, i("xhr", [t, n, this.startTime, this.endTime, "xhr"], this, r.D.ajax));
              }
            }
            function T(e, t) {
              var r = (0, U.e)(t),
                n = e.params;
              n.hostname = r.hostname, n.port = r.port, n.protocol = r.protocol, n.host = r.hostname + ":" + r.port, n.pathname = r.pathname, e.parsedOrigin = r, e.sameOrigin = r.sameOrigin;
            }
            function _(e, t) {
              e.params.status = t.status;
              var r = function (e, t) {
                var r = e.responseType;
                return "json" === r && null !== t ? t : "arraybuffer" === r || "blob" === r || "json" === r ? B(e.response) : "text" === r || "" === r || void 0 === r ? B(e.responseText) : void 0;
              }(t, e.lastSize);
              if (r && (e.metrics.rxSize = r), e.sameOrigin) {
                var n = t.getResponseHeader("X-NewRelic-App-Data");
                n && (e.params.cat = n.split(", ").pop());
              }
              e.loadCaptureCalled = !0;
            }
            t.on("new-xhr", a), t.on("open-xhr-start", s), t.on("open-xhr-end", c), t.on("send-xhr-start", d), t.on("xhr-cb-time", l), t.on("xhr-load-added", f), t.on("xhr-load-removed", h), t.on("xhr-resolved", p), t.on("addEventListener-end", m), t.on("removeEventListener-end", v), t.on("fn-end", y), t.on("fetch-before-start", w), t.on("fetch-start", A), t.on("fn-start", b), t.on("fetch-done", x);
          }(e, this.ee, this.handler, this.dt), this.importAggregator());
        }
      }
      var K = i(3614);
      const {
        BST_RESOURCE: Y,
        RESOURCE: J,
        START: ee,
        END: te,
        FEATURE_NAME: re,
        FN_END: ne,
        FN_START: ie,
        PUSH_STATE: oe
      } = K;
      var ae = i(7836);
      const {
        FEATURE_NAME: se,
        START: ce,
        END: ue,
        BODY: de,
        CB_END: le,
        JS_TIME: fe,
        FETCH: he,
        FN_START: pe,
        CB_START: ge,
        FN_END: me
      } = ae;
      var ve = i(4649);
      class be extends h {
        static featureName = ve.t;
        constructor(e, t) {
          let r = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, ve.t, r), this.importAggregator();
        }
      }
      new class extends t {
        constructor(t) {
          let r = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : (0, _.ky)(16);
          super(), u._A ? (this.agentIdentifier = r, this.sharedAggregator = new y({
            agentIdentifier: this.agentIdentifier
          }), this.features = {}, this.desiredFeatures = new Set(t.features || []), this.desiredFeatures.add(m), Object.assign(this, (0, s.j)(this.agentIdentifier, t, t.loaderType || "agent")), this.start()) : (0, e.Z)("Failed to initial the agent. Could not determine the runtime environment.");
        }
        get config() {
          return {
            info: (0, n.C5)(this.agentIdentifier),
            init: (0, n.P_)(this.agentIdentifier),
            loader_config: (0, n.DL)(this.agentIdentifier),
            runtime: (0, n.OP)(this.agentIdentifier)
          };
        }
        start() {
          const t = "features";
          try {
            const n = a(this.agentIdentifier),
              i = [...this.desiredFeatures];
            i.sort((e, t) => r.p[e.featureName] - r.p[t.featureName]), i.forEach(t => {
              if (n[t.featureName] || t.featureName === r.D.pageViewEvent) {
                const i = function (e) {
                  switch (e) {
                    case r.D.ajax:
                      return [r.D.jserrors];
                    case r.D.sessionTrace:
                      return [r.D.ajax, r.D.pageViewEvent];
                    case r.D.sessionReplay:
                      return [r.D.sessionTrace];
                    case r.D.pageViewTiming:
                      return [r.D.pageViewEvent];
                    default:
                      return [];
                  }
                }(t.featureName);
                i.every(e => n[e]) || (0, e.Z)("".concat(t.featureName, " is enabled but one or more dependent features has been disabled (").concat((0, D.P)(i), "). This may cause unintended consequences or missing data...")), this.features[t.featureName] = new t(this.agentIdentifier, this.sharedAggregator);
              }
            }), (0, T.Qy)(this.agentIdentifier, this.features, t);
          } catch (r) {
            (0, e.Z)("Failed to initialize all enabled instrument classes (agent aborted) -", r);
            for (const e in this.features) this.features[e].abortHandler?.();
            const n = (0, T.fP)();
            return delete n.initializedAgents[this.agentIdentifier]?.api, delete n.initializedAgents[this.agentIdentifier]?.[t], delete this.sharedAggregator, n.ee?.abort(), delete n.ee?.get(this.agentIdentifier), !1;
          }
        }
        addToTrace(t) {
          (0, e.Z)("Call to agent api addToTrace failed. The page action feature is not currently initialized.");
        }
        setCurrentRouteName(t) {
          (0, e.Z)("Call to agent api setCurrentRouteName failed. The spa feature is not currently initialized.");
        }
        interaction() {
          (0, e.Z)("Call to agent api interaction failed. The spa feature is not currently initialized.");
        }
      }({
        features: [Q, m, O, class extends h {
          static featureName = re;
          constructor(e, t) {
            if (super(e, t, re, !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2]), !u.il) return;
            const n = this.ee;
            let i;
            (0, F.QU)(n), this.eventsEE = (0, F.em)(n), this.eventsEE.on(ie, function (e, t) {
              this.bstStart = (0, g.z)();
            }), this.eventsEE.on(ne, function (e, t) {
              (0, c.p)("bst", [e[0], t, this.bstStart, (0, g.z)()], void 0, r.D.sessionTrace, n);
            }), n.on(oe + ee, function (e) {
              this.time = (0, g.z)(), this.startPath = location.pathname + location.hash;
            }), n.on(oe + te, function (e) {
              (0, c.p)("bstHist", [location.pathname + location.hash, this.startPath, this.time], void 0, r.D.sessionTrace, n);
            });
            try {
              i = new PerformanceObserver(e => {
                const t = e.getEntries();
                (0, c.p)(Y, [t], void 0, r.D.sessionTrace, n);
              }), i.observe({
                type: J,
                buffered: !0
              });
            } catch (e) {}
            this.importAggregator({
              resourceObserver: i
            });
          }
        }, P, be, k, class extends h {
          static featureName = se;
          constructor(e, t) {
            if (super(e, t, se, !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2]), !u.il) return;
            if (!(0, n.OP)(e).xhrWrappable) return;
            try {
              this.removeOnAbort = new AbortController();
            } catch (e) {}
            let r,
              i = 0;
            const o = this.ee.get("tracer"),
              a = (0, F._L)(this.ee),
              s = (0, F.Lg)(this.ee),
              c = (0, F.BV)(this.ee),
              d = (0, F.Kf)(this.ee),
              l = this.ee.get("events"),
              f = (0, F.u5)(this.ee),
              h = (0, F.QU)(this.ee),
              p = (0, F.Gm)(this.ee);
            function m(e, t) {
              h.emit("newURL", ["" + window.location, t]);
            }
            function v() {
              i++, r = window.location.hash, this[pe] = (0, g.z)();
            }
            function b() {
              i--, window.location.hash !== r && m(0, !0);
              var e = (0, g.z)();
              this[fe] = ~~this[fe] + e - this[pe], this[me] = e;
            }
            function y(e, t) {
              e.on(t, function () {
                this[t] = (0, g.z)();
              });
            }
            this.ee.on(pe, v), s.on(ge, v), a.on(ge, v), this.ee.on(me, b), s.on(le, b), a.on(le, b), this.ee.buffer([pe, me, "xhr-resolved"], this.featureName), l.buffer([pe], this.featureName), c.buffer(["setTimeout" + ue, "clearTimeout" + ce, pe], this.featureName), d.buffer([pe, "new-xhr", "send-xhr" + ce], this.featureName), f.buffer([he + ce, he + "-done", he + de + ce, he + de + ue], this.featureName), h.buffer(["newURL"], this.featureName), p.buffer([pe], this.featureName), s.buffer(["propagate", ge, le, "executor-err", "resolve" + ce], this.featureName), o.buffer([pe, "no-" + pe], this.featureName), a.buffer(["new-jsonp", "cb-start", "jsonp-error", "jsonp-end"], this.featureName), y(f, he + ce), y(f, he + "-done"), y(a, "new-jsonp"), y(a, "jsonp-end"), y(a, "cb-start"), h.on("pushState-end", m), h.on("replaceState-end", m), window.addEventListener("hashchange", m, (0, C.m$)(!0, this.removeOnAbort?.signal)), window.addEventListener("load", m, (0, C.m$)(!0, this.removeOnAbort?.signal)), window.addEventListener("popstate", function () {
              m(0, i > 1);
            }, (0, C.m$)(!0, this.removeOnAbort?.signal)), this.abortHandler = this.#i, this.importAggregator();
          }
          #i() {
            this.removeOnAbort?.abort(), this.abortHandler = void 0;
          }
        }],
        loaderType: "spa"
      });
    })();
  })();
})()</script>
      <link rel="shortcut icon" href="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/103/images/favSD.ico" type="image/x-icon">
      <link rel="icon" href="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/103/images/favSD.ico" type="image/x-icon">
      <link rel="stylesheet" href="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/b64013ec63c69e3d916174cbebae89d65b2419e1/arp.css">
      <link href="//cdn.pendo.io" rel="dns-prefetch">
      <link href="https://cdn.pendo.io" rel="preconnect" crossorigin="anonymous">
      <link rel="dns-prefetch" href="https://smetrics.elsevier.com">
      <script async="" id="reading-assistant-script-tag" src="/feature/assets/ai-components/S1319157818303379?client=arp&amp;componentVersion=V11" type="text/javascript"></script>
      <script type="text/javascript">
        var targetServerState = JSON.stringify({"4D6368F454EC41940A4C98A6@AdobeOrg":{"sdid":{"supplementalDataIDCurrent":"203476D511945285-2C992A4D5295B21E","supplementalDataIDCurrentConsumed":{"payload:target-global-mbox":true},"supplementalDataIDLastConsumed":{}}}});
        window.appData = window.appData || [];
        window.pageTargeting = {"region":"eu-west-1","platform":"sdtech","entitled":true,"crawler":"","journal":"Journal of King Saud University - Computer and Information Sciences","auth":"AE"};
        window.arp = {
          config: {"adobeSuite":"elsevier-sd-prod","arsUrl":"https://ars.els-cdn.com","recommendationsFeedback":{"enabled":true,"url":"https://feedback.recs.d.elsevier.com/raw/events","timeout":60000},"googleMapsApiKey":"AIzaSyCBYU6I6lrbEU6wQXUEIte3NwGtm3jwHQc","mediaBaseUrl":"https://ars.els-cdn.com/content/image/","strictMode":false,"seamlessAccess":{"enableSeamlessAccess":true,"scriptUrl":"https://unpkg.com/@theidentityselector/thiss-ds@1.0.13/dist/thiss-ds.js","persistenceUrl":"https://service.seamlessaccess.org/ps/","persistenceContext":"seamlessaccess.org","scienceDirectUrl":"https://www.sciencedirect.com","shibAuthUrl":"https://auth.elsevier.com/ShibAuth/institutionLogin"},"reaxys":{"apiUrl":"https://reaxys-sdlc.reaxys.com","origin":"sciencedirect","queryBuilderHostPath":"https://www.reaxys.com/reaxys/secured/hopinto.do","url":"https://www.reaxys.com"},"oneTrustCookie":{"enabled":true},"ssrn":{"url":"https://papers.ssrn.com","path":"/sol3/papers.cfm"},"plumX":"https://api.plu.mx/widget/elsevier/artifact","assetRoute":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/b64013ec63c69e3d916174cbebae89d65b2419e1"},
          subscriptions: [],
          subscribe: function(cb) {
            var self = this;
            var i = this.subscriptions.push(cb) - 1;
            return function unsubscribe() {
              self.subscriptions.splice(i, 1);
            }
          },
        };
        window.addEventListener('beforeprint', () => pendo.onGuideDismissed());
      </script>
    <script data-cfasync="false" src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" data-domain-script="865ea198-88cc-4e41-8952-1df75d554d02"></script><meta http-equiv="origin-trial" content="AlK2UR5SkAlj8jjdEc9p3F3xuFYlF6LYjAML3EOqw1g26eCwWPjdmecULvBH5MVPoqKYrOfPhYVL71xAXI1IBQoAAAB8eyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="Amm8/NmvvQfhwCib6I7ZsmUxiSCfOxWxHayJwyU1r3gRIItzr7bNQid6O8ZYaE1GSQTa69WwhPC9flq/oYkRBwsAAACCeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="A9wSqI5i0iwGdf6L1CERNdmsTPgVu44ewj8QxTBYgsv1LCPUVF7YmWOvTappqB1139jAymxUW/RO8zmMqo4zlAAAAACNeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiRmxlZGdlQmlkZGluZ0FuZEF1Y3Rpb25TZXJ2ZXIiLCJleHBpcnkiOjE3MzY4MTI4MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><meta http-equiv="origin-trial" content="A+d7vJfYtay4OUbdtRPZA3y7bKQLsxaMEPmxgfhBGqKXNrdkCQeJlUwqa6EBbSfjwFtJWTrWIioXeMW+y8bWAgQAAACTeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiRmxlZGdlQmlkZGluZ0FuZEF1Y3Rpb25TZXJ2ZXIiLCJleHBpcnkiOjE3MzY4MTI4MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><script src="https://securepubads.g.doubleclick.net/pagead/managed/js/gpt/m202412090101/pubads_impl.js" async=""></script><link href="https://securepubads.g.doubleclick.net/pagead/managed/dict/m202412050101/gpt" rel="compression-dictionary"><script src="https://unpkg.com/@theidentityselector/thiss-ds@1.0.13/dist/thiss-ds.js" async=""></script><script src="https://cdn.cookielaw.org/scripttemplates/202402.1.0/otBannerSdk.js" async="" type="text/javascript"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
</style><style id="onetrust-style">#onetrust-banner-sdk{-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}#onetrust-banner-sdk .onetrust-vendors-list-handler{cursor:pointer;color:#1f96db;font-size:inherit;font-weight:bold;text-decoration:none;margin-left:5px}#onetrust-banner-sdk .onetrust-vendors-list-handler:hover{color:#1f96db}#onetrust-banner-sdk:focus{outline:2px solid #000;outline-offset:-2px}#onetrust-banner-sdk a:focus{outline:2px solid #000}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{outline-offset:1px}#onetrust-banner-sdk.ot-bnr-w-logo .ot-bnr-logo{height:64px;width:64px}#onetrust-banner-sdk .ot-tcf2-vendor-count.ot-text-bold{font-weight:bold}#onetrust-banner-sdk .ot-close-icon,#onetrust-pc-sdk .ot-close-icon,#ot-sync-ntfy .ot-close-icon{background-size:contain;background-repeat:no-repeat;background-position:center;height:12px;width:12px}#onetrust-banner-sdk .powered-by-logo,#onetrust-banner-sdk .ot-pc-footer-logo a,#onetrust-pc-sdk .powered-by-logo,#onetrust-pc-sdk .ot-pc-footer-logo a,#ot-sync-ntfy .powered-by-logo,#ot-sync-ntfy .ot-pc-footer-logo a{background-size:contain;background-repeat:no-repeat;background-position:center;height:25px;width:152px;display:block;text-decoration:none;font-size:.75em}#onetrust-banner-sdk .powered-by-logo:hover,#onetrust-banner-sdk .ot-pc-footer-logo a:hover,#onetrust-pc-sdk .powered-by-logo:hover,#onetrust-pc-sdk .ot-pc-footer-logo a:hover,#ot-sync-ntfy .powered-by-logo:hover,#ot-sync-ntfy .ot-pc-footer-logo a:hover{color:#565656}#onetrust-banner-sdk h3 *,#onetrust-banner-sdk h4 *,#onetrust-banner-sdk h6 *,#onetrust-banner-sdk button *,#onetrust-banner-sdk a[data-parent-id] *,#onetrust-pc-sdk h3 *,#onetrust-pc-sdk h4 *,#onetrust-pc-sdk h6 *,#onetrust-pc-sdk button *,#onetrust-pc-sdk a[data-parent-id] *,#ot-sync-ntfy h3 *,#ot-sync-ntfy h4 *,#ot-sync-ntfy h6 *,#ot-sync-ntfy button *,#ot-sync-ntfy a[data-parent-id] *{font-size:inherit;font-weight:inherit;color:inherit}#onetrust-banner-sdk .ot-hide,#onetrust-pc-sdk .ot-hide,#ot-sync-ntfy .ot-hide{display:none !important}#onetrust-banner-sdk button.ot-link-btn:hover,#onetrust-pc-sdk button.ot-link-btn:hover,#ot-sync-ntfy button.ot-link-btn:hover{text-decoration:underline;opacity:1}#onetrust-pc-sdk .ot-sdk-row .ot-sdk-column{padding:0}#onetrust-pc-sdk .ot-sdk-container{padding-right:0}#onetrust-pc-sdk .ot-sdk-row{flex-direction:initial;width:100%}#onetrust-pc-sdk [type=checkbox]:checked,#onetrust-pc-sdk [type=checkbox]:not(:checked){pointer-events:initial}#onetrust-pc-sdk [type=checkbox]:disabled+label::before,#onetrust-pc-sdk [type=checkbox]:disabled+label:after,#onetrust-pc-sdk [type=checkbox]:disabled+label{pointer-events:none;opacity:.7}#onetrust-pc-sdk #vendor-list-content{transform:translate3d(0, 0, 0)}#onetrust-pc-sdk li input[type=checkbox]{z-index:1}#onetrust-pc-sdk li .ot-checkbox label{z-index:2}#onetrust-pc-sdk li .ot-checkbox input[type=checkbox]{height:auto;width:auto}#onetrust-pc-sdk li .host-title a,#onetrust-pc-sdk li .ot-host-name a,#onetrust-pc-sdk li .accordion-text,#onetrust-pc-sdk li .ot-acc-txt{z-index:2;position:relative}#onetrust-pc-sdk input{margin:3px .1ex}#onetrust-pc-sdk .pc-logo,#onetrust-pc-sdk .ot-pc-logo{height:60px;width:180px;background-position:center;background-size:contain;background-repeat:no-repeat;display:inline-flex;justify-content:center;align-items:center}#onetrust-pc-sdk .pc-logo img,#onetrust-pc-sdk .ot-pc-logo img{max-height:100%;max-width:100%}#onetrust-pc-sdk .screen-reader-only,#onetrust-pc-sdk .ot-scrn-rdr,.ot-sdk-cookie-policy .screen-reader-only,.ot-sdk-cookie-policy .ot-scrn-rdr{border:0;clip:rect(0 0 0 0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}#onetrust-pc-sdk.ot-fade-in,.onetrust-pc-dark-filter.ot-fade-in,#onetrust-banner-sdk.ot-fade-in{animation-name:onetrust-fade-in;animation-duration:400ms;animation-timing-function:ease-in-out}#onetrust-pc-sdk.ot-hide{display:none !important}.onetrust-pc-dark-filter.ot-hide{display:none !important}#ot-sdk-btn.ot-sdk-show-settings,#ot-sdk-btn.optanon-show-settings{color:#68b631;border:1px solid #68b631;height:auto;white-space:normal;word-wrap:break-word;padding:.8em 2em;font-size:.8em;line-height:1.2;cursor:pointer;-moz-transition:.1s ease;-o-transition:.1s ease;-webkit-transition:1s ease;transition:.1s ease}#ot-sdk-btn.ot-sdk-show-settings:hover,#ot-sdk-btn.optanon-show-settings:hover{color:#fff;background-color:#68b631}.onetrust-pc-dark-filter{background:rgba(0,0,0,.5);z-index:2147483646;width:100%;height:100%;overflow:hidden;position:fixed;top:0;bottom:0;left:0}@keyframes onetrust-fade-in{0%{opacity:0}100%{opacity:1}}.ot-cookie-label{text-decoration:underline}@media only screen and (min-width: 426px)and (max-width: 896px)and (orientation: landscape){#onetrust-pc-sdk p{font-size:.75em}}#onetrust-banner-sdk .banner-option-input:focus+label{outline:1px solid #000;outline-style:auto}.category-vendors-list-handler+a:focus,.category-vendors-list-handler+a:focus-visible{outline:2px solid #000}#onetrust-pc-sdk .ot-userid-title{margin-top:10px}#onetrust-pc-sdk .ot-userid-title>span,#onetrust-pc-sdk .ot-userid-timestamp>span{font-weight:700}#onetrust-pc-sdk .ot-userid-desc{font-style:italic}#onetrust-pc-sdk .ot-host-desc a{pointer-events:initial}#onetrust-pc-sdk .ot-ven-hdr>p a{position:relative;z-index:2;pointer-events:initial}#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info a,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info a{margin-right:auto}#onetrust-pc-sdk .ot-pc-footer-logo img{width:136px;height:16px}#onetrust-pc-sdk .ot-pur-vdr-count{font-weight:400;font-size:.7rem;padding-top:3px;display:block}#onetrust-banner-sdk .ot-optout-signal,#onetrust-pc-sdk .ot-optout-signal{border:1px solid #32ae88;border-radius:3px;padding:5px;margin-bottom:10px;background-color:#f9fffa;font-size:.85rem;line-height:2}#onetrust-banner-sdk .ot-optout-signal .ot-optout-icon,#onetrust-pc-sdk .ot-optout-signal .ot-optout-icon{display:inline;margin-right:5px}#onetrust-banner-sdk .ot-optout-signal svg,#onetrust-pc-sdk .ot-optout-signal svg{height:20px;width:30px;transform:scale(0.5)}#onetrust-banner-sdk .ot-optout-signal svg path,#onetrust-pc-sdk .ot-optout-signal svg path{fill:#32ae88}#onetrust-banner-sdk,#onetrust-pc-sdk,#ot-sdk-cookie-policy,#ot-sync-ntfy{font-size:16px}#onetrust-banner-sdk *,#onetrust-banner-sdk ::after,#onetrust-banner-sdk ::before,#onetrust-pc-sdk *,#onetrust-pc-sdk ::after,#onetrust-pc-sdk ::before,#ot-sdk-cookie-policy *,#ot-sdk-cookie-policy ::after,#ot-sdk-cookie-policy ::before,#ot-sync-ntfy *,#ot-sync-ntfy ::after,#ot-sync-ntfy ::before{-webkit-box-sizing:content-box;-moz-box-sizing:content-box;box-sizing:content-box}#onetrust-banner-sdk div,#onetrust-banner-sdk span,#onetrust-banner-sdk h1,#onetrust-banner-sdk h2,#onetrust-banner-sdk h3,#onetrust-banner-sdk h4,#onetrust-banner-sdk h5,#onetrust-banner-sdk h6,#onetrust-banner-sdk p,#onetrust-banner-sdk img,#onetrust-banner-sdk svg,#onetrust-banner-sdk button,#onetrust-banner-sdk section,#onetrust-banner-sdk a,#onetrust-banner-sdk label,#onetrust-banner-sdk input,#onetrust-banner-sdk ul,#onetrust-banner-sdk li,#onetrust-banner-sdk nav,#onetrust-banner-sdk table,#onetrust-banner-sdk thead,#onetrust-banner-sdk tr,#onetrust-banner-sdk td,#onetrust-banner-sdk tbody,#onetrust-banner-sdk .ot-main-content,#onetrust-banner-sdk .ot-toggle,#onetrust-banner-sdk #ot-content,#onetrust-banner-sdk #ot-pc-content,#onetrust-banner-sdk .checkbox,#onetrust-pc-sdk div,#onetrust-pc-sdk span,#onetrust-pc-sdk h1,#onetrust-pc-sdk h2,#onetrust-pc-sdk h3,#onetrust-pc-sdk h4,#onetrust-pc-sdk h5,#onetrust-pc-sdk h6,#onetrust-pc-sdk p,#onetrust-pc-sdk img,#onetrust-pc-sdk svg,#onetrust-pc-sdk button,#onetrust-pc-sdk section,#onetrust-pc-sdk a,#onetrust-pc-sdk label,#onetrust-pc-sdk input,#onetrust-pc-sdk ul,#onetrust-pc-sdk li,#onetrust-pc-sdk nav,#onetrust-pc-sdk table,#onetrust-pc-sdk thead,#onetrust-pc-sdk tr,#onetrust-pc-sdk td,#onetrust-pc-sdk tbody,#onetrust-pc-sdk .ot-main-content,#onetrust-pc-sdk .ot-toggle,#onetrust-pc-sdk #ot-content,#onetrust-pc-sdk #ot-pc-content,#onetrust-pc-sdk .checkbox,#ot-sdk-cookie-policy div,#ot-sdk-cookie-policy span,#ot-sdk-cookie-policy h1,#ot-sdk-cookie-policy h2,#ot-sdk-cookie-policy h3,#ot-sdk-cookie-policy h4,#ot-sdk-cookie-policy h5,#ot-sdk-cookie-policy h6,#ot-sdk-cookie-policy p,#ot-sdk-cookie-policy img,#ot-sdk-cookie-policy svg,#ot-sdk-cookie-policy button,#ot-sdk-cookie-policy section,#ot-sdk-cookie-policy a,#ot-sdk-cookie-policy label,#ot-sdk-cookie-policy input,#ot-sdk-cookie-policy ul,#ot-sdk-cookie-policy li,#ot-sdk-cookie-policy nav,#ot-sdk-cookie-policy table,#ot-sdk-cookie-policy thead,#ot-sdk-cookie-policy tr,#ot-sdk-cookie-policy td,#ot-sdk-cookie-policy tbody,#ot-sdk-cookie-policy .ot-main-content,#ot-sdk-cookie-policy .ot-toggle,#ot-sdk-cookie-policy #ot-content,#ot-sdk-cookie-policy #ot-pc-content,#ot-sdk-cookie-policy .checkbox,#ot-sync-ntfy div,#ot-sync-ntfy span,#ot-sync-ntfy h1,#ot-sync-ntfy h2,#ot-sync-ntfy h3,#ot-sync-ntfy h4,#ot-sync-ntfy h5,#ot-sync-ntfy h6,#ot-sync-ntfy p,#ot-sync-ntfy img,#ot-sync-ntfy svg,#ot-sync-ntfy button,#ot-sync-ntfy section,#ot-sync-ntfy a,#ot-sync-ntfy label,#ot-sync-ntfy input,#ot-sync-ntfy ul,#ot-sync-ntfy li,#ot-sync-ntfy nav,#ot-sync-ntfy table,#ot-sync-ntfy thead,#ot-sync-ntfy tr,#ot-sync-ntfy td,#ot-sync-ntfy tbody,#ot-sync-ntfy .ot-main-content,#ot-sync-ntfy .ot-toggle,#ot-sync-ntfy #ot-content,#ot-sync-ntfy #ot-pc-content,#ot-sync-ntfy .checkbox{font-family:inherit;font-weight:normal;-webkit-font-smoothing:auto;letter-spacing:normal;line-height:normal;padding:0;margin:0;height:auto;min-height:0;max-height:none;width:auto;min-width:0;max-width:none;border-radius:0;border:none;clear:none;float:none;position:static;bottom:auto;left:auto;right:auto;top:auto;text-align:left;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;white-space:normal;background:none;overflow:visible;vertical-align:baseline;visibility:visible;z-index:auto;box-shadow:none}#onetrust-banner-sdk label:before,#onetrust-banner-sdk label:after,#onetrust-banner-sdk .checkbox:after,#onetrust-banner-sdk .checkbox:before,#onetrust-pc-sdk label:before,#onetrust-pc-sdk label:after,#onetrust-pc-sdk .checkbox:after,#onetrust-pc-sdk .checkbox:before,#ot-sdk-cookie-policy label:before,#ot-sdk-cookie-policy label:after,#ot-sdk-cookie-policy .checkbox:after,#ot-sdk-cookie-policy .checkbox:before,#ot-sync-ntfy label:before,#ot-sync-ntfy label:after,#ot-sync-ntfy .checkbox:after,#ot-sync-ntfy .checkbox:before{content:"";content:none}#onetrust-banner-sdk .ot-sdk-container,#onetrust-pc-sdk .ot-sdk-container,#ot-sdk-cookie-policy .ot-sdk-container{position:relative;width:100%;max-width:100%;margin:0 auto;padding:0 20px;box-sizing:border-box}#onetrust-banner-sdk .ot-sdk-column,#onetrust-banner-sdk .ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-column,#onetrust-pc-sdk .ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-column,#ot-sdk-cookie-policy .ot-sdk-columns{width:100%;float:left;box-sizing:border-box;padding:0;display:initial}@media(min-width: 400px){#onetrust-banner-sdk .ot-sdk-container,#onetrust-pc-sdk .ot-sdk-container,#ot-sdk-cookie-policy .ot-sdk-container{width:90%;padding:0}}@media(min-width: 550px){#onetrust-banner-sdk .ot-sdk-container,#onetrust-pc-sdk .ot-sdk-container,#ot-sdk-cookie-policy .ot-sdk-container{width:100%}#onetrust-banner-sdk .ot-sdk-column,#onetrust-banner-sdk .ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-column,#onetrust-pc-sdk .ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-column,#ot-sdk-cookie-policy .ot-sdk-columns{margin-left:4%}#onetrust-banner-sdk .ot-sdk-column:first-child,#onetrust-banner-sdk .ot-sdk-columns:first-child,#onetrust-pc-sdk .ot-sdk-column:first-child,#onetrust-pc-sdk .ot-sdk-columns:first-child,#ot-sdk-cookie-policy .ot-sdk-column:first-child,#ot-sdk-cookie-policy .ot-sdk-columns:first-child{margin-left:0}#onetrust-banner-sdk .ot-sdk-two.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-two.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-two.ot-sdk-columns{width:13.3333333333%}#onetrust-banner-sdk .ot-sdk-three.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-three.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-three.ot-sdk-columns{width:22%}#onetrust-banner-sdk .ot-sdk-four.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-four.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-four.ot-sdk-columns{width:30.6666666667%}#onetrust-banner-sdk .ot-sdk-eight.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-eight.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-eight.ot-sdk-columns{width:65.3333333333%}#onetrust-banner-sdk .ot-sdk-nine.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-nine.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-nine.ot-sdk-columns{width:74%}#onetrust-banner-sdk .ot-sdk-ten.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-ten.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-ten.ot-sdk-columns{width:82.6666666667%}#onetrust-banner-sdk .ot-sdk-eleven.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-eleven.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-eleven.ot-sdk-columns{width:91.3333333333%}#onetrust-banner-sdk .ot-sdk-twelve.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-twelve.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-twelve.ot-sdk-columns{width:100%;margin-left:0}}#onetrust-banner-sdk h1,#onetrust-banner-sdk h2,#onetrust-banner-sdk h3,#onetrust-banner-sdk h4,#onetrust-banner-sdk h5,#onetrust-banner-sdk h6,#onetrust-pc-sdk h1,#onetrust-pc-sdk h2,#onetrust-pc-sdk h3,#onetrust-pc-sdk h4,#onetrust-pc-sdk h5,#onetrust-pc-sdk h6,#ot-sdk-cookie-policy h1,#ot-sdk-cookie-policy h2,#ot-sdk-cookie-policy h3,#ot-sdk-cookie-policy h4,#ot-sdk-cookie-policy h5,#ot-sdk-cookie-policy h6{margin-top:0;font-weight:600;font-family:inherit}#onetrust-banner-sdk h1,#onetrust-pc-sdk h1,#ot-sdk-cookie-policy h1{font-size:1.5rem;line-height:1.2}#onetrust-banner-sdk h2,#onetrust-pc-sdk h2,#ot-sdk-cookie-policy h2{font-size:1.5rem;line-height:1.25}#onetrust-banner-sdk h3,#onetrust-pc-sdk h3,#ot-sdk-cookie-policy h3{font-size:1.5rem;line-height:1.3}#onetrust-banner-sdk h4,#onetrust-pc-sdk h4,#ot-sdk-cookie-policy h4{font-size:1.5rem;line-height:1.35}#onetrust-banner-sdk h5,#onetrust-pc-sdk h5,#ot-sdk-cookie-policy h5{font-size:1.5rem;line-height:1.5}#onetrust-banner-sdk h6,#onetrust-pc-sdk h6,#ot-sdk-cookie-policy h6{font-size:1.5rem;line-height:1.6}@media(min-width: 550px){#onetrust-banner-sdk h1,#onetrust-pc-sdk h1,#ot-sdk-cookie-policy h1{font-size:1.5rem}#onetrust-banner-sdk h2,#onetrust-pc-sdk h2,#ot-sdk-cookie-policy h2{font-size:1.5rem}#onetrust-banner-sdk h3,#onetrust-pc-sdk h3,#ot-sdk-cookie-policy h3{font-size:1.5rem}#onetrust-banner-sdk h4,#onetrust-pc-sdk h4,#ot-sdk-cookie-policy h4{font-size:1.5rem}#onetrust-banner-sdk h5,#onetrust-pc-sdk h5,#ot-sdk-cookie-policy h5{font-size:1.5rem}#onetrust-banner-sdk h6,#onetrust-pc-sdk h6,#ot-sdk-cookie-policy h6{font-size:1.5rem}}#onetrust-banner-sdk p,#onetrust-pc-sdk p,#ot-sdk-cookie-policy p{margin:0 0 1em 0;font-family:inherit;line-height:normal}#onetrust-banner-sdk a,#onetrust-pc-sdk a,#ot-sdk-cookie-policy a{color:#565656;text-decoration:underline}#onetrust-banner-sdk a:hover,#onetrust-pc-sdk a:hover,#ot-sdk-cookie-policy a:hover{color:#565656;text-decoration:none}#onetrust-banner-sdk .ot-sdk-button,#onetrust-banner-sdk button,#onetrust-pc-sdk .ot-sdk-button,#onetrust-pc-sdk button,#ot-sdk-cookie-policy .ot-sdk-button,#ot-sdk-cookie-policy button{margin-bottom:1rem;font-family:inherit}#onetrust-banner-sdk .ot-sdk-button,#onetrust-banner-sdk button,#onetrust-pc-sdk .ot-sdk-button,#onetrust-pc-sdk button,#ot-sdk-cookie-policy .ot-sdk-button,#ot-sdk-cookie-policy button{display:inline-block;height:38px;padding:0 30px;color:#555;text-align:center;font-size:.9em;font-weight:400;line-height:38px;letter-spacing:.01em;text-decoration:none;white-space:nowrap;background-color:rgba(0,0,0,0);border-radius:2px;border:1px solid #bbb;cursor:pointer;box-sizing:border-box}#onetrust-banner-sdk .ot-sdk-button:hover,#onetrust-banner-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):hover,#onetrust-banner-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):focus,#onetrust-pc-sdk .ot-sdk-button:hover,#onetrust-pc-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):hover,#onetrust-pc-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):focus,#ot-sdk-cookie-policy .ot-sdk-button:hover,#ot-sdk-cookie-policy :not(.ot-leg-btn-container)>button:not(.ot-link-btn):hover,#ot-sdk-cookie-policy :not(.ot-leg-btn-container)>button:not(.ot-link-btn):focus{color:#333;border-color:#888;opacity:.7}#onetrust-banner-sdk .ot-sdk-button:focus,#onetrust-banner-sdk :not(.ot-leg-btn-container)>button:focus,#onetrust-pc-sdk .ot-sdk-button:focus,#onetrust-pc-sdk :not(.ot-leg-btn-container)>button:focus,#ot-sdk-cookie-policy .ot-sdk-button:focus,#ot-sdk-cookie-policy :not(.ot-leg-btn-container)>button:focus{outline:2px solid #000}#onetrust-banner-sdk .ot-sdk-button.ot-sdk-button-primary,#onetrust-banner-sdk button.ot-sdk-button-primary,#onetrust-banner-sdk input[type=submit].ot-sdk-button-primary,#onetrust-banner-sdk input[type=reset].ot-sdk-button-primary,#onetrust-banner-sdk input[type=button].ot-sdk-button-primary,#onetrust-pc-sdk .ot-sdk-button.ot-sdk-button-primary,#onetrust-pc-sdk button.ot-sdk-button-primary,#onetrust-pc-sdk input[type=submit].ot-sdk-button-primary,#onetrust-pc-sdk input[type=reset].ot-sdk-button-primary,#onetrust-pc-sdk input[type=button].ot-sdk-button-primary,#ot-sdk-cookie-policy .ot-sdk-button.ot-sdk-button-primary,#ot-sdk-cookie-policy button.ot-sdk-button-primary,#ot-sdk-cookie-policy input[type=submit].ot-sdk-button-primary,#ot-sdk-cookie-policy input[type=reset].ot-sdk-button-primary,#ot-sdk-cookie-policy input[type=button].ot-sdk-button-primary{color:#fff;background-color:#33c3f0;border-color:#33c3f0}#onetrust-banner-sdk .ot-sdk-button.ot-sdk-button-primary:hover,#onetrust-banner-sdk button.ot-sdk-button-primary:hover,#onetrust-banner-sdk input[type=submit].ot-sdk-button-primary:hover,#onetrust-banner-sdk input[type=reset].ot-sdk-button-primary:hover,#onetrust-banner-sdk input[type=button].ot-sdk-button-primary:hover,#onetrust-banner-sdk .ot-sdk-button.ot-sdk-button-primary:focus,#onetrust-banner-sdk button.ot-sdk-button-primary:focus,#onetrust-banner-sdk input[type=submit].ot-sdk-button-primary:focus,#onetrust-banner-sdk input[type=reset].ot-sdk-button-primary:focus,#onetrust-banner-sdk input[type=button].ot-sdk-button-primary:focus,#onetrust-pc-sdk .ot-sdk-button.ot-sdk-button-primary:hover,#onetrust-pc-sdk button.ot-sdk-button-primary:hover,#onetrust-pc-sdk input[type=submit].ot-sdk-button-primary:hover,#onetrust-pc-sdk input[type=reset].ot-sdk-button-primary:hover,#onetrust-pc-sdk input[type=button].ot-sdk-button-primary:hover,#onetrust-pc-sdk .ot-sdk-button.ot-sdk-button-primary:focus,#onetrust-pc-sdk button.ot-sdk-button-primary:focus,#onetrust-pc-sdk input[type=submit].ot-sdk-button-primary:focus,#onetrust-pc-sdk input[type=reset].ot-sdk-button-primary:focus,#onetrust-pc-sdk input[type=button].ot-sdk-button-primary:focus,#ot-sdk-cookie-policy .ot-sdk-button.ot-sdk-button-primary:hover,#ot-sdk-cookie-policy button.ot-sdk-button-primary:hover,#ot-sdk-cookie-policy input[type=submit].ot-sdk-button-primary:hover,#ot-sdk-cookie-policy input[type=reset].ot-sdk-button-primary:hover,#ot-sdk-cookie-policy input[type=button].ot-sdk-button-primary:hover,#ot-sdk-cookie-policy .ot-sdk-button.ot-sdk-button-primary:focus,#ot-sdk-cookie-policy button.ot-sdk-button-primary:focus,#ot-sdk-cookie-policy input[type=submit].ot-sdk-button-primary:focus,#ot-sdk-cookie-policy input[type=reset].ot-sdk-button-primary:focus,#ot-sdk-cookie-policy input[type=button].ot-sdk-button-primary:focus{color:#fff;background-color:#1eaedb;border-color:#1eaedb}#onetrust-banner-sdk input[type=text],#onetrust-pc-sdk input[type=text],#ot-sdk-cookie-policy input[type=text]{height:38px;padding:6px 10px;background-color:#fff;border:1px solid #d1d1d1;border-radius:4px;box-shadow:none;box-sizing:border-box}#onetrust-banner-sdk input[type=text],#onetrust-pc-sdk input[type=text],#ot-sdk-cookie-policy input[type=text]{-webkit-appearance:none;-moz-appearance:none;appearance:none}#onetrust-banner-sdk input[type=text]:focus,#onetrust-pc-sdk input[type=text]:focus,#ot-sdk-cookie-policy input[type=text]:focus{border:1px solid #000;outline:0}#onetrust-banner-sdk label,#onetrust-pc-sdk label,#ot-sdk-cookie-policy label{display:block;margin-bottom:.5rem;font-weight:600}#onetrust-banner-sdk input[type=checkbox],#onetrust-pc-sdk input[type=checkbox],#ot-sdk-cookie-policy input[type=checkbox]{display:inline}#onetrust-banner-sdk ul,#onetrust-pc-sdk ul,#ot-sdk-cookie-policy ul{list-style:circle inside}#onetrust-banner-sdk ul,#onetrust-pc-sdk ul,#ot-sdk-cookie-policy ul{padding-left:0;margin-top:0}#onetrust-banner-sdk ul ul,#onetrust-pc-sdk ul ul,#ot-sdk-cookie-policy ul ul{margin:1.5rem 0 1.5rem 3rem;font-size:90%}#onetrust-banner-sdk li,#onetrust-pc-sdk li,#ot-sdk-cookie-policy li{margin-bottom:1rem}#onetrust-banner-sdk th,#onetrust-banner-sdk td,#onetrust-pc-sdk th,#onetrust-pc-sdk td,#ot-sdk-cookie-policy th,#ot-sdk-cookie-policy td{padding:12px 15px;text-align:left;border-bottom:1px solid #e1e1e1}#onetrust-banner-sdk button,#onetrust-pc-sdk button,#ot-sdk-cookie-policy button{margin-bottom:1rem;font-family:inherit}#onetrust-banner-sdk .ot-sdk-container:after,#onetrust-banner-sdk .ot-sdk-row:after,#onetrust-pc-sdk .ot-sdk-container:after,#onetrust-pc-sdk .ot-sdk-row:after,#ot-sdk-cookie-policy .ot-sdk-container:after,#ot-sdk-cookie-policy .ot-sdk-row:after{content:"";display:table;clear:both}#onetrust-banner-sdk .ot-sdk-row,#onetrust-pc-sdk .ot-sdk-row,#ot-sdk-cookie-policy .ot-sdk-row{margin:0;max-width:none;display:block}#onetrust-banner-sdk{box-shadow:0 0 18px rgba(0,0,0,.2)}#onetrust-banner-sdk.otFlat{position:fixed;z-index:2147483645;bottom:0;right:0;left:0;background-color:#fff;max-height:90%;overflow-x:hidden;overflow-y:auto}#onetrust-banner-sdk.otFlat.top{top:0px;bottom:auto}#onetrust-banner-sdk.otRelFont{font-size:1rem}#onetrust-banner-sdk>.ot-sdk-container{overflow:hidden}#onetrust-banner-sdk::-webkit-scrollbar{width:11px}#onetrust-banner-sdk::-webkit-scrollbar-thumb{border-radius:10px;background:#c1c1c1}#onetrust-banner-sdk{scrollbar-arrow-color:#c1c1c1;scrollbar-darkshadow-color:#c1c1c1;scrollbar-face-color:#c1c1c1;scrollbar-shadow-color:#c1c1c1}#onetrust-banner-sdk #onetrust-policy{margin:1.25em 0 .625em 2em;overflow:hidden}#onetrust-banner-sdk #onetrust-policy .ot-gv-list-handler{float:left;font-size:.82em;padding:0;margin-bottom:0;border:0;line-height:normal;height:auto;width:auto}#onetrust-banner-sdk #onetrust-policy-title{font-size:1.2em;line-height:1.3;margin-bottom:10px}#onetrust-banner-sdk #onetrust-policy-text{clear:both;text-align:left;font-size:.88em;line-height:1.4}#onetrust-banner-sdk #onetrust-policy-text *{font-size:inherit;line-height:inherit}#onetrust-banner-sdk #onetrust-policy-text a{font-weight:bold;margin-left:5px}#onetrust-banner-sdk #onetrust-policy-title,#onetrust-banner-sdk #onetrust-policy-text{color:dimgray;float:left}#onetrust-banner-sdk #onetrust-button-group-parent{min-height:1px;text-align:center}#onetrust-banner-sdk #onetrust-button-group{display:inline-block}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{background-color:#68b631;color:#fff;border-color:#68b631;margin-right:1em;min-width:125px;height:auto;white-space:normal;word-break:break-word;word-wrap:break-word;padding:12px 10px;line-height:1.2;font-size:.813em;font-weight:600}#onetrust-banner-sdk #onetrust-pc-btn-handler.cookie-setting-link{background-color:#fff;border:none;color:#68b631;text-decoration:underline;padding-left:0;padding-right:0}#onetrust-banner-sdk .onetrust-close-btn-ui{width:44px;height:44px;background-size:12px;border:none;position:relative;margin:auto;padding:0}#onetrust-banner-sdk .banner_logo{display:none}#onetrust-banner-sdk.ot-bnr-w-logo .ot-bnr-logo{position:absolute;top:50%;transform:translateY(-50%);left:0px}#onetrust-banner-sdk.ot-bnr-w-logo #onetrust-policy{margin-left:65px}#onetrust-banner-sdk .ot-b-addl-desc{clear:both;float:left;display:block}#onetrust-banner-sdk #banner-options{float:left;display:table;margin-right:0;margin-left:1em;width:calc(100% - 1em)}#onetrust-banner-sdk .banner-option-input{cursor:pointer;width:auto;height:auto;border:none;padding:0;padding-right:3px;margin:0 0 10px;font-size:.82em;line-height:1.4}#onetrust-banner-sdk .banner-option-input *{pointer-events:none;font-size:inherit;line-height:inherit}#onetrust-banner-sdk .banner-option-input[aria-expanded=true]~.banner-option-details{display:block;height:auto}#onetrust-banner-sdk .banner-option-input[aria-expanded=true] .ot-arrow-container{transform:rotate(90deg)}#onetrust-banner-sdk .banner-option{margin-bottom:12px;margin-left:0;border:none;float:left;padding:0}#onetrust-banner-sdk .banner-option:first-child{padding-left:2px}#onetrust-banner-sdk .banner-option:not(:first-child){padding:0;border:none}#onetrust-banner-sdk .banner-option-header{cursor:pointer;display:inline-block}#onetrust-banner-sdk .banner-option-header :first-child{color:dimgray;font-weight:bold;float:left}#onetrust-banner-sdk .banner-option-header .ot-arrow-container{display:inline-block;border-top:6px solid rgba(0,0,0,0);border-bottom:6px solid rgba(0,0,0,0);border-left:6px solid dimgray;margin-left:10px;vertical-align:middle}#onetrust-banner-sdk .banner-option-details{display:none;font-size:.83em;line-height:1.5;padding:10px 0px 5px 10px;margin-right:10px;height:0px}#onetrust-banner-sdk .banner-option-details *{font-size:inherit;line-height:inherit;color:dimgray}#onetrust-banner-sdk .ot-arrow-container,#onetrust-banner-sdk .banner-option-details{transition:all 300ms ease-in 0s;-webkit-transition:all 300ms ease-in 0s;-moz-transition:all 300ms ease-in 0s;-o-transition:all 300ms ease-in 0s}#onetrust-banner-sdk .ot-dpd-container{float:left}#onetrust-banner-sdk .ot-dpd-title{margin-bottom:10px}#onetrust-banner-sdk .ot-dpd-title,#onetrust-banner-sdk .ot-dpd-desc{font-size:.88em;line-height:1.4;color:dimgray}#onetrust-banner-sdk .ot-dpd-title *,#onetrust-banner-sdk .ot-dpd-desc *{font-size:inherit;line-height:inherit}#onetrust-banner-sdk.ot-iab-2 #onetrust-policy-text *{margin-bottom:0}#onetrust-banner-sdk.ot-iab-2 .onetrust-vendors-list-handler{display:block;margin-left:0;margin-top:5px;clear:both;margin-bottom:0;padding:0;border:0;height:auto;width:auto}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group button{display:block}#onetrust-banner-sdk.ot-close-btn-link{padding-top:25px}#onetrust-banner-sdk.ot-close-btn-link #onetrust-close-btn-container{top:15px;transform:none;right:15px}#onetrust-banner-sdk.ot-close-btn-link #onetrust-close-btn-container button{padding:0;white-space:pre-wrap;border:none;height:auto;line-height:1.5;text-decoration:underline;font-size:.69em}#onetrust-banner-sdk #onetrust-policy-text,#onetrust-banner-sdk .ot-dpd-desc,#onetrust-banner-sdk .ot-b-addl-desc{font-size:.813em;line-height:1.5}#onetrust-banner-sdk .ot-dpd-desc{margin-bottom:10px}#onetrust-banner-sdk .ot-dpd-desc>.ot-b-addl-desc{margin-top:10px;margin-bottom:10px;font-size:1em}@media only screen and (max-width: 425px){#onetrust-banner-sdk #onetrust-close-btn-container{position:absolute;top:6px;right:2px}#onetrust-banner-sdk #onetrust-policy{margin-left:0;margin-top:3em}#onetrust-banner-sdk #onetrust-button-group{display:block}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{width:100%}#onetrust-banner-sdk .onetrust-close-btn-ui{top:auto;transform:none}#onetrust-banner-sdk #onetrust-policy-title{display:inline;float:none}#onetrust-banner-sdk #banner-options{margin:0;padding:0;width:100%}}@media only screen and (min-width: 426px)and (max-width: 896px){#onetrust-banner-sdk #onetrust-close-btn-container{position:absolute;top:0;right:0}#onetrust-banner-sdk #onetrust-policy{margin-left:1em;margin-right:1em}#onetrust-banner-sdk .onetrust-close-btn-ui{top:10px;right:10px}#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{width:95%}#onetrust-banner-sdk.ot-iab-2 #onetrust-group-container{width:100%}#onetrust-banner-sdk.ot-bnr-w-logo #onetrust-button-group-parent{padding-left:50px}#onetrust-banner-sdk #onetrust-button-group-parent{width:100%;position:relative;margin-left:0}#onetrust-banner-sdk #onetrust-button-group button{display:inline-block}#onetrust-banner-sdk #onetrust-button-group{margin-right:0;text-align:center}#onetrust-banner-sdk .has-reject-all-button #onetrust-pc-btn-handler{float:left}#onetrust-banner-sdk .has-reject-all-button #onetrust-reject-all-handler,#onetrust-banner-sdk .has-reject-all-button #onetrust-accept-btn-handler{float:right}#onetrust-banner-sdk .has-reject-all-button #onetrust-button-group{width:calc(100% - 2em);margin-right:0}#onetrust-banner-sdk .has-reject-all-button #onetrust-pc-btn-handler.cookie-setting-link{padding-left:0px;text-align:left}#onetrust-banner-sdk.ot-buttons-fw .ot-sdk-three button{width:100%;text-align:center}#onetrust-banner-sdk.ot-buttons-fw #onetrust-button-group-parent button{float:none}#onetrust-banner-sdk.ot-buttons-fw #onetrust-pc-btn-handler.cookie-setting-link{text-align:center}}@media only screen and (min-width: 550px){#onetrust-banner-sdk .banner-option:not(:first-child){border-left:1px solid #d8d8d8;padding-left:25px}}@media only screen and (min-width: 425px)and (max-width: 550px){#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group,#onetrust-banner-sdk.ot-iab-2 #onetrust-policy,#onetrust-banner-sdk.ot-iab-2 .banner-option{width:100%}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-accept-btn-handler,#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-reject-all-handler,#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-pc-btn-handler{width:100%}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-accept-btn-handler,#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-reject-all-handler{float:left}}@media only screen and (min-width: 769px){#onetrust-banner-sdk #onetrust-button-group{margin-right:30%}#onetrust-banner-sdk #banner-options{margin-left:2em;margin-right:5em;margin-bottom:1.25em;width:calc(100% - 7em)}}@media only screen and (min-width: 897px)and (max-width: 1023px){#onetrust-banner-sdk.vertical-align-content #onetrust-button-group-parent{position:absolute;top:50%;left:75%;transform:translateY(-50%)}#onetrust-banner-sdk #onetrust-close-btn-container{top:50%;margin:auto;transform:translate(-50%, -50%);position:absolute;padding:0;right:0}#onetrust-banner-sdk #onetrust-close-btn-container button{position:relative;margin:0;right:-22px;top:2px}}@media only screen and (min-width: 1024px){#onetrust-banner-sdk #onetrust-close-btn-container{top:50%;margin:auto;transform:translate(-50%, -50%);position:absolute;right:0}#onetrust-banner-sdk #onetrust-close-btn-container button{right:-12px}#onetrust-banner-sdk #onetrust-policy{margin-left:2em}#onetrust-banner-sdk.vertical-align-content #onetrust-button-group-parent{position:absolute;top:50%;left:60%;transform:translateY(-50%)}#onetrust-banner-sdk .ot-optout-signal{width:50%}#onetrust-banner-sdk.ot-iab-2 #onetrust-policy-title{width:50%}#onetrust-banner-sdk.ot-iab-2 #onetrust-policy-text,#onetrust-banner-sdk.ot-iab-2 :not(.ot-dpd-desc)>.ot-b-addl-desc{margin-bottom:1em;width:50%;border-right:1px solid #d8d8d8;padding-right:1rem}#onetrust-banner-sdk.ot-iab-2 #onetrust-policy-text{margin-bottom:0;padding-bottom:1em}#onetrust-banner-sdk.ot-iab-2 :not(.ot-dpd-desc)>.ot-b-addl-desc{margin-bottom:0;padding-bottom:1em}#onetrust-banner-sdk.ot-iab-2 .ot-dpd-container{width:45%;padding-left:1rem;display:inline-block;float:none}#onetrust-banner-sdk.ot-iab-2 .ot-dpd-title{line-height:1.7}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group-parent{left:auto;right:4%;margin-left:0}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group button{display:block}#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-button-group-parent{margin:auto;width:30%}#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{width:60%}#onetrust-banner-sdk #onetrust-button-group{margin-right:auto}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{margin-top:1em}}@media only screen and (min-width: 890px){#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group-parent{padding-left:3%;padding-right:4%;margin-left:0}#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group{margin-right:0;margin-top:1.25em;width:100%}#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group button{width:100%;margin-bottom:5px;margin-top:5px}#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group button:last-of-type{margin-bottom:20px}}@media only screen and (min-width: 1280px){#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{width:55%}#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-button-group-parent{width:44%;padding-left:2%;padding-right:2%}#onetrust-banner-sdk:not(.ot-iab-2).vertical-align-content #onetrust-button-group-parent{position:absolute;left:55%}}
        #onetrust-consent-sdk #onetrust-banner-sdk {background-color: #FFF;}
            #onetrust-consent-sdk #onetrust-policy-title,
                    #onetrust-consent-sdk #onetrust-policy-text,
                    #onetrust-consent-sdk .ot-b-addl-desc,
                    #onetrust-consent-sdk .ot-dpd-desc,
                    #onetrust-consent-sdk .ot-dpd-title,
                    #onetrust-consent-sdk #onetrust-policy-text *:not(.onetrust-vendors-list-handler),
                    #onetrust-consent-sdk .ot-dpd-desc *:not(.onetrust-vendors-list-handler),
                    #onetrust-consent-sdk #onetrust-banner-sdk #banner-options *,
                    #onetrust-banner-sdk .ot-cat-header,
                    #onetrust-banner-sdk .ot-optout-signal
                    {
                        color: #2E2E2E;
                    }
            #onetrust-consent-sdk #onetrust-banner-sdk .banner-option-details {
                    background-color: #E9E9E9;}
             #onetrust-consent-sdk #onetrust-banner-sdk a[href],
                    #onetrust-consent-sdk #onetrust-banner-sdk a[href] font,
                    #onetrust-consent-sdk #onetrust-banner-sdk .ot-link-btn
                        {
                            color: #007398;
                        }#onetrust-consent-sdk #onetrust-accept-btn-handler,
                         #onetrust-banner-sdk #onetrust-reject-all-handler {
                            background-color: #007398;border-color: #007398;
                color: #FFF;
            }
            #onetrust-consent-sdk #onetrust-banner-sdk *:focus,
            #onetrust-consent-sdk #onetrust-banner-sdk:focus {
               outline-color: #000000;
               outline-width: 1px;
            }
            #onetrust-consent-sdk #onetrust-pc-btn-handler,
            #onetrust-consent-sdk #onetrust-pc-btn-handler.cookie-setting-link {
                color: #6CC04A; border-color: #6CC04A;
                background-color:
                #FFF;
            }/*! Extra code to blur out background */
.onetrust-pc-dark-filter{
background:rgba(0,0,0,.5);
z-index:2147483646;
width:100%;
height:100%;
overflow:hidden;
position:fixed;
top:0;
bottom:0;
left:0;
backdrop-filter: initial
}

/*! v6.12.0 2021-01-19 */
div#onetrust-consent-sdk #onetrust-banner-sdk{border-top:2px solid #eb6500!important;outline:1px solid transparent;box-shadow:none;padding:24px}div#onetrust-consent-sdk button{border-radius:0!important;box-shadow:none!important;box-sizing:border-box!important;font-size:20px!important;font-weight:400!important;letter-spacing:0!important;max-width:none!important;white-space:nowrap!important}div#onetrust-consent-sdk button:not(.ot-link-btn){background-color:#007398!important;border:2px solid #007398!important;color:#fff!important;height:48px!important;padding:0 1em!important;width:auto!important}div#onetrust-consent-sdk button:hover{background-color:#fff!important;border-color:#eb6500!important;color:#2e2e2e!important}div#onetrust-consent-sdk button.ot-link-btn{color:#007398!important;font-size:16px!important;text-decoration:underline}div#onetrust-consent-sdk button.ot-link-btn:hover{color: #2e2e2e!important;text-decoration-color:#eb6500!important}div#onetrust-consent-sdk a,div#onetrust-pc-sdk a{color:#007398!important;text-decoration:underline!important}div#onetrust-consent-sdk a,div#onetrust-consent-sdk button,div#onetrust-consent-sdk p:hover{opacity:1!important}div#onetrust-consent-sdk a:focus,div#onetrust-consent-sdk button:focus,div#onetrust-consent-sdk input:focus{outline:2px solid #eb6500!important;outline-offset:1px!important}div#onetrust-banner-sdk .ot-sdk-container{padding:0;width:auto}div#onetrust-banner-sdk .ot-sdk-row{align-items:flex-start;box-sizing:border-box;display:flex;flex-direction:column;justify-content:space-between;margin:auto;max-width:1152px}div#onetrust-banner-sdk .ot-sdk-row:after{display:none}div#onetrust-banner-sdk #onetrust-group-container,div#onetrust-banner-sdk.ot-bnr-flift:not(.ot-iab-2) #onetrust-group-container,div#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{flex-grow:1;width:auto}div#onetrust-banner-sdk #onetrust-policy,div#onetrust-banner-sdk.ot-bnr-flift #onetrust-policy{margin:0;overflow:visible}div#onetrust-banner-sdk.ot-bnr-flift #onetrust-policy-text,div#onetrust-consent-sdk #onetrust-policy-text{font-size:16px;line-height:24px;max-width:44em;margin:0}div#onetrust-consent-sdk #onetrust-policy-text a[href]{font-weight:400;margin-left:8px}div#onetrust-banner-sdk #onetrust-button-group-parent{flex:0 0 auto;margin:32px 0 0;width:100%}div#onetrust-banner-sdk #onetrust-button-group{display:flex;flex-direction:row;flex-wrap:wrap;justify-content:flex-end;margin:-8px}div#onetrust-banner-sdk .banner-actions-container{display:flex;flex:1 0 auto}div#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group button:last-of-type,div#onetrust-consent-sdk #onetrust-accept-btn-handler,div#onetrust-consent-sdk #onetrust-pc-btn-handler{flex:1 0 auto;margin:8px;width:auto}div#onetrust-consent-sdk #onetrust-pc-btn-handler{background-color:#fff!important;color:#2e2e2e!important}div#onetrust-banner-sdk #onetrust-close-btn-container{display:none}@media only screen and (min-width:556px){div#onetrust-consent-sdk #onetrust-banner-sdk{padding:40px}div#onetrust-banner-sdk #onetrust-policy{margin:0 40px 0 0}div#onetrust-banner-sdk .ot-sdk-row{align-items:center;flex-direction:row}div#onetrust-banner-sdk #onetrust-button-group-parent,div#onetrust-banner-sdk.ot-bnr-flift:not(.ot-iab-2) #onetrust-button-group-parent,div#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-button-group-parent{margin:0;padding:0;width:auto}div#onetrust-banner-sdk #onetrust-button-group,div#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group{align-items:stretch;flex-direction:column-reverse;margin:0}div#onetrust-consent-sdk #onetrust-accept-btn-handler,div#onetrust-consent-sdk #onetrust-pc-btn-handler{flex:1 0 auto}}@media only screen and (min-width:768px){div#onetrust-banner-sdk #onetrust-policy{margin:0 48px 0 0}div#onetrust-consent-sdk #onetrust-banner-sdk{padding:48px}}div#onetrust-consent-sdk #onetrust-pc-sdk h5{font-size:16px;line-height:24px}div#onetrust-consent-sdk #onetrust-pc-sdk p,div#onetrust-pc-sdk #ot-pc-desc,div#onetrust-pc-sdk .category-host-list-handler,div#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{font-size:16px;font-weight:400;line-height:24px}div#onetrust-consent-sdk a:hover,div#onetrust-pc-sdk a:hover{color:#2e2e2e!important;text-decoration-color:#eb6500!important}div#onetrust-pc-sdk{border-radius:0;bottom:0;height:auto;left:0;margin:auto;max-width:100%;overflow:hidden;right:0;top:0;width:512px;max-height:800px}div#onetrust-pc-sdk .ot-pc-header{display:none}div#onetrust-pc-sdk #ot-pc-content{overscroll-behavior:contain;padding:0 12px 0 24px;margin:16px 4px 0 0;top:0;right:16px;left:0;width:auto}div#onetrust-pc-sdk #ot-category-title,div#onetrust-pc-sdk #ot-pc-title{font-size:24px;font-weight:400;line-height:32px;margin:0 0 16px}div#onetrust-pc-sdk #ot-pc-desc{padding:0}div#onetrust-pc-sdk #ot-pc-desc a{display:inline}div#onetrust-pc-sdk #accept-recommended-btn-handler{display:none!important}div#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:2px solid #eb6500;outline-offset:-1px;transition:none}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item{border-width:0 0 2px}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item:first-of-type{border-width:2px 0}div#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{padding:8px 0;width:100%}div#onetrust-pc-sdk .ot-plus-minus{transform:translateY(2px)}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item>button{background:0 0!important;border:0!important;height:44px!important;max-width:none!important;width:calc(100% - 48px)!important}div#onetrust-consent-sdk #onetrust-pc-sdk h5{font-weight:700}div#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr{padding:0}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item .ot-acc-grpdesc{padding:0;width:100%}div#onetrust-pc-sdk .ot-acc-grpcntr .ot-subgrp-cntr{border:0;padding:0}div#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp{margin:0}div#onetrust-pc-sdk .ot-always-active-group .ot-cat-header{width:calc(100% - 160px)}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{width:calc(100% - 88px)}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active{color:#2e2e2e;font-size:12px;font-weight:400;line-height:1.5;padding-right:48px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active:before{border-radius:12px;position:absolute;right:0;top:0;content:'';background:#fff;border:2px solid #939393;box-sizing:border-box;height:20px;width:40px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active:after{border-radius:50%;position:absolute;right:5px;top:4px;content:'';background-color:#eb6500;height:12px;width:12px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active,div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl{right:2px}div#onetrust-pc-sdk .ot-switch{display:block;height:20px;width:40px}div#onetrust-pc-sdk .ot-tgl input+.ot-switch .ot-switch-nob,div#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob{background:#fff;border:2px solid #939393;box-sizing:border-box;height:20px;width:40px}div#onetrust-pc-sdk .ot-tgl input+.ot-switch .ot-switch-nob:before{background-color:#737373;height:8px;left:4px;top:4px;width:8px}div#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob:before{background-color:#eb6500;height:12px;left:0;top:2px;width:12px}div#onetrust-pc-sdk .ot-tgl input:focus+.ot-switch .ot-switch-nob{box-shadow:0 0;outline:2px solid #eb6500!important;outline-offset:1px;transition:none}div#onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-grpcntr.ot-acc-txt{background-color:transparent;padding-left:3px}div#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr,div#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr{overflow:visible;width:100%}div#onetrust-pc-sdk .ot-pc-footer{border-top:0 solid}div#onetrust-pc-sdk .ot-btn-container{padding-top:24px;text-align:center}div#onetrust-pc-sdk .ot-pc-footer button{margin:8px 0;background-color:#fff}div#onetrust-pc-sdk .ot-pc-footer-logo{background-color:#fff}div#onetrust-pc-sdk #ot-lst-title span{font-size:24px;font-weight:400;line-height:32px}div#onetrust-pc-sdk #ot-host-lst .ot-host-desc,div#onetrust-pc-sdk #ot-host-lst .ot-host-expand,div#onetrust-pc-sdk #ot-host-lst .ot-host-name,div#onetrust-pc-sdk #ot-host-lst .ot-host-name a,div#onetrust-pc-sdk .back-btn-handler,div#onetrust-pc-sdk .ot-host-opt li>div div{font-size:16px;font-weight:400;line-height:24px}div#onetrust-pc-sdk #ot-host-lst .ot-acc-txt{width:100%}div#onetrust-pc-sdk #ot-pc-lst{top:0}div#onetrust-pc-sdk .back-btn-handler{text-decoration:none!important}div#onetrust-pc-sdk #filter-btn-handler:hover svg{filter:invert(1)}div#onetrust-pc-sdk .back-btn-handler svg{width:16px;height:16px}div#onetrust-pc-sdk .ot-host-item>button{background:0 0!important;border:0!important;height:66px!important;max-width:none!important;width:calc(100% - 5px)!important;transform:translate(2px,2px)}div#onetrust-pc-sdk .ot-host-item{border-bottom:2px solid #b9b9b9;padding:0}div#onetrust-pc-sdk .ot-host-item .ot-acc-hdr{margin:0 0 -6px;padding:8px 0}div#onetrust-pc-sdk ul li:first-child{border-top:2px solid #b9b9b9}div#onetrust-pc-sdk .ot-host-item .ot-plus-minus{margin:0 8px 0 0}div#onetrust-pc-sdk .ot-search-cntr{width:calc(100% - 48px)}div#onetrust-pc-sdk .ot-host-opt .ot-host-info{background-color:transparent}div#onetrust-pc-sdk .ot-host-opt li>div div{padding:0}div#onetrust-pc-sdk #vendor-search-handler{border-radius:0;border-color:#939393;border-style:solid;border-width:2px 0 2px 2px;font-size:20px;height:48px;margin:0}div#onetrust-pc-sdk #ot-pc-hdr{margin-left:24px}div#onetrust-pc-sdk .ot-lst-subhdr{width:calc(100% - 24px)}div#onetrust-pc-sdk .ot-lst-subhdr svg{right:0;top:8px}div#onetrust-pc-sdk .ot-fltr-cntr{box-sizing:border-box;right:0;width:48px}div#onetrust-pc-sdk #filter-btn-handler{width:48px!important;padding:8px!important}div#onetrust-consent-sdk #onetrust-pc-sdk #clear-filters-handler,div#onetrust-pc-sdk button#filter-apply-handler,div#onetrust-pc-sdk button#filter-cancel-handler{height:2em!important;padding-left:14px!important;padding-right:14px!important}div#onetrust-pc-sdk #ot-fltr-cnt{box-shadow:0 0;border:1px solid #8e8e8e;border-radius:0}div#onetrust-pc-sdk .ot-fltr-scrlcnt{max-height:calc(100% - 80px)}div#onetrust-pc-sdk #ot-fltr-modal{max-height:400px}div#onetrust-pc-sdk .ot-fltr-opt{margin-bottom:16px}div#onetrust-pc-sdk #ot-lst-cnt{margin-left:24px;width:calc(100% - 48px)}div#onetrust-pc-sdk #ot-anchor{display:none!important}

/* 2023-12-04  Fix for button order in mobile view*/
@media (max-width: 550px) {
  #onetrust-accept-btn-handler {order: 1;  }
  #onetrust-reject-all-handler { order: 2;  }
  #onetrust-pc-btn-handler { order: 3;  }
}


/*! Extra code to blur our background */
.onetrust-pc-dark-filter{
backdrop-filter: blur(3px)
}
#onetrust-pc-sdk.otPcCenter{overflow:hidden;position:fixed;margin:0 auto;top:5%;right:0;left:0;width:40%;max-width:575px;min-width:575px;border-radius:2.5px;z-index:2147483647;background-color:#fff;-webkit-box-shadow:0px 2px 10px -3px #999;-moz-box-shadow:0px 2px 10px -3px #999;box-shadow:0px 2px 10px -3px #999}#onetrust-pc-sdk.otPcCenter[dir=rtl]{right:0;left:0}#onetrust-pc-sdk.otRelFont{font-size:1rem}#onetrust-pc-sdk .ot-optout-signal{margin-top:.625rem}#onetrust-pc-sdk #ot-addtl-venlst .ot-arw-cntr,#onetrust-pc-sdk #ot-addtl-venlst .ot-plus-minus,#onetrust-pc-sdk .ot-hide-tgl{visibility:hidden}#onetrust-pc-sdk #ot-addtl-venlst .ot-arw-cntr *,#onetrust-pc-sdk #ot-addtl-venlst .ot-plus-minus *,#onetrust-pc-sdk .ot-hide-tgl *{visibility:hidden}#onetrust-pc-sdk #ot-gn-venlst .ot-ven-item .ot-acc-hdr{min-height:40px}#onetrust-pc-sdk .ot-pc-header{height:39px;padding:10px 0 10px 30px;border-bottom:1px solid #e9e9e9}#onetrust-pc-sdk #ot-pc-title,#onetrust-pc-sdk #ot-category-title,#onetrust-pc-sdk .ot-cat-header,#onetrust-pc-sdk #ot-lst-title,#onetrust-pc-sdk .ot-ven-hdr .ot-ven-name,#onetrust-pc-sdk .ot-always-active{font-weight:bold;color:dimgray}#onetrust-pc-sdk .ot-always-active-group .ot-cat-header{width:55%;font-weight:700}#onetrust-pc-sdk .ot-cat-item p{clear:both;float:left;margin-top:10px;margin-bottom:5px;line-height:1.5;font-size:.812em;color:dimgray}#onetrust-pc-sdk .ot-close-icon{height:44px;width:44px;background-size:10px}#onetrust-pc-sdk #ot-pc-title{float:left;font-size:1em;line-height:1.5;margin-bottom:10px;margin-top:10px;width:100%}#onetrust-pc-sdk #accept-recommended-btn-handler{margin-right:10px;margin-bottom:25px;outline-offset:-1px}#onetrust-pc-sdk #ot-pc-desc{clear:both;width:100%;font-size:.812em;line-height:1.5;margin-bottom:25px}#onetrust-pc-sdk #ot-pc-desc a{margin-left:5px}#onetrust-pc-sdk #ot-pc-desc *{font-size:inherit;line-height:inherit}#onetrust-pc-sdk #ot-pc-desc ul li{padding:10px 0px}#onetrust-pc-sdk a{color:#656565;cursor:pointer}#onetrust-pc-sdk a:hover{color:#3860be}#onetrust-pc-sdk label{margin-bottom:0}#onetrust-pc-sdk #vdr-lst-dsc{font-size:.812em;line-height:1.5;padding:10px 15px 5px 15px}#onetrust-pc-sdk button{max-width:394px;padding:12px 30px;line-height:1;word-break:break-word;word-wrap:break-word;white-space:normal;font-weight:bold;height:auto}#onetrust-pc-sdk .ot-link-btn{padding:0;margin-bottom:0;border:0;font-weight:normal;line-height:normal;width:auto;height:auto}#onetrust-pc-sdk #ot-pc-content{position:absolute;overflow-y:scroll;padding-left:0px;padding-right:30px;top:60px;bottom:110px;margin:1px 3px 0 30px;width:calc(100% - 63px)}#onetrust-pc-sdk .ot-vs-list .ot-always-active,#onetrust-pc-sdk .ot-cat-grp .ot-always-active{float:right;clear:none;color:#3860be;margin:0;font-size:.813em;line-height:1.3}#onetrust-pc-sdk .ot-pc-scrollbar::-webkit-scrollbar-track{margin-right:20px}#onetrust-pc-sdk .ot-pc-scrollbar::-webkit-scrollbar{width:11px}#onetrust-pc-sdk .ot-pc-scrollbar::-webkit-scrollbar-thumb{border-radius:10px;background:#d8d8d8}#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:#000 1px solid}#onetrust-pc-sdk .ot-pc-scrollbar{scrollbar-arrow-color:#d8d8d8;scrollbar-darkshadow-color:#d8d8d8;scrollbar-face-color:#d8d8d8;scrollbar-shadow-color:#d8d8d8}#onetrust-pc-sdk .save-preference-btn-handler{margin-right:20px}#onetrust-pc-sdk .ot-pc-refuse-all-handler{margin-right:10px}#onetrust-pc-sdk #ot-pc-desc .privacy-notice-link{margin-left:0;margin-right:8px}#onetrust-pc-sdk #ot-pc-desc .ot-imprint-handler{margin-left:0;margin-right:8px}#onetrust-pc-sdk .ot-subgrp-cntr{display:inline-block;clear:both;width:100%;padding-top:15px}#onetrust-pc-sdk .ot-switch+.ot-subgrp-cntr{padding-top:10px}#onetrust-pc-sdk ul.ot-subgrps{margin:0;font-size:initial}#onetrust-pc-sdk ul.ot-subgrps li p,#onetrust-pc-sdk ul.ot-subgrps li h5{font-size:.813em;line-height:1.4;color:dimgray}#onetrust-pc-sdk ul.ot-subgrps .ot-switch{min-height:auto}#onetrust-pc-sdk ul.ot-subgrps .ot-switch-nob{top:0}#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr{display:inline-block;width:100%}#onetrust-pc-sdk ul.ot-subgrps .ot-acc-txt{margin:0}#onetrust-pc-sdk ul.ot-subgrps li{padding:0;border:none}#onetrust-pc-sdk ul.ot-subgrps li h5{position:relative;top:5px;font-weight:bold;margin-bottom:0;float:left}#onetrust-pc-sdk li.ot-subgrp{margin-left:20px;overflow:auto}#onetrust-pc-sdk li.ot-subgrp>h5{width:calc(100% - 100px)}#onetrust-pc-sdk .ot-cat-item p>ul,#onetrust-pc-sdk li.ot-subgrp p>ul{margin:0px;list-style:disc;margin-left:15px;font-size:inherit}#onetrust-pc-sdk .ot-cat-item p>ul li,#onetrust-pc-sdk li.ot-subgrp p>ul li{font-size:inherit;padding-top:10px;padding-left:0px;padding-right:0px;border:none}#onetrust-pc-sdk .ot-cat-item p>ul li:last-child,#onetrust-pc-sdk li.ot-subgrp p>ul li:last-child{padding-bottom:10px}#onetrust-pc-sdk .ot-pc-logo{height:40px;width:120px}#onetrust-pc-sdk .ot-pc-footer{position:absolute;bottom:0px;width:100%;max-height:160px;border-top:1px solid #d8d8d8}#onetrust-pc-sdk.ot-ftr-stacked .ot-pc-refuse-all-handler{margin-bottom:0px}#onetrust-pc-sdk.ot-ftr-stacked #ot-pc-content{bottom:160px}#onetrust-pc-sdk.ot-ftr-stacked .ot-pc-footer button{width:100%;max-width:none}#onetrust-pc-sdk.ot-ftr-stacked .ot-btn-container{margin:0 30px;width:calc(100% - 60px);padding-right:0}#onetrust-pc-sdk .ot-pc-footer-logo{height:30px;width:100%;text-align:right;background:#f4f4f4}#onetrust-pc-sdk .ot-pc-footer-logo a{display:inline-block;margin-top:5px;margin-right:10px}#onetrust-pc-sdk[dir=rtl] .ot-pc-footer-logo{direction:rtl}#onetrust-pc-sdk[dir=rtl] .ot-pc-footer-logo a{margin-right:25px}#onetrust-pc-sdk .ot-tgl{float:right;position:relative;z-index:1}#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob{background-color:#468254;border:1px solid #fff}#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob:before{-webkit-transform:translateX(20px);-ms-transform:translateX(20px);transform:translateX(20px);background-color:#fff;border-color:#fff}#onetrust-pc-sdk .ot-tgl input:focus+.ot-switch{outline:#000 solid 1px}#onetrust-pc-sdk .ot-switch{position:relative;display:inline-block;width:45px;height:25px}#onetrust-pc-sdk .ot-switch-nob{position:absolute;cursor:pointer;top:0;left:0;right:0;bottom:0;background-color:#767676;border:1px solid #ddd;transition:all .2s ease-in 0s;-moz-transition:all .2s ease-in 0s;-o-transition:all .2s ease-in 0s;-webkit-transition:all .2s ease-in 0s;border-radius:20px}#onetrust-pc-sdk .ot-switch-nob:before{position:absolute;content:"";height:18px;width:18px;bottom:3px;left:3px;background-color:#fff;-webkit-transition:.4s;transition:.4s;border-radius:20px}#onetrust-pc-sdk .ot-chkbox input:checked~label::before{background-color:#3860be}#onetrust-pc-sdk .ot-chkbox input+label::after{content:none;color:#fff}#onetrust-pc-sdk .ot-chkbox input:checked+label::after{content:""}#onetrust-pc-sdk .ot-chkbox input:focus+label::before{outline-style:solid;outline-width:2px;outline-style:auto}#onetrust-pc-sdk .ot-chkbox label{position:relative;display:inline-block;padding-left:30px;cursor:pointer;font-weight:500}#onetrust-pc-sdk .ot-chkbox label::before,#onetrust-pc-sdk .ot-chkbox label::after{position:absolute;content:"";display:inline-block;border-radius:3px}#onetrust-pc-sdk .ot-chkbox label::before{height:18px;width:18px;border:1px solid #3860be;left:0px;top:auto}#onetrust-pc-sdk .ot-chkbox label::after{height:5px;width:9px;border-left:3px solid;border-bottom:3px solid;transform:rotate(-45deg);-o-transform:rotate(-45deg);-ms-transform:rotate(-45deg);-webkit-transform:rotate(-45deg);left:4px;top:5px}#onetrust-pc-sdk .ot-label-txt{display:none}#onetrust-pc-sdk .ot-chkbox input,#onetrust-pc-sdk .ot-tgl input{position:absolute;opacity:0;width:0;height:0}#onetrust-pc-sdk .ot-arw-cntr{float:right;position:relative;pointer-events:none}#onetrust-pc-sdk .ot-arw-cntr .ot-arw{width:16px;height:16px;margin-left:5px;color:dimgray;display:inline-block;vertical-align:middle;-webkit-transition:all 150ms ease-in 0s;-moz-transition:all 150ms ease-in 0s;-o-transition:all 150ms ease-in 0s;transition:all 150ms ease-in 0s}#onetrust-pc-sdk input:checked~.ot-acc-hdr .ot-arw,#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-arw-cntr svg{transform:rotate(90deg);-o-transform:rotate(90deg);-ms-transform:rotate(90deg);-webkit-transform:rotate(90deg)}#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:#000 1px solid}#onetrust-pc-sdk .ot-tgl-cntr,#onetrust-pc-sdk .ot-arw-cntr{display:inline-block}#onetrust-pc-sdk .ot-tgl-cntr{width:45px;float:right;margin-top:2px}#onetrust-pc-sdk #ot-lst-cnt .ot-tgl-cntr{margin-top:10px}#onetrust-pc-sdk .ot-always-active-subgroup{width:auto;padding-left:0px !important;top:3px;position:relative}#onetrust-pc-sdk .ot-label-status{padding-left:5px;font-size:.75em;display:none}#onetrust-pc-sdk .ot-arw-cntr{margin-top:-1px}#onetrust-pc-sdk .ot-arw-cntr svg{-webkit-transition:all 300ms ease-in 0s;-moz-transition:all 300ms ease-in 0s;-o-transition:all 300ms ease-in 0s;transition:all 300ms ease-in 0s;height:10px;width:10px}#onetrust-pc-sdk input:checked~.ot-acc-hdr .ot-arw{transform:rotate(90deg);-o-transform:rotate(90deg);-ms-transform:rotate(90deg);-webkit-transform:rotate(90deg)}#onetrust-pc-sdk .ot-arw{width:10px;margin-left:15px;transition:all 300ms ease-in 0s;-webkit-transition:all 300ms ease-in 0s;-moz-transition:all 300ms ease-in 0s;-o-transition:all 300ms ease-in 0s}#onetrust-pc-sdk .ot-vlst-cntr{margin-bottom:0}#onetrust-pc-sdk .ot-hlst-cntr{margin-top:5px;display:inline-block;width:100%}#onetrust-pc-sdk .category-vendors-list-handler,#onetrust-pc-sdk .category-vendors-list-handler+a,#onetrust-pc-sdk .category-host-list-handler{clear:both;color:#3860be;margin-left:0;font-size:.813em;text-decoration:none;float:left;overflow:hidden}#onetrust-pc-sdk .category-vendors-list-handler:hover,#onetrust-pc-sdk .category-vendors-list-handler+a:hover,#onetrust-pc-sdk .category-host-list-handler:hover{text-decoration-line:underline}#onetrust-pc-sdk .category-vendors-list-handler+a{clear:none}#onetrust-pc-sdk .ot-vlst-cntr .ot-ext-lnk,#onetrust-pc-sdk .ot-ven-hdr .ot-ext-lnk{display:inline-block;height:13px;width:13px;background-repeat:no-repeat;margin-left:1px;margin-top:6px;cursor:pointer}#onetrust-pc-sdk .ot-ven-hdr .ot-ext-lnk{margin-bottom:-1px}#onetrust-pc-sdk .back-btn-handler{font-size:1em;text-decoration:none}#onetrust-pc-sdk .back-btn-handler:hover{opacity:.6}#onetrust-pc-sdk #ot-lst-title h3{display:inline-block;word-break:break-word;word-wrap:break-word;margin-bottom:0;color:#656565;font-size:1em;font-weight:bold;margin-left:15px}#onetrust-pc-sdk #ot-lst-title{margin:10px 0 10px 0px;font-size:1em;text-align:left}#onetrust-pc-sdk #ot-pc-hdr{margin:0 0 0 30px;height:auto;width:auto}#onetrust-pc-sdk #ot-pc-hdr input::placeholder{color:#d4d4d4;font-style:italic}#onetrust-pc-sdk #vendor-search-handler{height:31px;width:100%;border-radius:50px;font-size:.8em;padding-right:35px;padding-left:15px;float:left;margin-left:15px}#onetrust-pc-sdk .ot-ven-name{display:block;width:auto;padding-right:5px}#onetrust-pc-sdk #ot-lst-cnt{overflow-y:auto;margin-left:20px;margin-right:7px;width:calc(100% - 27px);max-height:calc(100% - 80px);height:100%;transform:translate3d(0, 0, 0)}#onetrust-pc-sdk #ot-pc-lst{width:100%;bottom:100px;position:absolute;top:60px}#onetrust-pc-sdk #ot-pc-lst:not(.ot-enbl-chr) .ot-tgl-cntr .ot-arw-cntr,#onetrust-pc-sdk #ot-pc-lst:not(.ot-enbl-chr) .ot-tgl-cntr .ot-arw-cntr *{visibility:hidden}#onetrust-pc-sdk #ot-pc-lst .ot-tgl-cntr{right:12px;position:absolute}#onetrust-pc-sdk #ot-pc-lst .ot-arw-cntr{float:right;position:relative}#onetrust-pc-sdk #ot-pc-lst .ot-arw{margin-left:10px}#onetrust-pc-sdk #ot-pc-lst .ot-acc-hdr{overflow:hidden;cursor:pointer}#onetrust-pc-sdk .ot-vlst-cntr{overflow:hidden}#onetrust-pc-sdk #ot-sel-blk{overflow:hidden;width:100%;position:sticky;position:-webkit-sticky;top:0;z-index:3}#onetrust-pc-sdk #ot-back-arw{height:12px;width:12px}#onetrust-pc-sdk .ot-lst-subhdr{width:100%;display:inline-block}#onetrust-pc-sdk .ot-search-cntr{float:left;width:78%;position:relative}#onetrust-pc-sdk .ot-search-cntr>svg{width:30px;height:30px;position:absolute;float:left;right:-15px}#onetrust-pc-sdk .ot-fltr-cntr{float:right;right:50px;position:relative}#onetrust-pc-sdk #filter-btn-handler{background-color:#3860be;border-radius:17px;display:inline-block;position:relative;width:32px;height:32px;-moz-transition:.1s ease;-o-transition:.1s ease;-webkit-transition:1s ease;transition:.1s ease;padding:0;margin:0}#onetrust-pc-sdk #filter-btn-handler:hover{background-color:#3860be}#onetrust-pc-sdk #filter-btn-handler svg{width:12px;height:12px;margin:3px 10px 0 10px;display:block;position:static;right:auto;top:auto}#onetrust-pc-sdk .ot-ven-link,#onetrust-pc-sdk .ot-ven-legclaim-link{color:#3860be;text-decoration:none;font-weight:100;display:inline-block;padding-top:10px;transform:translate(0, 1%);-o-transform:translate(0, 1%);-ms-transform:translate(0, 1%);-webkit-transform:translate(0, 1%);position:relative;z-index:2}#onetrust-pc-sdk .ot-ven-link *,#onetrust-pc-sdk .ot-ven-legclaim-link *{font-size:inherit}#onetrust-pc-sdk .ot-ven-link:hover,#onetrust-pc-sdk .ot-ven-legclaim-link:hover{text-decoration:underline}#onetrust-pc-sdk .ot-ven-hdr{width:calc(100% - 160px);height:auto;float:left;word-break:break-word;word-wrap:break-word;vertical-align:middle;padding-bottom:3px}#onetrust-pc-sdk .ot-ven-link,#onetrust-pc-sdk .ot-ven-legclaim-link{letter-spacing:.03em;font-size:.75em;font-weight:400}#onetrust-pc-sdk .ot-ven-dets{border-radius:2px;background-color:#f8f8f8}#onetrust-pc-sdk .ot-ven-dets li:first-child p:first-child{border-top:none}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:not(:first-child){border-top:1px solid #ddd !important}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:nth-child(n+3) p{display:inline-block}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:nth-child(n+3) p:nth-of-type(odd){width:30%}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:nth-child(n+3) p:nth-of-type(even){width:50%;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc p,#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc h4{padding-top:5px;padding-bottom:5px;display:block}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc h4{display:inline-block}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc p:nth-last-child(-n+1){padding-bottom:10px}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc p:nth-child(-n+2):not(.disc-pur){padding-top:10px}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc .disc-pur-cont{display:inline}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc .disc-pur{position:relative;width:50% !important;word-break:break-word;word-wrap:break-word;left:calc(30% + 17px)}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc .disc-pur:nth-child(-n+1){position:static}#onetrust-pc-sdk .ot-ven-dets p,#onetrust-pc-sdk .ot-ven-dets h4,#onetrust-pc-sdk .ot-ven-dets span{font-size:.69em;text-align:left;vertical-align:middle;word-break:break-word;word-wrap:break-word;margin:0;padding-bottom:10px;padding-left:15px;color:#2e3644}#onetrust-pc-sdk .ot-ven-dets h4{padding-top:5px}#onetrust-pc-sdk .ot-ven-dets span{color:dimgray;padding:0;vertical-align:baseline}#onetrust-pc-sdk .ot-ven-dets .ot-ven-pur h4{border-top:1px solid #e9e9e9;border-bottom:1px solid #e9e9e9;padding-bottom:5px;margin-bottom:5px;font-weight:bold}#onetrust-pc-sdk #ot-host-lst .ot-sel-all{float:right;position:relative;margin-right:42px;top:10px}#onetrust-pc-sdk #ot-host-lst .ot-sel-all input[type=checkbox]{width:auto;height:auto}#onetrust-pc-sdk #ot-host-lst .ot-sel-all label{height:20px;width:20px;padding-left:0px}#onetrust-pc-sdk #ot-host-lst .ot-acc-txt{overflow:hidden;width:95%}#onetrust-pc-sdk .ot-host-hdr{position:relative;z-index:1;pointer-events:none;width:calc(100% - 125px);float:left}#onetrust-pc-sdk .ot-host-name,#onetrust-pc-sdk .ot-host-desc{display:inline-block;width:90%}#onetrust-pc-sdk .ot-host-name{pointer-events:none}#onetrust-pc-sdk .ot-host-hdr>a{text-decoration:underline;font-size:.82em;position:relative;z-index:2;float:left;margin-bottom:5px;pointer-events:initial}#onetrust-pc-sdk .ot-host-name+a{margin-top:5px}#onetrust-pc-sdk .ot-host-name,#onetrust-pc-sdk .ot-host-name a,#onetrust-pc-sdk .ot-host-desc,#onetrust-pc-sdk .ot-host-info{color:dimgray;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-host-name,#onetrust-pc-sdk .ot-host-name a{font-weight:bold;font-size:.82em;line-height:1.3}#onetrust-pc-sdk .ot-host-name a{font-size:1em}#onetrust-pc-sdk .ot-host-expand{margin-top:3px;margin-bottom:3px;clear:both;display:block;color:#3860be;font-size:.72em;font-weight:normal}#onetrust-pc-sdk .ot-host-expand *{font-size:inherit}#onetrust-pc-sdk .ot-host-desc,#onetrust-pc-sdk .ot-host-info{font-size:.688em;line-height:1.4;font-weight:normal}#onetrust-pc-sdk .ot-host-desc{margin-top:10px}#onetrust-pc-sdk .ot-host-opt{margin:0;font-size:inherit;display:inline-block;width:100%}#onetrust-pc-sdk .ot-host-opt li>div div{font-size:.8em;padding:5px 0}#onetrust-pc-sdk .ot-host-opt li>div div:nth-child(1){width:30%;float:left}#onetrust-pc-sdk .ot-host-opt li>div div:nth-child(2){width:70%;float:left;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-host-info{border:none;display:inline-block;width:calc(100% - 10px);padding:10px;margin-bottom:10px;background-color:#f8f8f8}#onetrust-pc-sdk .ot-host-info>div{overflow:auto}#onetrust-pc-sdk #no-results{text-align:center;margin-top:30px}#onetrust-pc-sdk #no-results p{font-size:1em;color:#2e3644;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk #no-results p span{font-weight:bold}#onetrust-pc-sdk #ot-fltr-modal{width:100%;height:auto;display:none;-moz-transition:.2s ease;-o-transition:.2s ease;-webkit-transition:2s ease;transition:.2s ease;overflow:hidden;opacity:1;right:0}#onetrust-pc-sdk #ot-fltr-modal .ot-label-txt{display:inline-block;font-size:.85em;color:dimgray}#onetrust-pc-sdk #ot-fltr-cnt{z-index:2147483646;background-color:#fff;position:absolute;height:90%;max-height:300px;width:325px;left:210px;margin-top:10px;margin-bottom:20px;padding-right:10px;border-radius:3px;-webkit-box-shadow:0px 0px 12px 2px #c7c5c7;-moz-box-shadow:0px 0px 12px 2px #c7c5c7;box-shadow:0px 0px 12px 2px #c7c5c7}#onetrust-pc-sdk .ot-fltr-scrlcnt{overflow-y:auto;overflow-x:hidden;clear:both;max-height:calc(100% - 60px)}#onetrust-pc-sdk #ot-anchor{border:12px solid rgba(0,0,0,0);display:none;position:absolute;z-index:2147483647;right:55px;top:75px;transform:rotate(45deg);-o-transform:rotate(45deg);-ms-transform:rotate(45deg);-webkit-transform:rotate(45deg);background-color:#fff;-webkit-box-shadow:-3px -3px 5px -2px #c7c5c7;-moz-box-shadow:-3px -3px 5px -2px #c7c5c7;box-shadow:-3px -3px 5px -2px #c7c5c7}#onetrust-pc-sdk .ot-fltr-btns{margin-left:15px}#onetrust-pc-sdk #filter-apply-handler{margin-right:15px}#onetrust-pc-sdk .ot-fltr-opt{margin-bottom:25px;margin-left:15px;width:75%;position:relative}#onetrust-pc-sdk .ot-fltr-opt p{display:inline-block;margin:0;font-size:.9em;color:#2e3644}#onetrust-pc-sdk .ot-chkbox label span{font-size:.85em;color:dimgray}#onetrust-pc-sdk .ot-chkbox input[type=checkbox]+label::after{content:none;color:#fff}#onetrust-pc-sdk .ot-chkbox input[type=checkbox]:checked+label::after{content:""}#onetrust-pc-sdk .ot-chkbox input[type=checkbox]:focus+label::before{outline-style:solid;outline-width:2px;outline-style:auto}#onetrust-pc-sdk #ot-selall-vencntr,#onetrust-pc-sdk #ot-selall-adtlvencntr,#onetrust-pc-sdk #ot-selall-hostcntr,#onetrust-pc-sdk #ot-selall-licntr,#onetrust-pc-sdk #ot-selall-gnvencntr{right:15px;position:relative;width:20px;height:20px;float:right}#onetrust-pc-sdk #ot-selall-vencntr label,#onetrust-pc-sdk #ot-selall-adtlvencntr label,#onetrust-pc-sdk #ot-selall-hostcntr label,#onetrust-pc-sdk #ot-selall-licntr label,#onetrust-pc-sdk #ot-selall-gnvencntr label{float:left;padding-left:0}#onetrust-pc-sdk #ot-ven-lst:first-child{border-top:1px solid #e2e2e2}#onetrust-pc-sdk ul{list-style:none;padding:0}#onetrust-pc-sdk ul li{position:relative;margin:0;padding:15px 15px 15px 10px;border-bottom:1px solid #e2e2e2}#onetrust-pc-sdk ul li h3{font-size:.75em;color:#656565;margin:0;display:inline-block;width:70%;height:auto;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk ul li p{margin:0;font-size:.7em}#onetrust-pc-sdk ul li input[type=checkbox]{position:absolute;cursor:pointer;width:100%;height:100%;opacity:0;margin:0;top:0;left:0}#onetrust-pc-sdk .ot-cat-item>button:focus,#onetrust-pc-sdk .ot-acc-cntr>button:focus,#onetrust-pc-sdk li>button:focus{outline:#000 solid 2px}#onetrust-pc-sdk .ot-cat-item>button,#onetrust-pc-sdk .ot-acc-cntr>button,#onetrust-pc-sdk li>button{position:absolute;cursor:pointer;width:100%;height:100%;margin:0;top:0;left:0;z-index:1;max-width:none;border:none}#onetrust-pc-sdk .ot-cat-item>button[aria-expanded=false]~.ot-acc-txt,#onetrust-pc-sdk .ot-acc-cntr>button[aria-expanded=false]~.ot-acc-txt,#onetrust-pc-sdk li>button[aria-expanded=false]~.ot-acc-txt{margin-top:0;max-height:0;opacity:0;overflow:hidden;width:100%;transition:.25s ease-out;display:none}#onetrust-pc-sdk .ot-cat-item>button[aria-expanded=true]~.ot-acc-txt,#onetrust-pc-sdk .ot-acc-cntr>button[aria-expanded=true]~.ot-acc-txt,#onetrust-pc-sdk li>button[aria-expanded=true]~.ot-acc-txt{transition:.1s ease-in;margin-top:10px;width:100%;overflow:auto;display:block}#onetrust-pc-sdk .ot-cat-item>button[aria-expanded=true]~.ot-acc-grpcntr,#onetrust-pc-sdk .ot-acc-cntr>button[aria-expanded=true]~.ot-acc-grpcntr,#onetrust-pc-sdk li>button[aria-expanded=true]~.ot-acc-grpcntr{width:auto;margin-top:0px;padding-bottom:10px}#onetrust-pc-sdk .ot-host-item>button:focus,#onetrust-pc-sdk .ot-ven-item>button:focus{outline:0;border:2px solid #000}#onetrust-pc-sdk .ot-hide-acc>button{pointer-events:none}#onetrust-pc-sdk .ot-hide-acc .ot-plus-minus>*,#onetrust-pc-sdk .ot-hide-acc .ot-arw-cntr>*{visibility:hidden}#onetrust-pc-sdk .ot-hide-acc .ot-acc-hdr{min-height:30px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt){padding-right:10px;width:calc(100% - 37px);margin-top:10px;max-height:calc(100% - 90px)}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk{background-color:#f9f9fc;border:1px solid #e2e2e2;width:calc(100% - 2px);padding-bottom:5px;padding-top:5px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt{border:unset;background-color:unset}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt .ot-sel-all-hdr{display:none}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt .ot-sel-all{padding-right:.5rem}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt .ot-sel-all .ot-chkbox{right:0}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) .ot-sel-all{padding-right:34px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) .ot-sel-all-chkbox{width:auto}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) ul li{border:1px solid #e2e2e2;margin-bottom:10px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) .ot-acc-cntr>.ot-acc-hdr{padding:10px 0 10px 15px}#onetrust-pc-sdk.ot-addtl-vendors .ot-sel-all-chkbox{float:right}#onetrust-pc-sdk.ot-addtl-vendors .ot-plus-minus~.ot-sel-all-chkbox{right:34px}#onetrust-pc-sdk.ot-addtl-vendors #ot-ven-lst:first-child{border-top:none}#onetrust-pc-sdk .ot-acc-cntr{position:relative;border-left:1px solid #e2e2e2;border-right:1px solid #e2e2e2;border-bottom:1px solid #e2e2e2}#onetrust-pc-sdk .ot-acc-cntr input{z-index:1}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr{background-color:#f9f9fc;padding:5px 0 5px 15px;width:auto}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr .ot-plus-minus{vertical-align:middle;top:auto}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr .ot-arw-cntr{right:10px}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr input{z-index:2}#onetrust-pc-sdk .ot-acc-cntr.ot-add-tech .ot-acc-hdr{padding:10px 0 10px 15px}#onetrust-pc-sdk .ot-acc-cntr>input[type=checkbox]:checked~.ot-acc-hdr{border-bottom:1px solid #e2e2e2}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-txt{padding-left:10px;padding-right:10px}#onetrust-pc-sdk .ot-acc-cntr button[aria-expanded=true]~.ot-acc-txt{width:auto}#onetrust-pc-sdk .ot-acc-cntr .ot-addtl-venbox{display:none}#onetrust-pc-sdk .ot-vlst-cntr{margin-bottom:0;width:100%}#onetrust-pc-sdk .ot-vensec-title{font-size:.813em;vertical-align:middle;display:inline-block}#onetrust-pc-sdk .category-vendors-list-handler,#onetrust-pc-sdk .category-vendors-list-handler+a{margin-left:0;margin-top:10px}#onetrust-pc-sdk #ot-selall-vencntr.line-through label::after,#onetrust-pc-sdk #ot-selall-adtlvencntr.line-through label::after,#onetrust-pc-sdk #ot-selall-licntr.line-through label::after,#onetrust-pc-sdk #ot-selall-hostcntr.line-through label::after,#onetrust-pc-sdk #ot-selall-gnvencntr.line-through label::after{height:auto;border-left:0;transform:none;-o-transform:none;-ms-transform:none;-webkit-transform:none;left:5px;top:9px}#onetrust-pc-sdk #ot-category-title{float:left;padding-bottom:10px;font-size:1em;width:100%}#onetrust-pc-sdk .ot-cat-grp{margin-top:10px}#onetrust-pc-sdk .ot-cat-item{line-height:1.1;margin-top:10px;display:inline-block;width:100%}#onetrust-pc-sdk .ot-btn-container{text-align:right}#onetrust-pc-sdk .ot-btn-container button{display:inline-block;font-size:.75em;letter-spacing:.08em;margin-top:19px}#onetrust-pc-sdk #close-pc-btn-handler.ot-close-icon{position:absolute;top:10px;right:0;z-index:1;padding:0;background-color:rgba(0,0,0,0);border:none}#onetrust-pc-sdk #close-pc-btn-handler.ot-close-icon svg{display:block;height:10px;width:10px}#onetrust-pc-sdk #clear-filters-handler{margin-top:20px;margin-bottom:10px;float:right;max-width:200px;text-decoration:none;color:#3860be;font-size:.9em;font-weight:bold;background-color:rgba(0,0,0,0);border-color:rgba(0,0,0,0);padding:1px}#onetrust-pc-sdk #clear-filters-handler:hover{color:#2285f7}#onetrust-pc-sdk #clear-filters-handler:focus{outline:#000 solid 1px}#onetrust-pc-sdk .ot-enbl-chr h4~.ot-tgl,#onetrust-pc-sdk .ot-enbl-chr h4~.ot-always-active{right:45px}#onetrust-pc-sdk .ot-enbl-chr h4~.ot-tgl+.ot-tgl{right:120px}#onetrust-pc-sdk .ot-enbl-chr .ot-pli-hdr.ot-leg-border-color span:first-child{width:90px}#onetrust-pc-sdk .ot-enbl-chr li.ot-subgrp>h5+.ot-tgl-cntr{padding-right:25px}#onetrust-pc-sdk .ot-plus-minus{width:20px;height:20px;font-size:1.5em;position:relative;display:inline-block;margin-right:5px;top:3px}#onetrust-pc-sdk .ot-plus-minus span{position:absolute;background:#27455c;border-radius:1px}#onetrust-pc-sdk .ot-plus-minus span:first-of-type{top:25%;bottom:25%;width:10%;left:45%}#onetrust-pc-sdk .ot-plus-minus span:last-of-type{left:25%;right:25%;height:10%;top:45%}#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-arw,#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-plus-minus span:first-of-type,#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-plus-minus span:last-of-type{transform:rotate(90deg)}#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-plus-minus span:last-of-type{left:50%;right:50%}#onetrust-pc-sdk #ot-selall-vencntr label,#onetrust-pc-sdk #ot-selall-adtlvencntr label,#onetrust-pc-sdk #ot-selall-hostcntr label,#onetrust-pc-sdk #ot-selall-licntr label{position:relative;display:inline-block;width:20px;height:20px}#onetrust-pc-sdk .ot-host-item .ot-plus-minus,#onetrust-pc-sdk .ot-ven-item .ot-plus-minus{float:left;margin-right:8px;top:10px}#onetrust-pc-sdk .ot-ven-item ul{list-style:none inside;font-size:100%;margin:0}#onetrust-pc-sdk .ot-ven-item ul li{margin:0 !important;padding:0;border:none !important}#onetrust-pc-sdk .ot-pli-hdr{color:#77808e;overflow:hidden;padding-top:7.5px;padding-bottom:7.5px;width:calc(100% - 2px);border-top-left-radius:3px;border-top-right-radius:3px}#onetrust-pc-sdk .ot-pli-hdr span:first-child{top:50%;transform:translateY(50%);max-width:90px}#onetrust-pc-sdk .ot-pli-hdr span:last-child{padding-right:10px;max-width:95px;text-align:center}#onetrust-pc-sdk .ot-li-title{float:right;font-size:.813em}#onetrust-pc-sdk .ot-pli-hdr.ot-leg-border-color{background-color:#f4f4f4;border:1px solid #d8d8d8}#onetrust-pc-sdk .ot-pli-hdr.ot-leg-border-color span:first-child{text-align:left;width:70px}#onetrust-pc-sdk li.ot-subgrp>h5,#onetrust-pc-sdk .ot-cat-header{width:calc(100% - 130px)}#onetrust-pc-sdk li.ot-subgrp>h5+.ot-tgl-cntr{padding-left:13px}#onetrust-pc-sdk .ot-acc-grpcntr .ot-acc-grpdesc{margin-bottom:5px}#onetrust-pc-sdk .ot-acc-grpcntr .ot-subgrp-cntr{border-top:1px solid #d8d8d8}#onetrust-pc-sdk .ot-acc-grpcntr .ot-vlst-cntr+.ot-subgrp-cntr{border-top:none}#onetrust-pc-sdk .ot-acc-hdr .ot-arw-cntr+.ot-tgl-cntr,#onetrust-pc-sdk .ot-acc-txt h4+.ot-tgl-cntr{padding-left:13px}#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item .ot-subgrp>h5,#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item .ot-cat-header{width:calc(100% - 145px)}#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item h5+.ot-tgl-cntr,#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item .ot-cat-header+.ot-tgl{padding-left:28px}#onetrust-pc-sdk .ot-sel-all-hdr,#onetrust-pc-sdk .ot-sel-all-chkbox{display:inline-block;width:100%;position:relative}#onetrust-pc-sdk .ot-sel-all-chkbox{z-index:1}#onetrust-pc-sdk .ot-sel-all{margin:0;position:relative;padding-right:23px;float:right}#onetrust-pc-sdk .ot-consent-hdr,#onetrust-pc-sdk .ot-li-hdr{float:right;font-size:.812em;line-height:normal;text-align:center;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-li-hdr{max-width:100px;padding-right:10px}#onetrust-pc-sdk .ot-consent-hdr{max-width:55px}#onetrust-pc-sdk #ot-selall-licntr{display:block;width:21px;height:auto;float:right;position:relative;right:80px}#onetrust-pc-sdk #ot-selall-licntr label{position:absolute}#onetrust-pc-sdk .ot-ven-ctgl{margin-left:66px}#onetrust-pc-sdk .ot-ven-litgl+.ot-arw-cntr{margin-left:81px}#onetrust-pc-sdk .ot-enbl-chr .ot-host-cnt .ot-tgl-cntr{width:auto}#onetrust-pc-sdk #ot-lst-cnt:not(.ot-host-cnt) .ot-tgl-cntr{width:auto;top:auto;height:20px}#onetrust-pc-sdk #ot-lst-cnt .ot-chkbox{position:relative;display:inline-block;width:20px;height:20px}#onetrust-pc-sdk #ot-lst-cnt .ot-chkbox label{position:absolute;padding:0;width:20px;height:20px}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info-cntr{border:1px solid #d8d8d8;padding:.75rem 2rem;padding-bottom:0;width:auto;margin-top:.5rem}#onetrust-pc-sdk .ot-acc-grpdesc+.ot-leg-btn-container{padding-left:20px;padding-right:20px;width:calc(100% - 40px);margin-bottom:5px}#onetrust-pc-sdk .ot-subgrp .ot-leg-btn-container{margin-bottom:5px}#onetrust-pc-sdk #ot-ven-lst .ot-leg-btn-container{margin-top:10px}#onetrust-pc-sdk .ot-leg-btn-container{display:inline-block;width:100%;margin-bottom:10px}#onetrust-pc-sdk .ot-leg-btn-container button{height:auto;padding:6.5px 8px;margin-bottom:0;letter-spacing:0;font-size:.75em;line-height:normal}#onetrust-pc-sdk .ot-leg-btn-container svg{display:none;height:14px;width:14px;padding-right:5px;vertical-align:sub}#onetrust-pc-sdk .ot-active-leg-btn{cursor:default;pointer-events:none}#onetrust-pc-sdk .ot-active-leg-btn svg{display:inline-block}#onetrust-pc-sdk .ot-remove-objection-handler{text-decoration:underline;padding:0;font-size:.75em;font-weight:600;line-height:1;padding-left:10px}#onetrust-pc-sdk .ot-obj-leg-btn-handler span{font-weight:bold;text-align:center;font-size:inherit;line-height:1.5}#onetrust-pc-sdk.ot-close-btn-link #close-pc-btn-handler{border:none;height:auto;line-height:1.5;text-decoration:underline;font-size:.69em;background:none;right:15px;top:15px;width:auto;font-weight:normal}#onetrust-pc-sdk .ot-pgph-link{font-size:.813em !important;margin-top:5px;position:relative}#onetrust-pc-sdk .ot-pgph-link.ot-pgph-link-subgroup{margin-bottom:1rem}#onetrust-pc-sdk .ot-pgph-contr{margin:0 2.5rem}#onetrust-pc-sdk .ot-pgph-title{font-size:1.18rem;margin-bottom:2rem}#onetrust-pc-sdk .ot-pgph-desc{font-size:1rem;font-weight:400;margin-bottom:2rem;line-height:1.5rem}#onetrust-pc-sdk .ot-pgph-desc:not(:last-child):after{content:"";width:96%;display:block;margin:0 auto;padding-bottom:2rem;border-bottom:1px solid #e9e9e9}#onetrust-pc-sdk .ot-cat-header{float:left;font-weight:600;font-size:.875em;line-height:1.5;max-width:90%;vertical-align:middle}#onetrust-pc-sdk .ot-vnd-item>button:focus{outline:#000 solid 2px}#onetrust-pc-sdk .ot-vnd-item>button{position:absolute;cursor:pointer;width:100%;height:100%;margin:0;top:0;left:0;z-index:1;max-width:none;border:none}#onetrust-pc-sdk .ot-vnd-item>button[aria-expanded=false]~.ot-acc-txt{margin-top:0;max-height:0;opacity:0;overflow:hidden;width:100%;transition:.25s ease-out;display:none}#onetrust-pc-sdk .ot-vnd-item>button[aria-expanded=true]~.ot-acc-txt{transition:.1s ease-in;margin-top:10px;width:100%;overflow:auto;display:block}#onetrust-pc-sdk .ot-vnd-item>button[aria-expanded=true]~.ot-acc-grpcntr{width:auto;margin-top:0px;padding-bottom:10px}#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item{position:relative;border-radius:2px;margin:0;padding:0;border:1px solid #d8d8d8;border-top:none;width:calc(100% - 2px);float:left}#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item:first-of-type{margin-top:10px;border-top:1px solid #d8d8d8}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpdesc{padding-left:20px;padding-right:20px;width:calc(100% - 40px);font-size:.812em;margin-bottom:10px;margin-top:15px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpdesc>ul{padding-top:10px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpdesc>ul li{padding-top:0;line-height:1.5;padding-bottom:10px}#onetrust-pc-sdk .ot-accordion-layout div+.ot-acc-grpdesc{margin-top:5px}#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr:first-child{margin-top:10px}#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr:last-child,#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr:last-child{margin-bottom:5px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{padding-top:11.5px;padding-bottom:11.5px;padding-left:20px;padding-right:20px;width:calc(100% - 40px);display:inline-block}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-txt{width:100%;padding:0}#onetrust-pc-sdk .ot-accordion-layout .ot-subgrp-cntr{padding-left:20px;padding-right:15px;padding-bottom:0;width:calc(100% - 35px)}#onetrust-pc-sdk .ot-accordion-layout .ot-subgrp{padding-right:5px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpcntr{z-index:1;position:relative}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header+.ot-arw-cntr{position:absolute;top:50%;transform:translateY(-50%);right:20px;margin-top:-2px}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header+.ot-arw-cntr .ot-arw{width:15px;height:20px;margin-left:5px;color:dimgray}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{float:none;color:#2e3644;margin:0;display:inline-block;height:auto;word-wrap:break-word;min-height:inherit}#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr,#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr{padding-left:20px;width:calc(100% - 20px);display:inline-block;margin-top:0;padding-bottom:2px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{position:relative;min-height:25px}#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl,#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active{position:absolute;top:50%;transform:translateY(-50%);right:20px}#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl+.ot-tgl{right:95px}#onetrust-pc-sdk .ot-accordion-layout .category-vendors-list-handler,#onetrust-pc-sdk .ot-accordion-layout .category-vendors-list-handler+a{margin-top:5px}#onetrust-pc-sdk #ot-lst-cnt{margin-top:1rem;max-height:calc(100% - 96px)}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info-cntr{border:1px solid #d8d8d8;padding:.75rem 2rem;padding-bottom:0;width:auto;margin-top:.5rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info{margin-bottom:1rem;padding-left:.75rem;padding-right:.75rem;display:flex;flex-direction:column}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info[data-vnd-info-key*=DPOEmail]{border-top:1px solid #d8d8d8;padding-top:1rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info[data-vnd-info-key*=DPOLink]{border-bottom:1px solid #d8d8d8;padding-bottom:1rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info .ot-vnd-lbl{font-weight:bold;font-size:.85em;margin-bottom:.5rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info .ot-vnd-cnt{margin-left:.5rem;font-weight:500;font-size:.85rem}#onetrust-pc-sdk .ot-vs-list,#onetrust-pc-sdk .ot-vnd-serv{width:auto;padding:1rem 1.25rem;padding-bottom:0}#onetrust-pc-sdk .ot-vs-list .ot-vnd-serv-hdr-cntr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-serv-hdr-cntr{padding-bottom:.75rem;border-bottom:1px solid #d8d8d8}#onetrust-pc-sdk .ot-vs-list .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr{font-weight:600;font-size:.95em;line-height:2;margin-left:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item{border:none;margin:0;padding:0}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item button,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item button{outline:none;border-bottom:1px solid #d8d8d8}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item button[aria-expanded=true],#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item button[aria-expanded=true]{border-bottom:none}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item:first-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item:first-child{margin-top:.25rem;border-top:unset}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item:last-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item:last-child{margin-bottom:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item:last-child button,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item:last-child button{border-bottom:none}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info-cntr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info-cntr{border:1px solid #d8d8d8;padding:.75rem 1.75rem;padding-bottom:0;width:auto;margin-top:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info{margin-bottom:1rem;padding-left:.75rem;padding-right:.75rem;display:flex;flex-direction:column}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOEmail],#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOEmail]{border-top:1px solid #d8d8d8;padding-top:1rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOLink],#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOLink]{border-bottom:1px solid #d8d8d8;padding-bottom:1rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info .ot-vnd-lbl,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info .ot-vnd-lbl{font-weight:bold;font-size:.85em;margin-bottom:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info .ot-vnd-cnt,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info .ot-vnd-cnt{margin-left:.5rem;font-weight:500;font-size:.85rem}#onetrust-pc-sdk .ot-vs-list.ot-vnd-subgrp-cnt,#onetrust-pc-sdk .ot-vnd-serv.ot-vnd-subgrp-cnt{padding-left:40px}#onetrust-pc-sdk .ot-vs-list.ot-vnd-subgrp-cnt .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr,#onetrust-pc-sdk .ot-vnd-serv.ot-vnd-subgrp-cnt .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr{font-size:.8em}#onetrust-pc-sdk .ot-vs-list.ot-vnd-subgrp-cnt .ot-cat-header,#onetrust-pc-sdk .ot-vnd-serv.ot-vnd-subgrp-cnt .ot-cat-header{font-size:.8em}#onetrust-pc-sdk .ot-subgrp-cntr .ot-vnd-serv{margin-bottom:1rem;padding:1rem .95rem}#onetrust-pc-sdk .ot-subgrp-cntr .ot-vnd-serv .ot-vnd-serv-hdr-cntr{padding-bottom:.75rem;border-bottom:1px solid #d8d8d8}#onetrust-pc-sdk .ot-subgrp-cntr .ot-vnd-serv .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr{font-weight:700;font-size:.8em;line-height:20px;margin-left:.82rem}#onetrust-pc-sdk .ot-subgrp-cntr .ot-cat-header{font-weight:700;font-size:.8em;line-height:20px}#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-vnd-serv .ot-vnd-lst-cont .ot-accordion-layout .ot-acc-hdr div.ot-chkbox{margin-left:.82rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr{padding:.7rem 0;margin:0;display:flex;width:100%;align-items:center;justify-content:space-between}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr div:first-child,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr div:first-child,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr div:first-child{margin-left:.5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr div:last-child,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr div:last-child,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr div:last-child{margin-right:.5rem;margin-left:.5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-always-active{position:relative;right:unset;top:unset;transform:unset}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-plus-minus{top:0}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-arw-cntr{float:none;top:unset;right:unset;transform:unset;margin-top:-2px;position:relative}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-cat-header{flex:1;margin:0 .5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-tgl{position:relative;transform:none;right:0;top:0;float:none}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox{position:relative;margin:0 .5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox label{padding:0}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox label::before{position:relative}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox input{position:absolute;cursor:pointer;width:100%;height:100%;opacity:0;margin:0;top:0;left:0;z-index:1}#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp .ot-acc-hdr h5.ot-cat-header,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp .ot-acc-hdr h4.ot-cat-header{margin:0}#onetrust-pc-sdk .ot-vs-config .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp h5{top:0;line-height:20px}#onetrust-pc-sdk .ot-vs-list{display:flex;flex-direction:column;padding:0;margin:.5rem 4px}#onetrust-pc-sdk .ot-vs-selc-all{display:flex;padding:0;float:unset;align-items:center;justify-content:flex-start}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf{justify-content:flex-end}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf.ot-caret-conf .ot-sel-all-chkbox{margin-right:48px}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf .ot-sel-all-chkbox{margin:0;padding:0;margin-right:14px;justify-content:flex-end}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf #ot-selall-vencntr.ot-chkbox,#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf #ot-selall-vencntr.ot-tgl{display:inline-block;right:unset;width:auto;height:auto;float:none}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf #ot-selall-vencntr label{width:45px;height:25px}#onetrust-pc-sdk .ot-vs-selc-all .ot-sel-all-chkbox{margin-right:11px;margin-left:.75rem;display:flex;align-items:center}#onetrust-pc-sdk .ot-vs-selc-all .sel-all-hdr{margin:0 1.25rem;font-size:.812em;line-height:normal;text-align:center;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-vnd-list-cnt #ot-selall-vencntr.ot-chkbox{float:unset;right:0}#onetrust-pc-sdk[dir=rtl] #ot-back-arw,#onetrust-pc-sdk[dir=rtl] input~.ot-acc-hdr .ot-arw{transform:rotate(180deg);-o-transform:rotate(180deg);-ms-transform:rotate(180deg);-webkit-transform:rotate(180deg)}#onetrust-pc-sdk[dir=rtl] input:checked~.ot-acc-hdr .ot-arw{transform:rotate(270deg);-o-transform:rotate(270deg);-ms-transform:rotate(270deg);-webkit-transform:rotate(270deg)}#onetrust-pc-sdk[dir=rtl] .ot-chkbox label::after{transform:rotate(45deg);-webkit-transform:rotate(45deg);-o-transform:rotate(45deg);-ms-transform:rotate(45deg);border-left:0;border-right:3px solid}#onetrust-pc-sdk[dir=rtl] .ot-search-cntr>svg{right:0}@media only screen and (max-width: 600px){#onetrust-pc-sdk.otPcCenter{left:0;min-width:100%;height:100%;top:0;border-radius:0}#onetrust-pc-sdk #ot-pc-content,#onetrust-pc-sdk.ot-ftr-stacked .ot-btn-container{margin:1px 3px 0 10px;padding-right:10px;width:calc(100% - 23px)}#onetrust-pc-sdk .ot-btn-container button{max-width:none;letter-spacing:.01em}#onetrust-pc-sdk #close-pc-btn-handler{top:10px;right:17px}#onetrust-pc-sdk p{font-size:.7em}#onetrust-pc-sdk #ot-pc-hdr{margin:10px 10px 0 5px;width:calc(100% - 15px)}#onetrust-pc-sdk .vendor-search-handler{font-size:1em}#onetrust-pc-sdk #ot-back-arw{margin-left:12px}#onetrust-pc-sdk #ot-lst-cnt{margin:0;padding:0 5px 0 10px;min-width:95%}#onetrust-pc-sdk .switch+p{max-width:80%}#onetrust-pc-sdk .ot-ftr-stacked button{width:100%}#onetrust-pc-sdk #ot-fltr-cnt{max-width:320px;width:90%;border-top-right-radius:0;border-bottom-right-radius:0;margin:0;margin-left:15px;left:auto;right:40px;top:85px}#onetrust-pc-sdk .ot-fltr-opt{margin-left:25px;margin-bottom:10px}#onetrust-pc-sdk .ot-pc-refuse-all-handler{margin-bottom:0}#onetrust-pc-sdk #ot-fltr-cnt{right:40px}}@media only screen and (max-width: 476px){#onetrust-pc-sdk .ot-fltr-cntr,#onetrust-pc-sdk #ot-fltr-cnt{right:10px}#onetrust-pc-sdk #ot-anchor{right:25px}#onetrust-pc-sdk button{width:100%}#onetrust-pc-sdk:not(.ot-addtl-vendors) #ot-pc-lst:not(.ot-enbl-chr) .ot-sel-all{padding-right:9px}#onetrust-pc-sdk:not(.ot-addtl-vendors) #ot-pc-lst:not(.ot-enbl-chr) .ot-tgl-cntr{right:0}}@media only screen and (max-width: 896px)and (max-height: 425px)and (orientation: landscape){#onetrust-pc-sdk.otPcCenter{left:0;top:0;min-width:100%;height:100%;border-radius:0}#onetrust-pc-sdk .ot-pc-header{height:auto;min-height:20px}#onetrust-pc-sdk .ot-pc-header .ot-pc-logo{max-height:30px}#onetrust-pc-sdk .ot-pc-footer{max-height:60px;overflow-y:auto}#onetrust-pc-sdk #ot-pc-content,#onetrust-pc-sdk #ot-pc-lst{bottom:70px}#onetrust-pc-sdk.ot-ftr-stacked #ot-pc-content{bottom:70px}#onetrust-pc-sdk #ot-anchor{left:initial;right:50px}#onetrust-pc-sdk #ot-lst-title{margin-top:12px}#onetrust-pc-sdk #ot-lst-title *{font-size:inherit}#onetrust-pc-sdk #ot-pc-hdr input{margin-right:0;padding-right:45px}#onetrust-pc-sdk .switch+p{max-width:85%}#onetrust-pc-sdk #ot-sel-blk{position:static}#onetrust-pc-sdk #ot-pc-lst{overflow:auto}#onetrust-pc-sdk #ot-lst-cnt{max-height:none;overflow:initial}#onetrust-pc-sdk #ot-lst-cnt.no-results{height:auto}#onetrust-pc-sdk input{font-size:1em !important}#onetrust-pc-sdk p{font-size:.6em}#onetrust-pc-sdk #ot-fltr-modal{width:100%;top:0}#onetrust-pc-sdk ul li p,#onetrust-pc-sdk .category-vendors-list-handler,#onetrust-pc-sdk .category-vendors-list-handler+a,#onetrust-pc-sdk .category-host-list-handler{font-size:.6em}#onetrust-pc-sdk.ot-shw-fltr #ot-anchor{display:none !important}#onetrust-pc-sdk.ot-shw-fltr #ot-pc-lst{height:100% !important;overflow:hidden;top:0px}#onetrust-pc-sdk.ot-shw-fltr #ot-fltr-cnt{margin:0;height:100%;max-height:none;padding:10px;top:0;width:calc(100% - 20px);position:absolute;right:0;left:0;max-width:none}#onetrust-pc-sdk.ot-shw-fltr .ot-fltr-scrlcnt{max-height:calc(100% - 65px)}}
            #onetrust-consent-sdk #onetrust-pc-sdk,
                #onetrust-consent-sdk #ot-search-cntr,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-switch.ot-toggle,
                #onetrust-consent-sdk #onetrust-pc-sdk ot-grp-hdr1 .checkbox,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-title:after
                ,#onetrust-consent-sdk #onetrust-pc-sdk #ot-sel-blk,
                        #onetrust-consent-sdk #onetrust-pc-sdk #ot-fltr-cnt,
                        #onetrust-consent-sdk #onetrust-pc-sdk #ot-anchor {
                    background-color: #FFF;
                }
               
            #onetrust-consent-sdk #onetrust-pc-sdk h3,
                #onetrust-consent-sdk #onetrust-pc-sdk h4,
                #onetrust-consent-sdk #onetrust-pc-sdk h5,
                #onetrust-consent-sdk #onetrust-pc-sdk h6,
                #onetrust-consent-sdk #onetrust-pc-sdk p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-ven-lst .ot-ven-opts p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-desc,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-title,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-li-title,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-sel-all-hdr span,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-info,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-fltr-modal #modal-header,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-checkbox label span,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst #ot-sel-blk p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst #ot-lst-title h3,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst .back-btn-handler p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst .ot-ven-name,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst #ot-ven-lst .consent-category,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-inactive-leg-btn,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-label-status,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-chkbox label span,
                #onetrust-consent-sdk #onetrust-pc-sdk #clear-filters-handler,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-optout-signal
                {
                    color: #2E2E2E;
                }
             #onetrust-consent-sdk #onetrust-pc-sdk .privacy-notice-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-pgph-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk .category-vendors-list-handler,
                    #onetrust-consent-sdk #onetrust-pc-sdk .category-vendors-list-handler + a,
                    #onetrust-consent-sdk #onetrust-pc-sdk .category-host-list-handler,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-ven-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-ven-legclaim-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-name a,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-acc-hdr .ot-host-expand,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-info a,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-content #ot-pc-desc .ot-link-btn,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info a,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info a
                    {
                        color: #007398;
                    }
            #onetrust-consent-sdk #onetrust-pc-sdk .category-vendors-list-handler:hover { text-decoration: underline;}
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-grpcntr.ot-acc-txt,
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-txt .ot-subgrp-tgl .ot-switch.ot-toggle
             {
                background-color: #F8F8F8;
            }
             #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-info,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-txt .ot-ven-dets
                            {
                                background-color: #F8F8F8;
                            }
        #onetrust-consent-sdk #onetrust-pc-sdk
            button:not(#clear-filters-handler):not(.ot-close-icon):not(#filter-btn-handler):not(.ot-remove-objection-handler):not(.ot-obj-leg-btn-handler):not([aria-expanded]):not(.ot-link-btn),
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-active-leg-btn {
                background-color: #007398;border-color: #007398;
                color: #FFF;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-active-menu {
                border-color: #007398;
            }
            
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-remove-objection-handler{
                background-color: transparent;
                border: 1px solid transparent;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-inactive-leg-btn {
                background-color: #FFFFFF;
                color: #78808E; border-color: #78808E;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-tgl input:focus + .ot-switch, .ot-switch .ot-switch-nob, .ot-switch .ot-switch-nob:before,
            #onetrust-pc-sdk .ot-checkbox input[type="checkbox"]:focus + label::before,
            #onetrust-pc-sdk .ot-chkbox input[type="checkbox"]:focus + label::before {
                outline-color: #000000;
                outline-width: 1px;
            }
            #onetrust-pc-sdk .ot-host-item > button:focus, #onetrust-pc-sdk .ot-ven-item > button:focus {
                border: 1px solid #000000;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk *:focus,
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-vlst-cntr > a:focus {
               outline: 1px solid #000000;
            }#onetrust-pc-sdk .ot-vlst-cntr .ot-ext-lnk,  #onetrust-pc-sdk .ot-ven-hdr .ot-ext-lnk{
                    background-image: url('https://cdn.cookielaw.org/logos/static/ot_external_link.svg');
                }
            /*! Extra code to blur out background */
.onetrust-pc-dark-filter{
background:rgba(0,0,0,.5);
z-index:2147483646;
width:100%;
height:100%;
overflow:hidden;
position:fixed;
top:0;
bottom:0;
left:0;
backdrop-filter: initial
}

/*! v6.12.0 2021-01-19 */
div#onetrust-consent-sdk #onetrust-banner-sdk{border-top:2px solid #eb6500!important;outline:1px solid transparent;box-shadow:none;padding:24px}div#onetrust-consent-sdk button{border-radius:0!important;box-shadow:none!important;box-sizing:border-box!important;font-size:20px!important;font-weight:400!important;letter-spacing:0!important;max-width:none!important;white-space:nowrap!important}div#onetrust-consent-sdk button:not(.ot-link-btn){background-color:#007398!important;border:2px solid #007398!important;color:#fff!important;height:48px!important;padding:0 1em!important;width:auto!important}div#onetrust-consent-sdk button:hover{background-color:#fff!important;border-color:#eb6500!important;color:#2e2e2e!important}div#onetrust-consent-sdk button.ot-link-btn{color:#007398!important;font-size:16px!important;text-decoration:underline}div#onetrust-consent-sdk button.ot-link-btn:hover{color: #2e2e2e!important;text-decoration-color:#eb6500!important}div#onetrust-consent-sdk a,div#onetrust-pc-sdk a{color:#007398!important;text-decoration:underline!important}div#onetrust-consent-sdk a,div#onetrust-consent-sdk button,div#onetrust-consent-sdk p:hover{opacity:1!important}div#onetrust-consent-sdk a:focus,div#onetrust-consent-sdk button:focus,div#onetrust-consent-sdk input:focus{outline:2px solid #eb6500!important;outline-offset:1px!important}div#onetrust-banner-sdk .ot-sdk-container{padding:0;width:auto}div#onetrust-banner-sdk .ot-sdk-row{align-items:flex-start;box-sizing:border-box;display:flex;flex-direction:column;justify-content:space-between;margin:auto;max-width:1152px}div#onetrust-banner-sdk .ot-sdk-row:after{display:none}div#onetrust-banner-sdk #onetrust-group-container,div#onetrust-banner-sdk.ot-bnr-flift:not(.ot-iab-2) #onetrust-group-container,div#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{flex-grow:1;width:auto}div#onetrust-banner-sdk #onetrust-policy,div#onetrust-banner-sdk.ot-bnr-flift #onetrust-policy{margin:0;overflow:visible}div#onetrust-banner-sdk.ot-bnr-flift #onetrust-policy-text,div#onetrust-consent-sdk #onetrust-policy-text{font-size:16px;line-height:24px;max-width:44em;margin:0}div#onetrust-consent-sdk #onetrust-policy-text a[href]{font-weight:400;margin-left:8px}div#onetrust-banner-sdk #onetrust-button-group-parent{flex:0 0 auto;margin:32px 0 0;width:100%}div#onetrust-banner-sdk #onetrust-button-group{display:flex;flex-direction:row;flex-wrap:wrap;justify-content:flex-end;margin:-8px}div#onetrust-banner-sdk .banner-actions-container{display:flex;flex:1 0 auto}div#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group button:last-of-type,div#onetrust-consent-sdk #onetrust-accept-btn-handler,div#onetrust-consent-sdk #onetrust-pc-btn-handler{flex:1 0 auto;margin:8px;width:auto}div#onetrust-consent-sdk #onetrust-pc-btn-handler{background-color:#fff!important;color:#2e2e2e!important}div#onetrust-banner-sdk #onetrust-close-btn-container{display:none}@media only screen and (min-width:556px){div#onetrust-consent-sdk #onetrust-banner-sdk{padding:40px}div#onetrust-banner-sdk #onetrust-policy{margin:0 40px 0 0}div#onetrust-banner-sdk .ot-sdk-row{align-items:center;flex-direction:row}div#onetrust-banner-sdk #onetrust-button-group-parent,div#onetrust-banner-sdk.ot-bnr-flift:not(.ot-iab-2) #onetrust-button-group-parent,div#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-button-group-parent{margin:0;padding:0;width:auto}div#onetrust-banner-sdk #onetrust-button-group,div#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group{align-items:stretch;flex-direction:column-reverse;margin:0}div#onetrust-consent-sdk #onetrust-accept-btn-handler,div#onetrust-consent-sdk #onetrust-pc-btn-handler{flex:1 0 auto}}@media only screen and (min-width:768px){div#onetrust-banner-sdk #onetrust-policy{margin:0 48px 0 0}div#onetrust-consent-sdk #onetrust-banner-sdk{padding:48px}}div#onetrust-consent-sdk #onetrust-pc-sdk h5{font-size:16px;line-height:24px}div#onetrust-consent-sdk #onetrust-pc-sdk p,div#onetrust-pc-sdk #ot-pc-desc,div#onetrust-pc-sdk .category-host-list-handler,div#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{font-size:16px;font-weight:400;line-height:24px}div#onetrust-consent-sdk a:hover,div#onetrust-pc-sdk a:hover{color:#2e2e2e!important;text-decoration-color:#eb6500!important}div#onetrust-pc-sdk{border-radius:0;bottom:0;height:auto;left:0;margin:auto;max-width:100%;overflow:hidden;right:0;top:0;width:512px;max-height:800px}div#onetrust-pc-sdk .ot-pc-header{display:none}div#onetrust-pc-sdk #ot-pc-content{overscroll-behavior:contain;padding:0 12px 0 24px;margin:16px 4px 0 0;top:0;right:16px;left:0;width:auto}div#onetrust-pc-sdk #ot-category-title,div#onetrust-pc-sdk #ot-pc-title{font-size:24px;font-weight:400;line-height:32px;margin:0 0 16px}div#onetrust-pc-sdk #ot-pc-desc{padding:0}div#onetrust-pc-sdk #ot-pc-desc a{display:inline}div#onetrust-pc-sdk #accept-recommended-btn-handler{display:none!important}div#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:2px solid #eb6500;outline-offset:-1px;transition:none}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item{border-width:0 0 2px}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item:first-of-type{border-width:2px 0}div#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{padding:8px 0;width:100%}div#onetrust-pc-sdk .ot-plus-minus{transform:translateY(2px)}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item>button{background:0 0!important;border:0!important;height:44px!important;max-width:none!important;width:calc(100% - 48px)!important}div#onetrust-consent-sdk #onetrust-pc-sdk h5{font-weight:700}div#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr{padding:0}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item .ot-acc-grpdesc{padding:0;width:100%}div#onetrust-pc-sdk .ot-acc-grpcntr .ot-subgrp-cntr{border:0;padding:0}div#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp{margin:0}div#onetrust-pc-sdk .ot-always-active-group .ot-cat-header{width:calc(100% - 160px)}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{width:calc(100% - 88px)}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active{color:#2e2e2e;font-size:12px;font-weight:400;line-height:1.5;padding-right:48px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active:before{border-radius:12px;position:absolute;right:0;top:0;content:'';background:#fff;border:2px solid #939393;box-sizing:border-box;height:20px;width:40px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active:after{border-radius:50%;position:absolute;right:5px;top:4px;content:'';background-color:#eb6500;height:12px;width:12px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active,div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl{right:2px}div#onetrust-pc-sdk .ot-switch{display:block;height:20px;width:40px}div#onetrust-pc-sdk .ot-tgl input+.ot-switch .ot-switch-nob,div#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob{background:#fff;border:2px solid #939393;box-sizing:border-box;height:20px;width:40px}div#onetrust-pc-sdk .ot-tgl input+.ot-switch .ot-switch-nob:before{background-color:#737373;height:8px;left:4px;top:4px;width:8px}div#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob:before{background-color:#eb6500;height:12px;left:0;top:2px;width:12px}div#onetrust-pc-sdk .ot-tgl input:focus+.ot-switch .ot-switch-nob{box-shadow:0 0;outline:2px solid #eb6500!important;outline-offset:1px;transition:none}div#onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-grpcntr.ot-acc-txt{background-color:transparent;padding-left:3px}div#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr,div#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr{overflow:visible;width:100%}div#onetrust-pc-sdk .ot-pc-footer{border-top:0 solid}div#onetrust-pc-sdk .ot-btn-container{padding-top:24px;text-align:center}div#onetrust-pc-sdk .ot-pc-footer button{margin:8px 0;background-color:#fff}div#onetrust-pc-sdk .ot-pc-footer-logo{background-color:#fff}div#onetrust-pc-sdk #ot-lst-title span{font-size:24px;font-weight:400;line-height:32px}div#onetrust-pc-sdk #ot-host-lst .ot-host-desc,div#onetrust-pc-sdk #ot-host-lst .ot-host-expand,div#onetrust-pc-sdk #ot-host-lst .ot-host-name,div#onetrust-pc-sdk #ot-host-lst .ot-host-name a,div#onetrust-pc-sdk .back-btn-handler,div#onetrust-pc-sdk .ot-host-opt li>div div{font-size:16px;font-weight:400;line-height:24px}div#onetrust-pc-sdk #ot-host-lst .ot-acc-txt{width:100%}div#onetrust-pc-sdk #ot-pc-lst{top:0}div#onetrust-pc-sdk .back-btn-handler{text-decoration:none!important}div#onetrust-pc-sdk #filter-btn-handler:hover svg{filter:invert(1)}div#onetrust-pc-sdk .back-btn-handler svg{width:16px;height:16px}div#onetrust-pc-sdk .ot-host-item>button{background:0 0!important;border:0!important;height:66px!important;max-width:none!important;width:calc(100% - 5px)!important;transform:translate(2px,2px)}div#onetrust-pc-sdk .ot-host-item{border-bottom:2px solid #b9b9b9;padding:0}div#onetrust-pc-sdk .ot-host-item .ot-acc-hdr{margin:0 0 -6px;padding:8px 0}div#onetrust-pc-sdk ul li:first-child{border-top:2px solid #b9b9b9}div#onetrust-pc-sdk .ot-host-item .ot-plus-minus{margin:0 8px 0 0}div#onetrust-pc-sdk .ot-search-cntr{width:calc(100% - 48px)}div#onetrust-pc-sdk .ot-host-opt .ot-host-info{background-color:transparent}div#onetrust-pc-sdk .ot-host-opt li>div div{padding:0}div#onetrust-pc-sdk #vendor-search-handler{border-radius:0;border-color:#939393;border-style:solid;border-width:2px 0 2px 2px;font-size:20px;height:48px;margin:0}div#onetrust-pc-sdk #ot-pc-hdr{margin-left:24px}div#onetrust-pc-sdk .ot-lst-subhdr{width:calc(100% - 24px)}div#onetrust-pc-sdk .ot-lst-subhdr svg{right:0;top:8px}div#onetrust-pc-sdk .ot-fltr-cntr{box-sizing:border-box;right:0;width:48px}div#onetrust-pc-sdk #filter-btn-handler{width:48px!important;padding:8px!important}div#onetrust-consent-sdk #onetrust-pc-sdk #clear-filters-handler,div#onetrust-pc-sdk button#filter-apply-handler,div#onetrust-pc-sdk button#filter-cancel-handler{height:2em!important;padding-left:14px!important;padding-right:14px!important}div#onetrust-pc-sdk #ot-fltr-cnt{box-shadow:0 0;border:1px solid #8e8e8e;border-radius:0}div#onetrust-pc-sdk .ot-fltr-scrlcnt{max-height:calc(100% - 80px)}div#onetrust-pc-sdk #ot-fltr-modal{max-height:400px}div#onetrust-pc-sdk .ot-fltr-opt{margin-bottom:16px}div#onetrust-pc-sdk #ot-lst-cnt{margin-left:24px;width:calc(100% - 48px)}div#onetrust-pc-sdk #ot-anchor{display:none!important}

/* 2023-12-04  Fix for button order in mobile view*/
@media (max-width: 550px) {
  #onetrust-accept-btn-handler {order: 1;  }
  #onetrust-reject-all-handler { order: 2;  }
  #onetrust-pc-btn-handler { order: 3;  }
}


/*! Extra code to blur our background */
.onetrust-pc-dark-filter{
backdrop-filter: blur(3px)
}
.ot-sdk-cookie-policy{font-family:inherit;font-size:16px}.ot-sdk-cookie-policy.otRelFont{font-size:1rem}.ot-sdk-cookie-policy h3,.ot-sdk-cookie-policy h4,.ot-sdk-cookie-policy h6,.ot-sdk-cookie-policy p,.ot-sdk-cookie-policy li,.ot-sdk-cookie-policy a,.ot-sdk-cookie-policy th,.ot-sdk-cookie-policy #cookie-policy-description,.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group,.ot-sdk-cookie-policy #cookie-policy-title{color:dimgray}.ot-sdk-cookie-policy #cookie-policy-description{margin-bottom:1em}.ot-sdk-cookie-policy h4{font-size:1.2em}.ot-sdk-cookie-policy h6{font-size:1em;margin-top:2em}.ot-sdk-cookie-policy th{min-width:75px}.ot-sdk-cookie-policy a,.ot-sdk-cookie-policy a:hover{background:#fff}.ot-sdk-cookie-policy thead{background-color:#f6f6f4;font-weight:bold}.ot-sdk-cookie-policy .ot-mobile-border{display:none}.ot-sdk-cookie-policy section{margin-bottom:2em}.ot-sdk-cookie-policy table{border-collapse:inherit}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy{font-family:inherit;font-size:1rem}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h3,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h4,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h6,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy p,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy li,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-title{color:dimgray}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description{margin-bottom:1em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-subgroup{margin-left:1.5em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group-desc,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-table-header,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy span,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td{font-size:.9em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td span,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td a{font-size:inherit}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group{font-size:1em;margin-bottom:.6em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-title{margin-bottom:1.2em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy>section{margin-bottom:1em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th{min-width:75px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a:hover{background:#fff}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy thead{background-color:#f6f6f4;font-weight:bold}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-mobile-border{display:none}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy section{margin-bottom:2em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-subgroup ul li{list-style:disc;margin-left:1.5em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-subgroup ul li h4{display:inline-block}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table{border-collapse:inherit;margin:auto;border:1px solid #d7d7d7;border-radius:5px;border-spacing:initial;width:100%;overflow:hidden}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table th,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table td{border-bottom:1px solid #d7d7d7;border-right:1px solid #d7d7d7}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr:last-child td{border-bottom:0px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr th:last-child,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr td:last-child{border-right:0px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-host,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-cookies-type{width:25%}.ot-sdk-cookie-policy[dir=rtl]{text-align:left}#ot-sdk-cookie-policy h3{font-size:1.5em}@media only screen and (max-width: 530px){.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) table,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) thead,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tbody,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) th,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr{display:block}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) thead tr{position:absolute;top:-9999px;left:-9999px}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr{margin:0 0 1em 0}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr:nth-child(odd),.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr:nth-child(odd) a{background:#f6f6f4}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td{border:none;border-bottom:1px solid #eee;position:relative;padding-left:50%}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td:before{position:absolute;height:100%;left:6px;width:40%;padding-right:10px}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) .ot-mobile-border{display:inline-block;background-color:#e4e4e4;position:absolute;height:100%;top:0;left:45%;width:2px}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td:before{content:attr(data-label);font-weight:bold}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) li{word-break:break-word;word-wrap:break-word}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table{overflow:hidden}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table td{border:none;border-bottom:1px solid #d7d7d7}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy thead,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy tbody,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy tr{display:block}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-host,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-cookies-type{width:auto}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy tr{margin:0 0 1em 0}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td:before{height:100%;width:40%;padding-right:10px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td:before{content:attr(data-label);font-weight:bold}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy li{word-break:break-word;word-wrap:break-word}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy thead tr{position:absolute;top:-9999px;left:-9999px;z-index:-9999}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr:last-child td{border-bottom:1px solid #d7d7d7;border-right:0px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr:last-child td:last-child{border-bottom:0px}}
                
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h5,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h6,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy li,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy p,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy span,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description {
                        color: #696969;
                    }
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th {
                        color: #696969;
                    }
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group {
                        color: #696969;
                    }
                    
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-title {
                            color: #696969;
                        }
                    
            
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table th {
                            background-color: #F8F8F8;
                        }
                    
            .ot-floating-button__front{background-image:url('https://cdn.cookielaw.org/logos/static/ot_persistent_cookie_icon.png')}</style><style data-styled="active" data-styled-version="6.1.13"></style></head>
    <body data-sd-ui-layer-boundary="true"><div id="MathJax_Message" style="display: none;"></div>
      <script type="text/javascript">
        window.__PRELOADED_STATE__ = {"abstracts":{"content":[{"$$":[{"$":{"id":"st005"},"#name":"section-title","_":"Abstract"},{"$$":[{"$":{"view":"all","id":"sp0005"},"#name":"simple-para","_":"Human Face expression Recognition is one of the most powerful and challenging tasks in social communication. Generally, face expressions are natural and direct means for human beings to communicate their emotions and intentions. Face expressions are the key characteristics of non-verbal communication. This paper describes the survey of Face Expression Recognition (FER) techniques which include the three major stages such as preprocessing, feature extraction and classification. This survey explains the various types of FER techniques with its major contributions. The performance of various FER techniques is compared based on the number of expressions recognized and complexity of algorithms. Databases like JAFFE, CK, and some other variety of facial expression databases are discussed in this survey. The study on classifiers gather from recent papers reveals a more powerful and reliable understanding of the peculiar characteristics of classifiers for research fellows."}],"$":{"view":"all","id":"as005"},"#name":"abstract-sec"}],"$":{"view":"all","id":"ab005","lang":"en","class":"author"},"#name":"abstract"}],"floats":[],"footnotes":[],"attachments":[]},"accessbarConfig":{"fallback":false,"id":"accessbar","version":"0.0.1","analytics":{"location":"accessbar","eventName":"ctaImpression"},"label":{},"ariaLabel":{"accessbar":"Download options and search","componentsList":"PDF Options"},"banners":[{"id":"BannerSsrn"}],"components":[{"target":"_blank","analytics":[{"ids":["accessbar:fta:single-article"],"eventName":"ctaClick"}],"label":"View&nbsp;**PDF**","ariaLabel":"View PDF. Opens in a new window.","id":"ViewPDF"},{"analytics":[{"ids":["accessbar:fta:full-issue"],"eventName":"ctaClick"}],"label":"Download full issue","id":"DownloadFullIssue"}],"search":{"inputPlaceHolder":"Search ScienceDirect","ariaLabel":{"input":"Search ScienceDirect","submit":"Submit search"},"formAction":"/search#submit","analytics":[{"ids":["accessbar:search"],"eventName":"searchStart"}],"id":"QuickSearch"}},"adobeTarget":{"sd:genai-question-and-answer":{}},"article":{"analyticsMetadata":{"accountId":"228598","accountName":"ScienceDirect Guests","loginStatus":"anonymous","userId":"12975512","isLoggedIn":false},"cid":"280416","content-family":"serial","copyright-line":"© 2018 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University.","cover-date-years":["2021"],"cover-date-start":"2021-07-01","cover-date-text":"July 2021","document-subtype":"rev","document-type":"article","entitledToken":"80A8634504EF352D11FE45616704CA690D1D6D96CCB74A7CAEFC7D720D5BDDF03B05F02120A10D42","genAiToken":"eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJhdWQiOiJnZW5BaUFwcHMiLCJwaWkiOiJTMTMxOTE1NzgxODMwMzM3OSIsInN1YiI6IjIyODU5OCIsImlzcyI6ImFycCIsInNlc3Npb25JZCI6IjhhZWU4ZTNhMmEyY2E3NGMxODliYzYwMGM0NDM4Y2E1NWU2MGd4cnFiIiwiZXhwIjoxNzM0ODkxOTQ0LCJpYXQiOjE3MzQ4OTAxNDQsInZlcnNpb24iOjEsImp0aSI6IjI5N2I0MmM0LWMyZTctNGVkOS1hOGY1LTZhNDRjYmE1NWJhZiJ9.INw-rl5KuxqTpZ36vnJSnI3_LMlOvdD4qOu3LxcMAUw","eid":"1-s2.0-S1319157818303379","doi":"10.1016/j.jksuci.2018.09.002","first-fp":"619","hub-eid":"1-s2.0-S1319157821X00067","issuePii":"S1319157821X00067","iss-first":"6","item-weight":"FULL-TEXT","language":"en","last-lp":"628","last-author":{"#name":"last-author","$":{"xmlns:ce":true,"xmlns:dm":true,"xmlns:sb":true},"$$":[{"#name":"author","$":{"id":"au010","author-id":"S1319157818303379-b908c9bd4cd45276712a16147ed94dc9"},"$$":[{"#name":"given-name","_":"W.R. Sam"},{"#name":"surname","_":"Emmanuel"},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTJDJTIyaHJlZiUyMiUzQSUyMm1haWx0byUzQXNhbV9lbW1hbnVlbCU0MG5tY2MuYWMuaW4lMjIlMkMlMjJpZCUyMiUzQSUyMmVtMDEwJTIyJTdEJTJDJTIyXyUyMiUzQSUyMnNhbV9lbW1hbnVlbCU0MG5tY2MuYWMuaW4lMjIlN0Q="}]}]},"normalized-first-auth-initial":"I","normalized-first-auth-surname":"REVINA","pages":[{"last-page":"628","first-page":"619"}],"pii":"S1319157818303379","srctitle":"Journal of King Saud University - Computer and Information Sciences","timestamp":"2022-11-30T20:01:34.883526Z","title":{"content":[{"#name":"article-footnote","$":{"id":"aep-article-footnote-id1"},"$$":[{"#name":"note-para","$":{"id":"np005","view":"all"},"_":"Peer review under responsibility of King Saud University."}]},{"#name":"title","$":{"id":"tm005"},"_":"A Survey on Human Face Expression Recognition Techniques"}],"floats":[],"footnotes":[{"#name":"article-footnote","$":{"id":"aep-article-footnote-id1"},"$$":[{"#name":"note-para","$":{"id":"np005","view":"all"},"_":"Peer review under responsibility of King Saud University."}]}],"attachments":[]},"vol-first":"33","vol-iss-suppl-text":"Volume 33, Issue 6","userSettings":{"forceAbstract":false,"creditCardPurchaseAllowed":true,"blockFullTextForAnonymousAccess":false,"disableWholeIssueDownload":false,"preventTransactionalAccess":false,"preventDocumentDelivery":true},"contentType":"JL","crossmark":true,"document-references":62,"freeHtmlGiven":false,"userProfile":{"departmentName":"ScienceDirect Guests","accessType":"GUEST","accountId":"228598","webUserId":"12975512","accountName":"ScienceDirect Guests","departmentId":"291352","userType":"NORMAL","hasMultipleOrganizations":false,"accountNumber":"C000228598"},"access":{"openAccess":true,"openArchive":false,"license":"http://creativecommons.org/licenses/by-nc-nd/4.0/"},"aipType":"none","articleEntitlement":{"entitled":true,"isCasaUser":false,"usageInfo":"(12975512,U|291352,D|228598,A|3,P|2,PL)(SDFE,CON|8aee8e3a2a2ca74c189bc600c4438ca55e60gxrqb,SSO|ANON_GUEST,ACCESS_TYPE)","entitledByAccount":false},"crawlerInformation":{"canCrawlPDFContent":false,"isCrawler":false},"dates":{"Available online":"5 September 2018","Received":"13 April 2018","Revised":["24 August 2018"],"Accepted":"3 September 2018","Publication date":"1 July 2021","Version of Record":"30 June 2021"},"downloadFullIssue":true,"entitlementReason":"openaccess","features":["aamAttachments","keywords","references","preview"],"hasBody":true,"has-large-authors":false,"hasScholarlyAbstract":true,"headerConfig":{"contactUrl":"https://service.elsevier.com/app/contact/supporthub/sciencedirect/","userName":"","userEmail":"","orgName":"ScienceDirect Guests","webUserId":"12975512","libraryBanner":null,"shib_regUrl":"","tick_regUrl":"","recentInstitutions":[],"canActivatePersonalization":false,"hasInstitutionalAssociation":false,"hasMultiOrg":false,"userType":"GUEST","userAnonymity":"ANON_GUEST","allowCart":true,"environment":"prod","cdnAssetsHost":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com"},"isCorpReq":false,"isPdfFullText":false,"issn":"13191578","issn-primary-formatted":"1319-1578","issRange":"6","isThirdParty":false,"pageCount":10,"pdfDownload":{"isPdfFullText":false,"urlMetadata":{"queryParams":{"md5":"eade4a7286505640e805ac4b8e74381c","pid":"1-s2.0-S1319157818303379-main.pdf"},"pii":"S1319157818303379","pdfExtension":"/pdfft","path":"science/article/pii"}},"publication-content":{"noElsevierLogo":false,"imprintPublisher":{"displayName":"Elsevier","id":"47"},"isSpecialIssue":false,"isSampleIssue":false,"transactionsBlocked":true,"publicationOpenAccess":{"oaStatus":"Full","oaArticleCount":2185,"openArchiveStatus":false,"openArchiveArticleCount":0,"openAccessStartDate":"","oaAllowsAuthorPaid":false},"issue-cover":{"attachment":[{"attachment-eid":"1-s2.0-S1319157821X00067-cov200h.gif","file-basename":"cov200h","extension":"gif","filename":"cov200h.gif","ucs-locator":["https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1319157821X00067/cover/DOWNSAMPLED200/image/gif/8cca35e4e5c3b4d86dcbe44318934ccc/cov200h.gif"],"attachment-type":"IMAGE-COVER-H200","filesize":"11491","pixel-height":"200","pixel-width":"149"},{"attachment-eid":"1-s2.0-S1319157821X00067-cov150h.gif","file-basename":"cov150h","extension":"gif","filename":"cov150h.gif","ucs-locator":["https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1319157821X00067/cover/DOWNSAMPLED/image/gif/fd9c1be99c1512fa110335bb9a099119/cov150h.gif"],"attachment-type":"IMAGE-COVER-H150","filesize":"6445","pixel-height":"150","pixel-width":"112"}]},"smallCoverUrl":"https://ars.els-cdn.com/content/image/S13191578.gif","title":"journal-of-king-saud-university-computer-and-information-sciences","serialCoverPgUrl":"https://ars.els-cdn.com/content/image/Dkingsauduni.gif","contentTypeCode":"JL","images":{"coverImage":"https://ars.els-cdn.com/content/image/1-s2.0-S1319157821X00067-cov150h.gif","logo":"https://ars.els-cdn.com/content/image/Dkingsauduni.gif","logoAltText":"Journal of King Saud University - Computer and Information Sciences"},"publicationCoverImageUrl":"https://ars.els-cdn.com/content/image/1-s2.0-S1319157821X00067-cov150h.gif"},"volRange":"33","open-research":{},"self-archiving":{},"titleString":"A Survey on Human Face Expression Recognition Techniques","ssrn":{},"renderingMode":"Article","isAbstract":false,"isContentVisible":false,"ajaxLinks":{"questionsAndAnswers":true,"referenceLinks":true,"references":true,"referredToBy":true,"toc":true,"body":true,"recommendations":true,"citingArticles":true,"authorMetadata":true},"pdfEmbed":false,"displayViewFullText":false},"authors":{"content":[{"#name":"author-group","$":{"id":"ag005"},"$$":[{"#name":"author","$":{"id":"au005","author-id":"S1319157818303379-c4ad26132cce46f3a2ab10e0f04ad549"},"$$":[{"#name":"given-name","_":"I.Michael"},{"#name":"surname","_":"Revina"},{"#name":"cross-ref","$":{"refid":"cor1","id":"c0305"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"⁎"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTJDJTIyaHJlZiUyMiUzQSUyMm1haWx0byUzQW1pY2hhZWxyZXZpbmEwOSU0MGdtYWlsLmNvbSUyMiUyQyUyMmlkJTIyJTNBJTIyZW0wMDUlMjIlN0QlMkMlMjJfJTIyJTNBJTIybWljaGFlbHJldmluYTA5JTQwZ21haWwuY29tJTIyJTdE"}]},{"#name":"author","$":{"id":"au010","author-id":"S1319157818303379-b908c9bd4cd45276712a16147ed94dc9"},"$$":[{"#name":"given-name","_":"W.R. Sam"},{"#name":"surname","_":"Emmanuel"},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTJDJTIyaHJlZiUyMiUzQSUyMm1haWx0byUzQXNhbV9lbW1hbnVlbCU0MG5tY2MuYWMuaW4lMjIlMkMlMjJpZCUyMiUzQSUyMmVtMDEwJTIyJTdEJTJDJTIyXyUyMiUzQSUyMnNhbV9lbW1hbnVlbCU0MG5tY2MuYWMuaW4lMjIlN0Q="}]},{"#name":"affiliation","$":{"id":"af005","affiliation-id":"S1319157818303379-2f0cf1535c544f9bf75a4f7b1d0d5544"},"$$":[{"#name":"textfn","_":"Reg No. 12417, N.M. Christian College, Marthandam Affiliated to Manonmaniam Sunadaranar University, Abishekapatti, Tirunelveli – 627012, Tamil Nadu, India"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Reg No. 12417"},{"#name":"organization","_":"N.M. Christian College"},{"#name":"organization","_":"Marthandam Affiliated to Manonmaniam Sunadaranar University"},{"#name":"address-line","_":"Abishekapatti"},{"#name":"city","_":"Tirunelveli – 627012"},{"#name":"state","_":"Tamil Nadu"},{"#name":"country","_":"India"}]}]},{"#name":"affiliation","$":{"id":"af010","affiliation-id":"S1319157818303379-4fb0a0d00f557ef6fec731be53b9de87"},"$$":[{"#name":"textfn","_":"Department of Computer Science, N.M. Christian College, Marthandam Affiliated to Manonmaniam Sunadaranar University, Abishekapatti, Tirunelveli – 627012, Tamil Nadu, India"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Computer Science"},{"#name":"organization","_":"N.M. Christian College"},{"#name":"organization","_":"Marthandam Affiliated to Manonmaniam Sunadaranar University"},{"#name":"address-line","_":"Abishekapatti"},{"#name":"city","_":"Tirunelveli – 627012"},{"#name":"state","_":"Tamil Nadu"},{"#name":"country","_":"India"}]}]},{"#name":"correspondence","$":{"id":"cor1"},"$$":[{"#name":"label","_":"⁎"},{"#name":"text","_":"Corresponding author."}]}]}],"floats":[],"footnotes":[],"affiliations":{"af005":{"#name":"affiliation","$":{"id":"af005","affiliation-id":"S1319157818303379-2f0cf1535c544f9bf75a4f7b1d0d5544"},"$$":[{"#name":"textfn","_":"Reg No. 12417, N.M. Christian College, Marthandam Affiliated to Manonmaniam Sunadaranar University, Abishekapatti, Tirunelveli – 627012, Tamil Nadu, India"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Reg No. 12417"},{"#name":"organization","_":"N.M. Christian College"},{"#name":"organization","_":"Marthandam Affiliated to Manonmaniam Sunadaranar University"},{"#name":"address-line","_":"Abishekapatti"},{"#name":"city","_":"Tirunelveli – 627012"},{"#name":"state","_":"Tamil Nadu"},{"#name":"country","_":"India"}]}]},"af010":{"#name":"affiliation","$":{"id":"af010","affiliation-id":"S1319157818303379-4fb0a0d00f557ef6fec731be53b9de87"},"$$":[{"#name":"textfn","_":"Department of Computer Science, N.M. Christian College, Marthandam Affiliated to Manonmaniam Sunadaranar University, Abishekapatti, Tirunelveli – 627012, Tamil Nadu, India"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Computer Science"},{"#name":"organization","_":"N.M. Christian College"},{"#name":"organization","_":"Marthandam Affiliated to Manonmaniam Sunadaranar University"},{"#name":"address-line","_":"Abishekapatti"},{"#name":"city","_":"Tirunelveli – 627012"},{"#name":"state","_":"Tamil Nadu"},{"#name":"country","_":"India"}]}]}},"correspondences":{"cor1":{"#name":"correspondence","$":{"id":"cor1"},"$$":[{"#name":"label","_":"⁎"},{"#name":"text","_":"Corresponding author."}]}},"attachments":[],"scopusAuthorIds":{},"articles":{}},"authorMetadata":[],"banner":{"expanded":false},"biographies":{},"body":{},"chapters":{"toc":[],"isLoading":false},"changeViewLinks":{"showFullTextLink":false,"showAbstractLink":false},"citingArticles":{},"combinedContentItems":{"content":[{"#name":"keywords","$$":[{"#name":"keywords","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:xoe":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"class":"keyword","id":"kg005","view":"all"},"$$":[{"#name":"section-title","$":{"id":"st010"},"_":"Keywords"},{"#name":"keyword","$":{"id":"k0005"},"$$":[{"#name":"text","_":"Classification"}]},{"#name":"keyword","$":{"id":"k0010"},"$$":[{"#name":"text","_":"Face Expression Recognition (FER)"}]},{"#name":"keyword","$":{"id":"k0015"},"$$":[{"#name":"text","_":"Feature extraction"}]},{"#name":"keyword","$":{"id":"k0020"},"$$":[{"#name":"text","_":"Preprocessing"}]}]}]}],"floats":[],"footnotes":[],"attachments":[]},"crossMark":{"isOpen":false},"domainConfig":{"cdnAssetsHost":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com","assetRoute":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/b64013ec63c69e3d916174cbebae89d65b2419e1"},"downloadIssue":{"openOnPageLoad":false,"isOpen":false,"downloadCapOpen":false,"articles":[],"selected":[]},"enrichedContent":{"tableOfContents":false,"researchData":{"hasResearchData":false,"dataProfile":{},"openData":{},"mendeleyData":{},"databaseLinking":{}},"geospatialData":{"attachments":[]},"interactiveCaseInsights":{},"virtualMicroscope":{}},"entitledRecommendations":{"openOnPageLoad":false,"isOpen":false,"articles":[],"selected":[],"currentPage":1,"totalPages":1},"exam":{},"helpText":{"keyDates":{"html":"<div class=\"key-dates-help\"><h3 class=\"u-margin-s-bottom u-h4\">Publication milestones</h3><p class=\"u-margin-m-bottom\">The dates displayed for an article provide information on when various publication milestones were reached at the journal that has published the article. Where applicable, activities on preceding journals at which the article was previously under consideration are not shown (for instance submission, revisions, rejection).</p><p class=\"u-margin-xs-bottom\">The publication milestones include:</p><ul class=\"key-dates-help-list u-margin-m-bottom u-padding-s-left\"><li><span class=\"u-text-italic\">Received</span>: The date the article was originally submitted to the journal.</li><li><span class=\"u-text-italic\">Revised</span>: The date the most recent revision of the article was submitted to the journal. Dates corresponding to intermediate revisions are not shown.</li><li><span class=\"u-text-italic\">Accepted</span>: The date the article was accepted for publication in the journal.</li><li><span class=\"u-text-italic\">Available online</span>: The date a version of the article was made available online in the journal.</li><li><span class=\"u-text-italic\">Version of Record</span>: The date the finalized version of the article was made available in the journal.</li></ul><p>More information on publishing policies can be found on the <a class=\"anchor anchor-secondary u-display-inline anchor-underline\" href=\"https://www.elsevier.com/about/policies-and-standards/publishing-ethics\" target=\"_blank\"><span class=\"anchor-text-container\"><span class=\"anchor-text\">Publishing Ethics Policies</span></span></a> page. View our <a class=\"anchor anchor-secondary u-display-inline anchor-underline\" href=\"https://www.elsevier.com/researcher/author/submit-your-paper\" target=\"_blank\"><span class=\"anchor-text-container\"><span class=\"anchor-text\">Publishing with Elsevier: step-by-step</span></span></a> page to learn more about the publishing process. For any questions on your own submission or other questions related to publishing an article, <a class=\"anchor anchor-secondary u-display-inline anchor-underline\" href=\"https://service.elsevier.com/app/phone/supporthub/publishing\" target=\"_blank\"><span class=\"anchor-text-container\"><span class=\"anchor-text\">contact our Researcher support team.</span></span></a></p></div>","title":"What do these dates mean?"}},"glossary":{},"issueNavigation":{"previous":{},"next":{}},"linkingHubLinks":{},"metrics":{"metricGroup":{"citations":[],"captures":[],"mentions":[],"socialMedia":[]},"isLoading":false,"error":false},"preview":{},"rawtext":"","recommendations":{},"references":{},"referenceLinks":{"internal":[],"internalLoaded":false,"external":[]},"refersTo":{},"referredToBy":{},"relatedContent":{"isModal":false,"isOpenSpecialIssueArticles":false,"isOpenVirtualSpecialIssueLink":false,"isOpenRecommendations":true,"isOpenSubstances":true,"citingArticles":[false,false,false,false,false,false],"recommendations":[false,false,false,false,false,false]},"seamlessAccess":{},"specialIssueArticles":{},"substances":{},"supplementaryFilesData":[],"tableOfContents":{"showEntitledTocLinks":true},"tail":{},"transientError":{"isOpen":false},"sidePanel":{"openState":1},"viewConfig":{"articleFeature":{"rightsAndContentLink":true,"sdAnswersButton":false},"pathPrefix":""},"virtualSpecialIssue":{"showVirtualSpecialIssueLink":false},"usageProps":{"itemStage":"S300","isAip":false,"tombAip":"0","sample":false},"userCookiePreferences":{"STRICTLY_NECESSARY":true,"PERFORMANCE":false,"FUNCTIONAL":false,"TARGETING":false}};
      </script>
      <noscript>
      JavaScript is disabled on your browser.
      Please enable JavaScript to use all the features on this page.
      <img src=https://smetrics.elsevier.com/b/ss/elsevier-sd-prod/1/G.4--NS/1734890144374?pageName=sd%3Aproduct%3Ajournal%3Aarticle&c16=els%3Arp%3Ast&c2=sd&v185=img&v33=ae%3AANON_GUEST&c1=ae%3A228598&c12=ae%3A12975512 />
    </noscript>
      <div id="elementForFocusReset" tabindex="-1"></div><a class="anchor sr-only sr-only-focusable u-display-inline anchor-primary" href="#screen-reader-main-content"><span class="anchor-text-container"><span class="anchor-text">Skip to main content</span></span></a><a class="anchor sr-only sr-only-focusable u-display-inline anchor-primary" href="#screen-reader-main-title"><span class="anchor-text-container"><span class="anchor-text">Skip to article</span></span></a>
      <div id="root"><div class="App" id="app" data-aa-name="root"><div class="page"><div class="sd-flex-container"><div class="sd-flex-content"><header id="gh-cnt"><div id="gh-main-cnt" class="u-flex-center-ver u-position-relative u-padding-s-hor u-padding-l-hor-from-xl"><a id="gh-branding" class="u-flex-center-ver" href="/" aria-label="ScienceDirect home page" data-aa-region="header" data-aa-name="ScienceDirect"><img class="gh-logo" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/24/images/elsevier-non-solus-new-grey.svg" alt="Elsevier logo" height="48" width="54"><svg xmlns="http://www.w3.org/2000/svg" version="1.1" height="15" viewBox="0 0 190 23" role="img" class="gh-wordmark u-margin-s-left" aria-labelledby="gh-wm-science-direct" focusable="false" aria-hidden="true" alt="ScienceDirect Wordmark"><title id="gh-wm-science-direct">ScienceDirect</title><g><path fill="#EB6500" d="M3.81 6.9c0-1.48 0.86-3.04 3.7-3.04 1.42 0 3.1 0.43 4.65 1.32l0.13-2.64c-1.42-0.63-2.97-0.96-4.78-0.96 -4.62 0-6.6 2.44-6.6 5.45 0 5.61 8.78 6.14 8.78 9.93 0 1.48-1.15 3.04-3.86 3.04 -1.72 0-3.4-0.56-4.72-1.39l-0.36 2.64c1.55 0.76 3.57 1.06 5.15 1.06 4.26 0 6.7-2.48 6.7-5.51C12.59 11.49 3.81 10.76 3.81 6.9M20.27 9.01c0.23-0.13 0.69-0.26 1.72-0.26 1.72 0 2.41 0.3 2.41 1.58h2.38c0-0.36 0-0.79-0.03-1.09 -0.23-1.98-2.15-2.67-4.88-2.67 -3 0-6.7 2.31-6.7 7.76 0 5.22 2.77 7.99 6.63 7.99 1.68 0 3.47-0.36 4.95-1.39l-0.2-2.31c-0.99 0.82-2.84 1.52-4.06 1.52 -2.14 0-4.55-1.71-4.55-5.91C17.93 10.2 20.01 9.18 20.27 9.01"></path><rect x="29.42" y="6.97" fill="#EB6500" width="2.54" height="14.95"></rect><path fill="#EB6500" d="M30.67 0.7c-0.92 0-1.65 0.92-1.65 1.81 0 0.93 0.76 1.85 1.65 1.85 0.89 0 1.68-0.96 1.68-1.88C32.35 1.55 31.56 0.7 30.67 0.7M48.06 14.13c0-5.18-1.42-7.56-6.01-7.56 -3.86 0-6.67 2.77-6.67 7.92 0 4.92 2.97 7.82 6.73 7.82 2.81 0 4.36-0.63 5.68-1.42l-0.2-2.31c-0.89 0.79-2.94 1.55-4.69 1.55 -3.14 0-4.88-1.95-4.88-5.51v-0.49H48.06M39.91 9.18c0.17-0.17 1.29-0.46 1.98-0.46 2.48 0 3.76 0.53 3.86 3.43h-7.46C38.56 10.27 39.71 9.37 39.91 9.18zM58.82 6.57c-2.24 0-3.63 1.12-4.85 2.61l-0.4-2.21h-2.34l0.13 1.19c0.1 0.76 0.13 1.78 0.13 2.97v10.79h2.54V11.88c0.69-0.96 2.15-2.48 2.48-2.64 0.23-0.13 1.29-0.4 2.08-0.4 2.28 0 2.48 1.15 2.54 3.43 0.03 1.19 0.03 3.17 0.03 3.17 0.03 3-0.1 6.47-0.1 6.47h2.54c0 0 0.07-4.49 0.07-6.96 0-1.48 0.03-2.97-0.1-4.46C63.31 7.43 61.49 6.57 58.82 6.57M72.12 9.01c0.23-0.13 0.69-0.26 1.72-0.26 1.72 0 2.41 0.3 2.41 1.58h2.38c0-0.36 0-0.79-0.03-1.09 -0.23-1.98-2.15-2.67-4.88-2.67 -3 0-6.7 2.31-6.7 7.76 0 5.22 2.77 7.99 6.63 7.99 1.68 0 3.47-0.36 4.95-1.39l-0.2-2.31c-0.99 0.82-2.84 1.52-4.06 1.52 -2.15 0-4.55-1.71-4.55-5.91C69.77 10.2 71.85 9.18 72.12 9.01M92.74 14.13c0-5.18-1.42-7.56-6.01-7.56 -3.86 0-6.67 2.77-6.67 7.92 0 4.92 2.97 7.82 6.73 7.82 2.81 0 4.36-0.63 5.68-1.42l-0.2-2.31c-0.89 0.79-2.94 1.55-4.69 1.55 -3.14 0-4.88-1.95-4.88-5.51v-0.49H92.74M84.59 9.18c0.17-0.17 1.29-0.46 1.98-0.46 2.48 0 3.76 0.53 3.86 3.43h-7.46C83.24 10.27 84.39 9.37 84.59 9.18zM103.9 1.98h-7.13v19.93h6.83c7.26 0 9.77-5.68 9.77-10.03C113.37 7.33 110.93 1.98 103.9 1.98M103.14 19.8h-3.76V4.1h4.09c5.38 0 6.96 4.39 6.96 7.79C110.43 16.87 108.19 19.8 103.14 19.8zM118.38 0.7c-0.92 0-1.65 0.92-1.65 1.81 0 0.93 0.76 1.85 1.65 1.85 0.89 0 1.69-0.96 1.69-1.88C120.07 1.55 119.28 0.7 118.38 0.7"></path><rect x="117.13" y="6.97" fill="#EB6500" width="2.54" height="14.95"></rect><path fill="#EB6500" d="M130.2 6.6c-1.62 0-2.87 1.45-3.4 2.74l-0.43-2.37h-2.34l0.13 1.19c0.1 0.76 0.13 1.75 0.13 2.9v10.86h2.54v-9.51c0.53-1.29 1.72-3.7 3.17-3.7 0.96 0 1.06 0.99 1.06 1.22l2.08-0.6V9.18c0-0.03-0.03-0.17-0.06-0.4C132.8 7.36 131.91 6.6 130.2 6.6M145.87 14.13c0-5.18-1.42-7.56-6.01-7.56 -3.86 0-6.67 2.77-6.67 7.92 0 4.92 2.97 7.82 6.73 7.82 2.81 0 4.36-0.63 5.68-1.42l-0.2-2.31c-0.89 0.79-2.94 1.55-4.69 1.55 -3.14 0-4.89-1.95-4.89-5.51v-0.49H145.87M137.72 9.18c0.17-0.17 1.29-0.46 1.98-0.46 2.48 0 3.76 0.53 3.86 3.43h-7.46C136.37 10.27 137.52 9.37 137.72 9.18zM153.23 9.01c0.23-0.13 0.69-0.26 1.72-0.26 1.72 0 2.41 0.3 2.41 1.58h2.38c0-0.36 0-0.79-0.03-1.09 -0.23-1.98-2.14-2.67-4.88-2.67 -3 0-6.7 2.31-6.7 7.76 0 5.22 2.77 7.99 6.63 7.99 1.69 0 3.47-0.36 4.95-1.39l-0.2-2.31c-0.99 0.82-2.84 1.52-4.06 1.52 -2.15 0-4.55-1.71-4.55-5.91C150.89 10.2 152.97 9.18 153.23 9.01M170 19.44c-0.92 0.36-1.72 0.69-2.51 0.69 -1.16 0-1.58-0.66-1.58-2.34V8.95h3.93V6.97h-3.93V2.97h-2.48v3.99h-2.71v1.98h2.71v9.67c0 2.64 1.39 3.73 3.33 3.73 1.15 0 2.54-0.39 3.43-0.79L170 19.44M173.68 5.96c-1.09 0-2-0.87-2-1.97 0-1.1 0.91-1.97 2-1.97s1.98 0.88 1.98 1.98C175.66 5.09 174.77 5.96 173.68 5.96zM173.67 2.46c-0.85 0-1.54 0.67-1.54 1.52 0 0.85 0.69 1.54 1.54 1.54 0.85 0 1.54-0.69 1.54-1.54C175.21 3.13 174.52 2.46 173.67 2.46zM174.17 5.05c-0.09-0.09-0.17-0.19-0.25-0.3l-0.41-0.56h-0.16v0.87h-0.39V2.92c0.22-0.01 0.47-0.03 0.66-0.03 0.41 0 0.82 0.16 0.82 0.64 0 0.29-0.21 0.55-0.49 0.63 0.23 0.32 0.45 0.62 0.73 0.91H174.17zM173.56 3.22l-0.22 0.01v0.63h0.22c0.26 0 0.43-0.05 0.43-0.34C174 3.28 173.83 3.21 173.56 3.22z"></path></g></svg></a><div class="gh-nav-cnt u-hide-from-print"><div class="gh-nav-links-container gh-nav-links-container-h u-hide-from-print gh-nav-content-container"><nav aria-label="links" class="gh-nav gh-nav-links gh-nav-h"><ul class="gh-nav-list u-list-reset"><li class="gh-nav-item gh-move-to-spine"><a class="anchor gh-nav-action text-s anchor-secondary anchor-medium" href="/browse/journals-and-books" id="gh-journals-books-link" data-aa-region="header" data-aa-name="Journals &amp; Books"><span class="anchor-text-container"><span class="anchor-text">Journals &amp; Books</span></span></a></li></ul></nav><nav aria-label="utilities" class="gh-nav gh-nav-utilities gh-nav-h"><ul class="gh-nav-list u-list-reset"><li class="gh-nav-help text-s u-flex-center-ver u-gap-6 gh-nav-action"><div class="gh-move-to-spine gh-help-button gh-help-icon gh-nav-item"><div class="popover" id="gh-help-icon-popover"><div id="popover-trigger-gh-help-icon-popover"><input type="hidden"><button class="button-link button-link-secondary gh-icon-btn button-link-medium button-link-icon-left" title="Help" aria-expanded="false" type="button"><svg focusable="false" viewBox="0 0 114 128" height="20" width="20" class="icon icon-help gh-icon"><path d="M57 8C35.69 7.69 15.11 21.17 6.68 40.71c-8.81 19.38-4.91 43.67 9.63 59.25 13.81 15.59 36.85 21.93 56.71 15.68 21.49-6.26 37.84-26.81 38.88-49.21 1.59-21.15-10.47-42.41-29.29-52.1C74.76 10.17 65.88 7.99 57 8zm0 10c20.38-.37 39.57 14.94 43.85 34.85 4.59 18.53-4.25 39.23-20.76 48.79-17.05 10.59-40.96 7.62-54.9-6.83-14.45-13.94-17.42-37.85-6.83-54.9C26.28 26.5 41.39 17.83 57 18zm-.14 14C45.31 32.26 40 40.43 40 50v2h10v-2c0-4.22 2.22-9.66 8-9.24 5.5.4 6.32 5.14 5.78 8.14C62.68 55.06 52 58.4 52 69.4V76h10v-5.56c0-8.16 11.22-11.52 12-21.7.74-9.86-5.56-16.52-16-16.74-.39-.01-.76-.01-1.14 0zM52 82v10h10V82H52z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Help</span></span></button></div></div></div></li><li class="gh-nav-search text-s u-flex-center-ver u-gap-6 gh-nav-action"><div class="gh-search-toggle gh-nav-item search-button-link"><a class="anchor button-link-secondary anchor-secondary u-margin-l-left gh-nav-action gh-icon-btn anchor-medium anchor-icon-left anchor-with-icon" href="/search" id="gh-search-link" title="Search" data-aa-button="search-in-header-opened-from-article" role="button"><svg focusable="false" viewBox="0 0 100 128" height="20" class="icon icon-search gh-icon"><path d="M19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6L73.23 80.19c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96S19.9 18.47 12.19 26.18C4.47 33.89.22 44.15.22 55.06s4.25 21.17 11.97 28.88C19.9 91.65 30.16 95.9 41.07 95.9c9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg><span class="anchor-text-container"><span class="anchor-text">Search</span></span></a></div></li></ul></nav></div></div><div class="gh-profile-container gh-move-to-spine u-hide-from-print"><a class="anchor text-s u-clr-grey8 u-margin-l-left gh-icon-btn anchor-primary anchor-medium anchor-icon-left anchor-with-icon" href="/user/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS1319157818303379&amp;from=globalheader" id="gh-myaccount-btn" data-aa-region="header" data-aa-name="personalsignin"><svg focusable="false" viewBox="0 0 106 128" height="20" aria-hidden="true" class="icon icon-person gh-cta-btn-icon"><path d="M11.07 120l.84-9.29C13.88 91.92 35.25 87.78 53 87.78c17.74 0 39.11 4.13 41.08 22.84l.84 9.38h10.04l-.93-10.34C101.88 89.23 83.89 78 53 78S4.11 89.22 1.95 109.73L1.04 120h10.03M53 17.71c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98S71.24 49.97 71.24 36.3c0-9.9-8.52-18.59-18.24-18.59zM53 70c-15.96 0-28-14.48-28-33.67C25 20.97 37.82 8 53 8s28 12.97 28 28.33C81 55.52 68.96 70 53 70"></path></svg><span class="anchor-text-container"><span class="anchor-text">My account</span></span></a></div><a class="anchor text-s u-clr-grey8 gh-move-to-spine u-hide-from-print u-margin-l-left anchor-secondary gh-icon-btn anchor-medium anchor-icon-left anchor-with-icon" href="/user/institution/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS1319157818303379" id="gh-institutionalsignin-btn" data-aa-region="header" data-aa-name="institutionalsignin"><svg focusable="false" viewBox="0 0 106 128" height="20" aria-hidden="true" class="icon icon-institution gh-cta-btn-icon"><path d="M84 98h10v10H12V98h10V52h14v46h10V52h14v46h10V52h14v46zM12 36.86l41-20.84 41 20.84V42H12v-5.14zM104 52V30.74L53 4.8 2 30.74V52h10v36H2v30h102V88H94V52h10z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Sign in</span></span></a><div id="gh-mobile-menu" class="mobile-menu u-hide-from-print"><div class="gh-hamburger u-fill-grey7"><button class="button-link u-flex-center-ver button-link-primary button-link-icon-left" aria-label="Toggle mobile menu" aria-expanded="false" type="button"><svg class="gh-hamburger-svg-el gh-hamburger-closed" role="img" aria-hidden="true" height="20" width="20"><path d="M0 14h40v2H0zm0-7h40v2H0zm0-7h40v2H0z"></path></svg></button></div><div id="gh-overlay" class="mobile-menu-overlay u-overlay u-display-none" role="button" tabindex="-1"></div><div id="gh-drawer" aria-label="Mobile menu" class="" role="navigation"></div></div></div></header><div class="Article" id="mathjax-container" role="main"><div class="accessbar-sticky"><div id="screen-reader-main-content"></div><div role="region" aria-label="Download options and search"><div class="accessbar"><div class="accessbar-label"></div><ul aria-label="PDF Options"><li class="ViewPDF"><a class="link-button accessbar-utility-component accessbar-utility-link link-button-primary link-button-icon-left" target="_blank" aria-label="View PDF. Opens in a new window." href="/science/article/pii/S1319157818303379/pdfft?md5=eade4a7286505640e805ac4b8e74381c&amp;pid=1-s2.0-S1319157818303379-main.pdf" rel="nofollow"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="link-button-text-container"><span class="link-button-text"><span>View&nbsp;<strong>PDF</strong></span></span></span></a></li><li class="DownloadFullIssue"><button class="button-link accessbar-utility-component button-link-primary" aria-label="Download full issue" type="button"><span class="button-link-text-container"><span class="button-link-text"><span>Download full issue</span></span></span></button></li></ul><form class="QuickSearch" action="/search#submit" method="get" aria-label="form"><div class="search-input"><div class="search-input-container search-input-container-no-label"><label class="search-input-label u-hide-visually" for="article-quick-search">Search ScienceDirect</label><input type="text" id="article-quick-search" name="qs" class="search-input-field" aria-describedby="article-quick-search-description-message" aria-invalid="false" aria-label="Search ScienceDirect" placeholder="Search ScienceDirect" value=""></div><div class="search-input-message-container"><div class="search-input-validation-error" aria-live="polite"></div><div id="article-quick-search-description-message"></div></div></div><button type="submit" class="button u-margin-xs-left button-primary small button-icon-only" aria-disabled="false" aria-label="Submit search"><svg focusable="false" viewBox="0 0 100 128" height="20" class="icon icon-search"><path d="M19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6L73.23 80.19c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96S19.9 18.47 12.19 26.18C4.47 33.89.22 44.15.22 55.06s4.25 21.17 11.97 28.88C19.9 91.65 30.16 95.9 41.07 95.9c9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg></button><input type="hidden" name="origin" value="article"><input type="hidden" name="zone" value="qSearch"></form></div></div></div><div class="article-wrapper grid row"><div role="navigation" class="u-display-block-from-lg col-lg-6 u-padding-s-top sticky-table-of-contents" aria-label="Table of contents"><div class="TableOfContents" lang="en"><div class="Outline" id="toc-outline"><h2 class="u-h4">Outline</h2><ol class="u-padding-xs-bottom"><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#ab005" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Abstract"><span class="anchor-text-container"><span class="anchor-text">Abstract</span></span></a></li><li class="ai-components-toc-entry toc-list-entry-outline-padding" id="ai-components-toc-entry"><span class="ai-badges"><strong class="badge-info badge-md u-font-sans sc-egkSDF hndnke text-2xs u-margin-xs-right">Beta</strong><strong class="badge-info badge-md u-font-sans sc-gtLWhw dNlFzS text-2xs">Powered by GenAI</strong></span><a class="anchor u-truncate-anchor-text anchor-primary" href="#ai-components-toc-id" title="Questions answered in this article"><span class="anchor-text-container"><span class="anchor-text">Questions answered in this article</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#kg005" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Keywords"><span class="anchor-text-container"><span class="anchor-text">Keywords</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#s0005" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="1. Introduction"><span class="anchor-text-container"><span class="anchor-text">1. Introduction</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#s0010" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="2. Face expression recognition system"><span class="anchor-text-container"><span class="anchor-text">2. Face expression recognition system</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#s0035" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="3. Performance comparison"><span class="anchor-text-container"><span class="anchor-text">3. Performance comparison</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#s0040" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="4. Conclusion"><span class="anchor-text-container"><span class="anchor-text">4. Conclusion</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#bi005" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="References"><span class="anchor-text-container"><span class="anchor-text">References</span></span></a></li></ol><button class="button-link u-margin-xs-top u-margin-s-bottom button-link-primary button-link-icon-right" aria-expanded="false" data-aa-button="sd:product:journal:article:type=menu:name=show-full-outline" type="button"><span class="button-link-text-container"><span class="button-link-text">Show full outline</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="PageDivider"></div></div><div class="CitedBy" id="toc-cited-by"><h2 class="u-h4"><a class="anchor anchor-primary" href="#section-cited-by"><span class="anchor-text-container"><span class="anchor-text">Cited by (178)</span></span></a></h2><div class="PageDivider"></div></div><div class="Figures" id="toc-figures"><h2 class="u-h4">Figures (7)</h2><ol class="u-margin-s-bottom"><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#f0005" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig. 1. Architecture of face expression recognition system" class="u-display-block" height="87px" src="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr1.sml" width="219px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#f0010" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig. 2. Sample images from JAFFE database" class="u-display-block" height="152px" src="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr2.sml" width="219px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#f0015" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig. 3. Sample images from CK database" class="u-display-block" height="163px" src="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr3.sml" width="219px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#f0020" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig. 4. Complexity rate of various FER techniques" class="u-display-block" height="164px" src="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr4.sml" width="198px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#f0025" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig. 5. Accuracy rate of various FER techniques" class="u-display-block" height="141px" src="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr5.sml" width="219px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#f0030" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig. 6. Availability of preprocessing and feature extraction" class="u-display-block" height="127px" src="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr6.sml" width="219px"></a></li></ol><button class="button-link u-margin-xs-top u-margin-s-bottom button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:type=menu:name=show-figures" type="button"><span class="button-link-text-container"><span class="button-link-text">Show 1 more figure</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="PageDivider"></div></div><div class="Tables" id="toc-tables"><h2 class="u-h4">Tables (3)</h2><ol class="u-padding-s-bottom"><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#t0005" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Algorithm analysis of 2D FER Techniques."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table 1</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#t0010" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="FER Databases description."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table 2</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#t0015" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Performance analysis of FER techniques."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table 3</span></span></a></li></ol><div class="PageDivider"></div></div></div></div><article class="col-lg-12 col-md-16 pad-left pad-right u-padding-s-top" lang="en"><div class="Publication" id="publication"><div class="publication-brand u-display-block-from-sm"><a class="anchor u-display-flex anchor-primary" href="/journal/journal-of-king-saud-university-computer-and-information-sciences" title="Go to Journal of King Saud University - Computer and Information Sciences on ScienceDirect"><span class="anchor-text-container"><span class="anchor-text"><img class="publication-brand-image" src="https://ars.els-cdn.com/content/image/Dkingsauduni.gif" alt="Journal of King Saud University - Computer and Information Sciences"></span></span></a></div><div class="publication-volume u-text-center"><h2 class="publication-title u-h3" id="publication-title"><a class="anchor anchor-secondary publication-title-link" href="/journal/journal-of-king-saud-university-computer-and-information-sciences" title="Go to Journal of King Saud University - Computer and Information Sciences on ScienceDirect"><span class="anchor-text-container"><span class="anchor-text">Journal of King Saud University - Computer and Information Sciences</span></span></a></h2><div class="text-xs"><a class="anchor anchor-primary" href="/journal/journal-of-king-saud-university-computer-and-information-sciences/vol/33/issue/6" title="Go to table of contents for this volume/issue"><span class="anchor-text-container"><span class="anchor-text">Volume 33, Issue 6</span></span></a>, <!-- -->July 2021<!-- -->, Pages 619-628</div></div><div class="publication-cover u-display-block-from-sm"><a class="anchor u-display-flex anchor-primary" href="/journal/journal-of-king-saud-university-computer-and-information-sciences/vol/33/issue/6"><span class="anchor-text-container"><span class="anchor-text"><img class="publication-cover-image" src="https://ars.els-cdn.com/content/image/1-s2.0-S1319157821X00067-cov150h.gif" alt="Journal of King Saud University - Computer and Information Sciences"></span></span></a></div></div><h1 id="screen-reader-main-title" class="Head u-font-serif u-h2 u-margin-s-ver"><span class="title-text">A Survey on Human Face Expression Recognition Techniques</span></h1><div class="Banner" id="banner"><div class="wrapper truncated"><div aria-live="polite"></div><div class="AuthorGroups"><div class="author-group" id="author-group"><span class="sr-only">Author links open overlay panel</span><button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au005" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">I.Michael</span> <span class="text surname">Revina</span></span><svg focusable="false" viewBox="0 0 106 128" height="20" title="Correspondence author icon" class="icon icon-person react-xocs-author-icon u-fill-grey8"><path d="M11.07 120l.84-9.29C13.88 91.92 35.25 87.78 53 87.78c17.74 0 39.11 4.13 41.08 22.84l.84 9.38h10.04l-.93-10.34C101.88 89.23 83.89 78 53 78S4.11 89.22 1.95 109.73L1.04 120h10.03M53 17.71c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98S71.24 49.97 71.24 36.3c0-9.9-8.52-18.59-18.24-18.59zM53 70c-15.96 0-28-14.48-28-33.67C25 20.97 37.82 8 53 8s28 12.97 28 28.33C81 55.52 68.96 70 53 70"></path></svg><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au010" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">W.R. Sam</span> <span class="text surname">Emmanuel</span></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button></div></div></div><button class="button-link u-margin-s-ver button-link-primary button-link-icon-right" id="show-more-btn" type="button" data-aa-button="icon-expand"><span class="button-link-text-container"><span class="button-link-text">Show more</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="banner-options u-padding-xs-bottom"><div class="toc-button-wrap u-display-inline-block u-display-none-from-lg u-margin-s-right"><button class="button-link button-link-secondary button-link-icon-left button-link-has-colored-icon" type="button"><svg focusable="false" viewBox="0 0 128 128" height="20" class="icon icon-list"><path d="M23 26a9 9 0 0 0-9 9 9 9 0 0 0 9 9 9 9 0 0 0 9-9 9 9 0 0 0-9-9zm23 4v10h68V30zM23 56a9 9 0 0 0-9 9 9 9 0 0 0 9 9 9 9 0 0 0 9-9 9 9 0 0 0-9-9zm23 4v10h68V60zM23 86a9 9 0 0 0-9 9 9 9 0 0 0 9 9 9 9 0 0 0 9-9 9 9 0 0 0-9-9zm23 4v10h68V90z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Outline</span></span></button></div><button class="button-link AddToMendeley button-link-secondary u-margin-s-right u-display-inline-flex-from-md button-link-icon-left button-link-has-colored-icon" type="button"><svg focusable="false" viewBox="0 0 86 128" height="20" class="icon icon-plus"><path d="M48 58V20H38v38H0v10h38v38h10V68h38V58z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Add to Mendeley</span></span></button><div class="Social u-display-inline-block" id="social"><div class="popover social-popover" id="social-popover"><div id="popover-trigger-social-popover"><button class="button-link button-link-secondary u-margin-s-right button-link-icon-left button-link-has-colored-icon" aria-expanded="false" aria-haspopup="true" type="button"><svg focusable="false" viewBox="0 0 114 128" height="20" class="icon icon-share"><path d="M90 112c-6.62 0-12-5.38-12-12s5.38-12 12-12 12 5.38 12 12-5.38 12-12 12zM24 76c-6.62 0-12-5.38-12-12s5.38-12 12-12 12 5.38 12 12-5.38 12-12 12zm66-60c6.62 0 12 5.38 12 12s-5.38 12-12 12-12-5.38-12-12 5.38-12 12-12zm0 62c-6.56 0-12.44 2.9-16.48 7.48L45.1 70.2c.58-1.98.9-4.04.9-6.2s-.32-4.22-.9-6.2l28.42-15.28C77.56 47.1 83.44 50 90 50c12.14 0 22-9.86 22-22S102.14 6 90 6s-22 9.86-22 22c0 1.98.28 3.9.78 5.72L40.14 49.1C36.12 44.76 30.38 42 24 42 11.86 42 2 51.86 2 64s9.86 22 22 22c6.38 0 12.12-2.76 16.14-7.12l28.64 15.38c-.5 1.84-.78 3.76-.78 5.74 0 12.14 9.86 22 22 22s22-9.86 22-22-9.86-22-22-22z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Share</span></span></button></div></div></div><div class="ExportCitation u-display-inline-block" id="export-citation"><div class="popover export-citation-popover" id="export-citation-popover"><div id="popover-trigger-export-citation-popover"><button class="button-link button-link-secondary button-link-icon-left button-link-has-colored-icon" aria-expanded="false" aria-haspopup="true" type="button"><svg focusable="false" viewBox="0 0 104 128" height="20" class="icon icon-cited-by-66"><path d="M2 58.78V106h44V64H12v-5.22C12 40.28 29.08 32 46 32V22C20.1 22 2 37.12 2 58.78zM102 32V22c-25.9 0-44 15.12-44 36.78V106h44V64H68v-5.22C68 40.28 85.08 32 102 32z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Cite</span></span></button></div></div></div></div></div><div class="ArticleIdentifierLinks u-margin-xs-bottom text-xs" id="article-identifier-links"><a class="anchor doi anchor-primary" href="https://doi.org/10.1016/j.jksuci.2018.09.002" target="_blank" rel="noreferrer noopener" aria-label="Persistent link using digital object identifier" title="Persistent link using digital object identifier"><span class="anchor-text-container"><span class="anchor-text">https://doi.org/10.1016/j.jksuci.2018.09.002</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor rights-and-content anchor-primary" href="https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&amp;contentID=S1319157818303379&amp;orderBeanReset=true" target="_blank" rel="noreferrer noopener"><span class="anchor-text-container"><span class="anchor-text">Get rights and content</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="LicenseInfo text-xs u-margin-xs-bottom"><div class="License"><span>Under a Creative Commons </span><a class="anchor anchor-primary" href="http://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" rel="noreferrer noopener"><span class="anchor-text-container"><span class="anchor-text">license</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="OpenAccessLabel"><span class="access-indicator"></span>open access</div></div><section class="ReferencedArticles"></section><div class="ReferencedArticles text-xs u-margin-s-ver" id="referred-to-by"><div class="referenced-article-title">Referred to by</div><div class="refers-to-content"><section class=""><div class="u-clamp-2-lines"><a class="anchor referenced-article-link text-s u-font-serif u-display-inline anchor-primary" href="/science/article/pii/S1319157820305401" id="title_S1319157820305401" title="Erratum regarding missing Declaration of Competing Interest statements in previously published articles"><span class="anchor-text-container"><span class="anchor-text"><span class="title-text">Erratum regarding missing Declaration of Competing Interest statements in previously published articles</span></span></span></a></div><div class="u-clr-grey6">Journal of King Saud University - Computer and Information Sciences, Volume 32, Issue 10, December 2020, Pages 1219</div><div class="u-margin-xs-bottom"></div><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1319157820305401/pdfft?md5=8dd1dbdc5432a1db099f5f877012df7e&amp;pid=1-s2.0-S1319157820305401-main.pdf" target="_blank" rel="nofollow" aria-describedby="title_S1319157820305401"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></section></div></div><div class="PageDivider"></div><div class="Abstracts u-font-serif" id="abstracts"><div class="abstract author" id="ab005" lang="en"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Abstract</h2><div id="as005"><div class="u-margin-s-bottom" id="sp0005"><span>Human <a href="/topics/agricultural-and-biological-sciences/face" title="Learn more about Face from ScienceDirect's AI-generated Topic Pages" class="topic-link">Face</a> expression Recognition is one of the most powerful and challenging tasks in social communication. Generally, face expressions are natural and direct means for </span><a href="/topics/biochemistry-genetics-and-molecular-biology/human" title="Learn more about human beings from ScienceDirect's AI-generated Topic Pages" class="topic-link">human beings</a> to communicate their emotions and intentions. Face expressions are the key characteristics of non-verbal communication. This paper describes the survey of Face Expression Recognition (FER) techniques which include the three major stages such as preprocessing, feature extraction and classification. This survey explains the various types of FER techniques with its major contributions. The performance of various FER techniques is compared based on the number of expressions recognized and complexity of algorithms. Databases like JAFFE, CK, and some other variety of facial expression databases are discussed in this survey. The study on classifiers gather from recent papers reveals a more powerful and reliable understanding of the peculiar characteristics of classifiers for research fellows.</div></div></div></div><div id="reading-assistant-main-body-section"><section class="sc-keTIit rhsah questions-and-answers" aria-labelledby="ai-components-toc-id"><div class="questions-and-answers-header"><div class="questions-and-answers-title u-display-flex"><h2 id="ai-components-toc-id">Questions answered in this article</h2><span class="ai-badges"><strong class="badge-info badge-md u-font-sans sc-egkSDF hndnke text-2xs u-margin-xs-right">Beta</strong><strong class="badge-info badge-md u-font-sans sc-gtLWhw dNlFzS text-2xs">Powered by GenAI</strong></span></div><i class="u-clr-grey7"><span>This is generative AI content and the quality may vary. </span><button class="button-link button-link-secondary u-text-italic u-clr-grey7 button-link-underline" type="button"><span class="button-link-text-container"><span class="button-link-text">Learn more</span></span></button><span>.</span></i></div><ol class="accordion-container u-font-sans " role="tablist"><li class="accordion-panel"><button id="0-accordion-tab-0" type="button" class="u-display-flex icon-left accordion-panel-title u-padding-xs-ver u-text-left" tabindex="0" role="tab" aria-expanded="false" aria-selected="true" aria-controls="0-accordion-panel-0"><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down accordion-icon"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg><div><span class="accordion-title"><span class="accordion-text">Which facial expressions are categorized in the classification process?</span></span></div></button></li><li class="accordion-panel"><button id="0-accordion-tab-1" type="button" class="u-display-flex icon-left accordion-panel-title u-padding-xs-ver u-text-left" tabindex="0" role="tab" aria-expanded="false" aria-selected="true" aria-controls="0-accordion-panel-1"><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down accordion-icon"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg><div><span class="accordion-title"><span class="accordion-text">What does appearance based feature extraction include?</span></span></div></button></li><li class="accordion-panel"><button id="0-accordion-tab-2" type="button" class="u-display-flex icon-left accordion-panel-title u-padding-xs-ver u-text-left" tabindex="0" role="tab" aria-expanded="false" aria-selected="true" aria-controls="0-accordion-panel-2"><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down accordion-icon"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg><div><span class="accordion-title"><span class="accordion-text">How does the Gabor filter extract features from face images?</span></span></div></button></li><li class="accordion-panel"><button id="0-accordion-tab-3" type="button" class="u-display-flex icon-left accordion-panel-title u-padding-xs-ver u-text-left" tabindex="0" role="tab" aria-expanded="false" aria-selected="true" aria-controls="0-accordion-panel-3"><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down accordion-icon"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg><div><span class="accordion-title"><span class="accordion-text">What are the classifiers used in FER techniques?</span></span></div></button></li><li class="accordion-panel"><button id="0-accordion-tab-4" type="button" class="u-display-flex icon-left accordion-panel-title u-padding-xs-ver u-text-left" tabindex="0" role="tab" aria-expanded="false" aria-selected="true" aria-controls="0-accordion-panel-4"><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down accordion-icon"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg><div><span class="accordion-title"><span class="accordion-text">What is the purpose of classification in FER?</span></span></div></button></li></ol></section></div><ul id="issue-navigation" class="issue-navigation u-margin-s-bottom u-bg-grey1"><li class="previous move-left u-padding-s-ver u-padding-s-left"><button class="button-alternative button-alternative-tertiary u-display-flex button-alternative-icon-left" disabled="" type="button"><svg focusable="false" viewBox="0 0 54 128" height="20" class="icon icon-navigate-left"><path d="M1 61l45-45 7 7-38 38 38 38-7 7z"></path></svg><span class="button-alternative-text-container"><span class="button-alternative-text">Previous<!-- --> <span class="extra-detail-1">article</span><span class="extra-detail-2"> in issue</span></span></span></button></li><li class="next move-right u-padding-s-ver u-padding-s-right"><a class="button-alternative button-alternative-tertiary u-display-flex button-alternative-icon-right" href="/science/article/pii/S1319157818312631"><span class="button-alternative-text-container"><span class="button-alternative-text">Next <span class="extra-detail-1">article</span><span class="extra-detail-2"> in issue</span></span></span><svg focusable="false" viewBox="0 0 54 128" height="20" class="icon icon-navigate-right"><path d="M1 99l38-38L1 23l7-7 45 45-45 45z"></path></svg></a></li></ul><div class="Keywords u-font-serif"><div id="kg005" class="keywords-section"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Keywords</h2><div id="k0005" class="keyword"><span>Classification</span></div><div id="k0010" class="keyword"><span>Face Expression Recognition (FER)</span></div><div id="k0015" class="keyword"><span>Feature extraction</span></div><div id="k0020" class="keyword"><span>Preprocessing</span></div></div></div><div class="Body u-font-serif" id="body"><div><section id="s0005"><h2 id="st015" class="u-h4 u-margin-l-top u-margin-xs-bottom">1. Introduction</h2><div class="u-margin-s-bottom" id="p0005">Human facial expressions are extremely essential in social communication. Normally communication involves both verbal and nonverbal. Non-verbal communications are expressed through facial expressions. Face expressions are the delicate signals of the larger communication. Non-verbal communication means communication between human and animals through eye contact, gesture, facial expressions, body language, and <a href="/topics/biochemistry-genetics-and-molecular-biology/paralanguage" title="Learn more about paralanguage from ScienceDirect's AI-generated Topic Pages" class="topic-link">paralanguage</a>.</div><div class="u-margin-s-bottom" id="p0010">Eye contact is the important phase of communication which provides the mixture of ideas. Eye contact controls the contribution, discussions and creates a link with others. Face expressions include the smile, sad, anger, disgust, surprise, and fear. A smile on human face shows their happiness and it expresses eye with a curved shape. The sad expression is the feeling of looseness which is normally expressed as rising skewed eyebrows and frown. The anger on human face is related to unpleasant and irritating conditions. The expression of anger is expressed with squeezed eyebrows, slender and stretched eyelids. The disgust expressions are expressed with pull down eyebrows and creased nose. The surprise or shock expression is expressed when some unpredicted happens. This is expressed with eye-widening and <a href="/topics/agricultural-and-biological-sciences/mouth" title="Learn more about mouth from ScienceDirect's AI-generated Topic Pages" class="topic-link">mouth</a> gaping and this expression is an easily identified one. The expression of fear is related with surprise expression which is expressed as growing skewed eyebrows.</div><div class="u-margin-s-bottom" id="p0015">FER has the important stage is feature extraction and classification. Feature extraction includes two types and they are geometric based and appearance based. The classification is also one of the important processes in which the above-mentioned expressions such as smile, sad, anger, disgust, surprise, and fear are categorized. The geometrically based feature extraction comprises eye, mouth, nose, eyebrow, other facial components and the appearance based feature extraction comprises the exact section of the face (<a class="anchor anchor-primary" href="#b0270" name="bb0270" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0270"><span class="anchor-text-container"><span class="anchor-text">Zhao and Zhang, 2016</span></span></a>).</div><div class="u-margin-s-bottom" id="p0020"><span>Generally, the face offers three different types of signals such as static, slow and rapid signals. The static signals are skin color which includes the several lasting aspects of face <a href="/topics/biochemistry-genetics-and-molecular-biology/skin-pigmentation" title="Learn more about skin pigmentation from ScienceDirect's AI-generated Topic Pages" class="topic-link">skin pigmentation</a><span>, greasy deposits, face shapes, the constitution of bones, cartilage and shape, location and size of <a href="/topics/computer-science/facial-feature" title="Learn more about facial features from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial features</a> such as brows, eyes, nose, mouth. The slow signals are permanent wrinkles which include the changes in </span></span><a href="/topics/computer-science/facial-appearance" title="Learn more about facial appearance from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial appearance</a> such as muscle tone and skin texture changes that happen slowly with time.</div><div class="u-margin-s-bottom" id="p0025"><span><span>The rapid signals are raising the eyebrows which include the face muscles movement, impermanent face appearance changes, impermanent wrinkles and changes in the location and shape of facial features. These <a href="/topics/biochemistry-genetics-and-molecular-biology/flushing" title="Learn more about flashes from ScienceDirect's AI-generated Topic Pages" class="topic-link">flashes</a> on the face remain for a few seconds. These three signals are altered with individual option while it is very hard to alter static and slow signals. Also, the face is a multi-message system and it is not only a multi-signal system. Messages are transmitted through a face which includes emotion, feel position, age, quality, intelligence, </span><a href="/topics/computer-science/attractiveness" title="Learn more about attractiveness from ScienceDirect's AI-generated Topic Pages" class="topic-link">attractiveness</a> and almost certainly other substances as well (</span><a class="anchor anchor-primary" href="#b0060" name="bb0060" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0060"><span class="anchor-text-container"><span class="anchor-text">Ekman and Friesen, 2003</span></span></a>).</div><div class="u-margin-s-bottom" id="p0030">This paper mainly focuses on various FER techniques with three major steps respectively preprocessing, feature extraction and classification. Also, this paper shows the advantages of different FER techniques and the performance analysis of different FER techniques. In this paper, only the image based FER techniques are chosen for the literature review and the video based FER techniques are not chosen. Mostly FER systems meet the problems of variation in illumination, pose variation, lighting variations, skin tone variations. Also this paper gives an essential research idea for future FER research.</div><div class="u-margin-s-bottom" id="p0035">Rest of the paper is structured as follows. <a class="anchor anchor-primary" href="#s0010" name="bs0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0010"><span class="anchor-text-container"><span class="anchor-text">Section 2</span></span></a> elaborates the detailed description of face expression recognition system. <a class="anchor anchor-primary" href="#s0035" name="bs0035" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0035"><span class="anchor-text-container"><span class="anchor-text">Section 3</span></span></a> evaluates the performance of FER techniques and through different table and charts. <a class="anchor anchor-primary" href="#s0040" name="bs0040" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0040"><span class="anchor-text-container"><span class="anchor-text">Section 4</span></span></a> provides suggestions along with the conclusion of this survey.</div></section><section id="s0010"><h2 id="st020" class="u-h4 u-margin-l-top u-margin-xs-bottom">2. Face expression recognition system</h2><div class="u-margin-s-bottom"><div id="p0040">The overview of the FER system is illustrated in <a class="anchor anchor-primary" href="#f0005" name="bf0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0005"><span class="anchor-text-container"><span class="anchor-text">Fig. 1</span></span></a>. The FER system includes the major stages such as face image preprocessing, feature extraction and classification.</div><figure class="figure text-xs" id="f0005"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr1.jpg" height="246" alt="" aria-describedby="cn0005"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr1_lrg.jpg" target="_blank" download="" title="Download high-res image (185KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (185KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr1.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cn0005"><p id="sp0010"><span class="label">Fig. 1</span>. Architecture of <a href="/topics/agricultural-and-biological-sciences/face" title="Learn more about face from ScienceDirect's AI-generated Topic Pages" class="topic-link">face</a> expression recognition system.</p></span></span></figure></div><section id="s0015"><h3 id="st025" class="u-h4 u-margin-m-top u-margin-xs-bottom">2.1. Preprocessing</h3><div class="u-margin-s-bottom" id="p0045">Preprocessing is a process which can be used to improve the performance of the FER system and it can be carried out before feature extraction process (<a class="anchor anchor-primary" href="#b0175" name="bb0175" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0175"><span class="anchor-text-container"><span class="anchor-text">Poursaberi et al., 2012</span></span></a>). Image preprocessing includes different types of processes such as image clarity and scaling, contrast adjustment, and additional enhancement processes (<a class="anchor anchor-primary" href="#b0015" name="bb0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0015"><span class="anchor-text-container"><span class="anchor-text">Bashyal et al., 2008</span></span></a>) to improve the expression frames (<a class="anchor anchor-primary" href="#b0220" name="bb0220" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0220"><span class="anchor-text-container"><span class="anchor-text">Taylor et al., 2014</span></span></a>).</div><div class="u-margin-s-bottom" id="p0050">The cropping and scaling processes were performed on the face image in which the nose of the face is taken as midpoint and the other important facial components are included physically (<a class="anchor anchor-primary" href="#b0255" name="bb0255" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0255"><span class="anchor-text-container"><span class="anchor-text">Zhang et al., 2011</span></span></a>). Bessel down sampling is used for face image size reduction but it protects the aspects and also the perceptual worth of the original image (<a class="anchor anchor-primary" href="#b0170" name="bb0170" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0170"><span class="anchor-text-container"><span class="anchor-text">Owusu et al., 2014</span></span></a><span>). The <a href="/topics/engineering/gaussian-filter" title="Learn more about Gaussian filter from ScienceDirect's AI-generated Topic Pages" class="topic-link">Gaussian filter</a> is used for resizing the input images which provides the smoothness to the images (</span><a class="anchor anchor-primary" href="#b0020" name="bb0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0020"><span class="anchor-text-container"><span class="anchor-text">Biswas, 2015</span></span></a>).</div><div class="u-margin-s-bottom" id="p0055">Normalization is the preprocessing method which can be designed for reduction of illumination and variations of the face images (<a class="anchor anchor-primary" href="#b0120" name="bb0120" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0120"><span class="anchor-text-container"><span class="anchor-text">Ji and Idrissi, 2012</span></span></a><span>) with the <a href="/topics/engineering/median-filter" title="Learn more about median filter from ScienceDirect's AI-generated Topic Pages" class="topic-link">median filter</a><span> and to achieve an improved face image. The normalization method also used for the extraction of <a href="/topics/biochemistry-genetics-and-molecular-biology/eye-position" title="Learn more about eye positions from ScienceDirect's AI-generated Topic Pages" class="topic-link">eye positions</a> which make more robust to personality differences for the FER system and it provides more clarity to the input images. Localization is a preprocessing method and it uses the Viola-Jones algorithm (</span></span><a class="anchor anchor-primary" href="#b0165" name="bb0165" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0165"><span class="anchor-text-container"><span class="anchor-text">Noh et al., 2007</span></span></a>, <a class="anchor anchor-primary" href="#b0055" name="bb0055" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0055"><span class="anchor-text-container"><span class="anchor-text">Demir, 2014</span></span></a>, <a class="anchor anchor-primary" href="#b0260" name="bb0260" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0260"><span class="anchor-text-container"><span class="anchor-text">Zhang et al., 2014</span></span></a>, <a class="anchor anchor-primary" href="#b0040" name="bb0040" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0040"><span class="anchor-text-container"><span class="anchor-text">Cossetin et al., 2016</span></span></a>, <a class="anchor anchor-primary" href="#b0195" name="bb0195" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0195"><span class="anchor-text-container"><span class="anchor-text">Salmam et al., 2016</span></span></a><span>) to detect the <a href="/topics/computer-science/facial-image" title="Learn more about facial images from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial images</a><span> from the input image. Detection of size and location of the face images using <a href="/topics/engineering/adaboost" title="Learn more about Adaboost from ScienceDirect's AI-generated Topic Pages" class="topic-link">Adaboost</a> learning algorithm and haar like features (</span></span><a class="anchor anchor-primary" href="#b0090" name="bb0090" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0090"><span class="anchor-text-container"><span class="anchor-text">Happy et al., 2015</span></span></a>, <a class="anchor anchor-primary" href="#b0145" name="bb0145" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0145"><span class="anchor-text-container"><span class="anchor-text">Mahersia and Hamrouni, 2015</span></span></a>). The localization is mainly used for spotting the size and locations of the face from the image.</div><div class="u-margin-s-bottom" id="p0060"><span>Face alignment is also the preprocessing method which can be performed by using the SIFT (Scale Invariant Feature Transform) flow algorithm. For this, first calculate <a href="/topics/computer-science/reference-image" title="Learn more about reference image from ScienceDirect's AI-generated Topic Pages" class="topic-link">reference image</a> for each face expressions. After that all the images are aligned through related reference images (</span><a class="anchor anchor-primary" href="#b0050" name="bb0050" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0050"><span class="anchor-text-container"><span class="anchor-text">Dahmane and Meunier, 2014</span></span></a><span>). ROI (Region of Interest) segmentation is one of the important type of preprocessing method which includes three important functions such as regulating the face dimensions by dividing the color components and of face image, eye or forehead and <a href="/topics/agricultural-and-biological-sciences/mouth" title="Learn more about mouth from ScienceDirect's AI-generated Topic Pages" class="topic-link">mouth</a> regions segmentation (</span><a class="anchor anchor-primary" href="#b0100" name="bb0100" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0100"><span class="anchor-text-container"><span class="anchor-text">Hernandez-matamoros et al., 2015</span></span></a>). In FER, ROI segmentation is most popular because for convenient segmentation of face organs from the face images.</div><div class="u-margin-s-bottom" id="p0065"><span>The <a href="/topics/computer-science/histogram-equalization" title="Learn more about histogram equalization from ScienceDirect's AI-generated Topic Pages" class="topic-link">histogram equalization</a><span> method is used to conquer the <a href="/topics/engineering/illumination-variation" title="Learn more about illumination variations from ScienceDirect's AI-generated Topic Pages" class="topic-link">illumination variations</a> (</span></span><a class="anchor anchor-primary" href="#b0055" name="bb0055" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0055"><span class="anchor-text-container"><span class="anchor-text">Demir, 2014</span></span></a>, <a class="anchor anchor-primary" href="#b0090" name="bb0090" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0090"><span class="anchor-text-container"><span class="anchor-text">Happy et al., 2015</span></span></a>, <a class="anchor anchor-primary" href="#b0040" name="bb0040" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0040"><span class="anchor-text-container"><span class="anchor-text">Cossetin et al., 2016</span></span></a>). This method is mainly used for enhancing the contrast of the face images and for exact lighting also used to improve the distinction between the intensities.</div><div class="u-margin-s-bottom" id="p0070"><span>In FER, more preprocessing methods are used but the ROI <a href="/topics/computer-science/segmentation-process" title="Learn more about segmentation process from ScienceDirect's AI-generated Topic Pages" class="topic-link">segmentation process</a> is more suitable because it detects the face organs accurately which organs are is mainly used for expression recognition. Next the histogram equalization is also another one important </span><a href="/topics/computer-science/preprocessing-technique" title="Learn more about preprocessing technique from ScienceDirect's AI-generated Topic Pages" class="topic-link">preprocessing technique</a> for FER because it improves the image distinction.</div></section><section id="s0020"><h3 id="st030" class="u-h4 u-margin-m-top u-margin-xs-bottom">2.2. Feature extraction</h3><div class="u-margin-s-bottom" id="p0075">Feature extraction process is the next stage of FER system. Feature extraction is finding and depicting of positive features of concern within an image for further processing. In <a href="/topics/engineering/image-processing" title="Learn more about image processing from ScienceDirect's AI-generated Topic Pages" class="topic-link">image processing</a><span> <a href="/topics/engineering/computervision" title="Learn more about computer vision from ScienceDirect's AI-generated Topic Pages" class="topic-link">computer vision</a> feature extraction is a significant stage, whereas it spots the move from graphic to implicit data depiction. Then these data depiction can be used as an input to the classification. The feature extraction methods are categorized into five types such as texture feature-based method, edge based method, global and local feature-based method, geometric feature-based method and patch-based method.</span></div><div class="u-margin-s-bottom" id="p0080"><span>The descriptors which extract the features based on the texture feature-based methods are described as follows. <a href="/topics/engineering/gabor-filter" title="Learn more about Gabor filter from ScienceDirect's AI-generated Topic Pages" class="topic-link">Gabor filter</a><span><span> is a <a href="/topics/engineering/texture-descriptor" title="Learn more about texture descriptor from ScienceDirect's AI-generated Topic Pages" class="topic-link">texture descriptor</a> for feature extraction and it includes the magnitude and </span><a href="/topics/computer-science/phase-information" title="Learn more about phase information from ScienceDirect's AI-generated Topic Pages" class="topic-link">phase information</a><span>. The <a href="/topics/earth-and-planetary-sciences/gabor-filter" title="Learn more about Gabor filter from ScienceDirect's AI-generated Topic Pages" class="topic-link">Gabor filter</a> with the magnitude feature confines the information about the organization of the face image. The phase feature precincts the information about the complete description of the magnitude features (</span></span></span><a class="anchor anchor-primary" href="#b0015" name="bb0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0015"><span class="anchor-text-container"><span class="anchor-text">Bashyal and Venayagamoorthy, 2008</span></span></a>, <a class="anchor anchor-primary" href="#b0170" name="bb0170" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0170"><span class="anchor-text-container"><span class="anchor-text">Owusu et al., 2014</span></span></a>, <a class="anchor anchor-primary" href="#b0260" name="bb0260" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0260"><span class="anchor-text-container"><span class="anchor-text">Zhang et al., 2014</span></span></a>, <a class="anchor anchor-primary" href="#b0100" name="bb0100" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0100"><span class="anchor-text-container"><span class="anchor-text">Hernandez-matamoros et al., 2015</span></span></a>, <a class="anchor anchor-primary" href="#b0095" name="bb0095" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0095"><span class="anchor-text-container"><span class="anchor-text">Hegde et al., 2016</span></span></a><span>). <a href="/topics/engineering/local-binary-pattern" title="Learn more about Local Binary Pattern from ScienceDirect's AI-generated Topic Pages" class="topic-link">Local Binary Pattern</a><span><span> (LBP) is also a <a href="/topics/computer-science/texture-descriptor" title="Learn more about texture descriptor from ScienceDirect's AI-generated Topic Pages" class="topic-link">texture descriptor</a> and it can be used for feature extraction. Generally LBP features are produced with the </span><a href="/topics/engineering/binary-code" title="Learn more about binary code from ScienceDirect's AI-generated Topic Pages" class="topic-link">binary code</a> and it can be obtained by using thresholding between the center pixel and its locality pixels (</span></span><a class="anchor anchor-primary" href="#b0090" name="bb0090" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0090"><span class="anchor-text-container"><span class="anchor-text">Happy et al., 2015</span></span></a>, <a class="anchor anchor-primary" href="#b0040" name="bb0040" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0040"><span class="anchor-text-container"><span class="anchor-text">Cossetin et al., 2016</span></span></a>). Also LBP with Three Orthogonal Planes (TOP) features are extracted for multi resolution approaches and (<a class="anchor anchor-primary" href="#b0265" name="bb0265" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0265"><span class="anchor-text-container"><span class="anchor-text">Zhao and Pietikäinen, 2009</span></span></a>). It is used for extracting non dynamic appearance based on features from the static face images (<a class="anchor anchor-primary" href="#b0120" name="bb0120" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0120"><span class="anchor-text-container"><span class="anchor-text">Ji and Idrissi, 2012</span></span></a><span><span>). The facial texture features are extracted using the <a href="/topics/biochemistry-genetics-and-molecular-biology/gaussian-distribution" title="Learn more about Gaussian from ScienceDirect's AI-generated Topic Pages" class="topic-link">Gaussian</a> Laguerre (GL) function which grants a steering pyramidal structure which extracts the texture features and the facial related occurrence information. Comparing to </span><a href="/topics/computer-science/gabor-function" title="Learn more about Gabor function from ScienceDirect's AI-generated Topic Pages" class="topic-link">Gabor function</a> GL uses the single filter instead of multiple filters (</span><a class="anchor anchor-primary" href="#b0175" name="bb0175" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0175"><span class="anchor-text-container"><span class="anchor-text">Poursaberi et al., 2012</span></span></a>). Moreover another descriptor which is used namely Vertical Time Backward (VTB) which also extracts the texture features of face images. Moments descriptor extracts the shape related features of significant facial components. Both VTB and moments descriptors are effective on spatiotemporal planes (<a class="anchor anchor-primary" href="#b0120" name="bb0120" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0120"><span class="anchor-text-container"><span class="anchor-text">Ji and Idrissi, 2012</span></span></a><span>). Weber <a href="/topics/engineering/local-descriptor" title="Learn more about Local Descriptor from ScienceDirect's AI-generated Topic Pages" class="topic-link">Local Descriptor</a> (WLD) is a feature extraction technique that extracts the high discriminant texture features from the segmented face images (</span><a class="anchor anchor-primary" href="#b0040" name="bb0040" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0040"><span class="anchor-text-container"><span class="anchor-text">Cossetin et al., 2016</span></span></a>). Feature extraction is performed with three stages using Supervised Descent Method (SDM). At first, the facial main positions are extracted. Next the related positions are selected. Finally it estimates the distance between the various components of the face (<a class="anchor anchor-primary" href="#b0195" name="bb0195" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0195"><span class="anchor-text-container"><span class="anchor-text">Salmam et al., 2016</span></span></a>). Weighted Projection based LBP (WPLBP) is also a feature extraction but based on the instructive regions which extracts the LBP features. After that based on the significance of the instructive regions these features are weighted (<a class="anchor anchor-primary" href="#b0125" name="bb0125" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0125"><span class="anchor-text-container"><span class="anchor-text">Kumar et al., 2016</span></span></a><span>). Discrete Contourlet Transform (DCT) extracts the texture features which can be performed by decomposition with two key stages. The stages are <a href="/topics/computer-science/laplacian-pyramid" title="Learn more about Laplacian Pyramid from ScienceDirect's AI-generated Topic Pages" class="topic-link">Laplacian Pyramid</a><span> (LP) and Directional <a href="/topics/engineering/filter-banks" title="Learn more about Filter Bank from ScienceDirect's AI-generated Topic Pages" class="topic-link">Filter Bank</a> (DFB) which is used in the transformed domain. In LP stage, partitions the image into low pass, band pass and confines the discontinuities position. The DFB stage processes the band pass and forms the linear composition by associating the discontinuities position (</span></span><a class="anchor anchor-primary" href="#b0020" name="bb0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0020"><span class="anchor-text-container"><span class="anchor-text">Biswas, 2015</span></span></a>).</div><div class="u-margin-s-bottom" id="p0085">The descriptors which extract the features based on the edge based methods are described as follows. Line Edge Map (LEM) descriptor is a facial expression descriptor which improves the geometrical structural features by using the dynamic two strip algorithm (Dyn2S) (<a class="anchor anchor-primary" href="#b0075" name="bb0075" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0075"><span class="anchor-text-container"><span class="anchor-text">Gao et al., 2003</span></span></a><span>). Based on the <a href="/topics/computer-science/motion-analysis" title="Learn more about motion analysis from ScienceDirect's AI-generated Topic Pages" class="topic-link">motion analysis</a><span> two types of <a href="/topics/computer-science/facial-feature" title="Learn more about facial features from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial features</a> are extracted such as non discriminative and discriminative facial features (</span></span><a class="anchor anchor-primary" href="#b0165" name="bb0165" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0165"><span class="anchor-text-container"><span class="anchor-text">Noh et al., 2007</span></span></a>). Graphics-processing unit based Active Shape Model (GASM) is the feature extraction method which can be performed with edge detection, enhancement, tone mapping and local appearance model matching. After that the image ratio features are extracted from the expressed face images (<a class="anchor anchor-primary" href="#b0215" name="bb0215" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0215"><span class="anchor-text-container"><span class="anchor-text">Song et al., 2010</span></span></a><span><span>). <a href="/topics/computer-science/histogram-of-oriented-gradient" title="Learn more about Histogram of Oriented Gradients from ScienceDirect's AI-generated Topic Pages" class="topic-link">Histogram of Oriented Gradients</a> (HOG) is a window supported </span><a href="/topics/computer-science/feature-descriptor" title="Learn more about feature descriptor from ScienceDirect's AI-generated Topic Pages" class="topic-link">feature descriptor</a><span> which uses the gradient filter. The extracted features are based on the <a href="/topics/computer-science/edge-information" title="Learn more about edge information from ScienceDirect's AI-generated Topic Pages" class="topic-link">edge information</a> of the registered face images. It extracts the visual features, for example a smile expression means curvature shaped eyes (</span></span><a class="anchor anchor-primary" href="#b0050" name="bb0050" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0050"><span class="anchor-text-container"><span class="anchor-text">Dahmane and Meunier, 2014</span></span></a>).</div><div class="u-margin-s-bottom" id="p0090"><span><span>The descriptors which extract the features based on the global and local feature-based methods are described as follows. <a href="/topics/engineering/principal-components" title="Learn more about Principal Component from ScienceDirect's AI-generated Topic Pages" class="topic-link">Principal Component</a> Analysis (PCA) method is used for feature extraction. It extracts the global and low dimensional features. </span><a href="/topics/engineering/independent-component-analysis" title="Learn more about Independent Component Analysis from ScienceDirect's AI-generated Topic Pages" class="topic-link">Independent Component Analysis</a><span> (ICA) is also a feature extraction method which extracts the local features using the <a href="/topics/engineering/multichannel" title="Learn more about multichannel from ScienceDirect's AI-generated Topic Pages" class="topic-link">multichannel</a> observations (</span></span><a class="anchor anchor-primary" href="#b0220" name="bb0220" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0220"><span class="anchor-text-container"><span class="anchor-text">Taylor et al., 2014</span></span></a><span>). Stepwise <a href="/topics/computer-science/linear-discriminant-analysis" title="Learn more about Linear Discriminant Analysis from ScienceDirect's AI-generated Topic Pages" class="topic-link">Linear Discriminant Analysis</a> (SWLDA) is the feature extraction technique which extracts the localized features with backward and forward regression models. Depends on the class labels the F-test values are estimated for both regression models (</span><a class="anchor anchor-primary" href="#b0210" name="bb0210" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0210"><span class="anchor-text-container"><span class="anchor-text">Siddiqi et al., 2015</span></span></a>).</div><div class="u-margin-s-bottom" id="p0095">The descriptors which extract the features based on the geometric feature-based methods are described as follows. Local Curvelet Transform (LCT) is a feature descriptor which extracts the geometric features which depends on wrapping mechanism. The extracted geometric features are mean, entropy and standard deviation (<a class="anchor anchor-primary" href="#b0055" name="bb0055" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0055"><span class="anchor-text-container"><span class="anchor-text">Demir, 2014</span></span></a><span><span>). Addition to these geometrical features energy, <a href="/topics/earth-and-planetary-sciences/kurtosis" title="Learn more about kurtosis from ScienceDirect's AI-generated Topic Pages" class="topic-link">kurtosis</a> are extracted by using three stage steerable </span><a href="/topics/computer-science/pyramid-representation" title="Learn more about pyramid representation from ScienceDirect's AI-generated Topic Pages" class="topic-link">pyramid representation</a> (</span><a class="anchor anchor-primary" href="#b0145" name="bb0145" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0145"><span class="anchor-text-container"><span class="anchor-text">Mahersia and Hamrouni, 2015</span></span></a>).</div><div class="u-margin-s-bottom" id="p0100"><span>The descriptors which extract the features based on patch-based methods are described as follows. Facial movement features are extracted as patches depending upon the distance characteristics. These are performed by using two processes such as extracting the patches and patch matching. The patch matching is performed by <a href="/topics/biochemistry-genetics-and-molecular-biology/translating-language" title="Learn more about translating from ScienceDirect's AI-generated Topic Pages" class="topic-link">translating</a> extracted patches into distance characteristics (</span><a class="anchor anchor-primary" href="#b0255" name="bb0255" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0255"><span class="anchor-text-container"><span class="anchor-text">Zhang et al., 2011</span></span></a>).</div><div class="u-margin-s-bottom" id="p0105"><span>The <a href="/topics/computer-science/texture-feature" title="Learn more about texture feature from ScienceDirect's AI-generated Topic Pages" class="topic-link">texture feature</a> based descriptors are more useful feature extraction method than the others because it extracts the texture features like related to the appearance which provides the important feature vectors for FER. Also Local Directional Number (LDN) pattern (</span><a class="anchor anchor-primary" href="#b0180" name="bb0180" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0180"><span class="anchor-text-container"><span class="anchor-text">Rahul and Cherian, 2016</span></span></a>), Local Directional Ternary Pattern (LDTP) (<a class="anchor anchor-primary" href="#b0190" name="bb0190" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0190"><span class="anchor-text-container"><span class="anchor-text">Ryu et al., 2017</span></span></a>), KL-transform Extended LBP (K-ELBP) (<a class="anchor anchor-primary" href="#b0080" name="bb0080" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0080"><span class="anchor-text-container"><span class="anchor-text">Guo et al., 2016</span></span></a><span>) and Discrete <a href="/topics/computer-science/wavelet-transforms" title="Learn more about Wavelet Transform from ScienceDirect's AI-generated Topic Pages" class="topic-link">Wavelet Transform</a> (DWT) (</span><a class="anchor anchor-primary" href="#b0160" name="bb0160" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0160"><span class="anchor-text-container"><span class="anchor-text">Nigam et al., 2018</span></span></a>) texture feature based descriptors are used as feature descriptors in recent years FER.</div><div class="u-margin-s-bottom" id="p0110">Several extracted features have high <a href="/topics/computer-science/dimensional-vector" title="Learn more about dimensional vectors from ScienceDirect's AI-generated Topic Pages" class="topic-link">dimensional vectors</a><span><span>. Generally these feature vectors are reduced by using various dimensionality <a href="/topics/computer-science/reduction-algorithm" title="Learn more about reduction algorithms from ScienceDirect's AI-generated Topic Pages" class="topic-link">reduction algorithms</a><span> such as PCA, Linear <a href="/topics/earth-and-planetary-sciences/discriminant-analysis" title="Learn more about Discriminant Analysis from ScienceDirect's AI-generated Topic Pages" class="topic-link">Discriminant Analysis</a>, Whitened </span></span><a href="/topics/computer-science/principle-component-analysis" title="Learn more about Principle Component Analysis from ScienceDirect's AI-generated Topic Pages" class="topic-link">Principle Component Analysis</a> and the important features are also selected with different algorithms such as Adaboost and similarity scores.</span></div></section><section id="s0025"><h3 id="st035" class="u-h4 u-margin-m-top u-margin-xs-bottom">2.3. Classification</h3><div class="u-margin-s-bottom" id="p0115">Classification is the final stage of FER system in which the classifier categorizes the expression such as smile, sad, surprise, anger, fear, disgust and neutral.</div><div class="u-margin-s-bottom" id="p0120"><span>The <a href="/topics/computer-science/directed-line-segment" title="Learn more about directed Line segment from ScienceDirect's AI-generated Topic Pages" class="topic-link">directed Line segment</a><span> <a href="/topics/computer-science/hausdorff-distance" title="Learn more about Hausdorff Distance from ScienceDirect's AI-generated Topic Pages" class="topic-link">Hausdorff Distance</a> (dLHD) method is used for recognition of expressions (</span></span><a class="anchor anchor-primary" href="#b0075" name="bb0075" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0075"><span class="anchor-text-container"><span class="anchor-text">Gao et al., 2003</span></span></a><span>). <a href="/topics/computer-science/euclidean-distance" title="Learn more about Euclidean distance from ScienceDirect's AI-generated Topic Pages" class="topic-link">Euclidean distance</a><span> metric is also used for classification purpose which uses the normalized score and similarity <a href="/topics/engineering/score-matrix" title="Learn more about score matrix from ScienceDirect's AI-generated Topic Pages" class="topic-link">score matrix</a><span> for estimating <a href="/topics/engineering/euclidean-distance" title="Learn more about Euclidean distance from ScienceDirect's AI-generated Topic Pages" class="topic-link">Euclidean distance</a> (</span></span></span><a class="anchor anchor-primary" href="#b0095" name="bb0095" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0095"><span class="anchor-text-container"><span class="anchor-text">Hegde et al., 2016</span></span></a>). Minimum Distance Classifier (MDC) is also one of the distance based classifier used for classification which estimates the distance between the feature vectors every sub image (<a class="anchor anchor-primary" href="#b0110" name="bb0110" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0110"><span class="anchor-text-container"><span class="anchor-text">Islam et al., 2018</span></span></a>). The KNN (k – Nearest Neighbors) algorithm is a classification method in which the relationship among the assessment models and the other models are estimated during the training stage (<a class="anchor anchor-primary" href="#b0175" name="bb0175" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0175"><span class="anchor-text-container"><span class="anchor-text">Poursaberi et al., 2012</span></span></a>).</div><div class="u-margin-s-bottom" id="p0125"><span><span><a href="/topics/computer-science/support-vector-machine" title="Learn more about Support Vector Machine from ScienceDirect's AI-generated Topic Pages" class="topic-link">Support Vector Machine</a> (SVM) is one of the </span><a href="/topics/computer-science/classification-technique" title="Learn more about classification techniques from ScienceDirect's AI-generated Topic Pages" class="topic-link">classification techniques</a> in which two types of approaches are involved. They are one against one and one against all approaches. One against all classification means it constructs one sample for each class (</span><a class="anchor anchor-primary" href="#b0265" name="bb0265" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0265"><span class="anchor-text-container"><span class="anchor-text">Zhao and Pietikäinen, 2009</span></span></a>, <a class="anchor anchor-primary" href="#b0255" name="bb0255" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0255"><span class="anchor-text-container"><span class="anchor-text">Zhang et al., 2011</span></span></a>, <a class="anchor anchor-primary" href="#b0260" name="bb0260" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0260"><span class="anchor-text-container"><span class="anchor-text">Zhang et al., 2014</span></span></a>, <a class="anchor anchor-primary" href="#b0020" name="bb0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0020"><span class="anchor-text-container"><span class="anchor-text">Biswas, 2015</span></span></a>). One against one classification means it constructs one class for each pair of classes (<a class="anchor anchor-primary" href="#b0090" name="bb0090" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0090"><span class="anchor-text-container"><span class="anchor-text">Happy et al., 2015</span></span></a>, <a class="anchor anchor-primary" href="#b0125" name="bb0125" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0125"><span class="anchor-text-container"><span class="anchor-text">Kumar et al., 2016</span></span></a>, <a class="anchor anchor-primary" href="#b0095" name="bb0095" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0095"><span class="anchor-text-container"><span class="anchor-text">Hegde et al., 2016</span></span></a>) and SVM is one of the strongest classification methods for advanced dimensionality troubles (<a class="anchor anchor-primary" href="#b0050" name="bb0050" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0050"><span class="anchor-text-container"><span class="anchor-text">Dahmane and Meunier, 2014</span></span></a><span>). SVM is the supervised <a href="/topics/engineering/machine-learning-technique" title="Learn more about machine learning technique from ScienceDirect's AI-generated Topic Pages" class="topic-link">machine learning technique</a> and it uses four types of kernels for its better performance (</span><a class="anchor anchor-primary" href="#b0100" name="bb0100" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0100"><span class="anchor-text-container"><span class="anchor-text">Hernandez-matamoros et al., 2015</span></span></a><span><span>). They are linear, polynomial, <a href="/topics/engineering/radial-basis-function" title="Learn more about Radial Basis Function from ScienceDirect's AI-generated Topic Pages" class="topic-link">Radial Basis Function</a> (RBF) and sigmoid. The linear kernel maps the </span><a href="/topics/computer-science/high-dimensional-data" title="Learn more about high dimensional data from ScienceDirect's AI-generated Topic Pages" class="topic-link">high dimensional data</a> and it is linearly separable (</span><a class="anchor anchor-primary" href="#b0260" name="bb0260" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0260"><span class="anchor-text-container"><span class="anchor-text">Zhang et al., 2014</span></span></a>, <a class="anchor anchor-primary" href="#b0125" name="bb0125" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0125"><span class="anchor-text-container"><span class="anchor-text">Kumar et al., 2016</span></span></a><span>). The RBF kernel uses the function that maps the <a href="/topics/computer-science/single-feature" title="Learn more about single feature from ScienceDirect's AI-generated Topic Pages" class="topic-link">single feature</a> into the high dimensional data (</span><a class="anchor anchor-primary" href="#b0215" name="bb0215" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0215"><span class="anchor-text-container"><span class="anchor-text">Song et al., 2010</span></span></a>, <a class="anchor anchor-primary" href="#b0230" name="bb0230" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0230"><span class="anchor-text-container"><span class="anchor-text">Wang et al., 2010</span></span></a>, <a class="anchor anchor-primary" href="#b0050" name="bb0050" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0050"><span class="anchor-text-container"><span class="anchor-text">Dahmane and Meunier, 2014</span></span></a>, <a class="anchor anchor-primary" href="#b0090" name="bb0090" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0090"><span class="anchor-text-container"><span class="anchor-text">Happy et al., 2015</span></span></a>, <a class="anchor anchor-primary" href="#b0095" name="bb0095" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0095"><span class="anchor-text-container"><span class="anchor-text">Hegde et al., 2016</span></span></a><span>). The polynomial kernel learns the <a href="/topics/agricultural-and-biological-sciences/nonlinear-model" title="Learn more about nonlinear models from ScienceDirect's AI-generated Topic Pages" class="topic-link">nonlinear models</a> and also resolves their similarity (</span><a class="anchor anchor-primary" href="#b0265" name="bb0265" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0265"><span class="anchor-text-container"><span class="anchor-text">Zhao and Pietikäinen, 2009</span></span></a>, <a class="anchor anchor-primary" href="#b0255" name="bb0255" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0255"><span class="anchor-text-container"><span class="anchor-text">Zhang et al., 2011</span></span></a>, <a class="anchor anchor-primary" href="#b0120" name="bb0120" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0120"><span class="anchor-text-container"><span class="anchor-text">Ji and Idrissi, 2012</span></span></a>, <a class="anchor anchor-primary" href="#b0020" name="bb0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0020"><span class="anchor-text-container"><span class="anchor-text">Biswas, 2015</span></span></a>).</div><div class="u-margin-s-bottom" id="p0130"><span>The <a href="/topics/biochemistry-genetics-and-molecular-biology/hidden-markov-model" title="Learn more about Hidden Markov Model from ScienceDirect's AI-generated Topic Pages" class="topic-link">Hidden Markov Model</a> (HMM) classifier is the statistical model which categorizes the expressions into different types (</span><a class="anchor anchor-primary" href="#b0220" name="bb0220" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0220"><span class="anchor-text-container"><span class="anchor-text">Taylor et al., 2014</span></span></a><span>). Hidden <a href="/topics/computer-science/conditional-random-field" title="Learn more about Conditional Random Fields from ScienceDirect's AI-generated Topic Pages" class="topic-link">Conditional Random Fields</a><span> (HCRF) representation is used for classification. It uses the full covariance <a href="/topics/earth-and-planetary-sciences/normal-density-functions" title="Learn more about Gaussian distribution from ScienceDirect's AI-generated Topic Pages" class="topic-link">Gaussian distribution</a> for superior classification performance (</span></span><a class="anchor anchor-primary" href="#b0210" name="bb0210" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0210"><span class="anchor-text-container"><span class="anchor-text">Siddiqi et al., 2015</span></span></a>).</div><div class="u-margin-s-bottom" id="p0135"><span><span>Online Sequential <a href="/topics/engineering/extreme-learning-machine" title="Learn more about Extreme Learning Machine from ScienceDirect's AI-generated Topic Pages" class="topic-link">Extreme Learning Machine</a> (OSELM) is a method that uses RBF for classification. OSELM mainly contains two stages. They are initialization and </span><a href="/topics/computer-science/sequential-learning" title="Learn more about sequential learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">sequential learning</a><span> stages. <a href="/topics/computer-science/initialization-stage" title="Learn more about Initialization stage from ScienceDirect's AI-generated Topic Pages" class="topic-link">Initialization stage</a><span> includes the <a href="/topics/computer-science/training-sample" title="Learn more about training samples from ScienceDirect's AI-generated Topic Pages" class="topic-link">training samples</a> (</span></span></span><a class="anchor anchor-primary" href="#b0055" name="bb0055" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0055"><span class="anchor-text-container"><span class="anchor-text">Demir, 2014</span></span></a><span>). Pair wise classifiers are also used for expression classification. It uses the one against one <a href="/topics/computer-science/classification-approach" title="Learn more about classification approach from ScienceDirect's AI-generated Topic Pages" class="topic-link">classification approach</a> so exacting separation is utilized (</span><a class="anchor anchor-primary" href="#b0040" name="bb0040" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0040"><span class="anchor-text-container"><span class="anchor-text">Cossetin et al., 2016</span></span></a>).</div><div class="u-margin-s-bottom" id="p0140"><span><span>ID3 <a href="/topics/biochemistry-genetics-and-molecular-biology/decision-trees" title="Learn more about Decision Tree from ScienceDirect's AI-generated Topic Pages" class="topic-link">Decision Tree</a><span> (DT) classifier is a rule based classifier which extracts the <a href="/topics/computer-science/predefined-rule" title="Learn more about predefined rules from ScienceDirect's AI-generated Topic Pages" class="topic-link">predefined rules</a><span> to produce competent rules. The predefined rules are generated from the <a href="/topics/computer-science/decision-trees" title="Learn more about decision tree from ScienceDirect's AI-generated Topic Pages" class="topic-link">decision tree</a> and it was constructed by </span></span></span><a href="/topics/computer-science/information-gain" title="Learn more about information gain from ScienceDirect's AI-generated Topic Pages" class="topic-link">information gain</a> metrics. The classification is performed using the least Boolean evaluation (</span><a class="anchor anchor-primary" href="#b0165" name="bb0165" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0165"><span class="anchor-text-container"><span class="anchor-text">Noh et al., 2007</span></span></a>, <a class="anchor anchor-primary" href="#b0185" name="bb0185" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0185"><span class="anchor-text-container"><span class="anchor-text">Rashid, 2016</span></span></a><span>). Classification and Regression <a href="/topics/biochemistry-genetics-and-molecular-biology/tree" title="Learn more about Tree from ScienceDirect's AI-generated Topic Pages" class="topic-link">Tree</a><span> (CART) is a <a href="/topics/engineering/machine-learning-algorithm" title="Learn more about machine learning algorithm from ScienceDirect's AI-generated Topic Pages" class="topic-link">machine learning algorithm</a> for classification. The metric likely Decision tree and Gini impurity are estimated. CART classifiers are signified by using the distance vectors (</span></span><a class="anchor anchor-primary" href="#b0195" name="bb0195" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0195"><span class="anchor-text-container"><span class="anchor-text">Salmam et al., 2016</span></span></a>).</div><div class="u-margin-s-bottom" id="p0145"><span>Learning <a href="/topics/computer-science/vector-quantization" title="Learn more about Vector Quantization from ScienceDirect's AI-generated Topic Pages" class="topic-link">Vector Quantization</a><span> (LVQ) is the unsupervised <a href="/topics/computer-science/clustering-algorithm" title="Learn more about clustering algorithm from ScienceDirect's AI-generated Topic Pages" class="topic-link">clustering algorithm</a> (</span></span><a class="anchor anchor-primary" href="#b0015" name="bb0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0015"><span class="anchor-text-container"><span class="anchor-text">Bashyal et al., 2008</span></span></a><span><span>) which has two layers namely competitive and output layers. The competitive layer has the neurons that are known as <a href="/topics/computer-science/subclasses" title="Learn more about subclasses from ScienceDirect's AI-generated Topic Pages" class="topic-link">subclasses</a>. The neuron which is the greatest match in competitive layer then put high for the class of exacting neuron in the output layer. </span><a href="/topics/biochemistry-genetics-and-molecular-biology/perceptron" title="Learn more about Multi Layer Perceptron from ScienceDirect's AI-generated Topic Pages" class="topic-link">Multi Layer Perceptron</a> (MLP) is also used for classification and it contains three layers such as input layer, output layer and processing layer in which neurons are present (</span><a class="anchor anchor-primary" href="#b0185" name="bb0185" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0185"><span class="anchor-text-container"><span class="anchor-text">Rashid, 2016</span></span></a>).</div><div class="u-margin-s-bottom" id="p0150"><span>The Multilayer <a href="/topics/computer-science/feedforward-neural-network" title="Learn more about Feed Forward Neural Network from ScienceDirect's AI-generated Topic Pages" class="topic-link">Feed Forward Neural Network</a><span> (MFFNN) classifier uses three layers such as input, hidden and output layers and <a href="/topics/chemical-engineering/backpropagation" title="Learn more about back propagation from ScienceDirect's AI-generated Topic Pages" class="topic-link">back propagation</a> algorithm for classification. In the training stage the weights are initialized and the activation units are estimated (</span></span><a class="anchor anchor-primary" href="#b0170" name="bb0170" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0170"><span class="anchor-text-container"><span class="anchor-text">Owusu et al., 2014</span></span></a><span><span><span>). Bayesian <a href="/topics/computer-science/neural-network" title="Learn more about neural network from ScienceDirect's AI-generated Topic Pages" class="topic-link">neural network</a> classifier is the classification method which also includes three layers such as input, hidden and output layers. The classical </span><a href="/topics/earth-and-planetary-sciences/back-propagation" title="Learn more about back propagation from ScienceDirect's AI-generated Topic Pages" class="topic-link">back propagation</a> algorithm is used with </span><a href="/topics/engineering/bayesian-classifier" title="Learn more about Bayesian classifier from ScienceDirect's AI-generated Topic Pages" class="topic-link">Bayesian classifier</a> for its better accuracy (</span><a class="anchor anchor-primary" href="#b0145" name="bb0145" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0145"><span class="anchor-text-container"><span class="anchor-text">Mahersia and Hamrouni, 2015</span></span></a><span><span>). Convolution <a href="/topics/agricultural-and-biological-sciences/neural-network" title="Learn more about Neural Network from ScienceDirect's AI-generated Topic Pages" class="topic-link">Neural Network</a><span> (CNN) consists of two layers such as <a href="/topics/computer-science/convolutional-layer" title="Learn more about convolutional layer from ScienceDirect's AI-generated Topic Pages" class="topic-link">convolutional layer</a> and </span></span><a href="/topics/engineering/subsamplings" title="Learn more about subsampling from ScienceDirect's AI-generated Topic Pages" class="topic-link">subsampling</a><span><span> layer in which the two dimensional images are taken as input. In <a href="/topics/engineering/convolutional-layer" title="Learn more about convolutional layer from ScienceDirect's AI-generated Topic Pages" class="topic-link">convolutional layer</a> the feature maps are produced by intricate the </span><a href="/topics/computer-science/convolution-kernel" title="Learn more about convolution kernels from ScienceDirect's AI-generated Topic Pages" class="topic-link">convolution kernels</a> with the two dimensional images where as in the subsampling layer, pooling and redeployment are performed (</span></span><a class="anchor anchor-primary" href="#b0205" name="bb0205" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0205"><span class="anchor-text-container"><span class="anchor-text">Shan et al., 2017</span></span></a>). The CNN also contains two important perceptions likely shared weight and sparse connectivity (<a class="anchor anchor-primary" href="#b0185" name="bb0185" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0185"><span class="anchor-text-container"><span class="anchor-text">Rashid, 2016</span></span></a><span><span>). In FER, the CNN classifier used as <a href="/topics/computer-science/multiple-classifier" title="Learn more about multiple classifiers from ScienceDirect's AI-generated Topic Pages" class="topic-link">multiple classifiers</a> for the different face regions. If CNN is framed for entire face image then first frame the CNN for mouth area and next for eye area likely for each other area </span><a href="/topics/computer-science/convolutional-neural-network" title="Learn more about CNNs from ScienceDirect's AI-generated Topic Pages" class="topic-link">CNNs</a> are framed (</span><a class="anchor anchor-primary" href="#b0045" name="bb0045" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0045"><span class="anchor-text-container"><span class="anchor-text">Cui et al., 2016</span></span></a>).</div><div class="u-margin-s-bottom" id="p0155"><span><a href="/topics/chemical-engineering/deep-neural-network" title="Learn more about Deep Neural Network from ScienceDirect's AI-generated Topic Pages" class="topic-link">Deep Neural Network</a> (DNN) contains various hidden layers and the more difficult functions are trained efficiently comparing with other neural networks (</span><a class="anchor anchor-primary" href="#b0130" name="bb0130" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0130"><span class="anchor-text-container"><span class="anchor-text">Li and Lam, 2015</span></span></a><span>). The <a href="/topics/engineering/deep-belief-network" title="Learn more about Deep Belief Network from ScienceDirect's AI-generated Topic Pages" class="topic-link">Deep Belief Network</a><span> (DBN) contains the hidden variable resides of the various number of Restricted <a href="/topics/computer-science/boltzmann-machine" title="Learn more about Boltzmann Machine from ScienceDirect's AI-generated Topic Pages" class="topic-link">Boltzmann Machine</a> (RBM) which are the undirected generative pattern (</span></span><a class="anchor anchor-primary" href="#b0135" name="bb0135" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0135"><span class="anchor-text-container"><span class="anchor-text">Lv, 2015</span></span></a><span>). DBN contains the <a href="/topics/engineering/backpropagation" title="Learn more about Back Propagation from ScienceDirect's AI-generated Topic Pages" class="topic-link">Back Propagation</a> (BP) layer classifies the high-level features using classification (</span><a class="anchor anchor-primary" href="#b0240" name="bb0240" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0240"><span class="anchor-text-container"><span class="anchor-text">Yang et al., 2016</span></span></a>). DBN generally includes two phases such as pre-learning and fine-tuning (<a class="anchor anchor-primary" href="#b0235" name="bb0235" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0235"><span class="anchor-text-container"><span class="anchor-text">Wu and Qiu, 2017</span></span></a>) in which RBM are developed separately in the first step whereas the BP are learning the input and output data in the last phase.</div><div class="u-margin-s-bottom" id="p0160">According to several classifiers SVM classifier gives better <a href="/topics/computer-science/recognition-accuracy" title="Learn more about recognition accuracy from ScienceDirect's AI-generated Topic Pages" class="topic-link">recognition accuracy</a><span> and it provides better classification. The <a href="/topics/chemical-engineering/neural-network" title="Learn more about neural network from ScienceDirect's AI-generated Topic Pages" class="topic-link">neural network</a> based classifier CNN gives better accuracy than the other neural network based classifiers. In FER, SVM classifier is more exploitable comparing with other classifiers for recognition of expressions.</span></div><div class="u-margin-s-bottom"><div id="p0165">The various FER techniques with their algorithm is analyzed in <a class="anchor anchor-primary" href="#t0005" name="bt0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0005"><span class="anchor-text-container"><span class="anchor-text">Table1</span></span></a><span> which includes the algorithms that are used for three important requirements such as preprocessing, feature extraction and classification. The various preprocessing methods used in this table are, face detection, image enhancement, normalization, Gabor filter, localization, face acquisition, down sampling, histogram equalization, face region detection, face alignment, ROI segmentation and resizing. The different feature extraction methods used in this table are LEM, Action based model, Gabor filter, LBP-TOP, GASM, Patch based, GL wavelet, LBP, VTB, Moments, PCA, ICA, LCT, HOG, Steerable pyramid, DCT, SWLDA, WLD, SDM, WPLBP, haar like features, LDN, LDTP, DWT, K-ELBP, 2DPCA and <a href="/topics/computer-science/eigenface" title="Learn more about eigenfaces from ScienceDirect's AI-generated Topic Pages" class="topic-link">eigenfaces</a><span>. Classifiers used in this table are ID3 decision tree, LVQ, SVM, KNN, HMM, MFFNN, OSLEM, Bayesian neural network, HCRF, pair wise, CART, Euclidean distance, CNN, MDC, <a href="/topics/biochemistry-genetics-and-molecular-biology/chi-square-testing" title="Learn more about Chi square test from ScienceDirect's AI-generated Topic Pages" class="topic-link">Chi square test</a> and fisher discrimination dictionary.</span></span></div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0005"><span class="captions text-s"><span id="cn0040"><p id="sp0045"><span class="label">Table 1</span>. Algorithm analysis of 2D FER Techniques.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1 valign-top"><th scope="col" class="align-left">Author, Year</th><th scope="col" class="align-left">Preprocessing method</th><th scope="col" class="align-left">Feature extraction method</th><th scope="col" class="align-left">Classification method</th></tr></thead><tbody><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0075" name="bb0075" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0075"><span class="anchor-text-container"><span class="anchor-text">Gao et al. (2003)</span></span></a></td><td class="align-left">Not reported</td><td class="align-left">LEM</td><td class="align-left">dLHD</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0165" name="bb0165" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0165"><span class="anchor-text-container"><span class="anchor-text">Noh et al. (2007)</span></span></a></td><td class="align-left">Face detection</td><td class="align-left">Action based model</td><td class="align-left">ID3 decision tree</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0015" name="bb0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0015"><span class="anchor-text-container"><span class="anchor-text">Bashyal et al. (2008)</span></span></a></td><td class="align-left">Image enhancement</td><td class="align-left">Gabor filter (GF)</td><td class="align-left">LVQ</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0265" name="bb0265" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0265"><span class="anchor-text-container"><span class="anchor-text">Zhao and Pietikäinen (2009)</span></span></a></td><td class="align-left">Not reported</td><td class="align-left">LBP – TOP</td><td class="align-left">SVM</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0215" name="bb0215" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0215"><span class="anchor-text-container"><span class="anchor-text">Song et al. (2010)</span></span></a></td><td class="align-left">Not specified</td><td class="align-left">GASM</td><td class="align-left">SVM</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0230" name="bb0230" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0230"><span class="anchor-text-container"><span class="anchor-text">Wang et al. (2010)</span></span></a></td><td class="align-left">Not reported</td><td class="align-left">Not reported</td><td class="align-left">SVM</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0255" name="bb0255" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0255"><span class="anchor-text-container"><span class="anchor-text">Zhang et al. (2011)</span></span></a></td><td class="align-left">Gabor filter</td><td class="align-left">Patch based</td><td class="align-left">SVM</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0175" name="bb0175" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0175"><span class="anchor-text-container"><span class="anchor-text">Poursaberi et al. (2012)</span></span></a></td><td class="align-left">Localization, Normalization</td><td class="align-left">GL Wavelet</td><td class="align-left">KNN</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0120" name="bb0120" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0120"><span class="anchor-text-container"><span class="anchor-text">Ji and Idrissi (2012)</span></span></a></td><td class="align-left">Face acquisition</td><td class="align-left">LBP, VTB, Moments</td><td class="align-left">SVM</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0220" name="bb0220" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0220"><span class="anchor-text-container"><span class="anchor-text">Taylor et al. (2014)</span></span></a></td><td class="align-left">Enhancement</td><td class="align-left">PCA ICA</td><td class="align-left">HMM</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0170" name="bb0170" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0170"><span class="anchor-text-container"><span class="anchor-text">Owusu et al. (2014)</span></span></a></td><td class="align-left">Down sampling</td><td class="align-left">GF</td><td class="align-left">MFFNN</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0055" name="bb0055" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0055"><span class="anchor-text-container"><span class="anchor-text">Demir (2014)</span></span></a></td><td class="align-left">Histogram equalization</td><td class="align-left">LCT</td><td class="align-left">OSLEM</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0260" name="bb0260" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0260"><span class="anchor-text-container"><span class="anchor-text">Zhang et al. (2014)</span></span></a></td><td class="align-left">Face region detection</td><td class="align-left">GF</td><td class="align-left">SVM</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0050" name="bb0050" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0050"><span class="anchor-text-container"><span class="anchor-text">Dahmane and Meunier (2014)</span></span></a></td><td class="align-left">Face alignment</td><td class="align-left">HOG</td><td class="align-left">SVM</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0145" name="bb0145" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0145"><span class="anchor-text-container"><span class="anchor-text">Mahersia and Hamrouni (2015)</span></span></a></td><td class="align-left">Normalization</td><td class="align-left">Steerable pyramid</td><td class="align-left">Bayesian neural network</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0100" name="bb0100" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0100"><span class="anchor-text-container"><span class="anchor-text">Hernandez-matamoros et al. (2015)</span></span></a></td><td class="align-left">ROI segmentation</td><td class="align-left">Gabor function</td><td class="align-left">SVM</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0090" name="bb0090" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0090"><span class="anchor-text-container"><span class="anchor-text">Happy et al. (2015)</span></span></a></td><td class="align-left">Histogram equalization</td><td class="align-left">LBP</td><td class="align-left">SVM</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0020" name="bb0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0020"><span class="anchor-text-container"><span class="anchor-text">Biswas (2015)</span></span></a></td><td class="align-left">Histogram equalization</td><td class="align-left">DCT</td><td class="align-left">SVM</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0210" name="bb0210" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0210"><span class="anchor-text-container"><span class="anchor-text">Siddiqi et al. (2015)</span></span></a></td><td class="align-left">Not specified</td><td class="align-left">SWLDA</td><td class="align-left">HCRF</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0040" name="bb0040" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0040"><span class="anchor-text-container"><span class="anchor-text">Cossetin et al. (2016)</span></span></a></td><td class="align-left">Histogram equalization</td><td class="align-left">LBP, WLD</td><td class="align-left">Pairwise Classifiers</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0195" name="bb0195" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0195"><span class="anchor-text-container"><span class="anchor-text">Salmam et al. (2016)</span></span></a></td><td class="align-left">Face detection</td><td class="align-left">SDM</td><td class="align-left">CART</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0125" name="bb0125" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0125"><span class="anchor-text-container"><span class="anchor-text">Kumar et al. (2016)</span></span></a></td><td class="align-left">Not reported</td><td class="align-left">WPLBP</td><td class="align-left">SVM</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0095" name="bb0095" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0095"><span class="anchor-text-container"><span class="anchor-text">Hegde et al. (2016)</span></span></a></td><td class="align-left">Resizing</td><td class="align-left">GF</td><td class="align-left">Euclidean distance (ED), SVM</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0185" name="bb0185" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0185"><span class="anchor-text-container"><span class="anchor-text">Rashid (2016)</span></span></a></td><td class="align-left">Balancing data</td><td class="align-left">Luxand Face SDK, ED</td><td class="align-left">DT, MLP, CNN</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0045" name="bb0045" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0045"><span class="anchor-text-container"><span class="anchor-text">Cui et al. (2016)</span></span></a></td><td class="align-left">Face detection, normalization</td><td class="align-left">Not reported</td><td class="align-left">CNN</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0115" name="bb0115" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0115"><span class="anchor-text-container"><span class="anchor-text">Jain et al. (2016)</span></span></a></td><td class="align-left">Face detection</td><td class="align-left">LBP</td><td class="align-left">ED, SVM, Neural Network</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0180" name="bb0180" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0180"><span class="anchor-text-container"><span class="anchor-text">Rahul and Cherian (2016)</span></span></a></td><td class="align-left">Face region cropping</td><td class="align-left">LDN</td><td class="align-left">Chi square test</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0080" name="bb0080" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0080"><span class="anchor-text-container"><span class="anchor-text">Guo et al. (2016)</span></span></a></td><td class="align-left">Normalization</td><td class="align-left">K-ELBP</td><td class="align-left">SVM</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0010" name="bb0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0010"><span class="anchor-text-container"><span class="anchor-text">Sharma and Rameshan, 2017</span></span></a></td><td class="align-left">Face normalization</td><td class="align-left">HOG, LBP, Eigen faces</td><td class="align-left">Fisher discrimination dictionary</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0205" name="bb0205" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0205"><span class="anchor-text-container"><span class="anchor-text">Shan et al. (2017)</span></span></a></td><td class="align-left">Histogram equalization</td><td class="align-left">Haar like features</td><td class="align-left">CNN</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0155" name="bb0155" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0155"><span class="anchor-text-container"><span class="anchor-text">Nazir et al. (2017)</span></span></a></td><td class="align-left">Face Detection</td><td class="align-left">HOG, DCT</td><td class="align-left">KNN</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0030" name="bb0030" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0030"><span class="anchor-text-container"><span class="anchor-text">Chang (2017)</span></span></a></td><td class="align-left">Face detection</td><td class="align-left">DCT, GF</td><td class="align-left">SVM</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0250" name="bb0250" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0250"><span class="anchor-text-container"><span class="anchor-text">Zhang et al. (2017)</span></span></a></td><td class="align-left">Localization</td><td class="align-left">Not reported</td><td class="align-left">CNN</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0190" name="bb0190" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0190"><span class="anchor-text-container"><span class="anchor-text">Ryu et al. (2017)</span></span></a></td><td class="align-left">Not reported</td><td class="align-left">LDTP</td><td class="align-left">SVM</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0160" name="bb0160" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0160"><span class="anchor-text-container"><span class="anchor-text">Nigam et al. (2018)</span></span></a></td><td class="align-left">Cropping, Normalization</td><td class="align-left">DWT, HOG</td><td class="align-left">SVM</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0035" name="bb0035" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0035"><span class="anchor-text-container"><span class="anchor-text">Clawson et al. (2018)</span></span></a></td><td class="align-left">Histogram equalization</td><td class="align-left">Not reported</td><td class="align-left">CNN</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0110" name="bb0110" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0110"><span class="anchor-text-container"><span class="anchor-text">Islam et al. (2018)</span></span></a></td><td class="align-left">Face detection</td><td class="align-left">2DPCA</td><td class="align-left">MDC</td></tr></tbody></table></div></div></div><div class="u-margin-s-bottom" id="p0170">In recent year papers, for preprocessing mostly the histogram equalization method is used. For feature extraction, Gabor filter, WPLBP, SDM, WLD, HOG are used. In feature extraction, the majority of the methods are based on the texture descriptor such as LBP based which gives improved results. In modern years, the classification uses the classifiers are SVM, Euclidean distance, CART, Neural network based classifiers and pair wise classifiers. The SVM classifier is highly used classifier in FER and it uses one- to –one, one- to all classification approach. Also, SVM with RBF kernel is most probably used which gives the highest classification performance comparing to other classifiers.</div><div class="u-margin-s-bottom" id="p0175">In 3D FER, the preprocessing of face images are performed by using the various methods such as smoothing, cropping, face alignment. The facial expressions are recognized from videos using the head gesticulation (<a class="anchor anchor-primary" href="#b0140" name="bb0140" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0140"><span class="anchor-text-container"><span class="anchor-text">Anisetti et al., 2005</span></span></a>). The features such as geometric features and appearance features are extracted from 3D faces using the various descriptors likely 3D surface descriptors (<a class="anchor anchor-primary" href="#b0245" name="bb0245" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0245"><span class="anchor-text-container"><span class="anchor-text">Yi Sun, 2008</span></span></a>), texture filters (<a class="anchor anchor-primary" href="#b0150" name="bb0150" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0150"><span class="anchor-text-container"><span class="anchor-text">Gaeta and Gerardo Iovane, 2013</span></span></a>), and covariance region descriptors (<a class="anchor anchor-primary" href="#b0225" name="bb0225" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0225"><span class="anchor-text-container"><span class="anchor-text">Hariri et al., 2017</span></span></a><span>). The facial landmarks eye and nose areas are localized by extracting the <a href="/topics/engineering/principal-curvature" title="Learn more about principal curvatures from ScienceDirect's AI-generated Topic Pages" class="topic-link">principal curvatures</a> and shape index for efficient recognition of facial expressions (</span><a class="anchor anchor-primary" href="#b0065" name="bb0065" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0065"><span class="anchor-text-container"><span class="anchor-text">Vezzetti et al., 2017</span></span></a>). The lip based features are easily extracted by using the geometric descriptors (<a class="anchor anchor-primary" href="#b0200" name="bb0200" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0200"><span class="anchor-text-container"><span class="anchor-text">Moos et al., 2014</span></span></a>) and the mean, median, histogram (<a class="anchor anchor-primary" href="#b0070" name="bb0070" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0070"><span class="anchor-text-container"><span class="anchor-text">Vezzetti et al., 2016</span></span></a>) features also extracted for 3D FER. The descriptors also formed from two facial components such as Basic Facial Shape Component (BFSC) and Expressional Shape Component (ESC) (<a class="anchor anchor-primary" href="#b0025" name="bb0025" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0025"><span class="anchor-text-container"><span class="anchor-text">Gong et al., 2009</span></span></a>). The classification is performed in 3D FER using the different types of classifiers likely multi-SVM (<a class="anchor anchor-primary" href="#b0225" name="bb0225" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0225"><span class="anchor-text-container"><span class="anchor-text">Hariri et al., 2017</span></span></a>), HMM (<a class="anchor anchor-primary" href="#b0245" name="bb0245" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0245"><span class="anchor-text-container"><span class="anchor-text">Yi Sun, 2008</span></span></a>), neural networks (<a class="anchor anchor-primary" href="#b0085" name="bb0085" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0085"><span class="anchor-text-container"><span class="anchor-text">Hamit Soyel, 2007</span></span></a>), Deep Fusion – CNN (<a class="anchor anchor-primary" href="#b0105" name="bb0105" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0105"><span class="anchor-text-container"><span class="anchor-text">Li et al., 2017</span></span></a><span>), <a href="/topics/biochemistry-genetics-and-molecular-biology/bayesian-learning" title="Learn more about Naïve Bayes from ScienceDirect's AI-generated Topic Pages" class="topic-link">Naïve Bayes</a> Classifier (NBC) (</span><a class="anchor anchor-primary" href="#b0005" name="bb0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0005"><span class="anchor-text-container"><span class="anchor-text">Arman Savran, 2017</span></span></a>). In 3D FER experimentation, mostly the Binghamton University 3D Facial Expression (BU-3DFE) database and Bosphorus databases are used.</div></section><section id="s0030"><h3 id="st040" class="u-h4 u-margin-m-top u-margin-xs-bottom">2.4. Database description</h3><div class="u-margin-s-bottom" id="p0180">Experiments are performed on FER by using various databases likely Japanese Female Facial Expressions (<a class="anchor anchor-primary" href="#b0275" name="bb0275" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0275"><span class="anchor-text-container"><span class="anchor-text">JAFFE, 2017</span></span></a>), Cohn – Kanade (<a class="anchor anchor-primary" href="#b0280" name="bb0280" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0280"><span class="anchor-text-container"><span class="anchor-text">CK, 2017</span></span></a>), Extended Cohn – Kanade (CK+), MMI (<a class="anchor anchor-primary" href="#b0285" name="bb0285" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0285"><span class="anchor-text-container"><span class="anchor-text">MMI, 2017</span></span></a>), Multimedia Understanding Group (<a class="anchor anchor-primary" href="#b0290" name="bb0290" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0290"><span class="anchor-text-container"><span class="anchor-text">MUG, 2017</span></span></a>), Taiwanese Facial Expression Image Database (<a class="anchor anchor-primary" href="#b0295" name="bb0295" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0295"><span class="anchor-text-container"><span class="anchor-text">TFEID, 2017</span></span></a>), Yale (<a class="anchor anchor-primary" href="#b0300" name="bb0300" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0300"><span class="anchor-text-container"><span class="anchor-text">Yale, 2017</span></span></a>), AR face database (<a class="anchor anchor-primary" href="#b0305" name="bb0305" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0305"><span class="anchor-text-container"><span class="anchor-text">AR, 2018</span></span></a>), Real-time database (<a class="anchor anchor-primary" href="#b0265" name="bb0265" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0265"><span class="anchor-text-container"><span class="anchor-text">Zhao and Pietikäinen, 2009</span></span></a>), Own database (<a class="anchor anchor-primary" href="#b0210" name="bb0210" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0210"><span class="anchor-text-container"><span class="anchor-text">Siddiqi et al., 2015</span></span></a>) and Karolinska Directed Emotional Faces (<a class="anchor anchor-primary" href="#b0310" name="bb0310" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0310"><span class="anchor-text-container"><span class="anchor-text">KDEF, 2018</span></span></a>).</div><div class="u-margin-s-bottom"><div id="p0185">In most of the experiments, JAFFE database is used. JAFFE holds ten Japanese female’s expressions with seven facial expressions and totally 213 images. Each image in JAFFE database contains 256 × 256 pixel resolution. Some of the sample images of JAFFE database are shown in <a class="anchor anchor-primary" href="#f0010" name="bf0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0010"><span class="anchor-text-container"><span class="anchor-text">Fig. 2</span></span></a>.</div><figure class="figure text-xs" id="f0010"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr2.jpg" height="186" alt="" aria-describedby="cn0010"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr2_lrg.jpg" target="_blank" download="" title="Download high-res image (113KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (113KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr2.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cn0010"><p id="sp0015"><span class="label">Fig. 2</span>. Sample images from JAFFE database.</p></span></span></figure></div><div class="u-margin-s-bottom"><div id="p0190">CK database also has seven expressions but it contains 132 subjects that are posed with natural and smile. It contains totally 486 image sequences with 640 × 490 pixel resolution of gray images. Some of the sample images of the CK database are shown in <a class="anchor anchor-primary" href="#f0015" name="bf0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0015"><span class="anchor-text-container"><span class="anchor-text">Fig. 3</span></span></a>.</div><figure class="figure text-xs" id="f0015"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr3.jpg" height="199" alt="" aria-describedby="cn0015"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr3_lrg.jpg" target="_blank" download="" title="Download high-res image (120KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (120KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr3.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cn0015"><p id="sp0020"><span class="label">Fig. 3</span>. Sample images from CK database.</p></span></span></figure></div><div class="u-margin-s-bottom"><div id="p0195"><a class="anchor anchor-primary" href="#t0010" name="bt0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0010"><span class="anchor-text-container"><span class="anchor-text">Table 2</span></span></a> shows the origin, acquisition, expression types, number of images, resolution details of the FER databases. The Real-time dataset is also used for FER which contains nearly 2250 images for six expressions and another one own dataset is used which contains 687 image pairs with 640 × 480 resolution.</div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0010"><span class="captions text-s"><span id="cn0045"><p id="sp0050"><span class="label">Table 2</span>. FER Databases description.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1 valign-top"><th scope="col" class="align-left">Database Name</th><th scope="col" class="align-left">Origin</th><th scope="col" class="align-left">Acquisition</th><th scope="col" class="align-left">Expressions</th><th scope="col" class="align-left">No. of images</th><th scope="col" class="align-left">Resolution</th></tr></thead><tbody><tr class="valign-top"><td class="align-left">Japanese Female Facial Expressions (JAFFE)</td><td class="align-left">Japan</td><td class="align-left">Photos are taken from Kyushu University</td><td class="align-left">Smile, sad, surprise, anger, fear, disgust, neutral</td><td class="align-left">213</td><td class="align-left">256 × 256</td></tr><tr class="valign-top"><td class="align-left">Yale</td><td class="align-left">California</td><td class="align-left">Photos are taken from U.C. San Diego Computer vision Laboratory</td><td class="align-left">Happy, normal, sad, sleepy, surprised, wink.</td><td class="align-left">165</td><td class="align-left">168 × 192</td></tr><tr class="valign-top"><td class="align-left">Cohn Kanade (CK)</td><td class="align-left">United States</td><td class="align-left">Photos are taken by Panasonic WV3230 cameras</td><td class="align-left">Joy, surprise, anger, fear, disgust, sadness</td><td class="align-left">486</td><td class="align-left">640 × 490</td></tr><tr class="valign-top"><td class="align-left">Extended Cohn Kanade (CK+)</td><td class="align-left">United States</td><td class="align-left">Photos are taken by Panasonic AG-7500 cameras</td><td class="align-left">Neutral, sadness, surprise, happiness, fear, anger, contempt and disgust</td><td class="align-left">593</td><td class="align-left">640 × 490</td></tr><tr class="valign-top"><td class="align-left">Multimedia Understanding Group (MUG)</td><td class="align-left">Caucasian</td><td class="align-left">High resolution and no occlusion photos are taken</td><td class="align-left">Neutral, sadness, surprise, happiness, fear, anger, and disgust</td><td class="align-left">1462</td><td class="align-left">896 × 896</td></tr><tr class="valign-top"><td class="align-left">AR face database</td><td class="align-left">Spain</td><td class="align-left">Photos are taken by Sony 3CCD cameras</td><td class="align-left">Neutral, smile, anger, scream</td><td class="align-left">4000</td><td class="align-left">768 × 576</td></tr><tr class="valign-top"><td class="align-left">MMI</td><td class="align-left">Netherlands</td><td class="align-left">Photos are taken by JVC GR-D23E Mini-DV cameras</td><td class="align-left">Disgust, Happiness, surprise, neutral, surprise, sad, fear</td><td class="align-left">250</td><td class="align-left">720 × 576</td></tr><tr class="valign-top"><td class="align-left">Taiwanese Facial Expression Image Database (TFEID)</td><td class="align-left">Taiwan</td><td class="align-left">Photos are taken by two CCD cameras simultaneously with different angles (0°,45°)</td><td class="align-left">Neutral, anger, contempt, disgust, fear, happiness, sadness, surprise</td><td class="align-left">7200</td><td class="align-left">600 × 480</td></tr><tr class="valign-top"><td class="align-left">Karolinska Directed Emotional Faces (KDEF)</td><td class="align-left">Sweden</td><td class="align-left">Photos are taken by Pentax LX cameras</td><td class="align-left">Angry, Fearful, Disgusted, Sad, Happy, Surprised, Neutral</td><td class="align-left">490</td><td class="align-left">762 × 562</td></tr></tbody></table></div></div></div></section></section><section id="s0035"><h2 id="st045" class="u-h4 u-margin-l-top u-margin-xs-bottom">3. Performance comparison</h2><div class="u-margin-s-bottom" id="p0200">The performance comparison of this survey is based on the complexity rate, <a href="/topics/computer-science/recognition-accuracy" title="Learn more about recognition accuracy from ScienceDirect's AI-generated Topic Pages" class="topic-link">recognition accuracy</a> on different databases, availability of preprocessing and feature extraction methods, expression count analysis, major contribution and advantages of the various FER techniques.</div><div class="u-margin-s-bottom"><div id="p0205">The complexity rates of the various FER techniques are shown in <a class="anchor anchor-primary" href="#f0020" name="bf0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0020"><span class="anchor-text-container"><span class="anchor-text">Fig. 4</span></span></a>. The x-axis indicates the complexity value of various FER techniques and the y-axis indicates the name of the FER methods. The complexity value of each method is calculated from its own papers which is categorized into three levels are less, medium and high. In <a class="anchor anchor-primary" href="#f0020" name="bf0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0020"><span class="anchor-text-container"><span class="anchor-text">Fig. 4</span></span></a><span><span><span>, the less complexity is denoted as 1, the medium complexity is denoted as 2 and the high complexity is denoted as 3. The complexity rates are less in <a href="/topics/computer-science/gabor-function" title="Learn more about Gabor functions from ScienceDirect's AI-generated Topic Pages" class="topic-link">Gabor functions</a>, DCT, </span><a href="/topics/engineering/local-binary-pattern" title="Learn more about LBP from ScienceDirect's AI-generated Topic Pages" class="topic-link">LBP</a> and </span><a href="/topics/engineering/local-descriptor" title="Learn more about WLD from ScienceDirect's AI-generated Topic Pages" class="topic-link">WLD</a> comparing with other methods.</span></div><figure class="figure text-xs" id="f0020"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr4.jpg" height="440" alt="" aria-describedby="cn0020"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr4_lrg.jpg" target="_blank" download="" title="Download high-res image (360KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (360KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr4.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cn0020"><p id="sp0025"><span class="label">Fig. 4</span>. Complexity rate of various <a href="/topics/agricultural-and-biological-sciences/face" title="Learn more about FER from ScienceDirect's AI-generated Topic Pages" class="topic-link">FER</a> techniques.</p></span></span></figure></div><div class="u-margin-s-bottom"><div id="p0210">The Accuracy rates of the various FER techniques are plotted in <a class="anchor anchor-primary" href="#f0025" name="bf0025" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0025"><span class="anchor-text-container"><span class="anchor-text">Fig. 5</span></span></a><span><span> where the x-axis indicates the name of the FER methods and the y-axis indicates the percentage of accuracy acquired in FER techniques. The accuracy of each method is analyzed from its own papers and difference databases are used in every paper, so the mean of the accuracy rate is calculated. The methods such as <a href="/topics/engineering/gabor-function" title="Learn more about Gabor functions from ScienceDirect's AI-generated Topic Pages" class="topic-link">Gabor functions</a> and DCT with </span><a href="/topics/engineering/support-vector-machine" title="Learn more about SVM from ScienceDirect's AI-generated Topic Pages" class="topic-link">SVM</a><span> classifier give better accuracy. LBP and <a href="/topics/computer-science/local-descriptor" title="Learn more about WLD from ScienceDirect's AI-generated Topic Pages" class="topic-link">WLD</a> descriptors with the pair wise classifiers give better accuracy rate.</span></span></div><figure class="figure text-xs" id="f0025"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr5.jpg" height="402" alt="" aria-describedby="cn0025"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr5_lrg.jpg" target="_blank" download="" title="Download high-res image (406KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (406KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr5.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cn0025"><p id="sp0030"><span class="label">Fig. 5</span>. Accuracy rate of various FER techniques.</p></span></span></figure></div><div class="u-margin-s-bottom"><div id="p0215">The availability of preprocessing and feature extraction is shown in <a class="anchor anchor-primary" href="#f0030" name="bf0030" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0030"><span class="anchor-text-container"><span class="anchor-text">Fig. 6</span></span></a>. The x-axis indicates the author name of the various FER techniques. The y-axis denotes the availability of preprocessing and feature extraction methods in survey papers. The availability of preprocessing and the feature extraction calculation is based on the presence of the preprocessing and feature extraction in FER papers. If the preprocessing is a presence in the paper then it is denoted as 1 otherwise denoted as 0 and it is represented as 0.1 for visible. Likewise the same procedure for calculating the availability of feature extraction in FER papers.</div><figure class="figure text-xs" id="f0030"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr6.jpg" height="360" alt="" aria-describedby="cn0030"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr6_lrg.jpg" target="_blank" download="" title="Download high-res image (416KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (416KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr6.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cn0030"><p id="sp0035"><span class="label">Fig. 6</span>. Availability of preprocessing and feature extraction.</p></span></span></figure></div><div class="u-margin-s-bottom"><div id="p0220">The expression count analysis of FER techniques are described in <a class="anchor anchor-primary" href="#f0035" name="bf0035" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0035"><span class="anchor-text-container"><span class="anchor-text">Fig. 7</span></span></a> here the x-axis denotes the name of the FER methods and the y-axis denotes the number of recognized expressions using the FER methods. The expression count is analyzed from its own papers and the maximum of seven numbers of expressions are recognized in most papers.</div><figure class="figure text-xs" id="f0035"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr7.jpg" height="310" alt="" aria-describedby="cn0035"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr7_lrg.jpg" target="_blank" download="" title="Download high-res image (402KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (402KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1319157818303379-gr7.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cn0035"><p id="sp0040"><span class="label">Fig. 7</span>. Expression count analysis.</p></span></span></figure></div><div class="u-margin-s-bottom"><div id="p0225">The performance analysis of various FER techniques is described in <a class="anchor anchor-primary" href="#t0015" name="bt0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0015"><span class="anchor-text-container"><span class="anchor-text">Table 3</span></span></a>. It includes the various fields such as author name, year, FER method name, database name, complexity rate, recognition accuracy, number of expressions recognized, major contributions and advantage of FER techniques. The author name and year field of the table denote the authors of various FER papers and the year denotes the publishing year of the FER papers. The FER method name field of the table describes the methods used for recognition of facial expressions. The databases used in the FER papers are JAFFE, CK, CK+, MMI, MUG, TFEID, AR, Yale, KDEF (Karolinska Directed Emotional Faces), Real-time and own dataset. The complexity rate of various FER techniques are denoted as less, medium, high and it is also illustrated in <a class="anchor anchor-primary" href="#f0020" name="bf0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0020"><span class="anchor-text-container"><span class="anchor-text">Fig. 4</span></span></a>. The recognition accuracy of the different techniques is from 75% to 99% and it is also illustrated in <a class="anchor anchor-primary" href="#f0025" name="bf0025" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0025"><span class="anchor-text-container"><span class="anchor-text">Fig. 5</span></span></a>. The number of expressions recognized in the FER survey papers is 7. The LEM method recognizes only 3 expressions and the majority of paper recognizes 6 or 7 expressions. The major contribution field of this table describes the major work involved in the FER papers and the advantage field indicates the benefits of the FER techniques.</div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0015"><span class="captions text-s"><span id="cn0050"><p id="sp0055"><span class="label">Table 3</span>. Performance analysis of FER techniques.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1 valign-top"><th scope="col" class="align-left">Author name, year</th><th scope="col" class="align-left">FER method name</th><th scope="col" class="align-left">Database name</th><th scope="col" class="align-left">Complexity</th><th scope="col" class="align-left">Recognition accuracy (%)</th><th scope="col" class="align-left">No. of expressions recognized</th><th scope="col" class="align-left">Major contribution</th><th scope="col" class="align-left">Advantages</th></tr></thead><tbody><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0075" name="bb0075" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0075"><span class="anchor-text-container"><span class="anchor-text">Gao et al. (2003)</span></span></a></td><td class="align-left">LEM, dLHD</td><td class="align-left">AR</td><td class="align-left">Less</td><td class="align-left">86.6</td><td class="align-left">3</td><td class="align-left">Oriented structural features are extracted</td><td class="align-left">Suitable for real time applications</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0165" name="bb0165" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0165"><span class="anchor-text-container"><span class="anchor-text">Noh et al. (2007)</span></span></a></td><td class="align-left">Action based, ID3 decision tree</td><td class="align-left">JAFFE</td><td class="align-left">Less</td><td class="align-left">75</td><td class="align-left">6</td><td class="align-left">Facial features are discriminative &amp; non discriminative</td><td class="align-left">Cost effective in speed and accuracy</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0015" name="bb0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0015"><span class="anchor-text-container"><span class="anchor-text">Bashyal et al. (2008)</span></span></a></td><td class="align-left">GF, LVQ</td><td class="align-left">JAFFE</td><td class="align-left">Less</td><td class="align-left">88.86</td><td class="align-left">Not reported</td><td class="align-left">LVQ performs better recognition for fear expressions</td><td class="align-left">Better accuracy for fear expressions</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0265" name="bb0265" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0265"><span class="anchor-text-container"><span class="anchor-text">Zhao and Pietikäinen (2009)</span></span></a></td><td class="align-left">GASM, SVM</td><td class="align-left">CK</td><td class="align-left">High</td><td class="align-left">93.85</td><td class="align-left">6</td><td class="align-left">Adaboost learning for multi resolution features</td><td class="align-left">Flexible feature selection</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0215" name="bb0215" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0215"><span class="anchor-text-container"><span class="anchor-text">Song et al. (2010)</span></span></a></td><td class="align-left">LBP-TOP, SVM</td><td class="align-left">JAFFE, CK Realtime</td><td class="align-left">Less</td><td class="align-left">86.85</td><td class="align-left">7</td><td class="align-left">Detection of facial features point motion &amp; image ratio features</td><td class="align-left">More robust to lighting variations</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0230" name="bb0230" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0230"><span class="anchor-text-container"><span class="anchor-text">Wang et al. (2010)</span></span></a></td><td class="align-left">SVM</td><td class="align-left">JAFFE</td><td class="align-left">Less</td><td class="align-left">87.5</td><td class="align-left">Not reported</td><td class="align-left">DKFER for emotion detection</td><td class="align-left">More efficient emotion detection</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0255" name="bb0255" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0255"><span class="anchor-text-container"><span class="anchor-text">Zhang et al. (2011)</span></span></a></td><td class="align-left">Patch based, SVM</td><td class="align-left">JAFFE, CK</td><td class="align-left">Less</td><td class="align-left">82.5</td><td class="align-left">6</td><td class="align-left">Capture facial movement features based on distance features</td><td class="align-left">Effective recognition performance</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0175" name="bb0175" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0175"><span class="anchor-text-container"><span class="anchor-text">Poursaberi et al. (2012)</span></span></a></td><td class="align-left">GL Wavelet, KNN</td><td class="align-left">JAFFE, CK, MMI</td><td class="align-left">Medium</td><td class="align-left">91.9</td><td class="align-left">6</td><td class="align-left">Extraction of texture and geometric information</td><td class="align-left">Wealthy capability for texture analysis</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0120" name="bb0120" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0120"><span class="anchor-text-container"><span class="anchor-text">Ji and Idrissi (2012)</span></span></a></td><td class="align-left">LBP, VTB, Moments, SVM</td><td class="align-left">CK, MMI</td><td class="align-left">Medium</td><td class="align-left">95.84</td><td class="align-left">6</td><td class="align-left">Extraction of spatial temporal Features</td><td class="align-left">Effective image based recognition</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0220" name="bb0220" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0220"><span class="anchor-text-container"><span class="anchor-text">Taylor et al. (2014)</span></span></a></td><td class="align-left">PCA, ICA, HMM</td><td class="align-left">Own</td><td class="align-left">Less</td><td class="align-left">98</td><td class="align-left">6</td><td class="align-left">Multilayer scheme to conquer similarity problems</td><td class="align-left">High accuracy with own dataset</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0170" name="bb0170" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0170"><span class="anchor-text-container"><span class="anchor-text">Owusu et al. (2014)</span></span></a></td><td class="align-left">GF, MFFNN</td><td class="align-left">JAFFE, Yale</td><td class="align-left">High</td><td class="align-left">94.16</td><td class="align-left">7</td><td class="align-left">Feature selection based on Adaboost</td><td class="align-left">Lowest computational cost</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0055" name="bb0055" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0055"><span class="anchor-text-container"><span class="anchor-text">Demir (2014)</span></span></a></td><td class="align-left">LCT, OSLEM</td><td class="align-left">JAFFE, CK</td><td class="align-left">High</td><td class="align-left">94.41</td><td class="align-left">7</td><td class="align-left">Extraction of statistical features mean, entropy and S.D</td><td class="align-left">Reliable algorithm for recognition</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0260" name="bb0260" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0260"><span class="anchor-text-container"><span class="anchor-text">Zhang et al. (2014)</span></span></a></td><td class="align-left">GF, SVM</td><td class="align-left">JAFFE, CK</td><td class="align-left">Less</td><td class="align-left">82.5</td><td class="align-left">7</td><td class="align-left">Template matching for finding similar features</td><td class="align-left">High robustness &amp; fast processing speed</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0050" name="bb0050" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0050"><span class="anchor-text-container"><span class="anchor-text">Dahmane and Meunier (2014)</span></span></a></td><td class="align-left">HOG, SVM</td><td class="align-left">JAFFE</td><td class="align-left">High</td><td class="align-left">85</td><td class="align-left">7</td><td class="align-left">SIFT flow algorithm for face Alignment</td><td class="align-left">Robust to rotation, occlusion &amp; clutter</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0145" name="bb0145" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0145"><span class="anchor-text-container"><span class="anchor-text">Mahersia and Hamrouni (2015)</span></span></a></td><td class="align-left">Streerable pyramid, Bayesian NN</td><td class="align-left">JAFFE, CK</td><td class="align-left">Less</td><td class="align-left">95.73</td><td class="align-left">7</td><td class="align-left">Statistical features are extracted from the steerable representation</td><td class="align-left">Robust features &amp; achieve good results</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0100" name="bb0100" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0100"><span class="anchor-text-container"><span class="anchor-text">Hernandez-matamoros et al. (2015)</span></span></a></td><td class="align-left">Gabor function, SVM</td><td class="align-left">KDEF</td><td class="align-left">Less</td><td class="align-left">99</td><td class="align-left">Not reported</td><td class="align-left">Segmentation of face into two Regions</td><td class="align-left">High performance with low cost</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0090" name="bb0090" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0090"><span class="anchor-text-container"><span class="anchor-text">Happy et al. (2015)</span></span></a></td><td class="align-left">LBP, SVM</td><td class="align-left">JAFFE, CK+</td><td class="align-left">Less</td><td class="align-left">93.3</td><td class="align-left">6</td><td class="align-left">Facial landmarks lip and eyebrow corners are detected</td><td class="align-left">Lower computational complexity</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0020" name="bb0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0020"><span class="anchor-text-container"><span class="anchor-text">Biswas (2015)</span></span></a></td><td class="align-left">DCT, SVM</td><td class="align-left">JAFFE, CK</td><td class="align-left">Less</td><td class="align-left">98.63</td><td class="align-left">6</td><td class="align-left">Each image is decomposed up to fourth level</td><td class="align-left">Very fast &amp; high accuracy</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0210" name="bb0210" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0210"><span class="anchor-text-container"><span class="anchor-text">Siddiqi et al. (2015)</span></span></a></td><td class="align-left">SWLDA, HCRF</td><td class="align-left">JAFFE, CK+,MMI, Yale</td><td class="align-left">High</td><td class="align-left">96.37</td><td class="align-left">6</td><td class="align-left">Expressions are categorized into 3 major categories</td><td class="align-left">High accuracy</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0040" name="bb0040" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0040"><span class="anchor-text-container"><span class="anchor-text">Cossetin et al. (2016)</span></span></a></td><td class="align-left">LBP, WLD, Pairwise classifier</td><td class="align-left">JAFFE, CK, TFEID</td><td class="align-left">Less</td><td class="align-left">98.91</td><td class="align-left">7</td><td class="align-left">Each pair wise classifier uses a particular subset</td><td class="align-left">High accuracy &amp; less computation power</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0195" name="bb0195" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0195"><span class="anchor-text-container"><span class="anchor-text">Salmam et al. (2016)</span></span></a></td><td class="align-left">SDM, CART</td><td class="align-left">JAFFE, CK</td><td class="align-left">Less</td><td class="align-left">89.9</td><td class="align-left">6</td><td class="align-left">Decision tree for training</td><td class="align-left">Improved recognition accuracy</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0125" name="bb0125" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0125"><span class="anchor-text-container"><span class="anchor-text">Kumar et al. (2016)</span></span></a></td><td class="align-left">WPLBP, SVM</td><td class="align-left">JAFFE, CK+, MMI</td><td class="align-left">Medium</td><td class="align-left">98.15</td><td class="align-left">7</td><td class="align-left">Extraction of discriminative features from informative face regions</td><td class="align-left">Lower misclassification</td></tr><tr class="valign-top"><td class="align-left"><a class="anchor anchor-primary" href="#b0095" name="bb0095" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0095"><span class="anchor-text-container"><span class="anchor-text">Hegde et al. (2016)</span></span></a></td><td class="align-left">GF, ED, SVM</td><td class="align-left">JAFFE, Yale</td><td class="align-left">Less</td><td class="align-left">88.58</td><td class="align-left">6</td><td class="align-left">Projects feature vector space into low dimension space</td><td class="align-left">Improves the recognition efficiency</td></tr></tbody></table></div></div></div><div class="u-margin-s-bottom" id="p0230">From this table clearly understand the combination of preprocessing method ROI segmentation, feature extraction method <a href="/topics/engineering/gabor-filter" title="Learn more about GF from ScienceDirect's AI-generated Topic Pages" class="topic-link">GF</a><span> and classification method <a href="/topics/chemical-engineering/support-vector-machine" title="Learn more about SVM from ScienceDirect's AI-generated Topic Pages" class="topic-link">SVM</a><span> gives better FER accuracy 99% and less complexity which are analyzed by using the KDEF database. Comparing with other FER methods <a href="/topics/biochemistry-genetics-and-molecular-biology/support-vector-machine" title="Learn more about SVM from ScienceDirect's AI-generated Topic Pages" class="topic-link">SVM</a><span> classification is mostly used which classifies the maximum 7 number of expressions. From this table JAFFE, CK databases are frequently used in many papers and the Real-time dataset is used with the <a href="/topics/agricultural-and-biological-sciences/support-vector-machine" title="Learn more about SVM from ScienceDirect's AI-generated Topic Pages" class="topic-link">SVM</a> classifier which gives 86.85% accuracy.</span></span></span></div></section><section id="s0040"><h2 id="st050" class="u-h4 u-margin-l-top u-margin-xs-bottom">4. Conclusion</h2><div class="u-margin-s-bottom" id="p0235">The important future enhancements described from recent papers are FER for side view faces using the subjective information of facial sub-regions and use different parameters to represent the pose of the face for real-time applications. FER is used in real-time applications such as driver sate surveillance, medical, robotics interaction, forensic section, detecting deceptions. This survey paper is useful for software developers to develop algorithms based on their accuracy and complexity. Also, it is helpful for hardware implementation to implement with low cost depends on their need. This survey compares algorithms based on preprocessing, feature extraction, classification and major contributions. The performance analysis is done based on the database, complexity rate, recognition accuracy and major contributions. This survey discusses the properties such as availability of preprocessing and feature extraction and expression count. The power of algorithms, advantages are discussed elaborately to reach the aim of this survey. ROI <a href="/topics/computer-science/segmentation-method" title="Learn more about segmentation method from ScienceDirect's AI-generated Topic Pages" class="topic-link">segmentation method</a> is used for preprocessing and it gives the highest accuracy 99%. According to feature extraction GF have less complexity which gives the accuracy always between 82.5% and 99%. The highest recognition accuracy of 99% is provided by the SVM classifier and it recognizes the several expressions such as disgust, sad, smile, surprise, anger, fear, neutral effectively. In 2D FER, mostly JAFFE and CK database are used for efficient performance than the other databases.</div></section></div></div><div class="related-content-links u-display-none-from-md"><button class="button-link button-link-primary button-link-small" type="button"><span class="button-link-text-container"><span class="button-link-text">Recommended articles</span></span></button></div><div class="Tail"></div><div><section class="bibliography u-font-serif text-s" id="bi005"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">References</h2><section class="bibliography-sec" id="bs005"><ol class="references" id="reference-links-bs005"><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0005" id="ref-id-b0005" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Arman Savran, 2017</span></span></a></span><span class="reference" id="h0005"><div class="contribution"><div class="authors u-font-sans">B.S. Arman Savran</div><div id="ref-id-h0005" class="title text-m">Non-rigid registration based model-free 3D facial expression recognition</div></div><div class="host u-font-sans">Comput. Vis. Image Underst., 162 (2017), pp. 146-165, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.cviu.2017.07.005" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.cviu.2017.07.005</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Non-rigid%20registration%20based%20model-free%203D%20facial%20expression%20recognition&amp;publication_year=2017&amp;author=B.S.%20Arman%20Savran" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0005"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0010" id="ref-id-b0010" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Sharma and Rameshan, 2017</span></span></a></span><span class="reference" id="h0010"><div class="other-ref"><span>B, K.S., Rameshan, R., 2017. Dictionary Based Approach for Facial Expression Recognition from Static Images. Int. Conf. Comput. Vision, Graph. Image Process. pp. 39–49.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=B%2C%20K.S.%2C%20Rameshan%2C%20R.%2C%202017.%20Dictionary%20Based%20Approach%20for%20Facial%20Expression%20Recognition%20from%20Static%20Images.%20Int.%20Conf.%20Comput.%20Vision%2C%20Graph.%20Image%20Process.%20pp.%2039%E2%80%9349." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0015" id="ref-id-b0015" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Bashyal and Venayagamoorthy, 2008</span></span></a></span><span class="reference" id="h0015"><div class="contribution"><div class="authors u-font-sans">S. Bashyal, G.K.V. Venayagamoorthy</div><div id="ref-id-h0015" class="title text-m">Recognition of facial expressions using Gabor wavelets and learning vector quantization</div></div><div class="host u-font-sans">Eng. Appl. Artif. Intell., 21 (2008), pp. 1056-1064, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.engappai.2007.11.010" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.engappai.2007.11.010</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0952197607001492/pdfft?md5=f9259d36b9c7ef94c1075a373a680926&amp;pid=1-s2.0-S0952197607001492-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-h0015"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0952197607001492" aria-describedby="ref-id-h0015"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-52949135954&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0015"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Recognition%20of%20facial%20expressions%20using%20Gabor%20wavelets%20and%20learning%20vector%20quantization&amp;publication_year=2008&amp;author=S.%20Bashyal&amp;author=G.K.V.%20Venayagamoorthy" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0015"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0020" id="ref-id-b0020" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Biswas, 2015</span></span></a></span><span class="reference" id="h0020"><div class="other-ref"><span>Biswas, S., 2015. An Efficient Expression Recognition Method using Contourlet Transform. Int. Conf. Percept. Mach. Intell. pp. 167–174.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Biswas%2C%20S.%2C%202015.%20An%20Efficient%20Expression%20Recognition%20Method%20using%20Contourlet%20Transform.%20Int.%20Conf.%20Percept.%20Mach.%20Intell.%20pp.%20167%E2%80%93174." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0025" id="ref-id-b0025" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Gong et al., 2009</span></span></a></span><span class="reference" id="h0025"><div class="other-ref"><span>Boqing Gong, YuemingWang, Jianzhuang Liu, X.T., 2009. Automatic Facial Expression Recognition on a Single 3D Face by Exploring Shape Deformation. Proc. 17th ACM Int. Conf. Multimed. pp. 569–572.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Boqing%20Gong%2C%20YuemingWang%2C%20Jianzhuang%20Liu%2C%20X.T.%2C%202009.%20Automatic%20Facial%20Expression%20Recognition%20on%20a%20Single%203D%20Face%20by%20Exploring%20Shape%20Deformation.%20Proc.%2017th%20ACM%20Int.%20Conf.%20Multimed.%20pp.%20569%E2%80%93572." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0030" id="ref-id-b0030" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Chang, 2017</span></span></a></span><span class="reference" id="h0030"><div class="contribution"><div class="authors u-font-sans">H.T.Y. Chang</div><div id="ref-id-h0030" class="title text-m">Facial expression recognition using a combination of multiple facial features and support vector machine</div></div><div class="host u-font-sans">Soft Comput., 22 (2017), pp. 4389-4405, <a class="anchor anchor-primary" href="https://doi.org/10.1007/s00500-017-2634-3" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s00500-017-2634-3</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85028069878&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0030"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20using%20a%20combination%20of%20multiple%20facial%20features%20and%20support%20vector%20machine&amp;publication_year=2017&amp;author=H.T.Y.%20Chang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0030"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0035" id="ref-id-b0035" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Clawson et al., 2018</span></span></a></span><span class="reference" id="h0035"><div class="other-ref"><span>Clawson, K., Delicato, L.S., Bowerman, C., 2018. Human Centric Facial Expression Recognition. Proc. Br. HCI pp. 1–12.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Clawson%2C%20K.%2C%20Delicato%2C%20L.S.%2C%20Bowerman%2C%20C.%2C%202018.%20Human%20Centric%20Facial%20Expression%20Recognition.%20Proc.%20Br.%20HCI%20pp.%201%E2%80%9312." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0040" id="ref-id-b0040" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Cossetin et al., 2016</span></span></a></span><span class="reference" id="h0040"><div class="other-ref"><span>Cossetin, M.J., Nievola, J.C., Koerich, A.L., 2016. Facial expression recognition using a pairwise feature selection and classification approach. IEEE Int. Jt. Conf. Neural Networks, pp. 5149–5155.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Cossetin%2C%20M.J.%2C%20Nievola%2C%20J.C.%2C%20Koerich%2C%20A.L.%2C%202016.%20Facial%20expression%20recognition%20using%20a%20pairwise%20feature%20selection%20and%20classification%20approach.%20IEEE%20Int.%20Jt.%20Conf.%20Neural%20Networks%2C%20pp.%205149%E2%80%935155." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0045" id="ref-id-b0045" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Cui et al., 2016</span></span></a></span><span class="reference" id="h0045"><div class="other-ref"><span>Cui, R., Liu, M., Liu, M., 2016. Facial expression recognition based on ensemble of mulitple cNNs. Chinese Conf. Biometric Recognit. 511–518. https://doi.org/10.1007/978-3-319-46654-5.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Cui%2C%20R.%2C%20Liu%2C%20M.%2C%20Liu%2C%20M.%2C%202016.%20Facial%20expression%20recognition%20based%20on%20ensemble%20of%20mulitple%20cNNs.%20Chinese%20Conf.%20Biometric%20Recognit.%20511%E2%80%93518.%20https%3A%2F%2Fdoi.org%2F10.1007%2F978-3-319-46654-5." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0050" id="ref-id-b0050" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Dahmane and Meunier, 2014</span></span></a></span><span class="reference" id="h0050"><div class="contribution"><div class="authors u-font-sans">M. Dahmane, J. Meunier</div><div id="ref-id-h0050" class="title text-m">Prototype-based modeling for facial expression analysis</div></div><div class="host u-font-sans">IEEE Trans. Multimed., 16 (2014), pp. 1574-1584</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84907430378&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0050"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Prototype-based%20modeling%20for%20facial%20expression%20analysis&amp;publication_year=2014&amp;author=M.%20Dahmane&amp;author=J.%20Meunier" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0050"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0055" id="ref-id-b0055" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Demir, 2014</span></span></a></span><span class="reference" id="h0055"><div class="contribution"><div class="authors u-font-sans">Y. Demir</div><div id="ref-id-h0055" class="title text-m">A new facial expression recognition based on curvelet transform and online sequential extreme learning machine initialized with spherical clustering</div></div><div class="host u-font-sans">Neural Comput. Appl., 27 (2014), pp. 131-142, <a class="anchor anchor-primary" href="https://doi.org/10.1007/s00521-014-1569-1" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s00521-014-1569-1</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84904161535&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0055"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20new%20facial%20expression%20recognition%20based%20on%20curvelet%20transform%20and%20online%20sequential%20extreme%20learning%20machine%20initialized%20with%20spherical%20clustering&amp;publication_year=2014&amp;author=Y.%20Demir" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0055"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0060" id="ref-id-b0060" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Ekman et al., 2003</span></span></a></span><span class="reference" id="h0060"><div class="other-ref"><span>Ekman, P., Friesen, W., 2003. Unmasking the Face: A Guide to Recognizing Emotions From Facial Clues.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Ekman%2C%20P.%2C%20Friesen%2C%20W.%2C%202003.%20Unmasking%20the%20Face%3A%20A%20Guide%20to%20Recognizing%20Emotions%20From%20Facial%20Clues." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0065" id="ref-id-b0065" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Vezzetti et al., 2017</span></span></a></span><span class="reference" id="h0065"><div class="contribution"><div class="authors u-font-sans">Enrico Vezzetti, Federica Marcolin, Stefano Tornincasa, N.D. Luca Ulrich</div><div id="ref-id-h0065" class="title text-m">3D geometry-based automatic landmark localization in presence of facial occlusions</div></div><div class="host u-font-sans">Multimed. Tools Appl., 77 (2017), pp. 14177-14205, <a class="anchor anchor-primary" href="https://doi.org/10.1007/s11042-017-5025-y" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s11042-017-5025-y</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=3D%20geometry-based%20automatic%20landmark%20localization%20in%20presence%20of%20facial%20occlusions&amp;publication_year=2017&amp;author=Enrico%20Vezzetti&amp;author=Federica%20Marcolin&amp;author=Stefano%20Tornincasa&amp;author=N.D.%20Luca%20Ulrich" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0065"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0070" id="ref-id-b0070" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Vezzetti et al., 2016</span></span></a></span><span class="reference" id="h0070"><div class="contribution"><div class="authors u-font-sans">Enrico Vezzetti, Stefano Tornincasa, Sandro Moos, Federica Marcolin, Maria Grazia Violante, Domenico Speranza, F.P. David Buisan</div><div id="ref-id-h0070" class="title text-m">3D human face analysis: automatic expression recognition</div></div><div class="host u-font-sans">Proc. Biomed. Eng. (2016), <a class="anchor anchor-primary" href="https://doi.org/10.2316/P.2016.832" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.2316/P.2016.832</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=3D%20human%20face%20analysis%3A%20automatic%20expression%20recognition&amp;publication_year=2016&amp;author=Enrico%20Vezzetti&amp;author=Stefano%20Tornincasa&amp;author=Sandro%20Moos&amp;author=Federica%20Marcolin&amp;author=Maria%20Grazia%20Violante&amp;author=Domenico%20Speranza&amp;author=F.P.%20David%20Buisan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0070"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0075" id="ref-id-b0075" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Gao et al., 2003</span></span></a></span><span class="reference" id="h0075"><div class="contribution"><div class="authors u-font-sans">Y. Gao, M.K.H. Leung, S.C. Hui, M.W. Tananda</div><div id="ref-id-h0075" class="title text-m">Facial expression recognition from line-based caricatures</div></div><div class="host u-font-sans">IEEE Trans. Syst. Man Cybern. A Syst. Hum., 33 (2003), pp. 407-412</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0141638688&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0075"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20from%20line-based%20caricatures&amp;publication_year=2003&amp;author=Y.%20Gao&amp;author=M.K.H.%20Leung&amp;author=S.C.%20Hui&amp;author=M.W.%20Tananda" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0075"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0080" id="ref-id-b0080" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Guo et al., 2016</span></span></a></span><span class="reference" id="h0080"><div class="contribution"><div class="authors u-font-sans">M. Guo, X. Hou, Y. Ma</div><div id="ref-id-h0080" class="title text-m">Facial expression recognition using ELBP based on covariance matrix transform in KLT</div></div><div class="host u-font-sans">Multimed. Tools Appl., 76 (2016), pp. 2995-3010, <a class="anchor anchor-primary" href="https://doi.org/10.1007/s11042-016-3282-9" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s11042-016-3282-9</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20using%20ELBP%20based%20on%20covariance%20matrix%20transform%20in%20KLT&amp;publication_year=2016&amp;author=M.%20Guo&amp;author=X.%20Hou&amp;author=Y.%20Ma" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0080"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0085" id="ref-id-b0085" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Hamit Soyel, 2007</span></span></a></span><span class="reference" id="h0085"><div class="contribution"><div class="authors u-font-sans">H.D. Hamit Soyel</div><div id="ref-id-h0085" class="title text-m">Facial expression recognition using 3D facial feature distances</div></div><div class="host u-font-sans">Int. Conf. Image Anal. Recognit. (2007), pp. 831-838</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20using%203D%20facial%20feature%20distances&amp;publication_year=2007&amp;author=H.D.%20Hamit%20Soyel" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0085"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0090" id="ref-id-b0090" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Happy et al., 2015</span></span></a></span><span class="reference" id="h0090"><div class="contribution"><div class="authors u-font-sans">S.L. Happy, S. Member, A. Routray</div><div id="ref-id-h0090" class="title text-m">Automatic facial expression recognition using features of salient facial patches</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 6 (2015), pp. 1-12</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84924061133&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0090"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Automatic%20facial%20expression%20recognition%20using%20features%20of%20salient%20facial%20patches&amp;publication_year=2015&amp;author=S.L.%20Happy&amp;author=S.%20Member&amp;author=A.%20Routray" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0090"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0095" id="ref-id-b0095" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Hegde et al., 2016</span></span></a></span><span class="reference" id="h0095"><div class="contribution"><div class="authors u-font-sans">G.P. Hegde, M. Seetha, N. Hegde</div><div id="ref-id-h0095" class="title text-m">Kernel locality preserving symmetrical weighted fisher discriminant analysis based subspace approach for expression recognition</div></div><div class="host u-font-sans">Eng. Sci. Technol. Int. J., 19 (2016), pp. 1321-1333, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.jestch.2016.03.005" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.jestch.2016.03.005</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S2215098615300616/pdfft?md5=746a3f92fd5779ef4dd48cff3edb207f&amp;pid=1-s2.0-S2215098615300616-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-h0095"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S2215098615300616" aria-describedby="ref-id-h0095"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85017365768&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0095"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Kernel%20locality%20preserving%20symmetrical%20weighted%20fisher%20discriminant%20analysis%20based%20subspace%20approach%20for%20expression%20recognition&amp;publication_year=2016&amp;author=G.P.%20Hegde&amp;author=M.%20Seetha&amp;author=N.%20Hegde" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0095"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0100" id="ref-id-b0100" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Hernandez-matamoros et al., 2015</span></span></a></span><span class="reference" id="h0100"><div class="other-ref"><span>Hernandez-matamoros, A., Bonarini, A., Escamilla-hernandez, E., Nakano-miyatake, M., 2015., A Facial Expression Recognition with Automatic Segmentation of Face Regions. Int. Conf. Intell. Softw. Methodol. Tools, Tech. 529–540. doi: 10.1007/978-3-319-22689-7.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Hernandez-matamoros%2C%20A.%2C%20Bonarini%2C%20A.%2C%20Escamilla-hernandez%2C%20E.%2C%20Nakano-miyatake%2C%20M.%2C%202015.%2C%20A%20Facial%20Expression%20Recognition%20with%20Automatic%20Segmentation%20of%20Face%20Regions.%20Int.%20Conf.%20Intell.%20Softw.%20Methodol.%20Tools%2C%20Tech.%20529%E2%80%93540.%20doi%3A%2010.1007%2F978-3-319-22689-7." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0105" id="ref-id-b0105" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Li et al., 2017</span></span></a></span><span class="reference" id="h0105"><div class="contribution"><div class="authors u-font-sans">Huibin Li, Jian Sun, Xu. Zongben, M. Liming Chen</div><div id="ref-id-h0105" class="title text-m">Multimodal 2D + 3D facial expression recognition with deep fusion convolutional neural network</div></div><div class="host u-font-sans">IEEE Trans. Multimed., 19 (2017), pp. 2816-2831, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TMM.2017.2713408" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TMM.2017.2713408</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85021792202&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0105"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Multimodal%202D%20%203D%20facial%20expression%20recognition%20with%20deep%20fusion%20convolutional%20neural%20network&amp;publication_year=2017&amp;author=Huibin%20Li&amp;author=Jian%20Sun&amp;author=Xu.%20Zongben&amp;author=M.%20Liming%20Chen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0105"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0110" id="ref-id-b0110" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Islam et al., 2018</span></span></a></span><span class="reference" id="h0110"><div class="contribution"><div class="authors u-font-sans">D.I. Islam, S.R.N. Anal, A. Datta</div><div id="ref-id-h0110" class="title text-m">Facial expression recognition using 2DPCA on segmented images</div></div><div class="host u-font-sans">Adv. Comput. Commun. Paradig., 289–297 (2018), <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-981-10-8237-5" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-981-10-8237-5</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20using%202DPCA%20on%20segmented%20images&amp;publication_year=2018&amp;author=D.I.%20Islam&amp;author=S.R.N.%20Anal&amp;author=A.%20Datta" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0110"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0115" id="ref-id-b0115" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Jain et al., 2016</span></span></a></span><span class="reference" id="h0115"><div class="contribution"><div class="authors u-font-sans">S. Jain, M. Durgesh, T. Ramesh</div><div id="ref-id-h0115" class="title text-m">Facial expression recognition using variants of LBP and classifier fusion</div></div><div class="host u-font-sans">Proc. Int. Conf. ICT Sustain. Dev., 725–732 (2016), <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-981-10-0129-1" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-981-10-0129-1</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20using%20variants%20of%20LBP%20and%20classifier%20fusion&amp;publication_year=2016&amp;author=S.%20Jain&amp;author=M.%20Durgesh&amp;author=T.%20Ramesh" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0115"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0120" id="ref-id-b0120" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Ji and Idrissi, 2012</span></span></a></span><span class="reference" id="h0120"><div class="contribution"><div class="authors u-font-sans">Y. Ji, K. Idrissi</div><div id="ref-id-h0120" class="title text-m">Automatic facial expression recognition based on spatiotemporal descriptors</div></div><div class="host u-font-sans">Pattern Recognit. Lett., 33 (2012), pp. 1373-1380, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.patrec.2012.03.006" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.patrec.2012.03.006</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0167865512000815/pdfft?md5=c30bd819f510bb0f940dd92ec97bac3f&amp;pid=1-s2.0-S0167865512000815-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-h0120"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0167865512000815" aria-describedby="ref-id-h0120"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84860224487&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0120"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Automatic%20facial%20expression%20recognition%20based%20on%20spatiotemporal%20descriptors&amp;publication_year=2012&amp;author=Y.%20Ji&amp;author=K.%20Idrissi" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0120"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0125" id="ref-id-b0125" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Kumar et al., 2016</span></span></a></span><span class="reference" id="h0125"><div class="contribution"><div class="authors u-font-sans">S. Kumar, M.K. Bhuyan, B.K. Chakraborty</div><div id="ref-id-h0125" class="title text-m">Extraction of informative regions of a face for facial expression recognition</div></div><div class="host u-font-sans">IET Comput. Vis., 10 (2016), pp. 567-576, <a class="anchor anchor-primary" href="https://doi.org/10.1049/iet-cvi.2015.0273" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1049/iet-cvi.2015.0273</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84984677437&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0125"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Extraction%20of%20informative%20regions%20of%20a%20face%20for%20facial%20expression%20recognition&amp;publication_year=2016&amp;author=S.%20Kumar&amp;author=M.K.%20Bhuyan&amp;author=B.K.%20Chakraborty" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0125"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0130" id="ref-id-b0130" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Li and Lam, 2015</span></span></a></span><span class="reference" id="h0130"><div class="contribution"><div class="authors u-font-sans">J. Li, E.Y. Lam</div><div id="ref-id-h0130" class="title text-m">Facial expression n recognition using deep neural networks</div></div><div class="host u-font-sans">IEEE Int. Conf. Imaging Syst. Tech. (2015)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20n%20recognition%20using%20deep%20neural%20networks&amp;publication_year=2015&amp;author=J.%20Li&amp;author=E.Y.%20Lam" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0130"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0135" id="ref-id-b0135" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Lv, 2015</span></span></a></span><span class="reference" id="h0135"><div class="other-ref"><span>Lv, Y., 2015. Facial expression recognition via deep learning. Int. Conf. Smart Comput. doi: 10.1109/SMARTCOMP.2014.7043872</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Lv%2C%20Y.%2C%202015.%20Facial%20expression%20recognition%20via%20deep%20learning.%20Int.%20Conf.%20Smart%20Comput.%20doi%3A%2010.1109%2FSMARTCOMP.2014.7043872" target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0140" id="ref-id-b0140" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Anisetti et al., 2005</span></span></a></span><span class="reference" id="h0140"><div class="other-ref"><span>M. Anisetti, V. Bellandi, F.B., 2005. Accurate 3D Model based Face Tracking for Facial Expression Recognition. Proc. Int. Conf. Vis. Imaging, Image Process. 93–98.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=M.%20Anisetti%2C%20V.%20Bellandi%2C%20F.B.%2C%202005.%20Accurate%203D%20Model%20based%20Face%20Tracking%20for%20Facial%20Expression%20Recognition.%20Proc.%20Int.%20Conf.%20Vis.%20Imaging%2C%20Image%20Process.%2093%E2%80%9398." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0145" id="ref-id-b0145" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Mahersia and Hamrouni, 2015</span></span></a></span><span class="reference" id="h0145"><div class="contribution"><div class="authors u-font-sans">H. Mahersia, K. Hamrouni</div><div id="ref-id-h0145" class="title text-m">Using multiple steerable fi lters and Bayesian regularization for facial expression recognition</div></div><div class="host u-font-sans">Eng. Appl. Artif. Intell., 38 (2015), pp. 190-202, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.engappai.2014.11.002" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.engappai.2014.11.002</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0952197614002668/pdfft?md5=2469858115e2c9d8f2c967ae8b0f310d&amp;pid=1-s2.0-S0952197614002668-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-h0145"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0952197614002668" aria-describedby="ref-id-h0145"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84916604160&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0145"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Using%20multiple%20steerable%20fi%20lters%20and%20Bayesian%20regularization%20for%20facial%20expression%20recognition&amp;publication_year=2015&amp;author=H.%20Mahersia&amp;author=K.%20Hamrouni" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0145"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0150" id="ref-id-b0150" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Gaeta and Gerardo Iovane, 2013</span></span></a></span><span class="reference" id="h0150"><div class="contribution"><div class="authors u-font-sans">Matteo Gaeta, E.S. Gerardo Iovane</div><div id="ref-id-h0150" class="title text-m">A 3D geometric approach to face detection and facial expression recognition</div></div><div class="host u-font-sans">J. Discret. Math. Sci. Cryptogr., 9 (2013), pp. 39-53, <a class="anchor anchor-primary" href="https://doi.org/10.1080/09720529.2006.10698059" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1080/09720529.2006.10698059</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%203D%20geometric%20approach%20to%20face%20detection%20and%20facial%20expression%20recognition&amp;publication_year=2013&amp;author=Matteo%20Gaeta&amp;author=E.S.%20Gerardo%20Iovane" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0150"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0155" id="ref-id-b0155" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Nazir et al., 2017</span></span></a></span><span class="reference" id="h0155"><div class="contribution"><div class="authors u-font-sans">M. Nazir, Z. Jan, M. Sajjad</div><div id="ref-id-h0155" class="title text-m">Facial expression recognition using histogram of oriented gradients based transformed features</div></div><div class="host u-font-sans">Cluster Comput. (2017), <a class="anchor anchor-primary" href="https://doi.org/10.1007/s10586-017-0921-5" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s10586-017-0921-5</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20using%20histogram%20of%20oriented%20gradients%20based%20transformed%20features&amp;publication_year=2017&amp;author=M.%20Nazir&amp;author=Z.%20Jan&amp;author=M.%20Sajjad" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0155"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0160" id="ref-id-b0160" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Nigam et al., 2018</span></span></a></span><span class="reference" id="h0160"><div class="contribution"><div class="authors u-font-sans">S. Nigam, R. Singh, A.K. Misra</div><div id="ref-id-h0160" class="title text-m">Efficient facial expression recognition using histogram of oriented gradients in wavelet domain</div></div><div class="host u-font-sans">Multimed. Tools Appl. (2018), pp. 1-23</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1007/978-3-319-76348-4_1" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0160"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85044442078&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0160"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Efficient%20facial%20expression%20recognition%20using%20histogram%20of%20oriented%20gradients%20in%20wavelet%20domain&amp;publication_year=2018&amp;author=S.%20Nigam&amp;author=R.%20Singh&amp;author=A.K.%20Misra" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0160"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0165" id="ref-id-b0165" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Noh et al., 2007</span></span></a></span><span class="reference" id="h0165"><div class="contribution"><div class="authors u-font-sans">S. Noh, H. Park, Y. Jin, J. Park</div><div id="ref-id-h0165" class="title text-m">Feature-adaptive motion energy analysis for facial expression recognition</div></div><div class="host u-font-sans">Int. Symp. Vis. Comput. (2007), pp. 452-463</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1007/978-3-540-76858-6_45" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0165"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-38149122273&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0165"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Feature-adaptive%20motion%20energy%20analysis%20for%20facial%20expression%20recognition&amp;publication_year=2007&amp;author=S.%20Noh&amp;author=H.%20Park&amp;author=Y.%20Jin&amp;author=J.%20Park" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0165"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0170" id="ref-id-b0170" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Owusu et al., 2014</span></span></a></span><span class="reference" id="h0170"><div class="contribution"><div class="authors u-font-sans">E. Owusu, Y. Zhan, Q.R. Mao</div><div id="ref-id-h0170" class="title text-m">A neural-ada boost based facial expression recognition system</div></div><div class="host u-font-sans">Expert Syst. Appl., 41 (2014), pp. 3383-3390, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.eswa.2013.11.041" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.eswa.2013.11.041</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0957417413009615/pdfft?md5=e935840a49d2b856ee4b986629ae814c&amp;pid=1-s2.0-S0957417413009615-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-h0170"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0957417413009615" aria-describedby="ref-id-h0170"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84891605856&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0170"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20neural-ada%20boost%20based%20facial%20expression%20recognition%20system&amp;publication_year=2014&amp;author=E.%20Owusu&amp;author=Y.%20Zhan&amp;author=Q.R.%20Mao" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0170"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0175" id="ref-id-b0175" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Poursaberi et al., 2012</span></span></a></span><span class="reference" id="h0175"><div class="contribution"><div class="authors u-font-sans">A. Poursaberi, H.A. Noubari, M. Gavrilova, S.N. Yanushkevich</div><div id="ref-id-h0175" class="title text-m">Gauss – Laguerre wavelet textural feature fusion with geometrical information for facial expression identification</div></div><div class="host u-font-sans">EURASIP J. Image Video Process. (2012), pp. 1-13</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Gauss%20%20Laguerre%20wavelet%20textural%20feature%20fusion%20with%20geometrical%20information%20for%20facial%20expression%20identification&amp;publication_year=2012&amp;author=A.%20Poursaberi&amp;author=H.A.%20Noubari&amp;author=M.%20Gavrilova&amp;author=S.N.%20Yanushkevich" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0175"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0180" id="ref-id-b0180" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Rahul and Cherian, 2016</span></span></a></span><span class="reference" id="h0180"><div class="contribution"><div class="authors u-font-sans">R.C. Rahul, M. Cherian</div><div id="ref-id-h0180" class="title text-m">Facial expression recognition using PCA and texture-based LDN descriptor</div></div><div class="host u-font-sans">Proc. Int. Conf. Soft Comput., 113–122 (2016), <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-81-322-2674-1" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-81-322-2674-1</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20using%20PCA%20and%20texture-based%20LDN%20descriptor&amp;publication_year=2016&amp;author=R.C.%20Rahul&amp;author=M.%20Cherian" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0180"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0185" id="ref-id-b0185" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Rashid, 2016</span></span></a></span><span class="reference" id="h0185"><div class="contribution"><div class="authors u-font-sans">T.A. Rashid</div><div id="ref-id-h0185" class="title text-m">Convolutional neural networks based method for improving facial expression recognition</div></div><div class="host u-font-sans">Intell. Syst. Technol. Appl., 73–84 (2016), <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-3-319-47952-1" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-3-319-47952-1</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Convolutional%20neural%20networks%20based%20method%20for%20improving%20facial%20expression%20recognition&amp;publication_year=2016&amp;author=T.A.%20Rashid" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0185"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0190" id="ref-id-b0190" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Ryu et al., 2017</span></span></a></span><span class="reference" id="h0190"><div class="contribution"><div class="authors u-font-sans">B. Ryu, S. Member, J. Kim</div><div id="ref-id-h0190" class="title text-m">Local directional ternary pattern for facial expression recognition</div></div><div class="host u-font-sans">IEEE Trans. Image Process., 26 (2017), pp. 6006-6018, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TIP.2017.2726010" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TIP.2017.2726010</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85023622229&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0190"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Local%20directional%20ternary%20pattern%20for%20facial%20expression%20recognition&amp;publication_year=2017&amp;author=B.%20Ryu&amp;author=S.%20Member&amp;author=J.%20Kim" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0190"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0195" id="ref-id-b0195" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Salmam et al., 2016</span></span></a></span><span class="reference" id="h0195"><div class="other-ref"><span>Salmam, F.Z., Madani, A., Kissi, M., 2016. Facial Expression Recognition using Decision Trees. IEEE 13th Int. Conf. Comput. Graph. Imaging Vis., 125–130. doi: 10.1109/CGiV.2016.33.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Salmam%2C%20F.Z.%2C%20Madani%2C%20A.%2C%20Kissi%2C%20M.%2C%202016.%20Facial%20Expression%20Recognition%20using%20Decision%20Trees.%20IEEE%2013th%20Int.%20Conf.%20Comput.%20Graph.%20Imaging%20Vis.%2C%20125%E2%80%93130.%20doi%3A%2010.1109%2FCGiV.2016.33." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0200" id="ref-id-b0200" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Moos et al., 2014</span></span></a></span><span class="reference" id="h0200"><div class="contribution"><div class="authors u-font-sans">Federica Marcolin, Sandro Moos, Stefano Tornincasa, Enrico Vezzetti, Maria Grazia Violante, Giulia Fracastoro, F.P. Domenico Speranza</div><div id="ref-id-h0200" class="title text-m">Cleft lip pathology diagnosis and foetal landmark extraction via 3D geometrical analysis</div></div><div class="host u-font-sans">Int. J. Interact. Des. Manuf., 11 (2014), pp. 1-18, <a class="anchor anchor-primary" href="https://doi.org/10.1007/s12008-014-0244-1" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s12008-014-0244-1</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Cleft%20lip%20pathology%20diagnosis%20and%20foetal%20landmark%20extraction%20via%203D%20geometrical%20analysis&amp;publication_year=2014&amp;author=Federica%20Marcolin&amp;author=Sandro%20Moos&amp;author=Stefano%20Tornincasa&amp;author=Enrico%20Vezzetti&amp;author=Maria%20Grazia%20Violante&amp;author=Giulia%20Fracastoro&amp;author=F.P.%20Domenico%20Speranza" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0200"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0205" id="ref-id-b0205" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Shan et al., 2017</span></span></a></span><span class="reference" id="h0205"><div class="other-ref"><span>Shan, K., Guo, J., You, W., Lu, D., Bie, R., 2017. Automatic Facial Expression Recognition Based on a Deep Convolutional-Neural-Network Structure. IEEE 15th Int. Conf. Softw. Eng. Res. Manag. Appl. 123–128.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Shan%2C%20K.%2C%20Guo%2C%20J.%2C%20You%2C%20W.%2C%20Lu%2C%20D.%2C%20Bie%2C%20R.%2C%202017.%20Automatic%20Facial%20Expression%20Recognition%20Based%20on%20a%20Deep%20Convolutional-Neural-Network%20Structure.%20IEEE%2015th%20Int.%20Conf.%20Softw.%20Eng.%20Res.%20Manag.%20Appl.%20123%E2%80%93128." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0210" id="ref-id-b0210" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Siddiqi et al., 2015</span></span></a></span><span class="reference" id="h0210"><div class="contribution"><div class="authors u-font-sans">M.H. Siddiqi, R. Ali, A.M. Khan, Y. Park, S. Lee</div><div id="ref-id-h0210" class="title text-m">Human facial expression recognition using stepwise linear discriminant analysis and hidden conditional random fields</div></div><div class="host u-font-sans">IEEE Trans. Image Process., 24 (2015), pp. 1386-1398</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84924605599&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0210"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Human%20facial%20expression%20recognition%20using%20stepwise%20linear%20discriminant%20analysis%20and%20hidden%20conditional%20random%20fields&amp;publication_year=2015&amp;author=M.H.%20Siddiqi&amp;author=R.%20Ali&amp;author=A.M.%20Khan&amp;author=Y.%20Park&amp;author=S.%20Lee" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0210"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0215" id="ref-id-b0215" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Song et al., 2010</span></span></a></span><span class="reference" id="h0215"><div class="contribution"><div class="authors u-font-sans">M. Song, D. Tao, Z. Liu, X. Li, M. Zhou</div><div id="ref-id-h0215" class="title text-m">Image ratio features for facial expression recognition application</div></div><div class="host u-font-sans">IEEE Trans. Syst. Man Cybern. Part B Cybern., 40 (2010), pp. 779-788</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-77952581437&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0215"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Image%20ratio%20features%20for%20facial%20expression%20recognition%20application&amp;publication_year=2010&amp;author=M.%20Song&amp;author=D.%20Tao&amp;author=Z.%20Liu&amp;author=X.%20Li&amp;author=M.%20Zhou" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0215"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0220" id="ref-id-b0220" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Taylor et al., 2014</span></span></a></span><span class="reference" id="h0220"><div class="contribution"><div class="authors u-font-sans">P. Taylor, M.H. Siddiqi, R. Ali, A. Sattar, A.M. Khan, M.H. Siddiqi, R. Ali, A. Sattar, A.M. Khan, S. Lee</div><div id="ref-id-h0220" class="title text-m">Depth camera-based facial expression recognition system using multilayer scheme</div></div><div class="host u-font-sans">IETE Tech. Rev., 31 (2014), pp. 277-286, <a class="anchor anchor-primary" href="https://doi.org/10.1080/02564602.2014.944588" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1080/02564602.2014.944588</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Depth%20camera-based%20facial%20expression%20recognition%20system%20using%20multilayer%20scheme&amp;publication_year=2014&amp;author=P.%20Taylor&amp;author=M.H.%20Siddiqi&amp;author=R.%20Ali&amp;author=A.%20Sattar&amp;author=A.M.%20Khan&amp;author=M.H.%20Siddiqi&amp;author=R.%20Ali&amp;author=A.%20Sattar&amp;author=A.M.%20Khan&amp;author=S.%20Lee" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0220"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0225" id="ref-id-b0225" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Hariri et al., 2017</span></span></a></span><span class="reference" id="h0225"><div class="contribution"><div class="authors u-font-sans">Walid Hariri, Hedi Tabia, Nadir Farah, D.D. Abdallah Benouareth</div><div id="ref-id-h0225" class="title text-m">3D facial expression recognition using kernel methods on Riemannian manifold</div></div><div class="host u-font-sans">Eng. Appl. Artif. Intell., 64 (2017), pp. 25-32, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.engappai.2017.05.009" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.engappai.2017.05.009</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0952197617301033/pdfft?md5=93ccf76222e5357badbed25f28d51faf&amp;pid=1-s2.0-S0952197617301033-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-h0225"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0952197617301033" aria-describedby="ref-id-h0225"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85020879446&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0225"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=3D%20facial%20expression%20recognition%20using%20kernel%20methods%20on%20Riemannian%20manifold&amp;publication_year=2017&amp;author=Walid%20Hariri&amp;author=Hedi%20Tabia&amp;author=Nadir%20Farah&amp;author=D.D.%20Abdallah%20Benouareth" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0225"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0230" id="ref-id-b0230" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Wang et al., 2010</span></span></a></span><span class="reference" id="h0230"><div class="other-ref"><span>Wang, H., Hu, Y., Anderson, M., Rollins, P., 2010. Emotion Detection via Discriminative Kernel Method. Proc. 3rd Int. Conf. PErvasive Technol. Relat. to Assist. Environ.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Wang%2C%20H.%2C%20Hu%2C%20Y.%2C%20Anderson%2C%20M.%2C%20Rollins%2C%20P.%2C%202010.%20Emotion%20Detection%20via%20Discriminative%20Kernel%20Method.%20Proc.%203rd%20Int.%20Conf.%20PErvasive%20Technol.%20Relat.%20to%20Assist.%20Environ." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0235" id="ref-id-b0235" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Wu and Qiu, 2017</span></span></a></span><span class="reference" id="h0235"><div class="other-ref"><span>Wu, Y., Qiu, W., 2017. Facial Expression Recognition based on Improved Deep Belief Networks. AIP Conf. Proc. 1864. doi: 10.1063/1.4992947.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Wu%2C%20Y.%2C%20Qiu%2C%20W.%2C%202017.%20Facial%20Expression%20Recognition%20based%20on%20Improved%20Deep%20Belief%20Networks.%20AIP%20Conf.%20Proc.%201864.%20doi%3A%2010.1063%2F1.4992947." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0240" id="ref-id-b0240" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Yang et al., 2016</span></span></a></span><span class="reference" id="h0240"><div class="contribution"><div class="authors u-font-sans">Y. Yang, T.A. Office, D. Fang, D. Zhu, I. Science</div><div id="ref-id-h0240" class="title text-m">Facial expression recognition using deep belief network</div></div><div class="host u-font-sans">Rev. Tec. Ing. Univ. Zulia, 39 (2016), pp. 384-392</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.3390/nu8060384" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0240"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20using%20deep%20belief%20network&amp;publication_year=2016&amp;author=Y.%20Yang&amp;author=T.A.%20Office&amp;author=D.%20Fang&amp;author=D.%20Zhu&amp;author=I.%20Science" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0240"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0245" id="ref-id-b0245" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Yi Sun, 2008</span></span></a></span><span class="reference" id="h0245"><div class="contribution"><div class="authors u-font-sans">L.Y. Yi Sun</div><div id="ref-id-h0245" class="title text-m">Facial expression recognition based on 3D dynamic range model sequences</div></div><div class="host u-font-sans">Eur. Conf. Comput. Vis. (2008), pp. 58-71</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20based%20on%203D%20dynamic%20range%20model%20sequences&amp;publication_year=2008&amp;author=L.Y.%20Yi%20Sun" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0245"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0250" id="ref-id-b0250" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Zhang et al., 2017</span></span></a></span><span class="reference" id="h0250"><div class="contribution"><div class="authors u-font-sans">C. Zhang, P. Wang, K. Chen</div><div id="ref-id-h0250" class="title text-m">Identity-aware convolutional neural networks for facial expression recognition</div></div><div class="host u-font-sans">J. Syst. Eng. Electron., 28 (2017), pp. 784-792</div><div class="host u-font-sans"><a class="anchor anchor-primary" href="https://doi.org/10.21629/JSEE.2017.04.18" target="_blank"><span class="anchor-text-container"><span class="anchor-text">https://doi.org/10.21629/JSEE.2017.04.18</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85029665811&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0250"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Identity-aware%20convolutional%20neural%20networks%20for%20facial%20expression%20recognition&amp;publication_year=2017&amp;author=C.%20Zhang&amp;author=P.%20Wang&amp;author=K.%20Chen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0250"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0255" id="ref-id-b0255" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Zhang et al., 2011</span></span></a></span><span class="reference" id="h0255"><div class="contribution"><div class="authors u-font-sans">L. Zhang, S. Member, D. Tjondronegoro</div><div id="ref-id-h0255" class="title text-m">Facial expression recognition using facial movement features</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 2 (2011), pp. 219-229</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84863290657&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0255"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20using%20facial%20movement%20features&amp;publication_year=2011&amp;author=L.%20Zhang&amp;author=S.%20Member&amp;author=D.%20Tjondronegoro" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0255"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0260" id="ref-id-b0260" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Zhang et al., 2014</span></span></a></span><span class="reference" id="h0260"><div class="contribution"><div class="authors u-font-sans">L. Zhang, D. Tjondronegoro, V. Chandran</div><div id="ref-id-h0260" class="title text-m">Random Gabor based templates for facial expression recognition in images with facial occlusion</div></div><div class="host u-font-sans">Neurocomputing, 145 (2014), pp. 451-464, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.neucom.2014.05.008" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.neucom.2014.05.008</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0925231214005712/pdfft?md5=6ee8a9f7d802fe911d78b5b74ea73f7b&amp;pid=1-s2.0-S0925231214005712-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-h0260"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0925231214005712" aria-describedby="ref-id-h0260"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84906816764&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0260"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Random%20Gabor%20based%20templates%20for%20facial%20expression%20recognition%20in%20images%20with%20facial%20occlusion&amp;publication_year=2014&amp;author=L.%20Zhang&amp;author=D.%20Tjondronegoro&amp;author=V.%20Chandran" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0260"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0265" id="ref-id-b0265" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Zhao and Pietikäinen, 2009</span></span></a></span><span class="reference" id="h0265"><div class="contribution"><div class="authors u-font-sans">G. Zhao, M. Pietikäinen</div><div id="ref-id-h0265" class="title text-m">Boosted multi-resolution spatiotemporal descriptors for facial expression recognition</div></div><div class="host u-font-sans">Pattern Recognit. Lett., 30 (2009), pp. 1117-1127, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.patrec.2009.03.018" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.patrec.2009.03.018</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0167865509000695/pdfft?md5=31b870af43213edcee5a31ef1e64efbf&amp;pid=1-s2.0-S0167865509000695-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-h0265"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0167865509000695" aria-describedby="ref-id-h0265"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-67650260840&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0265"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Boosted%20multi-resolution%20spatiotemporal%20descriptors%20for%20facial%20expression%20recognition&amp;publication_year=2009&amp;author=G.%20Zhao&amp;author=M.%20Pietik%C3%A4inen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0265"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0270" id="ref-id-b0270" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Zhao and Zhang, 2016</span></span></a></span><span class="reference" id="h0270"><div class="contribution"><div class="authors u-font-sans">X. Zhao, S. Zhang</div><div id="ref-id-h0270" class="title text-m">A review on facial expression recognition : feature extraction and classification</div></div><div class="host u-font-sans">IETE Tech. Rev., 33 (2016), pp. 505-517, <a class="anchor anchor-primary" href="https://doi.org/10.1080/02564602.2015.1117403" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1080/02564602.2015.1117403</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85006427749&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0270"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20review%20on%20facial%20expression%20recognition%20%3A%20feature%20extraction%20and%20classification&amp;publication_year=2016&amp;author=X.%20Zhao&amp;author=S.%20Zhang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0270"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0275" id="ref-id-b0275" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Expression, 2017</span></span></a></span><span class="reference" id="h0275"><div class="other-ref"><span>The Japanese Female Facial Expression (JAFFE) Database, “http://www.kasrl.org/jaffe.html”, accessed on 2017.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=The%20Japanese%20Female%20Facial%20Expression%20(JAFFE)%20Database%2C%20%E2%80%9Chttp%3A%2F%2Fwww.kasrl.org%2Fjaffe.html%E2%80%9D%2C%20accessed%20on%202017." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0280" id="ref-id-b0280" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Cohn-Kanade AU-Coded Expression Database, “http://www.pitt.edu/∼emotion/ck spread.htm”, accessed on, 2017</span></span></a></span><span class="reference" id="h0280"><div class="other-ref"><span>Cohn-Kanade AU-Coded Expression Database, “<a class="anchor anchor-primary" href="http://www.pitt.edu/~emotion/ck%20spread.htm" target="_blank"><span class="anchor-text-container"><span class="anchor-text">http://www.pitt.edu/~emotion/ck spread.htm</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>”, accessed on 2017.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Cohn-Kanade%20AU-Coded%20Expression%20Database%2C%20%E2%80%9Chttp%3A%2F%2Fwww.pitt.edu%2F~emotion%2Fck%20spread.htm%E2%80%9D%2C%20accessed%20on%202017." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0285" id="ref-id-b0285" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">MMI, 2017</span></span></a></span><span class="reference" id="h0285"><div class="other-ref"><span>MMI facial expression database, “<a class="anchor anchor-primary" href="https://mmifacedb.eu/" target="_blank"><span class="anchor-text-container"><span class="anchor-text">https://mmifacedb.eu/</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>”, accessed on 2017.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=MMI%20facial%20expression%20database%2C%20%E2%80%9Chttps%3A%2F%2Fmmifacedb.eu%2F%E2%80%9D%2C%20accessed%20on%202017." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0290" id="ref-id-b0290" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Multimedia Understanding Group (MUG) Database, https://mug.ee.auth.gr/fed, accessed on, 2017</span></span></a></span><span class="reference" id="h0290"><div class="other-ref"><span>Multimedia Understanding Group (MUG) Database, <a class="anchor anchor-primary" href="https://mug.ee.auth.gr/fed" target="_blank"><span class="anchor-text-container"><span class="anchor-text">https://mug.ee.auth.gr/fed</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>, accessed on 2017.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Multimedia%20Understanding%20Group%20(MUG)%20Database%2C%20https%3A%2F%2Fmug.ee.auth.gr%2Ffed%2C%20accessed%20on%202017." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0295" id="ref-id-b0295" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Taiwanese Facial Expression Image Database (TFEID), 2017</span></span></a></span><span class="reference" id="h0295"><div class="other-ref"><span>Taiwanese Facial Expression Image Database (TFEID), http://bml.ym.edu.tw/tfeid, accessed on 2017.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Taiwanese%20Facial%20Expression%20Image%20Database%20(TFEID)%2C%20http%3A%2F%2Fbml.ym.edu.tw%2Ftfeid%2C%20accessed%20on%202017." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0300" id="ref-id-b0300" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Yale face database, 2017</span></span></a></span><span class="reference" id="h0300"><div class="other-ref"><span>Yale face database, http://vision.ucsd.edu/content/yale-face-database, accessed on 2017.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Yale%20face%20database%2C%20http%3A%2F%2Fvision.ucsd.edu%2Fcontent%2Fyale-face-database%2C%20accessed%20on%202017." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0305" id="ref-id-b0305" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">AR face database, 2018</span></span></a></span><span class="reference" id="h0305"><div class="other-ref"><span>AR face database, http://www2.ece.ohio-state.edu/∼aleix/ARdatabase.html, accessed on 2018.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=AR%20face%20database%2C%20http%3A%2F%2Fwww2.ece.ohio-state.edu%2F%E2%88%BCaleix%2FARdatabase.html%2C%20accessed%20on%202018." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0310" id="ref-id-b0310" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">The Karolinska Directed Emotional Faces (KDEF) database, 2018</span></span></a></span><span class="reference" id="h0310"><div class="other-ref"><span>The Karolinska Directed Emotional Faces (KDEF) database, http://www.emotionlab.se/kdef/, accessed on 2018.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=The%20Karolinska%20Directed%20Emotional%20Faces%20(KDEF)%20database%2C%20http%3A%2F%2Fwww.emotionlab.se%2Fkdef%2F%2C%20accessed%20on%202018." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li></ol></section></section></div><div id="section-cited-by"><section aria-label="Cited by" class="ListArticles preview"><div class="PageDivider"></div><header id="citing-articles-header"><h2 class="u-h4 u-margin-l-ver u-font-serif">Cited by (178)</h2></header><div aria-describedby="citing-articles-header"><div class="citing-articles u-margin-l-bottom"><ul><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-0-title"><a class="anchor anchor-primary" href="/science/article/pii/S1110016823000327"><span class="anchor-text-container"><span class="anchor-text">A comprehensive survey on deep facial expression recognition: challenges, applications, and future guidelines</span></span></a></h3><div>2023, Alexandria Engineering Journal</div></div><div class="buttons"><button class="button-link button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby="citing-articles-article-0-title" aria-controls="citing-articles-article-0" aria-expanded="false" type="button"><span class="button-link-text-container"><span class="button-link-text">Show abstract</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"><div class="u-margin-ver-m"><div class="u-margin-s-bottom" id="sp0005">Facial expression recognition (FER) is an emerging and multifaceted research topic. Applications of FER in healthcare, security, safe driving, and so forth have contributed to the credibility of these methods and their adoption in human-computer interaction for intelligent outcomes. Computational FER mimics human facial expression coding skills and conveys important cues that complement speech to assist listeners. Similarly, FER methods based on deep learning and artificial intelligence (AI) techniques have been developed with edge modules to ensure efficiency and real-time processing. To this end, numerous studies have explored different aspects of FER. Surveys of FER have focused on the literature on hand-crafted techniques, with a focus on general methods for local servers but largely neglecting edge vision-inspired deep learning and AI-based FER technologies. To consider these missing aspects, in this study, the existing literature on FER is thoroughly analyzed and surveyed, and the working flow of FER methods, their integral and intermediate steps, and pattern structures are highlighted. Further, the limitations in existing FER surveys are discussed. Next, FER datasets are investigated in depth, and the associated challenges and problems are discussed. In contrast to existing surveys, FER methods are considered for edge vision (on e.g., smartphone or Raspberry Pi, devices, etc.), and different measures to evaluate the performance of FER methods are comprehensively discussed. Finally, recommendations and some avenues for future research are suggested to facilitate further development and implementation of FER technologies.</div></div></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-1-title"><a class="anchor anchor-primary" href="/science/article/pii/S0020025521010136"><span class="anchor-text-container"><span class="anchor-text">A survey on facial emotion recognition techniques: A state-of-the-art literature review</span></span></a></h3><div>2022, Information Sciences</div></div><div class="buttons"><button class="button-link button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby="citing-articles-article-1-title" aria-controls="citing-articles-article-1" aria-expanded="false" type="button"><span class="button-link-text-container"><span class="button-link-text">Show abstract</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"><div class="u-margin-ver-m"><div class="u-margin-s-bottom" id="sp005">In this survey, a systematic literature review of the state-of-the-art on emotion expression recognition from facial images is presented. The paper has as main objective arise the most commonly used strategies employed to interpret and recognize facial emotion expressions, published over the past few years. For this purpose, a total of 51 papers were analyzed over the literature totaling 94 distinct methods, collected from well-established scientific databases (ACM Digital Library, IEEE Xplore, Science Direct and Scopus), whose works were categorized according to its main construction concept. From the analyzed works, it was possible to categorize them into two main trends: classical and those approaches specifically designed by the use of neural networks. The obtained statistical analysis demonstrated a marginally better recognition precision for the classical approaches when faced to neural networks counterpart, but with a reduced capacity of generalization. Additionally, the present study verified the most popular datasets for facial expression and emotion recognition showing the pros and cons each and, thereby, demonstrating a real demand for reliable data-sources regarding artificial and natural experimental environments.</div></div></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-2-title"><a class="anchor anchor-primary" href="/science/article/pii/S1568494621005445"><span class="anchor-text-container"><span class="anchor-text">WOA-TLBO: Whale optimization algorithm with Teaching-learning-based optimization for global optimization and facial emotion recognition</span></span></a></h3><div>2021, Applied Soft Computing</div><div class="CitedSection u-margin-s-top"><div class="u-margin-s-left"><div class="cite-header u-text-italic u-font-sans">Citation Excerpt :</div><p class="u-font-serif text-xs">The challenges are primarily due to interpersonal differences, the subtlety of facial expressions, posture, and illumination, etc. Facial Emotion Recognition  [1,2] has become an increasingly important research area that involves many applications such as human–computer interaction (HCI), driver safety, healthcare, deceive detection and security, etc. Currently, FER in pictures has intrigued rising consideration, which is for models further convoluted due to low-resolution faces and backgrounds.</p></div></div></div><div class="buttons"><button class="button-link button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby="citing-articles-article-2-title" aria-controls="citing-articles-article-2" aria-expanded="false" type="button"><span class="button-link-text-container"><span class="button-link-text">Show abstract</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"><div class="u-margin-ver-m"><div class="u-margin-s-bottom" id="d1e10533">The Whale Optimization Algorithm (WOA) is a recently developed algorithm that is based on the chasing mechanism of humpback whales. Benefiting from the unique structure, WOA has virtuous global search capability. One of the drawbacks of this algorithm is the slow convergence rate that limits its real-world application. In resolving complicated global optimization problems, without any exertion for adequate fine-tuning preliminary constraints, Teaching-learning-based optimization (TLBO) is smooth to plunge into local optimal, but it has a fast convergence speed. By given the features of WOA and TLBO, an active hybrid WOA-TLBO algorithm is proposed for resolving optimization difficulties. To explore the enactment of the proposed WOA-TLBO algorithm, several experimentations are accompanied by regular benchmark test functions and compared with six other algorithms. The investigational outcomes indicate the more magnificent concert of the proposed WOA-TLBO algorithm for the benchmark function results. The proposed method has also been applied to the Facial Emotion Recognition (FER) functional problem. FER is the thought-provoking investigation zone that empowers us to classify the expression of the human face in everyday life. Centered on the portions’ actions in the human face, the maximum of the standard approaches fail to distinguish the expressions precisely as the expressions. In this paper, we have proposed FER’s productive process using WOA-TLBO based MultiSVNN (Multi-Support Vector Neural Network). Investigational outcomes deliver an indication of the virtuous enactment of the proposed technique resolutions in terms of accurateness.</div></div></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-3-title"><a class="anchor anchor-primary" href="https://doi.org/10.1049/ipr2.12700" target="_blank"><span class="anchor-text-container"><span class="anchor-text">Automatic facial expression recognition combining texture and shape features from prominent facial regions</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></h3><div>2023, IET Image Processing</div></div><div class="buttons"></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-4-title"><a class="anchor anchor-primary" href="https://doi.org/10.1109/TIM.2023.3243661" target="_blank"><span class="anchor-text-container"><span class="anchor-text">Understanding Deep Learning Techniques for Recognition of Human Emotions Using Facial Expressions: A Comprehensive Survey</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></h3><div>2023, IEEE Transactions on Instrumentation and Measurement</div></div><div class="buttons"></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-5-title"><a class="anchor anchor-primary" href="https://doi.org/10.1109/ACCESS.2021.3131733" target="_blank"><span class="anchor-text-container"><span class="anchor-text">A Survey of AI-Based Facial Emotion Recognition: Features, ML DL Techniques, Age-Wise Datasets and Future Directions</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></h3><div>2021, IEEE Access</div></div><div class="buttons"></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"></div></div></li></ul><a class="button-alternative button-alternative-secondary large-alternative button-alternative-icon-left" href="http://www.scopus.com/scopus/inward/citedby.url?partnerID=10&amp;rel=3.0.0&amp;eid=2-s2.0-85052997066&amp;md5=6968597482783fd145d9d33b7c5e491" target="_blank" id="citing-articles-view-all-btn"><svg focusable="false" viewBox="0 0 54 128" height="20" class="icon icon-navigate-right"><path d="M1 99l38-38L1 23l7-7 45 45-45 45z"></path></svg><span class="button-alternative-text-container"><span class="button-alternative-text">View all citing articles on Scopus</span></span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></a></div></div></section></div><div class="Footnotes text-xs"><dl class="footnote"><dt class="footnote-label"></dt><dd class="footnote-detail u-padding-xs-top"><div class="u-margin-s-bottom" id="np005">Peer review under responsibility of King Saud University.</div></dd></dl></div><div class="Copyright"><span class="copyright-line">© 2018 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University.</span></div></article><div class="u-display-block-from-md col-lg-6 col-md-8 pad-right u-padding-s-top"><aside class="RelatedContent u-clr-grey8" aria-label="Related content"><section class="RelatedContentPanel u-margin-s-bottom"><header id="recommended-articles-header" class="related-content-panel-header u-margin-s-bottom"><button class="button-link button-link-secondary related-content-panel-toggle is-up button-link-icon-right button-link-has-colored-icon" aria-expanded="true" data-aa-button="sd:product:journal:article:location=recommended-articles:type=close" type="button"><span class="button-link-text-container"><span class="button-link-text"><h2 class="section-title u-h4"><span class="related-content-panel-title-text">Recommended articles</span></h2></span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class="" aria-hidden="false" aria-describedby="recommended-articles-header"><div id="recommended-articles" class="text-xs"><ul><li class="RelatedContentPanelItem u-display-block"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article0-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0167865517302313" title="Multi angle optimal pattern-based deep learning for automatic facial expression recognition"><span class="anchor-text-container"><span class="anchor-text"><span>Multi angle optimal pattern-based deep learning for automatic facial expression recognition</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Pattern Recognition Letters, Volume 139, 2020, pp. 157-165</div></div><div class="authors"><span>Deepak Kumar</span> <span>Jain</span>, …, <span>Kaiqi</span> <span>Huang</span></div></div><div class="buttons"></div></li><li class="RelatedContentPanelItem u-display-block"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article1-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0923596520300540" title="Human emotion recognition by optimally fusing facial expression and speech feature"><span class="anchor-text-container"><span class="anchor-text"><span>Human emotion recognition by optimally fusing facial expression and speech feature</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Signal Processing: Image Communication, Volume 84, 2020, Article 115831</div></div><div class="authors"><span>Xusheng</span> <span>Wang</span>, …, <span>Congjun</span> <span>Cao</span></div></div><div class="buttons"></div></li><li class="RelatedContentPanelItem u-display-block"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article2-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S1319157817303981" title="On development of novel hybrid and robust adaptive models for net asset value prediction"><span class="anchor-text-container"><span class="anchor-text"><span>On development of novel hybrid and robust adaptive models for net asset value prediction</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Journal of King Saud University - Computer and Information Sciences, Volume 33, Issue 6, 2021, pp. 647-657</div></div><div class="authors"><span>Babita</span> <span>Majhi</span>, …, <span>Ritanjali</span> <span>Majhi</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1319157817303981/pdfft?md5=c8746d5fa7222188d4c215684094149c&amp;pid=1-s2.0-S1319157817303981-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article2-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li><li class="RelatedContentPanelItem u-display-none"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article3-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0925231219306137" title="Three convolutional neural network models for facial expression recognition in the wild"><span class="anchor-text-container"><span class="anchor-text"><span>Three convolutional neural network models for facial expression recognition in the wild</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Neurocomputing, Volume 355, 2019, pp. 82-92</div></div><div class="authors"><span>Jie</span> <span>Shao</span>, <span>Yongsheng</span> <span>Qian</span></div></div><div class="buttons"></div></li><li class="RelatedContentPanelItem u-display-none"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article4-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0167865522000204" title="CERN: Compact facial expression recognition net"><span class="anchor-text-container"><span class="anchor-text"><span>CERN: Compact facial expression recognition net</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Pattern Recognition Letters, Volume 155, 2022, pp. 9-18</div></div><div class="authors"><span>Darshan</span> <span>Gera</span>, …, <span>Anwesh</span> <span>Jami</span></div></div><div class="buttons"></div></li><li class="RelatedContentPanelItem u-display-none"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article5-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0167865517302271" title="Facial expressions classification and false label reduction using LDA and threefold SVM"><span class="anchor-text-container"><span class="anchor-text"><span>Facial expressions classification and false label reduction using LDA and threefold SVM</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Pattern Recognition Letters, Volume 139, 2020, pp. 166-173</div></div><div class="authors"><span>Jamal Hussain</span> <span>Shah</span>, …, <span>Steven Lawrence</span> <span>Fernandes</span></div></div><div class="buttons"></div></li></ul></div><button class="button-link more-recommendations-button u-margin-s-bottom button-link-primary button-link-icon-right" type="button"><span class="button-link-text-container"><span class="button-link-text">Show 3 more articles</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div></section><section class="RelatedContentPanel u-margin-s-bottom"><header id="metrics-header" class="related-content-panel-header u-margin-s-bottom"><button class="button-link button-link-secondary related-content-panel-toggle is-up button-link-icon-right button-link-has-colored-icon" aria-expanded="true" type="button"><span class="button-link-text-container"><span class="button-link-text"><h2 class="section-title u-h4"><span class="related-content-panel-title-text">Article Metrics</span></h2></span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class="" aria-hidden="false" aria-describedby="metrics-header"><div class="plumX-metrics"><h3 class="text-s metric-title metric-title-citations">Citations</h3><ul><li class="text-xs metrics"><span>Citation Indexes</span><span>176</span></li></ul><h3 class="text-s metric-title metric-title-captures">Captures</h3><ul><li class="text-xs metrics"><span>Readers</span><span>435</span></li></ul><h3 class="text-s metric-title metric-title-mentions">Mentions</h3><ul><li class="text-xs metrics"><span>Blog Mentions</span><span>1</span></li></ul><h3 class="text-s metric-title metric-title-social-media">Social Media</h3><ul><li class="text-xs metrics"><span>Shares, Likes &amp; Comments</span><span>7</span></li></ul><div class="metrics u-margin-m-top u-margin-s-bottom"><img src="https://cdn.plu.mx/3ba727faf225e19d2c759f6ebffc511d/plumx-logo.png" class="plumX-logo" alt="PlumX Metrics Logo"><a class="anchor text-xs anchor-primary" href="https://plu.mx/plum/a/?doi=10.1016%2Fj.jksuci.2018.09.002&amp;theme=plum-sciencedirect-theme&amp;hideUsage=true" target="_blank"><span class="anchor-text-container"><span class="anchor-text">View details</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></div></div></section></aside></div></div></div></div><footer role="contentinfo" class="els-footer u-bg-white text-xs u-padding-s-hor u-padding-m-hor-from-sm u-padding-l-hor-from-md u-padding-l-ver u-margin-l-top u-margin-xl-top-from-sm u-margin-l-top-from-md"><div class="els-footer-elsevier u-margin-m-bottom u-margin-0-bottom-from-md u-margin-s-right u-margin-m-right-from-md u-margin-l-right-from-lg"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="https://www.elsevier.com/" target="_blank" aria-label="Elsevier home page (opens in a new tab)" rel="nofollow"><img class="footer-logo" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/47/images/elsevier-non-solus-new-with-wordmark.svg" alt="Elsevier logo with wordmark" height="64" width="58" loading="lazy"></a></div><div class="els-footer-content"><div class="u-remove-if-print"><ul class="els-footer-links u-margin-xs-bottom" style="list-style:none"><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsevier.com/solutions/sciencedirect" target="_blank" id="els-footer-about-science-direct" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">About ScienceDirect</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="/user/institution/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS1319157818303379" id="els-footer-remote-access" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Remote access</span></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://sd-cart.elsevier.com/?" target="_blank" id="els-footer-shopping-cart" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Shopping cart</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsmediakits.com" target="_blank" id="els-footer-advertise" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Advertise</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://service.elsevier.com/app/contact/supporthub/sciencedirect/" target="_blank" id="els-footer-contact-support" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Contact and support</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsevier.com/legal/elsevier-website-terms-and-conditions" target="_blank" id="els-footer-terms-condition" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Terms and conditions</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsevier.com/legal/privacy-policy" target="_blank" id="els-footer-privacy-policy" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Privacy policy</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li></ul></div><p id="els-footer-cookie-message" class="u-remove-if-print">Cookies are used by this site. <!-- --> <button class="button-link ot-sdk-show-settings cookie-btn button-link-primary button-link-small" id="ot-sdk-btn" type="button">Cookie Settings</button></p><p id="els-footer-copyright">All content on this site: Copyright © <!-- -->2024<!-- --> Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the Creative Commons licensing terms apply.</p></div><div class="els-footer-relx u-margin-0-top u-margin-m-top-from-xs u-margin-0-top-from-md"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="https://www.relx.com/" target="_blank" aria-label="RELX home page (opens in a new tab)" id="els-footer-relx" rel="nofollow"><img loading="lazy" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/60/images/logo-relx-tm.svg" width="93" height="20" alt="RELX group home page"></a></div></footer></div></div></div></div>
      <div id="floating-ui-node" class="floating-ui-node" data-sd-ui-floating-ui="true"></div>
      
      <script src="https://assets.adobedtm.com/4a848ae9611a/032db4f73473/launch-a6263b31083f.min.js" type="image/ot-performance" async=""></script>
      
<script type="text/javascript">
    window.pageData = {"content":[{"contentType":"JL","format":"MIME-XHTML","id":"sd:article:pii:S1319157818303379","type":"sd:article:JL:scope-full","detail":"sd:article:subtype:rev","publicationType":"journal","issn":"1319-1578","volumeNumber":"33","issueNumber":"6","provider":"elsevier","entitlementType":"openaccess"}],"page":{"businessUnit":"ELS:RP:ST","language":"en","name":"product:journal:article","noTracking":"false","productAppVersion":"full-direct","productName":"SD","type":"CP-CA","environment":"prod","loadTimestamp":1734890144361,"loadTime":"","testId":"sd:qna-article-page:beta-test"},"visitor":{"accessType":"ae:ANON_GUEST","accountId":"ae:228598","accountName":"ae:ScienceDirect Guests","loginStatus":"anonymous","userId":"ae:12975512","ipAddress":"77.165.246.201","appSessionId":"818b1109-3100-4058-889f-9b05b4f71390"}};
    window.pageData.page.loadTime = performance ? Math.round(performance.now()).toString() : '';

    try {
      appData.push({
      event: 'pageLoad',
      page: pageData.page,
      visitor: pageData.visitor,
      content: pageData.content
      })
    } catch(e) {
        console.warn("There was an error loading or running Adobe DTM: ", e);
    }
</script>
      <script nomodule="" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/73/js/core-js/3.20.2/core-js.es.minified.js" type="text/javascript"></script>
      <script src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/108/js/react/18.3.1/react.production.min.js" type="text/javascript"></script>
      <script src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/108/js/react-dom/18.3.1/react-dom.production.min.js" type="text/javascript"></script>
      <script async="" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/b64013ec63c69e3d916174cbebae89d65b2419e1/arp.js" type="text/javascript"></script>
      <script type="text/javascript">
    const pendoData = {"visitor":{"pageName":"SD:product:journal:article","pageType":"CP-CA","pageProduct":"SD","pageLanguage":"en","pageEnvironment":"prod","accessType":"ae:ANON_GUEST","countryCode":"NL"},"account":{"id":"ae:228598","name":"ae:ScienceDirect Guests"},"events":{}};;
    pendoData.events = {
      ready: function () {
        pendo.addAltText();
      },
    };
    function runPendo(data, options) {
  const {
    firstDelay,
    maxRetries,
    urlPrefix,
    urlSuffix,
    apiKey
  } = options;
  (function (apiKey) {
    (function (p, e, n, d, o) {
      var v, w, x, y, z;
      o = p[d] = p[d] || {};
      o._q = [];
      v = ['initialize', 'identify', 'updateOptions', 'pageLoad'];
      for (w = 0, x = v.length; w < x; ++w) (function (m) {
        o[m] = o[m] || function () {
          o._q[m === v[0] ? 'unshift' : 'push']([m].concat([].slice.call(arguments, 0)));
        };
      })(v[w]);
      y = e.createElement(n);
      y.async = !0;
      y.src = urlPrefix + apiKey + urlSuffix;
      z = e.getElementsByTagName(n)[0];
      z.parentNode.insertBefore(y, z);
    })(window, document, 'script', 'pendo');
    pendo.addAltText = function () {
      var target = document.querySelector('body');
      var observer = new MutationObserver(function (mutations) {
        mutations.forEach(function (mutation) {
          if (mutation?.addedNodes?.length) {
            if (mutation.addedNodes[0]?.className?.includes("_pendo-badge")) {
              const badge = mutation.addedNodes[0];
              const altText = badge?.attributes['aria-label'].value ? badge?.attributes['aria-label'].value : 'Feedback';
              const pendoBadgeImage = pendo.dom(`#${badge?.attributes?.id.value} img`);
              if (pendoBadgeImage.length) {
                pendoBadgeImage[0]?.setAttribute('alt', altText);
              }
            }
          }
        });
      });
      var config = {
        attributeFilter: ['data-layout'],
        attributes: true,
        childList: true,
        characterData: true,
        subtree: false
      };
      observer.observe(target, config);
    };
  })(apiKey);
  (function watchAndSetPendo(nextDelay, retryAttempt) {
    if (typeof pageDataTracker === 'object' && typeof pageDataTracker.getVisitorId === 'function' && pageDataTracker.getVisitorId()) {
      data.visitor.id = pageDataTracker.getVisitorId();
      console.debug(`initializing pendo`);
      pendo.initialize(data);
    } else {
      if (retryAttempt > 0) {
        return setTimeout(function () {
          watchAndSetPendo(nextDelay * 2, retryAttempt - 1);
        }, nextDelay);
      }
      pendo.initialize(data);
      console.debug(`gave up ... pendo initialized`);
    }
  })(firstDelay, maxRetries);
}
    runPendo(pendoData, {
      firstDelay: 100,
      maxRetries: 5,
      urlPrefix: 'https://cdn.pendo.io/agent/static/',
      urlSuffix: '/pendo.js',
      apiKey: 'd6c1d995-bc7e-4e53-77f1-2ea4ecbb9565',
    });
  </script>
      <span id="pendo-answer-rating"></span>
      <script type="text/x-mathjax-config;executed=true">
        MathJax.Hub.Config({
          displayAlign: 'left',
          "fast-preview": {
            disabled: true
          },
          CommonHTML: { linebreaks: { automatic: true } },
          PreviewHTML: { linebreaks: { automatic: true } },
          'HTML-CSS': { linebreaks: { automatic: true } },
          SVG: {
            scale: 90,
            linebreaks: { automatic: true }
          }
        });
      </script>
      <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=MML_SVG" type="text/javascript"></script>
      <script async="" src="https://www.googletagservices.com/tag/js/gpt.js" type="text/javascript"></script>
      <script async="" src="https://scholar.google.com/scholar_js/casa.js" type="text/javascript"></script>
      <script data-cfasync="false">
      (function initOneTrust()  {
        const monitor = {
  init: () => {},
  loaded: () => {},
};
        function enableGroup(group) {
  document.querySelectorAll(`script[type*="ot-${group}"]`).forEach(script => {
    script.type = 'text/javascript';
    document.head.appendChild(script);
  });
}
        function runOneTrustCookies(doClear, monitor) {
  const oneTrustConsentSdkId = 'onetrust-consent-sdk';
  const emptyNodeSelectors = 'h3.ot-host-name, h4.ot-host-desc, button.ot-host-box';
  const ariaLabelledByButtonNodes = 'div.ot-accordion-layout > button';
  const ariaAttribute = 'aria-labelledby';
  function adjustOneTrustDOM() {
    const oneTrustRoot = document.getElementById('onetrust-consent-sdk');

    /* remove empty nodes */
    [...(oneTrustRoot?.querySelectorAll(emptyNodeSelectors) ?? [])].filter(e => e.textContent === '').forEach(e => e.remove());

    /* remove invalid aria-labelledby values */
    oneTrustRoot?.querySelectorAll(ariaLabelledByButtonNodes).forEach(e => {
      const presentIdValue = e.getAttribute(ariaAttribute)?.split(' ').filter(label => document.getElementById(label)).join(' ');
      if (presentIdValue) {
        e.setAttribute(ariaAttribute, presentIdValue);
      }
    });
  }
  function observeOneTrustLoaded(shouldSetOTDefaults, isConsentPresent) {
    const cb = (mutationList, observer) => {
      const oneTrustRoot = mutationList.filter(mutationRecord => mutationRecord.type === 'childList' && mutationRecord.addedNodes.length).map(mutationRecord => [...mutationRecord.addedNodes]).flat().find(e => e.id === oneTrustConsentSdkId);
      if (oneTrustRoot && typeof OneTrust !== 'undefined') {
        monitor.loaded(true);
        OneTrust.OnConsentChanged(() => {
          const perfAllowed = decodeURIComponent(document.cookie.match('(^| )OptanonConsent=([^;]+)')?.[2])?.match('groups=([0-9:0|1,?]+)&?')?.[1]?.match('2:([0|1])')[1] === '1';
          if (perfAllowed) {
            enableGroup('performance');
          }
        });
        if (!isConsentPresent && (shouldSetOTDefaults || OneTrust.GetDomainData().ConsentModel.Name === 'implied consent')) {
          OneTrust.AllowAll();
        }
        document.dispatchEvent(new CustomEvent('@sdtech/onetrust/loaded', {}));
        observer.disconnect();
        adjustOneTrustDOM();
      }
    };
    const observer = new MutationObserver(cb);
    observer.observe(document.querySelector('body'), {
      childList: true
    });
  }
  if (doClear) {
    document.cookie = 'OptanonAlertBoxClosed=; expires=Thu, 01 Jan 1970 00:00:00 UTC; samesite=lax; path=/';
  }
  const isConsentPresent = !!decodeURIComponent(document.cookie.match('(^| )OptanonConsent=([^;]+)')?.[2])?.match('groups=([0-9:0|1,?]+)&?')?.[1];
  const shouldSetOTDefaults = 'true' === 'false' && !document.cookie?.match('OptanonAlertBoxClosed=');
  if (shouldSetOTDefaults) {
    const date = new Date();
    date.setFullYear(date.getFullYear() + 1);
    document.cookie = `OptanonAlertBoxClosed=${new Date().toISOString()}; expires=${date.toUTCString()}; samesite=lax; path=/; domain=sciencedirect.com`;
  }
  observeOneTrustLoaded(shouldSetOTDefaults, isConsentPresent, monitor);
  window.addOTScript = () => {
    const otSDK = document.createElement('script');
    otSDK.setAttribute('data-cfasync', 'false');
    otSDK.setAttribute('src', 'https://cdn.cookielaw.org/scripttemplates/otSDKStub.js');
    otSDK.setAttribute('data-document-language', 'true');
    otSDK.setAttribute('data-domain-script', '865ea198-88cc-4e41-8952-1df75d554d02');
    window.addOTScript = () => {};
    document.head.appendChild(otSDK);
    monitor.init();
  };
  window.addEventListener('load', () => window.addOTScript());
}
        if (document.location.host.match(/.sciencedirect.com$/)) {
          runOneTrustCookies(true, monitor);
        }
        else {
          window.addEventListener('load', (event) => {
            enableGroup('performance');
          });
        }
      }());
    </script>
    
  <iframe id="ps_nwsxip" src="https://service.seamlessaccess.org/ps/" style="display: none; position: absolute; top: -999px; left: -999px;"></iframe><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div id="onetrust-consent-sdk"><div class="onetrust-pc-dark-filter ot-fade-in" style="z-index:2147483645;"></div><div id="onetrust-banner-sdk" class="otFlat bottom ot-wo-title ot-buttons-fw" tabindex="0" role="region" aria-label="Cookie banner"><div role="alertdialog" aria-describedby="onetrust-policy-text" aria-modal="true" aria-label="Privacy"><div class="ot-sdk-container"><div class="ot-sdk-row"><div id="onetrust-group-container" class="ot-sdk-eight ot-sdk-columns"><div class="banner_logo"></div><div id="onetrust-policy"><div id="onetrust-policy-text">We use cookies that are necessary to make our site work. We may also use additional cookies to analyze, improve, and personalize our content and your digital experience. For more information, see our<a class="ot-cookie-policy-link" href="https://www.elsevier.com/legal/cookienotice" target="_blank" aria-label=", opens in a new tab" rel="noopener">Cookie Policy</a></div></div></div><div id="onetrust-button-group-parent" class="ot-sdk-three ot-sdk-columns"><div id="onetrust-button-group"><button id="onetrust-pc-btn-handler" class="cookie-setting-link">Cookie Settings</button>  <button id="onetrust-accept-btn-handler">Accept all cookies</button></div></div></div></div><!-- Close Button --><div id="onetrust-close-btn-container"></div><!-- Close Button END--></div></div><div id="onetrust-pc-sdk" class="otPcCenter ot-hide ot-fade-in" lang="en" aria-label="Preference center" role="region"><div role="alertdialog" aria-modal="true" aria-describedby="ot-pc-desc" style="height: 100%;" aria-label="Cookie Preference Center"><!-- Close Button --><div class="ot-pc-header"><!-- Logo Tag --><div class="ot-pc-logo" role="img" aria-label="Company Logo"><img alt="Company Logo" src="https://cdn.cookielaw.org/logos/static/ot_company_logo.png"></div></div><!-- Close Button --><div id="ot-pc-content" class="ot-pc-scrollbar"><div class="ot-optout-signal ot-hide"><div class="ot-optout-icon"><svg xmlns="http://www.w3.org/2000/svg"><path class="ot-floating-button__svg-fill" d="M14.588 0l.445.328c1.807 1.303 3.961 2.533 6.461 3.688 2.015.93 4.576 1.746 7.682 2.446 0 14.178-4.73 24.133-14.19 29.864l-.398.236C4.863 30.87 0 20.837 0 6.462c3.107-.7 5.668-1.516 7.682-2.446 2.709-1.251 5.01-2.59 6.906-4.016zm5.87 13.88a.75.75 0 00-.974.159l-5.475 6.625-3.005-2.997-.077-.067a.75.75 0 00-.983 1.13l4.172 4.16 6.525-7.895.06-.083a.75.75 0 00-.16-.973z" fill="#FFF" fill-rule="evenodd"></path></svg></div><span></span></div><h2 id="ot-pc-title">Cookie Preference Center</h2><div id="ot-pc-desc">We use cookies which are necessary to make our site work. We may also use additional cookies to analyse, improve and personalise our content and your digital experience. For more information, see our <a href="https://www.elsevier.com/legal/cookienotice/_nocache" target="_blank">Cookie Policy</a> and the list of <a href="https://support.google.com/admanager/answer/9012903" target="_blank">Google Ad-Tech Vendors</a>.
<br>
<br>
You may choose not to allow some types of cookies. However, blocking some types may impact your experience of our site and the services we are able to offer. See the different category headings below to find out more or change your settings.
<br>
</div><button id="accept-recommended-btn-handler">Allow all</button><section class="ot-sdk-row ot-cat-grp"><h3 id="ot-category-title"> Manage Consent Preferences</h3><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="1"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-1" aria-labelledby="ot-header-id-1 ot-status-id-1"></button><!-- Accordion header --><div class="ot-acc-hdr ot-always-active-group"><div class="ot-plus-minus"><span></span><span></span></div><h4 class="ot-cat-header" id="ot-header-id-1">Strictly Necessary Cookies</h4><div id="ot-status-id-1" class="ot-always-active">Always active</div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-1">These cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.
<br><br></p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" aria-label="Cookie Details List" data-parent-id="1">Cookie Details List‎</button></div></div></div><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="3"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-3" aria-labelledby="ot-header-id-3"></button><!-- Accordion header --><div class="ot-acc-hdr"><div class="ot-plus-minus"><span></span><span></span></div><h4 class="ot-cat-header" id="ot-header-id-3">Functional Cookies</h4><div class="ot-tgl"><input type="checkbox" name="ot-group-id-3" id="ot-group-id-3" role="switch" class="category-switch-handler" data-optanongroupid="3" aria-labelledby="ot-header-id-3"> <label class="ot-switch" for="ot-group-id-3"><span class="ot-switch-nob" aria-checked="false" role="switch" aria-label="Functional Cookies"></span> <span class="ot-label-txt">Functional Cookies</span></label> </div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-3">These cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.</p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" aria-label="Cookie Details List" data-parent-id="3">Cookie Details List‎</button></div></div></div><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="2"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-2" aria-labelledby="ot-header-id-2"></button><!-- Accordion header --><div class="ot-acc-hdr"><div class="ot-plus-minus"><span></span><span></span></div><h4 class="ot-cat-header" id="ot-header-id-2">Performance Cookies</h4><div class="ot-tgl"><input type="checkbox" name="ot-group-id-2" id="ot-group-id-2" role="switch" class="category-switch-handler" data-optanongroupid="2" aria-labelledby="ot-header-id-2"> <label class="ot-switch" for="ot-group-id-2"><span class="ot-switch-nob" aria-checked="false" role="switch" aria-label="Performance Cookies"></span> <span class="ot-label-txt">Performance Cookies</span></label> </div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-2">These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.</p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" aria-label="Cookie Details List" data-parent-id="2">Cookie Details List‎</button></div></div></div><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="4"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-4" aria-labelledby="ot-header-id-4"></button><!-- Accordion header --><div class="ot-acc-hdr"><div class="ot-plus-minus"><span></span><span></span></div><h4 class="ot-cat-header" id="ot-header-id-4">Targeting Cookies</h4><div class="ot-tgl"><input type="checkbox" name="ot-group-id-4" id="ot-group-id-4" role="switch" class="category-switch-handler" data-optanongroupid="4" aria-labelledby="ot-header-id-4"> <label class="ot-switch" for="ot-group-id-4"><span class="ot-switch-nob" aria-checked="false" role="switch" aria-label="Targeting Cookies"></span> <span class="ot-label-txt">Targeting Cookies</span></label> </div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-4">These cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. If you do not allow these cookies, you will experience less targeted advertising.</p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" aria-label="Cookie Details List" data-parent-id="4">Cookie Details List‎</button></div></div></div><!-- Groups sections starts --><!-- Group section ends --><!-- Accordion Group section starts --><!-- Accordion Group section ends --></section></div><section id="ot-pc-lst" class="ot-hide ot-hosts-ui ot-pc-scrollbar"><div id="ot-pc-hdr"><div id="ot-lst-title"><button class="ot-link-btn back-btn-handler" aria-label="Back"><svg id="ot-back-arw" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 444.531 444.531" xml:space="preserve"><title>Back Button</title><g><path fill="#656565" d="M213.13,222.409L351.88,83.653c7.05-7.043,10.567-15.657,10.567-25.841c0-10.183-3.518-18.793-10.567-25.835
                    l-21.409-21.416C323.432,3.521,314.817,0,304.637,0s-18.791,3.521-25.841,10.561L92.649,196.425
                    c-7.044,7.043-10.566,15.656-10.566,25.841s3.521,18.791,10.566,25.837l186.146,185.864c7.05,7.043,15.66,10.564,25.841,10.564
                    s18.795-3.521,25.834-10.564l21.409-21.412c7.05-7.039,10.567-15.604,10.567-25.697c0-10.085-3.518-18.746-10.567-25.978
                    L213.13,222.409z"></path></g></svg></button><h3>Cookie List</h3></div><div class="ot-lst-subhdr"><div class="ot-search-cntr"><p role="status" class="ot-scrn-rdr"></p><input id="vendor-search-handler" type="text" name="vendor-search-handler" placeholder="Search…" aria-label="Cookie list search"> <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 -30 110 110" aria-hidden="true"><title>Search Icon</title><path fill="#2e3644" d="M55.146,51.887L41.588,37.786c3.486-4.144,5.396-9.358,5.396-14.786c0-12.682-10.318-23-23-23s-23,10.318-23,23
            s10.318,23,23,23c4.761,0,9.298-1.436,13.177-4.162l13.661,14.208c0.571,0.593,1.339,0.92,2.162,0.92
            c0.779,0,1.518-0.297,2.079-0.837C56.255,54.982,56.293,53.08,55.146,51.887z M23.984,6c9.374,0,17,7.626,17,17s-7.626,17-17,17
            s-17-7.626-17-17S14.61,6,23.984,6z"></path></svg></div><div class="ot-fltr-cntr"><button id="filter-btn-handler" aria-label="Filter" aria-haspopup="true"><svg role="presentation" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 402.577 402.577" xml:space="preserve"><title>Filter Icon</title><g><path fill="#fff" d="M400.858,11.427c-3.241-7.421-8.85-11.132-16.854-11.136H18.564c-7.993,0-13.61,3.715-16.846,11.136
      c-3.234,7.801-1.903,14.467,3.999,19.985l140.757,140.753v138.755c0,4.955,1.809,9.232,5.424,12.854l73.085,73.083
      c3.429,3.614,7.71,5.428,12.851,5.428c2.282,0,4.66-0.479,7.135-1.43c7.426-3.238,11.14-8.851,11.14-16.845V172.166L396.861,31.413
      C402.765,25.895,404.093,19.231,400.858,11.427z"></path></g></svg></button></div><div id="ot-anchor"></div><section id="ot-fltr-modal"><div id="ot-fltr-cnt"><button id="clear-filters-handler">Clear</button><div class="ot-fltr-scrlcnt ot-pc-scrollbar"><div class="ot-fltr-opts"><div class="ot-fltr-opt"><div class="ot-chkbox"><input id="chkbox-id" type="checkbox" class="category-filter-handler"> <label for="chkbox-id"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div></div></div><div class="ot-fltr-btns"><button id="filter-apply-handler">Apply</button> <button id="filter-cancel-handler">Cancel</button></div></div></div></section></div></div><section id="ot-lst-cnt" class="ot-host-cnt ot-pc-scrollbar"><div id="ot-sel-blk"><div class="ot-sel-all"><div class="ot-sel-all-hdr"><span class="ot-consent-hdr">Consent</span> <span class="ot-li-hdr">Leg.Interest</span></div><div class="ot-sel-all-chkbox"><div class="ot-chkbox" id="ot-selall-hostcntr"><input id="select-all-hosts-groups-handler" type="checkbox"> <label for="select-all-hosts-groups-handler"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div><div class="ot-chkbox" id="ot-selall-vencntr"><input id="select-all-vendor-groups-handler" type="checkbox"> <label for="select-all-vendor-groups-handler"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div><div class="ot-chkbox" id="ot-selall-licntr"><input id="select-all-vendor-leg-handler" type="checkbox"> <label for="select-all-vendor-leg-handler"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div></div></div></div><div class="ot-sdk-row"><div class="ot-sdk-column"><ul id="ot-host-lst"></ul></div></div></section></section><div class="ot-pc-footer ot-pc-scrollbar"><div class="ot-btn-container"> <button class="save-preference-btn-handler onetrust-close-btn-handler">Confirm my choices</button></div><!-- Footer logo --><div class="ot-pc-footer-logo"><a href="https://www.onetrust.com/products/cookie-consent/" target="_blank" rel="noopener noreferrer" aria-label="Powered by OneTrust Opens in a new Tab"><img alt="Powered by Onetrust" src="https://cdn.cookielaw.org/logos/static/powered_by_logo.svg" title="Powered by OneTrust Opens in a new Tab"></a></div></div><!-- Cookie subgroup container --><!-- Vendor list link --><!-- Cookie lost link --><!-- Toggle HTML element --><!-- Checkbox HTML --><!-- plus minus--><!-- Arrow SVG element --><!-- Accordion basic element --><span class="ot-scrn-rdr" aria-atomic="true" aria-live="polite"></span><!-- Vendor Service container and item template --></div><iframe class="ot-text-resize" sandbox="allow-same-origin" title="onetrust-text-resize" style="position: absolute; top: -50000px; width: 100em;" aria-hidden="true"></iframe></div></div></body></html>