<html lang="en-US" class="toolbar-stuck"><head>
      <meta name="citation_pii" content="S1053811915008873">
<meta name="citation_issn" content="1053-8119">
<meta name="citation_volume" content="124">
<meta name="citation_lastpage" content="946">
<meta name="citation_publisher" content="Academic Press">
<meta name="citation_firstpage" content="931">
<meta name="citation_fulltext_world_readable" content="">
<meta name="citation_journal_title" content="NeuroImage">
<meta name="citation_type" content="JOUR">
<meta name="citation_doi" content="10.1016/j.neuroimage.2015.09.065">
<meta name="dc.identifier" content="10.1016/j.neuroimage.2015.09.065">
<meta name="citation_article_type" content="Full-length article">
<meta property="og:description" content="In the current study, electroencephalography (EEG) was recorded simultaneously with facial electromyography (fEMG) to determine whether emotional face…">
<meta property="og:image" content="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915X00176-cov150h.gif">
<meta name="citation_title" content="Emotional facial expressions evoke faster orienting responses, but weaker emotional responses at neural and behavioural levels compared to scenes: A simultaneous EEG and facial EMG study">
<meta property="og:title" content="Emotional facial expressions evoke faster orienting responses, but weaker emotional responses at neural and behavioural levels compared to scenes: A simultaneous EEG and facial EMG study">
<meta name="citation_publication_date" content="2016/01/01">
<meta name="citation_online_date" content="2015/10/08">
<meta name="robots" content="INDEX,FOLLOW,NOARCHIVE,NOCACHE,NOODP,NOYDIR">
      <title>Emotional facial expressions evoke faster orienting responses, but weaker emotional responses at neural and behavioural levels compared to scenes: A simultaneous EEG and facial EMG study - ScienceDirect</title>
      <link rel="canonical" href="https://www.sciencedirect.com/science/article/pii/S1053811915008873">
      <meta name="tdm-reservation" content="1">
      <meta name="tdm-policy" content="https://www.elsevier.com/tdm/tdmrep-policy.json">
      <meta property="og:type" content="article">
      <meta name="viewport" content="initial-scale=1">
      <meta name="SDTech" content="Proudly brought to you by the SD Technology team">
      <script async="" src="https://cdn.pendo.io/agent/static/d6c1d995-bc7e-4e53-77f1-2ea4ecbb9565/pendo.js"></script><script type="text/javascript">(function newRelicBrowserProSPA() {
  ;
  window.NREUM || (NREUM = {});
  NREUM.init = {
    privacy: {
      cookies_enabled: false
    },
    ajax: {
      deny_list: ["bam-cell.nr-data.net"]
    }
  };
  ;
  NREUM.loader_config = {
    accountID: "2128461",
    trustKey: "2038175",
    agentID: "1118783207",
    licenseKey: "7ac4127487",
    applicationID: "814813181"
  };
  ;
  NREUM.info = {
    beacon: "bam.nr-data.net",
    errorBeacon: "bam.nr-data.net",
    licenseKey: "7ac4127487",
    applicationID: "814813181",
    sa: 1
  };
  ; /*! For license information please see nr-loader-spa-1.238.0.min.js.LICENSE.txt */
  (() => {
    "use strict";

    var e,
      t,
      r = {
        5763: (e, t, r) => {
          r.d(t, {
            P_: () => f,
            Mt: () => p,
            C5: () => s,
            DL: () => v,
            OP: () => T,
            lF: () => D,
            Yu: () => y,
            Dg: () => h,
            CX: () => c,
            GE: () => b,
            sU: () => _
          });
          var n = r(8632),
            i = r(9567);
          const o = {
              beacon: n.ce.beacon,
              errorBeacon: n.ce.errorBeacon,
              licenseKey: void 0,
              applicationID: void 0,
              sa: void 0,
              queueTime: void 0,
              applicationTime: void 0,
              ttGuid: void 0,
              user: void 0,
              account: void 0,
              product: void 0,
              extra: void 0,
              jsAttributes: {},
              userAttributes: void 0,
              atts: void 0,
              transactionName: void 0,
              tNamePlain: void 0
            },
            a = {};
          function s(e) {
            if (!e) throw new Error("All info objects require an agent identifier!");
            if (!a[e]) throw new Error("Info for ".concat(e, " was never set"));
            return a[e];
          }
          function c(e, t) {
            if (!e) throw new Error("All info objects require an agent identifier!");
            a[e] = (0, i.D)(t, o), (0, n.Qy)(e, a[e], "info");
          }
          var u = r(7056);
          const d = () => {
              const e = {
                blockSelector: "[data-nr-block]",
                maskInputOptions: {
                  password: !0
                }
              };
              return {
                allow_bfcache: !0,
                privacy: {
                  cookies_enabled: !0
                },
                ajax: {
                  deny_list: void 0,
                  block_internal: !0,
                  enabled: !0,
                  harvestTimeSeconds: 10
                },
                distributed_tracing: {
                  enabled: void 0,
                  exclude_newrelic_header: void 0,
                  cors_use_newrelic_header: void 0,
                  cors_use_tracecontext_headers: void 0,
                  allowed_origins: void 0
                },
                session: {
                  domain: void 0,
                  expiresMs: u.oD,
                  inactiveMs: u.Hb
                },
                ssl: void 0,
                obfuscate: void 0,
                jserrors: {
                  enabled: !0,
                  harvestTimeSeconds: 10
                },
                metrics: {
                  enabled: !0
                },
                page_action: {
                  enabled: !0,
                  harvestTimeSeconds: 30
                },
                page_view_event: {
                  enabled: !0
                },
                page_view_timing: {
                  enabled: !0,
                  harvestTimeSeconds: 30,
                  long_task: !1
                },
                session_trace: {
                  enabled: !0,
                  harvestTimeSeconds: 10
                },
                harvest: {
                  tooManyRequestsDelay: 60
                },
                session_replay: {
                  enabled: !1,
                  harvestTimeSeconds: 60,
                  sampleRate: .1,
                  errorSampleRate: .1,
                  maskTextSelector: "*",
                  maskAllInputs: !0,
                  get blockClass() {
                    return "nr-block";
                  },
                  get ignoreClass() {
                    return "nr-ignore";
                  },
                  get maskTextClass() {
                    return "nr-mask";
                  },
                  get blockSelector() {
                    return e.blockSelector;
                  },
                  set blockSelector(t) {
                    e.blockSelector += ",".concat(t);
                  },
                  get maskInputOptions() {
                    return e.maskInputOptions;
                  },
                  set maskInputOptions(t) {
                    e.maskInputOptions = {
                      ...t,
                      password: !0
                    };
                  }
                },
                spa: {
                  enabled: !0,
                  harvestTimeSeconds: 10
                }
              };
            },
            l = {};
          function f(e) {
            if (!e) throw new Error("All configuration objects require an agent identifier!");
            if (!l[e]) throw new Error("Configuration for ".concat(e, " was never set"));
            return l[e];
          }
          function h(e, t) {
            if (!e) throw new Error("All configuration objects require an agent identifier!");
            l[e] = (0, i.D)(t, d()), (0, n.Qy)(e, l[e], "config");
          }
          function p(e, t) {
            if (!e) throw new Error("All configuration objects require an agent identifier!");
            var r = f(e);
            if (r) {
              for (var n = t.split("."), i = 0; i < n.length - 1; i++) if ("object" != typeof (r = r[n[i]])) return;
              r = r[n[n.length - 1]];
            }
            return r;
          }
          const g = {
              accountID: void 0,
              trustKey: void 0,
              agentID: void 0,
              licenseKey: void 0,
              applicationID: void 0,
              xpid: void 0
            },
            m = {};
          function v(e) {
            if (!e) throw new Error("All loader-config objects require an agent identifier!");
            if (!m[e]) throw new Error("LoaderConfig for ".concat(e, " was never set"));
            return m[e];
          }
          function b(e, t) {
            if (!e) throw new Error("All loader-config objects require an agent identifier!");
            m[e] = (0, i.D)(t, g), (0, n.Qy)(e, m[e], "loader_config");
          }
          const y = (0, n.mF)().o;
          var w = r(385),
            A = r(6818);
          const x = {
              buildEnv: A.Re,
              bytesSent: {},
              queryBytesSent: {},
              customTransaction: void 0,
              disabled: !1,
              distMethod: A.gF,
              isolatedBacklog: !1,
              loaderType: void 0,
              maxBytes: 3e4,
              offset: Math.floor(w._A?.performance?.timeOrigin || w._A?.performance?.timing?.navigationStart || Date.now()),
              onerror: void 0,
              origin: "" + w._A.location,
              ptid: void 0,
              releaseIds: {},
              session: void 0,
              xhrWrappable: "function" == typeof w._A.XMLHttpRequest?.prototype?.addEventListener,
              version: A.q4,
              denyList: void 0
            },
            E = {};
          function T(e) {
            if (!e) throw new Error("All runtime objects require an agent identifier!");
            if (!E[e]) throw new Error("Runtime for ".concat(e, " was never set"));
            return E[e];
          }
          function _(e, t) {
            if (!e) throw new Error("All runtime objects require an agent identifier!");
            E[e] = (0, i.D)(t, x), (0, n.Qy)(e, E[e], "runtime");
          }
          function D(e) {
            return function (e) {
              try {
                const t = s(e);
                return !!t.licenseKey && !!t.errorBeacon && !!t.applicationID;
              } catch (e) {
                return !1;
              }
            }(e);
          }
        },
        9567: (e, t, r) => {
          r.d(t, {
            D: () => i
          });
          var n = r(50);
          function i(e, t) {
            try {
              if (!e || "object" != typeof e) return (0, n.Z)("Setting a Configurable requires an object as input");
              if (!t || "object" != typeof t) return (0, n.Z)("Setting a Configurable requires a model to set its initial properties");
              const r = Object.create(Object.getPrototypeOf(t), Object.getOwnPropertyDescriptors(t)),
                o = 0 === Object.keys(r).length ? e : r;
              for (let a in o) if (void 0 !== e[a]) try {
                "object" == typeof e[a] && "object" == typeof t[a] ? r[a] = i(e[a], t[a]) : r[a] = e[a];
              } catch (e) {
                (0, n.Z)("An error occurred while setting a property of a Configurable", e);
              }
              return r;
            } catch (e) {
              (0, n.Z)("An error occured while setting a Configurable", e);
            }
          }
        },
        6818: (e, t, r) => {
          r.d(t, {
            Re: () => i,
            gF: () => o,
            q4: () => n
          });
          const n = "1.238.0",
            i = "PROD",
            o = "CDN";
        },
        385: (e, t, r) => {
          r.d(t, {
            FN: () => a,
            IF: () => u,
            Nk: () => l,
            Tt: () => s,
            _A: () => o,
            il: () => n,
            pL: () => c,
            v6: () => i,
            w1: () => d
          });
          const n = "undefined" != typeof window && !!window.document,
            i = "undefined" != typeof WorkerGlobalScope && ("undefined" != typeof self && self instanceof WorkerGlobalScope && self.navigator instanceof WorkerNavigator || "undefined" != typeof globalThis && globalThis instanceof WorkerGlobalScope && globalThis.navigator instanceof WorkerNavigator),
            o = n ? window : "undefined" != typeof WorkerGlobalScope && ("undefined" != typeof self && self instanceof WorkerGlobalScope && self || "undefined" != typeof globalThis && globalThis instanceof WorkerGlobalScope && globalThis),
            a = "" + o?.location,
            s = /iPad|iPhone|iPod/.test(navigator.userAgent),
            c = s && "undefined" == typeof SharedWorker,
            u = (() => {
              const e = navigator.userAgent.match(/Firefox[/\s](\d+\.\d+)/);
              return Array.isArray(e) && e.length >= 2 ? +e[1] : 0;
            })(),
            d = Boolean(n && window.document.documentMode),
            l = !!navigator.sendBeacon;
        },
        1117: (e, t, r) => {
          r.d(t, {
            w: () => o
          });
          var n = r(50);
          const i = {
            agentIdentifier: "",
            ee: void 0
          };
          class o {
            constructor(e) {
              try {
                if ("object" != typeof e) return (0, n.Z)("shared context requires an object as input");
                this.sharedContext = {}, Object.assign(this.sharedContext, i), Object.entries(e).forEach(e => {
                  let [t, r] = e;
                  Object.keys(i).includes(t) && (this.sharedContext[t] = r);
                });
              } catch (e) {
                (0, n.Z)("An error occured while setting SharedContext", e);
              }
            }
          }
        },
        8e3: (e, t, r) => {
          r.d(t, {
            L: () => d,
            R: () => c
          });
          var n = r(8325),
            i = r(1284),
            o = r(4322),
            a = r(3325);
          const s = {};
          function c(e, t) {
            const r = {
              staged: !1,
              priority: a.p[t] || 0
            };
            u(e), s[e].get(t) || s[e].set(t, r);
          }
          function u(e) {
            e && (s[e] || (s[e] = new Map()));
          }
          function d() {
            let e = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : "",
              t = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : "feature";
            if (u(e), !e || !s[e].get(t)) return a(t);
            s[e].get(t).staged = !0;
            const r = [...s[e]];
            function a(t) {
              const r = e ? n.ee.get(e) : n.ee,
                a = o.X.handlers;
              if (r.backlog && a) {
                var s = r.backlog[t],
                  c = a[t];
                if (c) {
                  for (var u = 0; s && u < s.length; ++u) l(s[u], c);
                  (0, i.D)(c, function (e, t) {
                    (0, i.D)(t, function (t, r) {
                      r[0].on(e, r[1]);
                    });
                  });
                }
                delete a[t], r.backlog[t] = null, r.emit("drain-" + t, []);
              }
            }
            r.every(e => {
              let [t, r] = e;
              return r.staged;
            }) && (r.sort((e, t) => e[1].priority - t[1].priority), r.forEach(e => {
              let [t] = e;
              a(t);
            }));
          }
          function l(e, t) {
            var r = e[1];
            (0, i.D)(t[r], function (t, r) {
              var n = e[0];
              if (r[0] === n) {
                var i = r[1],
                  o = e[3],
                  a = e[2];
                i.apply(o, a);
              }
            });
          }
        },
        8325: (e, t, r) => {
          r.d(t, {
            A: () => c,
            ee: () => u
          });
          var n = r(8632),
            i = r(2210),
            o = r(5763);
          class a {
            constructor(e) {
              this.contextId = e;
            }
          }
          var s = r(3117);
          const c = "nr@context:".concat(s.a),
            u = function e(t, r) {
              var n = {},
                s = {},
                d = {},
                f = !1;
              try {
                f = 16 === r.length && (0, o.OP)(r).isolatedBacklog;
              } catch (e) {}
              var h = {
                on: g,
                addEventListener: g,
                removeEventListener: function (e, t) {
                  var r = n[e];
                  if (!r) return;
                  for (var i = 0; i < r.length; i++) r[i] === t && r.splice(i, 1);
                },
                emit: function (e, r, n, i, o) {
                  !1 !== o && (o = !0);
                  if (u.aborted && !i) return;
                  t && o && t.emit(e, r, n);
                  for (var a = p(n), c = m(e), d = c.length, l = 0; l < d; l++) c[l].apply(a, r);
                  var f = b()[s[e]];
                  f && f.push([h, e, r, a]);
                  return a;
                },
                get: v,
                listeners: m,
                context: p,
                buffer: function (e, t) {
                  const r = b();
                  if (t = t || "feature", h.aborted) return;
                  Object.entries(e || {}).forEach(e => {
                    let [n, i] = e;
                    s[i] = t, t in r || (r[t] = []);
                  });
                },
                abort: l,
                aborted: !1,
                isBuffering: function (e) {
                  return !!b()[s[e]];
                },
                debugId: r,
                backlog: f ? {} : t && "object" == typeof t.backlog ? t.backlog : {}
              };
              return h;
              function p(e) {
                return e && e instanceof a ? e : e ? (0, i.X)(e, c, () => new a(c)) : new a(c);
              }
              function g(e, t) {
                n[e] = m(e).concat(t);
              }
              function m(e) {
                return n[e] || [];
              }
              function v(t) {
                return d[t] = d[t] || e(h, t);
              }
              function b() {
                return h.backlog;
              }
            }(void 0, "globalEE"),
            d = (0, n.fP)();
          function l() {
            u.aborted = !0, u.backlog = {};
          }
          d.ee || (d.ee = u);
        },
        5546: (e, t, r) => {
          r.d(t, {
            E: () => n,
            p: () => i
          });
          var n = r(8325).ee.get("handle");
          function i(e, t, r, i, o) {
            o ? (o.buffer([e], i), o.emit(e, t, r)) : (n.buffer([e], i), n.emit(e, t, r));
          }
        },
        4322: (e, t, r) => {
          r.d(t, {
            X: () => o
          });
          var n = r(5546);
          o.on = a;
          var i = o.handlers = {};
          function o(e, t, r, o) {
            a(o || n.E, i, e, t, r);
          }
          function a(e, t, r, i, o) {
            o || (o = "feature"), e || (e = n.E);
            var a = t[o] = t[o] || {};
            (a[r] = a[r] || []).push([e, i]);
          }
        },
        3239: (e, t, r) => {
          r.d(t, {
            bP: () => s,
            iz: () => c,
            m$: () => a
          });
          var n = r(385);
          let i = !1,
            o = !1;
          try {
            const e = {
              get passive() {
                return i = !0, !1;
              },
              get signal() {
                return o = !0, !1;
              }
            };
            n._A.addEventListener("test", null, e), n._A.removeEventListener("test", null, e);
          } catch (e) {}
          function a(e, t) {
            return i || o ? {
              capture: !!e,
              passive: i,
              signal: t
            } : !!e;
          }
          function s(e, t) {
            let r = arguments.length > 2 && void 0 !== arguments[2] && arguments[2],
              n = arguments.length > 3 ? arguments[3] : void 0;
            window.addEventListener(e, t, a(r, n));
          }
          function c(e, t) {
            let r = arguments.length > 2 && void 0 !== arguments[2] && arguments[2],
              n = arguments.length > 3 ? arguments[3] : void 0;
            document.addEventListener(e, t, a(r, n));
          }
        },
        3117: (e, t, r) => {
          r.d(t, {
            a: () => n
          });
          const n = (0, r(4402).Rl)();
        },
        4402: (e, t, r) => {
          r.d(t, {
            Ht: () => u,
            M: () => c,
            Rl: () => a,
            ky: () => s
          });
          var n = r(385);
          const i = "xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx";
          function o(e, t) {
            return e ? 15 & e[t] : 16 * Math.random() | 0;
          }
          function a() {
            const e = n._A?.crypto || n._A?.msCrypto;
            let t,
              r = 0;
            return e && e.getRandomValues && (t = e.getRandomValues(new Uint8Array(31))), i.split("").map(e => "x" === e ? o(t, ++r).toString(16) : "y" === e ? (3 & o() | 8).toString(16) : e).join("");
          }
          function s(e) {
            const t = n._A?.crypto || n._A?.msCrypto;
            let r,
              i = 0;
            t && t.getRandomValues && (r = t.getRandomValues(new Uint8Array(31)));
            const a = [];
            for (var s = 0; s < e; s++) a.push(o(r, ++i).toString(16));
            return a.join("");
          }
          function c() {
            return s(16);
          }
          function u() {
            return s(32);
          }
        },
        7056: (e, t, r) => {
          r.d(t, {
            Bq: () => n,
            Hb: () => o,
            oD: () => i
          });
          const n = "NRBA",
            i = 144e5,
            o = 18e5;
        },
        7894: (e, t, r) => {
          function n() {
            return Math.round(performance.now());
          }
          r.d(t, {
            z: () => n
          });
        },
        7243: (e, t, r) => {
          r.d(t, {
            e: () => o
          });
          var n = r(385),
            i = {};
          function o(e) {
            if (e in i) return i[e];
            if (0 === (e || "").indexOf("data:")) return {
              protocol: "data"
            };
            let t;
            var r = n._A?.location,
              o = {};
            if (n.il) t = document.createElement("a"), t.href = e;else try {
              t = new URL(e, r.href);
            } catch (e) {
              return o;
            }
            o.port = t.port;
            var a = t.href.split("://");
            !o.port && a[1] && (o.port = a[1].split("/")[0].split("@").pop().split(":")[1]), o.port && "0" !== o.port || (o.port = "https" === a[0] ? "443" : "80"), o.hostname = t.hostname || r.hostname, o.pathname = t.pathname, o.protocol = a[0], "/" !== o.pathname.charAt(0) && (o.pathname = "/" + o.pathname);
            var s = !t.protocol || ":" === t.protocol || t.protocol === r.protocol,
              c = t.hostname === r.hostname && t.port === r.port;
            return o.sameOrigin = s && (!t.hostname || c), "/" === o.pathname && (i[e] = o), o;
          }
        },
        50: (e, t, r) => {
          function n(e, t) {
            "function" == typeof console.warn && (console.warn("New Relic: ".concat(e)), t && console.warn(t));
          }
          r.d(t, {
            Z: () => n
          });
        },
        2587: (e, t, r) => {
          r.d(t, {
            N: () => c,
            T: () => u
          });
          var n = r(8325),
            i = r(5546),
            o = r(8e3),
            a = r(3325);
          const s = {
            stn: [a.D.sessionTrace],
            err: [a.D.jserrors, a.D.metrics],
            ins: [a.D.pageAction],
            spa: [a.D.spa],
            sr: [a.D.sessionReplay, a.D.sessionTrace]
          };
          function c(e, t) {
            const r = n.ee.get(t);
            e && "object" == typeof e && (Object.entries(e).forEach(e => {
              let [t, n] = e;
              void 0 === u[t] && (s[t] ? s[t].forEach(e => {
                n ? (0, i.p)("feat-" + t, [], void 0, e, r) : (0, i.p)("block-" + t, [], void 0, e, r), (0, i.p)("rumresp-" + t, [Boolean(n)], void 0, e, r);
              }) : n && (0, i.p)("feat-" + t, [], void 0, void 0, r), u[t] = Boolean(n));
            }), Object.keys(s).forEach(e => {
              void 0 === u[e] && (s[e]?.forEach(t => (0, i.p)("rumresp-" + e, [!1], void 0, t, r)), u[e] = !1);
            }), (0, o.L)(t, a.D.pageViewEvent));
          }
          const u = {};
        },
        2210: (e, t, r) => {
          r.d(t, {
            X: () => i
          });
          var n = Object.prototype.hasOwnProperty;
          function i(e, t, r) {
            if (n.call(e, t)) return e[t];
            var i = r();
            if (Object.defineProperty && Object.keys) try {
              return Object.defineProperty(e, t, {
                value: i,
                writable: !0,
                enumerable: !1
              }), i;
            } catch (e) {}
            return e[t] = i, i;
          }
        },
        1284: (e, t, r) => {
          r.d(t, {
            D: () => n
          });
          const n = (e, t) => Object.entries(e || {}).map(e => {
            let [r, n] = e;
            return t(r, n);
          });
        },
        4351: (e, t, r) => {
          r.d(t, {
            P: () => o
          });
          var n = r(8325);
          const i = () => {
            const e = new WeakSet();
            return (t, r) => {
              if ("object" == typeof r && null !== r) {
                if (e.has(r)) return;
                e.add(r);
              }
              return r;
            };
          };
          function o(e) {
            try {
              return JSON.stringify(e, i());
            } catch (e) {
              try {
                n.ee.emit("internal-error", [e]);
              } catch (e) {}
            }
          }
        },
        3960: (e, t, r) => {
          r.d(t, {
            K: () => a,
            b: () => o
          });
          var n = r(3239);
          function i() {
            return "undefined" == typeof document || "complete" === document.readyState;
          }
          function o(e, t) {
            if (i()) return e();
            (0, n.bP)("load", e, t);
          }
          function a(e) {
            if (i()) return e();
            (0, n.iz)("DOMContentLoaded", e);
          }
        },
        8632: (e, t, r) => {
          r.d(t, {
            EZ: () => u,
            Qy: () => c,
            ce: () => o,
            fP: () => a,
            gG: () => d,
            mF: () => s
          });
          var n = r(7894),
            i = r(385);
          const o = {
            beacon: "bam.nr-data.net",
            errorBeacon: "bam.nr-data.net"
          };
          function a() {
            return i._A.NREUM || (i._A.NREUM = {}), void 0 === i._A.newrelic && (i._A.newrelic = i._A.NREUM), i._A.NREUM;
          }
          function s() {
            let e = a();
            return e.o || (e.o = {
              ST: i._A.setTimeout,
              SI: i._A.setImmediate,
              CT: i._A.clearTimeout,
              XHR: i._A.XMLHttpRequest,
              REQ: i._A.Request,
              EV: i._A.Event,
              PR: i._A.Promise,
              MO: i._A.MutationObserver,
              FETCH: i._A.fetch
            }), e;
          }
          function c(e, t, r) {
            let i = a();
            const o = i.initializedAgents || {},
              s = o[e] || {};
            return Object.keys(s).length || (s.initializedAt = {
              ms: (0, n.z)(),
              date: new Date()
            }), i.initializedAgents = {
              ...o,
              [e]: {
                ...s,
                [r]: t
              }
            }, i;
          }
          function u(e, t) {
            a()[e] = t;
          }
          function d() {
            return function () {
              let e = a();
              const t = e.info || {};
              e.info = {
                beacon: o.beacon,
                errorBeacon: o.errorBeacon,
                ...t
              };
            }(), function () {
              let e = a();
              const t = e.init || {};
              e.init = {
                ...t
              };
            }(), s(), function () {
              let e = a();
              const t = e.loader_config || {};
              e.loader_config = {
                ...t
              };
            }(), a();
          }
        },
        7956: (e, t, r) => {
          r.d(t, {
            N: () => i
          });
          var n = r(3239);
          function i(e) {
            let t = arguments.length > 1 && void 0 !== arguments[1] && arguments[1],
              r = arguments.length > 2 ? arguments[2] : void 0,
              i = arguments.length > 3 ? arguments[3] : void 0;
            return void (0, n.iz)("visibilitychange", function () {
              if (t) return void ("hidden" == document.visibilityState && e());
              e(document.visibilityState);
            }, r, i);
          }
        },
        1214: (e, t, r) => {
          r.d(t, {
            em: () => b,
            u5: () => j,
            QU: () => O,
            _L: () => I,
            Gm: () => H,
            Lg: () => L,
            BV: () => G,
            Kf: () => K
          });
          var n = r(8325),
            i = r(3117);
          const o = "nr@original:".concat(i.a);
          var a = Object.prototype.hasOwnProperty,
            s = !1;
          function c(e, t) {
            return e || (e = n.ee), r.inPlace = function (e, t, n, i, o) {
              n || (n = "");
              const a = "-" === n.charAt(0);
              for (let s = 0; s < t.length; s++) {
                const c = t[s],
                  u = e[c];
                d(u) || (e[c] = r(u, a ? c + n : n, i, c, o));
              }
            }, r.flag = o, r;
            function r(t, r, n, s, c) {
              return d(t) ? t : (r || (r = ""), nrWrapper[o] = t, function (e, t, r) {
                if (Object.defineProperty && Object.keys) try {
                  return Object.keys(e).forEach(function (r) {
                    Object.defineProperty(t, r, {
                      get: function () {
                        return e[r];
                      },
                      set: function (t) {
                        return e[r] = t, t;
                      }
                    });
                  }), t;
                } catch (e) {
                  u([e], r);
                }
                for (var n in e) a.call(e, n) && (t[n] = e[n]);
              }(t, nrWrapper, e), nrWrapper);
              function nrWrapper() {
                var o, a, d, l;
                try {
                  a = this, o = [...arguments], d = "function" == typeof n ? n(o, a) : n || {};
                } catch (t) {
                  u([t, "", [o, a, s], d], e);
                }
                i(r + "start", [o, a, s], d, c);
                try {
                  return l = t.apply(a, o);
                } catch (e) {
                  throw i(r + "err", [o, a, e], d, c), e;
                } finally {
                  i(r + "end", [o, a, l], d, c);
                }
              }
            }
            function i(r, n, i, o) {
              if (!s || t) {
                var a = s;
                s = !0;
                try {
                  e.emit(r, n, i, t, o);
                } catch (t) {
                  u([t, r, n, i], e);
                }
                s = a;
              }
            }
          }
          function u(e, t) {
            t || (t = n.ee);
            try {
              t.emit("internal-error", e);
            } catch (e) {}
          }
          function d(e) {
            return !(e && e instanceof Function && e.apply && !e[o]);
          }
          var l = r(2210),
            f = r(385);
          const h = {},
            p = f._A.XMLHttpRequest,
            g = "addEventListener",
            m = "removeEventListener",
            v = "nr@wrapped:".concat(n.A);
          function b(e) {
            var t = function (e) {
              return (e || n.ee).get("events");
            }(e);
            if (h[t.debugId]++) return t;
            h[t.debugId] = 1;
            var r = c(t, !0);
            function i(e) {
              r.inPlace(e, [g, m], "-", o);
            }
            function o(e, t) {
              return e[1];
            }
            return "getPrototypeOf" in Object && (f.il && y(document, i), y(f._A, i), y(p.prototype, i)), t.on(g + "-start", function (e, t) {
              var n = e[1];
              if (null !== n && ("function" == typeof n || "object" == typeof n)) {
                var i = (0, l.X)(n, v, function () {
                  var e = {
                    object: function () {
                      if ("function" != typeof n.handleEvent) return;
                      return n.handleEvent.apply(n, arguments);
                    },
                    function: n
                  }[typeof n];
                  return e ? r(e, "fn-", null, e.name || "anonymous") : n;
                });
                this.wrapped = e[1] = i;
              }
            }), t.on(m + "-start", function (e) {
              e[1] = this.wrapped || e[1];
            }), t;
          }
          function y(e, t) {
            let r = e;
            for (; "object" == typeof r && !Object.prototype.hasOwnProperty.call(r, g);) r = Object.getPrototypeOf(r);
            for (var n = arguments.length, i = new Array(n > 2 ? n - 2 : 0), o = 2; o < n; o++) i[o - 2] = arguments[o];
            r && t(r, ...i);
          }
          var w = "fetch-",
            A = w + "body-",
            x = ["arrayBuffer", "blob", "json", "text", "formData"],
            E = f._A.Request,
            T = f._A.Response,
            _ = "prototype";
          const D = {};
          function j(e) {
            const t = function (e) {
              return (e || n.ee).get("fetch");
            }(e);
            if (!(E && T && f._A.fetch)) return t;
            if (D[t.debugId]++) return t;
            function r(e, r, i) {
              var o = e[r];
              "function" == typeof o && (e[r] = function () {
                var e,
                  r = [...arguments],
                  a = {};
                t.emit(i + "before-start", [r], a), a[n.A] && a[n.A].dt && (e = a[n.A].dt);
                var s = o.apply(this, r);
                return t.emit(i + "start", [r, e], s), s.then(function (e) {
                  return t.emit(i + "end", [null, e], s), e;
                }, function (e) {
                  throw t.emit(i + "end", [e], s), e;
                });
              });
            }
            return D[t.debugId] = 1, x.forEach(e => {
              r(E[_], e, A), r(T[_], e, A);
            }), r(f._A, "fetch", w), t.on(w + "end", function (e, r) {
              var n = this;
              if (r) {
                var i = r.headers.get("content-length");
                null !== i && (n.rxSize = i), t.emit(w + "done", [null, r], n);
              } else t.emit(w + "done", [e], n);
            }), t;
          }
          const C = {},
            N = ["pushState", "replaceState"];
          function O(e) {
            const t = function (e) {
              return (e || n.ee).get("history");
            }(e);
            return !f.il || C[t.debugId]++ || (C[t.debugId] = 1, c(t).inPlace(window.history, N, "-")), t;
          }
          var S = r(3239);
          const P = {},
            R = ["appendChild", "insertBefore", "replaceChild"];
          function I(e) {
            const t = function (e) {
              return (e || n.ee).get("jsonp");
            }(e);
            if (!f.il || P[t.debugId]) return t;
            P[t.debugId] = !0;
            var r = c(t),
              i = /[?&](?:callback|cb)=([^&#]+)/,
              o = /(.*)\.([^.]+)/,
              a = /^(\w+)(\.|$)(.*)$/;
            function s(e, t) {
              if (!e) return t;
              const r = e.match(a),
                n = r[1];
              return s(r[3], t[n]);
            }
            return r.inPlace(Node.prototype, R, "dom-"), t.on("dom-start", function (e) {
              !function (e) {
                if (!e || "string" != typeof e.nodeName || "script" !== e.nodeName.toLowerCase()) return;
                if ("function" != typeof e.addEventListener) return;
                var n = (a = e.src, c = a.match(i), c ? c[1] : null);
                var a, c;
                if (!n) return;
                var u = function (e) {
                  var t = e.match(o);
                  if (t && t.length >= 3) return {
                    key: t[2],
                    parent: s(t[1], window)
                  };
                  return {
                    key: e,
                    parent: window
                  };
                }(n);
                if ("function" != typeof u.parent[u.key]) return;
                var d = {};
                function l() {
                  t.emit("jsonp-end", [], d), e.removeEventListener("load", l, (0, S.m$)(!1)), e.removeEventListener("error", f, (0, S.m$)(!1));
                }
                function f() {
                  t.emit("jsonp-error", [], d), t.emit("jsonp-end", [], d), e.removeEventListener("load", l, (0, S.m$)(!1)), e.removeEventListener("error", f, (0, S.m$)(!1));
                }
                r.inPlace(u.parent, [u.key], "cb-", d), e.addEventListener("load", l, (0, S.m$)(!1)), e.addEventListener("error", f, (0, S.m$)(!1)), t.emit("new-jsonp", [e.src], d);
              }(e[0]);
            }), t;
          }
          const k = {};
          function H(e) {
            const t = function (e) {
              return (e || n.ee).get("mutation");
            }(e);
            if (!f.il || k[t.debugId]) return t;
            k[t.debugId] = !0;
            var r = c(t),
              i = f._A.MutationObserver;
            return i && (window.MutationObserver = function (e) {
              return this instanceof i ? new i(r(e, "fn-")) : i.apply(this, arguments);
            }, MutationObserver.prototype = i.prototype), t;
          }
          const z = {};
          function L(e) {
            const t = function (e) {
              return (e || n.ee).get("promise");
            }(e);
            if (z[t.debugId]) return t;
            z[t.debugId] = !0;
            var r = t.context,
              i = c(t),
              a = f._A.Promise;
            return a && function () {
              function e(r) {
                var n = t.context(),
                  o = i(r, "executor-", n, null, !1);
                const s = Reflect.construct(a, [o], e);
                return t.context(s).getCtx = function () {
                  return n;
                }, s;
              }
              f._A.Promise = e, Object.defineProperty(e, "name", {
                value: "Promise"
              }), e.toString = function () {
                return a.toString();
              }, Object.setPrototypeOf(e, a), ["all", "race"].forEach(function (r) {
                const n = a[r];
                e[r] = function (e) {
                  let i = !1;
                  [...(e || [])].forEach(e => {
                    this.resolve(e).then(a("all" === r), a(!1));
                  });
                  const o = n.apply(this, arguments);
                  return o;
                  function a(e) {
                    return function () {
                      t.emit("propagate", [null, !i], o, !1, !1), i = i || !e;
                    };
                  }
                };
              }), ["resolve", "reject"].forEach(function (r) {
                const n = a[r];
                e[r] = function (e) {
                  const r = n.apply(this, arguments);
                  return e !== r && t.emit("propagate", [e, !0], r, !1, !1), r;
                };
              }), e.prototype = a.prototype;
              const n = a.prototype.then;
              a.prototype.then = function () {
                var e = this,
                  o = r(e);
                o.promise = e;
                for (var a = arguments.length, s = new Array(a), c = 0; c < a; c++) s[c] = arguments[c];
                s[0] = i(s[0], "cb-", o, null, !1), s[1] = i(s[1], "cb-", o, null, !1);
                const u = n.apply(this, s);
                return o.nextPromise = u, t.emit("propagate", [e, !0], u, !1, !1), u;
              }, a.prototype.then[o] = n, t.on("executor-start", function (e) {
                e[0] = i(e[0], "resolve-", this, null, !1), e[1] = i(e[1], "resolve-", this, null, !1);
              }), t.on("executor-err", function (e, t, r) {
                e[1](r);
              }), t.on("cb-end", function (e, r, n) {
                t.emit("propagate", [n, !0], this.nextPromise, !1, !1);
              }), t.on("propagate", function (e, r, n) {
                this.getCtx && !r || (this.getCtx = function () {
                  if (e instanceof Promise) var r = t.context(e);
                  return r && r.getCtx ? r.getCtx() : this;
                });
              });
            }(), t;
          }
          const M = {},
            B = "setTimeout",
            F = "setInterval",
            U = "clearTimeout",
            q = "-start",
            Z = "-",
            V = [B, "setImmediate", F, U, "clearImmediate"];
          function G(e) {
            const t = function (e) {
              return (e || n.ee).get("timer");
            }(e);
            if (M[t.debugId]++) return t;
            M[t.debugId] = 1;
            var r = c(t);
            return r.inPlace(f._A, V.slice(0, 2), B + Z), r.inPlace(f._A, V.slice(2, 3), F + Z), r.inPlace(f._A, V.slice(3), U + Z), t.on(F + q, function (e, t, n) {
              e[0] = r(e[0], "fn-", null, n);
            }), t.on(B + q, function (e, t, n) {
              this.method = n, this.timerDuration = isNaN(e[1]) ? 0 : +e[1], e[0] = r(e[0], "fn-", this, n);
            }), t;
          }
          var W = r(50);
          const X = {},
            Q = ["open", "send"];
          function K(e) {
            var t = e || n.ee;
            const r = function (e) {
              return (e || n.ee).get("xhr");
            }(t);
            if (X[r.debugId]++) return r;
            X[r.debugId] = 1, b(t);
            var i = c(r),
              o = f._A.XMLHttpRequest,
              a = f._A.MutationObserver,
              s = f._A.Promise,
              u = f._A.setInterval,
              d = "readystatechange",
              l = ["onload", "onerror", "onabort", "onloadstart", "onloadend", "onprogress", "ontimeout"],
              h = [],
              p = f._A.XMLHttpRequest = function (e) {
                const t = new o(e),
                  n = r.context(t);
                try {
                  r.emit("new-xhr", [t], n), t.addEventListener(d, (a = n, function () {
                    var e = this;
                    e.readyState > 3 && !a.resolved && (a.resolved = !0, r.emit("xhr-resolved", [], e)), i.inPlace(e, l, "fn-", A);
                  }), (0, S.m$)(!1));
                } catch (e) {
                  (0, W.Z)("An error occurred while intercepting XHR", e);
                  try {
                    r.emit("internal-error", [e]);
                  } catch (e) {}
                }
                var a;
                return t;
              };
            function g(e, t) {
              i.inPlace(t, ["onreadystatechange"], "fn-", A);
            }
            if (function (e, t) {
              for (var r in e) t[r] = e[r];
            }(o, p), p.prototype = o.prototype, i.inPlace(p.prototype, Q, "-xhr-", A), r.on("send-xhr-start", function (e, t) {
              g(e, t), function (e) {
                h.push(e), a && (m ? m.then(w) : u ? u(w) : (v = -v, y.data = v));
              }(t);
            }), r.on("open-xhr-start", g), a) {
              var m = s && s.resolve();
              if (!u && !s) {
                var v = 1,
                  y = document.createTextNode(v);
                new a(w).observe(y, {
                  characterData: !0
                });
              }
            } else t.on("fn-end", function (e) {
              e[0] && e[0].type === d || w();
            });
            function w() {
              for (var e = 0; e < h.length; e++) g(0, h[e]);
              h.length && (h = []);
            }
            function A(e, t) {
              return t;
            }
            return r;
          }
        },
        7825: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.ajax;
        },
        6660: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.jserrors;
        },
        3081: (e, t, r) => {
          r.d(t, {
            gF: () => o,
            mY: () => i,
            t9: () => n,
            vz: () => s,
            xS: () => a
          });
          const n = r(3325).D.metrics,
            i = "sm",
            o = "cm",
            a = "storeSupportabilityMetrics",
            s = "storeEventMetrics";
        },
        4649: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.pageAction;
        },
        7633: (e, t, r) => {
          r.d(t, {
            Dz: () => i,
            OJ: () => a,
            qw: () => o,
            t9: () => n
          });
          const n = r(3325).D.pageViewEvent,
            i = "firstbyte",
            o = "domcontent",
            a = "windowload";
        },
        9251: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.pageViewTiming;
        },
        3614: (e, t, r) => {
          r.d(t, {
            BST_RESOURCE: () => i,
            END: () => s,
            FEATURE_NAME: () => n,
            FN_END: () => u,
            FN_START: () => c,
            PUSH_STATE: () => d,
            RESOURCE: () => o,
            START: () => a
          });
          const n = r(3325).D.sessionTrace,
            i = "bstResource",
            o = "resource",
            a = "-start",
            s = "-end",
            c = "fn" + a,
            u = "fn" + s,
            d = "pushState";
        },
        7836: (e, t, r) => {
          r.d(t, {
            BODY: () => x,
            CB_END: () => E,
            CB_START: () => u,
            END: () => A,
            FEATURE_NAME: () => i,
            FETCH: () => _,
            FETCH_BODY: () => v,
            FETCH_DONE: () => m,
            FETCH_START: () => g,
            FN_END: () => c,
            FN_START: () => s,
            INTERACTION: () => f,
            INTERACTION_API: () => d,
            INTERACTION_EVENTS: () => o,
            JSONP_END: () => b,
            JSONP_NODE: () => p,
            JS_TIME: () => T,
            MAX_TIMER_BUDGET: () => a,
            REMAINING: () => l,
            SPA_NODE: () => h,
            START: () => w,
            originalSetTimeout: () => y
          });
          var n = r(5763);
          const i = r(3325).D.spa,
            o = ["click", "submit", "keypress", "keydown", "keyup", "change"],
            a = 999,
            s = "fn-start",
            c = "fn-end",
            u = "cb-start",
            d = "api-ixn-",
            l = "remaining",
            f = "interaction",
            h = "spaNode",
            p = "jsonpNode",
            g = "fetch-start",
            m = "fetch-done",
            v = "fetch-body-",
            b = "jsonp-end",
            y = n.Yu.ST,
            w = "-start",
            A = "-end",
            x = "-body",
            E = "cb" + A,
            T = "jsTime",
            _ = "fetch";
        },
        5938: (e, t, r) => {
          r.d(t, {
            W: () => o
          });
          var n = r(5763),
            i = r(8325);
          class o {
            constructor(e, t, r) {
              this.agentIdentifier = e, this.aggregator = t, this.ee = i.ee.get(e, (0, n.OP)(this.agentIdentifier).isolatedBacklog), this.featureName = r, this.blocked = !1;
            }
          }
        },
        9144: (e, t, r) => {
          r.d(t, {
            j: () => m
          });
          var n = r(3325),
            i = r(5763),
            o = r(5546),
            a = r(8325),
            s = r(7894),
            c = r(8e3),
            u = r(3960),
            d = r(385),
            l = r(50),
            f = r(3081),
            h = r(8632);
          function p() {
            const e = (0, h.gG)();
            ["setErrorHandler", "finished", "addToTrace", "inlineHit", "addRelease", "addPageAction", "setCurrentRouteName", "setPageViewName", "setCustomAttribute", "interaction", "noticeError", "setUserId", "setApplicationVersion"].forEach(t => {
              e[t] = function () {
                for (var r = arguments.length, n = new Array(r), i = 0; i < r; i++) n[i] = arguments[i];
                return function (t) {
                  for (var r = arguments.length, n = new Array(r > 1 ? r - 1 : 0), i = 1; i < r; i++) n[i - 1] = arguments[i];
                  let o = [];
                  return Object.values(e.initializedAgents).forEach(e => {
                    e.exposed && e.api[t] && o.push(e.api[t](...n));
                  }), o.length > 1 ? o : o[0];
                }(t, ...n);
              };
            });
          }
          var g = r(2587);
          function m(e) {
            let t = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : {},
              m = arguments.length > 2 ? arguments[2] : void 0,
              v = arguments.length > 3 ? arguments[3] : void 0,
              {
                init: b,
                info: y,
                loader_config: w,
                runtime: A = {
                  loaderType: m
                },
                exposed: x = !0
              } = t;
            const E = (0, h.gG)();
            y || (b = E.init, y = E.info, w = E.loader_config), (0, i.Dg)(e, b || {}), (0, i.GE)(e, w || {}), y.jsAttributes ??= {}, d.v6 && (y.jsAttributes.isWorker = !0), (0, i.CX)(e, y);
            const T = (0, i.P_)(e);
            A.denyList = [...(T.ajax?.deny_list || []), ...(T.ajax?.block_internal ? [y.beacon, y.errorBeacon] : [])], (0, i.sU)(e, A), p();
            const _ = function (e, t) {
              t || (0, c.R)(e, "api");
              const h = {};
              var p = a.ee.get(e),
                g = p.get("tracer"),
                m = "api-",
                v = m + "ixn-";
              function b(t, r, n, o) {
                const a = (0, i.C5)(e);
                return null === r ? delete a.jsAttributes[t] : (0, i.CX)(e, {
                  ...a,
                  jsAttributes: {
                    ...a.jsAttributes,
                    [t]: r
                  }
                }), A(m, n, !0, o || null === r ? "session" : void 0)(t, r);
              }
              function y() {}
              ["setErrorHandler", "finished", "addToTrace", "inlineHit", "addRelease"].forEach(e => h[e] = A(m, e, !0, "api")), h.addPageAction = A(m, "addPageAction", !0, n.D.pageAction), h.setCurrentRouteName = A(m, "routeName", !0, n.D.spa), h.setPageViewName = function (t, r) {
                if ("string" == typeof t) return "/" !== t.charAt(0) && (t = "/" + t), (0, i.OP)(e).customTransaction = (r || "http://custom.transaction") + t, A(m, "setPageViewName", !0)();
              }, h.setCustomAttribute = function (e, t) {
                let r = arguments.length > 2 && void 0 !== arguments[2] && arguments[2];
                if ("string" == typeof e) {
                  if (["string", "number"].includes(typeof t) || null === t) return b(e, t, "setCustomAttribute", r);
                  (0, l.Z)("Failed to execute setCustomAttribute.\nNon-null value must be a string or number type, but a type of <".concat(typeof t, "> was provided."));
                } else (0, l.Z)("Failed to execute setCustomAttribute.\nName must be a string type, but a type of <".concat(typeof e, "> was provided."));
              }, h.setUserId = function (e) {
                if ("string" == typeof e || null === e) return b("enduser.id", e, "setUserId", !0);
                (0, l.Z)("Failed to execute setUserId.\nNon-null value must be a string type, but a type of <".concat(typeof e, "> was provided."));
              }, h.setApplicationVersion = function (e) {
                if ("string" == typeof e || null === e) return b("application.version", e, "setApplicationVersion", !1);
                (0, l.Z)("Failed to execute setApplicationVersion. Expected <String | null>, but got <".concat(typeof e, ">."));
              }, h.interaction = function () {
                return new y().get();
              };
              var w = y.prototype = {
                createTracer: function (e, t) {
                  var r = {},
                    i = this,
                    a = "function" == typeof t;
                  return (0, o.p)(v + "tracer", [(0, s.z)(), e, r], i, n.D.spa, p), function () {
                    if (g.emit((a ? "" : "no-") + "fn-start", [(0, s.z)(), i, a], r), a) try {
                      return t.apply(this, arguments);
                    } catch (e) {
                      throw g.emit("fn-err", [arguments, this, e], r), e;
                    } finally {
                      g.emit("fn-end", [(0, s.z)()], r);
                    }
                  };
                }
              };
              function A(e, t, r, i) {
                return function () {
                  return (0, o.p)(f.xS, ["API/" + t + "/called"], void 0, n.D.metrics, p), i && (0, o.p)(e + t, [(0, s.z)(), ...arguments], r ? null : this, i, p), r ? void 0 : this;
                };
              }
              function x() {
                r.e(111).then(r.bind(r, 7438)).then(t => {
                  let {
                    setAPI: r
                  } = t;
                  r(e), (0, c.L)(e, "api");
                }).catch(() => (0, l.Z)("Downloading runtime APIs failed..."));
              }
              return ["actionText", "setName", "setAttribute", "save", "ignore", "onEnd", "getContext", "end", "get"].forEach(e => {
                w[e] = A(v, e, void 0, n.D.spa);
              }), h.noticeError = function (e, t) {
                "string" == typeof e && (e = new Error(e)), (0, o.p)(f.xS, ["API/noticeError/called"], void 0, n.D.metrics, p), (0, o.p)("err", [e, (0, s.z)(), !1, t], void 0, n.D.jserrors, p);
              }, d.il ? (0, u.b)(() => x(), !0) : x(), h;
            }(e, v);
            return (0, h.Qy)(e, _, "api"), (0, h.Qy)(e, x, "exposed"), (0, h.EZ)("activatedFeatures", g.T), _;
          }
        },
        3325: (e, t, r) => {
          r.d(t, {
            D: () => n,
            p: () => i
          });
          const n = {
              ajax: "ajax",
              jserrors: "jserrors",
              metrics: "metrics",
              pageAction: "page_action",
              pageViewEvent: "page_view_event",
              pageViewTiming: "page_view_timing",
              sessionReplay: "session_replay",
              sessionTrace: "session_trace",
              spa: "spa"
            },
            i = {
              [n.pageViewEvent]: 1,
              [n.pageViewTiming]: 2,
              [n.metrics]: 3,
              [n.jserrors]: 4,
              [n.ajax]: 5,
              [n.sessionTrace]: 6,
              [n.pageAction]: 7,
              [n.spa]: 8,
              [n.sessionReplay]: 9
            };
        }
      },
      n = {};
    function i(e) {
      var t = n[e];
      if (void 0 !== t) return t.exports;
      var o = n[e] = {
        exports: {}
      };
      return r[e](o, o.exports, i), o.exports;
    }
    i.m = r, i.d = (e, t) => {
      for (var r in t) i.o(t, r) && !i.o(e, r) && Object.defineProperty(e, r, {
        enumerable: !0,
        get: t[r]
      });
    }, i.f = {}, i.e = e => Promise.all(Object.keys(i.f).reduce((t, r) => (i.f[r](e, t), t), [])), i.u = e => "nr-spa.1097a448-1.238.0.min.js", i.o = (e, t) => Object.prototype.hasOwnProperty.call(e, t), e = {}, t = "NRBA-1.238.0.PROD:", i.l = (r, n, o, a) => {
      if (e[r]) e[r].push(n);else {
        var s, c;
        if (void 0 !== o) for (var u = document.getElementsByTagName("script"), d = 0; d < u.length; d++) {
          var l = u[d];
          if (l.getAttribute("src") == r || l.getAttribute("data-webpack") == t + o) {
            s = l;
            break;
          }
        }
        s || (c = !0, (s = document.createElement("script")).charset = "utf-8", s.timeout = 120, i.nc && s.setAttribute("nonce", i.nc), s.setAttribute("data-webpack", t + o), s.src = r), e[r] = [n];
        var f = (t, n) => {
            s.onerror = s.onload = null, clearTimeout(h);
            var i = e[r];
            if (delete e[r], s.parentNode && s.parentNode.removeChild(s), i && i.forEach(e => e(n)), t) return t(n);
          },
          h = setTimeout(f.bind(null, void 0, {
            type: "timeout",
            target: s
          }), 12e4);
        s.onerror = f.bind(null, s.onerror), s.onload = f.bind(null, s.onload), c && document.head.appendChild(s);
      }
    }, i.r = e => {
      "undefined" != typeof Symbol && Symbol.toStringTag && Object.defineProperty(e, Symbol.toStringTag, {
        value: "Module"
      }), Object.defineProperty(e, "__esModule", {
        value: !0
      });
    }, i.p = "https://js-agent.newrelic.com/", (() => {
      var e = {
        801: 0,
        92: 0
      };
      i.f.j = (t, r) => {
        var n = i.o(e, t) ? e[t] : void 0;
        if (0 !== n) if (n) r.push(n[2]);else {
          var o = new Promise((r, i) => n = e[t] = [r, i]);
          r.push(n[2] = o);
          var a = i.p + i.u(t),
            s = new Error();
          i.l(a, r => {
            if (i.o(e, t) && (0 !== (n = e[t]) && (e[t] = void 0), n)) {
              var o = r && ("load" === r.type ? "missing" : r.type),
                a = r && r.target && r.target.src;
              s.message = "Loading chunk " + t + " failed.\n(" + o + ": " + a + ")", s.name = "ChunkLoadError", s.type = o, s.request = a, n[1](s);
            }
          }, "chunk-" + t, t);
        }
      };
      var t = (t, r) => {
          var n,
            o,
            [a, s, c] = r,
            u = 0;
          if (a.some(t => 0 !== e[t])) {
            for (n in s) i.o(s, n) && (i.m[n] = s[n]);
            if (c) c(i);
          }
          for (t && t(r); u < a.length; u++) o = a[u], i.o(e, o) && e[o] && e[o][0](), e[o] = 0;
        },
        r = self["webpackChunk:NRBA-1.238.0.PROD"] = self["webpackChunk:NRBA-1.238.0.PROD"] || [];
      r.forEach(t.bind(null, 0)), r.push = t.bind(null, r.push.bind(r));
    })(), (() => {
      var e = i(50);
      class t {
        addPageAction(t, r) {
          (0, e.Z)("Call to agent api addPageAction failed. The session trace feature is not currently initialized.");
        }
        setPageViewName(t, r) {
          (0, e.Z)("Call to agent api setPageViewName failed. The page view feature is not currently initialized.");
        }
        setCustomAttribute(t, r, n) {
          (0, e.Z)("Call to agent api setCustomAttribute failed. The js errors feature is not currently initialized.");
        }
        noticeError(t, r) {
          (0, e.Z)("Call to agent api noticeError failed. The js errors feature is not currently initialized.");
        }
        setUserId(t) {
          (0, e.Z)("Call to agent api setUserId failed. The js errors feature is not currently initialized.");
        }
        setApplicationVersion(t) {
          (0, e.Z)("Call to agent api setApplicationVersion failed. The agent is not currently initialized.");
        }
        setErrorHandler(t) {
          (0, e.Z)("Call to agent api setErrorHandler failed. The js errors feature is not currently initialized.");
        }
        finished(t) {
          (0, e.Z)("Call to agent api finished failed. The page action feature is not currently initialized.");
        }
        addRelease(t, r) {
          (0, e.Z)("Call to agent api addRelease failed. The agent is not currently initialized.");
        }
      }
      var r = i(3325),
        n = i(5763);
      const o = Object.values(r.D);
      function a(e) {
        const t = {};
        return o.forEach(r => {
          t[r] = function (e, t) {
            return !1 !== (0, n.Mt)(t, "".concat(e, ".enabled"));
          }(r, e);
        }), t;
      }
      var s = i(9144);
      var c = i(5546),
        u = i(385),
        d = i(8e3),
        l = i(5938),
        f = i(3960);
      class h extends l.W {
        constructor(e, t, r) {
          let n = !(arguments.length > 3 && void 0 !== arguments[3]) || arguments[3];
          super(e, t, r), this.auto = n, this.abortHandler, this.featAggregate, this.onAggregateImported, n && (0, d.R)(e, r);
        }
        importAggregator() {
          let t = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : {};
          if (this.featAggregate || !this.auto) return;
          const r = u.il && !0 === (0, n.Mt)(this.agentIdentifier, "privacy.cookies_enabled");
          let o;
          this.onAggregateImported = new Promise(e => {
            o = e;
          });
          const a = async () => {
            let n;
            try {
              if (r) {
                const {
                  setupAgentSession: e
                } = await i.e(111).then(i.bind(i, 3228));
                n = e(this.agentIdentifier);
              }
            } catch (t) {
              (0, e.Z)("A problem occurred when starting up session manager. This page will not start or extend any session.", t);
            }
            try {
              if (!this.shouldImportAgg(this.featureName, n)) return (0, d.L)(this.agentIdentifier, this.featureName), void o(!1);
              const {
                  lazyFeatureLoader: e
                } = await i.e(111).then(i.bind(i, 8582)),
                {
                  Aggregate: r
                } = await e(this.featureName, "aggregate");
              this.featAggregate = new r(this.agentIdentifier, this.aggregator, t), o(!0);
            } catch (t) {
              (0, e.Z)("Downloading and initializing ".concat(this.featureName, " failed..."), t), this.abortHandler?.(), o(!1);
            }
          };
          u.il ? (0, f.b)(() => a(), !0) : a();
        }
        shouldImportAgg(e, t) {
          return e !== r.D.sessionReplay || !!n.Yu.MO && !1 !== (0, n.Mt)(this.agentIdentifier, "session_trace.enabled") && (!!t?.isNew || !!t?.state.sessionReplay);
        }
      }
      var p = i(7633),
        g = i(7894);
      class m extends h {
        static featureName = p.t9;
        constructor(e, t) {
          let i = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          if (super(e, t, p.t9, i), ("undefined" == typeof PerformanceNavigationTiming || u.Tt) && "undefined" != typeof PerformanceTiming) {
            const t = (0, n.OP)(e);
            t[p.Dz] = Math.max(Date.now() - t.offset, 0), (0, f.K)(() => t[p.qw] = Math.max((0, g.z)() - t[p.Dz], 0)), (0, f.b)(() => {
              const e = (0, g.z)();
              t[p.OJ] = Math.max(e - t[p.Dz], 0), (0, c.p)("timing", ["load", e], void 0, r.D.pageViewTiming, this.ee);
            });
          }
          this.importAggregator();
        }
      }
      var v = i(1117),
        b = i(1284);
      class y extends v.w {
        constructor(e) {
          super(e), this.aggregatedData = {};
        }
        store(e, t, r, n, i) {
          var o = this.getBucket(e, t, r, i);
          return o.metrics = function (e, t) {
            t || (t = {
              count: 0
            });
            return t.count += 1, (0, b.D)(e, function (e, r) {
              t[e] = w(r, t[e]);
            }), t;
          }(n, o.metrics), o;
        }
        merge(e, t, r, n, i) {
          var o = this.getBucket(e, t, n, i);
          if (o.metrics) {
            var a = o.metrics;
            a.count += r.count, (0, b.D)(r, function (e, t) {
              if ("count" !== e) {
                var n = a[e],
                  i = r[e];
                i && !i.c ? a[e] = w(i.t, n) : a[e] = function (e, t) {
                  if (!t) return e;
                  t.c || (t = A(t.t));
                  return t.min = Math.min(e.min, t.min), t.max = Math.max(e.max, t.max), t.t += e.t, t.sos += e.sos, t.c += e.c, t;
                }(i, a[e]);
              }
            });
          } else o.metrics = r;
        }
        storeMetric(e, t, r, n) {
          var i = this.getBucket(e, t, r);
          return i.stats = w(n, i.stats), i;
        }
        getBucket(e, t, r, n) {
          this.aggregatedData[e] || (this.aggregatedData[e] = {});
          var i = this.aggregatedData[e][t];
          return i || (i = this.aggregatedData[e][t] = {
            params: r || {}
          }, n && (i.custom = n)), i;
        }
        get(e, t) {
          return t ? this.aggregatedData[e] && this.aggregatedData[e][t] : this.aggregatedData[e];
        }
        take(e) {
          for (var t = {}, r = "", n = !1, i = 0; i < e.length; i++) t[r = e[i]] = x(this.aggregatedData[r]), t[r].length && (n = !0), delete this.aggregatedData[r];
          return n ? t : null;
        }
      }
      function w(e, t) {
        return null == e ? function (e) {
          e ? e.c++ : e = {
            c: 1
          };
          return e;
        }(t) : t ? (t.c || (t = A(t.t)), t.c += 1, t.t += e, t.sos += e * e, e > t.max && (t.max = e), e < t.min && (t.min = e), t) : {
          t: e
        };
      }
      function A(e) {
        return {
          t: e,
          min: e,
          max: e,
          sos: e * e,
          c: 1
        };
      }
      function x(e) {
        return "object" != typeof e ? [] : (0, b.D)(e, E);
      }
      function E(e, t) {
        return t;
      }
      var T = i(8632),
        _ = i(4402),
        D = i(4351);
      var j = i(7956),
        C = i(3239),
        N = i(9251);
      class O extends h {
        static featureName = N.t;
        constructor(e, t) {
          let r = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, N.t, r), u.il && ((0, n.OP)(e).initHidden = Boolean("hidden" === document.visibilityState), (0, j.N)(() => (0, c.p)("docHidden", [(0, g.z)()], void 0, N.t, this.ee), !0), (0, C.bP)("pagehide", () => (0, c.p)("winPagehide", [(0, g.z)()], void 0, N.t, this.ee)), this.importAggregator());
        }
      }
      var S = i(3081);
      class P extends h {
        static featureName = S.t9;
        constructor(e, t) {
          let r = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, S.t9, r), this.importAggregator();
        }
      }
      var R = i(6660);
      class I {
        constructor(e, t, r, n) {
          this.name = "UncaughtError", this.message = e, this.sourceURL = t, this.line = r, this.column = n;
        }
      }
      class k extends h {
        static featureName = R.t;
        #e = new Set();
        constructor(e, t) {
          let n = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, R.t, n);
          try {
            this.removeOnAbort = new AbortController();
          } catch (e) {}
          this.ee.on("fn-err", (e, t, n) => {
            this.abortHandler && !this.#e.has(n) && (this.#e.add(n), (0, c.p)("err", [this.#t(n), (0, g.z)()], void 0, r.D.jserrors, this.ee));
          }), this.ee.on("internal-error", e => {
            this.abortHandler && (0, c.p)("ierr", [this.#t(e), (0, g.z)(), !0], void 0, r.D.jserrors, this.ee);
          }), u._A.addEventListener("unhandledrejection", e => {
            this.abortHandler && (0, c.p)("err", [this.#r(e), (0, g.z)(), !1, {
              unhandledPromiseRejection: 1
            }], void 0, r.D.jserrors, this.ee);
          }, (0, C.m$)(!1, this.removeOnAbort?.signal)), u._A.addEventListener("error", e => {
            this.abortHandler && (this.#e.has(e.error) ? this.#e.delete(e.error) : (0, c.p)("err", [this.#n(e), (0, g.z)()], void 0, r.D.jserrors, this.ee));
          }, (0, C.m$)(!1, this.removeOnAbort?.signal)), this.abortHandler = this.#i, this.importAggregator();
        }
        #i() {
          this.removeOnAbort?.abort(), this.#e.clear(), this.abortHandler = void 0;
        }
        #t(e) {
          return e instanceof Error ? e : void 0 !== e?.message ? new I(e.message, e.filename || e.sourceURL, e.lineno || e.line, e.colno || e.col) : new I("string" == typeof e ? e : (0, D.P)(e));
        }
        #r(e) {
          let t = "Unhandled Promise Rejection: ";
          if (e?.reason instanceof Error) try {
            return e.reason.message = t + e.reason.message, e.reason;
          } catch (t) {
            return e.reason;
          }
          if (void 0 === e.reason) return new I(t);
          const r = this.#t(e.reason);
          return r.message = t + r.message, r;
        }
        #n(e) {
          return e.error instanceof Error ? e.error : new I(e.message, e.filename, e.lineno, e.colno);
        }
      }
      var H = i(2210);
      let z = 1;
      const L = "nr@id";
      function M(e) {
        const t = typeof e;
        return !e || "object" !== t && "function" !== t ? -1 : e === u._A ? 0 : (0, H.X)(e, L, function () {
          return z++;
        });
      }
      function B(e) {
        if ("string" == typeof e && e.length) return e.length;
        if ("object" == typeof e) {
          if ("undefined" != typeof ArrayBuffer && e instanceof ArrayBuffer && e.byteLength) return e.byteLength;
          if ("undefined" != typeof Blob && e instanceof Blob && e.size) return e.size;
          if (!("undefined" != typeof FormData && e instanceof FormData)) try {
            return (0, D.P)(e).length;
          } catch (e) {
            return;
          }
        }
      }
      var F = i(1214),
        U = i(7243);
      class q {
        constructor(e) {
          this.agentIdentifier = e;
        }
        generateTracePayload(e) {
          if (!this.shouldGenerateTrace(e)) return null;
          var t = (0, n.DL)(this.agentIdentifier);
          if (!t) return null;
          var r = (t.accountID || "").toString() || null,
            i = (t.agentID || "").toString() || null,
            o = (t.trustKey || "").toString() || null;
          if (!r || !i) return null;
          var a = (0, _.M)(),
            s = (0, _.Ht)(),
            c = Date.now(),
            u = {
              spanId: a,
              traceId: s,
              timestamp: c
            };
          return (e.sameOrigin || this.isAllowedOrigin(e) && this.useTraceContextHeadersForCors()) && (u.traceContextParentHeader = this.generateTraceContextParentHeader(a, s), u.traceContextStateHeader = this.generateTraceContextStateHeader(a, c, r, i, o)), (e.sameOrigin && !this.excludeNewrelicHeader() || !e.sameOrigin && this.isAllowedOrigin(e) && this.useNewrelicHeaderForCors()) && (u.newrelicHeader = this.generateTraceHeader(a, s, c, r, i, o)), u;
        }
        generateTraceContextParentHeader(e, t) {
          return "00-" + t + "-" + e + "-01";
        }
        generateTraceContextStateHeader(e, t, r, n, i) {
          return i + "@nr=0-1-" + r + "-" + n + "-" + e + "----" + t;
        }
        generateTraceHeader(e, t, r, n, i, o) {
          if (!("function" == typeof u._A?.btoa)) return null;
          var a = {
            v: [0, 1],
            d: {
              ty: "Browser",
              ac: n,
              ap: i,
              id: e,
              tr: t,
              ti: r
            }
          };
          return o && n !== o && (a.d.tk = o), btoa((0, D.P)(a));
        }
        shouldGenerateTrace(e) {
          return this.isDtEnabled() && this.isAllowedOrigin(e);
        }
        isAllowedOrigin(e) {
          var t = !1,
            r = {};
          if ((0, n.Mt)(this.agentIdentifier, "distributed_tracing") && (r = (0, n.P_)(this.agentIdentifier).distributed_tracing), e.sameOrigin) t = !0;else if (r.allowed_origins instanceof Array) for (var i = 0; i < r.allowed_origins.length; i++) {
            var o = (0, U.e)(r.allowed_origins[i]);
            if (e.hostname === o.hostname && e.protocol === o.protocol && e.port === o.port) {
              t = !0;
              break;
            }
          }
          return t;
        }
        isDtEnabled() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !!e.enabled;
        }
        excludeNewrelicHeader() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !!e.exclude_newrelic_header;
        }
        useNewrelicHeaderForCors() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !1 !== e.cors_use_newrelic_header;
        }
        useTraceContextHeadersForCors() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !!e.cors_use_tracecontext_headers;
        }
      }
      var Z = i(7825),
        V = ["load", "error", "abort", "timeout"],
        G = V.length,
        W = n.Yu.REQ,
        X = n.Yu.XHR;
      class Q extends h {
        static featureName = Z.t;
        constructor(e, t) {
          let i = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, Z.t, i), (0, n.OP)(e).xhrWrappable && (this.dt = new q(e), this.handler = (e, t, r, n) => (0, c.p)(e, t, r, n, this.ee), (0, F.u5)(this.ee), (0, F.Kf)(this.ee), function (e, t, i, o) {
            function a(e) {
              var t = this;
              t.totalCbs = 0, t.called = 0, t.cbTime = 0, t.end = E, t.ended = !1, t.xhrGuids = {}, t.lastSize = null, t.loadCaptureCalled = !1, t.params = this.params || {}, t.metrics = this.metrics || {}, e.addEventListener("load", function (r) {
                _(t, e);
              }, (0, C.m$)(!1)), u.IF || e.addEventListener("progress", function (e) {
                t.lastSize = e.loaded;
              }, (0, C.m$)(!1));
            }
            function s(e) {
              this.params = {
                method: e[0]
              }, T(this, e[1]), this.metrics = {};
            }
            function c(t, r) {
              var i = (0, n.DL)(e);
              i.xpid && this.sameOrigin && r.setRequestHeader("X-NewRelic-ID", i.xpid);
              var a = o.generateTracePayload(this.parsedOrigin);
              if (a) {
                var s = !1;
                a.newrelicHeader && (r.setRequestHeader("newrelic", a.newrelicHeader), s = !0), a.traceContextParentHeader && (r.setRequestHeader("traceparent", a.traceContextParentHeader), a.traceContextStateHeader && r.setRequestHeader("tracestate", a.traceContextStateHeader), s = !0), s && (this.dt = a);
              }
            }
            function d(e, r) {
              var n = this.metrics,
                i = e[0],
                o = this;
              if (n && i) {
                var a = B(i);
                a && (n.txSize = a);
              }
              this.startTime = (0, g.z)(), this.listener = function (e) {
                try {
                  "abort" !== e.type || o.loadCaptureCalled || (o.params.aborted = !0), ("load" !== e.type || o.called === o.totalCbs && (o.onloadCalled || "function" != typeof r.onload) && "function" == typeof o.end) && o.end(r);
                } catch (e) {
                  try {
                    t.emit("internal-error", [e]);
                  } catch (e) {}
                }
              };
              for (var s = 0; s < G; s++) r.addEventListener(V[s], this.listener, (0, C.m$)(!1));
            }
            function l(e, t, r) {
              this.cbTime += e, t ? this.onloadCalled = !0 : this.called += 1, this.called !== this.totalCbs || !this.onloadCalled && "function" == typeof r.onload || "function" != typeof this.end || this.end(r);
            }
            function f(e, t) {
              var r = "" + M(e) + !!t;
              this.xhrGuids && !this.xhrGuids[r] && (this.xhrGuids[r] = !0, this.totalCbs += 1);
            }
            function h(e, t) {
              var r = "" + M(e) + !!t;
              this.xhrGuids && this.xhrGuids[r] && (delete this.xhrGuids[r], this.totalCbs -= 1);
            }
            function p() {
              this.endTime = (0, g.z)();
            }
            function m(e, r) {
              r instanceof X && "load" === e[0] && t.emit("xhr-load-added", [e[1], e[2]], r);
            }
            function v(e, r) {
              r instanceof X && "load" === e[0] && t.emit("xhr-load-removed", [e[1], e[2]], r);
            }
            function b(e, t, r) {
              t instanceof X && ("onload" === r && (this.onload = !0), ("load" === (e[0] && e[0].type) || this.onload) && (this.xhrCbStart = (0, g.z)()));
            }
            function y(e, r) {
              this.xhrCbStart && t.emit("xhr-cb-time", [(0, g.z)() - this.xhrCbStart, this.onload, r], r);
            }
            function w(e) {
              var t,
                r = e[1] || {};
              if ("string" == typeof e[0] ? 0 === (t = e[0]).length && u.il && (t = "" + u._A.location.href) : e[0] && e[0].url ? t = e[0].url : u._A?.URL && e[0] && e[0] instanceof URL ? t = e[0].href : "function" == typeof e[0].toString && (t = e[0].toString()), "string" == typeof t && 0 !== t.length) {
                t && (this.parsedOrigin = (0, U.e)(t), this.sameOrigin = this.parsedOrigin.sameOrigin);
                var n = o.generateTracePayload(this.parsedOrigin);
                if (n && (n.newrelicHeader || n.traceContextParentHeader)) if (e[0] && e[0].headers) s(e[0].headers, n) && (this.dt = n);else {
                  var i = {};
                  for (var a in r) i[a] = r[a];
                  i.headers = new Headers(r.headers || {}), s(i.headers, n) && (this.dt = n), e.length > 1 ? e[1] = i : e.push(i);
                }
              }
              function s(e, t) {
                var r = !1;
                return t.newrelicHeader && (e.set("newrelic", t.newrelicHeader), r = !0), t.traceContextParentHeader && (e.set("traceparent", t.traceContextParentHeader), t.traceContextStateHeader && e.set("tracestate", t.traceContextStateHeader), r = !0), r;
              }
            }
            function A(e, t) {
              this.params = {}, this.metrics = {}, this.startTime = (0, g.z)(), this.dt = t, e.length >= 1 && (this.target = e[0]), e.length >= 2 && (this.opts = e[1]);
              var r,
                n = this.opts || {},
                i = this.target;
              "string" == typeof i ? r = i : "object" == typeof i && i instanceof W ? r = i.url : u._A?.URL && "object" == typeof i && i instanceof URL && (r = i.href), T(this, r);
              var o = ("" + (i && i instanceof W && i.method || n.method || "GET")).toUpperCase();
              this.params.method = o, this.txSize = B(n.body) || 0;
            }
            function x(e, t) {
              var n;
              this.endTime = (0, g.z)(), this.params || (this.params = {}), this.params.status = t ? t.status : 0, "string" == typeof this.rxSize && this.rxSize.length > 0 && (n = +this.rxSize);
              var o = {
                txSize: this.txSize,
                rxSize: n,
                duration: (0, g.z)() - this.startTime
              };
              i("xhr", [this.params, o, this.startTime, this.endTime, "fetch"], this, r.D.ajax);
            }
            function E(e) {
              var t = this.params,
                n = this.metrics;
              if (!this.ended) {
                this.ended = !0;
                for (var o = 0; o < G; o++) e.removeEventListener(V[o], this.listener, !1);
                t.aborted || (n.duration = (0, g.z)() - this.startTime, this.loadCaptureCalled || 4 !== e.readyState ? null == t.status && (t.status = 0) : _(this, e), n.cbTime = this.cbTime, i("xhr", [t, n, this.startTime, this.endTime, "xhr"], this, r.D.ajax));
              }
            }
            function T(e, t) {
              var r = (0, U.e)(t),
                n = e.params;
              n.hostname = r.hostname, n.port = r.port, n.protocol = r.protocol, n.host = r.hostname + ":" + r.port, n.pathname = r.pathname, e.parsedOrigin = r, e.sameOrigin = r.sameOrigin;
            }
            function _(e, t) {
              e.params.status = t.status;
              var r = function (e, t) {
                var r = e.responseType;
                return "json" === r && null !== t ? t : "arraybuffer" === r || "blob" === r || "json" === r ? B(e.response) : "text" === r || "" === r || void 0 === r ? B(e.responseText) : void 0;
              }(t, e.lastSize);
              if (r && (e.metrics.rxSize = r), e.sameOrigin) {
                var n = t.getResponseHeader("X-NewRelic-App-Data");
                n && (e.params.cat = n.split(", ").pop());
              }
              e.loadCaptureCalled = !0;
            }
            t.on("new-xhr", a), t.on("open-xhr-start", s), t.on("open-xhr-end", c), t.on("send-xhr-start", d), t.on("xhr-cb-time", l), t.on("xhr-load-added", f), t.on("xhr-load-removed", h), t.on("xhr-resolved", p), t.on("addEventListener-end", m), t.on("removeEventListener-end", v), t.on("fn-end", y), t.on("fetch-before-start", w), t.on("fetch-start", A), t.on("fn-start", b), t.on("fetch-done", x);
          }(e, this.ee, this.handler, this.dt), this.importAggregator());
        }
      }
      var K = i(3614);
      const {
        BST_RESOURCE: Y,
        RESOURCE: J,
        START: ee,
        END: te,
        FEATURE_NAME: re,
        FN_END: ne,
        FN_START: ie,
        PUSH_STATE: oe
      } = K;
      var ae = i(7836);
      const {
        FEATURE_NAME: se,
        START: ce,
        END: ue,
        BODY: de,
        CB_END: le,
        JS_TIME: fe,
        FETCH: he,
        FN_START: pe,
        CB_START: ge,
        FN_END: me
      } = ae;
      var ve = i(4649);
      class be extends h {
        static featureName = ve.t;
        constructor(e, t) {
          let r = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, ve.t, r), this.importAggregator();
        }
      }
      new class extends t {
        constructor(t) {
          let r = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : (0, _.ky)(16);
          super(), u._A ? (this.agentIdentifier = r, this.sharedAggregator = new y({
            agentIdentifier: this.agentIdentifier
          }), this.features = {}, this.desiredFeatures = new Set(t.features || []), this.desiredFeatures.add(m), Object.assign(this, (0, s.j)(this.agentIdentifier, t, t.loaderType || "agent")), this.start()) : (0, e.Z)("Failed to initial the agent. Could not determine the runtime environment.");
        }
        get config() {
          return {
            info: (0, n.C5)(this.agentIdentifier),
            init: (0, n.P_)(this.agentIdentifier),
            loader_config: (0, n.DL)(this.agentIdentifier),
            runtime: (0, n.OP)(this.agentIdentifier)
          };
        }
        start() {
          const t = "features";
          try {
            const n = a(this.agentIdentifier),
              i = [...this.desiredFeatures];
            i.sort((e, t) => r.p[e.featureName] - r.p[t.featureName]), i.forEach(t => {
              if (n[t.featureName] || t.featureName === r.D.pageViewEvent) {
                const i = function (e) {
                  switch (e) {
                    case r.D.ajax:
                      return [r.D.jserrors];
                    case r.D.sessionTrace:
                      return [r.D.ajax, r.D.pageViewEvent];
                    case r.D.sessionReplay:
                      return [r.D.sessionTrace];
                    case r.D.pageViewTiming:
                      return [r.D.pageViewEvent];
                    default:
                      return [];
                  }
                }(t.featureName);
                i.every(e => n[e]) || (0, e.Z)("".concat(t.featureName, " is enabled but one or more dependent features has been disabled (").concat((0, D.P)(i), "). This may cause unintended consequences or missing data...")), this.features[t.featureName] = new t(this.agentIdentifier, this.sharedAggregator);
              }
            }), (0, T.Qy)(this.agentIdentifier, this.features, t);
          } catch (r) {
            (0, e.Z)("Failed to initialize all enabled instrument classes (agent aborted) -", r);
            for (const e in this.features) this.features[e].abortHandler?.();
            const n = (0, T.fP)();
            return delete n.initializedAgents[this.agentIdentifier]?.api, delete n.initializedAgents[this.agentIdentifier]?.[t], delete this.sharedAggregator, n.ee?.abort(), delete n.ee?.get(this.agentIdentifier), !1;
          }
        }
        addToTrace(t) {
          (0, e.Z)("Call to agent api addToTrace failed. The page action feature is not currently initialized.");
        }
        setCurrentRouteName(t) {
          (0, e.Z)("Call to agent api setCurrentRouteName failed. The spa feature is not currently initialized.");
        }
        interaction() {
          (0, e.Z)("Call to agent api interaction failed. The spa feature is not currently initialized.");
        }
      }({
        features: [Q, m, O, class extends h {
          static featureName = re;
          constructor(e, t) {
            if (super(e, t, re, !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2]), !u.il) return;
            const n = this.ee;
            let i;
            (0, F.QU)(n), this.eventsEE = (0, F.em)(n), this.eventsEE.on(ie, function (e, t) {
              this.bstStart = (0, g.z)();
            }), this.eventsEE.on(ne, function (e, t) {
              (0, c.p)("bst", [e[0], t, this.bstStart, (0, g.z)()], void 0, r.D.sessionTrace, n);
            }), n.on(oe + ee, function (e) {
              this.time = (0, g.z)(), this.startPath = location.pathname + location.hash;
            }), n.on(oe + te, function (e) {
              (0, c.p)("bstHist", [location.pathname + location.hash, this.startPath, this.time], void 0, r.D.sessionTrace, n);
            });
            try {
              i = new PerformanceObserver(e => {
                const t = e.getEntries();
                (0, c.p)(Y, [t], void 0, r.D.sessionTrace, n);
              }), i.observe({
                type: J,
                buffered: !0
              });
            } catch (e) {}
            this.importAggregator({
              resourceObserver: i
            });
          }
        }, P, be, k, class extends h {
          static featureName = se;
          constructor(e, t) {
            if (super(e, t, se, !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2]), !u.il) return;
            if (!(0, n.OP)(e).xhrWrappable) return;
            try {
              this.removeOnAbort = new AbortController();
            } catch (e) {}
            let r,
              i = 0;
            const o = this.ee.get("tracer"),
              a = (0, F._L)(this.ee),
              s = (0, F.Lg)(this.ee),
              c = (0, F.BV)(this.ee),
              d = (0, F.Kf)(this.ee),
              l = this.ee.get("events"),
              f = (0, F.u5)(this.ee),
              h = (0, F.QU)(this.ee),
              p = (0, F.Gm)(this.ee);
            function m(e, t) {
              h.emit("newURL", ["" + window.location, t]);
            }
            function v() {
              i++, r = window.location.hash, this[pe] = (0, g.z)();
            }
            function b() {
              i--, window.location.hash !== r && m(0, !0);
              var e = (0, g.z)();
              this[fe] = ~~this[fe] + e - this[pe], this[me] = e;
            }
            function y(e, t) {
              e.on(t, function () {
                this[t] = (0, g.z)();
              });
            }
            this.ee.on(pe, v), s.on(ge, v), a.on(ge, v), this.ee.on(me, b), s.on(le, b), a.on(le, b), this.ee.buffer([pe, me, "xhr-resolved"], this.featureName), l.buffer([pe], this.featureName), c.buffer(["setTimeout" + ue, "clearTimeout" + ce, pe], this.featureName), d.buffer([pe, "new-xhr", "send-xhr" + ce], this.featureName), f.buffer([he + ce, he + "-done", he + de + ce, he + de + ue], this.featureName), h.buffer(["newURL"], this.featureName), p.buffer([pe], this.featureName), s.buffer(["propagate", ge, le, "executor-err", "resolve" + ce], this.featureName), o.buffer([pe, "no-" + pe], this.featureName), a.buffer(["new-jsonp", "cb-start", "jsonp-error", "jsonp-end"], this.featureName), y(f, he + ce), y(f, he + "-done"), y(a, "new-jsonp"), y(a, "jsonp-end"), y(a, "cb-start"), h.on("pushState-end", m), h.on("replaceState-end", m), window.addEventListener("hashchange", m, (0, C.m$)(!0, this.removeOnAbort?.signal)), window.addEventListener("load", m, (0, C.m$)(!0, this.removeOnAbort?.signal)), window.addEventListener("popstate", function () {
              m(0, i > 1);
            }, (0, C.m$)(!0, this.removeOnAbort?.signal)), this.abortHandler = this.#i, this.importAggregator();
          }
          #i() {
            this.removeOnAbort?.abort(), this.abortHandler = void 0;
          }
        }],
        loaderType: "spa"
      });
    })();
  })();
})()</script>
      <link rel="shortcut icon" href="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/103/images/favSD.ico" type="image/x-icon">
      <link rel="icon" href="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/103/images/favSD.ico" type="image/x-icon">
      <link rel="stylesheet" href="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/b64013ec63c69e3d916174cbebae89d65b2419e1/arp.css">
      <link href="//cdn.pendo.io" rel="dns-prefetch">
      <link href="https://cdn.pendo.io" rel="preconnect" crossorigin="anonymous">
      <link rel="dns-prefetch" href="https://smetrics.elsevier.com">
      <script async="" id="reading-assistant-script-tag" src="/feature/assets/ai-components/S1053811915008873?client=arp&amp;componentVersion=V11" type="text/javascript"></script>
      <script type="text/javascript">
        var targetServerState = JSON.stringify({"4D6368F454EC41940A4C98A6@AdobeOrg":{"sdid":{"supplementalDataIDCurrent":"6598C47C2A0EE6C1-0C7D38CA345DC735","supplementalDataIDCurrentConsumed":{"payload:target-global-mbox":true},"supplementalDataIDLastConsumed":{}}}});
        window.appData = window.appData || [];
        window.pageTargeting = {"region":"eu-west-1","platform":"sdtech","entitled":true,"crawler":"","journal":"NeuroImage","auth":"AE"};
        window.arp = {
          config: {"adobeSuite":"elsevier-sd-prod","arsUrl":"https://ars.els-cdn.com","recommendationsFeedback":{"enabled":true,"url":"https://feedback.recs.d.elsevier.com/raw/events","timeout":60000},"googleMapsApiKey":"AIzaSyCBYU6I6lrbEU6wQXUEIte3NwGtm3jwHQc","mediaBaseUrl":"https://ars.els-cdn.com/content/image/","strictMode":false,"seamlessAccess":{"enableSeamlessAccess":true,"scriptUrl":"https://unpkg.com/@theidentityselector/thiss-ds@1.0.13/dist/thiss-ds.js","persistenceUrl":"https://service.seamlessaccess.org/ps/","persistenceContext":"seamlessaccess.org","scienceDirectUrl":"https://www.sciencedirect.com","shibAuthUrl":"https://auth.elsevier.com/ShibAuth/institutionLogin"},"reaxys":{"apiUrl":"https://reaxys-sdlc.reaxys.com","origin":"sciencedirect","queryBuilderHostPath":"https://www.reaxys.com/reaxys/secured/hopinto.do","url":"https://www.reaxys.com"},"oneTrustCookie":{"enabled":true},"ssrn":{"url":"https://papers.ssrn.com","path":"/sol3/papers.cfm"},"plumX":"https://api.plu.mx/widget/elsevier/artifact","assetRoute":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/b64013ec63c69e3d916174cbebae89d65b2419e1"},
          subscriptions: [],
          subscribe: function(cb) {
            var self = this;
            var i = this.subscriptions.push(cb) - 1;
            return function unsubscribe() {
              self.subscriptions.splice(i, 1);
            }
          },
        };
        window.addEventListener('beforeprint', () => pendo.onGuideDismissed());
      </script>
    <script data-cfasync="false" src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" data-domain-script="865ea198-88cc-4e41-8952-1df75d554d02"></script><meta http-equiv="origin-trial" content="AlK2UR5SkAlj8jjdEc9p3F3xuFYlF6LYjAML3EOqw1g26eCwWPjdmecULvBH5MVPoqKYrOfPhYVL71xAXI1IBQoAAAB8eyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="Amm8/NmvvQfhwCib6I7ZsmUxiSCfOxWxHayJwyU1r3gRIItzr7bNQid6O8ZYaE1GSQTa69WwhPC9flq/oYkRBwsAAACCeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="A9wSqI5i0iwGdf6L1CERNdmsTPgVu44ewj8QxTBYgsv1LCPUVF7YmWOvTappqB1139jAymxUW/RO8zmMqo4zlAAAAACNeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiRmxlZGdlQmlkZGluZ0FuZEF1Y3Rpb25TZXJ2ZXIiLCJleHBpcnkiOjE3MzY4MTI4MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><meta http-equiv="origin-trial" content="A+d7vJfYtay4OUbdtRPZA3y7bKQLsxaMEPmxgfhBGqKXNrdkCQeJlUwqa6EBbSfjwFtJWTrWIioXeMW+y8bWAgQAAACTeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiRmxlZGdlQmlkZGluZ0FuZEF1Y3Rpb25TZXJ2ZXIiLCJleHBpcnkiOjE3MzY4MTI4MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><script src="https://securepubads.g.doubleclick.net/pagead/managed/js/gpt/m202412090101/pubads_impl.js" async=""></script><link href="https://securepubads.g.doubleclick.net/pagead/managed/dict/m202412050101/gpt" rel="compression-dictionary"><script src="https://unpkg.com/@theidentityselector/thiss-ds@1.0.13/dist/thiss-ds.js" async=""></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><script src="https://cdn.cookielaw.org/scripttemplates/202402.1.0/otBannerSdk.js" async="" type="text/javascript"></script><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
</style><style id="onetrust-style">#onetrust-banner-sdk{-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}#onetrust-banner-sdk .onetrust-vendors-list-handler{cursor:pointer;color:#1f96db;font-size:inherit;font-weight:bold;text-decoration:none;margin-left:5px}#onetrust-banner-sdk .onetrust-vendors-list-handler:hover{color:#1f96db}#onetrust-banner-sdk:focus{outline:2px solid #000;outline-offset:-2px}#onetrust-banner-sdk a:focus{outline:2px solid #000}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{outline-offset:1px}#onetrust-banner-sdk.ot-bnr-w-logo .ot-bnr-logo{height:64px;width:64px}#onetrust-banner-sdk .ot-tcf2-vendor-count.ot-text-bold{font-weight:bold}#onetrust-banner-sdk .ot-close-icon,#onetrust-pc-sdk .ot-close-icon,#ot-sync-ntfy .ot-close-icon{background-size:contain;background-repeat:no-repeat;background-position:center;height:12px;width:12px}#onetrust-banner-sdk .powered-by-logo,#onetrust-banner-sdk .ot-pc-footer-logo a,#onetrust-pc-sdk .powered-by-logo,#onetrust-pc-sdk .ot-pc-footer-logo a,#ot-sync-ntfy .powered-by-logo,#ot-sync-ntfy .ot-pc-footer-logo a{background-size:contain;background-repeat:no-repeat;background-position:center;height:25px;width:152px;display:block;text-decoration:none;font-size:.75em}#onetrust-banner-sdk .powered-by-logo:hover,#onetrust-banner-sdk .ot-pc-footer-logo a:hover,#onetrust-pc-sdk .powered-by-logo:hover,#onetrust-pc-sdk .ot-pc-footer-logo a:hover,#ot-sync-ntfy .powered-by-logo:hover,#ot-sync-ntfy .ot-pc-footer-logo a:hover{color:#565656}#onetrust-banner-sdk h3 *,#onetrust-banner-sdk h4 *,#onetrust-banner-sdk h6 *,#onetrust-banner-sdk button *,#onetrust-banner-sdk a[data-parent-id] *,#onetrust-pc-sdk h3 *,#onetrust-pc-sdk h4 *,#onetrust-pc-sdk h6 *,#onetrust-pc-sdk button *,#onetrust-pc-sdk a[data-parent-id] *,#ot-sync-ntfy h3 *,#ot-sync-ntfy h4 *,#ot-sync-ntfy h6 *,#ot-sync-ntfy button *,#ot-sync-ntfy a[data-parent-id] *{font-size:inherit;font-weight:inherit;color:inherit}#onetrust-banner-sdk .ot-hide,#onetrust-pc-sdk .ot-hide,#ot-sync-ntfy .ot-hide{display:none !important}#onetrust-banner-sdk button.ot-link-btn:hover,#onetrust-pc-sdk button.ot-link-btn:hover,#ot-sync-ntfy button.ot-link-btn:hover{text-decoration:underline;opacity:1}#onetrust-pc-sdk .ot-sdk-row .ot-sdk-column{padding:0}#onetrust-pc-sdk .ot-sdk-container{padding-right:0}#onetrust-pc-sdk .ot-sdk-row{flex-direction:initial;width:100%}#onetrust-pc-sdk [type=checkbox]:checked,#onetrust-pc-sdk [type=checkbox]:not(:checked){pointer-events:initial}#onetrust-pc-sdk [type=checkbox]:disabled+label::before,#onetrust-pc-sdk [type=checkbox]:disabled+label:after,#onetrust-pc-sdk [type=checkbox]:disabled+label{pointer-events:none;opacity:.7}#onetrust-pc-sdk #vendor-list-content{transform:translate3d(0, 0, 0)}#onetrust-pc-sdk li input[type=checkbox]{z-index:1}#onetrust-pc-sdk li .ot-checkbox label{z-index:2}#onetrust-pc-sdk li .ot-checkbox input[type=checkbox]{height:auto;width:auto}#onetrust-pc-sdk li .host-title a,#onetrust-pc-sdk li .ot-host-name a,#onetrust-pc-sdk li .accordion-text,#onetrust-pc-sdk li .ot-acc-txt{z-index:2;position:relative}#onetrust-pc-sdk input{margin:3px .1ex}#onetrust-pc-sdk .pc-logo,#onetrust-pc-sdk .ot-pc-logo{height:60px;width:180px;background-position:center;background-size:contain;background-repeat:no-repeat;display:inline-flex;justify-content:center;align-items:center}#onetrust-pc-sdk .pc-logo img,#onetrust-pc-sdk .ot-pc-logo img{max-height:100%;max-width:100%}#onetrust-pc-sdk .screen-reader-only,#onetrust-pc-sdk .ot-scrn-rdr,.ot-sdk-cookie-policy .screen-reader-only,.ot-sdk-cookie-policy .ot-scrn-rdr{border:0;clip:rect(0 0 0 0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}#onetrust-pc-sdk.ot-fade-in,.onetrust-pc-dark-filter.ot-fade-in,#onetrust-banner-sdk.ot-fade-in{animation-name:onetrust-fade-in;animation-duration:400ms;animation-timing-function:ease-in-out}#onetrust-pc-sdk.ot-hide{display:none !important}.onetrust-pc-dark-filter.ot-hide{display:none !important}#ot-sdk-btn.ot-sdk-show-settings,#ot-sdk-btn.optanon-show-settings{color:#68b631;border:1px solid #68b631;height:auto;white-space:normal;word-wrap:break-word;padding:.8em 2em;font-size:.8em;line-height:1.2;cursor:pointer;-moz-transition:.1s ease;-o-transition:.1s ease;-webkit-transition:1s ease;transition:.1s ease}#ot-sdk-btn.ot-sdk-show-settings:hover,#ot-sdk-btn.optanon-show-settings:hover{color:#fff;background-color:#68b631}.onetrust-pc-dark-filter{background:rgba(0,0,0,.5);z-index:2147483646;width:100%;height:100%;overflow:hidden;position:fixed;top:0;bottom:0;left:0}@keyframes onetrust-fade-in{0%{opacity:0}100%{opacity:1}}.ot-cookie-label{text-decoration:underline}@media only screen and (min-width: 426px)and (max-width: 896px)and (orientation: landscape){#onetrust-pc-sdk p{font-size:.75em}}#onetrust-banner-sdk .banner-option-input:focus+label{outline:1px solid #000;outline-style:auto}.category-vendors-list-handler+a:focus,.category-vendors-list-handler+a:focus-visible{outline:2px solid #000}#onetrust-pc-sdk .ot-userid-title{margin-top:10px}#onetrust-pc-sdk .ot-userid-title>span,#onetrust-pc-sdk .ot-userid-timestamp>span{font-weight:700}#onetrust-pc-sdk .ot-userid-desc{font-style:italic}#onetrust-pc-sdk .ot-host-desc a{pointer-events:initial}#onetrust-pc-sdk .ot-ven-hdr>p a{position:relative;z-index:2;pointer-events:initial}#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info a,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info a{margin-right:auto}#onetrust-pc-sdk .ot-pc-footer-logo img{width:136px;height:16px}#onetrust-pc-sdk .ot-pur-vdr-count{font-weight:400;font-size:.7rem;padding-top:3px;display:block}#onetrust-banner-sdk .ot-optout-signal,#onetrust-pc-sdk .ot-optout-signal{border:1px solid #32ae88;border-radius:3px;padding:5px;margin-bottom:10px;background-color:#f9fffa;font-size:.85rem;line-height:2}#onetrust-banner-sdk .ot-optout-signal .ot-optout-icon,#onetrust-pc-sdk .ot-optout-signal .ot-optout-icon{display:inline;margin-right:5px}#onetrust-banner-sdk .ot-optout-signal svg,#onetrust-pc-sdk .ot-optout-signal svg{height:20px;width:30px;transform:scale(0.5)}#onetrust-banner-sdk .ot-optout-signal svg path,#onetrust-pc-sdk .ot-optout-signal svg path{fill:#32ae88}#onetrust-banner-sdk,#onetrust-pc-sdk,#ot-sdk-cookie-policy,#ot-sync-ntfy{font-size:16px}#onetrust-banner-sdk *,#onetrust-banner-sdk ::after,#onetrust-banner-sdk ::before,#onetrust-pc-sdk *,#onetrust-pc-sdk ::after,#onetrust-pc-sdk ::before,#ot-sdk-cookie-policy *,#ot-sdk-cookie-policy ::after,#ot-sdk-cookie-policy ::before,#ot-sync-ntfy *,#ot-sync-ntfy ::after,#ot-sync-ntfy ::before{-webkit-box-sizing:content-box;-moz-box-sizing:content-box;box-sizing:content-box}#onetrust-banner-sdk div,#onetrust-banner-sdk span,#onetrust-banner-sdk h1,#onetrust-banner-sdk h2,#onetrust-banner-sdk h3,#onetrust-banner-sdk h4,#onetrust-banner-sdk h5,#onetrust-banner-sdk h6,#onetrust-banner-sdk p,#onetrust-banner-sdk img,#onetrust-banner-sdk svg,#onetrust-banner-sdk button,#onetrust-banner-sdk section,#onetrust-banner-sdk a,#onetrust-banner-sdk label,#onetrust-banner-sdk input,#onetrust-banner-sdk ul,#onetrust-banner-sdk li,#onetrust-banner-sdk nav,#onetrust-banner-sdk table,#onetrust-banner-sdk thead,#onetrust-banner-sdk tr,#onetrust-banner-sdk td,#onetrust-banner-sdk tbody,#onetrust-banner-sdk .ot-main-content,#onetrust-banner-sdk .ot-toggle,#onetrust-banner-sdk #ot-content,#onetrust-banner-sdk #ot-pc-content,#onetrust-banner-sdk .checkbox,#onetrust-pc-sdk div,#onetrust-pc-sdk span,#onetrust-pc-sdk h1,#onetrust-pc-sdk h2,#onetrust-pc-sdk h3,#onetrust-pc-sdk h4,#onetrust-pc-sdk h5,#onetrust-pc-sdk h6,#onetrust-pc-sdk p,#onetrust-pc-sdk img,#onetrust-pc-sdk svg,#onetrust-pc-sdk button,#onetrust-pc-sdk section,#onetrust-pc-sdk a,#onetrust-pc-sdk label,#onetrust-pc-sdk input,#onetrust-pc-sdk ul,#onetrust-pc-sdk li,#onetrust-pc-sdk nav,#onetrust-pc-sdk table,#onetrust-pc-sdk thead,#onetrust-pc-sdk tr,#onetrust-pc-sdk td,#onetrust-pc-sdk tbody,#onetrust-pc-sdk .ot-main-content,#onetrust-pc-sdk .ot-toggle,#onetrust-pc-sdk #ot-content,#onetrust-pc-sdk #ot-pc-content,#onetrust-pc-sdk .checkbox,#ot-sdk-cookie-policy div,#ot-sdk-cookie-policy span,#ot-sdk-cookie-policy h1,#ot-sdk-cookie-policy h2,#ot-sdk-cookie-policy h3,#ot-sdk-cookie-policy h4,#ot-sdk-cookie-policy h5,#ot-sdk-cookie-policy h6,#ot-sdk-cookie-policy p,#ot-sdk-cookie-policy img,#ot-sdk-cookie-policy svg,#ot-sdk-cookie-policy button,#ot-sdk-cookie-policy section,#ot-sdk-cookie-policy a,#ot-sdk-cookie-policy label,#ot-sdk-cookie-policy input,#ot-sdk-cookie-policy ul,#ot-sdk-cookie-policy li,#ot-sdk-cookie-policy nav,#ot-sdk-cookie-policy table,#ot-sdk-cookie-policy thead,#ot-sdk-cookie-policy tr,#ot-sdk-cookie-policy td,#ot-sdk-cookie-policy tbody,#ot-sdk-cookie-policy .ot-main-content,#ot-sdk-cookie-policy .ot-toggle,#ot-sdk-cookie-policy #ot-content,#ot-sdk-cookie-policy #ot-pc-content,#ot-sdk-cookie-policy .checkbox,#ot-sync-ntfy div,#ot-sync-ntfy span,#ot-sync-ntfy h1,#ot-sync-ntfy h2,#ot-sync-ntfy h3,#ot-sync-ntfy h4,#ot-sync-ntfy h5,#ot-sync-ntfy h6,#ot-sync-ntfy p,#ot-sync-ntfy img,#ot-sync-ntfy svg,#ot-sync-ntfy button,#ot-sync-ntfy section,#ot-sync-ntfy a,#ot-sync-ntfy label,#ot-sync-ntfy input,#ot-sync-ntfy ul,#ot-sync-ntfy li,#ot-sync-ntfy nav,#ot-sync-ntfy table,#ot-sync-ntfy thead,#ot-sync-ntfy tr,#ot-sync-ntfy td,#ot-sync-ntfy tbody,#ot-sync-ntfy .ot-main-content,#ot-sync-ntfy .ot-toggle,#ot-sync-ntfy #ot-content,#ot-sync-ntfy #ot-pc-content,#ot-sync-ntfy .checkbox{font-family:inherit;font-weight:normal;-webkit-font-smoothing:auto;letter-spacing:normal;line-height:normal;padding:0;margin:0;height:auto;min-height:0;max-height:none;width:auto;min-width:0;max-width:none;border-radius:0;border:none;clear:none;float:none;position:static;bottom:auto;left:auto;right:auto;top:auto;text-align:left;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;white-space:normal;background:none;overflow:visible;vertical-align:baseline;visibility:visible;z-index:auto;box-shadow:none}#onetrust-banner-sdk label:before,#onetrust-banner-sdk label:after,#onetrust-banner-sdk .checkbox:after,#onetrust-banner-sdk .checkbox:before,#onetrust-pc-sdk label:before,#onetrust-pc-sdk label:after,#onetrust-pc-sdk .checkbox:after,#onetrust-pc-sdk .checkbox:before,#ot-sdk-cookie-policy label:before,#ot-sdk-cookie-policy label:after,#ot-sdk-cookie-policy .checkbox:after,#ot-sdk-cookie-policy .checkbox:before,#ot-sync-ntfy label:before,#ot-sync-ntfy label:after,#ot-sync-ntfy .checkbox:after,#ot-sync-ntfy .checkbox:before{content:"";content:none}#onetrust-banner-sdk .ot-sdk-container,#onetrust-pc-sdk .ot-sdk-container,#ot-sdk-cookie-policy .ot-sdk-container{position:relative;width:100%;max-width:100%;margin:0 auto;padding:0 20px;box-sizing:border-box}#onetrust-banner-sdk .ot-sdk-column,#onetrust-banner-sdk .ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-column,#onetrust-pc-sdk .ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-column,#ot-sdk-cookie-policy .ot-sdk-columns{width:100%;float:left;box-sizing:border-box;padding:0;display:initial}@media(min-width: 400px){#onetrust-banner-sdk .ot-sdk-container,#onetrust-pc-sdk .ot-sdk-container,#ot-sdk-cookie-policy .ot-sdk-container{width:90%;padding:0}}@media(min-width: 550px){#onetrust-banner-sdk .ot-sdk-container,#onetrust-pc-sdk .ot-sdk-container,#ot-sdk-cookie-policy .ot-sdk-container{width:100%}#onetrust-banner-sdk .ot-sdk-column,#onetrust-banner-sdk .ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-column,#onetrust-pc-sdk .ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-column,#ot-sdk-cookie-policy .ot-sdk-columns{margin-left:4%}#onetrust-banner-sdk .ot-sdk-column:first-child,#onetrust-banner-sdk .ot-sdk-columns:first-child,#onetrust-pc-sdk .ot-sdk-column:first-child,#onetrust-pc-sdk .ot-sdk-columns:first-child,#ot-sdk-cookie-policy .ot-sdk-column:first-child,#ot-sdk-cookie-policy .ot-sdk-columns:first-child{margin-left:0}#onetrust-banner-sdk .ot-sdk-two.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-two.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-two.ot-sdk-columns{width:13.3333333333%}#onetrust-banner-sdk .ot-sdk-three.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-three.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-three.ot-sdk-columns{width:22%}#onetrust-banner-sdk .ot-sdk-four.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-four.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-four.ot-sdk-columns{width:30.6666666667%}#onetrust-banner-sdk .ot-sdk-eight.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-eight.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-eight.ot-sdk-columns{width:65.3333333333%}#onetrust-banner-sdk .ot-sdk-nine.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-nine.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-nine.ot-sdk-columns{width:74%}#onetrust-banner-sdk .ot-sdk-ten.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-ten.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-ten.ot-sdk-columns{width:82.6666666667%}#onetrust-banner-sdk .ot-sdk-eleven.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-eleven.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-eleven.ot-sdk-columns{width:91.3333333333%}#onetrust-banner-sdk .ot-sdk-twelve.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-twelve.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-twelve.ot-sdk-columns{width:100%;margin-left:0}}#onetrust-banner-sdk h1,#onetrust-banner-sdk h2,#onetrust-banner-sdk h3,#onetrust-banner-sdk h4,#onetrust-banner-sdk h5,#onetrust-banner-sdk h6,#onetrust-pc-sdk h1,#onetrust-pc-sdk h2,#onetrust-pc-sdk h3,#onetrust-pc-sdk h4,#onetrust-pc-sdk h5,#onetrust-pc-sdk h6,#ot-sdk-cookie-policy h1,#ot-sdk-cookie-policy h2,#ot-sdk-cookie-policy h3,#ot-sdk-cookie-policy h4,#ot-sdk-cookie-policy h5,#ot-sdk-cookie-policy h6{margin-top:0;font-weight:600;font-family:inherit}#onetrust-banner-sdk h1,#onetrust-pc-sdk h1,#ot-sdk-cookie-policy h1{font-size:1.5rem;line-height:1.2}#onetrust-banner-sdk h2,#onetrust-pc-sdk h2,#ot-sdk-cookie-policy h2{font-size:1.5rem;line-height:1.25}#onetrust-banner-sdk h3,#onetrust-pc-sdk h3,#ot-sdk-cookie-policy h3{font-size:1.5rem;line-height:1.3}#onetrust-banner-sdk h4,#onetrust-pc-sdk h4,#ot-sdk-cookie-policy h4{font-size:1.5rem;line-height:1.35}#onetrust-banner-sdk h5,#onetrust-pc-sdk h5,#ot-sdk-cookie-policy h5{font-size:1.5rem;line-height:1.5}#onetrust-banner-sdk h6,#onetrust-pc-sdk h6,#ot-sdk-cookie-policy h6{font-size:1.5rem;line-height:1.6}@media(min-width: 550px){#onetrust-banner-sdk h1,#onetrust-pc-sdk h1,#ot-sdk-cookie-policy h1{font-size:1.5rem}#onetrust-banner-sdk h2,#onetrust-pc-sdk h2,#ot-sdk-cookie-policy h2{font-size:1.5rem}#onetrust-banner-sdk h3,#onetrust-pc-sdk h3,#ot-sdk-cookie-policy h3{font-size:1.5rem}#onetrust-banner-sdk h4,#onetrust-pc-sdk h4,#ot-sdk-cookie-policy h4{font-size:1.5rem}#onetrust-banner-sdk h5,#onetrust-pc-sdk h5,#ot-sdk-cookie-policy h5{font-size:1.5rem}#onetrust-banner-sdk h6,#onetrust-pc-sdk h6,#ot-sdk-cookie-policy h6{font-size:1.5rem}}#onetrust-banner-sdk p,#onetrust-pc-sdk p,#ot-sdk-cookie-policy p{margin:0 0 1em 0;font-family:inherit;line-height:normal}#onetrust-banner-sdk a,#onetrust-pc-sdk a,#ot-sdk-cookie-policy a{color:#565656;text-decoration:underline}#onetrust-banner-sdk a:hover,#onetrust-pc-sdk a:hover,#ot-sdk-cookie-policy a:hover{color:#565656;text-decoration:none}#onetrust-banner-sdk .ot-sdk-button,#onetrust-banner-sdk button,#onetrust-pc-sdk .ot-sdk-button,#onetrust-pc-sdk button,#ot-sdk-cookie-policy .ot-sdk-button,#ot-sdk-cookie-policy button{margin-bottom:1rem;font-family:inherit}#onetrust-banner-sdk .ot-sdk-button,#onetrust-banner-sdk button,#onetrust-pc-sdk .ot-sdk-button,#onetrust-pc-sdk button,#ot-sdk-cookie-policy .ot-sdk-button,#ot-sdk-cookie-policy button{display:inline-block;height:38px;padding:0 30px;color:#555;text-align:center;font-size:.9em;font-weight:400;line-height:38px;letter-spacing:.01em;text-decoration:none;white-space:nowrap;background-color:rgba(0,0,0,0);border-radius:2px;border:1px solid #bbb;cursor:pointer;box-sizing:border-box}#onetrust-banner-sdk .ot-sdk-button:hover,#onetrust-banner-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):hover,#onetrust-banner-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):focus,#onetrust-pc-sdk .ot-sdk-button:hover,#onetrust-pc-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):hover,#onetrust-pc-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):focus,#ot-sdk-cookie-policy .ot-sdk-button:hover,#ot-sdk-cookie-policy :not(.ot-leg-btn-container)>button:not(.ot-link-btn):hover,#ot-sdk-cookie-policy :not(.ot-leg-btn-container)>button:not(.ot-link-btn):focus{color:#333;border-color:#888;opacity:.7}#onetrust-banner-sdk .ot-sdk-button:focus,#onetrust-banner-sdk :not(.ot-leg-btn-container)>button:focus,#onetrust-pc-sdk .ot-sdk-button:focus,#onetrust-pc-sdk :not(.ot-leg-btn-container)>button:focus,#ot-sdk-cookie-policy .ot-sdk-button:focus,#ot-sdk-cookie-policy :not(.ot-leg-btn-container)>button:focus{outline:2px solid #000}#onetrust-banner-sdk .ot-sdk-button.ot-sdk-button-primary,#onetrust-banner-sdk button.ot-sdk-button-primary,#onetrust-banner-sdk input[type=submit].ot-sdk-button-primary,#onetrust-banner-sdk input[type=reset].ot-sdk-button-primary,#onetrust-banner-sdk input[type=button].ot-sdk-button-primary,#onetrust-pc-sdk .ot-sdk-button.ot-sdk-button-primary,#onetrust-pc-sdk button.ot-sdk-button-primary,#onetrust-pc-sdk input[type=submit].ot-sdk-button-primary,#onetrust-pc-sdk input[type=reset].ot-sdk-button-primary,#onetrust-pc-sdk input[type=button].ot-sdk-button-primary,#ot-sdk-cookie-policy .ot-sdk-button.ot-sdk-button-primary,#ot-sdk-cookie-policy button.ot-sdk-button-primary,#ot-sdk-cookie-policy input[type=submit].ot-sdk-button-primary,#ot-sdk-cookie-policy input[type=reset].ot-sdk-button-primary,#ot-sdk-cookie-policy input[type=button].ot-sdk-button-primary{color:#fff;background-color:#33c3f0;border-color:#33c3f0}#onetrust-banner-sdk .ot-sdk-button.ot-sdk-button-primary:hover,#onetrust-banner-sdk button.ot-sdk-button-primary:hover,#onetrust-banner-sdk input[type=submit].ot-sdk-button-primary:hover,#onetrust-banner-sdk input[type=reset].ot-sdk-button-primary:hover,#onetrust-banner-sdk input[type=button].ot-sdk-button-primary:hover,#onetrust-banner-sdk .ot-sdk-button.ot-sdk-button-primary:focus,#onetrust-banner-sdk button.ot-sdk-button-primary:focus,#onetrust-banner-sdk input[type=submit].ot-sdk-button-primary:focus,#onetrust-banner-sdk input[type=reset].ot-sdk-button-primary:focus,#onetrust-banner-sdk input[type=button].ot-sdk-button-primary:focus,#onetrust-pc-sdk .ot-sdk-button.ot-sdk-button-primary:hover,#onetrust-pc-sdk button.ot-sdk-button-primary:hover,#onetrust-pc-sdk input[type=submit].ot-sdk-button-primary:hover,#onetrust-pc-sdk input[type=reset].ot-sdk-button-primary:hover,#onetrust-pc-sdk input[type=button].ot-sdk-button-primary:hover,#onetrust-pc-sdk .ot-sdk-button.ot-sdk-button-primary:focus,#onetrust-pc-sdk button.ot-sdk-button-primary:focus,#onetrust-pc-sdk input[type=submit].ot-sdk-button-primary:focus,#onetrust-pc-sdk input[type=reset].ot-sdk-button-primary:focus,#onetrust-pc-sdk input[type=button].ot-sdk-button-primary:focus,#ot-sdk-cookie-policy .ot-sdk-button.ot-sdk-button-primary:hover,#ot-sdk-cookie-policy button.ot-sdk-button-primary:hover,#ot-sdk-cookie-policy input[type=submit].ot-sdk-button-primary:hover,#ot-sdk-cookie-policy input[type=reset].ot-sdk-button-primary:hover,#ot-sdk-cookie-policy input[type=button].ot-sdk-button-primary:hover,#ot-sdk-cookie-policy .ot-sdk-button.ot-sdk-button-primary:focus,#ot-sdk-cookie-policy button.ot-sdk-button-primary:focus,#ot-sdk-cookie-policy input[type=submit].ot-sdk-button-primary:focus,#ot-sdk-cookie-policy input[type=reset].ot-sdk-button-primary:focus,#ot-sdk-cookie-policy input[type=button].ot-sdk-button-primary:focus{color:#fff;background-color:#1eaedb;border-color:#1eaedb}#onetrust-banner-sdk input[type=text],#onetrust-pc-sdk input[type=text],#ot-sdk-cookie-policy input[type=text]{height:38px;padding:6px 10px;background-color:#fff;border:1px solid #d1d1d1;border-radius:4px;box-shadow:none;box-sizing:border-box}#onetrust-banner-sdk input[type=text],#onetrust-pc-sdk input[type=text],#ot-sdk-cookie-policy input[type=text]{-webkit-appearance:none;-moz-appearance:none;appearance:none}#onetrust-banner-sdk input[type=text]:focus,#onetrust-pc-sdk input[type=text]:focus,#ot-sdk-cookie-policy input[type=text]:focus{border:1px solid #000;outline:0}#onetrust-banner-sdk label,#onetrust-pc-sdk label,#ot-sdk-cookie-policy label{display:block;margin-bottom:.5rem;font-weight:600}#onetrust-banner-sdk input[type=checkbox],#onetrust-pc-sdk input[type=checkbox],#ot-sdk-cookie-policy input[type=checkbox]{display:inline}#onetrust-banner-sdk ul,#onetrust-pc-sdk ul,#ot-sdk-cookie-policy ul{list-style:circle inside}#onetrust-banner-sdk ul,#onetrust-pc-sdk ul,#ot-sdk-cookie-policy ul{padding-left:0;margin-top:0}#onetrust-banner-sdk ul ul,#onetrust-pc-sdk ul ul,#ot-sdk-cookie-policy ul ul{margin:1.5rem 0 1.5rem 3rem;font-size:90%}#onetrust-banner-sdk li,#onetrust-pc-sdk li,#ot-sdk-cookie-policy li{margin-bottom:1rem}#onetrust-banner-sdk th,#onetrust-banner-sdk td,#onetrust-pc-sdk th,#onetrust-pc-sdk td,#ot-sdk-cookie-policy th,#ot-sdk-cookie-policy td{padding:12px 15px;text-align:left;border-bottom:1px solid #e1e1e1}#onetrust-banner-sdk button,#onetrust-pc-sdk button,#ot-sdk-cookie-policy button{margin-bottom:1rem;font-family:inherit}#onetrust-banner-sdk .ot-sdk-container:after,#onetrust-banner-sdk .ot-sdk-row:after,#onetrust-pc-sdk .ot-sdk-container:after,#onetrust-pc-sdk .ot-sdk-row:after,#ot-sdk-cookie-policy .ot-sdk-container:after,#ot-sdk-cookie-policy .ot-sdk-row:after{content:"";display:table;clear:both}#onetrust-banner-sdk .ot-sdk-row,#onetrust-pc-sdk .ot-sdk-row,#ot-sdk-cookie-policy .ot-sdk-row{margin:0;max-width:none;display:block}#onetrust-banner-sdk{box-shadow:0 0 18px rgba(0,0,0,.2)}#onetrust-banner-sdk.otFlat{position:fixed;z-index:2147483645;bottom:0;right:0;left:0;background-color:#fff;max-height:90%;overflow-x:hidden;overflow-y:auto}#onetrust-banner-sdk.otFlat.top{top:0px;bottom:auto}#onetrust-banner-sdk.otRelFont{font-size:1rem}#onetrust-banner-sdk>.ot-sdk-container{overflow:hidden}#onetrust-banner-sdk::-webkit-scrollbar{width:11px}#onetrust-banner-sdk::-webkit-scrollbar-thumb{border-radius:10px;background:#c1c1c1}#onetrust-banner-sdk{scrollbar-arrow-color:#c1c1c1;scrollbar-darkshadow-color:#c1c1c1;scrollbar-face-color:#c1c1c1;scrollbar-shadow-color:#c1c1c1}#onetrust-banner-sdk #onetrust-policy{margin:1.25em 0 .625em 2em;overflow:hidden}#onetrust-banner-sdk #onetrust-policy .ot-gv-list-handler{float:left;font-size:.82em;padding:0;margin-bottom:0;border:0;line-height:normal;height:auto;width:auto}#onetrust-banner-sdk #onetrust-policy-title{font-size:1.2em;line-height:1.3;margin-bottom:10px}#onetrust-banner-sdk #onetrust-policy-text{clear:both;text-align:left;font-size:.88em;line-height:1.4}#onetrust-banner-sdk #onetrust-policy-text *{font-size:inherit;line-height:inherit}#onetrust-banner-sdk #onetrust-policy-text a{font-weight:bold;margin-left:5px}#onetrust-banner-sdk #onetrust-policy-title,#onetrust-banner-sdk #onetrust-policy-text{color:dimgray;float:left}#onetrust-banner-sdk #onetrust-button-group-parent{min-height:1px;text-align:center}#onetrust-banner-sdk #onetrust-button-group{display:inline-block}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{background-color:#68b631;color:#fff;border-color:#68b631;margin-right:1em;min-width:125px;height:auto;white-space:normal;word-break:break-word;word-wrap:break-word;padding:12px 10px;line-height:1.2;font-size:.813em;font-weight:600}#onetrust-banner-sdk #onetrust-pc-btn-handler.cookie-setting-link{background-color:#fff;border:none;color:#68b631;text-decoration:underline;padding-left:0;padding-right:0}#onetrust-banner-sdk .onetrust-close-btn-ui{width:44px;height:44px;background-size:12px;border:none;position:relative;margin:auto;padding:0}#onetrust-banner-sdk .banner_logo{display:none}#onetrust-banner-sdk.ot-bnr-w-logo .ot-bnr-logo{position:absolute;top:50%;transform:translateY(-50%);left:0px}#onetrust-banner-sdk.ot-bnr-w-logo #onetrust-policy{margin-left:65px}#onetrust-banner-sdk .ot-b-addl-desc{clear:both;float:left;display:block}#onetrust-banner-sdk #banner-options{float:left;display:table;margin-right:0;margin-left:1em;width:calc(100% - 1em)}#onetrust-banner-sdk .banner-option-input{cursor:pointer;width:auto;height:auto;border:none;padding:0;padding-right:3px;margin:0 0 10px;font-size:.82em;line-height:1.4}#onetrust-banner-sdk .banner-option-input *{pointer-events:none;font-size:inherit;line-height:inherit}#onetrust-banner-sdk .banner-option-input[aria-expanded=true]~.banner-option-details{display:block;height:auto}#onetrust-banner-sdk .banner-option-input[aria-expanded=true] .ot-arrow-container{transform:rotate(90deg)}#onetrust-banner-sdk .banner-option{margin-bottom:12px;margin-left:0;border:none;float:left;padding:0}#onetrust-banner-sdk .banner-option:first-child{padding-left:2px}#onetrust-banner-sdk .banner-option:not(:first-child){padding:0;border:none}#onetrust-banner-sdk .banner-option-header{cursor:pointer;display:inline-block}#onetrust-banner-sdk .banner-option-header :first-child{color:dimgray;font-weight:bold;float:left}#onetrust-banner-sdk .banner-option-header .ot-arrow-container{display:inline-block;border-top:6px solid rgba(0,0,0,0);border-bottom:6px solid rgba(0,0,0,0);border-left:6px solid dimgray;margin-left:10px;vertical-align:middle}#onetrust-banner-sdk .banner-option-details{display:none;font-size:.83em;line-height:1.5;padding:10px 0px 5px 10px;margin-right:10px;height:0px}#onetrust-banner-sdk .banner-option-details *{font-size:inherit;line-height:inherit;color:dimgray}#onetrust-banner-sdk .ot-arrow-container,#onetrust-banner-sdk .banner-option-details{transition:all 300ms ease-in 0s;-webkit-transition:all 300ms ease-in 0s;-moz-transition:all 300ms ease-in 0s;-o-transition:all 300ms ease-in 0s}#onetrust-banner-sdk .ot-dpd-container{float:left}#onetrust-banner-sdk .ot-dpd-title{margin-bottom:10px}#onetrust-banner-sdk .ot-dpd-title,#onetrust-banner-sdk .ot-dpd-desc{font-size:.88em;line-height:1.4;color:dimgray}#onetrust-banner-sdk .ot-dpd-title *,#onetrust-banner-sdk .ot-dpd-desc *{font-size:inherit;line-height:inherit}#onetrust-banner-sdk.ot-iab-2 #onetrust-policy-text *{margin-bottom:0}#onetrust-banner-sdk.ot-iab-2 .onetrust-vendors-list-handler{display:block;margin-left:0;margin-top:5px;clear:both;margin-bottom:0;padding:0;border:0;height:auto;width:auto}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group button{display:block}#onetrust-banner-sdk.ot-close-btn-link{padding-top:25px}#onetrust-banner-sdk.ot-close-btn-link #onetrust-close-btn-container{top:15px;transform:none;right:15px}#onetrust-banner-sdk.ot-close-btn-link #onetrust-close-btn-container button{padding:0;white-space:pre-wrap;border:none;height:auto;line-height:1.5;text-decoration:underline;font-size:.69em}#onetrust-banner-sdk #onetrust-policy-text,#onetrust-banner-sdk .ot-dpd-desc,#onetrust-banner-sdk .ot-b-addl-desc{font-size:.813em;line-height:1.5}#onetrust-banner-sdk .ot-dpd-desc{margin-bottom:10px}#onetrust-banner-sdk .ot-dpd-desc>.ot-b-addl-desc{margin-top:10px;margin-bottom:10px;font-size:1em}@media only screen and (max-width: 425px){#onetrust-banner-sdk #onetrust-close-btn-container{position:absolute;top:6px;right:2px}#onetrust-banner-sdk #onetrust-policy{margin-left:0;margin-top:3em}#onetrust-banner-sdk #onetrust-button-group{display:block}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{width:100%}#onetrust-banner-sdk .onetrust-close-btn-ui{top:auto;transform:none}#onetrust-banner-sdk #onetrust-policy-title{display:inline;float:none}#onetrust-banner-sdk #banner-options{margin:0;padding:0;width:100%}}@media only screen and (min-width: 426px)and (max-width: 896px){#onetrust-banner-sdk #onetrust-close-btn-container{position:absolute;top:0;right:0}#onetrust-banner-sdk #onetrust-policy{margin-left:1em;margin-right:1em}#onetrust-banner-sdk .onetrust-close-btn-ui{top:10px;right:10px}#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{width:95%}#onetrust-banner-sdk.ot-iab-2 #onetrust-group-container{width:100%}#onetrust-banner-sdk.ot-bnr-w-logo #onetrust-button-group-parent{padding-left:50px}#onetrust-banner-sdk #onetrust-button-group-parent{width:100%;position:relative;margin-left:0}#onetrust-banner-sdk #onetrust-button-group button{display:inline-block}#onetrust-banner-sdk #onetrust-button-group{margin-right:0;text-align:center}#onetrust-banner-sdk .has-reject-all-button #onetrust-pc-btn-handler{float:left}#onetrust-banner-sdk .has-reject-all-button #onetrust-reject-all-handler,#onetrust-banner-sdk .has-reject-all-button #onetrust-accept-btn-handler{float:right}#onetrust-banner-sdk .has-reject-all-button #onetrust-button-group{width:calc(100% - 2em);margin-right:0}#onetrust-banner-sdk .has-reject-all-button #onetrust-pc-btn-handler.cookie-setting-link{padding-left:0px;text-align:left}#onetrust-banner-sdk.ot-buttons-fw .ot-sdk-three button{width:100%;text-align:center}#onetrust-banner-sdk.ot-buttons-fw #onetrust-button-group-parent button{float:none}#onetrust-banner-sdk.ot-buttons-fw #onetrust-pc-btn-handler.cookie-setting-link{text-align:center}}@media only screen and (min-width: 550px){#onetrust-banner-sdk .banner-option:not(:first-child){border-left:1px solid #d8d8d8;padding-left:25px}}@media only screen and (min-width: 425px)and (max-width: 550px){#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group,#onetrust-banner-sdk.ot-iab-2 #onetrust-policy,#onetrust-banner-sdk.ot-iab-2 .banner-option{width:100%}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-accept-btn-handler,#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-reject-all-handler,#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-pc-btn-handler{width:100%}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-accept-btn-handler,#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-reject-all-handler{float:left}}@media only screen and (min-width: 769px){#onetrust-banner-sdk #onetrust-button-group{margin-right:30%}#onetrust-banner-sdk #banner-options{margin-left:2em;margin-right:5em;margin-bottom:1.25em;width:calc(100% - 7em)}}@media only screen and (min-width: 897px)and (max-width: 1023px){#onetrust-banner-sdk.vertical-align-content #onetrust-button-group-parent{position:absolute;top:50%;left:75%;transform:translateY(-50%)}#onetrust-banner-sdk #onetrust-close-btn-container{top:50%;margin:auto;transform:translate(-50%, -50%);position:absolute;padding:0;right:0}#onetrust-banner-sdk #onetrust-close-btn-container button{position:relative;margin:0;right:-22px;top:2px}}@media only screen and (min-width: 1024px){#onetrust-banner-sdk #onetrust-close-btn-container{top:50%;margin:auto;transform:translate(-50%, -50%);position:absolute;right:0}#onetrust-banner-sdk #onetrust-close-btn-container button{right:-12px}#onetrust-banner-sdk #onetrust-policy{margin-left:2em}#onetrust-banner-sdk.vertical-align-content #onetrust-button-group-parent{position:absolute;top:50%;left:60%;transform:translateY(-50%)}#onetrust-banner-sdk .ot-optout-signal{width:50%}#onetrust-banner-sdk.ot-iab-2 #onetrust-policy-title{width:50%}#onetrust-banner-sdk.ot-iab-2 #onetrust-policy-text,#onetrust-banner-sdk.ot-iab-2 :not(.ot-dpd-desc)>.ot-b-addl-desc{margin-bottom:1em;width:50%;border-right:1px solid #d8d8d8;padding-right:1rem}#onetrust-banner-sdk.ot-iab-2 #onetrust-policy-text{margin-bottom:0;padding-bottom:1em}#onetrust-banner-sdk.ot-iab-2 :not(.ot-dpd-desc)>.ot-b-addl-desc{margin-bottom:0;padding-bottom:1em}#onetrust-banner-sdk.ot-iab-2 .ot-dpd-container{width:45%;padding-left:1rem;display:inline-block;float:none}#onetrust-banner-sdk.ot-iab-2 .ot-dpd-title{line-height:1.7}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group-parent{left:auto;right:4%;margin-left:0}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group button{display:block}#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-button-group-parent{margin:auto;width:30%}#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{width:60%}#onetrust-banner-sdk #onetrust-button-group{margin-right:auto}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{margin-top:1em}}@media only screen and (min-width: 890px){#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group-parent{padding-left:3%;padding-right:4%;margin-left:0}#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group{margin-right:0;margin-top:1.25em;width:100%}#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group button{width:100%;margin-bottom:5px;margin-top:5px}#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group button:last-of-type{margin-bottom:20px}}@media only screen and (min-width: 1280px){#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{width:55%}#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-button-group-parent{width:44%;padding-left:2%;padding-right:2%}#onetrust-banner-sdk:not(.ot-iab-2).vertical-align-content #onetrust-button-group-parent{position:absolute;left:55%}}
        #onetrust-consent-sdk #onetrust-banner-sdk {background-color: #FFF;}
            #onetrust-consent-sdk #onetrust-policy-title,
                    #onetrust-consent-sdk #onetrust-policy-text,
                    #onetrust-consent-sdk .ot-b-addl-desc,
                    #onetrust-consent-sdk .ot-dpd-desc,
                    #onetrust-consent-sdk .ot-dpd-title,
                    #onetrust-consent-sdk #onetrust-policy-text *:not(.onetrust-vendors-list-handler),
                    #onetrust-consent-sdk .ot-dpd-desc *:not(.onetrust-vendors-list-handler),
                    #onetrust-consent-sdk #onetrust-banner-sdk #banner-options *,
                    #onetrust-banner-sdk .ot-cat-header,
                    #onetrust-banner-sdk .ot-optout-signal
                    {
                        color: #2E2E2E;
                    }
            #onetrust-consent-sdk #onetrust-banner-sdk .banner-option-details {
                    background-color: #E9E9E9;}
             #onetrust-consent-sdk #onetrust-banner-sdk a[href],
                    #onetrust-consent-sdk #onetrust-banner-sdk a[href] font,
                    #onetrust-consent-sdk #onetrust-banner-sdk .ot-link-btn
                        {
                            color: #007398;
                        }#onetrust-consent-sdk #onetrust-accept-btn-handler,
                         #onetrust-banner-sdk #onetrust-reject-all-handler {
                            background-color: #007398;border-color: #007398;
                color: #FFF;
            }
            #onetrust-consent-sdk #onetrust-banner-sdk *:focus,
            #onetrust-consent-sdk #onetrust-banner-sdk:focus {
               outline-color: #000000;
               outline-width: 1px;
            }
            #onetrust-consent-sdk #onetrust-pc-btn-handler,
            #onetrust-consent-sdk #onetrust-pc-btn-handler.cookie-setting-link {
                color: #6CC04A; border-color: #6CC04A;
                background-color:
                #FFF;
            }/*! Extra code to blur out background */
.onetrust-pc-dark-filter{
background:rgba(0,0,0,.5);
z-index:2147483646;
width:100%;
height:100%;
overflow:hidden;
position:fixed;
top:0;
bottom:0;
left:0;
backdrop-filter: initial
}

/*! v6.12.0 2021-01-19 */
div#onetrust-consent-sdk #onetrust-banner-sdk{border-top:2px solid #eb6500!important;outline:1px solid transparent;box-shadow:none;padding:24px}div#onetrust-consent-sdk button{border-radius:0!important;box-shadow:none!important;box-sizing:border-box!important;font-size:20px!important;font-weight:400!important;letter-spacing:0!important;max-width:none!important;white-space:nowrap!important}div#onetrust-consent-sdk button:not(.ot-link-btn){background-color:#007398!important;border:2px solid #007398!important;color:#fff!important;height:48px!important;padding:0 1em!important;width:auto!important}div#onetrust-consent-sdk button:hover{background-color:#fff!important;border-color:#eb6500!important;color:#2e2e2e!important}div#onetrust-consent-sdk button.ot-link-btn{color:#007398!important;font-size:16px!important;text-decoration:underline}div#onetrust-consent-sdk button.ot-link-btn:hover{color: #2e2e2e!important;text-decoration-color:#eb6500!important}div#onetrust-consent-sdk a,div#onetrust-pc-sdk a{color:#007398!important;text-decoration:underline!important}div#onetrust-consent-sdk a,div#onetrust-consent-sdk button,div#onetrust-consent-sdk p:hover{opacity:1!important}div#onetrust-consent-sdk a:focus,div#onetrust-consent-sdk button:focus,div#onetrust-consent-sdk input:focus{outline:2px solid #eb6500!important;outline-offset:1px!important}div#onetrust-banner-sdk .ot-sdk-container{padding:0;width:auto}div#onetrust-banner-sdk .ot-sdk-row{align-items:flex-start;box-sizing:border-box;display:flex;flex-direction:column;justify-content:space-between;margin:auto;max-width:1152px}div#onetrust-banner-sdk .ot-sdk-row:after{display:none}div#onetrust-banner-sdk #onetrust-group-container,div#onetrust-banner-sdk.ot-bnr-flift:not(.ot-iab-2) #onetrust-group-container,div#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{flex-grow:1;width:auto}div#onetrust-banner-sdk #onetrust-policy,div#onetrust-banner-sdk.ot-bnr-flift #onetrust-policy{margin:0;overflow:visible}div#onetrust-banner-sdk.ot-bnr-flift #onetrust-policy-text,div#onetrust-consent-sdk #onetrust-policy-text{font-size:16px;line-height:24px;max-width:44em;margin:0}div#onetrust-consent-sdk #onetrust-policy-text a[href]{font-weight:400;margin-left:8px}div#onetrust-banner-sdk #onetrust-button-group-parent{flex:0 0 auto;margin:32px 0 0;width:100%}div#onetrust-banner-sdk #onetrust-button-group{display:flex;flex-direction:row;flex-wrap:wrap;justify-content:flex-end;margin:-8px}div#onetrust-banner-sdk .banner-actions-container{display:flex;flex:1 0 auto}div#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group button:last-of-type,div#onetrust-consent-sdk #onetrust-accept-btn-handler,div#onetrust-consent-sdk #onetrust-pc-btn-handler{flex:1 0 auto;margin:8px;width:auto}div#onetrust-consent-sdk #onetrust-pc-btn-handler{background-color:#fff!important;color:#2e2e2e!important}div#onetrust-banner-sdk #onetrust-close-btn-container{display:none}@media only screen and (min-width:556px){div#onetrust-consent-sdk #onetrust-banner-sdk{padding:40px}div#onetrust-banner-sdk #onetrust-policy{margin:0 40px 0 0}div#onetrust-banner-sdk .ot-sdk-row{align-items:center;flex-direction:row}div#onetrust-banner-sdk #onetrust-button-group-parent,div#onetrust-banner-sdk.ot-bnr-flift:not(.ot-iab-2) #onetrust-button-group-parent,div#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-button-group-parent{margin:0;padding:0;width:auto}div#onetrust-banner-sdk #onetrust-button-group,div#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group{align-items:stretch;flex-direction:column-reverse;margin:0}div#onetrust-consent-sdk #onetrust-accept-btn-handler,div#onetrust-consent-sdk #onetrust-pc-btn-handler{flex:1 0 auto}}@media only screen and (min-width:768px){div#onetrust-banner-sdk #onetrust-policy{margin:0 48px 0 0}div#onetrust-consent-sdk #onetrust-banner-sdk{padding:48px}}div#onetrust-consent-sdk #onetrust-pc-sdk h5{font-size:16px;line-height:24px}div#onetrust-consent-sdk #onetrust-pc-sdk p,div#onetrust-pc-sdk #ot-pc-desc,div#onetrust-pc-sdk .category-host-list-handler,div#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{font-size:16px;font-weight:400;line-height:24px}div#onetrust-consent-sdk a:hover,div#onetrust-pc-sdk a:hover{color:#2e2e2e!important;text-decoration-color:#eb6500!important}div#onetrust-pc-sdk{border-radius:0;bottom:0;height:auto;left:0;margin:auto;max-width:100%;overflow:hidden;right:0;top:0;width:512px;max-height:800px}div#onetrust-pc-sdk .ot-pc-header{display:none}div#onetrust-pc-sdk #ot-pc-content{overscroll-behavior:contain;padding:0 12px 0 24px;margin:16px 4px 0 0;top:0;right:16px;left:0;width:auto}div#onetrust-pc-sdk #ot-category-title,div#onetrust-pc-sdk #ot-pc-title{font-size:24px;font-weight:400;line-height:32px;margin:0 0 16px}div#onetrust-pc-sdk #ot-pc-desc{padding:0}div#onetrust-pc-sdk #ot-pc-desc a{display:inline}div#onetrust-pc-sdk #accept-recommended-btn-handler{display:none!important}div#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:2px solid #eb6500;outline-offset:-1px;transition:none}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item{border-width:0 0 2px}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item:first-of-type{border-width:2px 0}div#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{padding:8px 0;width:100%}div#onetrust-pc-sdk .ot-plus-minus{transform:translateY(2px)}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item>button{background:0 0!important;border:0!important;height:44px!important;max-width:none!important;width:calc(100% - 48px)!important}div#onetrust-consent-sdk #onetrust-pc-sdk h5{font-weight:700}div#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr{padding:0}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item .ot-acc-grpdesc{padding:0;width:100%}div#onetrust-pc-sdk .ot-acc-grpcntr .ot-subgrp-cntr{border:0;padding:0}div#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp{margin:0}div#onetrust-pc-sdk .ot-always-active-group .ot-cat-header{width:calc(100% - 160px)}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{width:calc(100% - 88px)}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active{color:#2e2e2e;font-size:12px;font-weight:400;line-height:1.5;padding-right:48px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active:before{border-radius:12px;position:absolute;right:0;top:0;content:'';background:#fff;border:2px solid #939393;box-sizing:border-box;height:20px;width:40px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active:after{border-radius:50%;position:absolute;right:5px;top:4px;content:'';background-color:#eb6500;height:12px;width:12px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active,div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl{right:2px}div#onetrust-pc-sdk .ot-switch{display:block;height:20px;width:40px}div#onetrust-pc-sdk .ot-tgl input+.ot-switch .ot-switch-nob,div#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob{background:#fff;border:2px solid #939393;box-sizing:border-box;height:20px;width:40px}div#onetrust-pc-sdk .ot-tgl input+.ot-switch .ot-switch-nob:before{background-color:#737373;height:8px;left:4px;top:4px;width:8px}div#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob:before{background-color:#eb6500;height:12px;left:0;top:2px;width:12px}div#onetrust-pc-sdk .ot-tgl input:focus+.ot-switch .ot-switch-nob{box-shadow:0 0;outline:2px solid #eb6500!important;outline-offset:1px;transition:none}div#onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-grpcntr.ot-acc-txt{background-color:transparent;padding-left:3px}div#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr,div#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr{overflow:visible;width:100%}div#onetrust-pc-sdk .ot-pc-footer{border-top:0 solid}div#onetrust-pc-sdk .ot-btn-container{padding-top:24px;text-align:center}div#onetrust-pc-sdk .ot-pc-footer button{margin:8px 0;background-color:#fff}div#onetrust-pc-sdk .ot-pc-footer-logo{background-color:#fff}div#onetrust-pc-sdk #ot-lst-title span{font-size:24px;font-weight:400;line-height:32px}div#onetrust-pc-sdk #ot-host-lst .ot-host-desc,div#onetrust-pc-sdk #ot-host-lst .ot-host-expand,div#onetrust-pc-sdk #ot-host-lst .ot-host-name,div#onetrust-pc-sdk #ot-host-lst .ot-host-name a,div#onetrust-pc-sdk .back-btn-handler,div#onetrust-pc-sdk .ot-host-opt li>div div{font-size:16px;font-weight:400;line-height:24px}div#onetrust-pc-sdk #ot-host-lst .ot-acc-txt{width:100%}div#onetrust-pc-sdk #ot-pc-lst{top:0}div#onetrust-pc-sdk .back-btn-handler{text-decoration:none!important}div#onetrust-pc-sdk #filter-btn-handler:hover svg{filter:invert(1)}div#onetrust-pc-sdk .back-btn-handler svg{width:16px;height:16px}div#onetrust-pc-sdk .ot-host-item>button{background:0 0!important;border:0!important;height:66px!important;max-width:none!important;width:calc(100% - 5px)!important;transform:translate(2px,2px)}div#onetrust-pc-sdk .ot-host-item{border-bottom:2px solid #b9b9b9;padding:0}div#onetrust-pc-sdk .ot-host-item .ot-acc-hdr{margin:0 0 -6px;padding:8px 0}div#onetrust-pc-sdk ul li:first-child{border-top:2px solid #b9b9b9}div#onetrust-pc-sdk .ot-host-item .ot-plus-minus{margin:0 8px 0 0}div#onetrust-pc-sdk .ot-search-cntr{width:calc(100% - 48px)}div#onetrust-pc-sdk .ot-host-opt .ot-host-info{background-color:transparent}div#onetrust-pc-sdk .ot-host-opt li>div div{padding:0}div#onetrust-pc-sdk #vendor-search-handler{border-radius:0;border-color:#939393;border-style:solid;border-width:2px 0 2px 2px;font-size:20px;height:48px;margin:0}div#onetrust-pc-sdk #ot-pc-hdr{margin-left:24px}div#onetrust-pc-sdk .ot-lst-subhdr{width:calc(100% - 24px)}div#onetrust-pc-sdk .ot-lst-subhdr svg{right:0;top:8px}div#onetrust-pc-sdk .ot-fltr-cntr{box-sizing:border-box;right:0;width:48px}div#onetrust-pc-sdk #filter-btn-handler{width:48px!important;padding:8px!important}div#onetrust-consent-sdk #onetrust-pc-sdk #clear-filters-handler,div#onetrust-pc-sdk button#filter-apply-handler,div#onetrust-pc-sdk button#filter-cancel-handler{height:2em!important;padding-left:14px!important;padding-right:14px!important}div#onetrust-pc-sdk #ot-fltr-cnt{box-shadow:0 0;border:1px solid #8e8e8e;border-radius:0}div#onetrust-pc-sdk .ot-fltr-scrlcnt{max-height:calc(100% - 80px)}div#onetrust-pc-sdk #ot-fltr-modal{max-height:400px}div#onetrust-pc-sdk .ot-fltr-opt{margin-bottom:16px}div#onetrust-pc-sdk #ot-lst-cnt{margin-left:24px;width:calc(100% - 48px)}div#onetrust-pc-sdk #ot-anchor{display:none!important}

/* 2023-12-04  Fix for button order in mobile view*/
@media (max-width: 550px) {
  #onetrust-accept-btn-handler {order: 1;  }
  #onetrust-reject-all-handler { order: 2;  }
  #onetrust-pc-btn-handler { order: 3;  }
}


/*! Extra code to blur our background */
.onetrust-pc-dark-filter{
backdrop-filter: blur(3px)
}
#onetrust-pc-sdk.otPcCenter{overflow:hidden;position:fixed;margin:0 auto;top:5%;right:0;left:0;width:40%;max-width:575px;min-width:575px;border-radius:2.5px;z-index:2147483647;background-color:#fff;-webkit-box-shadow:0px 2px 10px -3px #999;-moz-box-shadow:0px 2px 10px -3px #999;box-shadow:0px 2px 10px -3px #999}#onetrust-pc-sdk.otPcCenter[dir=rtl]{right:0;left:0}#onetrust-pc-sdk.otRelFont{font-size:1rem}#onetrust-pc-sdk .ot-optout-signal{margin-top:.625rem}#onetrust-pc-sdk #ot-addtl-venlst .ot-arw-cntr,#onetrust-pc-sdk #ot-addtl-venlst .ot-plus-minus,#onetrust-pc-sdk .ot-hide-tgl{visibility:hidden}#onetrust-pc-sdk #ot-addtl-venlst .ot-arw-cntr *,#onetrust-pc-sdk #ot-addtl-venlst .ot-plus-minus *,#onetrust-pc-sdk .ot-hide-tgl *{visibility:hidden}#onetrust-pc-sdk #ot-gn-venlst .ot-ven-item .ot-acc-hdr{min-height:40px}#onetrust-pc-sdk .ot-pc-header{height:39px;padding:10px 0 10px 30px;border-bottom:1px solid #e9e9e9}#onetrust-pc-sdk #ot-pc-title,#onetrust-pc-sdk #ot-category-title,#onetrust-pc-sdk .ot-cat-header,#onetrust-pc-sdk #ot-lst-title,#onetrust-pc-sdk .ot-ven-hdr .ot-ven-name,#onetrust-pc-sdk .ot-always-active{font-weight:bold;color:dimgray}#onetrust-pc-sdk .ot-always-active-group .ot-cat-header{width:55%;font-weight:700}#onetrust-pc-sdk .ot-cat-item p{clear:both;float:left;margin-top:10px;margin-bottom:5px;line-height:1.5;font-size:.812em;color:dimgray}#onetrust-pc-sdk .ot-close-icon{height:44px;width:44px;background-size:10px}#onetrust-pc-sdk #ot-pc-title{float:left;font-size:1em;line-height:1.5;margin-bottom:10px;margin-top:10px;width:100%}#onetrust-pc-sdk #accept-recommended-btn-handler{margin-right:10px;margin-bottom:25px;outline-offset:-1px}#onetrust-pc-sdk #ot-pc-desc{clear:both;width:100%;font-size:.812em;line-height:1.5;margin-bottom:25px}#onetrust-pc-sdk #ot-pc-desc a{margin-left:5px}#onetrust-pc-sdk #ot-pc-desc *{font-size:inherit;line-height:inherit}#onetrust-pc-sdk #ot-pc-desc ul li{padding:10px 0px}#onetrust-pc-sdk a{color:#656565;cursor:pointer}#onetrust-pc-sdk a:hover{color:#3860be}#onetrust-pc-sdk label{margin-bottom:0}#onetrust-pc-sdk #vdr-lst-dsc{font-size:.812em;line-height:1.5;padding:10px 15px 5px 15px}#onetrust-pc-sdk button{max-width:394px;padding:12px 30px;line-height:1;word-break:break-word;word-wrap:break-word;white-space:normal;font-weight:bold;height:auto}#onetrust-pc-sdk .ot-link-btn{padding:0;margin-bottom:0;border:0;font-weight:normal;line-height:normal;width:auto;height:auto}#onetrust-pc-sdk #ot-pc-content{position:absolute;overflow-y:scroll;padding-left:0px;padding-right:30px;top:60px;bottom:110px;margin:1px 3px 0 30px;width:calc(100% - 63px)}#onetrust-pc-sdk .ot-vs-list .ot-always-active,#onetrust-pc-sdk .ot-cat-grp .ot-always-active{float:right;clear:none;color:#3860be;margin:0;font-size:.813em;line-height:1.3}#onetrust-pc-sdk .ot-pc-scrollbar::-webkit-scrollbar-track{margin-right:20px}#onetrust-pc-sdk .ot-pc-scrollbar::-webkit-scrollbar{width:11px}#onetrust-pc-sdk .ot-pc-scrollbar::-webkit-scrollbar-thumb{border-radius:10px;background:#d8d8d8}#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:#000 1px solid}#onetrust-pc-sdk .ot-pc-scrollbar{scrollbar-arrow-color:#d8d8d8;scrollbar-darkshadow-color:#d8d8d8;scrollbar-face-color:#d8d8d8;scrollbar-shadow-color:#d8d8d8}#onetrust-pc-sdk .save-preference-btn-handler{margin-right:20px}#onetrust-pc-sdk .ot-pc-refuse-all-handler{margin-right:10px}#onetrust-pc-sdk #ot-pc-desc .privacy-notice-link{margin-left:0;margin-right:8px}#onetrust-pc-sdk #ot-pc-desc .ot-imprint-handler{margin-left:0;margin-right:8px}#onetrust-pc-sdk .ot-subgrp-cntr{display:inline-block;clear:both;width:100%;padding-top:15px}#onetrust-pc-sdk .ot-switch+.ot-subgrp-cntr{padding-top:10px}#onetrust-pc-sdk ul.ot-subgrps{margin:0;font-size:initial}#onetrust-pc-sdk ul.ot-subgrps li p,#onetrust-pc-sdk ul.ot-subgrps li h5{font-size:.813em;line-height:1.4;color:dimgray}#onetrust-pc-sdk ul.ot-subgrps .ot-switch{min-height:auto}#onetrust-pc-sdk ul.ot-subgrps .ot-switch-nob{top:0}#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr{display:inline-block;width:100%}#onetrust-pc-sdk ul.ot-subgrps .ot-acc-txt{margin:0}#onetrust-pc-sdk ul.ot-subgrps li{padding:0;border:none}#onetrust-pc-sdk ul.ot-subgrps li h5{position:relative;top:5px;font-weight:bold;margin-bottom:0;float:left}#onetrust-pc-sdk li.ot-subgrp{margin-left:20px;overflow:auto}#onetrust-pc-sdk li.ot-subgrp>h5{width:calc(100% - 100px)}#onetrust-pc-sdk .ot-cat-item p>ul,#onetrust-pc-sdk li.ot-subgrp p>ul{margin:0px;list-style:disc;margin-left:15px;font-size:inherit}#onetrust-pc-sdk .ot-cat-item p>ul li,#onetrust-pc-sdk li.ot-subgrp p>ul li{font-size:inherit;padding-top:10px;padding-left:0px;padding-right:0px;border:none}#onetrust-pc-sdk .ot-cat-item p>ul li:last-child,#onetrust-pc-sdk li.ot-subgrp p>ul li:last-child{padding-bottom:10px}#onetrust-pc-sdk .ot-pc-logo{height:40px;width:120px}#onetrust-pc-sdk .ot-pc-footer{position:absolute;bottom:0px;width:100%;max-height:160px;border-top:1px solid #d8d8d8}#onetrust-pc-sdk.ot-ftr-stacked .ot-pc-refuse-all-handler{margin-bottom:0px}#onetrust-pc-sdk.ot-ftr-stacked #ot-pc-content{bottom:160px}#onetrust-pc-sdk.ot-ftr-stacked .ot-pc-footer button{width:100%;max-width:none}#onetrust-pc-sdk.ot-ftr-stacked .ot-btn-container{margin:0 30px;width:calc(100% - 60px);padding-right:0}#onetrust-pc-sdk .ot-pc-footer-logo{height:30px;width:100%;text-align:right;background:#f4f4f4}#onetrust-pc-sdk .ot-pc-footer-logo a{display:inline-block;margin-top:5px;margin-right:10px}#onetrust-pc-sdk[dir=rtl] .ot-pc-footer-logo{direction:rtl}#onetrust-pc-sdk[dir=rtl] .ot-pc-footer-logo a{margin-right:25px}#onetrust-pc-sdk .ot-tgl{float:right;position:relative;z-index:1}#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob{background-color:#468254;border:1px solid #fff}#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob:before{-webkit-transform:translateX(20px);-ms-transform:translateX(20px);transform:translateX(20px);background-color:#fff;border-color:#fff}#onetrust-pc-sdk .ot-tgl input:focus+.ot-switch{outline:#000 solid 1px}#onetrust-pc-sdk .ot-switch{position:relative;display:inline-block;width:45px;height:25px}#onetrust-pc-sdk .ot-switch-nob{position:absolute;cursor:pointer;top:0;left:0;right:0;bottom:0;background-color:#767676;border:1px solid #ddd;transition:all .2s ease-in 0s;-moz-transition:all .2s ease-in 0s;-o-transition:all .2s ease-in 0s;-webkit-transition:all .2s ease-in 0s;border-radius:20px}#onetrust-pc-sdk .ot-switch-nob:before{position:absolute;content:"";height:18px;width:18px;bottom:3px;left:3px;background-color:#fff;-webkit-transition:.4s;transition:.4s;border-radius:20px}#onetrust-pc-sdk .ot-chkbox input:checked~label::before{background-color:#3860be}#onetrust-pc-sdk .ot-chkbox input+label::after{content:none;color:#fff}#onetrust-pc-sdk .ot-chkbox input:checked+label::after{content:""}#onetrust-pc-sdk .ot-chkbox input:focus+label::before{outline-style:solid;outline-width:2px;outline-style:auto}#onetrust-pc-sdk .ot-chkbox label{position:relative;display:inline-block;padding-left:30px;cursor:pointer;font-weight:500}#onetrust-pc-sdk .ot-chkbox label::before,#onetrust-pc-sdk .ot-chkbox label::after{position:absolute;content:"";display:inline-block;border-radius:3px}#onetrust-pc-sdk .ot-chkbox label::before{height:18px;width:18px;border:1px solid #3860be;left:0px;top:auto}#onetrust-pc-sdk .ot-chkbox label::after{height:5px;width:9px;border-left:3px solid;border-bottom:3px solid;transform:rotate(-45deg);-o-transform:rotate(-45deg);-ms-transform:rotate(-45deg);-webkit-transform:rotate(-45deg);left:4px;top:5px}#onetrust-pc-sdk .ot-label-txt{display:none}#onetrust-pc-sdk .ot-chkbox input,#onetrust-pc-sdk .ot-tgl input{position:absolute;opacity:0;width:0;height:0}#onetrust-pc-sdk .ot-arw-cntr{float:right;position:relative;pointer-events:none}#onetrust-pc-sdk .ot-arw-cntr .ot-arw{width:16px;height:16px;margin-left:5px;color:dimgray;display:inline-block;vertical-align:middle;-webkit-transition:all 150ms ease-in 0s;-moz-transition:all 150ms ease-in 0s;-o-transition:all 150ms ease-in 0s;transition:all 150ms ease-in 0s}#onetrust-pc-sdk input:checked~.ot-acc-hdr .ot-arw,#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-arw-cntr svg{transform:rotate(90deg);-o-transform:rotate(90deg);-ms-transform:rotate(90deg);-webkit-transform:rotate(90deg)}#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:#000 1px solid}#onetrust-pc-sdk .ot-tgl-cntr,#onetrust-pc-sdk .ot-arw-cntr{display:inline-block}#onetrust-pc-sdk .ot-tgl-cntr{width:45px;float:right;margin-top:2px}#onetrust-pc-sdk #ot-lst-cnt .ot-tgl-cntr{margin-top:10px}#onetrust-pc-sdk .ot-always-active-subgroup{width:auto;padding-left:0px !important;top:3px;position:relative}#onetrust-pc-sdk .ot-label-status{padding-left:5px;font-size:.75em;display:none}#onetrust-pc-sdk .ot-arw-cntr{margin-top:-1px}#onetrust-pc-sdk .ot-arw-cntr svg{-webkit-transition:all 300ms ease-in 0s;-moz-transition:all 300ms ease-in 0s;-o-transition:all 300ms ease-in 0s;transition:all 300ms ease-in 0s;height:10px;width:10px}#onetrust-pc-sdk input:checked~.ot-acc-hdr .ot-arw{transform:rotate(90deg);-o-transform:rotate(90deg);-ms-transform:rotate(90deg);-webkit-transform:rotate(90deg)}#onetrust-pc-sdk .ot-arw{width:10px;margin-left:15px;transition:all 300ms ease-in 0s;-webkit-transition:all 300ms ease-in 0s;-moz-transition:all 300ms ease-in 0s;-o-transition:all 300ms ease-in 0s}#onetrust-pc-sdk .ot-vlst-cntr{margin-bottom:0}#onetrust-pc-sdk .ot-hlst-cntr{margin-top:5px;display:inline-block;width:100%}#onetrust-pc-sdk .category-vendors-list-handler,#onetrust-pc-sdk .category-vendors-list-handler+a,#onetrust-pc-sdk .category-host-list-handler{clear:both;color:#3860be;margin-left:0;font-size:.813em;text-decoration:none;float:left;overflow:hidden}#onetrust-pc-sdk .category-vendors-list-handler:hover,#onetrust-pc-sdk .category-vendors-list-handler+a:hover,#onetrust-pc-sdk .category-host-list-handler:hover{text-decoration-line:underline}#onetrust-pc-sdk .category-vendors-list-handler+a{clear:none}#onetrust-pc-sdk .ot-vlst-cntr .ot-ext-lnk,#onetrust-pc-sdk .ot-ven-hdr .ot-ext-lnk{display:inline-block;height:13px;width:13px;background-repeat:no-repeat;margin-left:1px;margin-top:6px;cursor:pointer}#onetrust-pc-sdk .ot-ven-hdr .ot-ext-lnk{margin-bottom:-1px}#onetrust-pc-sdk .back-btn-handler{font-size:1em;text-decoration:none}#onetrust-pc-sdk .back-btn-handler:hover{opacity:.6}#onetrust-pc-sdk #ot-lst-title h3{display:inline-block;word-break:break-word;word-wrap:break-word;margin-bottom:0;color:#656565;font-size:1em;font-weight:bold;margin-left:15px}#onetrust-pc-sdk #ot-lst-title{margin:10px 0 10px 0px;font-size:1em;text-align:left}#onetrust-pc-sdk #ot-pc-hdr{margin:0 0 0 30px;height:auto;width:auto}#onetrust-pc-sdk #ot-pc-hdr input::placeholder{color:#d4d4d4;font-style:italic}#onetrust-pc-sdk #vendor-search-handler{height:31px;width:100%;border-radius:50px;font-size:.8em;padding-right:35px;padding-left:15px;float:left;margin-left:15px}#onetrust-pc-sdk .ot-ven-name{display:block;width:auto;padding-right:5px}#onetrust-pc-sdk #ot-lst-cnt{overflow-y:auto;margin-left:20px;margin-right:7px;width:calc(100% - 27px);max-height:calc(100% - 80px);height:100%;transform:translate3d(0, 0, 0)}#onetrust-pc-sdk #ot-pc-lst{width:100%;bottom:100px;position:absolute;top:60px}#onetrust-pc-sdk #ot-pc-lst:not(.ot-enbl-chr) .ot-tgl-cntr .ot-arw-cntr,#onetrust-pc-sdk #ot-pc-lst:not(.ot-enbl-chr) .ot-tgl-cntr .ot-arw-cntr *{visibility:hidden}#onetrust-pc-sdk #ot-pc-lst .ot-tgl-cntr{right:12px;position:absolute}#onetrust-pc-sdk #ot-pc-lst .ot-arw-cntr{float:right;position:relative}#onetrust-pc-sdk #ot-pc-lst .ot-arw{margin-left:10px}#onetrust-pc-sdk #ot-pc-lst .ot-acc-hdr{overflow:hidden;cursor:pointer}#onetrust-pc-sdk .ot-vlst-cntr{overflow:hidden}#onetrust-pc-sdk #ot-sel-blk{overflow:hidden;width:100%;position:sticky;position:-webkit-sticky;top:0;z-index:3}#onetrust-pc-sdk #ot-back-arw{height:12px;width:12px}#onetrust-pc-sdk .ot-lst-subhdr{width:100%;display:inline-block}#onetrust-pc-sdk .ot-search-cntr{float:left;width:78%;position:relative}#onetrust-pc-sdk .ot-search-cntr>svg{width:30px;height:30px;position:absolute;float:left;right:-15px}#onetrust-pc-sdk .ot-fltr-cntr{float:right;right:50px;position:relative}#onetrust-pc-sdk #filter-btn-handler{background-color:#3860be;border-radius:17px;display:inline-block;position:relative;width:32px;height:32px;-moz-transition:.1s ease;-o-transition:.1s ease;-webkit-transition:1s ease;transition:.1s ease;padding:0;margin:0}#onetrust-pc-sdk #filter-btn-handler:hover{background-color:#3860be}#onetrust-pc-sdk #filter-btn-handler svg{width:12px;height:12px;margin:3px 10px 0 10px;display:block;position:static;right:auto;top:auto}#onetrust-pc-sdk .ot-ven-link,#onetrust-pc-sdk .ot-ven-legclaim-link{color:#3860be;text-decoration:none;font-weight:100;display:inline-block;padding-top:10px;transform:translate(0, 1%);-o-transform:translate(0, 1%);-ms-transform:translate(0, 1%);-webkit-transform:translate(0, 1%);position:relative;z-index:2}#onetrust-pc-sdk .ot-ven-link *,#onetrust-pc-sdk .ot-ven-legclaim-link *{font-size:inherit}#onetrust-pc-sdk .ot-ven-link:hover,#onetrust-pc-sdk .ot-ven-legclaim-link:hover{text-decoration:underline}#onetrust-pc-sdk .ot-ven-hdr{width:calc(100% - 160px);height:auto;float:left;word-break:break-word;word-wrap:break-word;vertical-align:middle;padding-bottom:3px}#onetrust-pc-sdk .ot-ven-link,#onetrust-pc-sdk .ot-ven-legclaim-link{letter-spacing:.03em;font-size:.75em;font-weight:400}#onetrust-pc-sdk .ot-ven-dets{border-radius:2px;background-color:#f8f8f8}#onetrust-pc-sdk .ot-ven-dets li:first-child p:first-child{border-top:none}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:not(:first-child){border-top:1px solid #ddd !important}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:nth-child(n+3) p{display:inline-block}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:nth-child(n+3) p:nth-of-type(odd){width:30%}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:nth-child(n+3) p:nth-of-type(even){width:50%;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc p,#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc h4{padding-top:5px;padding-bottom:5px;display:block}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc h4{display:inline-block}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc p:nth-last-child(-n+1){padding-bottom:10px}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc p:nth-child(-n+2):not(.disc-pur){padding-top:10px}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc .disc-pur-cont{display:inline}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc .disc-pur{position:relative;width:50% !important;word-break:break-word;word-wrap:break-word;left:calc(30% + 17px)}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc .disc-pur:nth-child(-n+1){position:static}#onetrust-pc-sdk .ot-ven-dets p,#onetrust-pc-sdk .ot-ven-dets h4,#onetrust-pc-sdk .ot-ven-dets span{font-size:.69em;text-align:left;vertical-align:middle;word-break:break-word;word-wrap:break-word;margin:0;padding-bottom:10px;padding-left:15px;color:#2e3644}#onetrust-pc-sdk .ot-ven-dets h4{padding-top:5px}#onetrust-pc-sdk .ot-ven-dets span{color:dimgray;padding:0;vertical-align:baseline}#onetrust-pc-sdk .ot-ven-dets .ot-ven-pur h4{border-top:1px solid #e9e9e9;border-bottom:1px solid #e9e9e9;padding-bottom:5px;margin-bottom:5px;font-weight:bold}#onetrust-pc-sdk #ot-host-lst .ot-sel-all{float:right;position:relative;margin-right:42px;top:10px}#onetrust-pc-sdk #ot-host-lst .ot-sel-all input[type=checkbox]{width:auto;height:auto}#onetrust-pc-sdk #ot-host-lst .ot-sel-all label{height:20px;width:20px;padding-left:0px}#onetrust-pc-sdk #ot-host-lst .ot-acc-txt{overflow:hidden;width:95%}#onetrust-pc-sdk .ot-host-hdr{position:relative;z-index:1;pointer-events:none;width:calc(100% - 125px);float:left}#onetrust-pc-sdk .ot-host-name,#onetrust-pc-sdk .ot-host-desc{display:inline-block;width:90%}#onetrust-pc-sdk .ot-host-name{pointer-events:none}#onetrust-pc-sdk .ot-host-hdr>a{text-decoration:underline;font-size:.82em;position:relative;z-index:2;float:left;margin-bottom:5px;pointer-events:initial}#onetrust-pc-sdk .ot-host-name+a{margin-top:5px}#onetrust-pc-sdk .ot-host-name,#onetrust-pc-sdk .ot-host-name a,#onetrust-pc-sdk .ot-host-desc,#onetrust-pc-sdk .ot-host-info{color:dimgray;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-host-name,#onetrust-pc-sdk .ot-host-name a{font-weight:bold;font-size:.82em;line-height:1.3}#onetrust-pc-sdk .ot-host-name a{font-size:1em}#onetrust-pc-sdk .ot-host-expand{margin-top:3px;margin-bottom:3px;clear:both;display:block;color:#3860be;font-size:.72em;font-weight:normal}#onetrust-pc-sdk .ot-host-expand *{font-size:inherit}#onetrust-pc-sdk .ot-host-desc,#onetrust-pc-sdk .ot-host-info{font-size:.688em;line-height:1.4;font-weight:normal}#onetrust-pc-sdk .ot-host-desc{margin-top:10px}#onetrust-pc-sdk .ot-host-opt{margin:0;font-size:inherit;display:inline-block;width:100%}#onetrust-pc-sdk .ot-host-opt li>div div{font-size:.8em;padding:5px 0}#onetrust-pc-sdk .ot-host-opt li>div div:nth-child(1){width:30%;float:left}#onetrust-pc-sdk .ot-host-opt li>div div:nth-child(2){width:70%;float:left;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-host-info{border:none;display:inline-block;width:calc(100% - 10px);padding:10px;margin-bottom:10px;background-color:#f8f8f8}#onetrust-pc-sdk .ot-host-info>div{overflow:auto}#onetrust-pc-sdk #no-results{text-align:center;margin-top:30px}#onetrust-pc-sdk #no-results p{font-size:1em;color:#2e3644;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk #no-results p span{font-weight:bold}#onetrust-pc-sdk #ot-fltr-modal{width:100%;height:auto;display:none;-moz-transition:.2s ease;-o-transition:.2s ease;-webkit-transition:2s ease;transition:.2s ease;overflow:hidden;opacity:1;right:0}#onetrust-pc-sdk #ot-fltr-modal .ot-label-txt{display:inline-block;font-size:.85em;color:dimgray}#onetrust-pc-sdk #ot-fltr-cnt{z-index:2147483646;background-color:#fff;position:absolute;height:90%;max-height:300px;width:325px;left:210px;margin-top:10px;margin-bottom:20px;padding-right:10px;border-radius:3px;-webkit-box-shadow:0px 0px 12px 2px #c7c5c7;-moz-box-shadow:0px 0px 12px 2px #c7c5c7;box-shadow:0px 0px 12px 2px #c7c5c7}#onetrust-pc-sdk .ot-fltr-scrlcnt{overflow-y:auto;overflow-x:hidden;clear:both;max-height:calc(100% - 60px)}#onetrust-pc-sdk #ot-anchor{border:12px solid rgba(0,0,0,0);display:none;position:absolute;z-index:2147483647;right:55px;top:75px;transform:rotate(45deg);-o-transform:rotate(45deg);-ms-transform:rotate(45deg);-webkit-transform:rotate(45deg);background-color:#fff;-webkit-box-shadow:-3px -3px 5px -2px #c7c5c7;-moz-box-shadow:-3px -3px 5px -2px #c7c5c7;box-shadow:-3px -3px 5px -2px #c7c5c7}#onetrust-pc-sdk .ot-fltr-btns{margin-left:15px}#onetrust-pc-sdk #filter-apply-handler{margin-right:15px}#onetrust-pc-sdk .ot-fltr-opt{margin-bottom:25px;margin-left:15px;width:75%;position:relative}#onetrust-pc-sdk .ot-fltr-opt p{display:inline-block;margin:0;font-size:.9em;color:#2e3644}#onetrust-pc-sdk .ot-chkbox label span{font-size:.85em;color:dimgray}#onetrust-pc-sdk .ot-chkbox input[type=checkbox]+label::after{content:none;color:#fff}#onetrust-pc-sdk .ot-chkbox input[type=checkbox]:checked+label::after{content:""}#onetrust-pc-sdk .ot-chkbox input[type=checkbox]:focus+label::before{outline-style:solid;outline-width:2px;outline-style:auto}#onetrust-pc-sdk #ot-selall-vencntr,#onetrust-pc-sdk #ot-selall-adtlvencntr,#onetrust-pc-sdk #ot-selall-hostcntr,#onetrust-pc-sdk #ot-selall-licntr,#onetrust-pc-sdk #ot-selall-gnvencntr{right:15px;position:relative;width:20px;height:20px;float:right}#onetrust-pc-sdk #ot-selall-vencntr label,#onetrust-pc-sdk #ot-selall-adtlvencntr label,#onetrust-pc-sdk #ot-selall-hostcntr label,#onetrust-pc-sdk #ot-selall-licntr label,#onetrust-pc-sdk #ot-selall-gnvencntr label{float:left;padding-left:0}#onetrust-pc-sdk #ot-ven-lst:first-child{border-top:1px solid #e2e2e2}#onetrust-pc-sdk ul{list-style:none;padding:0}#onetrust-pc-sdk ul li{position:relative;margin:0;padding:15px 15px 15px 10px;border-bottom:1px solid #e2e2e2}#onetrust-pc-sdk ul li h3{font-size:.75em;color:#656565;margin:0;display:inline-block;width:70%;height:auto;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk ul li p{margin:0;font-size:.7em}#onetrust-pc-sdk ul li input[type=checkbox]{position:absolute;cursor:pointer;width:100%;height:100%;opacity:0;margin:0;top:0;left:0}#onetrust-pc-sdk .ot-cat-item>button:focus,#onetrust-pc-sdk .ot-acc-cntr>button:focus,#onetrust-pc-sdk li>button:focus{outline:#000 solid 2px}#onetrust-pc-sdk .ot-cat-item>button,#onetrust-pc-sdk .ot-acc-cntr>button,#onetrust-pc-sdk li>button{position:absolute;cursor:pointer;width:100%;height:100%;margin:0;top:0;left:0;z-index:1;max-width:none;border:none}#onetrust-pc-sdk .ot-cat-item>button[aria-expanded=false]~.ot-acc-txt,#onetrust-pc-sdk .ot-acc-cntr>button[aria-expanded=false]~.ot-acc-txt,#onetrust-pc-sdk li>button[aria-expanded=false]~.ot-acc-txt{margin-top:0;max-height:0;opacity:0;overflow:hidden;width:100%;transition:.25s ease-out;display:none}#onetrust-pc-sdk .ot-cat-item>button[aria-expanded=true]~.ot-acc-txt,#onetrust-pc-sdk .ot-acc-cntr>button[aria-expanded=true]~.ot-acc-txt,#onetrust-pc-sdk li>button[aria-expanded=true]~.ot-acc-txt{transition:.1s ease-in;margin-top:10px;width:100%;overflow:auto;display:block}#onetrust-pc-sdk .ot-cat-item>button[aria-expanded=true]~.ot-acc-grpcntr,#onetrust-pc-sdk .ot-acc-cntr>button[aria-expanded=true]~.ot-acc-grpcntr,#onetrust-pc-sdk li>button[aria-expanded=true]~.ot-acc-grpcntr{width:auto;margin-top:0px;padding-bottom:10px}#onetrust-pc-sdk .ot-host-item>button:focus,#onetrust-pc-sdk .ot-ven-item>button:focus{outline:0;border:2px solid #000}#onetrust-pc-sdk .ot-hide-acc>button{pointer-events:none}#onetrust-pc-sdk .ot-hide-acc .ot-plus-minus>*,#onetrust-pc-sdk .ot-hide-acc .ot-arw-cntr>*{visibility:hidden}#onetrust-pc-sdk .ot-hide-acc .ot-acc-hdr{min-height:30px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt){padding-right:10px;width:calc(100% - 37px);margin-top:10px;max-height:calc(100% - 90px)}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk{background-color:#f9f9fc;border:1px solid #e2e2e2;width:calc(100% - 2px);padding-bottom:5px;padding-top:5px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt{border:unset;background-color:unset}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt .ot-sel-all-hdr{display:none}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt .ot-sel-all{padding-right:.5rem}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt .ot-sel-all .ot-chkbox{right:0}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) .ot-sel-all{padding-right:34px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) .ot-sel-all-chkbox{width:auto}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) ul li{border:1px solid #e2e2e2;margin-bottom:10px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) .ot-acc-cntr>.ot-acc-hdr{padding:10px 0 10px 15px}#onetrust-pc-sdk.ot-addtl-vendors .ot-sel-all-chkbox{float:right}#onetrust-pc-sdk.ot-addtl-vendors .ot-plus-minus~.ot-sel-all-chkbox{right:34px}#onetrust-pc-sdk.ot-addtl-vendors #ot-ven-lst:first-child{border-top:none}#onetrust-pc-sdk .ot-acc-cntr{position:relative;border-left:1px solid #e2e2e2;border-right:1px solid #e2e2e2;border-bottom:1px solid #e2e2e2}#onetrust-pc-sdk .ot-acc-cntr input{z-index:1}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr{background-color:#f9f9fc;padding:5px 0 5px 15px;width:auto}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr .ot-plus-minus{vertical-align:middle;top:auto}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr .ot-arw-cntr{right:10px}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr input{z-index:2}#onetrust-pc-sdk .ot-acc-cntr.ot-add-tech .ot-acc-hdr{padding:10px 0 10px 15px}#onetrust-pc-sdk .ot-acc-cntr>input[type=checkbox]:checked~.ot-acc-hdr{border-bottom:1px solid #e2e2e2}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-txt{padding-left:10px;padding-right:10px}#onetrust-pc-sdk .ot-acc-cntr button[aria-expanded=true]~.ot-acc-txt{width:auto}#onetrust-pc-sdk .ot-acc-cntr .ot-addtl-venbox{display:none}#onetrust-pc-sdk .ot-vlst-cntr{margin-bottom:0;width:100%}#onetrust-pc-sdk .ot-vensec-title{font-size:.813em;vertical-align:middle;display:inline-block}#onetrust-pc-sdk .category-vendors-list-handler,#onetrust-pc-sdk .category-vendors-list-handler+a{margin-left:0;margin-top:10px}#onetrust-pc-sdk #ot-selall-vencntr.line-through label::after,#onetrust-pc-sdk #ot-selall-adtlvencntr.line-through label::after,#onetrust-pc-sdk #ot-selall-licntr.line-through label::after,#onetrust-pc-sdk #ot-selall-hostcntr.line-through label::after,#onetrust-pc-sdk #ot-selall-gnvencntr.line-through label::after{height:auto;border-left:0;transform:none;-o-transform:none;-ms-transform:none;-webkit-transform:none;left:5px;top:9px}#onetrust-pc-sdk #ot-category-title{float:left;padding-bottom:10px;font-size:1em;width:100%}#onetrust-pc-sdk .ot-cat-grp{margin-top:10px}#onetrust-pc-sdk .ot-cat-item{line-height:1.1;margin-top:10px;display:inline-block;width:100%}#onetrust-pc-sdk .ot-btn-container{text-align:right}#onetrust-pc-sdk .ot-btn-container button{display:inline-block;font-size:.75em;letter-spacing:.08em;margin-top:19px}#onetrust-pc-sdk #close-pc-btn-handler.ot-close-icon{position:absolute;top:10px;right:0;z-index:1;padding:0;background-color:rgba(0,0,0,0);border:none}#onetrust-pc-sdk #close-pc-btn-handler.ot-close-icon svg{display:block;height:10px;width:10px}#onetrust-pc-sdk #clear-filters-handler{margin-top:20px;margin-bottom:10px;float:right;max-width:200px;text-decoration:none;color:#3860be;font-size:.9em;font-weight:bold;background-color:rgba(0,0,0,0);border-color:rgba(0,0,0,0);padding:1px}#onetrust-pc-sdk #clear-filters-handler:hover{color:#2285f7}#onetrust-pc-sdk #clear-filters-handler:focus{outline:#000 solid 1px}#onetrust-pc-sdk .ot-enbl-chr h4~.ot-tgl,#onetrust-pc-sdk .ot-enbl-chr h4~.ot-always-active{right:45px}#onetrust-pc-sdk .ot-enbl-chr h4~.ot-tgl+.ot-tgl{right:120px}#onetrust-pc-sdk .ot-enbl-chr .ot-pli-hdr.ot-leg-border-color span:first-child{width:90px}#onetrust-pc-sdk .ot-enbl-chr li.ot-subgrp>h5+.ot-tgl-cntr{padding-right:25px}#onetrust-pc-sdk .ot-plus-minus{width:20px;height:20px;font-size:1.5em;position:relative;display:inline-block;margin-right:5px;top:3px}#onetrust-pc-sdk .ot-plus-minus span{position:absolute;background:#27455c;border-radius:1px}#onetrust-pc-sdk .ot-plus-minus span:first-of-type{top:25%;bottom:25%;width:10%;left:45%}#onetrust-pc-sdk .ot-plus-minus span:last-of-type{left:25%;right:25%;height:10%;top:45%}#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-arw,#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-plus-minus span:first-of-type,#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-plus-minus span:last-of-type{transform:rotate(90deg)}#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-plus-minus span:last-of-type{left:50%;right:50%}#onetrust-pc-sdk #ot-selall-vencntr label,#onetrust-pc-sdk #ot-selall-adtlvencntr label,#onetrust-pc-sdk #ot-selall-hostcntr label,#onetrust-pc-sdk #ot-selall-licntr label{position:relative;display:inline-block;width:20px;height:20px}#onetrust-pc-sdk .ot-host-item .ot-plus-minus,#onetrust-pc-sdk .ot-ven-item .ot-plus-minus{float:left;margin-right:8px;top:10px}#onetrust-pc-sdk .ot-ven-item ul{list-style:none inside;font-size:100%;margin:0}#onetrust-pc-sdk .ot-ven-item ul li{margin:0 !important;padding:0;border:none !important}#onetrust-pc-sdk .ot-pli-hdr{color:#77808e;overflow:hidden;padding-top:7.5px;padding-bottom:7.5px;width:calc(100% - 2px);border-top-left-radius:3px;border-top-right-radius:3px}#onetrust-pc-sdk .ot-pli-hdr span:first-child{top:50%;transform:translateY(50%);max-width:90px}#onetrust-pc-sdk .ot-pli-hdr span:last-child{padding-right:10px;max-width:95px;text-align:center}#onetrust-pc-sdk .ot-li-title{float:right;font-size:.813em}#onetrust-pc-sdk .ot-pli-hdr.ot-leg-border-color{background-color:#f4f4f4;border:1px solid #d8d8d8}#onetrust-pc-sdk .ot-pli-hdr.ot-leg-border-color span:first-child{text-align:left;width:70px}#onetrust-pc-sdk li.ot-subgrp>h5,#onetrust-pc-sdk .ot-cat-header{width:calc(100% - 130px)}#onetrust-pc-sdk li.ot-subgrp>h5+.ot-tgl-cntr{padding-left:13px}#onetrust-pc-sdk .ot-acc-grpcntr .ot-acc-grpdesc{margin-bottom:5px}#onetrust-pc-sdk .ot-acc-grpcntr .ot-subgrp-cntr{border-top:1px solid #d8d8d8}#onetrust-pc-sdk .ot-acc-grpcntr .ot-vlst-cntr+.ot-subgrp-cntr{border-top:none}#onetrust-pc-sdk .ot-acc-hdr .ot-arw-cntr+.ot-tgl-cntr,#onetrust-pc-sdk .ot-acc-txt h4+.ot-tgl-cntr{padding-left:13px}#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item .ot-subgrp>h5,#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item .ot-cat-header{width:calc(100% - 145px)}#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item h5+.ot-tgl-cntr,#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item .ot-cat-header+.ot-tgl{padding-left:28px}#onetrust-pc-sdk .ot-sel-all-hdr,#onetrust-pc-sdk .ot-sel-all-chkbox{display:inline-block;width:100%;position:relative}#onetrust-pc-sdk .ot-sel-all-chkbox{z-index:1}#onetrust-pc-sdk .ot-sel-all{margin:0;position:relative;padding-right:23px;float:right}#onetrust-pc-sdk .ot-consent-hdr,#onetrust-pc-sdk .ot-li-hdr{float:right;font-size:.812em;line-height:normal;text-align:center;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-li-hdr{max-width:100px;padding-right:10px}#onetrust-pc-sdk .ot-consent-hdr{max-width:55px}#onetrust-pc-sdk #ot-selall-licntr{display:block;width:21px;height:auto;float:right;position:relative;right:80px}#onetrust-pc-sdk #ot-selall-licntr label{position:absolute}#onetrust-pc-sdk .ot-ven-ctgl{margin-left:66px}#onetrust-pc-sdk .ot-ven-litgl+.ot-arw-cntr{margin-left:81px}#onetrust-pc-sdk .ot-enbl-chr .ot-host-cnt .ot-tgl-cntr{width:auto}#onetrust-pc-sdk #ot-lst-cnt:not(.ot-host-cnt) .ot-tgl-cntr{width:auto;top:auto;height:20px}#onetrust-pc-sdk #ot-lst-cnt .ot-chkbox{position:relative;display:inline-block;width:20px;height:20px}#onetrust-pc-sdk #ot-lst-cnt .ot-chkbox label{position:absolute;padding:0;width:20px;height:20px}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info-cntr{border:1px solid #d8d8d8;padding:.75rem 2rem;padding-bottom:0;width:auto;margin-top:.5rem}#onetrust-pc-sdk .ot-acc-grpdesc+.ot-leg-btn-container{padding-left:20px;padding-right:20px;width:calc(100% - 40px);margin-bottom:5px}#onetrust-pc-sdk .ot-subgrp .ot-leg-btn-container{margin-bottom:5px}#onetrust-pc-sdk #ot-ven-lst .ot-leg-btn-container{margin-top:10px}#onetrust-pc-sdk .ot-leg-btn-container{display:inline-block;width:100%;margin-bottom:10px}#onetrust-pc-sdk .ot-leg-btn-container button{height:auto;padding:6.5px 8px;margin-bottom:0;letter-spacing:0;font-size:.75em;line-height:normal}#onetrust-pc-sdk .ot-leg-btn-container svg{display:none;height:14px;width:14px;padding-right:5px;vertical-align:sub}#onetrust-pc-sdk .ot-active-leg-btn{cursor:default;pointer-events:none}#onetrust-pc-sdk .ot-active-leg-btn svg{display:inline-block}#onetrust-pc-sdk .ot-remove-objection-handler{text-decoration:underline;padding:0;font-size:.75em;font-weight:600;line-height:1;padding-left:10px}#onetrust-pc-sdk .ot-obj-leg-btn-handler span{font-weight:bold;text-align:center;font-size:inherit;line-height:1.5}#onetrust-pc-sdk.ot-close-btn-link #close-pc-btn-handler{border:none;height:auto;line-height:1.5;text-decoration:underline;font-size:.69em;background:none;right:15px;top:15px;width:auto;font-weight:normal}#onetrust-pc-sdk .ot-pgph-link{font-size:.813em !important;margin-top:5px;position:relative}#onetrust-pc-sdk .ot-pgph-link.ot-pgph-link-subgroup{margin-bottom:1rem}#onetrust-pc-sdk .ot-pgph-contr{margin:0 2.5rem}#onetrust-pc-sdk .ot-pgph-title{font-size:1.18rem;margin-bottom:2rem}#onetrust-pc-sdk .ot-pgph-desc{font-size:1rem;font-weight:400;margin-bottom:2rem;line-height:1.5rem}#onetrust-pc-sdk .ot-pgph-desc:not(:last-child):after{content:"";width:96%;display:block;margin:0 auto;padding-bottom:2rem;border-bottom:1px solid #e9e9e9}#onetrust-pc-sdk .ot-cat-header{float:left;font-weight:600;font-size:.875em;line-height:1.5;max-width:90%;vertical-align:middle}#onetrust-pc-sdk .ot-vnd-item>button:focus{outline:#000 solid 2px}#onetrust-pc-sdk .ot-vnd-item>button{position:absolute;cursor:pointer;width:100%;height:100%;margin:0;top:0;left:0;z-index:1;max-width:none;border:none}#onetrust-pc-sdk .ot-vnd-item>button[aria-expanded=false]~.ot-acc-txt{margin-top:0;max-height:0;opacity:0;overflow:hidden;width:100%;transition:.25s ease-out;display:none}#onetrust-pc-sdk .ot-vnd-item>button[aria-expanded=true]~.ot-acc-txt{transition:.1s ease-in;margin-top:10px;width:100%;overflow:auto;display:block}#onetrust-pc-sdk .ot-vnd-item>button[aria-expanded=true]~.ot-acc-grpcntr{width:auto;margin-top:0px;padding-bottom:10px}#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item{position:relative;border-radius:2px;margin:0;padding:0;border:1px solid #d8d8d8;border-top:none;width:calc(100% - 2px);float:left}#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item:first-of-type{margin-top:10px;border-top:1px solid #d8d8d8}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpdesc{padding-left:20px;padding-right:20px;width:calc(100% - 40px);font-size:.812em;margin-bottom:10px;margin-top:15px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpdesc>ul{padding-top:10px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpdesc>ul li{padding-top:0;line-height:1.5;padding-bottom:10px}#onetrust-pc-sdk .ot-accordion-layout div+.ot-acc-grpdesc{margin-top:5px}#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr:first-child{margin-top:10px}#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr:last-child,#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr:last-child{margin-bottom:5px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{padding-top:11.5px;padding-bottom:11.5px;padding-left:20px;padding-right:20px;width:calc(100% - 40px);display:inline-block}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-txt{width:100%;padding:0}#onetrust-pc-sdk .ot-accordion-layout .ot-subgrp-cntr{padding-left:20px;padding-right:15px;padding-bottom:0;width:calc(100% - 35px)}#onetrust-pc-sdk .ot-accordion-layout .ot-subgrp{padding-right:5px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpcntr{z-index:1;position:relative}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header+.ot-arw-cntr{position:absolute;top:50%;transform:translateY(-50%);right:20px;margin-top:-2px}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header+.ot-arw-cntr .ot-arw{width:15px;height:20px;margin-left:5px;color:dimgray}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{float:none;color:#2e3644;margin:0;display:inline-block;height:auto;word-wrap:break-word;min-height:inherit}#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr,#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr{padding-left:20px;width:calc(100% - 20px);display:inline-block;margin-top:0;padding-bottom:2px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{position:relative;min-height:25px}#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl,#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active{position:absolute;top:50%;transform:translateY(-50%);right:20px}#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl+.ot-tgl{right:95px}#onetrust-pc-sdk .ot-accordion-layout .category-vendors-list-handler,#onetrust-pc-sdk .ot-accordion-layout .category-vendors-list-handler+a{margin-top:5px}#onetrust-pc-sdk #ot-lst-cnt{margin-top:1rem;max-height:calc(100% - 96px)}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info-cntr{border:1px solid #d8d8d8;padding:.75rem 2rem;padding-bottom:0;width:auto;margin-top:.5rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info{margin-bottom:1rem;padding-left:.75rem;padding-right:.75rem;display:flex;flex-direction:column}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info[data-vnd-info-key*=DPOEmail]{border-top:1px solid #d8d8d8;padding-top:1rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info[data-vnd-info-key*=DPOLink]{border-bottom:1px solid #d8d8d8;padding-bottom:1rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info .ot-vnd-lbl{font-weight:bold;font-size:.85em;margin-bottom:.5rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info .ot-vnd-cnt{margin-left:.5rem;font-weight:500;font-size:.85rem}#onetrust-pc-sdk .ot-vs-list,#onetrust-pc-sdk .ot-vnd-serv{width:auto;padding:1rem 1.25rem;padding-bottom:0}#onetrust-pc-sdk .ot-vs-list .ot-vnd-serv-hdr-cntr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-serv-hdr-cntr{padding-bottom:.75rem;border-bottom:1px solid #d8d8d8}#onetrust-pc-sdk .ot-vs-list .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr{font-weight:600;font-size:.95em;line-height:2;margin-left:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item{border:none;margin:0;padding:0}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item button,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item button{outline:none;border-bottom:1px solid #d8d8d8}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item button[aria-expanded=true],#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item button[aria-expanded=true]{border-bottom:none}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item:first-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item:first-child{margin-top:.25rem;border-top:unset}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item:last-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item:last-child{margin-bottom:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item:last-child button,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item:last-child button{border-bottom:none}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info-cntr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info-cntr{border:1px solid #d8d8d8;padding:.75rem 1.75rem;padding-bottom:0;width:auto;margin-top:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info{margin-bottom:1rem;padding-left:.75rem;padding-right:.75rem;display:flex;flex-direction:column}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOEmail],#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOEmail]{border-top:1px solid #d8d8d8;padding-top:1rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOLink],#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOLink]{border-bottom:1px solid #d8d8d8;padding-bottom:1rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info .ot-vnd-lbl,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info .ot-vnd-lbl{font-weight:bold;font-size:.85em;margin-bottom:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info .ot-vnd-cnt,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info .ot-vnd-cnt{margin-left:.5rem;font-weight:500;font-size:.85rem}#onetrust-pc-sdk .ot-vs-list.ot-vnd-subgrp-cnt,#onetrust-pc-sdk .ot-vnd-serv.ot-vnd-subgrp-cnt{padding-left:40px}#onetrust-pc-sdk .ot-vs-list.ot-vnd-subgrp-cnt .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr,#onetrust-pc-sdk .ot-vnd-serv.ot-vnd-subgrp-cnt .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr{font-size:.8em}#onetrust-pc-sdk .ot-vs-list.ot-vnd-subgrp-cnt .ot-cat-header,#onetrust-pc-sdk .ot-vnd-serv.ot-vnd-subgrp-cnt .ot-cat-header{font-size:.8em}#onetrust-pc-sdk .ot-subgrp-cntr .ot-vnd-serv{margin-bottom:1rem;padding:1rem .95rem}#onetrust-pc-sdk .ot-subgrp-cntr .ot-vnd-serv .ot-vnd-serv-hdr-cntr{padding-bottom:.75rem;border-bottom:1px solid #d8d8d8}#onetrust-pc-sdk .ot-subgrp-cntr .ot-vnd-serv .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr{font-weight:700;font-size:.8em;line-height:20px;margin-left:.82rem}#onetrust-pc-sdk .ot-subgrp-cntr .ot-cat-header{font-weight:700;font-size:.8em;line-height:20px}#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-vnd-serv .ot-vnd-lst-cont .ot-accordion-layout .ot-acc-hdr div.ot-chkbox{margin-left:.82rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr{padding:.7rem 0;margin:0;display:flex;width:100%;align-items:center;justify-content:space-between}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr div:first-child,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr div:first-child,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr div:first-child{margin-left:.5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr div:last-child,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr div:last-child,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr div:last-child{margin-right:.5rem;margin-left:.5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-always-active{position:relative;right:unset;top:unset;transform:unset}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-plus-minus{top:0}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-arw-cntr{float:none;top:unset;right:unset;transform:unset;margin-top:-2px;position:relative}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-cat-header{flex:1;margin:0 .5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-tgl{position:relative;transform:none;right:0;top:0;float:none}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox{position:relative;margin:0 .5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox label{padding:0}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox label::before{position:relative}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox input{position:absolute;cursor:pointer;width:100%;height:100%;opacity:0;margin:0;top:0;left:0;z-index:1}#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp .ot-acc-hdr h5.ot-cat-header,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp .ot-acc-hdr h4.ot-cat-header{margin:0}#onetrust-pc-sdk .ot-vs-config .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp h5{top:0;line-height:20px}#onetrust-pc-sdk .ot-vs-list{display:flex;flex-direction:column;padding:0;margin:.5rem 4px}#onetrust-pc-sdk .ot-vs-selc-all{display:flex;padding:0;float:unset;align-items:center;justify-content:flex-start}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf{justify-content:flex-end}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf.ot-caret-conf .ot-sel-all-chkbox{margin-right:48px}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf .ot-sel-all-chkbox{margin:0;padding:0;margin-right:14px;justify-content:flex-end}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf #ot-selall-vencntr.ot-chkbox,#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf #ot-selall-vencntr.ot-tgl{display:inline-block;right:unset;width:auto;height:auto;float:none}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf #ot-selall-vencntr label{width:45px;height:25px}#onetrust-pc-sdk .ot-vs-selc-all .ot-sel-all-chkbox{margin-right:11px;margin-left:.75rem;display:flex;align-items:center}#onetrust-pc-sdk .ot-vs-selc-all .sel-all-hdr{margin:0 1.25rem;font-size:.812em;line-height:normal;text-align:center;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-vnd-list-cnt #ot-selall-vencntr.ot-chkbox{float:unset;right:0}#onetrust-pc-sdk[dir=rtl] #ot-back-arw,#onetrust-pc-sdk[dir=rtl] input~.ot-acc-hdr .ot-arw{transform:rotate(180deg);-o-transform:rotate(180deg);-ms-transform:rotate(180deg);-webkit-transform:rotate(180deg)}#onetrust-pc-sdk[dir=rtl] input:checked~.ot-acc-hdr .ot-arw{transform:rotate(270deg);-o-transform:rotate(270deg);-ms-transform:rotate(270deg);-webkit-transform:rotate(270deg)}#onetrust-pc-sdk[dir=rtl] .ot-chkbox label::after{transform:rotate(45deg);-webkit-transform:rotate(45deg);-o-transform:rotate(45deg);-ms-transform:rotate(45deg);border-left:0;border-right:3px solid}#onetrust-pc-sdk[dir=rtl] .ot-search-cntr>svg{right:0}@media only screen and (max-width: 600px){#onetrust-pc-sdk.otPcCenter{left:0;min-width:100%;height:100%;top:0;border-radius:0}#onetrust-pc-sdk #ot-pc-content,#onetrust-pc-sdk.ot-ftr-stacked .ot-btn-container{margin:1px 3px 0 10px;padding-right:10px;width:calc(100% - 23px)}#onetrust-pc-sdk .ot-btn-container button{max-width:none;letter-spacing:.01em}#onetrust-pc-sdk #close-pc-btn-handler{top:10px;right:17px}#onetrust-pc-sdk p{font-size:.7em}#onetrust-pc-sdk #ot-pc-hdr{margin:10px 10px 0 5px;width:calc(100% - 15px)}#onetrust-pc-sdk .vendor-search-handler{font-size:1em}#onetrust-pc-sdk #ot-back-arw{margin-left:12px}#onetrust-pc-sdk #ot-lst-cnt{margin:0;padding:0 5px 0 10px;min-width:95%}#onetrust-pc-sdk .switch+p{max-width:80%}#onetrust-pc-sdk .ot-ftr-stacked button{width:100%}#onetrust-pc-sdk #ot-fltr-cnt{max-width:320px;width:90%;border-top-right-radius:0;border-bottom-right-radius:0;margin:0;margin-left:15px;left:auto;right:40px;top:85px}#onetrust-pc-sdk .ot-fltr-opt{margin-left:25px;margin-bottom:10px}#onetrust-pc-sdk .ot-pc-refuse-all-handler{margin-bottom:0}#onetrust-pc-sdk #ot-fltr-cnt{right:40px}}@media only screen and (max-width: 476px){#onetrust-pc-sdk .ot-fltr-cntr,#onetrust-pc-sdk #ot-fltr-cnt{right:10px}#onetrust-pc-sdk #ot-anchor{right:25px}#onetrust-pc-sdk button{width:100%}#onetrust-pc-sdk:not(.ot-addtl-vendors) #ot-pc-lst:not(.ot-enbl-chr) .ot-sel-all{padding-right:9px}#onetrust-pc-sdk:not(.ot-addtl-vendors) #ot-pc-lst:not(.ot-enbl-chr) .ot-tgl-cntr{right:0}}@media only screen and (max-width: 896px)and (max-height: 425px)and (orientation: landscape){#onetrust-pc-sdk.otPcCenter{left:0;top:0;min-width:100%;height:100%;border-radius:0}#onetrust-pc-sdk .ot-pc-header{height:auto;min-height:20px}#onetrust-pc-sdk .ot-pc-header .ot-pc-logo{max-height:30px}#onetrust-pc-sdk .ot-pc-footer{max-height:60px;overflow-y:auto}#onetrust-pc-sdk #ot-pc-content,#onetrust-pc-sdk #ot-pc-lst{bottom:70px}#onetrust-pc-sdk.ot-ftr-stacked #ot-pc-content{bottom:70px}#onetrust-pc-sdk #ot-anchor{left:initial;right:50px}#onetrust-pc-sdk #ot-lst-title{margin-top:12px}#onetrust-pc-sdk #ot-lst-title *{font-size:inherit}#onetrust-pc-sdk #ot-pc-hdr input{margin-right:0;padding-right:45px}#onetrust-pc-sdk .switch+p{max-width:85%}#onetrust-pc-sdk #ot-sel-blk{position:static}#onetrust-pc-sdk #ot-pc-lst{overflow:auto}#onetrust-pc-sdk #ot-lst-cnt{max-height:none;overflow:initial}#onetrust-pc-sdk #ot-lst-cnt.no-results{height:auto}#onetrust-pc-sdk input{font-size:1em !important}#onetrust-pc-sdk p{font-size:.6em}#onetrust-pc-sdk #ot-fltr-modal{width:100%;top:0}#onetrust-pc-sdk ul li p,#onetrust-pc-sdk .category-vendors-list-handler,#onetrust-pc-sdk .category-vendors-list-handler+a,#onetrust-pc-sdk .category-host-list-handler{font-size:.6em}#onetrust-pc-sdk.ot-shw-fltr #ot-anchor{display:none !important}#onetrust-pc-sdk.ot-shw-fltr #ot-pc-lst{height:100% !important;overflow:hidden;top:0px}#onetrust-pc-sdk.ot-shw-fltr #ot-fltr-cnt{margin:0;height:100%;max-height:none;padding:10px;top:0;width:calc(100% - 20px);position:absolute;right:0;left:0;max-width:none}#onetrust-pc-sdk.ot-shw-fltr .ot-fltr-scrlcnt{max-height:calc(100% - 65px)}}
            #onetrust-consent-sdk #onetrust-pc-sdk,
                #onetrust-consent-sdk #ot-search-cntr,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-switch.ot-toggle,
                #onetrust-consent-sdk #onetrust-pc-sdk ot-grp-hdr1 .checkbox,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-title:after
                ,#onetrust-consent-sdk #onetrust-pc-sdk #ot-sel-blk,
                        #onetrust-consent-sdk #onetrust-pc-sdk #ot-fltr-cnt,
                        #onetrust-consent-sdk #onetrust-pc-sdk #ot-anchor {
                    background-color: #FFF;
                }
               
            #onetrust-consent-sdk #onetrust-pc-sdk h3,
                #onetrust-consent-sdk #onetrust-pc-sdk h4,
                #onetrust-consent-sdk #onetrust-pc-sdk h5,
                #onetrust-consent-sdk #onetrust-pc-sdk h6,
                #onetrust-consent-sdk #onetrust-pc-sdk p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-ven-lst .ot-ven-opts p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-desc,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-title,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-li-title,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-sel-all-hdr span,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-info,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-fltr-modal #modal-header,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-checkbox label span,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst #ot-sel-blk p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst #ot-lst-title h3,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst .back-btn-handler p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst .ot-ven-name,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst #ot-ven-lst .consent-category,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-inactive-leg-btn,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-label-status,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-chkbox label span,
                #onetrust-consent-sdk #onetrust-pc-sdk #clear-filters-handler,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-optout-signal
                {
                    color: #2E2E2E;
                }
             #onetrust-consent-sdk #onetrust-pc-sdk .privacy-notice-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-pgph-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk .category-vendors-list-handler,
                    #onetrust-consent-sdk #onetrust-pc-sdk .category-vendors-list-handler + a,
                    #onetrust-consent-sdk #onetrust-pc-sdk .category-host-list-handler,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-ven-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-ven-legclaim-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-name a,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-acc-hdr .ot-host-expand,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-info a,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-content #ot-pc-desc .ot-link-btn,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info a,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info a
                    {
                        color: #007398;
                    }
            #onetrust-consent-sdk #onetrust-pc-sdk .category-vendors-list-handler:hover { text-decoration: underline;}
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-grpcntr.ot-acc-txt,
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-txt .ot-subgrp-tgl .ot-switch.ot-toggle
             {
                background-color: #F8F8F8;
            }
             #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-info,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-txt .ot-ven-dets
                            {
                                background-color: #F8F8F8;
                            }
        #onetrust-consent-sdk #onetrust-pc-sdk
            button:not(#clear-filters-handler):not(.ot-close-icon):not(#filter-btn-handler):not(.ot-remove-objection-handler):not(.ot-obj-leg-btn-handler):not([aria-expanded]):not(.ot-link-btn),
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-active-leg-btn {
                background-color: #007398;border-color: #007398;
                color: #FFF;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-active-menu {
                border-color: #007398;
            }
            
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-remove-objection-handler{
                background-color: transparent;
                border: 1px solid transparent;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-inactive-leg-btn {
                background-color: #FFFFFF;
                color: #78808E; border-color: #78808E;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-tgl input:focus + .ot-switch, .ot-switch .ot-switch-nob, .ot-switch .ot-switch-nob:before,
            #onetrust-pc-sdk .ot-checkbox input[type="checkbox"]:focus + label::before,
            #onetrust-pc-sdk .ot-chkbox input[type="checkbox"]:focus + label::before {
                outline-color: #000000;
                outline-width: 1px;
            }
            #onetrust-pc-sdk .ot-host-item > button:focus, #onetrust-pc-sdk .ot-ven-item > button:focus {
                border: 1px solid #000000;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk *:focus,
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-vlst-cntr > a:focus {
               outline: 1px solid #000000;
            }#onetrust-pc-sdk .ot-vlst-cntr .ot-ext-lnk,  #onetrust-pc-sdk .ot-ven-hdr .ot-ext-lnk{
                    background-image: url('https://cdn.cookielaw.org/logos/static/ot_external_link.svg');
                }
            /*! Extra code to blur out background */
.onetrust-pc-dark-filter{
background:rgba(0,0,0,.5);
z-index:2147483646;
width:100%;
height:100%;
overflow:hidden;
position:fixed;
top:0;
bottom:0;
left:0;
backdrop-filter: initial
}

/*! v6.12.0 2021-01-19 */
div#onetrust-consent-sdk #onetrust-banner-sdk{border-top:2px solid #eb6500!important;outline:1px solid transparent;box-shadow:none;padding:24px}div#onetrust-consent-sdk button{border-radius:0!important;box-shadow:none!important;box-sizing:border-box!important;font-size:20px!important;font-weight:400!important;letter-spacing:0!important;max-width:none!important;white-space:nowrap!important}div#onetrust-consent-sdk button:not(.ot-link-btn){background-color:#007398!important;border:2px solid #007398!important;color:#fff!important;height:48px!important;padding:0 1em!important;width:auto!important}div#onetrust-consent-sdk button:hover{background-color:#fff!important;border-color:#eb6500!important;color:#2e2e2e!important}div#onetrust-consent-sdk button.ot-link-btn{color:#007398!important;font-size:16px!important;text-decoration:underline}div#onetrust-consent-sdk button.ot-link-btn:hover{color: #2e2e2e!important;text-decoration-color:#eb6500!important}div#onetrust-consent-sdk a,div#onetrust-pc-sdk a{color:#007398!important;text-decoration:underline!important}div#onetrust-consent-sdk a,div#onetrust-consent-sdk button,div#onetrust-consent-sdk p:hover{opacity:1!important}div#onetrust-consent-sdk a:focus,div#onetrust-consent-sdk button:focus,div#onetrust-consent-sdk input:focus{outline:2px solid #eb6500!important;outline-offset:1px!important}div#onetrust-banner-sdk .ot-sdk-container{padding:0;width:auto}div#onetrust-banner-sdk .ot-sdk-row{align-items:flex-start;box-sizing:border-box;display:flex;flex-direction:column;justify-content:space-between;margin:auto;max-width:1152px}div#onetrust-banner-sdk .ot-sdk-row:after{display:none}div#onetrust-banner-sdk #onetrust-group-container,div#onetrust-banner-sdk.ot-bnr-flift:not(.ot-iab-2) #onetrust-group-container,div#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{flex-grow:1;width:auto}div#onetrust-banner-sdk #onetrust-policy,div#onetrust-banner-sdk.ot-bnr-flift #onetrust-policy{margin:0;overflow:visible}div#onetrust-banner-sdk.ot-bnr-flift #onetrust-policy-text,div#onetrust-consent-sdk #onetrust-policy-text{font-size:16px;line-height:24px;max-width:44em;margin:0}div#onetrust-consent-sdk #onetrust-policy-text a[href]{font-weight:400;margin-left:8px}div#onetrust-banner-sdk #onetrust-button-group-parent{flex:0 0 auto;margin:32px 0 0;width:100%}div#onetrust-banner-sdk #onetrust-button-group{display:flex;flex-direction:row;flex-wrap:wrap;justify-content:flex-end;margin:-8px}div#onetrust-banner-sdk .banner-actions-container{display:flex;flex:1 0 auto}div#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group button:last-of-type,div#onetrust-consent-sdk #onetrust-accept-btn-handler,div#onetrust-consent-sdk #onetrust-pc-btn-handler{flex:1 0 auto;margin:8px;width:auto}div#onetrust-consent-sdk #onetrust-pc-btn-handler{background-color:#fff!important;color:#2e2e2e!important}div#onetrust-banner-sdk #onetrust-close-btn-container{display:none}@media only screen and (min-width:556px){div#onetrust-consent-sdk #onetrust-banner-sdk{padding:40px}div#onetrust-banner-sdk #onetrust-policy{margin:0 40px 0 0}div#onetrust-banner-sdk .ot-sdk-row{align-items:center;flex-direction:row}div#onetrust-banner-sdk #onetrust-button-group-parent,div#onetrust-banner-sdk.ot-bnr-flift:not(.ot-iab-2) #onetrust-button-group-parent,div#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-button-group-parent{margin:0;padding:0;width:auto}div#onetrust-banner-sdk #onetrust-button-group,div#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group{align-items:stretch;flex-direction:column-reverse;margin:0}div#onetrust-consent-sdk #onetrust-accept-btn-handler,div#onetrust-consent-sdk #onetrust-pc-btn-handler{flex:1 0 auto}}@media only screen and (min-width:768px){div#onetrust-banner-sdk #onetrust-policy{margin:0 48px 0 0}div#onetrust-consent-sdk #onetrust-banner-sdk{padding:48px}}div#onetrust-consent-sdk #onetrust-pc-sdk h5{font-size:16px;line-height:24px}div#onetrust-consent-sdk #onetrust-pc-sdk p,div#onetrust-pc-sdk #ot-pc-desc,div#onetrust-pc-sdk .category-host-list-handler,div#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{font-size:16px;font-weight:400;line-height:24px}div#onetrust-consent-sdk a:hover,div#onetrust-pc-sdk a:hover{color:#2e2e2e!important;text-decoration-color:#eb6500!important}div#onetrust-pc-sdk{border-radius:0;bottom:0;height:auto;left:0;margin:auto;max-width:100%;overflow:hidden;right:0;top:0;width:512px;max-height:800px}div#onetrust-pc-sdk .ot-pc-header{display:none}div#onetrust-pc-sdk #ot-pc-content{overscroll-behavior:contain;padding:0 12px 0 24px;margin:16px 4px 0 0;top:0;right:16px;left:0;width:auto}div#onetrust-pc-sdk #ot-category-title,div#onetrust-pc-sdk #ot-pc-title{font-size:24px;font-weight:400;line-height:32px;margin:0 0 16px}div#onetrust-pc-sdk #ot-pc-desc{padding:0}div#onetrust-pc-sdk #ot-pc-desc a{display:inline}div#onetrust-pc-sdk #accept-recommended-btn-handler{display:none!important}div#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:2px solid #eb6500;outline-offset:-1px;transition:none}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item{border-width:0 0 2px}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item:first-of-type{border-width:2px 0}div#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{padding:8px 0;width:100%}div#onetrust-pc-sdk .ot-plus-minus{transform:translateY(2px)}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item>button{background:0 0!important;border:0!important;height:44px!important;max-width:none!important;width:calc(100% - 48px)!important}div#onetrust-consent-sdk #onetrust-pc-sdk h5{font-weight:700}div#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr{padding:0}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item .ot-acc-grpdesc{padding:0;width:100%}div#onetrust-pc-sdk .ot-acc-grpcntr .ot-subgrp-cntr{border:0;padding:0}div#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp{margin:0}div#onetrust-pc-sdk .ot-always-active-group .ot-cat-header{width:calc(100% - 160px)}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{width:calc(100% - 88px)}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active{color:#2e2e2e;font-size:12px;font-weight:400;line-height:1.5;padding-right:48px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active:before{border-radius:12px;position:absolute;right:0;top:0;content:'';background:#fff;border:2px solid #939393;box-sizing:border-box;height:20px;width:40px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active:after{border-radius:50%;position:absolute;right:5px;top:4px;content:'';background-color:#eb6500;height:12px;width:12px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active,div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl{right:2px}div#onetrust-pc-sdk .ot-switch{display:block;height:20px;width:40px}div#onetrust-pc-sdk .ot-tgl input+.ot-switch .ot-switch-nob,div#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob{background:#fff;border:2px solid #939393;box-sizing:border-box;height:20px;width:40px}div#onetrust-pc-sdk .ot-tgl input+.ot-switch .ot-switch-nob:before{background-color:#737373;height:8px;left:4px;top:4px;width:8px}div#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob:before{background-color:#eb6500;height:12px;left:0;top:2px;width:12px}div#onetrust-pc-sdk .ot-tgl input:focus+.ot-switch .ot-switch-nob{box-shadow:0 0;outline:2px solid #eb6500!important;outline-offset:1px;transition:none}div#onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-grpcntr.ot-acc-txt{background-color:transparent;padding-left:3px}div#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr,div#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr{overflow:visible;width:100%}div#onetrust-pc-sdk .ot-pc-footer{border-top:0 solid}div#onetrust-pc-sdk .ot-btn-container{padding-top:24px;text-align:center}div#onetrust-pc-sdk .ot-pc-footer button{margin:8px 0;background-color:#fff}div#onetrust-pc-sdk .ot-pc-footer-logo{background-color:#fff}div#onetrust-pc-sdk #ot-lst-title span{font-size:24px;font-weight:400;line-height:32px}div#onetrust-pc-sdk #ot-host-lst .ot-host-desc,div#onetrust-pc-sdk #ot-host-lst .ot-host-expand,div#onetrust-pc-sdk #ot-host-lst .ot-host-name,div#onetrust-pc-sdk #ot-host-lst .ot-host-name a,div#onetrust-pc-sdk .back-btn-handler,div#onetrust-pc-sdk .ot-host-opt li>div div{font-size:16px;font-weight:400;line-height:24px}div#onetrust-pc-sdk #ot-host-lst .ot-acc-txt{width:100%}div#onetrust-pc-sdk #ot-pc-lst{top:0}div#onetrust-pc-sdk .back-btn-handler{text-decoration:none!important}div#onetrust-pc-sdk #filter-btn-handler:hover svg{filter:invert(1)}div#onetrust-pc-sdk .back-btn-handler svg{width:16px;height:16px}div#onetrust-pc-sdk .ot-host-item>button{background:0 0!important;border:0!important;height:66px!important;max-width:none!important;width:calc(100% - 5px)!important;transform:translate(2px,2px)}div#onetrust-pc-sdk .ot-host-item{border-bottom:2px solid #b9b9b9;padding:0}div#onetrust-pc-sdk .ot-host-item .ot-acc-hdr{margin:0 0 -6px;padding:8px 0}div#onetrust-pc-sdk ul li:first-child{border-top:2px solid #b9b9b9}div#onetrust-pc-sdk .ot-host-item .ot-plus-minus{margin:0 8px 0 0}div#onetrust-pc-sdk .ot-search-cntr{width:calc(100% - 48px)}div#onetrust-pc-sdk .ot-host-opt .ot-host-info{background-color:transparent}div#onetrust-pc-sdk .ot-host-opt li>div div{padding:0}div#onetrust-pc-sdk #vendor-search-handler{border-radius:0;border-color:#939393;border-style:solid;border-width:2px 0 2px 2px;font-size:20px;height:48px;margin:0}div#onetrust-pc-sdk #ot-pc-hdr{margin-left:24px}div#onetrust-pc-sdk .ot-lst-subhdr{width:calc(100% - 24px)}div#onetrust-pc-sdk .ot-lst-subhdr svg{right:0;top:8px}div#onetrust-pc-sdk .ot-fltr-cntr{box-sizing:border-box;right:0;width:48px}div#onetrust-pc-sdk #filter-btn-handler{width:48px!important;padding:8px!important}div#onetrust-consent-sdk #onetrust-pc-sdk #clear-filters-handler,div#onetrust-pc-sdk button#filter-apply-handler,div#onetrust-pc-sdk button#filter-cancel-handler{height:2em!important;padding-left:14px!important;padding-right:14px!important}div#onetrust-pc-sdk #ot-fltr-cnt{box-shadow:0 0;border:1px solid #8e8e8e;border-radius:0}div#onetrust-pc-sdk .ot-fltr-scrlcnt{max-height:calc(100% - 80px)}div#onetrust-pc-sdk #ot-fltr-modal{max-height:400px}div#onetrust-pc-sdk .ot-fltr-opt{margin-bottom:16px}div#onetrust-pc-sdk #ot-lst-cnt{margin-left:24px;width:calc(100% - 48px)}div#onetrust-pc-sdk #ot-anchor{display:none!important}

/* 2023-12-04  Fix for button order in mobile view*/
@media (max-width: 550px) {
  #onetrust-accept-btn-handler {order: 1;  }
  #onetrust-reject-all-handler { order: 2;  }
  #onetrust-pc-btn-handler { order: 3;  }
}


/*! Extra code to blur our background */
.onetrust-pc-dark-filter{
backdrop-filter: blur(3px)
}
.ot-sdk-cookie-policy{font-family:inherit;font-size:16px}.ot-sdk-cookie-policy.otRelFont{font-size:1rem}.ot-sdk-cookie-policy h3,.ot-sdk-cookie-policy h4,.ot-sdk-cookie-policy h6,.ot-sdk-cookie-policy p,.ot-sdk-cookie-policy li,.ot-sdk-cookie-policy a,.ot-sdk-cookie-policy th,.ot-sdk-cookie-policy #cookie-policy-description,.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group,.ot-sdk-cookie-policy #cookie-policy-title{color:dimgray}.ot-sdk-cookie-policy #cookie-policy-description{margin-bottom:1em}.ot-sdk-cookie-policy h4{font-size:1.2em}.ot-sdk-cookie-policy h6{font-size:1em;margin-top:2em}.ot-sdk-cookie-policy th{min-width:75px}.ot-sdk-cookie-policy a,.ot-sdk-cookie-policy a:hover{background:#fff}.ot-sdk-cookie-policy thead{background-color:#f6f6f4;font-weight:bold}.ot-sdk-cookie-policy .ot-mobile-border{display:none}.ot-sdk-cookie-policy section{margin-bottom:2em}.ot-sdk-cookie-policy table{border-collapse:inherit}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy{font-family:inherit;font-size:1rem}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h3,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h4,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h6,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy p,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy li,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-title{color:dimgray}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description{margin-bottom:1em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-subgroup{margin-left:1.5em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group-desc,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-table-header,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy span,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td{font-size:.9em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td span,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td a{font-size:inherit}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group{font-size:1em;margin-bottom:.6em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-title{margin-bottom:1.2em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy>section{margin-bottom:1em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th{min-width:75px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a:hover{background:#fff}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy thead{background-color:#f6f6f4;font-weight:bold}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-mobile-border{display:none}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy section{margin-bottom:2em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-subgroup ul li{list-style:disc;margin-left:1.5em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-subgroup ul li h4{display:inline-block}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table{border-collapse:inherit;margin:auto;border:1px solid #d7d7d7;border-radius:5px;border-spacing:initial;width:100%;overflow:hidden}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table th,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table td{border-bottom:1px solid #d7d7d7;border-right:1px solid #d7d7d7}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr:last-child td{border-bottom:0px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr th:last-child,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr td:last-child{border-right:0px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-host,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-cookies-type{width:25%}.ot-sdk-cookie-policy[dir=rtl]{text-align:left}#ot-sdk-cookie-policy h3{font-size:1.5em}@media only screen and (max-width: 530px){.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) table,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) thead,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tbody,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) th,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr{display:block}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) thead tr{position:absolute;top:-9999px;left:-9999px}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr{margin:0 0 1em 0}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr:nth-child(odd),.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr:nth-child(odd) a{background:#f6f6f4}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td{border:none;border-bottom:1px solid #eee;position:relative;padding-left:50%}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td:before{position:absolute;height:100%;left:6px;width:40%;padding-right:10px}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) .ot-mobile-border{display:inline-block;background-color:#e4e4e4;position:absolute;height:100%;top:0;left:45%;width:2px}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td:before{content:attr(data-label);font-weight:bold}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) li{word-break:break-word;word-wrap:break-word}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table{overflow:hidden}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table td{border:none;border-bottom:1px solid #d7d7d7}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy thead,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy tbody,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy tr{display:block}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-host,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-cookies-type{width:auto}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy tr{margin:0 0 1em 0}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td:before{height:100%;width:40%;padding-right:10px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td:before{content:attr(data-label);font-weight:bold}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy li{word-break:break-word;word-wrap:break-word}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy thead tr{position:absolute;top:-9999px;left:-9999px;z-index:-9999}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr:last-child td{border-bottom:1px solid #d7d7d7;border-right:0px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr:last-child td:last-child{border-bottom:0px}}
                
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h5,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h6,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy li,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy p,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy span,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description {
                        color: #696969;
                    }
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th {
                        color: #696969;
                    }
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group {
                        color: #696969;
                    }
                    
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-title {
                            color: #696969;
                        }
                    
            
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table th {
                            background-color: #F8F8F8;
                        }
                    
            .ot-floating-button__front{background-image:url('https://cdn.cookielaw.org/logos/static/ot_persistent_cookie_icon.png')}</style></head>
    <body data-sd-ui-layer-boundary="true"><div id="MathJax_Message" style="display: none;"></div>
      <script type="text/javascript">
        window.__PRELOADED_STATE__ = {"abstracts":{"content":[{"$$":[{"$":{"id":"st0005"},"#name":"section-title","_":"Abstract"},{"$$":[{"$$":[{"#name":"__text__","_":"In the current study, electroencephalography (EEG) was recorded simultaneously with facial electromyography (fEMG) to determine whether emotional faces and emotional scenes are processed differently at the neural level. In addition, it was investigated whether these differences can be observed at the behavioural level via spontaneous facial muscle activity. Emotional content of the stimuli did not affect early P1 activity. Emotional faces elicited enhanced amplitudes of the face-sensitive N170 component, while its counterpart, the scene-related N100, was not sensitive to emotional content of scenes. At 220–280"},{"$":{"sp":"0.25"},"#name":"hsp"},{"#name":"__text__","_":"ms, the early posterior negativity (EPN) was enhanced only slightly for fearful as compared to neutral or happy faces. However, its amplitudes were significantly enhanced during processing of scenes with positive content, particularly over the right hemisphere. Scenes of positive content also elicited enhanced spontaneous zygomatic activity from 500–750"},{"$":{"sp":"0.25"},"#name":"hsp"},{"#name":"__text__","_":"ms onwards, while happy faces elicited no such changes. Contrastingly, both fearful faces and negative scenes elicited enhanced spontaneous corrugator activity at 500–750"},{"$":{"sp":"0.25"},"#name":"hsp"},{"#name":"__text__","_":"ms after stimulus onset. However, relative to baseline EMG changes occurred earlier for faces (250"},{"$":{"sp":"0.25"},"#name":"hsp"},{"#name":"__text__","_":"ms) than for scenes (500"},{"$":{"sp":"0.25"},"#name":"hsp"},{"#name":"__text__","_":"ms) whereas for scenes activity changes were more pronounced over the whole viewing period. Taking into account all effects, the data suggests that emotional facial expressions evoke faster attentional orienting, but weaker affective neural activity and emotional behavioural responses compared to emotional scenes."}],"$":{"view":"all","id":"sp0005"},"#name":"simple-para"}],"$":{"view":"all","id":"as0005"},"#name":"abstract-sec"}],"$":{"view":"all","id":"ab0005","class":"author"},"#name":"abstract"},{"$$":[{"$":{"id":"st0175"},"#name":"section-title","_":"Highlights"},{"$$":[{"$$":[{"$$":[{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"p0335"},"#name":"para","_":"Faces and scenes elicit different emotion-related brain activities."}],"$":{"id":"u0005"},"#name":"list-item"},{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"p0340"},"#name":"para","_":"Faces and scenes elicit different facial expression-related muscle activities."}],"$":{"id":"u0010"},"#name":"list-item"},{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"p0345"},"#name":"para","_":"Faces and scenes are shown to be different emotion elicitors."}],"$":{"id":"u0015"},"#name":"list-item"}],"$":{"id":"l0005"},"#name":"list"}],"$":{"view":"all","id":"sp0065"},"#name":"simple-para"}],"$":{"view":"all","id":"as0010"},"#name":"abstract-sec"}],"$":{"view":"all","id":"ab0010","class":"author-highlights"},"#name":"abstract"}],"floats":[],"footnotes":[],"attachments":[]},"accessbarConfig":{"fallback":false,"id":"accessbar","version":"0.0.1","analytics":{"location":"accessbar","eventName":"ctaImpression"},"label":{},"ariaLabel":{"accessbar":"Download options and search","componentsList":"PDF Options"},"banners":[{"id":"BannerSsrn"}],"components":[{"target":"_blank","analytics":[{"ids":["accessbar:fta:single-article"],"eventName":"ctaClick"}],"label":"View&nbsp;**PDF**","ariaLabel":"View PDF. Opens in a new window.","id":"ViewPDF"},{"analytics":[{"ids":["accessbar:fta:full-issue"],"eventName":"ctaClick"}],"label":"Download full issue","id":"DownloadFullIssue"}],"search":{"inputPlaceHolder":"Search ScienceDirect","ariaLabel":{"input":"Search ScienceDirect","submit":"Submit search"},"formAction":"/search#submit","analytics":[{"ids":["accessbar:search"],"eventName":"searchStart"}],"id":"QuickSearch"}},"adobeTarget":{"sd:genai-question-and-answer":{}},"article":{"analyticsMetadata":{"accountId":"228598","accountName":"ScienceDirect Guests","loginStatus":"anonymous","userId":"12975512","isLoggedIn":false},"cid":"272508","content-family":"serial","copyright-line":"Copyright © 2015 The Authors. Published by Elsevier Inc.","cover-date-years":["2016"],"cover-date-start":"2016-01-01","cover-date-text":"1 January 2016","document-subtype":"fla","document-type":"article","entitledToken":"FBF1A904563A7B65ED0C8DB8BF1D8DE3449844506737FEB19CFB9AA61E7C920C5265F8045AD61422","genAiToken":"eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJhdWQiOiJnZW5BaUFwcHMiLCJwaWkiOiJTMTA1MzgxMTkxNTAwODg3MyIsInN1YiI6IjIyODU5OCIsImlzcyI6ImFycCIsInNlc3Npb25JZCI6ImZkZjljODY0MmI4MGEwNDNmOTk5NmUwNjRmNjdmYmUxYjY2NWd4cnFiIiwiZXhwIjoxNzM0ODkzNDQ5LCJpYXQiOjE3MzQ4OTE2NDksInZlcnNpb24iOjEsImp0aSI6IjZjYzdhNDQ3LWZkNmQtNDM5NC05MmNhLTljMTE4YzdhNjIzNCJ9.-jQgJW3IaPVMvBqmb_jcGVm1tnNRP_6G9lpV5BIJTZk","eid":"1-s2.0-S1053811915008873","doi":"10.1016/j.neuroimage.2015.09.065","first-fp":"931","hub-eid":"1-s2.0-S1053811915X00176","issuePii":"S1053811915X00176","item-weight":"FULL-TEXT","language":"en","last-lp":"946","last-author":{"#name":"last-author","$":{"xmlns:ce":true,"xmlns:dm":true,"xmlns:sb":true},"$$":[{"#name":"author","$":{"id":"au0015"},"$$":[{"#name":"given-name","_":"Peter"},{"#name":"surname","_":"Walla"},{"#name":"cross-ref","$":{"refid":"af0005","id":"cf0025"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"cross-ref","$":{"refid":"af0010","id":"cf0030"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]},{"#name":"cross-ref","$":{"refid":"af0025","id":"cf0035"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"e"}]},{"#name":"cross-ref","$":{"refid":"af0030","id":"cf0040"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"f"}]},{"#name":"cross-ref","$":{"refid":"cr0005","id":"cf0045"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"⁎"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMmlkJTIyJTNBJTIyZW0wMDA1JTIyJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTdEJTJDJTIyXyUyMiUzQSUyMlBldGVyLldhbGxhJTQwbmV3Y2FzdGxlLmVkdS5hdSUyMiU3RA=="}]}]},"normalized-first-auth-initial":"A","normalized-first-auth-surname":"MAVRATZAKIS","pages":[{"last-page":"946","first-page":"931"}],"pii":"S1053811915008873","self-archiving":{"#name":"self-archiving","$":{"xmlns:xocs":true},"$$":[{"#name":"sa-start-date","_":"2016-11-11T00:00:00Z"},{"#name":"sa-embargo-status","_":"UnderEmbargo"},{"#name":"sa-user-license","_":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}]},"srctitle":"NeuroImage","suppl":"PA","timestamp":"2017-09-21T23:36:53.254618-04:00","title":{"content":[{"#name":"title","$":{"id":"ti0005"},"_":"Emotional facial expressions evoke faster orienting responses, but weaker emotional responses at neural and behavioural levels compared to scenes: A simultaneous EEG and facial EMG study"}],"floats":[],"footnotes":[],"attachments":[]},"vol-first":"124","vol-iss-suppl-text":"Volume 124, Part A","userSettings":{"forceAbstract":false,"creditCardPurchaseAllowed":true,"blockFullTextForAnonymousAccess":false,"disableWholeIssueDownload":false,"preventTransactionalAccess":false,"preventDocumentDelivery":true},"contentType":"JL","crossmark":true,"document-references":118,"freeHtmlGiven":false,"userProfile":{"departmentName":"ScienceDirect Guests","accessType":"GUEST","accountId":"228598","webUserId":"12975512","accountName":"ScienceDirect Guests","departmentId":"291352","userType":"NORMAL","hasMultipleOrganizations":false,"accountNumber":"C000228598"},"access":{"openAccess":true,"openArchive":false,"license":"http://creativecommons.org/licenses/by/4.0/"},"aipType":"none","articleEntitlement":{"entitled":true,"isCasaUser":false,"usageInfo":"(12975512,U|291352,D|228598,A|3,P|2,PL)(SDFE,CON|fdf9c8642b80a043f9996e064f67fbe1b665gxrqb,SSO|ANON_GUEST,ACCESS_TYPE)","entitledByAccount":false},"crawlerInformation":{"canCrawlPDFContent":false,"isCrawler":false},"dates":{"Available online":"8 October 2015","Received":"16 January 2015","Revised":[],"Accepted":"29 September 2015","Publication date":"1 January 2016","Version of Record":"11 November 2015"},"downloadFullIssue":true,"entitlementReason":"openaccess","features":["keywords","references","preview"],"hasBody":true,"has-large-authors":false,"hasScholarlyAbstract":true,"headerConfig":{"contactUrl":"https://service.elsevier.com/app/contact/supporthub/sciencedirect/","userName":"","userEmail":"","orgName":"ScienceDirect Guests","webUserId":"12975512","libraryBanner":null,"shib_regUrl":"","tick_regUrl":"","recentInstitutions":[],"canActivatePersonalization":false,"hasInstitutionalAssociation":false,"hasMultiOrg":false,"userType":"GUEST","userAnonymity":"ANON_GUEST","allowCart":true,"environment":"prod","cdnAssetsHost":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com"},"isCorpReq":false,"isPdfFullText":false,"issn":"10538119","issn-primary-formatted":"1053-8119","issRange":"","isThirdParty":false,"pageCount":16,"pdfDownload":{"isPdfFullText":false,"urlMetadata":{"queryParams":{"md5":"73fd19e992f38a364d9834d34199d6b3","pid":"1-s2.0-S1053811915008873-main.pdf"},"pii":"S1053811915008873","pdfExtension":"/pdfft","path":"science/article/pii"}},"publication-content":{"noElsevierLogo":false,"imprintPublisher":{"displayName":"Academic Press","id":"350"},"isSpecialIssue":false,"isSampleIssue":false,"transactionsBlocked":false,"publicationOpenAccess":{"oaStatus":"Partial","oaArticleCount":5492,"openArchiveStatus":false,"openArchiveArticleCount":87,"openAccessStartDate":"2020-01-01T00:00:00Z","oaAllowsAuthorPaid":true},"issue-cover":{"attachment":[{"attachment-eid":"1-s2.0-S1053811915X00176-cov200h.gif","file-basename":"cov200h","extension":"gif","filename":"cov200h.gif","ucs-locator":["https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1053811915X00176/cover/DOWNSAMPLED200/image/gif/44f393cfadaa64e6c1a363760fac35b3/cov200h.gif","https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1053811915X00176/cover/DOWNSAMPLED200/image/gif/44f393cfadaa64e6c1a363760fac35b3/cov200h.gif"],"attachment-type":"IMAGE-COVER-H200","filesize":"19346","pixel-height":"200","pixel-width":"150"},{"attachment-eid":"1-s2.0-S1053811915X00176-cov150h.gif","file-basename":"cov150h","extension":"gif","filename":"cov150h.gif","ucs-locator":["https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1053811915X00176/cover/DOWNSAMPLED/image/gif/20e2ff587704088077dddbe6e7bbef54/cov150h.gif","https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S1053811915X00176/cover/DOWNSAMPLED/image/gif/20e2ff587704088077dddbe6e7bbef54/cov150h.gif"],"attachment-type":"IMAGE-COVER-H150","filesize":"15425","pixel-height":"150","pixel-width":"113"}]},"smallCoverUrl":"https://ars.els-cdn.com/content/image/S10538119.gif","title":"neuroimage","contentTypeCode":"JL","images":{"coverImage":"https://ars.els-cdn.com/content/image/1-s2.0-S1053811915X00176-cov150h.gif","logo":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/b64013ec63c69e3d916174cbebae89d65b2419e1/image/elsevier-non-solus.png","logoAltText":"Elsevier"},"publicationCoverImageUrl":"https://ars.els-cdn.com/content/image/1-s2.0-S1053811915X00176-cov150h.gif"},"volRange":"124","open-research":{},"titleString":"Emotional facial expressions evoke faster orienting responses, but weaker emotional responses at neural and behavioural levels compared to scenes: A simultaneous EEG and facial EMG study","ssrn":{},"renderingMode":"Article","isAbstract":false,"isContentVisible":false,"ajaxLinks":{"citingArticles":true,"referenceLinks":true,"references":true,"referredToBy":true,"toc":true,"body":true,"recommendations":true,"authorMetadata":true,"substances":true},"pdfEmbed":false,"displayViewFullText":false},"authors":{"content":[{"#name":"author-group","$":{"id":"ag0005"},"$$":[{"#name":"author","$":{"id":"au0005"},"$$":[{"#name":"given-name","_":"Aimee"},{"#name":"surname","_":"Mavratzakis"},{"#name":"cross-ref","$":{"refid":"af0005","id":"cf0005"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"cross-ref","$":{"refid":"af0010","id":"cf0010"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]}]},{"#name":"author","$":{"id":"au0010"},"$$":[{"#name":"given-name","_":"Cornelia"},{"#name":"surname","_":"Herbert"},{"#name":"cross-ref","$":{"refid":"af0015","id":"cf0015"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"c"}]},{"#name":"cross-ref","$":{"refid":"af0020","id":"cf0020"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"d"}]}]},{"#name":"author","$":{"id":"au0015"},"$$":[{"#name":"given-name","_":"Peter"},{"#name":"surname","_":"Walla"},{"#name":"cross-ref","$":{"refid":"af0005","id":"cf0025"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"cross-ref","$":{"refid":"af0010","id":"cf0030"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]},{"#name":"cross-ref","$":{"refid":"af0025","id":"cf0035"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"e"}]},{"#name":"cross-ref","$":{"refid":"af0030","id":"cf0040"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"f"}]},{"#name":"cross-ref","$":{"refid":"cr0005","id":"cf0045"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"⁎"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMmlkJTIyJTNBJTIyZW0wMDA1JTIyJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTdEJTJDJTIyXyUyMiUzQSUyMlBldGVyLldhbGxhJTQwbmV3Y2FzdGxlLmVkdS5hdSUyMiU3RA=="}]},{"#name":"affiliation","$":{"id":"af0005"},"$$":[{"#name":"label","_":"a"},{"#name":"textfn","$":{"id":"tn0005"},"_":"School of Psychology, University of Newcastle, NSW, Australia"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"School of Psychology"},{"#name":"organization","_":"University of Newcastle"},{"#name":"state","_":"NSW"},{"#name":"country","_":"Australia"}]}]},{"#name":"affiliation","$":{"id":"af0010"},"$$":[{"#name":"label","_":"b"},{"#name":"textfn","$":{"id":"tn0010"},"_":"Centre for Translational Neuroscience and Mental Health, University of Newcastle, NSW Australia"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Centre for Translational Neuroscience and Mental Health"},{"#name":"organization","_":"University of Newcastle"},{"#name":"state","_":"NSW"},{"#name":"country","_":"Australia"}]}]},{"#name":"affiliation","$":{"id":"af0015"},"$$":[{"#name":"label","_":"c"},{"#name":"textfn","$":{"id":"tn0015"},"_":"University Clinic for Psychiatry and Psychotherapy, University of Tübingen, Germany"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"University Clinic for Psychiatry and Psychotherapy"},{"#name":"organization","_":"University of Tübingen"},{"#name":"country","_":"Germany"}]}]},{"#name":"affiliation","$":{"id":"af0020"},"$$":[{"#name":"label","_":"d"},{"#name":"textfn","$":{"id":"tn0020"},"_":"Department of Psychology, University of Würzburg, Germany"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Psychology"},{"#name":"organization","_":"University of Würzburg"},{"#name":"country","_":"Germany"}]}]},{"#name":"affiliation","$":{"id":"af0025"},"$$":[{"#name":"label","_":"e"},{"#name":"textfn","$":{"id":"tn0030"},"_":"CanBeLab, Department of Psychology, Webster Vienna Private University, Palais Wenkheim, Vienna, Austria"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"CanBeLab"},{"#name":"organization","_":"Department of Psychology"},{"#name":"organization","_":"Webster Vienna Private University, Palais Wenkheim"},{"#name":"city","_":"Vienna"},{"#name":"country","_":"Austria"}]}]},{"#name":"affiliation","$":{"id":"af0030"},"$$":[{"#name":"label","_":"f"},{"#name":"textfn","$":{"id":"tn0035"},"_":"Faculty of Psychology, University of Vienna, Vienna, Austria"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Faculty of Psychology"},{"#name":"organization","_":"University of Vienna"},{"#name":"city","_":"Vienna"},{"#name":"country","_":"Austria"}]}]},{"#name":"correspondence","$":{"id":"cr0005"},"$$":[{"#name":"label","_":"⁎"},{"#name":"text","$":{"id":"tx0005"},"_":"Corresponding author at: School of Psychology, University of Newcastle, Behavioural Sciences Building, University Drive, Callaghan, NSW 2308, Australia."},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"School of Psychology"},{"#name":"organization","_":"University of Newcastle"},{"#name":"address-line","_":"Behavioural Sciences Building"},{"#name":"address-line","_":"University Drive"},{"#name":"city","_":"Callaghan"},{"#name":"state","_":"NSW"},{"#name":"postal-code","_":"2308"},{"#name":"country","_":"Australia"}]}]}]}],"floats":[],"footnotes":[],"affiliations":{"af0005":{"#name":"affiliation","$":{"id":"af0005"},"$$":[{"#name":"label","_":"a"},{"#name":"textfn","$":{"id":"tn0005"},"_":"School of Psychology, University of Newcastle, NSW, Australia"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"School of Psychology"},{"#name":"organization","_":"University of Newcastle"},{"#name":"state","_":"NSW"},{"#name":"country","_":"Australia"}]}]},"af0010":{"#name":"affiliation","$":{"id":"af0010"},"$$":[{"#name":"label","_":"b"},{"#name":"textfn","$":{"id":"tn0010"},"_":"Centre for Translational Neuroscience and Mental Health, University of Newcastle, NSW Australia"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Centre for Translational Neuroscience and Mental Health"},{"#name":"organization","_":"University of Newcastle"},{"#name":"state","_":"NSW"},{"#name":"country","_":"Australia"}]}]},"af0015":{"#name":"affiliation","$":{"id":"af0015"},"$$":[{"#name":"label","_":"c"},{"#name":"textfn","$":{"id":"tn0015"},"_":"University Clinic for Psychiatry and Psychotherapy, University of Tübingen, Germany"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"University Clinic for Psychiatry and Psychotherapy"},{"#name":"organization","_":"University of Tübingen"},{"#name":"country","_":"Germany"}]}]},"af0020":{"#name":"affiliation","$":{"id":"af0020"},"$$":[{"#name":"label","_":"d"},{"#name":"textfn","$":{"id":"tn0020"},"_":"Department of Psychology, University of Würzburg, Germany"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Psychology"},{"#name":"organization","_":"University of Würzburg"},{"#name":"country","_":"Germany"}]}]},"af0025":{"#name":"affiliation","$":{"id":"af0025"},"$$":[{"#name":"label","_":"e"},{"#name":"textfn","$":{"id":"tn0030"},"_":"CanBeLab, Department of Psychology, Webster Vienna Private University, Palais Wenkheim, Vienna, Austria"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"CanBeLab"},{"#name":"organization","_":"Department of Psychology"},{"#name":"organization","_":"Webster Vienna Private University, Palais Wenkheim"},{"#name":"city","_":"Vienna"},{"#name":"country","_":"Austria"}]}]},"af0030":{"#name":"affiliation","$":{"id":"af0030"},"$$":[{"#name":"label","_":"f"},{"#name":"textfn","$":{"id":"tn0035"},"_":"Faculty of Psychology, University of Vienna, Vienna, Austria"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Faculty of Psychology"},{"#name":"organization","_":"University of Vienna"},{"#name":"city","_":"Vienna"},{"#name":"country","_":"Austria"}]}]}},"correspondences":{"cr0005":{"#name":"correspondence","$":{"id":"cr0005"},"$$":[{"#name":"label","_":"⁎"},{"#name":"text","$":{"id":"tx0005"},"_":"Corresponding author at: School of Psychology, University of Newcastle, Behavioural Sciences Building, University Drive, Callaghan, NSW 2308, Australia."},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"School of Psychology"},{"#name":"organization","_":"University of Newcastle"},{"#name":"address-line","_":"Behavioural Sciences Building"},{"#name":"address-line","_":"University Drive"},{"#name":"city","_":"Callaghan"},{"#name":"state","_":"NSW"},{"#name":"postal-code","_":"2308"},{"#name":"country","_":"Australia"}]}]}},"attachments":[],"scopusAuthorIds":{},"articles":{}},"authorMetadata":[],"banner":{"expanded":false},"biographies":{},"body":{},"chapters":{"toc":[],"isLoading":false},"changeViewLinks":{"showFullTextLink":false,"showAbstractLink":false},"citingArticles":{},"combinedContentItems":{"content":[{"#name":"keywords","$$":[{"#name":"keywords","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:xoe":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"ks0005","view":"all","class":"keyword"},"$$":[{"#name":"section-title","$":{"id":"st0010"},"_":"Keywords"},{"#name":"keyword","$":{"id":"kw0045"},"$$":[{"#name":"text","$":{"id":"tx0010"},"_":"Emotion"}]},{"#name":"keyword","$":{"id":"kw0050"},"$$":[{"#name":"text","$":{"id":"tx0015"},"_":"Affective processing"}]},{"#name":"keyword","$":{"id":"kw0055"},"$$":[{"#name":"text","$":{"id":"tx0020"},"_":"Faces and scenes"}]},{"#name":"keyword","$":{"id":"kw0060"},"$$":[{"#name":"text","$":{"id":"tx0025"},"_":"Electroencephalography"}]},{"#name":"keyword","$":{"id":"kw0065"},"$$":[{"#name":"text","$":{"id":"tx0030"},"_":"Spontaneous facial EMG"}]},{"#name":"keyword","$":{"id":"kw0070"},"$$":[{"#name":"text","$":{"id":"tx0035"},"_":"N170"}]},{"#name":"keyword","$":{"id":"kw0075"},"$$":[{"#name":"text","$":{"id":"tx0040"},"_":"N100"}]},{"#name":"keyword","$":{"id":"kw0080"},"$$":[{"#name":"text","$":{"id":"tx0045"},"_":"Early posterior negativity"}]}]}]}],"floats":[],"footnotes":[],"attachments":[]},"crossMark":{"isOpen":false},"domainConfig":{"cdnAssetsHost":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com","assetRoute":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/b64013ec63c69e3d916174cbebae89d65b2419e1"},"downloadIssue":{"openOnPageLoad":false,"isOpen":false,"downloadCapOpen":false,"articles":[],"selected":[]},"enrichedContent":{"tableOfContents":false,"researchData":{"hasResearchData":false,"dataProfile":{},"openData":{},"mendeleyData":{},"databaseLinking":{}},"geospatialData":{"attachments":[]},"interactiveCaseInsights":{},"virtualMicroscope":{}},"entitledRecommendations":{"openOnPageLoad":false,"isOpen":false,"articles":[],"selected":[],"currentPage":1,"totalPages":1},"exam":{},"helpText":{"keyDates":{"html":"<div class=\"key-dates-help\"><h3 class=\"u-margin-s-bottom u-h4\">Publication milestones</h3><p class=\"u-margin-m-bottom\">The dates displayed for an article provide information on when various publication milestones were reached at the journal that has published the article. Where applicable, activities on preceding journals at which the article was previously under consideration are not shown (for instance submission, revisions, rejection).</p><p class=\"u-margin-xs-bottom\">The publication milestones include:</p><ul class=\"key-dates-help-list u-margin-m-bottom u-padding-s-left\"><li><span class=\"u-text-italic\">Received</span>: The date the article was originally submitted to the journal.</li><li><span class=\"u-text-italic\">Revised</span>: The date the most recent revision of the article was submitted to the journal. Dates corresponding to intermediate revisions are not shown.</li><li><span class=\"u-text-italic\">Accepted</span>: The date the article was accepted for publication in the journal.</li><li><span class=\"u-text-italic\">Available online</span>: The date a version of the article was made available online in the journal.</li><li><span class=\"u-text-italic\">Version of Record</span>: The date the finalized version of the article was made available in the journal.</li></ul><p>More information on publishing policies can be found on the <a class=\"anchor anchor-secondary u-display-inline anchor-underline\" href=\"https://www.elsevier.com/about/policies-and-standards/publishing-ethics\" target=\"_blank\"><span class=\"anchor-text-container\"><span class=\"anchor-text\">Publishing Ethics Policies</span></span></a> page. View our <a class=\"anchor anchor-secondary u-display-inline anchor-underline\" href=\"https://www.elsevier.com/researcher/author/submit-your-paper\" target=\"_blank\"><span class=\"anchor-text-container\"><span class=\"anchor-text\">Publishing with Elsevier: step-by-step</span></span></a> page to learn more about the publishing process. For any questions on your own submission or other questions related to publishing an article, <a class=\"anchor anchor-secondary u-display-inline anchor-underline\" href=\"https://service.elsevier.com/app/phone/supporthub/publishing\" target=\"_blank\"><span class=\"anchor-text-container\"><span class=\"anchor-text\">contact our Researcher support team.</span></span></a></p></div>","title":"What do these dates mean?"}},"glossary":{},"issueNavigation":{"previous":{},"next":{}},"linkingHubLinks":{},"metrics":{"metricGroup":{"citations":[],"captures":[],"mentions":[],"socialMedia":[]},"isLoading":false,"error":false},"preview":{},"rawtext":"","recommendations":{},"references":{},"referenceLinks":{"internal":[],"internalLoaded":false,"external":[]},"refersTo":{},"referredToBy":{},"relatedContent":{"isModal":false,"isOpenSpecialIssueArticles":false,"isOpenVirtualSpecialIssueLink":false,"isOpenRecommendations":true,"isOpenSubstances":true,"citingArticles":[false,false,false,false,false,false],"recommendations":[false,false,false,false,false,false]},"seamlessAccess":{},"specialIssueArticles":{},"substances":{},"supplementaryFilesData":[],"tableOfContents":{"showEntitledTocLinks":true},"tail":{},"transientError":{"isOpen":false},"sidePanel":{"openState":1},"viewConfig":{"articleFeature":{"rightsAndContentLink":true,"sdAnswersButton":false},"pathPrefix":""},"virtualSpecialIssue":{"showVirtualSpecialIssueLink":false},"usageProps":{"itemStage":"S300","isAip":false,"tombAip":"0","sample":false},"userCookiePreferences":{"STRICTLY_NECESSARY":true,"PERFORMANCE":false,"FUNCTIONAL":false,"TARGETING":false}};
      </script>
      <noscript>
      JavaScript is disabled on your browser.
      Please enable JavaScript to use all the features on this page.
      <img src=https://smetrics.elsevier.com/b/ss/elsevier-sd-prod/1/G.4--NS/1734891649078?pageName=sd%3Aproduct%3Ajournal%3Aarticle&c16=els%3Arp%3Ast&c2=sd&v185=img&v33=ae%3AANON_GUEST&c1=ae%3A228598&c12=ae%3A12975512 />
    </noscript>
      <div id="elementForFocusReset" tabindex="-1"></div><a class="anchor sr-only sr-only-focusable u-display-inline anchor-primary" href="#screen-reader-main-content"><span class="anchor-text-container"><span class="anchor-text">Skip to main content</span></span></a><a class="anchor sr-only sr-only-focusable u-display-inline anchor-primary" href="#screen-reader-main-title"><span class="anchor-text-container"><span class="anchor-text">Skip to article</span></span></a>
      <div id="root"><div class="App" id="app" data-aa-name="root"><div class="page"><div class="sd-flex-container"><div class="sd-flex-content"><header id="gh-cnt"><div id="gh-main-cnt" class="u-flex-center-ver u-position-relative u-padding-s-hor u-padding-l-hor-from-xl"><a id="gh-branding" class="u-flex-center-ver" href="/" aria-label="ScienceDirect home page" data-aa-region="header" data-aa-name="ScienceDirect"><img class="gh-logo" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/24/images/elsevier-non-solus-new-grey.svg" alt="Elsevier logo" height="48" width="54"><svg xmlns="http://www.w3.org/2000/svg" version="1.1" height="15" viewBox="0 0 190 23" role="img" class="gh-wordmark u-margin-s-left" aria-labelledby="gh-wm-science-direct" focusable="false" aria-hidden="true" alt="ScienceDirect Wordmark"><title id="gh-wm-science-direct">ScienceDirect</title><g><path fill="#EB6500" d="M3.81 6.9c0-1.48 0.86-3.04 3.7-3.04 1.42 0 3.1 0.43 4.65 1.32l0.13-2.64c-1.42-0.63-2.97-0.96-4.78-0.96 -4.62 0-6.6 2.44-6.6 5.45 0 5.61 8.78 6.14 8.78 9.93 0 1.48-1.15 3.04-3.86 3.04 -1.72 0-3.4-0.56-4.72-1.39l-0.36 2.64c1.55 0.76 3.57 1.06 5.15 1.06 4.26 0 6.7-2.48 6.7-5.51C12.59 11.49 3.81 10.76 3.81 6.9M20.27 9.01c0.23-0.13 0.69-0.26 1.72-0.26 1.72 0 2.41 0.3 2.41 1.58h2.38c0-0.36 0-0.79-0.03-1.09 -0.23-1.98-2.15-2.67-4.88-2.67 -3 0-6.7 2.31-6.7 7.76 0 5.22 2.77 7.99 6.63 7.99 1.68 0 3.47-0.36 4.95-1.39l-0.2-2.31c-0.99 0.82-2.84 1.52-4.06 1.52 -2.14 0-4.55-1.71-4.55-5.91C17.93 10.2 20.01 9.18 20.27 9.01"></path><rect x="29.42" y="6.97" fill="#EB6500" width="2.54" height="14.95"></rect><path fill="#EB6500" d="M30.67 0.7c-0.92 0-1.65 0.92-1.65 1.81 0 0.93 0.76 1.85 1.65 1.85 0.89 0 1.68-0.96 1.68-1.88C32.35 1.55 31.56 0.7 30.67 0.7M48.06 14.13c0-5.18-1.42-7.56-6.01-7.56 -3.86 0-6.67 2.77-6.67 7.92 0 4.92 2.97 7.82 6.73 7.82 2.81 0 4.36-0.63 5.68-1.42l-0.2-2.31c-0.89 0.79-2.94 1.55-4.69 1.55 -3.14 0-4.88-1.95-4.88-5.51v-0.49H48.06M39.91 9.18c0.17-0.17 1.29-0.46 1.98-0.46 2.48 0 3.76 0.53 3.86 3.43h-7.46C38.56 10.27 39.71 9.37 39.91 9.18zM58.82 6.57c-2.24 0-3.63 1.12-4.85 2.61l-0.4-2.21h-2.34l0.13 1.19c0.1 0.76 0.13 1.78 0.13 2.97v10.79h2.54V11.88c0.69-0.96 2.15-2.48 2.48-2.64 0.23-0.13 1.29-0.4 2.08-0.4 2.28 0 2.48 1.15 2.54 3.43 0.03 1.19 0.03 3.17 0.03 3.17 0.03 3-0.1 6.47-0.1 6.47h2.54c0 0 0.07-4.49 0.07-6.96 0-1.48 0.03-2.97-0.1-4.46C63.31 7.43 61.49 6.57 58.82 6.57M72.12 9.01c0.23-0.13 0.69-0.26 1.72-0.26 1.72 0 2.41 0.3 2.41 1.58h2.38c0-0.36 0-0.79-0.03-1.09 -0.23-1.98-2.15-2.67-4.88-2.67 -3 0-6.7 2.31-6.7 7.76 0 5.22 2.77 7.99 6.63 7.99 1.68 0 3.47-0.36 4.95-1.39l-0.2-2.31c-0.99 0.82-2.84 1.52-4.06 1.52 -2.15 0-4.55-1.71-4.55-5.91C69.77 10.2 71.85 9.18 72.12 9.01M92.74 14.13c0-5.18-1.42-7.56-6.01-7.56 -3.86 0-6.67 2.77-6.67 7.92 0 4.92 2.97 7.82 6.73 7.82 2.81 0 4.36-0.63 5.68-1.42l-0.2-2.31c-0.89 0.79-2.94 1.55-4.69 1.55 -3.14 0-4.88-1.95-4.88-5.51v-0.49H92.74M84.59 9.18c0.17-0.17 1.29-0.46 1.98-0.46 2.48 0 3.76 0.53 3.86 3.43h-7.46C83.24 10.27 84.39 9.37 84.59 9.18zM103.9 1.98h-7.13v19.93h6.83c7.26 0 9.77-5.68 9.77-10.03C113.37 7.33 110.93 1.98 103.9 1.98M103.14 19.8h-3.76V4.1h4.09c5.38 0 6.96 4.39 6.96 7.79C110.43 16.87 108.19 19.8 103.14 19.8zM118.38 0.7c-0.92 0-1.65 0.92-1.65 1.81 0 0.93 0.76 1.85 1.65 1.85 0.89 0 1.69-0.96 1.69-1.88C120.07 1.55 119.28 0.7 118.38 0.7"></path><rect x="117.13" y="6.97" fill="#EB6500" width="2.54" height="14.95"></rect><path fill="#EB6500" d="M130.2 6.6c-1.62 0-2.87 1.45-3.4 2.74l-0.43-2.37h-2.34l0.13 1.19c0.1 0.76 0.13 1.75 0.13 2.9v10.86h2.54v-9.51c0.53-1.29 1.72-3.7 3.17-3.7 0.96 0 1.06 0.99 1.06 1.22l2.08-0.6V9.18c0-0.03-0.03-0.17-0.06-0.4C132.8 7.36 131.91 6.6 130.2 6.6M145.87 14.13c0-5.18-1.42-7.56-6.01-7.56 -3.86 0-6.67 2.77-6.67 7.92 0 4.92 2.97 7.82 6.73 7.82 2.81 0 4.36-0.63 5.68-1.42l-0.2-2.31c-0.89 0.79-2.94 1.55-4.69 1.55 -3.14 0-4.89-1.95-4.89-5.51v-0.49H145.87M137.72 9.18c0.17-0.17 1.29-0.46 1.98-0.46 2.48 0 3.76 0.53 3.86 3.43h-7.46C136.37 10.27 137.52 9.37 137.72 9.18zM153.23 9.01c0.23-0.13 0.69-0.26 1.72-0.26 1.72 0 2.41 0.3 2.41 1.58h2.38c0-0.36 0-0.79-0.03-1.09 -0.23-1.98-2.14-2.67-4.88-2.67 -3 0-6.7 2.31-6.7 7.76 0 5.22 2.77 7.99 6.63 7.99 1.69 0 3.47-0.36 4.95-1.39l-0.2-2.31c-0.99 0.82-2.84 1.52-4.06 1.52 -2.15 0-4.55-1.71-4.55-5.91C150.89 10.2 152.97 9.18 153.23 9.01M170 19.44c-0.92 0.36-1.72 0.69-2.51 0.69 -1.16 0-1.58-0.66-1.58-2.34V8.95h3.93V6.97h-3.93V2.97h-2.48v3.99h-2.71v1.98h2.71v9.67c0 2.64 1.39 3.73 3.33 3.73 1.15 0 2.54-0.39 3.43-0.79L170 19.44M173.68 5.96c-1.09 0-2-0.87-2-1.97 0-1.1 0.91-1.97 2-1.97s1.98 0.88 1.98 1.98C175.66 5.09 174.77 5.96 173.68 5.96zM173.67 2.46c-0.85 0-1.54 0.67-1.54 1.52 0 0.85 0.69 1.54 1.54 1.54 0.85 0 1.54-0.69 1.54-1.54C175.21 3.13 174.52 2.46 173.67 2.46zM174.17 5.05c-0.09-0.09-0.17-0.19-0.25-0.3l-0.41-0.56h-0.16v0.87h-0.39V2.92c0.22-0.01 0.47-0.03 0.66-0.03 0.41 0 0.82 0.16 0.82 0.64 0 0.29-0.21 0.55-0.49 0.63 0.23 0.32 0.45 0.62 0.73 0.91H174.17zM173.56 3.22l-0.22 0.01v0.63h0.22c0.26 0 0.43-0.05 0.43-0.34C174 3.28 173.83 3.21 173.56 3.22z"></path></g></svg></a><div class="gh-nav-cnt u-hide-from-print"><div class="gh-nav-links-container gh-nav-links-container-h u-hide-from-print gh-nav-content-container"><nav aria-label="links" class="gh-nav gh-nav-links gh-nav-h"><ul class="gh-nav-list u-list-reset"><li class="gh-nav-item gh-move-to-spine"><a class="anchor gh-nav-action text-s anchor-secondary anchor-medium" href="/browse/journals-and-books" id="gh-journals-books-link" data-aa-region="header" data-aa-name="Journals &amp; Books"><span class="anchor-text-container"><span class="anchor-text">Journals &amp; Books</span></span></a></li></ul></nav><nav aria-label="utilities" class="gh-nav gh-nav-utilities gh-nav-h"><ul class="gh-nav-list u-list-reset"><li class="gh-nav-help text-s u-flex-center-ver u-gap-6 gh-nav-action"><div class="gh-move-to-spine gh-help-button gh-help-icon gh-nav-item"><div class="popover" id="gh-help-icon-popover"><div id="popover-trigger-gh-help-icon-popover"><input type="hidden"><button class="button-link button-link-secondary gh-icon-btn button-link-medium button-link-icon-left" title="Help" aria-expanded="false" type="button"><svg focusable="false" viewBox="0 0 114 128" height="20" width="20" class="icon icon-help gh-icon"><path d="M57 8C35.69 7.69 15.11 21.17 6.68 40.71c-8.81 19.38-4.91 43.67 9.63 59.25 13.81 15.59 36.85 21.93 56.71 15.68 21.49-6.26 37.84-26.81 38.88-49.21 1.59-21.15-10.47-42.41-29.29-52.1C74.76 10.17 65.88 7.99 57 8zm0 10c20.38-.37 39.57 14.94 43.85 34.85 4.59 18.53-4.25 39.23-20.76 48.79-17.05 10.59-40.96 7.62-54.9-6.83-14.45-13.94-17.42-37.85-6.83-54.9C26.28 26.5 41.39 17.83 57 18zm-.14 14C45.31 32.26 40 40.43 40 50v2h10v-2c0-4.22 2.22-9.66 8-9.24 5.5.4 6.32 5.14 5.78 8.14C62.68 55.06 52 58.4 52 69.4V76h10v-5.56c0-8.16 11.22-11.52 12-21.7.74-9.86-5.56-16.52-16-16.74-.39-.01-.76-.01-1.14 0zM52 82v10h10V82H52z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Help</span></span></button></div></div></div></li><li class="gh-nav-search text-s u-flex-center-ver u-gap-6 gh-nav-action"><div class="gh-search-toggle gh-nav-item search-button-link"><a class="anchor button-link-secondary anchor-secondary u-margin-l-left gh-nav-action gh-icon-btn anchor-medium anchor-icon-left anchor-with-icon" href="/search" id="gh-search-link" title="Search" data-aa-button="search-in-header-opened-from-article" role="button"><svg focusable="false" viewBox="0 0 100 128" height="20" class="icon icon-search gh-icon"><path d="M19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6L73.23 80.19c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96S19.9 18.47 12.19 26.18C4.47 33.89.22 44.15.22 55.06s4.25 21.17 11.97 28.88C19.9 91.65 30.16 95.9 41.07 95.9c9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg><span class="anchor-text-container"><span class="anchor-text">Search</span></span></a></div></li></ul></nav></div></div><div class="gh-profile-container gh-move-to-spine u-hide-from-print"><a class="anchor text-s u-clr-grey8 u-margin-l-left gh-icon-btn anchor-primary anchor-medium anchor-icon-left anchor-with-icon" href="/user/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS1053811915008873&amp;from=globalheader" id="gh-myaccount-btn" data-aa-region="header" data-aa-name="personalsignin"><svg focusable="false" viewBox="0 0 106 128" height="20" aria-hidden="true" class="icon icon-person gh-cta-btn-icon"><path d="M11.07 120l.84-9.29C13.88 91.92 35.25 87.78 53 87.78c17.74 0 39.11 4.13 41.08 22.84l.84 9.38h10.04l-.93-10.34C101.88 89.23 83.89 78 53 78S4.11 89.22 1.95 109.73L1.04 120h10.03M53 17.71c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98S71.24 49.97 71.24 36.3c0-9.9-8.52-18.59-18.24-18.59zM53 70c-15.96 0-28-14.48-28-33.67C25 20.97 37.82 8 53 8s28 12.97 28 28.33C81 55.52 68.96 70 53 70"></path></svg><span class="anchor-text-container"><span class="anchor-text">My account</span></span></a></div><a class="anchor text-s u-clr-grey8 gh-move-to-spine u-hide-from-print u-margin-l-left anchor-secondary gh-icon-btn anchor-medium anchor-icon-left anchor-with-icon" href="/user/institution/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS1053811915008873" id="gh-institutionalsignin-btn" data-aa-region="header" data-aa-name="institutionalsignin"><svg focusable="false" viewBox="0 0 106 128" height="20" aria-hidden="true" class="icon icon-institution gh-cta-btn-icon"><path d="M84 98h10v10H12V98h10V52h14v46h10V52h14v46h10V52h14v46zM12 36.86l41-20.84 41 20.84V42H12v-5.14zM104 52V30.74L53 4.8 2 30.74V52h10v36H2v30h102V88H94V52h10z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Sign in</span></span></a><div id="gh-mobile-menu" class="mobile-menu u-hide-from-print"><div class="gh-hamburger u-fill-grey7"><button class="button-link u-flex-center-ver button-link-primary button-link-icon-left" aria-label="Toggle mobile menu" aria-expanded="false" type="button"><svg class="gh-hamburger-svg-el gh-hamburger-closed" role="img" aria-hidden="true" height="20" width="20"><path d="M0 14h40v2H0zm0-7h40v2H0zm0-7h40v2H0z"></path></svg></button></div><div id="gh-overlay" class="mobile-menu-overlay u-overlay u-display-none" role="button" tabindex="-1"></div><div id="gh-drawer" aria-label="Mobile menu" class="" role="navigation"></div></div></div></header><div class="Article" id="mathjax-container" role="main"><div class="accessbar-sticky"><div id="screen-reader-main-content"></div><div role="region" aria-label="Download options and search"><div class="accessbar"><div class="accessbar-label"></div><ul aria-label="PDF Options"><li class="ViewPDF"><a class="link-button accessbar-utility-component accessbar-utility-link link-button-primary link-button-icon-left" target="_blank" aria-label="View PDF. Opens in a new window." href="/science/article/pii/S1053811915008873/pdfft?md5=73fd19e992f38a364d9834d34199d6b3&amp;pid=1-s2.0-S1053811915008873-main.pdf" rel="nofollow"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="link-button-text-container"><span class="link-button-text"><span>View&nbsp;<strong>PDF</strong></span></span></span></a></li><li class="DownloadFullIssue"><button class="button-link accessbar-utility-component button-link-primary" aria-label="Download full issue" type="button"><span class="button-link-text-container"><span class="button-link-text"><span>Download full issue</span></span></span></button></li></ul><form class="QuickSearch" action="/search#submit" method="get" aria-label="form"><div class="search-input"><div class="search-input-container search-input-container-no-label"><label class="search-input-label u-hide-visually" for="article-quick-search">Search ScienceDirect</label><input type="text" id="article-quick-search" name="qs" class="search-input-field" aria-describedby="article-quick-search-description-message" aria-invalid="false" aria-label="Search ScienceDirect" placeholder="Search ScienceDirect" value=""></div><div class="search-input-message-container"><div class="search-input-validation-error" aria-live="polite"></div><div id="article-quick-search-description-message"></div></div></div><button type="submit" class="button u-margin-xs-left button-primary small button-icon-only" aria-disabled="false" aria-label="Submit search"><svg focusable="false" viewBox="0 0 100 128" height="20" class="icon icon-search"><path d="M19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6L73.23 80.19c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96S19.9 18.47 12.19 26.18C4.47 33.89.22 44.15.22 55.06s4.25 21.17 11.97 28.88C19.9 91.65 30.16 95.9 41.07 95.9c9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg></button><input type="hidden" name="origin" value="article"><input type="hidden" name="zone" value="qSearch"></form></div></div></div><div class="article-wrapper grid row"><div role="navigation" class="u-display-block-from-lg col-lg-6 u-padding-s-top sticky-table-of-contents" aria-label="Table of contents"><div class="TableOfContents" lang="en"><div class="Outline" id="toc-outline"><h2 class="u-h4">Outline</h2><ol class="u-padding-xs-bottom"><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#ab0010" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Highlights"><span class="anchor-text-container"><span class="anchor-text">Highlights</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#ab0005" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Abstract"><span class="anchor-text-container"><span class="anchor-text">Abstract</span></span></a></li><li class="ai-components-toc-entry" id="ai-components-toc-entry"></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#ks0005" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Keywords"><span class="anchor-text-container"><span class="anchor-text">Keywords</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#s0005" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Introduction"><span class="anchor-text-container"><span class="anchor-text">Introduction</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#s0010" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Methods"><span class="anchor-text-container"><span class="anchor-text">Methods</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#s0055" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Results"><span class="anchor-text-container"><span class="anchor-text">Results</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#s0100" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Discussion"><span class="anchor-text-container"><span class="anchor-text">Discussion</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#ac0005" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Acknowledgments"><span class="anchor-text-container"><span class="anchor-text">Acknowledgments</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#bi0005" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="References"><span class="anchor-text-container"><span class="anchor-text">References</span></span></a></li></ol><button class="button-link u-margin-xs-top u-margin-s-bottom button-link-primary button-link-icon-right" aria-expanded="false" data-aa-button="sd:product:journal:article:type=menu:name=show-full-outline" type="button"><span class="button-link-text-container"><span class="button-link-text">Show full outline</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="PageDivider"></div></div><div class="CitedBy" id="toc-cited-by"><h2 class="u-h4"><a class="anchor anchor-primary" href="#section-cited-by"><span class="anchor-text-container"><span class="anchor-text">Cited by (67)</span></span></a></h2><div class="PageDivider"></div></div><div class="Figures" id="toc-figures"><h2 class="u-h4">Figures (7)</h2><ol class="u-margin-s-bottom"><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#f0005" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig.1. Mean luminance and spatial frequency values (top) and pre-evaluated…" class="u-display-block" height="164px" src="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr1.sml" width="205px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#f0010" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig.2. An example of the trial structure for each emotion recognition task" class="u-display-block" height="164px" src="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr2.sml" width="143px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#f0015" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig.3. Effects of the Pic–Pic and Pic–Word tasks on early visual processing of…" class="u-display-block" height="164px" src="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr3.sml" width="195px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#f0020" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig.4. Effects of emotion category on early visual processing of emotional faces and…" class="u-display-block" height="164px" src="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr4.sml" width="163px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#f0025" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig.5. Mean zygomatic muscle EMG amplitudes (μV) and error bars for 1 standard error,…" class="u-display-block" height="121px" src="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr5.sml" width="219px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#f0030" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig.6. Mean corrugator muscle EMG amplitudes (μV) and error bars for 1 standard error,…" class="u-display-block" height="164px" src="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr6.sml" width="188px"></a></li></ol><button class="button-link u-margin-xs-top u-margin-s-bottom button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:type=menu:name=show-figures" type="button"><span class="button-link-text-container"><span class="button-link-text">Show 1 more figure</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="PageDivider"></div></div><div class="Tables" id="toc-tables"><h2 class="u-h4">Tables (4)</h2><ol class="u-padding-s-bottom"><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#t0005" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Summary of significant factor main effects and/or significant factor interactions related to EEG data."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;1</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#t0010" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Summary of significant factor main effects and/or significant factor interactions related to zygomaticus major EMG data."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;2</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#t0015" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Summary of significant factor main effects and/or significant factor interactions related to corrugator supercilii EMG data."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;3</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#t0020" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Summary of significant factor main effects and/or significant factor interactions related to SC data."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;4</span></span></a></li></ol><div class="PageDivider"></div></div></div></div><article class="col-lg-12 col-md-16 pad-left pad-right u-padding-s-top" lang="en"><div class="Publication" id="publication"><div class="publication-brand u-display-block-from-sm"><a class="anchor u-display-flex anchor-primary" href="/journal/neuroimage" title="Go to NeuroImage on ScienceDirect"><span class="anchor-text-container"><span class="anchor-text"><img class="publication-brand-image" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/b64013ec63c69e3d916174cbebae89d65b2419e1/image/elsevier-non-solus.png" alt="Elsevier"></span></span></a></div><div class="publication-volume u-text-center"><h2 class="publication-title u-h3" id="publication-title"><a class="anchor anchor-secondary publication-title-link" href="/journal/neuroimage" title="Go to NeuroImage on ScienceDirect"><span class="anchor-text-container"><span class="anchor-text">NeuroImage</span></span></a></h2><div class="text-xs"><a class="anchor anchor-primary" href="/journal/neuroimage/vol/124/part/PA" title="Go to table of contents for this volume/issue"><span class="anchor-text-container"><span class="anchor-text">Volume 124, Part A</span></span></a>, <!-- -->1 January 2016<!-- -->, Pages 931-946</div></div><div class="publication-cover u-display-block-from-sm"><a class="anchor u-display-flex anchor-primary" href="/journal/neuroimage/vol/124/part/PA"><span class="anchor-text-container"><span class="anchor-text"><img class="publication-cover-image" src="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915X00176-cov150h.gif" alt="NeuroImage"></span></span></a></div></div><h1 id="screen-reader-main-title" class="Head u-font-serif u-h2 u-margin-s-ver"><span class="title-text">Emotional facial expressions evoke faster orienting responses, but weaker emotional responses at neural and behavioural levels compared to scenes: A simultaneous EEG and facial EMG study</span></h1><div class="Banner" id="banner"><div class="wrapper truncated"><div aria-live="polite"></div><div class="AuthorGroups"><div class="author-group" id="author-group"><span class="sr-only">Author links open overlay panel</span><button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0005" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Aimee</span> <span class="text surname">Mavratzakis</span> </span><span class="author-ref" id="baf0005"><sup>a</sup></span> <span class="author-ref" id="baf0010"><sup>b</sup></span></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0010" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Cornelia</span> <span class="text surname">Herbert</span> </span><span class="author-ref" id="baf0015"><sup>c</sup></span> <span class="author-ref" id="baf0020"><sup>d</sup></span></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0015" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Peter</span> <span class="text surname">Walla</span> </span><span class="author-ref" id="baf0005"><sup>a</sup></span> <span class="author-ref" id="baf0010"><sup>b</sup></span> <span class="author-ref" id="baf0025"><sup>e</sup></span> <span class="author-ref" id="baf0030"><sup>f</sup></span><svg focusable="false" viewBox="0 0 106 128" height="20" title="Correspondence author icon" class="icon icon-person react-xocs-author-icon u-fill-grey8"><path d="M11.07 120l.84-9.29C13.88 91.92 35.25 87.78 53 87.78c17.74 0 39.11 4.13 41.08 22.84l.84 9.38h10.04l-.93-10.34C101.88 89.23 83.89 78 53 78S4.11 89.22 1.95 109.73L1.04 120h10.03M53 17.71c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98S71.24 49.97 71.24 36.3c0-9.9-8.52-18.59-18.24-18.59zM53 70c-15.96 0-28-14.48-28-33.67C25 20.97 37.82 8 53 8s28 12.97 28 28.33C81 55.52 68.96 70 53 70"></path></svg><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button></div></div></div><button class="button-link u-margin-s-ver button-link-primary button-link-icon-right" id="show-more-btn" type="button" data-aa-button="icon-expand"><span class="button-link-text-container"><span class="button-link-text">Show more</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="banner-options u-padding-xs-bottom"><div class="toc-button-wrap u-display-inline-block u-display-none-from-lg u-margin-s-right"><button class="button-link button-link-secondary button-link-icon-left button-link-has-colored-icon" type="button"><svg focusable="false" viewBox="0 0 128 128" height="20" class="icon icon-list"><path d="M23 26a9 9 0 0 0-9 9 9 9 0 0 0 9 9 9 9 0 0 0 9-9 9 9 0 0 0-9-9zm23 4v10h68V30zM23 56a9 9 0 0 0-9 9 9 9 0 0 0 9 9 9 9 0 0 0 9-9 9 9 0 0 0-9-9zm23 4v10h68V60zM23 86a9 9 0 0 0-9 9 9 9 0 0 0 9 9 9 9 0 0 0 9-9 9 9 0 0 0-9-9zm23 4v10h68V90z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Outline</span></span></button></div><button class="button-link AddToMendeley button-link-secondary u-margin-s-right u-display-inline-flex-from-md button-link-icon-left button-link-has-colored-icon" type="button"><svg focusable="false" viewBox="0 0 86 128" height="20" class="icon icon-plus"><path d="M48 58V20H38v38H0v10h38v38h10V68h38V58z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Add to Mendeley</span></span></button><div class="Social u-display-inline-block" id="social"><div class="popover social-popover" id="social-popover"><div id="popover-trigger-social-popover"><button class="button-link button-link-secondary u-margin-s-right button-link-icon-left button-link-has-colored-icon" aria-expanded="false" aria-haspopup="true" type="button"><svg focusable="false" viewBox="0 0 114 128" height="20" class="icon icon-share"><path d="M90 112c-6.62 0-12-5.38-12-12s5.38-12 12-12 12 5.38 12 12-5.38 12-12 12zM24 76c-6.62 0-12-5.38-12-12s5.38-12 12-12 12 5.38 12 12-5.38 12-12 12zm66-60c6.62 0 12 5.38 12 12s-5.38 12-12 12-12-5.38-12-12 5.38-12 12-12zm0 62c-6.56 0-12.44 2.9-16.48 7.48L45.1 70.2c.58-1.98.9-4.04.9-6.2s-.32-4.22-.9-6.2l28.42-15.28C77.56 47.1 83.44 50 90 50c12.14 0 22-9.86 22-22S102.14 6 90 6s-22 9.86-22 22c0 1.98.28 3.9.78 5.72L40.14 49.1C36.12 44.76 30.38 42 24 42 11.86 42 2 51.86 2 64s9.86 22 22 22c6.38 0 12.12-2.76 16.14-7.12l28.64 15.38c-.5 1.84-.78 3.76-.78 5.74 0 12.14 9.86 22 22 22s22-9.86 22-22-9.86-22-22-22z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Share</span></span></button></div></div></div><div class="ExportCitation u-display-inline-block" id="export-citation"><div class="popover export-citation-popover" id="export-citation-popover"><div id="popover-trigger-export-citation-popover"><button class="button-link button-link-secondary button-link-icon-left button-link-has-colored-icon" aria-expanded="false" aria-haspopup="true" type="button"><svg focusable="false" viewBox="0 0 104 128" height="20" class="icon icon-cited-by-66"><path d="M2 58.78V106h44V64H12v-5.22C12 40.28 29.08 32 46 32V22C20.1 22 2 37.12 2 58.78zM102 32V22c-25.9 0-44 15.12-44 36.78V106h44V64H68v-5.22C68 40.28 85.08 32 102 32z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Cite</span></span></button></div></div></div></div></div><div class="ArticleIdentifierLinks u-margin-xs-bottom text-xs" id="article-identifier-links"><a class="anchor doi anchor-primary" href="https://doi.org/10.1016/j.neuroimage.2015.09.065" target="_blank" rel="noreferrer noopener" aria-label="Persistent link using digital object identifier" title="Persistent link using digital object identifier"><span class="anchor-text-container"><span class="anchor-text">https://doi.org/10.1016/j.neuroimage.2015.09.065</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor rights-and-content anchor-primary" href="https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&amp;contentID=S1053811915008873&amp;orderBeanReset=true" target="_blank" rel="noreferrer noopener"><span class="anchor-text-container"><span class="anchor-text">Get rights and content</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="LicenseInfo text-xs u-margin-xs-bottom"><div class="License"><span>Under a Creative Commons </span><a class="anchor anchor-primary" href="http://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noreferrer noopener"><span class="anchor-text-container"><span class="anchor-text">license</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="OpenAccessLabel"><span class="access-indicator"></span>open access</div></div><section class="ReferencedArticles"></section><section class="ReferencedArticles"></section><div class="PageDivider"></div><div class="Abstracts u-font-serif" id="abstracts"><div class="abstract author-highlights" id="ab0010"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Highlights</h2><div id="as0010"><div class="u-margin-s-bottom" id="sp0065"><ul class="list"><li class="react-xocs-list-item"><span class="list-label">•</span><span><div class="u-margin-s-bottom" id="p0335">Faces and scenes elicit different emotion-related brain activities.</div></span></li><li class="react-xocs-list-item"><span class="list-label">•</span><span><div class="u-margin-s-bottom" id="p0340">Faces and scenes elicit different facial expression-related muscle activities.</div></span></li><li class="react-xocs-list-item"><span class="list-label">•</span><span><div class="u-margin-s-bottom" id="p0345">Faces and scenes are shown to be different emotion elicitors.</div></span></li></ul></div></div></div><div class="abstract author" id="ab0005"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Abstract</h2><div id="as0005"><div class="u-margin-s-bottom" id="sp0005"><!-- --><span><span>In the current study, <a href="/topics/psychology/electroencephalography" title="Learn more about electroencephalography from ScienceDirect's AI-generated Topic Pages" class="topic-link">electroencephalography</a> (EEG) was recorded simultaneously with facial </span><a href="/topics/psychology/electromyography" title="Learn more about electromyography from ScienceDirect's AI-generated Topic Pages" class="topic-link">electromyography</a><span> (fEMG) to determine whether emotional <a href="/topics/neuroscience/face" title="Learn more about faces from ScienceDirect's AI-generated Topic Pages" class="topic-link">faces</a> and emotional scenes are processed differently at the neural level. In addition, it was investigated whether these differences can be observed at the behavioural level via spontaneous facial muscle activity. Emotional content of the stimuli did not affect early P1 activity. Emotional faces elicited enhanced amplitudes of the face-sensitive N170 component, while its counterpart, the scene-related N100, was not sensitive to emotional content of scenes. At 220–280</span></span>&nbsp;<!-- -->ms, the early posterior negativity (EPN) was enhanced only slightly for fearful as compared to neutral or happy faces. However, its amplitudes were significantly enhanced during processing of scenes with positive content, particularly over the right hemisphere. Scenes of positive content also elicited enhanced spontaneous zygomatic activity from 500–750<!-- -->&nbsp;<!-- -->ms onwards, while happy faces elicited no such changes. Contrastingly, both fearful faces and negative scenes elicited enhanced spontaneous corrugator activity at 500–750<!-- -->&nbsp;<!-- -->ms after stimulus onset. However, relative to baseline EMG changes occurred earlier for faces (250<!-- -->&nbsp;<!-- -->ms) than for scenes (500<!-- -->&nbsp;<!-- -->ms) whereas for scenes activity changes were more pronounced over the whole viewing period. Taking into account all effects, the data suggests that emotional facial expressions evoke faster attentional orienting, but weaker affective neural activity and emotional behavioural responses compared to emotional scenes.</div></div></div></div><div id="reading-assistant-main-body-section"></div><ul id="issue-navigation" class="issue-navigation u-margin-s-bottom u-bg-grey1"><li class="previous move-left u-padding-s-ver u-padding-s-left"><a class="button-alternative button-alternative-tertiary u-display-flex button-alternative-icon-left" href="/science/article/pii/S1053811915008885"><svg focusable="false" viewBox="0 0 54 128" height="20" class="icon icon-navigate-left"><path d="M1 61l45-45 7 7-38 38 38 38-7 7z"></path></svg><span class="button-alternative-text-container"><span class="button-alternative-text">Previous <span class="extra-detail-1">article</span><span class="extra-detail-2"> in issue</span></span></span></a></li><li class="next move-right u-padding-s-ver u-padding-s-right"><a class="button-alternative button-alternative-tertiary u-display-flex button-alternative-icon-right" href="/science/article/pii/S1053811915008733"><span class="button-alternative-text-container"><span class="button-alternative-text">Next <span class="extra-detail-1">article</span><span class="extra-detail-2"> in issue</span></span></span><svg focusable="false" viewBox="0 0 54 128" height="20" class="icon icon-navigate-right"><path d="M1 99l38-38L1 23l7-7 45 45-45 45z"></path></svg></a></li></ul><div class="Keywords u-font-serif"><div id="ks0005" class="keywords-section"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Keywords</h2><div id="kw0045" class="keyword"><span id="tx0010">Emotion</span></div><div id="kw0050" class="keyword"><span id="tx0015">Affective processing</span></div><div id="kw0055" class="keyword"><span id="tx0020">Faces and scenes</span></div><div id="kw0060" class="keyword"><span id="tx0025">Electroencephalography</span></div><div id="kw0065" class="keyword"><span id="tx0030">Spontaneous facial EMG</span></div><div id="kw0070" class="keyword"><span id="tx0035">N170</span></div><div id="kw0075" class="keyword"><span id="tx0040">N100</span></div><div id="kw0080" class="keyword"><span id="tx0045">Early posterior negativity</span></div></div></div><div class="Body u-font-serif" id="body"><div><section id="s0005"><h2 id="st0015" class="u-h4 u-margin-l-top u-margin-xs-bottom">Introduction</h2><div class="u-margin-s-bottom" id="p0005">In emotion research two kinds of stimuli are frequently used: facial expressions (e.g. a smiling or sad face) and emotionally evocative scenes (e.g. snakes, erotic pictures). But, do these forms of emotional stimuli undergo the same neural processing? Despite each being intrinsically emotionally evocative, only a few studies exist that have compared affective processing of faces and scenes in the same experiment and context.</div><div class="u-margin-s-bottom" id="p0010">A recent meta-analysis comparing 157 functional Magnetic Resonance Imaging (fMRI) studies that used either emotional faces or emotional scenes (<a class="anchor anchor-primary" href="#bb0470" name="bbb0470" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0470"><span class="anchor-text-container"><span class="anchor-text">Sabatinelli et al., 2011</span></span></a><span>) revealed multiple clusters of <a href="/topics/neuroscience/brain-activation" title="Learn more about brain activations from ScienceDirect's AI-generated Topic Pages" class="topic-link">brain activations</a> unique to these different forms of stimuli even after the subtraction of neural activity related to basic visual processing. Although this suggests that both types of stimuli might be processed differently in the brain, direct comparisons of the time course of affective processing for faces and scenes are lacking and little is known about whether both stimulus classes elicit similar expressive behavioural reactions.</span></div><div class="u-margin-s-bottom" id="p0015">In other contexts it is obvious that faces and scenes are indeed quite different. For example, facial expressions elicit mimicry and facial feedback mechanisms might modify emotion-related processing (see <a class="anchor anchor-primary" href="#bb0400" name="bbb0400" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0400"><span class="anchor-text-container"><span class="anchor-text">Niedenthal et al., 2001</span></span></a><span>). Facial expressions can be understood as interpersonal, facilitating social transactions, and require complex neural processing to translate these emotional cues into social meaning. Emotional scenes on the other hand are more intrapersonal and directly elicit motivational <a href="/topics/neuroscience/behavior-neuroscience" title="Learn more about behaviours from ScienceDirect's AI-generated Topic Pages" class="topic-link">behaviours</a> without needing to translate their meaning beyond knowing whether to approach or avoid. Different facial expressions are more similar to each other than different scene pictures are. Various processing differences between face and scene stimuli have been described. </span><a class="anchor anchor-primary" href="#bb0255" name="bbb0255" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0255"><span class="anchor-text-container"><span class="anchor-text">Hariri et al. (2002)</span></span></a><span> found varying <a href="/topics/neuroscience/amygdala" title="Learn more about amygdala from ScienceDirect's AI-generated Topic Pages" class="topic-link">amygdala</a> activity depending on whether a fearful face or a threatening scene was presented to their participants. In their fMRI study, </span><a class="anchor anchor-primary" href="#bb0295" name="bbb0295" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0295"><span class="anchor-text-container"><span class="anchor-text">Keightley et al. (2011)</span></span></a> reported about their conclusion that the contextual information in emotional scenes may facilitate memory via additional visual processing, whereas memory for emotional faces may rely more on cognitive control mediated by rostrolateral prefrontal regions. <a class="anchor anchor-primary" href="#bb0195" name="bbb0195" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0195"><span class="anchor-text-container"><span class="anchor-text">Epstein et al. (2006)</span></span></a> investigated differences between face and scene inversion. Their results demonstrate that both face and scene inversion cause a shift from specialised processing streams towards generic object-processing mechanisms, but this shift only leads to a reliable behavioural deficit in the case of face inversion.</div><div class="u-margin-s-bottom" id="p0020">The temporal characteristics of neural affective processing have been relatively well documented for emotional faces (<a class="anchor anchor-primary" href="#bb0560" name="bbb0560" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0560"><span class="anchor-text-container"><span class="anchor-text">Vuilleumier and Pourtois, 2007</span></span></a>, <a class="anchor anchor-primary" href="#bb0585" name="bbb0585" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0585"><span class="anchor-text-container"><span class="anchor-text">Wieser and Brosch, 2012</span></span></a>) and emotional scenes (<a class="anchor anchor-primary" href="#bb0420" name="bbb0420" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0420"><span class="anchor-text-container"><span class="anchor-text">Olofsson et al., 2008</span></span></a>, <a class="anchor anchor-primary" href="#bb0480" name="bbb0480" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0480"><span class="anchor-text-container"><span class="anchor-text">Schupp et al., 2006a</span></span></a><span>). Anatomically, visual information passes through the extrastriate <a href="/topics/neuroscience/visual-cortex" title="Learn more about visual cortex from ScienceDirect's AI-generated Topic Pages" class="topic-link">visual cortex</a> where low-lying physical stimulus properties such as luminance and spatial complexity determine which aspects of visual information receive rapid attentional capture and further processing (</span><a class="anchor anchor-primary" href="#bb0105" name="bbb0105" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0105"><span class="anchor-text-container"><span class="anchor-text">Clark and Hillyard, 1996</span></span></a>, <a class="anchor anchor-primary" href="#bb0235" name="bbb0235" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0235"><span class="anchor-text-container"><span class="anchor-text">Givre et al., 1994</span></span></a>, <a class="anchor anchor-primary" href="#bb0270" name="bbb0270" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0270"><span class="anchor-text-container"><span class="anchor-text">Hillyard and Anllo-Vento, 1998</span></span></a>, <a class="anchor anchor-primary" href="#bb0370" name="bbb0370" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0370"><span class="anchor-text-container"><span class="anchor-text">Mangun et al., 1993</span></span></a>, <a class="anchor anchor-primary" href="#bb0465" name="bbb0465" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0465"><span class="anchor-text-container"><span class="anchor-text">Rugg et al., 1987</span></span></a>). This rapid-attentional capture is seen in scalp-recorded potentials as a prominent positively charged deflection in amplitude over lateral posterior occipital sites at approximately 100&nbsp;<span>ms post-stimulus (termed the P100 component, or P1 to represent the first positive peak in neural activity), where the size of the amplitude deflection indexes the degree of attentional capture of the related stimulus. Attended-to information then undergoes object recognition processing in <a href="/topics/neuroscience/neural-circuit" title="Learn more about neural circuits from ScienceDirect's AI-generated Topic Pages" class="topic-link">neural circuits</a> proceeding through bilateral ventral–lateral streams from the visual cortex into the temporal cortices (</span><a class="anchor anchor-primary" href="#bb0010" name="bbb0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0010"><span class="anchor-text-container"><span class="anchor-text">Allison et al., 1999</span></span></a><span>). Here, the <a href="/topics/neuroscience/fusiform-gyrus" title="Learn more about fusiform gyrus from ScienceDirect's AI-generated Topic Pages" class="topic-link">fusiform gyrus</a>, a well-studied structure located in the inferior temporal lobes, facilitates face recognition via a highly specialised process of collating local facial features into a holistic global face representation (</span><a class="anchor anchor-primary" href="#bb0460" name="bbb0460" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0460"><span class="anchor-text-container"><span class="anchor-text">Rossion et al., 2003</span></span></a>). This activity is observed in scalp-recorded potentials as a strong negatively charged deflection in amplitude over lateral temporal–occipital areas approximately 170&nbsp;ms post-stimulus onset (<a class="anchor anchor-primary" href="#bb0040" name="bbb0040" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0040"><span class="anchor-text-container"><span class="anchor-text">Bentin et al., 1996</span></span></a>, <a class="anchor anchor-primary" href="#bb0130" name="bbb0130" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0130"><span class="anchor-text-container"><span class="anchor-text">Deffke et al., 2007</span></span></a>), hence the name N170. Other stimuli such as complex scenes also undergo category-specific processing across more widely distributed hierarchically organised circuits in the ventral–lateral streams, with this activity being observed as a more modest negative deflection in amplitude at around 150–200&nbsp;ms after stimulus onset over lateral temporal-occipital scalp locations (termed the N100). From here, it has been posited that affective information of faces and scenes begins to influence neural activity, seen at lateral–occipital scalp recordings as a more stable negative shift in polarity when viewing emotionally-evocative relative to neutral stimuli. This posterior negativity (i.e. the early posterior negativity or EPN) typically emerges at the offset of the N100/N170, around 200 to 250&nbsp;ms post-stimulus and has been found to be modulated as a function of increased attentional allocation and greater motivational relevance of the emotionally evocative stimuli (<a class="anchor anchor-primary" href="#bb0065" name="bbb0065" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0065"><span class="anchor-text-container"><span class="anchor-text">Bublatzky and Schupp, 2011</span></span></a>, <a class="anchor anchor-primary" href="#bb0220" name="bbb0220" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0220"><span class="anchor-text-container"><span class="anchor-text">Foti et al., 2009</span></span></a>, <a class="anchor anchor-primary" href="#bb0480" name="bbb0480" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0480"><span class="anchor-text-container"><span class="anchor-text">Schupp et al., 2006a</span></span></a>, <a class="anchor anchor-primary" href="#bb0580" name="bbb0580" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0580"><span class="anchor-text-container"><span class="anchor-text">Weinberg and Hajcak, 2010</span></span></a>).</div><div class="u-margin-s-bottom" id="p0025">There is however evidence that affective information can influence activity at earlier stages of processing relative to the EPN. Several studies have reported larger N170 amplitudes when viewing negatively-valanced facial expressions such as fear and anger (e.g. <a class="anchor anchor-primary" href="#bb0035" name="bbb0035" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0035"><span class="anchor-text-container"><span class="anchor-text">Batty and Taylor, 2003</span></span></a>, <a class="anchor anchor-primary" href="#bb0355" name="bbb0355" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0355"><span class="anchor-text-container"><span class="anchor-text">Leppänen et al., 2008</span></span></a>, <a class="anchor anchor-primary" href="#bb0440" name="bbb0440" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0440"><span class="anchor-text-container"><span class="anchor-text">Pourtois et al., 2005</span></span></a>, <a class="anchor anchor-primary" href="#bb0535" name="bbb0535" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0535"><span class="anchor-text-container"><span class="anchor-text">Stekelenburg and de Gelder, 2004</span></span></a>), which has been interpreted as an innate attentional ‘negativity bias’ (<a class="anchor anchor-primary" href="#bb0095" name="bbb0095" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0095"><span class="anchor-text-container"><span class="anchor-text">Carretie et al., 2009</span></span></a>, <a class="anchor anchor-primary" href="#bb0280" name="bbb0280" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0280"><span class="anchor-text-container"><span class="anchor-text">Holmes et al., 2005</span></span></a>). The same controversy exists for emotional scenes, with some studies reporting a negativity bias for highly unpleasant threatening or fearful scenes in the time window of the N100. Affective modulation has even been reported as early as 100&nbsp;ms post-stimulus (<a class="anchor anchor-primary" href="#bb0035" name="bbb0035" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0035"><span class="anchor-text-container"><span class="anchor-text">Batty and Taylor, 2003</span></span></a>, <a class="anchor anchor-primary" href="#bb0175" name="bbb0175" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0175"><span class="anchor-text-container"><span class="anchor-text">Eger et al., 2003</span></span></a>, <a class="anchor anchor-primary" href="#bb0180" name="bbb0180" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0180"><span class="anchor-text-container"><span class="anchor-text">Eimer and Holmes, 2002</span></span></a>, <a class="anchor anchor-primary" href="#bb0285" name="bbb0285" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0285"><span class="anchor-text-container"><span class="anchor-text">Holmes et al., 2003</span></span></a>, <a class="anchor anchor-primary" href="#bb0430" name="bbb0430" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0430"><span class="anchor-text-container"><span class="anchor-text">Pizzagalli et al., 1999</span></span></a>, <a class="anchor anchor-primary" href="#bb0440" name="bbb0440" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0440"><span class="anchor-text-container"><span class="anchor-text">Pourtois et al., 2005</span></span></a>, <a class="anchor anchor-primary" href="#bb0455" name="bbb0455" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0455"><span class="anchor-text-container"><span class="anchor-text">Recio et al., 2014</span></span></a>, <a class="anchor anchor-primary" href="#bb0525" name="bbb0525" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0525"><span class="anchor-text-container"><span class="anchor-text">Smith et al., 2013</span></span></a>, <a class="anchor anchor-primary" href="#bb0540" name="bbb0540" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0540"><span class="anchor-text-container"><span class="anchor-text">Streit et al., 2003</span></span></a>). Differences as well as similarities in affective stimulus processing may be better understood by directly comparing when these processes occur for emotional faces and scenes in a single experimental framework. This would also allow a direct comparison of behavioural reactions elicited by faces and scenes.</div><div class="u-margin-s-bottom" id="p0030">In the current study, we were interested in investigating how affective neural activity during emotional face and scene perceptions translates into emotional behaviour, building on the idea that emotional behaviour should be understood as a consequence of subcortical affective neural activity (<a class="anchor anchor-primary" href="#bb0565" name="bbb0565" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0565"><span class="anchor-text-container"><span class="anchor-text">Walla and Panksepp, 2013</span></span></a>). Spontaneous facial muscle activity is an emotion-related behavioural phenomenon that is thought to play a crucial role in social emotion recognition, whereby perceiving an emotional facial expression elicits a rapid or spontaneous micro-simulation of the perceived facial expression by the perceiver less than 1000&nbsp;ms post onset (<a class="anchor anchor-primary" href="#bb0005" name="bbb0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0005"><span class="anchor-text-container"><span class="anchor-text">Achaibou et al., 2008</span></span></a>, <a class="anchor anchor-primary" href="#bb0160" name="bbb0160" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0160"><span class="anchor-text-container"><span class="anchor-text">Dimberg et al., 2000</span></span></a>, <a class="anchor anchor-primary" href="#bb0240" name="bbb0240" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0240"><span class="anchor-text-container"><span class="anchor-text">Grèzes et al., 2013</span></span></a>, <a class="anchor anchor-primary" href="#bb0300" name="bbb0300" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0300"><span class="anchor-text-container"><span class="anchor-text">Korb et al., 2010</span></span></a>, <a class="anchor anchor-primary" href="#bb0380" name="bbb0380" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0380"><span class="anchor-text-container"><span class="anchor-text">Moody et al., 2007</span></span></a>). By utilising the excellent temporal resolution offered by electromyography to measure facial muscle activity (fEMG), these studies have shown that zygomaticus major ‘cheek’ muscles rapidly and spontaneously contract in response to smiling faces while corrugator supercilii ‘eyebrow’ muscles rapidly and spontaneously contract in response to angry or fearful faces. The phenomenon is thought to facilitate emotion recognition by triggering the reactivation of specific neural regions that are involved in producing that same emotion in the perceiver, leading to a realisation of the other person's emotional state (e.g. <a class="anchor anchor-primary" href="#bb0025" name="bbb0025" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0025"><span class="anchor-text-container"><span class="anchor-text">Barsalou, 2003</span></span></a>, <a class="anchor anchor-primary" href="#bb0030" name="bbb0030" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0030"><span class="anchor-text-container"><span class="anchor-text">Barsalou et al., 2003</span></span></a>, <a class="anchor anchor-primary" href="#bb0100" name="bbb0100" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0100"><span class="anchor-text-container"><span class="anchor-text">Clark et al., 2008</span></span></a>, <a class="anchor anchor-primary" href="#bb0395" name="bbb0395" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0395"><span class="anchor-text-container"><span class="anchor-text">Niedenthal, 2007</span></span></a>). Moreover, empirical evidence suggests that spontaneous facial reactions play a causal role in emotion recognition whereby selectively preventing movement in facial muscle/s required to simulate an expression leads to poor recognition ability for that facial expression in another person (<a class="anchor anchor-primary" href="#bb0215" name="bbb0215" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0215"><span class="anchor-text-container"><span class="anchor-text">Foroni and Semin, 2011</span></span></a>, <a class="anchor anchor-primary" href="#bb0400" name="bbb0400" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0400"><span class="anchor-text-container"><span class="anchor-text">Niedenthal et al., 2001</span></span></a>, <a class="anchor anchor-primary" href="#bb0405" name="bbb0405" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0405"><span class="anchor-text-container"><span class="anchor-text">Oberman et al., 2007</span></span></a>, <a class="anchor anchor-primary" href="#bb0435" name="bbb0435" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0435"><span class="anchor-text-container"><span class="anchor-text">Ponari et al., 2012</span></span></a>).</div><div class="u-margin-s-bottom" id="p0035">However, emotional scenes have also been shown to evoke spontaneous facial reactions (<a class="anchor anchor-primary" href="#bb0150" name="bbb0150" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0150"><span class="anchor-text-container"><span class="anchor-text">Dimberg et al., 1998</span></span></a><span>). In contrast to faces emotional scenes often do not contain any third-party emotion to recognise. This raises the question of whether and in what ways spontaneous facial muscle activity may differ when elicited by emotional faces compared to scenes, such as in latency or strength of the response. To this extent, the objective of the current study was to investigate differences in emotional <a href="/topics/psychology/evoked-potential" title="Learn more about responses evoked from ScienceDirect's AI-generated Topic Pages" class="topic-link">responses evoked</a> by happy, fearful and neutral faces versus positive, neutral and negative scenes: (1) during early visually-evoked stages of neural activity including the P1, N100/N170, and EPN; and (2) in spontaneous zygomatic and corrugator facial reactions; and (3) to examine correlations between affective neural activity and emotional behaviour.</span></div><div class="u-margin-s-bottom"><div id="p0040">When considering motivationally-relevant emotion processing, arousal must be taken into consideration, because stimuli that evoke heightened arousal have been shown to modulate both neural and facial muscle activity independent of emotional valence or stimulus type (<a class="anchor anchor-primary" href="#bb0070" name="bbb0070" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0070"><span class="anchor-text-container"><span class="anchor-text">Cacioppo et al., 1986</span></span></a>, <a class="anchor anchor-primary" href="#bb0110" name="bbb0110" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0110"><span class="anchor-text-container"><span class="anchor-text">Cuthbert et al., 2000</span></span></a>, <a class="anchor anchor-primary" href="#bb0205" name="bbb0205" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0205"><span class="anchor-text-container"><span class="anchor-text">Feng et al., 2014</span></span></a>, <a class="anchor anchor-primary" href="#bb0325" name="bbb0325" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0325"><span class="anchor-text-container"><span class="anchor-text">Lang et al., 1993</span></span></a>). For this reason the face and scene stimuli chosen for the current study were relatively low-arousing (see <a class="anchor anchor-primary" href="#f0005" name="bf0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0005"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;1</span></span></a><span> bottom right graph). However, it was still possible that face and scene stimuli could evoke different degrees of arousal. Therefore, arousal responses to pictures were also recorded via the <a href="/topics/neuroscience/electrodermal-response" title="Learn more about skin conductance response from ScienceDirect's AI-generated Topic Pages" class="topic-link">skin conductance response</a><span> (SCR), a neurophysiological measure of sweat gland activity which is controlled by the sympathetic part of the <a href="/topics/psychology/autonomic-nervous-system" title="Learn more about autonomic nervous system from ScienceDirect's AI-generated Topic Pages" class="topic-link">autonomic nervous system</a>. SCRs could therefore be used to differentiate neural and behavioural effects associated with enhanced levels of arousal from those associated with emotional valence or stimulus type.</span></span></div><figure class="figure text-xs" id="f0005"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr1.jpg" height="499" alt="" aria-describedby="ca0005"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr1_lrg.jpg" target="_blank" download="" title="Download high-res image (193KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (193KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr1.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="ca0005"><p id="sp0010"><span class="label">Fig.&nbsp;1</span>. Mean luminance and spatial frequency values (top) and pre-evaluated pleasantness (bottom left) and arousal (bottom right) ratings for the final collection of stimuli. Pleasantness was rated on a scale of 1 (very unpleasant) to 9 (very pleasant). Arousal was rated on a scale of 1 (very calm) to 9 (very arousing). Error bars represent one standard error of the mean.*&nbsp;=&nbsp;The differences are significant at .05 alpha level.</p></span></span></figure></div><div class="u-margin-s-bottom"><div id="p0045">A secondary aim of this study was to examine whether or not early emotion processing is influenced by the depth of conceptual emotion processing, and, if so, whether such effects might suppress or enhance spontaneous facial reactions. Traditionally, the delayed match-to-sample task involves the ‘passive’ presentation of a first stimulus (e.g. an emotional facial expression) followed by an ‘active’ presentation of a second stimulus, at which point some judgement must be made regarding the second stimulus as a function of the first, usually whether or not they express the same type of emotion. In the current study, we varied the semantic format of emotion recognition between three ‘delayed match-to-sample’ emotion-matching tasks. A consistent presentation format was always used for the first emotional stimulus (i.e. always an emotional picture) while for the second emotional stimulus, the presentation format was varied across tasks to be either another emotional picture, an emotional word or to freely label the depicted emotion (see <a class="anchor anchor-primary" href="#f0010" name="bf0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0010"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;2</span></span></a> for examples of each task). Hence, one version of the task was to compare an emotional picture with another emotional picture; the second version was to compare an emotional picture with an emotional word; and the third task had no second emotional stimulus to compare the first emotional picture with, instead participants had to freely label the depicted emotion.</div><figure class="figure text-xs" id="f0010"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr2.jpg" height="814" alt="" aria-describedby="ca0010"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr2_lrg.jpg" target="_blank" download="" title="Download high-res image (389KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (389KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr2.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="ca0010"><p id="sp0015"><span class="label">Fig.&nbsp;2</span>. <span>An example of the trial structure for each emotion recognition task. The top figure illustrates examples for the versions using emotional <a href="/topics/neuroscience/face" title="Learn more about faces from ScienceDirect's AI-generated Topic Pages" class="topic-link">faces</a>, and the bottom figure illustrates the same trial examples, but using emotional scenes. From the left to right of each figure is an example of the Picture–Picture matching task; the Picture–Word matching; and the Picture-Labelling task. Notice that each task begins exactly the same, with a fixation cross followed by a passively viewed picture (Stimulus 1; a happy, neutral or fearful picture (face or scene, depending on the task version)). After this, the trial structure changed according to type of task being completed. Neural and facial muscle activity during the 3000</span>&nbsp;ms time window corresponding to Stimulus 1 was analysed in this study to determine whether processing of emotional information is differently influenced by the way that information needs to be used.</p></span></span></figure></div><div class="u-margin-s-bottom" id="p0050">Most research using the match-to-sample paradigm focuses on neural or behavioural activity associated with the second ‘active’ stimulus (e.g. <a class="anchor anchor-primary" href="#bb0275" name="bbb0275" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0275"><span class="anchor-text-container"><span class="anchor-text">Hirai et al., 2008</span></span></a>, <a class="anchor anchor-primary" href="#bb0390" name="bbb0390" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0390"><span class="anchor-text-container"><span class="anchor-text">Narumoto et al., 2001</span></span></a><span>). However the focus of the current study was neural and behavioural activity associated with the first passively viewed stimulus. This design specifically allowed us to examine whether, when emotional pictures are viewed under exactly the same presentation conditions, does passive emotion processing and responding vary as a function of the semantic level of emotion recognition? Due to unresolved muscle-related artefact issues in the picture-labelling task, we here focus on <a href="/topics/psychology/electroencephalography" title="Learn more about EEG from ScienceDirect's AI-generated Topic Pages" class="topic-link">EEG</a> and fEMG effects associated with the ‘picture–picture matching’ and ‘picture–word matching’ tasks.</span></div></section><section id="s0010"><h2 id="st0020" class="u-h4 u-margin-l-top u-margin-xs-bottom">Methods</h2><section id="s0015"><h3 id="st0025" class="u-h4 u-margin-m-top u-margin-xs-bottom">Participants</h3><div class="u-margin-s-bottom" id="p0055"><span>Participants were 27 undergraduate students enrolled at the University of Newcastle. Data of four participants were excluded from the analysis due to technical issues with the EMG and <a href="/topics/psychology/skin-conductance" title="Learn more about skin conductance from ScienceDirect's AI-generated Topic Pages" class="topic-link">skin conductance</a> recording equipment (two females and one male) and too few remaining EEG trials after artefact removal (one female). The mean age of the remaining 23 participants is 21</span>&nbsp;years (SD&nbsp;=&nbsp;<span><span>1.72) (17 females). Participants were native speakers of English, right-handed, non-smokers, had no known history of neuropathology and were not taking <a href="/topics/neuroscience/central-nervous-system" title="Learn more about central nervous system from ScienceDirect's AI-generated Topic Pages" class="topic-link">central nervous system</a> targeted medication such as antidepressants or </span><a href="/topics/neuroscience/analeptic" title="Learn more about stimulants from ScienceDirect's AI-generated Topic Pages" class="topic-link">stimulants</a> at the time of testing. Participants provided written informed consent and the project was approved by the University of Newcastle Human Research Ethics Committee [H-2012-0229].</span></div></section><section id="s0020"><h3 id="st0030" class="u-h4 u-margin-m-top u-margin-xs-bottom">Stimuli</h3><div class="u-margin-s-bottom" id="p0060">The 270 happy, fearful and neutral face stimuli were taken from the Radboud Faces Database (RAFD; <a class="anchor anchor-primary" href="#bb0330" name="bbb0330" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0330"><span class="anchor-text-container"><span class="anchor-text">Langner et al., 2010</span></span></a>) and Set A of the Karolinska Directed Emotional Faces Database (KDEF; <a class="anchor anchor-primary" href="#bb0360" name="bbb0360" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0360"><span class="anchor-text-container"><span class="anchor-text">Lundqvist et al., 1998</span></span></a>). For face stimuli, each face was cropped to remove hair, ears, clothing etc. from the image, leaving only the necessary elements of the face for distinguishing an emotional expression. The 270 positive, negative and neutral scene stimuli were taken from the Geneva Affective Picture Database (GAPED; <a class="anchor anchor-primary" href="#bb0115" name="bbb0115" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0115"><span class="anchor-text-container"><span class="anchor-text">Dan-Glauser and Scherer, 2011</span></span></a>) and the International Affective Picture System (IAPS; <a class="anchor anchor-primary" href="#bb0320" name="bbb0320" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0320"><span class="anchor-text-container"><span class="anchor-text">Lang et al., 2005</span></span></a>). Positive scenes included nature scenes, baby animals, appetising food and erotic scenes depicting a male and female embrace. Negative scenes were specifically selected based on a study by <a class="anchor anchor-primary" href="#bb0375" name="bbb0375" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0375"><span class="anchor-text-container"><span class="anchor-text">Mikels et al. (2005)</span></span></a>, which categorised IAPS stimuli into discrete emotion categories including fear. Obvious thematic characteristics of the discrete IAPS fear collection, such as spiders and snakes, were then used as a basis for selecting negative (mainly fearful) scenes from the GAPED database, for which no discrete emotional categorisation exists. For neutral scenes, we specifically chose stimuli that visually represented neutrality (e.g. a stair case, computer, light bulb), because valence ratings are not accurate predictors of emotional categorisation (<a class="anchor anchor-primary" href="#bb0045" name="bbb0045" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0045"><span class="anchor-text-container"><span class="anchor-text">Blairy et al., 1999</span></span></a>). Scene stimuli featuring a forward-facing face were excluded from the scenes collection.</div><div class="u-margin-s-bottom" id="p0065">The stimuli collections were rated on levels of valence and arousal in a pilot study of a larger pool of images using an independent group of 42 participants (23 females) with a mean age of 25&nbsp;years (<em>SD</em>&nbsp;=&nbsp;4.61). Participants rated equal samples of face and scene stimuli using the Self-assessment manikin (SAM; <a class="anchor anchor-primary" href="#bb9010" name="bbb9010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb9010"><span class="anchor-text-container"><span class="anchor-text">Bradley and Lang, 1994</span></span></a>). Pictures with ratings that best balanced valence and arousal levels across stimulus categories were then chosen to be included in the study. Mean valence and arousal ratings in addition to luminance and spatial frequency values for the final collection are displayed in <a class="anchor anchor-primary" href="#f0005" name="bf0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0005"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;1</span></span></a>.</div><div class="u-margin-s-bottom" id="p0070">There is existing empirical evidence showing that low level physical features of visual stimuli such as luminance and spatial frequency modify early brain activities (e.g. <a class="anchor anchor-primary" href="#bb0015" name="bbb0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0015"><span class="anchor-text-container"><span class="anchor-text">Alorda et al., 2007</span></span></a>, <a class="anchor anchor-primary" href="#bb0120" name="bbb0120" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0120"><span class="anchor-text-container"><span class="anchor-text">De Cesarei and Codispoti, 2012</span></span></a>, <a class="anchor anchor-primary" href="#bb0135" name="bbb0135" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0135"><span class="anchor-text-container"><span class="anchor-text">Delplanque et al., 2007</span></span></a>), a phenomenon also known as exogenous brain activity effects (<a class="anchor anchor-primary" href="#bb0170" name="bbb0170" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0170"><span class="anchor-text-container"><span class="anchor-text">Donchin, 1978</span></span></a>). Since our motivation was focused on early brain activity effects related to emotion-specific content of faces versus scenes it is important to look at physical features of all visual stimuli, in particular luminance and spatial frequency and to test whether or not potential differences in luminance and spatial frequency across stimuli could theoretically explain any early brain activity differences that are described in the frame of this paper.</div><div class="u-margin-s-bottom" id="p0075"><span>For this purpose we ran a spatial frequency analysis of all our images and calculated analytic statistics to test whether or not spatial frequencies differed between stimulus categories (faces and scenes) and also between emotion categories. An <a href="/topics/psychology/analysis-of-variance" title="Learn more about ANOVA from ScienceDirect's AI-generated Topic Pages" class="topic-link">ANOVA</a> including all spatial frequency values was run and revealed a highly significant main stimulus category effect (p</span>&nbsp;&lt;&nbsp;.001) and a significant main emotion category effect (p&nbsp;=&nbsp;.009). However, the interaction of both factors was not significant (p&nbsp;=&nbsp;.097) (all Greenhouse–Geisser corrected). The pattern of these results is understood as demonstrating that mean spatial frequencies of our images differ between faces and scenes. In addition, spatial frequencies of our images differ as a function of emotion category, but the way they differ between emotion categories does not depend on stimulus category. Descriptive statistics shows that scenes had overall higher spatial frequencies than faces (reflected in the main stimulus category effect), which we interpret as a result of higher complexity of scenes compared to faces. In both stimulus categories it can be seen that neutral images are associated with lower spatial frequency values compared to both positive and negative emotion categories (reflected in the main emotion category effect and the not significant interaction of both factors).</div><div class="u-margin-s-bottom" id="p0080">Further, t-tests revealed significant differences between spatial frequencies of neutral and negative faces (p&nbsp;=&nbsp;.010; T&nbsp;=&nbsp;2.618), also neutral and positive faces (p&nbsp;=&nbsp;.047; T&nbsp;=&nbsp;−&nbsp;2.009), but not negative and positive faces (p&nbsp;=&nbsp;.557; T&nbsp;=&nbsp;.590). Spatial frequencies of negative scenes don't differ from those of neutral scenes (p&nbsp;=&nbsp;.312; T&nbsp;=&nbsp;1.017), but they do differ between neutral and positive scenes (p&nbsp;=&nbsp;.009; T&nbsp;=&nbsp;−&nbsp;2.679). No differences are found between positive and negative scenes (p&nbsp;=&nbsp;.107; T&nbsp;=&nbsp;−&nbsp;1.626).</div><div class="u-margin-s-bottom" id="p0085">In summary, positive and negative stimuli are associated with higher spatial frequencies than neutral stimuli, which is true for both stimulus categories. In general, scenes are associated with higher spatial frequencies compared to faces (see <a class="anchor anchor-primary" href="#f0005" name="bf0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0005"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;1</span></span></a>).</div><div class="u-margin-s-bottom" id="p0090">It also turned out that luminance differences exist. An ANOVA revealed a significant main stimulus category effect on luminance values (F&nbsp;=&nbsp;10.106; p&nbsp;=&nbsp;.002). There is also a highly significant emotion category effect (F&nbsp;=&nbsp;15.911; p&nbsp;&lt;&nbsp;.001), but a not significant interaction of those two factors (F&nbsp;=&nbsp;2.931; p&nbsp;=&nbsp;.058) (all Greenhouse–Geisser corrected). Similar to spatial frequency data luminance data also demonstrate that differences exist between emotion categories, but that these differences do not depend on stimulus category. Paired-sample T-Tests revealed that image luminance differs significantly between fearful faces and negative scenes (t&nbsp;=&nbsp;4.177; p&nbsp;&lt;&nbsp;.001). No other differences were found to be significant (see <a class="anchor anchor-primary" href="#f0005" name="bf0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0005"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;1</span></span></a>).</div><div class="u-margin-s-bottom" id="p0095">Overall, we can summarise that both physical features show similar patterns in terms of how they differ across stimulus and emotion categories. There are emotion-specific differences in physical features, but those differences are independent from stimulus category. In other words, any stimulus category effect on brain activities can theoretically be explained by differences in physical image features, but stimulus-specific emotion category effects cannot.</div><div class="u-margin-s-bottom" id="p0100">For instance, taking a closer look at the present early EEG effects we notice that the P1 stimulus category effects could theoretically be explained by spatial frequency and/or luminance differences of our stimuli, but this is not the case for the task-dependent effects and also not for all of the emotion-specific brain activities that differ between faces and scenes as they were found in later time windows.</div></section><section id="s0025"><h3 id="st0035" class="u-h4 u-margin-m-top u-margin-xs-bottom">Tasks and procedure</h3><div class="u-margin-s-bottom" id="p0105">During individual testing sessions, participants sat in a reclining chair under dim lighting and positioned in front of a display monitor to allow 9.9°&nbsp;×&nbsp;8.5° of visual angle (300&nbsp;×&nbsp;399&nbsp;pixels). After being connected to the recording equipment, participants completed three delayed match-to-sample emotion recognition tasks in random order. Each task was completed once with face stimuli and once with scene stimuli (blocked sessions randomised within tasks; <a class="anchor anchor-primary" href="#f0010" name="bf0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0010"><span class="anchor-text-container"><span class="anchor-text">Figs.&nbsp;2</span></span></a>a and b). At the beginning of the session participants were informed of what each task involved so as to minimise potential practice effects of task order bias. Then for each task, instructions for that task were repeated and six practice trials were completed. Practice trials were not included in the analyses.</div><div class="u-margin-s-bottom" id="p0110">Trials in each task always began with a fixation cross followed by a happy, neutral or fearful picture (Stimulus 1; ‘S1’) for 3000&nbsp;ms, which required no overt response from the participant other than to simply view the picture. S1 was proceeded by a second stimulus (Stimulus 2; ‘S2’) which required an active response. For one of the tasks (Picture–Picture matching; ‘Pic–Pic’), the S2 was another happy, neutral or fearful picture, while for another one (Picture–Word matching; ‘Pic–Word’), the S2 was one of the three emotion category labels ‘fear’, ‘neutral’ or ‘happy’ presented in block white letters against a black background. The active response for the Pic–Pic and Pic–Word tasks was a forced choice (match/mismatch) judgement of whether S1 and S2 represented the same emotion (happy, fear or neutral) by pressing one of two buttons (‘Z’ or ‘/’) with the corresponding index finger as quickly as possible without forgoing accuracy. For a third task (Picture labelling; ‘Pic-Label’), the S2 was the symbol ‘?’ which cued the participant to say out loud any one word that best described the emotion depicted in S1. For the Pic-Label task, participants continued to the next trial by pressing the space bar. Key responses at the end of each trial cued a 1000&nbsp;ms inter-trial interval before the next trial began. It should be emphasised that the S1 presentation conditions were identical across the three tasks, i.e., the S1 was always a passively viewed picture. Hence, the S2 event served as the experimental manipulation to examine whether emotional information (S1) is processed differently depending on the emotion context of S2 and the task. Accordingly, the event of interest for the analysis was the 3000&nbsp;ms time window corresponding to the S1 presentation.</div><div class="u-margin-s-bottom" id="p0115">There were 90 trials for each face and scene version of the tasks (30 fearful (negative), 30 neutral and 30 happy (positive) S1 presentations). All S1 pictures were novel (i.e., no picture was presented more than once) and were presented in colour (as were the S2 Pic–Pic pictures). S1 stimuli were randomly presented within tasks, and for the Pic–Pic and Pic–Word tasks, S2 items were also randomised. Hence, each S1–S2 pairing was randomly generated, however the frequency of match/mismatch and S1–S2 emotion category combinations was balanced. Participants were given a short break midway and at the end of each 90-trial task. The experiment took approximately one hour to complete.</div></section><section id="s0030"><h3 id="st0040" class="u-h4 u-margin-m-top u-margin-xs-bottom">Measures and data reduction</h3><div class="u-margin-s-bottom" id="p0120">Due to the different nature and dynamics of biosignals recorded in the frame of this study we chose different epoch lengths for the different measures. Since we focus on early brain activity changes we set maximum epoch length to 1&nbsp;s, but actually display ERPs only until 400&nbsp;ms post stimulus, because during this period early changes occur. Facial EMG epochs were set to 1.5&nbsp;s to potentially capture later effects and skin conductance epochs were set to 4&nbsp;s, because of their less dynamic nature and their delay (see more details below).</div><section id="s0035"><h4 id="st0045" class="u-margin-m-top u-margin-xs-bottom">EEG recordings</h4><div class="u-margin-s-bottom" id="p0125">Scalp EEG, measured in micro Volts (μV), was recorded using a 64 channel Biosemi cap and amplifier (<a class="anchor anchor-primary" href="http://www.Biosemi.com" target="_blank"><span class="anchor-text-container"><span class="anchor-text">http://www.Biosemi.com</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>) sampled continuously at 2048&nbsp;Hz using an electrode layout corresponding to the 10-10-electrode placement standard, and referenced to a common-mode signal. The data was down-sampled offline to 256&nbsp;Hz and filtered from 0.1 to 30&nbsp;Hz using EEG Display software. Eye blink artefacts were corrected using a set of linear regression weights at each EEG electrode derived from an averaged eye blink (<a class="anchor anchor-primary" href="#bb0520" name="bbb0520" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0520"><span class="anchor-text-container"><span class="anchor-text">Semlitsch et al., 1986</span></span></a>). Segments of the EEG record containing gross artefact were detected by an automated procedure that applied amplitude thresholds within a number of frequency bands. For each S1 stimulus, EEG epochs were extracted from 100&nbsp;ms pre-stimulus to 1000&nbsp;ms post-stimulus, and baseline corrected across the pre-stimulus interval. Trials containing artefact exceeding ±&nbsp;100&nbsp;μV were excluded. Finally, trials were averaged to produce the ERP for each image type.</div><div class="u-margin-s-bottom" id="p0130">Noisy channels were interpolated prior to data reduction for five participants, involving no more than one electrode within each analysed cluster. The first two trials completed during each face and scene recognition task were removed. On average, 14% of trials were removed (4/30 trials per condition for the 12 analysed conditions, SD&nbsp;=&nbsp;2.92). Trials were then group-averaged to create a single waveform per condition at each electrode location and re-referenced to an average of all electrodes excluding the mastoid and ocular sites.</div><div class="u-margin-s-bottom" id="p0135">The event-related components of interest were identified by visual inspection of the electrode montage to identify clusters with prominent activity, and then by software-facilitated comparisons to verify the exact electrode locations of peak amplitude deflection. The high number of factors being analysed increased the likelihood of generating false positive effects. To reduce this risk, we employed procedures similar to those used by <a class="anchor anchor-primary" href="#bb0490" name="bbb0490" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0490"><span class="anchor-text-container"><span class="anchor-text">Schupp et al. (2003)</span></span></a> for calculating the grand mean of activity evoked during experimental conditions for each component analysed. These included averaging the activity of two electrode locations showing the greatest peak deflection, and then averaging over a time interval of at least 10 data samples (40&nbsp;ms) centred over the peak. The P1 component peaked over posterior–occipital electrodes PO7/O1 and PO8/O2at 130&nbsp;ms on average for face stimuli, approximately 20&nbsp;ms earlier than for scene stimuli, which had an average latency of 150&nbsp;ms. For each participant a single mean amplitude for each of the 12 experimental conditions was calculated for the 120–160&nbsp;ms time interval. At temporal–occipital regions face stimuli elicited a prominent N170 component, while scene stimuli elicited only a small N100. These components were immediately followed by a slower progressive negative shift, the so called early posterior negativity (EPN). The N100/N170 and EPN were most prominent over temporal–occipital electrode locations P7/P9 and P8/P10. For each experimental condition, a single mean amplitude was again calculated for each participant for the 150–190&nbsp;ms time interval corresponding to the N100/N170 and for the 220–280&nbsp;ms time interval corresponding to the EPN.</div></section><section id="s0040"><h4 id="st0050" class="u-margin-m-top u-margin-xs-bottom">fEMG recordings</h4><div class="u-margin-s-bottom" id="p0140">The corrugator supercilii (CS) muscles, which furrow the eyebrows, were used to reference muscle potential changes corresponding to face and scene stimuli of negative content (see <a class="anchor anchor-primary" href="#bb0190" name="bbb0190" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0190"><span class="anchor-text-container"><span class="anchor-text">Ekman and Friesen (1978)</span></span></a>). The zygomaticus major (ZM) muscles, which lift the cheeks and lips were used to reference muscle potential changes corresponding to face and scene of positive content. fEMG of the CS and ZM, measured in micro Volts (μV), was recorded using a NeXus-10 wireless amplifier (<a class="anchor anchor-primary" href="http://www.Mindmedia.com" target="_blank"><span class="anchor-text-container"><span class="anchor-text">http://www.Mindmedia.com</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>) connected via Bluetooth to a PC laptop, and output measurements were recorded using the NeXus-customised Biotrace&nbsp;+&nbsp;Software. A NeXus Trigger Interface was used to synchronise the onset of trial events between the continuous EEG and EMG recordings to within less than 1&nbsp;ms accuracy (<a class="anchor anchor-primary" href="http://www.Mindmedia.com" target="_blank"><span class="anchor-text-container"><span class="anchor-text">http://www.Mindmedia.com</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>).</div><div class="u-margin-s-bottom" id="p0145">Bipolar electromyography (EMG) was used to record muscle potential changes of both muscles on both sides of the face. We used dual channel electrode cables with carbon coating and active shielding technology for low noise and an additional ground electrode cable attached to the back of the neck (see <a class="anchor anchor-primary" href="#bb0450" name="bbb0450" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0450"><span class="anchor-text-container"><span class="anchor-text">Reaz et al., 2006</span></span></a>, <a class="anchor anchor-primary" href="#bb0570" name="bbb0570" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0570"><span class="anchor-text-container"><span class="anchor-text">Wand, 2015</span></span></a>). The EMG sampling rate was 2048&nbsp;Hz. A band pass filter from 20&nbsp;Hz to 500&nbsp;Hz was applied during online recording. Raw EMG data were then recalculated by using the root mean square (RMS) method (epoch-size&nbsp;=&nbsp;1/16&nbsp;s) to transform EMG signals into amplitudes.</div><div class="u-margin-s-bottom" id="p0150">The resulting amplitudes were then subject to statistical analysis. Using a Matlab based program (<a class="anchor anchor-primary" href="http://www.mathworks.com" target="_blank"><span class="anchor-text-container"><span class="anchor-text">www.mathworks.com</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>), a single 1750&nbsp;ms epoch time-locked to 250&nbsp;ms preceding the onset of each S1 stimulus presentation was then extracted and divided into seven 250&nbsp;ms time intervals by averaging across data points. The first time window (−&nbsp;250–0&nbsp;ms) served as a baseline correction for the six following intervals (0–250, 250–500, 500–750, 750–1000, 1000–1250, 1250–1500&nbsp;ms) which were the subject of the analysis. After the removal of gross artefacts, the time windows were baseline corrected and the first two trials for each face and scene recognition task were removed from analysis. An inspection of within-trial and across-trial variance was carried out for each data set using an outlier criterion of 3.5 SD or greater. On average, 18% of trials were removed from the ZM data (5/30 trials per condition for the 12 analysed conditions, SD&nbsp;=&nbsp;3.56) and 13% of trials were removed from the CS data (4/30 trials per condition, SD&nbsp;=&nbsp;3.35).</div></section><section id="s0045"><h4 id="st0055" class="u-margin-m-top u-margin-xs-bottom">Skin conductance recordings</h4><div class="u-margin-s-bottom" id="p0155">Skin conductance was recorded at a rate of 32&nbsp;Hz with a Nexus-10-SC/GSR sensor (Two finger sensor) connected to the Nexus-10 recording system with a 24 bit resolution which is able to register changes of less than 0.0001&nbsp;μS. Because the galvanic skin response is slow-changing, a 4250&nbsp;ms epoch was extracted, time-locked to 250&nbsp;ms preceding the onset of S1, with −&nbsp;250 to 0&nbsp;ms serving as the baseline correction interval. The residual was divided into four 1000&nbsp;ms time intervals for further analysis (0–1000, 1000–2000, 2000–3000, 3000–4000&nbsp;ms). Like with EMG data, after the removal of gross artefacts, the time windows were baseline corrected and the first two trials for each face and scene recognition task were removed from analysis. An inspection of within-trial and across-trial variance was carried out for each data set using an outlier criterion of 3.5 SD or greater. On average, 22% of trials were removed (7/30 trials per condition, SD&nbsp;=&nbsp;3.56).</div></section></section><section id="s0050"><h3 id="st0060" class="u-h4 u-margin-m-top u-margin-xs-bottom">Statistical analyses</h3><div class="u-margin-s-bottom" id="p0160">The analysis was a fully within-subjects design with three factors: Stimulus type (Faces, Scenes)&nbsp;×&nbsp;Emotion (Fear (negative), Neutral, Happy (positive))&nbsp;×&nbsp;Task (Pic–Pic, Pic–Word). Note again that the Pic-Label task was not included in the current analysis. For each event-related potential (ERP) component of interest, condition grand means for the extracted time intervals were subject to a 4-way Stimulus type&nbsp;×&nbsp;Emotion&nbsp;×&nbsp;Task&nbsp;×&nbsp;<span>Hemisphere (Left, Right) repeated measures analysis of variance (RM ANOVA). For the ZM, CS and <a href="/topics/psychology/skin-conductance-response" title="Learn more about SCR from ScienceDirect's AI-generated Topic Pages" class="topic-link">SCR</a> analyses, condition grand means for each time interval (six time intervals for ZM and CS, and four for SCR) were subject to 3-way Stimulus type</span>&nbsp;×&nbsp;Emotion&nbsp;×&nbsp;Task RM ANOVAs. Significant interactions between Stimulus type and Emotion (<em>p</em>&nbsp;&lt;&nbsp;.05) were further investigated where appropriate with secondary RM ANOVAs, conducted separately for each stimulus type. All other significant interactions involving the factor Stimulus type (<em>p</em>&nbsp;&lt;&nbsp;.05) were further investigated with paired-samples <em>t</em>-tests with bonferroni alpha corrections. For Sphericity violations (<em>p</em>&nbsp;&lt;&nbsp;.05), Greenhouse–Geisser epsilon adjustments were applied if ϵ&nbsp;&lt;&nbsp;.75, otherwise Hyundt–Feldt. All main effects are reported, however because the primary objective of the analysis was to investigate differences in emotional face and scene processing, only the interactions involving the factor Stimulus type are reported.</div></section></section><section id="s0055"><h2 id="st0065" class="u-h4 u-margin-l-top u-margin-xs-bottom">Results</h2><section id="s0060"><h3 id="st0070" class="u-h4 u-margin-m-top u-margin-xs-bottom">EEG data</h3><section id="s0065"><h4 id="st0075" class="u-margin-m-top u-margin-xs-bottom">P1 component</h4><div class="u-margin-s-bottom"><div id="p0165">Emotional content did not affect P1 amplitudes, however the type of stimulus and task did. In addition to the different latencies at which the P1 emerged for faces and scenes, significant main effects also emerged for the factors Stimulus type (<em>F</em> (1, 22)&nbsp;=&nbsp;15.64, <em>p</em>&nbsp;=&nbsp;.001, <em>η2</em>&nbsp;=&nbsp;.42) and Task (<em>F</em> (1, 22)&nbsp;=&nbsp;24.89, <em>p</em>&nbsp;&lt;&nbsp;.001, <em>η2</em>&nbsp;=&nbsp;.53). As can be seen in the top left and right waveforms in <a class="anchor anchor-primary" href="#f0015" name="bf0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0015"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;3</span></span></a>, scenes evoked a larger mean P1 deflection compared to faces, while pictures viewed during the Pic–Pic task produced larger mean P1 deflections compared to the Pic–Word task. The factors Stimulus type and Task also interacted significantly (<em>F</em> (1, 22)&nbsp;=&nbsp;5.43, <em>p</em>&nbsp;=&nbsp;.029, <em>η2</em>&nbsp;=&nbsp;.20), indicating that the effect of the Pic–Pic task on P1 amplitudes was greater for scenes than for faces (see also topography maps in <a class="anchor anchor-primary" href="#f0015" name="bf0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0015"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;3</span></span></a>, far right).</div><figure class="figure text-xs" id="f0015"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr3.jpg" height="597" alt="" aria-describedby="ca0015"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr3_lrg.jpg" target="_blank" download="" title="Download high-res image (292KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (292KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr3.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="ca0015"><p id="sp0020"><span class="label">Fig.&nbsp;3</span>. <span>Effects of the Pic–Pic and Pic–Word tasks on <a href="/topics/psychology/early-visual-processing" title="Learn more about early visual processing from ScienceDirect's AI-generated Topic Pages" class="topic-link">early visual processing</a><span> of emotional <a href="/topics/neuroscience/face" title="Learn more about faces from ScienceDirect's AI-generated Topic Pages" class="topic-link">faces</a> and scenes. For quick reference, examples of the tasks are shown in the top right corner. Waveforms show grand-averaged ERPs collapsed across emotion categories and time-locked to the onset of the passively viewed Stimulus 1 (onset</span></span>&nbsp;=&nbsp;0&nbsp;ms). Waveforms in the top panel represent brain activity recorded at left and right posterior–occipital electrode regions. At the far right are corresponding topographic maps of P1-related brain activity. Waveforms in the bottom panel represent brain activity recorded at left and right lateral occipital electrode regions. L&nbsp;=&nbsp;Left hemisphere. R&nbsp;=&nbsp;Right hemisphere.</p></span></span></figure></div></section><section id="s0070"><h4 id="st0080" class="u-margin-m-top u-margin-xs-bottom">N170 component (faces)/N100 component (scenes)</h4><div class="u-margin-s-bottom" id="p0170">The factor Stimulus type produced a strongly significant main effect (<em>F</em> (1, 22)&nbsp;=&nbsp;324.22, <em>p</em>&nbsp;&lt;&nbsp;.001, <em>η2</em>&nbsp;=&nbsp;.94) indicating that, in line with past research, faces evoked a much larger negative deflection in the N100 time window, the so called N170 component, compared to scenes. A significant main effect of Task was also observed (<em>F</em> (1, 22)&nbsp;=&nbsp;33.60, <em>p</em>&nbsp;&lt;&nbsp;.001, <em>η2</em>&nbsp;=&nbsp;.60), indicating that pictures viewed during the Pic–Word task produced larger mean N100/N170 deflections compared to the Pic–Pic task (<a class="anchor anchor-primary" href="#f0015" name="bf0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0015"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;3</span></span></a> bottom left and right waveforms).</div><div class="u-margin-s-bottom"><div id="p0175">Stimulus type also interacted separately with the factors Hemisphere (<em>F</em> (1, 22)&nbsp;=&nbsp;17.64, <em>p</em>&nbsp;=&nbsp;&lt;&nbsp;.001, <em>η2</em>&nbsp;=&nbsp;.45) and Emotion (<em>F</em> (2, 44)&nbsp;=&nbsp;7.63, <em>p</em>&nbsp;=&nbsp;.001, <em>η2</em>&nbsp;=&nbsp;.26). Effects of emotion content are shown in <a class="anchor anchor-primary" href="#f0020" name="bf0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0020"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;4</span></span></a> top left and right waveforms, and bar graphs depicting the mean activity across the categories are also displayed at the bottom left. The Stimulus type&nbsp;×&nbsp;Hemisphere interaction indicated that regardless of task or emotional expression, face stimuli produced greater activity over the right hemisphere relative to left compared to scenes, which produced no observable lateralised effects. However, note that when we followed up this effect by comparing the total mean activity produced by faces over left and right hemispheres (i.e. collapsing the means of emotion and task conditions), right hemispheric activity was only marginally greater than left (<em>p</em>&nbsp;=&nbsp;.059). More critically, the Stimulus type&nbsp;×&nbsp;<span>Emotion interaction was further investigated using separate secondary <a href="/topics/psychology/analysis-of-variance" title="Learn more about ANOVAs from ScienceDirect's AI-generated Topic Pages" class="topic-link">ANOVAs</a> for each stimulus type. These ANOVAs showed that the N100 was not sensitive to the emotional content of scenes (</span><em>F</em> (2, 44)&nbsp;=&nbsp;1.64, <em>p</em>&nbsp;=&nbsp;.206, <em>η2</em>&nbsp;=&nbsp;.07), but that the N170 was differently modulated depending on the emotional facial expression (<em>F</em> (2, 44)&nbsp;=&nbsp;6.27, <em>p</em>&nbsp;=&nbsp;.004, <em>η2</em>&nbsp;=&nbsp;.22). Contrasts confirmed that fearful faces evoked significantly (<em>p</em>&nbsp;=&nbsp;.004; <em>p</em>&nbsp;=&nbsp;.043) more negative N170 amplitudes compared to neutral and happy faces, respectively, while amplitudes for happy and neutral facial expressions were not different (<em>p</em>&nbsp;=&nbsp;.131).<a class="anchor anchor-primary" href="#fn0005" name="bfn0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fn0005"><span class="anchor-text-container"><span class="anchor-text"><sup>1</sup></span></span></a></div><figure class="figure text-xs" id="f0020"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr4.jpg" height="696" alt="" aria-describedby="ca0020"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr4_lrg.jpg" target="_blank" download="" title="Download high-res image (269KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (269KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr4.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="ca0020"><p id="sp0025"><span class="label">Fig.&nbsp;4</span>. <span>Effects of emotion category on <a href="/topics/psychology/early-visual-processing" title="Learn more about early visual processing from ScienceDirect's AI-generated Topic Pages" class="topic-link">early visual processing</a> of emotional faces and scenes. In the top panel, waveforms represent brain activity recorded at left and right lateral occipital regions and show grand-averaged ERPs collapsed across task categories and time-locked to the onset of the passively viewed Stimulus 1 (onset</span>&nbsp;=&nbsp;0&nbsp;ms). The bar graph at the bottom left illustrates the mean N170-related activity averaged across left and right hemispheres. The bar graph at the bottom right illustrates EPN-related activity recorded over left and right hemispheres. Error bars represent one standard error of the mean. *&nbsp;=&nbsp;The differences are significant after Bonferroni corrections. L&nbsp;=&nbsp;Left hemisphere. R&nbsp;=&nbsp;Right hemisphere.</p></span></span></figure></div></section><section id="s0075"><h4 id="st0085" class="u-margin-m-top u-margin-xs-bottom">EPN time window</h4><div class="u-margin-s-bottom"><div id="p0180">Waveforms corresponding to the EPN are also presented in <a class="anchor anchor-primary" href="#f0020" name="bf0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0020"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;4</span></span></a> (top left and right waveforms), and bar graphs depicting the mean activity across the categories are displayed at the bottom right. Main effects of Stimulus type (<em>F</em> (1, 22)&nbsp;=&nbsp;412.62, <em>p</em>&nbsp;&lt;&nbsp;.001, <em>η2</em>&nbsp;=&nbsp;.95), Task (<em>F</em> (1, 22)&nbsp;=&nbsp;19.08, <em>p</em>&nbsp;&lt;&nbsp;.001, <em>η2</em>&nbsp;=&nbsp;.46), and Emotion (<em>F</em> (1.70, 37.38)&nbsp;=&nbsp;11.60, <em>p</em>&nbsp;&lt;&nbsp;.001, <em>η</em>2&nbsp;=&nbsp;.35, with sphericity corrections χ2&nbsp;=&nbsp;6.16, ϵ&nbsp;=&nbsp;.85, <em>p</em>&nbsp;=&nbsp;.046) re-emerged, as did the interaction between Stimulus type and Emotion (<em>F</em> (2, 44)&nbsp;=&nbsp;43.11, <em>p</em>&nbsp;&lt;&nbsp;.001, <em>η2</em>&nbsp;=&nbsp;.66). Additionally, a significant main effect of Hemisphere emerged (<em>F</em> (1, 22)&nbsp;=&nbsp;5.70, <em>p</em>&nbsp;=&nbsp;.026, <em>η2</em>&nbsp;=&nbsp;.21), which led to a three-way interaction between Stimulus type, Emotion, and Hemisphere (<em>F</em> (2, 44)&nbsp;=&nbsp;5.54, <em>p</em>&nbsp;=&nbsp;.007, <em>η2</em>&nbsp;=&nbsp;.20). Accordingly, secondary ANOVAs of each stimulus type were performed with Emotion and Hemisphere as within-subjects factors. For faces, Emotion (<em>F</em> (2, 44)&nbsp;=&nbsp;11.63, <em>p</em>&nbsp;&lt;&nbsp;.001, <em>η2</em>&nbsp;=&nbsp;.35) and Hemisphere (<em>F</em> (1, 22)&nbsp;=&nbsp;4.53, <em>p</em>&nbsp;=&nbsp;.045, <em>η2</em>&nbsp;=&nbsp;.17) produced significant main effects but did not interact (<em>p</em>&nbsp;=&nbsp;.171). Contrasts confirmed that the negative-going shift in activity during fearful face presentations was significantly greater than during happy and (<em>p</em>&nbsp;&lt;&nbsp;.001) and neutral (<em>p</em>&nbsp;=&nbsp;.013) face presentations. The effect of Hemisphere further indicated greater negative-going activity over the left hemisphere, but see below for a more thorough interpretation. For scene stimuli, Emotion (<em>F</em> (2, 44)&nbsp;=&nbsp;41.96, <em>p</em>&nbsp;&lt;&nbsp;.001, <em>η2</em>&nbsp;=&nbsp;.66) and Hemisphere (<em>F</em> (1, 22)&nbsp;=&nbsp;5.95, <em>p</em>&nbsp;=&nbsp;.023, <em>η2</em>&nbsp;=&nbsp;.21) produced significant main effects and did interact significantly (<em>F</em> (2, 44)&nbsp;=&nbsp;3.74, <em>p</em>&nbsp;=&nbsp;.032, <em>η2</em>&nbsp;=&nbsp;.15). The EPN was differently sensitive to emotional content in scenes than in faces however, with positive scenes eliciting significantly greater negative-going activity compared to negative and neutral scenes (both <em>p</em>'s&nbsp;&lt;&nbsp;.001). Critically, the interaction of Emotion and Hemisphere evoked by scenes revealed that although scenes (and faces) generated more negativity over the left hemisphere, there was greater discrimination of positive from negative scenes over the right hemisphere (<em>p</em>&nbsp;=&nbsp;.005), hence the EPN was more robust over the right hemisphere. In short, fearful faces and positive scenes elicited a stronger EPN overall compared to other emotional stimuli, but positive scenes also elicited a lateral difference in EPN magnitude whereas fearful faces did not. See <a class="anchor anchor-primary" href="#t0005" name="bt0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0005"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;1</span></span></a> summarising all EEG-related statistical significancies.</div><div class="tables frame-topbot colsep-0 rowsep-0" id="t0005"><span class="captions text-s"><span id="ca0040"><p id="sp0045"><span class="label">Table&nbsp;1</span>. Summary of significant factor main effects and/or significant factor interactions related to EEG data.</p></span></span><div class="groups"><table><thead><tr class="valign-top rowsep-1"><th scope="col" class="align-left">EEG</th><th scope="col" class="align-left">P1</th><th scope="col" class="align-left">N170/N100</th><th scope="col" class="align-left">EPN</th></tr></thead><tbody><tr class="valign-top"><th class="align-left" scope="row"><em>Task</em></th><td></td><td class="align-left">P&nbsp;&lt;&nbsp;.001</td><td class="align-left">P&nbsp;&lt;&nbsp;.001</td></tr><tr class="valign-top"><th class="align-left" scope="row"><em>Emotion</em></th><td></td><td></td><td class="align-left">P&nbsp;&lt;&nbsp;.001</td></tr><tr class="valign-top"><th class="align-left" scope="row"><em>Stimulus</em></th><td class="align-left">P&nbsp;&lt;&nbsp;.001</td><td class="align-left">P&nbsp;&lt;&nbsp;.001</td><td class="align-left">P&nbsp;&lt;&nbsp;.001</td></tr><tr class="valign-top"><th class="align-left" scope="row"><em>Hemisphere</em></th><td></td><td></td><td class="align-left">P&nbsp;=&nbsp;.026</td></tr><tr class="valign-top"><th class="align-left" scope="row"><em>stimulus * task</em></th><td class="align-left">P&nbsp;=&nbsp;0.29</td><td></td><td></td></tr><tr class="valign-top"><th class="align-left" scope="row"><em>stimulus * hemisphere</em></th><td></td><td class="align-left">P&nbsp;&lt;&nbsp;.001</td><td></td></tr><tr class="valign-top"><th class="align-left" scope="row"><em>stimulus * emotion</em></th><td></td><td class="align-left">P&nbsp;=&nbsp;.001</td><td class="align-left">P&nbsp;&lt;&nbsp;.001</td></tr><tr class="valign-top"><th class="align-left" scope="row"><em>stimulus* emotion * hemisphere</em></th><td></td><td></td><td class="align-left">P&nbsp;=&nbsp;.007</td></tr></tbody></table></div></div></div></section></section><section id="s0080"><h3 id="st0090" class="u-h4 u-margin-m-top u-margin-xs-bottom">EMG data</h3><section id="s0085"><h4 id="st0095" class="u-margin-m-top u-margin-xs-bottom">Zygomatic recordings</h4><div class="u-margin-s-bottom"><div id="p0185">No main effects emerged from the six ANOVAs, however a significant three-way interaction between Stimulus type, Emotion and Task emerged at the third time interval corresponding to the time between 500 and 750&nbsp;ms post-stimulus (<em>F</em> (2, 44)&nbsp;=&nbsp;3.22, <em>p</em>&nbsp;=&nbsp;.049, <em>η2</em>&nbsp;=&nbsp;.13) followed by a sustained interaction between Stimulus type and Emotion over the next three time intervals (750–1500&nbsp;ms; <em>F</em> (2, 44)&nbsp;=&nbsp;4.54, 6.45, 4.85, <em>p</em>&nbsp;=&nbsp;.016, .003, .012, <em>η2</em>&nbsp;=&nbsp;.17, .23, .18, respectively). As seen in <a class="anchor anchor-primary" href="#f0025" name="bf0025" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0025"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;5</span></span></a>, these interactions collectively indicated that positive scenes evoked spontaneous ZM activity, while happy faces did not. The initial three-way interaction between Stimulus type, Emotion and Task also suggested that positive scenes elicited spontaneous activity earlier during the Pic–Word task compared to the Pic–Pic task. However, when the corresponding data for scene stimuli was submitted to a secondary ANOVA with Task and Emotion as the within-subjects factors, the modulatory effect of Task disappeared (<em>p</em>&nbsp;=&nbsp;.202), and, consistent with the effects at ensuing time intervals, was replaced with a significant Stimulus type&nbsp;×&nbsp;Emotion interaction (<em>F</em> (2, 44)&nbsp;=&nbsp;3.62, <em>p</em>&nbsp;=&nbsp;.035, <em>η2</em>&nbsp;=&nbsp;.14). As expected, the secondary ANOVA for face stimuli revealed no effects of task or emotion (all p-values&nbsp;&gt;&nbsp;.2)</div><figure class="figure text-xs" id="f0025"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr5.jpg" height="393" alt="" aria-describedby="ca0025"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr5_lrg.jpg" target="_blank" download="" title="Download high-res image (128KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (128KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr5.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="ca0025"><p id="sp0030"><span class="label">Fig.&nbsp;5</span>. <span>Mean zygomatic muscle <a href="/topics/psychology/electromyography" title="Learn more about EMG from ScienceDirect's AI-generated Topic Pages" class="topic-link">EMG</a> amplitudes (μV) and error bars for 1 standard error, time-locked to the onset of the passively viewed Stimulus 1 (onset</span>&nbsp;=&nbsp;0&nbsp;ms). *&nbsp;=&nbsp;The differences are significant after Bonferroni corrections.</p></span></span></figure></div><div class="u-margin-s-bottom"><div id="p0190">Paired samples <em>t</em>-tests were used to determine significant fluctuations in ZM activity between emotional scene categories at each time interval for intervals three to six (corrected significance threshold&nbsp;=&nbsp;.017). From approximately 500–750&nbsp;ms, positive scenes evoked significant differences in ZM activity relative to negative scenes (<em>p</em>&nbsp;=&nbsp;.034, .005, .019, .088 (trend), respectively for intervals 3–6). Significant differences between positive and neutral scenes did not emerge until 750–1000&nbsp;ms, but were reliably strong across the epoch (<em>p</em>&nbsp;=&nbsp;.228, .004, .001, &lt;&nbsp;.001, respectively for intervals 3–6). See <a class="anchor anchor-primary" href="#t0010" name="bt0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0010"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;2</span></span></a> summarising all statistically significant zygomaticus effects.</div><div class="tables frame-topbot colsep-0 rowsep-0" id="t0010"><span class="captions text-s"><span id="ca0045"><p id="sp0050"><span class="label">Table&nbsp;2</span>. Summary of significant factor main effects and/or significant factor interactions related to <em>zygomaticus major</em> EMG data.</p></span></span><div class="groups"><table><thead><tr class="valign-top rowsep-1"><th scope="col" class="align-left">EMG</th><th scope="col" class="align-left" colspan="6">Zygomaticus</th></tr></thead><tbody><tr class="valign-top"><td scope="row"><span class="screen-reader-only">Empty Cell</span></td><td class="align-left"><em>0–250</em></td><td class="align-left">250–500</td><td class="align-left">500–750</td><td class="align-left">750–1000</td><td class="align-left">1000–1250</td><td class="align-left">1250–1500</td></tr><tr class="valign-top"><th class="align-left" scope="row"><em>Stimulus * Emotion</em></th><td></td><td></td><td></td><td class="align-left">P&nbsp;=&nbsp;.016</td><td class="align-left">P&nbsp;=&nbsp;.003</td><td class="align-left">P&nbsp;=&nbsp;.012</td></tr><tr class="valign-top"><th class="align-left" scope="row"><em>Stimulus* Emotion * Task</em></th><td></td><td></td><td class="align-left">P&nbsp;=&nbsp;.049</td><td></td><td></td><td></td></tr></tbody></table></div></div></div></section><section id="s0090"><h4 id="st0100" class="u-margin-m-top u-margin-xs-bottom">Corrugator recordings</h4><div class="u-margin-s-bottom"><div id="p0195">As shown in the top panel in <a class="anchor anchor-primary" href="#f0030" name="bf0030" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0030"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;6</span></span></a>, CS activity was characterised by a rapid reduction in muscle activity from stimulus onset to 750&nbsp;ms during all face and scene S1 presentations, however the rate of this decline was faster when faces were viewed compared to scenes, which led to a significant main effect of Stimulus type in the 250–500&nbsp;ms time window (<em>F</em> (1, 22)&nbsp;=&nbsp;5.58, <em>p</em>&nbsp;=&nbsp;.027, <em>η</em><sup>2</sup>&nbsp;=&nbsp;.20).<a class="anchor anchor-primary" href="#fn0010" name="bfn0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fn0010"><span class="anchor-text-container"><span class="anchor-text"><sup>2</sup></span></span></a> This apparent relaxation of corrugator muscles at the point of stimulus onset has been demonstrated by others (e.g. <a class="anchor anchor-primary" href="#bb0005" name="bbb0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0005"><span class="anchor-text-container"><span class="anchor-text">Achaibou et al., 2008</span></span></a>, <a class="anchor anchor-primary" href="#bb0155" name="bbb0155" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0155"><span class="anchor-text-container"><span class="anchor-text">Dimberg and Petterson, 2000</span></span></a>, <a class="anchor anchor-primary" href="#bb0160" name="bbb0160" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0160"><span class="anchor-text-container"><span class="anchor-text">Dimberg et al., 2000</span></span></a>, <a class="anchor anchor-primary" href="#bb0165" name="bbb0165" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0165"><span class="anchor-text-container"><span class="anchor-text">Dimberg et al., 2002</span></span></a>), and is thought to be the result of increased tension in corrugator muscles at baseline due to anticipatory focus and attention towards an imminent visual stimulus presentation (<a class="anchor anchor-primary" href="#bb0555" name="bbb0555" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0555"><span class="anchor-text-container"><span class="anchor-text">van Boxtel and Jessurun, 1993</span></span></a>, <a class="anchor anchor-primary" href="#bb0550" name="bbb0550" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0550"><span class="anchor-text-container"><span class="anchor-text">Van Boxtel et al., 1996</span></span></a>).</div><figure class="figure text-xs" id="f0030"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr6.jpg" height="620" alt="" aria-describedby="ca0030"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr6_lrg.jpg" target="_blank" download="" title="Download high-res image (193KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (193KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr6.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="ca0030"><p id="sp0035"><span class="label">Fig.&nbsp;6</span>. <span>Mean corrugator muscle <a href="/topics/psychology/electromyography" title="Learn more about EMG from ScienceDirect's AI-generated Topic Pages" class="topic-link">EMG</a> amplitudes (μV) and error bars for 1 standard error, time-locked to the onset of the passively viewed Stimulus 1 (onset</span>&nbsp;=&nbsp;0&nbsp;ms). The top graph illustrates the mean amplitudes evoked by emotional faces compared with scenes after collapsing across emotion and task categories in order to highlight early latency differences involving corrugator muscle relaxation between the stimuli, presumably reflecting differences in the speed of early attentional orienting. The bottom graphs illustrate the different patterns of spontaneous corrugator activity elicited by emotional faces and scenes. Note that amplitude values differ across the scales in the top and bottom panel graphs. *&nbsp;=&nbsp;The differences are significant after Bonferroni corrections.</p></span></span></figure></div><div class="u-margin-s-bottom"><div id="p0200">Then, from 500 to 750&nbsp;ms, spontaneous muscle activity emerged as a function of emotion category for both face and scene stimuli, seen via a significant main effect of emotion (<em>F</em> (2, 44)&nbsp;=&nbsp;9.39, <em>p</em>&nbsp;&lt;&nbsp;.001, <em>η</em><sup>2</sup>&nbsp;=&nbsp;.30) that remained reliably significant across the next three time windows.<a class="anchor anchor-primary" href="#fn0015" name="bfn0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fn0015"><span class="anchor-text-container"><span class="anchor-text"><sup>3</sup></span></span></a> As seen in the bottom panel in <a class="anchor anchor-primary" href="#f0030" name="bf0030" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0030"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;6</span></span></a><span>, spontaneous activity tended to be greater for fearful and neutral faces and negative and neutral scenes, while happy faces and positive scenes led to greater relaxation of the <a href="/topics/neuroscience/corrugator-supercilii-muscle" title="Learn more about CS muscles from ScienceDirect's AI-generated Topic Pages" class="topic-link">CS muscles</a>. Paired samples </span><em>t</em>-tests were again used to determine significant fluctuations in CS activity between emotional categories, done separately for faces and scenes at each time interval of interest i.e. intervals three to six (with a corrected significance threshold of .017). For faces, spontaneous emotion-related activity emerged only briefly at 500–750&nbsp;ms as a trend (comparisons at all other intervals, <em>p</em>&nbsp;&gt;&nbsp;.07). Here, fearful and neutral expressions evoked significantly (<em>p</em>&nbsp;=&nbsp;.017; <em>p</em>&nbsp;=&nbsp;.061 (only trend)) greater CS activity compared to happy expressions, respectively. Contrastingly, negative scenes evoked a stronger, more enduring spontaneous effect from 500–750&nbsp;ms onwards, producing significantly greater activity relative to positive scenes across most of the epoch (<em>p</em>&nbsp;=&nbsp;.017, &lt;&nbsp;.001, .091, .005, respectively for intervals 3-6). The generally stronger activity evoked by neutral compared to positive scenes reached significance only at the fourth interval between 750 and 1000&nbsp;ms (<em>p</em>&nbsp;=&nbsp;.638, .014, .078, .186, respectively for intervals 3–6), and similar to the effects observed in ZM activity, negative and neutral scenes evoked very little difference in CS activity (<em>p</em>&nbsp;=&nbsp;.043, .161, .801, .048, respectively for intervals 3–6). See <a class="anchor anchor-primary" href="#t0015" name="bt0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0015"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;3</span></span></a> summarising all statistically significant corrugator effects.</div><div class="tables frame-topbot colsep-0 rowsep-0" id="t0015"><span class="captions text-s"><span id="ca0050"><p id="sp0055"><span class="label">Table&nbsp;3</span>. Summary of significant factor main effects and/or significant factor interactions related to <em>corrugator supercilii</em> EMG data.</p></span></span><div class="groups"><table><thead><tr class="valign-top rowsep-1"><th scope="col" class="align-left">EMG</th><th scope="col" class="align-left" colspan="6">Corrugator</th></tr></thead><tbody><tr class="valign-top"><td scope="row"><span class="screen-reader-only">Empty Cell</span></td><td class="align-left"><em>0–250</em></td><td class="align-left">250–500</td><td class="align-left">500–750</td><td class="align-left">750–1000</td><td class="align-left">1000–1250</td><td class="align-left">1250–1500</td></tr><tr class="valign-top"><th class="align-left" scope="row"><em>Stimulus</em></th><td></td><td class="align-left">P&nbsp;=&nbsp;.027</td><td></td><td></td><td></td><td></td></tr><tr class="valign-top"><th class="align-left" scope="row"><em>Emotion</em></th><td></td><td></td><td class="align-left">P&nbsp;&lt;&nbsp;.001</td><td class="align-left">P&nbsp;=&nbsp;.004</td><td class="align-left">P&nbsp;=&nbsp;.006</td><td class="align-left">P&nbsp;=&nbsp;.020</td></tr><tr class="valign-top"><th class="align-left" scope="row"><em>Stimulus* Emotion * Task</em></th><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table></div></div></div></section></section><section id="s0095"><h3 id="st0105" class="u-h4 u-margin-m-top u-margin-xs-bottom">Skin conductance recordings</h3><div class="u-margin-s-bottom"><div id="p0205">The ANOVA corresponding to the first 1000&nbsp;ms post S1-onset showed a trend towards a significant main effect of Stimulus type (<em>F</em> (1, 22)&nbsp;=&nbsp;3.97, <em>p</em>&nbsp;=&nbsp;.059, <em>η</em><sup>2</sup>&nbsp;=&nbsp;.15), followed by a significant effect between 1000 and 2000&nbsp;ms (<em>F</em> (1, 22)&nbsp;=&nbsp;5.22, <em>p</em>&nbsp;=&nbsp;.032, <em>η</em><sup>2</sup>&nbsp;=&nbsp;.19), which then diminished from 2000&nbsp;ms onwards (<em>p</em>&nbsp;&gt;&nbsp;.3). As seen in <a class="anchor anchor-primary" href="#f0035" name="bf0035" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0035"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;7</span></span></a>, skin conductance levels were greater overall at an early post-stimulus stage and tended to decrease over the three second presentation, with face stimuli evoking slightly greater activity during the early stage. That the effect occurred at such an early stage relative to a typical skin conductance response which emerges more slowly at around 2&nbsp;s post stimulus, suggests that differences between faces and scenes were a residual effect related to the S2 active response stage of preceding trials. Nevertheless, the significant effect indicates that faces and scenes evoked different arousal-related activity which was independent of emotional content. See <a class="anchor anchor-primary" href="#t0020" name="bt0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0020"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;4</span></span></a> summarising all statistically significant skin conductance effects.</div><figure class="figure text-xs" id="f0035"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr7.jpg" height="279" alt="" aria-describedby="ca0035"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr7_lrg.jpg" target="_blank" download="" title="Download high-res image (133KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (133KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1053811915008873-gr7.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="ca0035"><p id="sp0040"><span class="label">Fig.&nbsp;7</span>. <span>Mean <a href="/topics/psychology/skin-conductance" title="Learn more about skin conductance from ScienceDirect's AI-generated Topic Pages" class="topic-link">skin conductance</a> amplitudes (μS) and error bars for 1 standard error evoked by emotional faces and scenes time-locked to the onset of the passively viewed Stimulus 1 (onset</span>&nbsp;=&nbsp;0&nbsp;ms). Means were calculated by collapsing across emotion and task categories. *&nbsp;=&nbsp;The differences are significant after Bonferroni corrections.</p></span></span></figure><div class="tables frame-topbot colsep-0 rowsep-0" id="t0020"><span class="captions text-s"><span id="ca0055"><p id="sp0060"><span class="label">Table&nbsp;4</span>. Summary of significant factor main effects and/or significant factor interactions related to SC data.</p></span></span><div class="groups"><table><thead><tr class="valign-top rowsep-1"><th scope="col" class="align-left" colspan="5">SC</th></tr></thead><tbody><tr class="valign-top"><td scope="row"><span class="screen-reader-only">Empty Cell</span></td><td class="align-left"><em>0–1000</em></td><td class="align-left">1000–2000</td><td class="align-left">2000–3000</td><td class="align-left">3000–4000</td></tr><tr class="valign-top"><th class="align-left" scope="row"><em>Stimulus</em></th><td class="align-left">P&nbsp;=&nbsp;.059</td><td class="align-left">P&nbsp;=&nbsp;.032</td><td></td><td></td></tr></tbody></table></div></div></div></section></section><section id="s0100"><h2 id="st0110" class="u-h4 u-margin-l-top u-margin-xs-bottom">Discussion</h2><div class="u-margin-s-bottom" id="p0210">The aim of the current study was to determine the differences in emotional face and scene processing at neural and behavioural (i.e. spontaneous facial activity) levels. Using EEG to measure neural activity we found that the early visually-evoked P1 component peaked earlier for faces than for scenes, and that the type of task differently modulated the depth of this visual-related processing for faces and scenes. For faces the N170 was sensitive to the emotional content of the stimuli whereas the N100 for scenes was not. The EPN was sensitive to the emotional content of both faces and scenes, but differently so. For faces, the EPN was enhanced by fearful expressions as was the N170, while for scenes, positive content elicited enhanced EPN amplitudes, more prominent over the right hemisphere. Using fEMG we found that positive scenes but not happy faces elicited enhanced spontaneous zygomatic activity, whereas both fearful faces and negative scenes elicited enhanced spontaneous corrugator activity, but again this emotion effect was more enduring for scenes. Furthermore, prior to the influence of emotion, corrugator activity was marked by a rapid orienting response that occurred faster for faces than for scenes, which was akin to early P1 effects. Finally, skin conductance responses revealed slightly greater arousal levels when viewing faces than when viewing scenes. That the effect occurred at an early stage relative to a typical skin conductance response which emerges more slowly at around 2&nbsp;s post stimulus, suggests that differences between faces and scenes were a residual effect related to the S2 active response stage of preceding trials. Nevertheless, the significant effect indicates that faces and scenes evoked different arousal-related activity which was independent of emotional content.</div><section id="s0105"><h3 id="st0115" class="u-h4 u-margin-m-top u-margin-xs-bottom">Early neural processing of emotional faces and scenes</h3><section id="s0110"><h4 id="st0120" class="u-margin-m-top u-margin-xs-bottom">P1 component</h4><div class="u-margin-s-bottom" id="p0215">Neural activity at the early visually evoked P1 component showed that stimulus-specific features of faces and scenes evoked different degrees of rapid attentional processing irrespective of emotional content. Scenes generated greater visually-evoked cortical activity compared to faces, most likely because of their greater degree of complexity (<a class="anchor anchor-primary" href="#bb0050" name="bbb0050" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0050"><span class="anchor-text-container"><span class="anchor-text">Bradley et al., 2007</span></span></a><span>). Bradley et al. also found that picture complexity influences the magnitude of <a href="/topics/neuroscience/evoked-potential" title="Learn more about evoked potentials from ScienceDirect's AI-generated Topic Pages" class="topic-link">evoked potentials</a><span> proceeding the P1, which would explain the large differences in visually <a href="/topics/psychology/evoked-potential" title="Learn more about evoked potentials from ScienceDirect's AI-generated Topic Pages" class="topic-link">evoked potentials</a> between stimulus groups in the current study. The Pic–Pic task also differently influenced the depth of P1-related visual processing of scenes and faces in a manner that was proportional to the complexity of the stimuli. In other words, processing of scene stimuli, which were more complex, was considerably enhanced, whereas processing of face stimuli, which were less complex, was only slightly enhanced.</span></span></div><div class="u-margin-s-bottom" id="p0220">It is important to mention that even though we can rule out that both luminance and spatial frequency (see method section) explain later task- and emotion-specific effects it is theoretically possible that those physical features explain category-specific effects like the ones described above.</div><div class="u-margin-s-bottom" id="p0225">Beyond this stage, other early processes involved in face and object perception, i.e. the N100/N170 and EPN components, were not differently modulated by faces and scenes as a function of the recognition tasks.</div></section><section id="s0115"><h4 id="st0125" class="u-margin-m-top u-margin-xs-bottom">N100/N170 component</h4><div class="u-margin-s-bottom" id="p0230">A critical finding at the neural level was that affective information in faces influenced neural activity earlier than affective information in scenes, as reflected by the enhanced activity of the N100/N170 for fearful faces, but not for emotional compared to neutral scenes. Recently, <a class="anchor anchor-primary" href="#bb0545" name="bbb0545" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0545"><span class="anchor-text-container"><span class="anchor-text">Thom et al. (2014)</span></span></a> also compared neural activity generated by emotional faces and scenes in a single experimental paradigm, and found that neural processes underlying the N100/N170 component were sensitive to affective information for scenes. These differences between Thom et al.'s and our findings may be explained by methodological differences including that, in their study, some emotional scene stimuli contained faces (fear, joy and angry stimuli, but not neutral, which were all inanimate objects), while we specifically did not include scenes with forward-facing faces. Also, their participants were all males whereas in the current study participants were a mixture of males and females. The latter distinction is important because males have been shown to generate enhanced N100/N170 amplitudes compared to females (<a class="anchor anchor-primary" href="#bb0445" name="bbb0445" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0445"><span class="anchor-text-container"><span class="anchor-text">Proverbio et al., 2009</span></span></a>), and it has been shown that natural scenes with and without human faces can evoke significantly different activity in the N100/N170 time window (<a class="anchor anchor-primary" href="#bb0210" name="bbb0210" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0210"><span class="anchor-text-container"><span class="anchor-text">Ferri et al., 2012</span></span></a>, <a class="anchor anchor-primary" href="#bb0445" name="bbb0445" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0445"><span class="anchor-text-container"><span class="anchor-text">Proverbio et al., 2009</span></span></a>). It is possible that these factors, particularly when combined, led to selective enhancement of amplitudes in the N100/N170 time window during their emotional scene presentations.</div><div class="u-margin-s-bottom" id="p0235"><span>The negativity bias of fearful facial expressions relative to neutral and other emotional expressions has received a variety of interpretations in the literature. Some have suggested that the early discrimination of fearful from neutral faces is due to crude threat or signs of danger which rapidly activate <a href="/topics/neuroscience/neural-circuit" title="Learn more about neural circuits from ScienceDirect's AI-generated Topic Pages" class="topic-link">neural circuits</a> specialised for detecting danger (e.g. </span><a class="anchor anchor-primary" href="#bb0200" name="bbb0200" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0200"><span class="anchor-text-container"><span class="anchor-text">Esteves et al., 1994</span></span></a>, <a class="anchor anchor-primary" href="#bb0245" name="bbb0245" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0245"><span class="anchor-text-container"><span class="anchor-text">Hansen and Hansen, 1988</span></span></a>, <a class="anchor anchor-primary" href="#bb0335" name="bbb0335" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0335"><span class="anchor-text-container"><span class="anchor-text">LeDoux, 2003</span></span></a>, <a class="anchor anchor-primary" href="#bb0410" name="bbb0410" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0410"><span class="anchor-text-container"><span class="anchor-text">Öhman, 2005</span></span></a>, <a class="anchor anchor-primary" href="#bb0415" name="bbb0415" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0415"><span class="anchor-text-container"><span class="anchor-text">Öhman and Mineka, 2001</span></span></a>). However, this theory likely does not explain our results, as we would have expected negative scenes, which included fearful components such as snakes and spiders to evoke such activity as well, particularly considering that detecting negative scenes is highly survival-relevant, and that negative scenes in this study were rated as more unpleasant and more arousing than fearful faces.</div><div class="u-margin-s-bottom" id="p0240"><a class="anchor anchor-primary" href="#bb0560" name="bbb0560" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0560"><span class="anchor-text-container"><span class="anchor-text">Vuilleumier and Pourtois (2007)</span></span></a> instead reason that the anatomical regions involved in facial expression recognition may be spatially organised according to emotion categories, in that sub-regions associated with encoding facial features more unique to one expression are spatially segregated from sub-regions associated with encoding those that are more unique to another expression. Hence, emotion category-specific modulation of the N170 component may be reflecting these regional variations rather than motivational emotional significance of the stimuli (<a class="anchor anchor-primary" href="#bb0560" name="bbb0560" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0560"><span class="anchor-text-container"><span class="anchor-text">Vuilleumier and Pourtois, 2007</span></span></a>). In support of this theory, they argue that activity in the N170 time window has been found to be differently sensitive to a range of facial expressions in addition to fear such as surprise and disgust. Along this line, faces with more similar expressions such as anger and fear (both negative and with overlapping facial muscle contraction) evoke more similar modulatory activity than when compared with happy faces (<a class="anchor anchor-primary" href="#bb0545" name="bbb0545" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0545"><span class="anchor-text-container"><span class="anchor-text">Thom et al., 2014</span></span></a>). On this basis, and when considering that emotional content is identified faster for faces compared to scenes (<a class="anchor anchor-primary" href="#bb0060" name="bbb0060" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0060"><span class="anchor-text-container"><span class="anchor-text">Britton et al., 2006</span></span></a>, <a class="anchor anchor-primary" href="#bb0165" name="bbb0165" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0165"><span class="anchor-text-container"><span class="anchor-text">Dimberg et al., 2002</span></span></a>, <a class="anchor anchor-primary" href="#bb0185" name="bbb0185" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0185"><span class="anchor-text-container"><span class="anchor-text">Eisenbarth et al., 2011</span></span></a>), the current data could be reflecting faster extraction of low level semantic affective information from faces than from scenes, but not necessarily faster identification of motivational emotional significance.</div></section><section id="s0120"><h4 id="st0130" class="u-margin-m-top u-margin-xs-bottom">EPN time window</h4><div class="u-margin-s-bottom" id="p0245">An EPN emerged at a similar latency for emotional faces and scenes, occurring immediately following the offset of the N100/N170 components. For faces, the EPN was pronounced only for fearful expressions, while for scenes, the EPN was pronounced only for positive content. When considered separately, these selective modulations of the EPN are in line with past research showing enhanced negativity for fearful faces in the EPN time window compared to neutral (<a class="anchor anchor-primary" href="#bb0355" name="bbb0355" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0355"><span class="anchor-text-container"><span class="anchor-text">Leppänen et al., 2008</span></span></a>, <a class="anchor anchor-primary" href="#bb0535" name="bbb0535" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0535"><span class="anchor-text-container"><span class="anchor-text">Stekelenburg and de Gelder, 2004</span></span></a>) and happy expressions (<a class="anchor anchor-primary" href="#bb0265" name="bbb0265" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0265"><span class="anchor-text-container"><span class="anchor-text">Herbert et al., 2013b</span></span></a>, <a class="anchor anchor-primary" href="#bb0385" name="bbb0385" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0385"><span class="anchor-text-container"><span class="anchor-text">Mühlberger et al., 2009</span></span></a>, <a class="anchor anchor-primary" href="#bb0495" name="bbb0495" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0495"><span class="anchor-text-container"><span class="anchor-text">Schupp et al., 2004b</span></span></a>), and enhanced EPN negativity for positive scenes compared to negative and neutral scenes (<a class="anchor anchor-primary" href="#bb0065" name="bbb0065" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0065"><span class="anchor-text-container"><span class="anchor-text">Bublatzky and Schupp, 2011</span></span></a>, <a class="anchor anchor-primary" href="#bb0225" name="bbb0225" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0225"><span class="anchor-text-container"><span class="anchor-text">Franken et al., 2008</span></span></a>, <a class="anchor anchor-primary" href="#bb0485" name="bbb0485" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0485"><span class="anchor-text-container"><span class="anchor-text">Schupp et al., 2004a</span></span></a>, <a class="anchor anchor-primary" href="#bb0510" name="bbb0510" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0510"><span class="anchor-text-container"><span class="anchor-text">Schupp et al., 2006b</span></span></a>, <a class="anchor anchor-primary" href="#bb0515" name="bbb0515" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0515"><span class="anchor-text-container"><span class="anchor-text">Schupp et al., 2007</span></span></a>, <a class="anchor anchor-primary" href="#bb0505" name="bbb0505" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0505"><span class="anchor-text-container"><span class="anchor-text">Schupp et al., 2013b</span></span></a>, <a class="anchor anchor-primary" href="#bb0580" name="bbb0580" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0580"><span class="anchor-text-container"><span class="anchor-text">Weinberg and Hajcak, 2010</span></span></a>). Further research also supports our observation that neural activity during the EPN time-frame was generally more negative over the left hemisphere compared with the right (<a class="anchor anchor-primary" href="#bb0495" name="bbb0495" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0495"><span class="anchor-text-container"><span class="anchor-text">Schupp et al., 2004b</span></span></a>, <a class="anchor anchor-primary" href="#bb0525" name="bbb0525" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0525"><span class="anchor-text-container"><span class="anchor-text">Smith et al., 2013</span></span></a>), while stronger emotion-specific modulation occurred over the right hemisphere, and seemed to be an exclusive effect of scene stimuli (<a class="anchor anchor-primary" href="#bb0290" name="bbb0290" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0290"><span class="anchor-text-container"><span class="anchor-text">Junghöfer et al., 2001</span></span></a>, <a class="anchor anchor-primary" href="#bb0515" name="bbb0515" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0515"><span class="anchor-text-container"><span class="anchor-text">Schupp et al., 2007</span></span></a>). These findings suggest that EPN activity includes a commonality between processing of fearful facial expressions and positive scenes.</div><div class="u-margin-s-bottom" id="p0250">Still, there seems to be no clear explanation in the literature addressing why emotional faces and scenes evoke such different category-specific responses in the EPN time window. Speculation has centred on the motivational significance of affective cues, particularly in that erotica are highly arousing stimuli. In the present study, stimuli with low or moderate arousal were chosen which could have facilitated an arousal-driven processing bias for positive scenes due to the erotica content. However skin conductance recordings during these presentations do not support this interpretation, and instead, suggest that all face stimuli evoked enhanced arousal levels compared to scenes. Findings from <a class="anchor anchor-primary" href="#bb0545" name="bbb0545" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0545"><span class="anchor-text-container"><span class="anchor-text">Thom et al.'s study of faces and scenes (2014)</span></span></a> also showed that despite positive scenes (including erotica) being rated as more arousing than all other emotional scene categories, these stimuli did not produce enhanced EPN activity. Thus it seems that other factors are more likely.</div><div class="u-margin-s-bottom" id="p0255">Experiments involving EPN analyses have inevitably become more elaborate, and there is now some evidence linking EPN activity to modulations of self-reference or task relevance (e.g. <a class="anchor anchor-primary" href="#bb0260" name="bbb0260" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0260"><span class="anchor-text-container"><span class="anchor-text">Herbert et al., 2013a</span></span></a>, <a class="anchor anchor-primary" href="#bb0265" name="bbb0265" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0265"><span class="anchor-text-container"><span class="anchor-text">Herbert et al., 2013b</span></span></a>, <a class="anchor anchor-primary" href="#bb0535" name="bbb0535" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0535"><span class="anchor-text-container"><span class="anchor-text">Stekelenburg and de Gelder, 2004</span></span></a>), which could also be extended to explain the differential effects of face and scene stimuli seen in the current study. Evidence from several studies suggest that as stimuli become more salient with respect to the complexity of required processing, typically as a result of task demands, so too does the degree of EPN modulation during the associated stimulus presentations, suggesting that EPN activity could be a precursor to more conscious levels of stimulus evaluation (<a class="anchor anchor-primary" href="#bb0260" name="bbb0260" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0260"><span class="anchor-text-container"><span class="anchor-text">Herbert et al., 2013a</span></span></a>, <a class="anchor anchor-primary" href="#bb0535" name="bbb0535" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0535"><span class="anchor-text-container"><span class="anchor-text">Stekelenburg and de Gelder, 2004</span></span></a>).</div><div class="u-margin-s-bottom" id="p0260">For example, in a study requiring participants to categorise fearful and neutral faces as either upright or inverted, upright fearful faces predictably evoked increased EPN activity compared to upright neutral faces. When faces were inverted however, the EPN was enhanced for both fearful and neutral faces compared to when the same faces were shown in the upright position (<a class="anchor anchor-primary" href="#bb0535" name="bbb0535" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0535"><span class="anchor-text-container"><span class="anchor-text">Stekelenburg and de Gelder, 2004</span></span></a>). This suggests that the EPN is sensitive to task-induced changes in stimulus complexity.</div><div class="u-margin-s-bottom" id="p0265">These effects extend even to self-referential emotion processing whereby emotional faces preceded by matched self-relevant word cues such as ‘my fear’ or ‘my happiness’ evoked enhanced EPN activity compared to when the preceding cues were meaningless letter strings (<a class="anchor anchor-primary" href="#bb0265" name="bbb0265" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0265"><span class="anchor-text-container"><span class="anchor-text">Herbert et al., 2013b</span></span></a>), supporting the view that the self-reference of affective stimuli facilitates motivated attention capture to emotional stimuli as reflected by the EPN component. In another study, participants were asked to use specific cue words (e.g. cues like ‘no fear’, ‘no panic’ etc.) to intentionally regulate their feelings evoked by fearful and happy faces. Using these cue words that attenuated the emotion described in the picture (e.g. no fear paired with a fearful face) attenuated ERP amplitudes to fearful faces as early as in the EPN time window (<a class="anchor anchor-primary" href="#bb0260" name="bbb0260" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0260"><span class="anchor-text-container"><span class="anchor-text">Herbert et al., 2013a</span></span></a>).</div><div class="u-margin-s-bottom" id="p0270">In the frame of active versus passive tasks, similar effects have also been documented during the EPN time interval. For example, <a class="anchor anchor-primary" href="#bb0515" name="bbb0515" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0515"><span class="anchor-text-container"><span class="anchor-text">Schupp et al. (2007b)</span></span></a> compared the effect of passively viewing emotional scenes versus the effect of actively counting the number of times a specifically valanced scene was presented. They found typical EPN modulation during the passive viewing task, whereby erotica (pleasant stimuli) elicited enhanced EPN activity compared to mutilation (unpleasant) and neutral stimuli, but when participants were required to count the number of presentations occurring for each emotion category, EPN activity was significantly modulated relative to when the same category of emotional stimuli were passively viewed (<a class="anchor anchor-primary" href="#bb0515" name="bbb0515" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0515"><span class="anchor-text-container"><span class="anchor-text">Schupp et al., 2007b</span></span></a>). More recently, a study by <a class="anchor anchor-primary" href="#bb0500" name="bbb0500" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0500"><span class="anchor-text-container"><span class="anchor-text">Schupp et al. (2013a)</span></span></a> comparing passive viewing to active semantic categorisation also suggested a link between EPN activity and higher cognitive evaluations. Here, emotional scenes were overlayed with simple pictures of either animals or other non-animal scenes and objects. In the active categorisation task, participants were required to judge whether the foreground picture was either an animal or not, while no response was required during passive viewing. Again, in the passive viewing task, stimuli with pleasant scenes as the background image evoked stronger EPN activity than did stimuli with unpleasant or neutral background scenes. However, active categorisation led to the diminishment of emotion-specific modulation, and instead, EPN activity was enhanced overall relative to the passive viewing task regardless of the emotional background. Moreover, EPN activity was significantly enhanced for foreground pictures of animals compared to non-animals, suggesting that task difficulty and stimulus salience can override emotional significance in the EPN time window.</div><div class="u-margin-s-bottom" id="p0275"><span>In the current experiment, where the task was to passively view the S1 stimuli, but also to implicitly identify its emotional content, stimulus salience, and thus EPN activity, should have been driven, at least partially, by emotion recognition difficulty. Fearful facial expressions and positive scenes, which evoked enhanced EPN activity, may have been more difficult to recognise relative to other sub-categories, resulting in a call for more sophisticated <a href="/topics/psychology/cognitive-process" title="Learn more about cognitive processing from ScienceDirect's AI-generated Topic Pages" class="topic-link">cognitive processing</a> to accurately identify and categorise these stimuli. This is exactly what past research predicts. </span><a class="anchor anchor-primary" href="#bb0455" name="bbb0455" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0455"><span class="anchor-text-container"><span class="anchor-text">Recio et al. (2014)</span></span></a> twice demonstrated that recognising fearful faces as expressing fear was more difficult than recognising happiness from smiling faces and neutrality from neutral faces, and found that negative expressions (i.e., anger, disgust, fear, sadness and surprise) were most often confused, whereas happy faces enjoy a recognition advantage in the EPN time window (<a class="anchor anchor-primary" href="#bb0075" name="bbb0075" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0075"><span class="anchor-text-container"><span class="anchor-text">Calvo and Beltran, 2013</span></span></a>). As would be predicted by these findings, other research further shows that happy faces are identified and responded to faster than angry faces (<a class="anchor anchor-primary" href="#bb0530" name="bbb0530" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0530"><span class="anchor-text-container"><span class="anchor-text">Sonnby-Borgström, 2002</span></span></a>; see also <a class="anchor anchor-primary" href="#bb0340" name="bbb0340" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0340"><span class="anchor-text-container"><span class="anchor-text">Leppänen and Hietanen, 2004</span></span></a>, <a class="anchor anchor-primary" href="#bb0080" name="bbb0080" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0080"><span class="anchor-text-container"><span class="anchor-text">Calvo and Lundqvist, 2008</span></span></a>) supporting the view that happy faces are attention grabbing due to their salience (i.e. the biologically determined social relevance of a happy face for both interaction partners including the perceiver and the receiver (<a class="anchor anchor-primary" href="#bb0055" name="bbb0055" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0055"><span class="anchor-text-container"><span class="anchor-text">Becker and Srinivasan, 2014</span></span></a>)).</div><div class="u-margin-s-bottom" id="p0280">Speculatively, this could also explain why positive scenes evoked enhanced EPN activity relative to other emotional scene categories. The negative, mainly fearful (e.g. spiders and snakes) and neutral (e.g. a computer, a chair) scene stimuli used in the current study were generally rather obvious and intuitive to categorise. However, the content of positive scene stimuli was more varied (e.g. extreme sports, nature scenes, appetising foods and erotica), thereby providing less-intuitive cues directly linked to ‘happiness’. Hence, emotion-category specific modulation of the EPN may have been related to differences in recognition difficulty, resulting in a call for more sophisticated <a href="/topics/neuroscience/cognitive-process" title="Learn more about cognitive processing from ScienceDirect's AI-generated Topic Pages" class="topic-link">cognitive processing</a> to accurately categorise emotional stimuli which lacked intuitive cues, thereby causing fearful facial expressions and positive scenes to become more salient with respect to EPN-related brain activity.</div></section></section><section id="s0125"><h3 id="st0135" class="u-h4 u-margin-m-top u-margin-xs-bottom">Spontaneous facial reactions to emotional faces and scenes</h3><div class="u-margin-s-bottom" id="p0285">Emotion-related spontaneous activity of both the zygomatic and corrugator muscles emerged between 500 and 1000&nbsp;ms post stimulus, which is consistent with other fEMG studies of emotional faces (<a class="anchor anchor-primary" href="#bb0140" name="bbb0140" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0140"><span class="anchor-text-container"><span class="anchor-text">Dimberg, 1982</span></span></a>, <a class="anchor anchor-primary" href="#bb0145" name="bbb0145" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0145"><span class="anchor-text-container"><span class="anchor-text">Dimberg, 1997</span></span></a>, <a class="anchor anchor-primary" href="#bb0380" name="bbb0380" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0380"><span class="anchor-text-container"><span class="anchor-text">Moody et al., 2007</span></span></a>) and scenes (<a class="anchor anchor-primary" href="#bb0150" name="bbb0150" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0150"><span class="anchor-text-container"><span class="anchor-text">Dimberg et al., 1998</span></span></a>). It suggests that faces and scenes trigger the same neural affective processes, or processes with similar latencies leading to spontaneous facial reactions. This is also consistent with the observation that emotional significance was detected in neural activity at a similar latency for face and scene stimuli.</div><div class="u-margin-s-bottom" id="p0290">One difference between faces and scenes however, was that spontaneous zygomatic activity was evoked by positive scenes but not by happy faces. <a class="anchor anchor-primary" href="#bb0315" name="bbb0315" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0315"><span class="anchor-text-container"><span class="anchor-text">Künecke et al. (2014)</span></span></a> also found no effect of happy faces on spontaneous zygomatic activity, while fearful facial expressions evoked reliably enhanced corrugator activity. Moreover, their experiment was also conducted in the frame of an emotion recognition task which, similar to the current study, required a delayed rather than immediate recognition judgement. As has been discussed above, one possible explanation for why positive scenes but not happy faces evoked spontaneous reactions is that, in the frame of an emotion recognition task, smiling faces may simply be easy to recognise and semantically categorise, whereas semantically categorising positive scenes requires more effortful mental processing. If this is the case, we want to speculate on these findings and point to two possible ideas: firstly, emotion-related spontaneous facial activity is linked to emotion recognition processes. Secondly, the mechanisms are triggered only when emotional information is ambiguous, as has been suggested by others (<a class="anchor anchor-primary" href="#bb0590" name="bbb0590" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0590"><span class="anchor-text-container"><span class="anchor-text">Winkielman et al., 2009</span></span></a>). Moreover, basic motor-mimicry is not necessary for social-emotion recognition, as has been demonstrated by others (<a class="anchor anchor-primary" href="#bb0240" name="bbb0240" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0240"><span class="anchor-text-container"><span class="anchor-text">Grèzes et al., 2013</span></span></a>, <a class="anchor anchor-primary" href="#bb0365" name="bbb0365" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0365"><span class="anchor-text-container"><span class="anchor-text">Magnée et al., 2007</span></span></a>, <a class="anchor anchor-primary" href="#bb0380" name="bbb0380" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0380"><span class="anchor-text-container"><span class="anchor-text">Moody et al., 2007</span></span></a>). The latter interpretation is particularly supported by the observation that happy faces did not seem to evoke any substantial change in zygomatic activity relative to baseline.</div><div class="u-margin-s-bottom" id="p0295">In addition to effects related to the content of the images, motivational factors also seemed to contribute to the differential spontaneous reactions to faces and scenes. Emotional scenes not only elicited spontaneous activity in the emotion-appropriate muscles, but did so quite strongly compared to faces. The disparity is particularly evident in the observed corrugator activity in that fearful (and neutral<a class="anchor anchor-primary" href="#fn0020" name="bfn0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fn0020"><span class="anchor-text-container"><span class="anchor-text"><sup>4</sup></span></span></a>) faces evoked only a momentarily enhanced response, whereas negative scenes evoked a strong stable response which seemed to become even stronger at the same time that the effect of fearful faces diminished. <a class="anchor anchor-primary" href="#bb0020" name="bbb0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0020"><span class="anchor-text-container"><span class="anchor-text">Alpers et al. (2011)</span></span></a><span> also reported stronger orbicularis oculi (ring muscle around the eyes) activity (an index of the Duchene smile) to pleasant scenes than to smiling faces, and more broadly, demonstrated greater activation of other behavioural indexes of motivation by emotional scenes compared to faces including decreased heart rate acceleration, greater <a href="/topics/neuroscience/startle-response" title="Learn more about startle reflex from ScienceDirect's AI-generated Topic Pages" class="topic-link">startle reflex</a> modulation and increased skin conductance levels, findings that were also replicated by </span><a class="anchor anchor-primary" href="#bb0575" name="bbb0575" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0575"><span class="anchor-text-container"><span class="anchor-text">Wangelin et al. (2012)</span></span></a>.<a class="anchor anchor-primary" href="#fn0025" name="bfn0025" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fn0025"><span class="anchor-text-container"><span class="anchor-text"><sup>5</sup></span></span></a><span><span> Indeed, stimulus arousal ratings in both our study and Alpers' study also point to motivational influences in that emotional scenes were rated as more arousing than emotional faces. It should also be considered that in such a task-primed context with little ‘real-world’ social-motivational relevance, the veracity of motivational influences of facial expressions on <a href="/topics/neuroscience/behavior-neuroscience" title="Learn more about behaviour from ScienceDirect's AI-generated Topic Pages" class="topic-link">behaviour</a> diminish, or perhaps strong stable behavioural–emotional responses are not necessary during </span><a href="/topics/neuroscience/social-interaction" title="Learn more about social interactions from ScienceDirect's AI-generated Topic Pages" class="topic-link">social interactions</a>, and instead could hinder one's ability to keep up with the naturally dynamic exchange of affective signals in social–emotional interactions. Contrastingly, emotionally evocative scenes more often involve an immediate approach/avoidance overt physical response, and thus even at such an automated stage of behaviour, it is logical to expect these stimuli will evoke more stable and enduring motivational reactions.</span></div></section><section id="s0130"><h3 id="st0140" class="u-h4 u-margin-m-top u-margin-xs-bottom">Consistencies between neural activity and spontaneous facial reactions</h3><section id="s0135"><h4 id="st0145" class="u-margin-m-top u-margin-xs-bottom">Stronger emotional responses to scenes</h4><div class="u-margin-s-bottom" id="p0300">Consistent with the observation that emotional scenes evoked stronger and more stable spontaneous behavioural activations compared to faces, neural processes related to the EPN were also more strongly and stably activated by scenes than by faces. These effects also emerged in <a class="anchor anchor-primary" href="#bb0545" name="bbb0545" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0545"><span class="anchor-text-container"><span class="anchor-text">Thom et al.'s comparison of faces and scenes (2014)</span></span></a><span><span>, where they discussed this effect in terms of scenes activating stronger motivationally relevant activity, particularly with respect to theories pointing to increased <a href="/topics/neuroscience/amygdala" title="Learn more about amygdala from ScienceDirect's AI-generated Topic Pages" class="topic-link">amygdala</a> activations as a contributing source of EPN activity. Thom and colleagues interpretation is further supported by functional MRI data showing that biologically relevant stimuli do activate stronger </span><a href="/topics/neuroscience/functional-connectivity" title="Learn more about functional connectivity from ScienceDirect's AI-generated Topic Pages" class="topic-link">functional connectivity</a> between visual areas and the amygdala than do socially relevant stimuli (</span><a class="anchor anchor-primary" href="#bb0475" name="bbb0475" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0475"><span class="anchor-text-container"><span class="anchor-text">Sakaki et al., 2012</span></span></a>). Hence motivational factors may influence how long evoked affective responses persist, rather than or in addition to the strength of the response. Such a distinction may have gone unnoticed in past facial EMG research, given that many studies focus on an averaged amplitude for a block of time (some up to six seconds), rather than shorter sequential averages.</div><div class="u-margin-s-bottom" id="p0305">The findings therefore draw on two similarities between EPN activity and spontaneous facial reactions including stimulus salience, which is largely a product of task demands, and the persistence of motivational responses, which begs the question of whether EPN activity may be a precursor to motivated behaviour. To test this theory, responses to positive scenes were analysed (chosen because this stimuli evoked the strongest EPN activity as well as reliably strong zygomatic activity, and thus would be of greatest interest and least implicated by floor/ceiling effects) via a post hoc correlation of right hemispheric EPN activity, and spontaneous zygomatic activity averaged over the time frame of strongest emotional responding (750–1500&nbsp;ms). If stronger EPN activity is related to stronger spontaneous zygomatic responses, then we would at least expect to see a negative linear correlation between these physiological outputs indicating that the stronger the negative shift in EPN-evoked potentials, the greater the amplitude of zygomatic muscle contractions. Even without attention to potential outliers, the analysis of the 23 data sets revealed a modest correlation in the predicted direction (<em>r</em>&nbsp;=&nbsp;−&nbsp;.29) which was trending towards significance (<em>p</em>&nbsp;=&nbsp;.091), while after removing one outlier, the correlation strengthened considerably (<em>r</em>&nbsp;=&nbsp;−&nbsp;.47, <em>p</em>&nbsp;=&nbsp;.014). Of course, this correlation may represent a consistency in the absolute levels of neural and behavioural activity within individuals. Therefore, further research is necessary to determine the meaning of this relationship between EPN activity and behavioural responses. In addition, it would also be interesting to investigate the modulation of late ERP potentials such as the LPP. The LPP has been shown to be influenced by emotional and cognitive factors. In contrast to early ERP potentials such as the EPN, however, the LPP is thought to reflect top-down controlled affective processing (<a class="anchor anchor-primary" href="#bb0250" name="bbb0250" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0250"><span class="anchor-text-container"><span class="anchor-text">Hajcak and Nieuwenhuis, 2006</span></span></a>; for review see <a class="anchor anchor-primary" href="#bb0420" name="bbb0420" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0420"><span class="anchor-text-container"><span class="anchor-text">Olofsson et al., 2008</span></span></a>, <a class="anchor anchor-primary" href="#bb0305" name="bbb0305" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0305"><span class="anchor-text-container"><span class="anchor-text">Korb et al., 2012</span></span></a>).</div></section><section id="s0140"><h4 id="st0150" class="u-margin-m-top u-margin-xs-bottom">Faster attentional orienting to faces</h4><div class="u-margin-s-bottom" id="p0310">Of course it would also be interesting to explore the relationship between fEMG activity and LPP modulation. However, as argued above such a correlation would not reveal if cortical or facial changes occurred first. In the present study we show that EPN modulation is correlated with changes in fEMG activity, which suggests that facilitated early cortical processing of emotional scenes is a prerequisite for changes in facial muscle activity.</div><div class="u-margin-s-bottom" id="p0315">Early neural activity and behavioural responses to faces and scenes suggests that viewing emotional faces engages attention processes faster than when viewing emotionally evocative scenes. This was reflected in the earlier onset latency of the P1 component and the faster release of tension in the corrugator eyebrow muscles when viewing faces. Such differences can also be seen in evoked potentials in other studies of emotional faces and scenes (<a class="anchor anchor-primary" href="#bb0310" name="bbb0310" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0310"><span class="anchor-text-container"><span class="anchor-text">Kujawa et al., 2012</span></span></a>, <a class="anchor anchor-primary" href="#bb0545" name="bbb0545" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0545"><span class="anchor-text-container"><span class="anchor-text">Thom et al., 2014</span></span></a>). These findings could assist in explaining why the valence of emotional faces is identified faster than the valence of emotional scenes when measured via button press (<a class="anchor anchor-primary" href="#bb0060" name="bbb0060" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0060"><span class="anchor-text-container"><span class="anchor-text">Britton et al., 2006</span></span></a>) and voluntary mimicry latencies (<a class="anchor anchor-primary" href="#bb0165" name="bbb0165" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0165"><span class="anchor-text-container"><span class="anchor-text">Dimberg et al., 2002</span></span></a>, <a class="anchor anchor-primary" href="#bb0185" name="bbb0185" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0185"><span class="anchor-text-container"><span class="anchor-text">Eisenbarth et al., 2011</span></span></a>), and may be a factor underlying why attention disengagement from socially-relevant stimuli to a secondary target is faster than from biologically-relevant stimuli (<a class="anchor anchor-primary" href="#bb0475" name="bbb0475" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0475"><span class="anchor-text-container"><span class="anchor-text">Sakaki et al., 2012</span></span></a>).</div></section></section><section id="s0145"><h3 id="st0170" class="u-h4 u-margin-m-top u-margin-xs-bottom">Conclusions</h3><div class="u-margin-s-bottom" id="p0320">The findings in this study point to two key differences between emotional face and scene stimuli during early visual processing, including rapid attentional capture mechanisms and motivated response mechanisms. Broadly, there was a logical chronology of stages of perception that could be seen in both neural activity and behavioural output in that selective attention determined what was attended to, which manifested into affectively-driven selective emotional responses. However, not all stimulus percepts affecting neural activity were apparent in behavioural activity. The picture- versus word-primed recognition tasks elicited different neural activity at the P1 and N100/N170 components, which broadly, reflects variations in basic object recognition processes. However, the recognition tasks did not influence the nature of spontaneous behavioural responses. This suggests that fast motivated emotional behaviour, including spontaneous facial reactions, is relatively robust to variations in basic object and face recognition pathways such as variations in semantic-priming, and logically corroborates with the idea that behavioural responses to emotional stimuli are grounded in motivational emotional significance.</div><div class="u-margin-s-bottom" id="p0325">Further research investigating consistencies between neural activity and behavioural responses is needed to replicate and extend the current research. Critically though, this will require careful sequential temporal analyses, as opposed to amplitude-only analyses, particularly at very early time frames of less than one second, which is quite rare in the facial EMG literature. However, the fact that EMG techniques offer excellent temporal resolution matching that of EEG means that such studies are viable and will be very valuable.</div></section></section></div><section id="ac0005"><h2 id="st0160" class="u-h4 u-margin-l-top u-margin-xs-bottom">Acknowledgments</h2><div class="u-margin-s-bottom" id="p0330">We thank Ross Fulham, Tony Kemp, Melinda Vardanega, Natalie Townsend, Jennifer Gilchrist, Samantha Allen and Jacob Duffy for their assistance performing the study.</div></section></div><div class="related-content-links u-display-none-from-md"><button class="button-link button-link-primary button-link-small" type="button"><span class="button-link-text-container"><span class="button-link-text">Recommended articles</span></span></button></div><div class="Tail"></div><div><section class="bibliography u-font-serif text-s" id="bi0005"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">References</h2><section class="bibliography-sec" id="bs0005"><ol class="references" id="reference-links-bs0005"><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0005" id="ref-id-bb0005" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Achaibou et al., 2008</span></span></a></span><span class="reference" id="rf0180"><div class="contribution"><div class="authors u-font-sans">A. Achaibou, G. Pourtois, S. Schwartz, P. Vuilleumier</div><div id="ref-id-rf0180" class="title text-m">Simultaneous recording of EEG and facial muscle reactions during spontaneous emotional mimicry</div></div><div class="host u-font-sans">Neuropsychologia, 46 (4) (2008), pp. 1104-1113, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.neuropsychologia.2007.10.019" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.neuropsychologia.2007.10.019</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0028393207003697/pdfft?md5=1c34f1348e41744063fd5d935b6349e3&amp;pid=1-s2.0-S0028393207003697-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0180"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0028393207003697" aria-describedby="ref-id-rf0180"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-39149099192&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0180"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Simultaneous%20recording%20of%20EEG%20and%20facial%20muscle%20reactions%20during%20spontaneous%20emotional%20mimicry&amp;publication_year=2008&amp;author=A.%20Achaibou&amp;author=G.%20Pourtois&amp;author=S.%20Schwartz&amp;author=P.%20Vuilleumier" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0180"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb9000" id="ref-id-bb9000" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Adams et al., 2012</span></span></a></span><span class="reference" id="rf9000"><div class="contribution"><div class="authors u-font-sans">R.B. Adams, A.J. Nelson, J.A. Soto, U. Hess, R.E. Kleck</div><div id="ref-id-rf9000" class="title text-m">Emotion in the neutral face: a mechanism for impression formation?</div></div><div class="host u-font-sans">Cogn. Emot., 26 (2012), pp. 431-44110</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1080/02699931.2012.666502" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf9000"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84859631621&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf9000"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20in%20the%20neutral%20face%3A%20a%20mechanism%20for%20impression%20formation&amp;publication_year=2012&amp;author=R.B.%20Adams&amp;author=A.J.%20Nelson&amp;author=J.A.%20Soto&amp;author=U.%20Hess&amp;author=R.E.%20Kleck" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf9000"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0010" id="ref-id-bb0010" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Allison et al., 1999</span></span></a></span><span class="reference" id="rf0185"><div class="contribution"><div class="authors u-font-sans">T. Allison, A. Puce, D.D. Spencer, G. McCarthy</div><div id="ref-id-rf0185" class="title text-m">Electrophysiological studies of human face perception. I: Potentials generated in occipitotemporal cortex by face and non-face stimuli</div></div><div class="host u-font-sans">Cereb. Cortex, 9 (5) (1999), pp. 415-430, <a class="anchor anchor-primary" href="https://doi.org/10.1093/cercor/9.5.415" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1093/cercor/9.5.415</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0032812269&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0185"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Electrophysiological%20studies%20of%20human%20face%20perception.%20I%3A%20Potentials%20generated%20in%20occipitotemporal%20cortex%20by%20face%20and%20non-face%20stimuli&amp;publication_year=1999&amp;author=T.%20Allison&amp;author=A.%20Puce&amp;author=D.D.%20Spencer&amp;author=G.%20McCarthy" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0185"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0015" id="ref-id-bb0015" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Alorda et al., 2007</span></span></a></span><span class="reference" id="rf0190"><div class="contribution"><div class="authors u-font-sans">C. Alorda, I. Serrano-Pedraza, J. Javier Campos-Buenoc, V. Sierra-Vázquez, P. Montoya</div><div id="ref-id-rf0190" class="title text-m">Low spatial frequency filtering modulates early brain processing of affective complex pictures</div></div><div class="host u-font-sans">Neuropsychologia, 45 (2007), pp. 3223-3233</div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0028393207002357/pdfft?md5=91974e2da17412c9fd8e949687491145&amp;pid=1-s2.0-S0028393207002357-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0190"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0028393207002357" aria-describedby="ref-id-rf0190"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-34848855085&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0190"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Low%20spatial%20frequency%20filtering%20modulates%20early%20brain%20processing%20of%20affective%20complex%20pictures&amp;publication_year=2007&amp;author=C.%20Alorda&amp;author=I.%20Serrano-Pedraza&amp;author=J.%20Javier%20Campos-Buenoc&amp;author=V.%20Sierra-V%C3%A1zquez&amp;author=P.%20Montoya" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0190"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0020" id="ref-id-bb0020" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Alpers et al., 2011</span></span></a></span><span class="reference" id="rf0195"><div class="contribution"><div class="authors u-font-sans">G.W. Alpers, D. Adolph, P. Pauli</div><div id="ref-id-rf0195" class="title text-m">Emotional scenes and facial expressions elicit different psychophysiological responses</div></div><div class="host u-font-sans">Int. J. Psychophysiol., 80 (3) (2011), pp. 173-181, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.ijpsycho.2011.01.010" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.ijpsycho.2011.01.010</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0167876011000110/pdfft?md5=bc346964e3e2c4e4bb188ed31a487bed&amp;pid=1-s2.0-S0167876011000110-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0195"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0167876011000110" aria-describedby="ref-id-rf0195"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-79955740882&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0195"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotional%20scenes%20and%20facial%20expressions%20elicit%20different%20psychophysiological%20responses&amp;publication_year=2011&amp;author=G.W.%20Alpers&amp;author=D.%20Adolph&amp;author=P.%20Pauli" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0195"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0025" id="ref-id-bb0025" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Barsalou, 2003</span></span></a></span><span class="reference" id="rf0200"><div class="contribution"><div class="authors u-font-sans">L. Barsalou</div><div id="ref-id-rf0200" class="title text-m">Situated simulation in the human conceptual system</div></div><div class="host u-font-sans">Lang. Cogn. Process., 18 (5-6) (2003), pp. 513-562, <a class="anchor anchor-primary" href="https://doi.org/10.1080/01690960344000026" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1080/01690960344000026</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0041744775&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0200"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Situated%20simulation%20in%20the%20human%20conceptual%20system&amp;publication_year=2003&amp;author=L.%20Barsalou" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0200"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0030" id="ref-id-bb0030" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Barsalou et al., 2003</span></span></a></span><span class="reference" id="rf0205"><div class="contribution"><div class="authors u-font-sans">L.W. Barsalou, W. Kyle Simmons, A.K. Barbey, C.D. Wilson</div><div id="ref-id-rf0205" class="title text-m">Grounding conceptual knowledge in modality-specific systems</div></div><div class="host u-font-sans">Trends Cogn. Sci., 7 (2) (2003), pp. 84-91, <a class="anchor anchor-primary" href="https://doi.org/10.1016/S1364-6613(02)00029-3" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/S1364-6613(02)00029-3</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1364661302000293/pdfft?md5=9e72111ee0d8749a3f2730a547c04d2f&amp;pid=1-s2.0-S1364661302000293-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0205"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1364661302000293" aria-describedby="ref-id-rf0205"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0037312096&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0205"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Grounding%20conceptual%20knowledge%20in%20modality-specific%20systems&amp;publication_year=2003&amp;author=L.W.%20Barsalou&amp;author=W.%20Kyle%20Simmons&amp;author=A.K.%20Barbey&amp;author=C.D.%20Wilson" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0205"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0035" id="ref-id-bb0035" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Batty and Taylor, 2003</span></span></a></span><span class="reference" id="rf0010"><div class="contribution"><div class="authors u-font-sans">M. Batty, M.J. Taylor</div><div id="ref-id-rf0010" class="title text-m">Early processing of the six basic facial emotional expressions</div></div><div class="host u-font-sans">Cogn. Brain Res., 17 (3) (2003), pp. 613-620</div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0926641003001745/pdfft?md5=1a7e4a113a9f4b45856c24eabf924719&amp;pid=1-s2.0-S0926641003001745-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0010"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0926641003001745" aria-describedby="ref-id-rf0010"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-1642270808&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0010"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Early%20processing%20of%20the%20six%20basic%20facial%20emotional%20expressions&amp;publication_year=2003&amp;author=M.%20Batty&amp;author=M.J.%20Taylor" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0010"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0055" id="ref-id-bb0055" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Becker and Srinivasan, 2014</span></span></a></span><span class="reference" id="rf0015"><div class="contribution"><div class="authors u-font-sans">D.V. Becker, N. Srinivasan</div><div id="ref-id-rf0015" class="title text-m">The vividness of the happy face</div></div><div class="host u-font-sans">Curr. Dir. Psychol. Sci., 23 (3) (2014), pp. 189-194</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1177/0963721414533702" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0015"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84901770891&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0015"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20vividness%20of%20the%20happy%20face&amp;publication_year=2014&amp;author=D.V.%20Becker&amp;author=N.%20Srinivasan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0015"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0040" id="ref-id-bb0040" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Bentin et al., 1996</span></span></a></span><span class="reference" id="rf0210"><div class="contribution"><div class="authors u-font-sans">S. Bentin, T. Allison, A. Puce, E. Perez, G. McCarthy</div><div id="ref-id-rf0210" class="title text-m">Electrophysiological studies of face perception in humans</div></div><div class="host u-font-sans">J. Cogn. Neurosci., 8 (6) (1996), pp. 551-565, <a class="anchor anchor-primary" href="https://doi.org/10.1162/jocn.1996.8.6.551" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1162/jocn.1996.8.6.551</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0030469420&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0210"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Electrophysiological%20studies%20of%20face%20perception%20in%20humans&amp;publication_year=1996&amp;author=S.%20Bentin&amp;author=T.%20Allison&amp;author=A.%20Puce&amp;author=E.%20Perez&amp;author=G.%20McCarthy" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0210"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0045" id="ref-id-bb0045" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Blairy et al., 1999</span></span></a></span><span class="reference" id="rf0215"><div class="contribution"><div class="authors u-font-sans">S. Blairy, P. Herrera, U. Hess</div><div id="ref-id-rf0215" class="title text-m">Mimicry and the judgment of emotional facial expressions</div></div><div class="host u-font-sans">J. Nonverbal Behav., 23 (1) (1999), pp. 5-41, <a class="anchor anchor-primary" href="https://doi.org/10.1023/A:1021370825283" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1023/A:1021370825283</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0033095799&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0215"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Mimicry%20and%20the%20judgment%20of%20emotional%20facial%20expressions&amp;publication_year=1999&amp;author=S.%20Blairy&amp;author=P.%20Herrera&amp;author=U.%20Hess" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0215"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb9010" id="ref-id-bb9010" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Bradley and Lang, 1994</span></span></a></span><span class="reference" id="rf9010"><div class="contribution"><div class="authors u-font-sans">M.M. Bradley, P.J. Lang</div><div id="ref-id-rf9010" class="title text-m">Measuring emotion: the self-assessment manikin and the semantic differential</div></div><div class="host u-font-sans">J. Behav. Ther. &amp; Exp. Psychiat., 25 (1) (1994), pp. 49-59</div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/0005791694900639/pdf?md5=093406f8b9ca9ee10d7c705013850d63&amp;pid=1-s2.0-0005791694900639-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf9010"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/0005791694900639" aria-describedby="ref-id-rf9010"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0028307950&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf9010"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Measuring%20emotion%3A%20the%20self-assessment%20manikin%20and%20the%20semantic%20differential&amp;publication_year=1994&amp;author=M.M.%20Bradley&amp;author=P.J.%20Lang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf9010"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0050" id="ref-id-bb0050" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Bradley et al., 2007</span></span></a></span><span class="reference" id="rf0220"><div class="contribution"><div class="authors u-font-sans">M.M. Bradley, S. Hamby, A. Löw, P.J. Lang</div><div id="ref-id-rf0220" class="title text-m">Brain potentials in perception: picture complexity and emotional arousal</div></div><div class="host u-font-sans">Psychophysiology, 44 (3) (2007), pp. 364-373, <a class="anchor anchor-primary" href="https://doi.org/10.1111/j.1469-8986.2007.00520.x" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1111/j.1469-8986.2007.00520.x</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-34247147647&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0220"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Brain%20potentials%20in%20perception%3A%20picture%20complexity%20and%20emotional%20arousal&amp;publication_year=2007&amp;author=M.M.%20Bradley&amp;author=S.%20Hamby&amp;author=A.%20L%C3%B6w&amp;author=P.J.%20Lang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0220"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb9020" id="ref-id-bb9020" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Blasi et al., 2009</span></span></a></span><span class="reference" id="rf9020"><div class="contribution"><div class="authors u-font-sans">G. Blasi, A.R. Hariri, G. Alce, P. Taurisano, F. Sambataro, S. Das, A. Bertolino, D.R. Weinberger, V.S. Mattay</div><div id="ref-id-rf9020" class="title text-m">Preferential amygdala reactivity to the negative assessment of neutral faces</div></div><div class="host u-font-sans">Biol. Psychiatry, 66 (2009), pp. 847-853</div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0006322309007859/pdfft?md5=a7fb4b8bb2894a2d41929e51834b476c&amp;pid=1-s2.0-S0006322309007859-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf9020"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0006322309007859" aria-describedby="ref-id-rf9020"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-70349754175&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf9020"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Preferential%20amygdala%20reactivity%20to%20the%20negative%20assessment%20of%20neutral%20faces&amp;publication_year=2009&amp;author=G.%20Blasi&amp;author=A.R.%20Hariri&amp;author=G.%20Alce&amp;author=P.%20Taurisano&amp;author=F.%20Sambataro&amp;author=S.%20Das&amp;author=A.%20Bertolino&amp;author=D.R.%20Weinberger&amp;author=V.S.%20Mattay" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf9020"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0060" id="ref-id-bb0060" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Britton et al., 2006</span></span></a></span><span class="reference" id="rf0225"><div class="contribution"><div class="authors u-font-sans">J.C. Britton, S.F. Taylor, K.D. Sudheimer, I. Liberzon</div><div id="ref-id-rf0225" class="title text-m">Facial expressions and complex IAPS pictures: common and differential networks</div></div><div class="host u-font-sans">NeuroImage, 31 (2) (2006), pp. 906-919, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.neuroimage.2005.12.050" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.neuroimage.2005.12.050</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1053811905025565/pdfft?md5=2b8a242917ac52cae10cd62505bf1add&amp;pid=1-s2.0-S1053811905025565-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0225"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1053811905025565" aria-describedby="ref-id-rf0225"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-33646853345&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0225"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expressions%20and%20complex%20IAPS%20pictures%3A%20common%20and%20differential%20networks&amp;publication_year=2006&amp;author=J.C.%20Britton&amp;author=S.F.%20Taylor&amp;author=K.D.%20Sudheimer&amp;author=I.%20Liberzon" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0225"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0065" id="ref-id-bb0065" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Bublatzky and Schupp, 2011</span></span></a></span><span class="reference" id="rf0230"><div class="contribution"><div class="authors u-font-sans">F. Bublatzky, H.T. Schupp</div><div id="ref-id-rf0230" class="title text-m">Pictures cueing threat: brain dynamics in viewing explicitly instructed danger cues</div></div><div class="host u-font-sans">Soc. Cogn. Affect. Neurosci., 7 (6) (2011), pp. 611-622, <a class="anchor anchor-primary" href="https://doi.org/10.1093/scan/nsr032" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1093/scan/nsr032</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Pictures%20cueing%20threat%3A%20brain%20dynamics%20in%20viewing%20explicitly%20instructed%20danger%20cues&amp;publication_year=2011&amp;author=F.%20Bublatzky&amp;author=H.T.%20Schupp" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0230"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0070" id="ref-id-bb0070" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Cacioppo et al., 1986</span></span></a></span><span class="reference" id="rf0235"><div class="contribution"><div class="authors u-font-sans">J.T. Cacioppo, R.E. Petty, M.E. Losch, H.S. Kim</div><div id="ref-id-rf0235" class="title text-m">Electromyographic activity over facial muscle regions can differentiate the valence and intensity of affective reactions</div></div><div class="host u-font-sans">J. Pers. Soc. Psychol., 50 (2) (1986), p. 260, <a class="anchor anchor-primary" href="https://doi.org/10.1037//0022-3514.50.2.260" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1037//0022-3514.50.2.260</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0022666852&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0235"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Electromyographic%20activity%20over%20facial%20muscle%20regions%20can%20differentiate%20the%20valence%20and%20intensity%20of%20affective%20reactions&amp;publication_year=1986&amp;author=J.T.%20Cacioppo&amp;author=R.E.%20Petty&amp;author=M.E.%20Losch&amp;author=H.S.%20Kim" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0235"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0075" id="ref-id-bb0075" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Calvo and Beltran, 2013</span></span></a></span><span class="reference" id="rf0020"><div class="contribution"><div class="authors u-font-sans">M.G. Calvo, D. Beltran</div><div id="ref-id-rf0020" class="title text-m">Recognition advantage of happy faces: tracing the neurocognitive processes</div></div><div class="host u-font-sans">Neuropsychologia, 51 (2013), pp. 2051-2060</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84882749978&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0020"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Recognition%20advantage%20of%20happy%20faces%3A%20tracing%20the%20neurocognitive%20processes&amp;publication_year=2013&amp;author=M.G.%20Calvo&amp;author=D.%20Beltran" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0020"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0080" id="ref-id-bb0080" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Calvo and Lundqvist, 2008</span></span></a></span><span class="reference" id="rf0240"><div class="contribution"><div class="authors u-font-sans">M.G. Calvo, D. Lundqvist</div><div id="ref-id-rf0240" class="title text-m">Facial expressions of emotion (KDEF):Identification under different display-duration conditions</div></div><div class="host u-font-sans">Behav. Res. Methods, 40 (2008), pp. 109-115</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-43249130708&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0240"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expressions%20of%20emotion%20%3AIdentification%20under%20different%20display-duration%20conditions&amp;publication_year=2008&amp;author=M.G.%20Calvo&amp;author=D.%20Lundqvist" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0240"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0095" id="ref-id-bb0095" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Carretie et al., 2009</span></span></a></span><span class="reference" id="rf0030"><div class="contribution"><div class="authors u-font-sans">L. Carretie, J. Albert, S. Lopez-Martin, M. Tapia</div><div id="ref-id-rf0030" class="title text-m">Negative brain: an integrative review on the neural processes activated by unpleasant stimuli</div></div><div class="host u-font-sans">Int. J. Psychophysiol., 71 (1) (2009), pp. 57-63</div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0167876008007575/pdfft?md5=323b4438c7fa75f8ba1a5c3008e0a2fa&amp;pid=1-s2.0-S0167876008007575-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0030"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0167876008007575" aria-describedby="ref-id-rf0030"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-58149290054&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0030"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Negative%20brain%3A%20an%20integrative%20review%20on%20the%20neural%20processes%20activated%20by%20unpleasant%20stimuli&amp;publication_year=2009&amp;author=L.%20Carretie&amp;author=J.%20Albert&amp;author=S.%20Lopez-Martin&amp;author=M.%20Tapia" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0030"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0105" id="ref-id-bb0105" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Clark and Hillyard, 1996</span></span></a></span><span class="reference" id="rf0250"><div class="contribution"><div class="authors u-font-sans">V.P. Clark, S.A. Hillyard</div><div id="ref-id-rf0250" class="title text-m">Spatial Selective Attention Affects Early Extrastriate But Not Striate Components of the Visual Evoked Potential</div></div><div class="host u-font-sans">J. Cogn. Neurosci., 8 (5) (1996), pp. 387-402, <a class="anchor anchor-primary" href="https://doi.org/10.1162/jocn.1996.8.5.387" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1162/jocn.1996.8.5.387</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0029788915&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0250"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Spatial%20Selective%20Attention%20Affects%20Early%20Extrastriate%20But%20Not%20Striate%20Components%20of%20the%20Visual%20Evoked%20Potential&amp;publication_year=1996&amp;author=V.P.%20Clark&amp;author=S.A.%20Hillyard" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0250"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0100" id="ref-id-bb0100" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Clark et al., 2008</span></span></a></span><span class="reference" id="rf0245"><div class="contribution"><div class="authors u-font-sans">T.F. Clark, P. Winkielman, D.N. McIntosh</div><div id="ref-id-rf0245" class="title text-m">Autism and the extraction of emotion from briefly presented facial expressions: stumbling at the first step of empathy</div></div><div class="host u-font-sans">Emotion, 8 (6) (2008), p. 803, <a class="anchor anchor-primary" href="https://doi.org/10.1037/A0014124" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1037/A0014124</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-58249121919&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0245"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Autism%20and%20the%20extraction%20of%20emotion%20from%20briefly%20presented%20facial%20expressions%3A%20stumbling%20at%20the%20first%20step%20of%20empathy&amp;publication_year=2008&amp;author=T.F.%20Clark&amp;author=P.%20Winkielman&amp;author=D.N.%20McIntosh" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0245"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0110" id="ref-id-bb0110" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Cuthbert et al., 2000</span></span></a></span><span class="reference" id="rf0035"><div class="contribution"><div class="authors u-font-sans">B.N. Cuthbert, H.T. Schupp, M.M. Bradley, N. Birbaumer, P.J. Lang</div><div id="ref-id-rf0035" class="title text-m">Brain potentials in affective picture processing: covariation with autonomic arousal and affective report</div></div><div class="host u-font-sans">Biol. Psychol., 52 (2) (2000), pp. 95-111</div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0301051199000447/pdfft?md5=d66ea3db780a7f017a0b0604293ab18c&amp;pid=1-s2.0-S0301051199000447-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0035"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0301051199000447" aria-describedby="ref-id-rf0035"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0033997477&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0035"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Brain%20potentials%20in%20affective%20picture%20processing%3A%20covariation%20with%20autonomic%20arousal%20and%20affective%20report&amp;publication_year=2000&amp;author=B.N.%20Cuthbert&amp;author=H.T.%20Schupp&amp;author=M.M.%20Bradley&amp;author=N.%20Birbaumer&amp;author=P.J.%20Lang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0035"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0115" id="ref-id-bb0115" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Dan-Glauser and Scherer, 2011</span></span></a></span><span class="reference" id="rf0255"><div class="contribution"><div class="authors u-font-sans">E.S. Dan-Glauser, K.R. Scherer</div><div id="ref-id-rf0255" class="title text-m">The Geneva affective picture database (GAPED): a new 730-picture database focusing on valence and normative significance</div></div><div class="host u-font-sans">Behav. Res. Methods, 43 (2) (2011), pp. 468-477, <a class="anchor anchor-primary" href="https://doi.org/10.3758/s13428-011-0064-1" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.3758/s13428-011-0064-1</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-79956272191&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0255"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20Geneva%20affective%20picture%20database%20%3A%20a%20new%20730-picture%20database%20focusing%20on%20valence%20and%20normative%20significance&amp;publication_year=2011&amp;author=E.S.%20Dan-Glauser&amp;author=K.R.%20Scherer" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0255"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0120" id="ref-id-bb0120" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">De Cesarei and Codispoti, 2012</span></span></a></span><span class="reference" id="rf0260"><div class="contribution"><div class="authors u-font-sans">A. De Cesarei, M. Codispoti</div><div id="ref-id-rf0260" class="title text-m">Spatial frequencies and emotional perception</div></div><div class="host u-font-sans">Rev. Neurosci., 24 (1) (2012), pp. 1-16</div><div class="comment">(11/2012)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Spatial%20frequencies%20and%20emotional%20perception&amp;publication_year=2012&amp;author=A.%20De%20Cesarei&amp;author=M.%20Codispoti" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0260"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0130" id="ref-id-bb0130" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Deffke et al., 2007</span></span></a></span><span class="reference" id="rf0045"><div class="contribution"><div class="authors u-font-sans">I. Deffke, T. Sander, J. Heidenreich, W. Sommer, G. Curio, L. Trahms, A. Lueschow</div><div id="ref-id-rf0045" class="title text-m">MEG/EEG sources of the 170-ms response to faces are co-localized in the fusiform gyrus</div></div><div class="host u-font-sans">NeuroImage, 35 (4) (2007), pp. 1495-1501</div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S105381190700078X/pdfft?md5=73d5bb10d767ba6b3157f36fa1083723&amp;pid=1-s2.0-S105381190700078X-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0045"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S105381190700078X" aria-describedby="ref-id-rf0045"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-34147104952&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0045"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=MEGEEG%20sources%20of%20the%20170-ms%20response%20to%20faces%20are%20co-localized%20in%20the%20fusiform%20gyrus&amp;publication_year=2007&amp;author=I.%20Deffke&amp;author=T.%20Sander&amp;author=J.%20Heidenreich&amp;author=W.%20Sommer&amp;author=G.%20Curio&amp;author=L.%20Trahms&amp;author=A.%20Lueschow" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0045"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0135" id="ref-id-bb0135" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Delplanque et al., 2007</span></span></a></span><span class="reference" id="rf0050"><div class="contribution"><div class="authors u-font-sans">S. Delplanque, K. N'Diaye, K.R. Scherer, D. Grandjean</div><div id="ref-id-rf0050" class="title text-m">Spatial frequencies or emotional effects? A systematic measure of spatial frequencies for IAPS pictures by a discrete wavelet analysis</div></div><div class="host u-font-sans">J. Neurosci. Methods, 165 (1) (2007), pp. 144-150</div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0165027007002610/pdfft?md5=8067cffa5b2f75843ecdb4bda9ee8562&amp;pid=1-s2.0-S0165027007002610-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0050"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0165027007002610" aria-describedby="ref-id-rf0050"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-34447501836&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0050"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Spatial%20frequencies%20or%20emotional%20effects%20A%20systematic%20measure%20of%20spatial%20frequencies%20for%20IAPS%20pictures%20by%20a%20discrete%20wavelet%20analysis&amp;publication_year=2007&amp;author=S.%20Delplanque&amp;author=K.%20N'Diaye&amp;author=K.R.%20Scherer&amp;author=D.%20Grandjean" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0050"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0140" id="ref-id-bb0140" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Dimberg, 1982</span></span></a></span><span class="reference" id="rf0055"><div class="contribution"><div class="authors u-font-sans">U. Dimberg</div><div id="ref-id-rf0055" class="title text-m">Facial reactions to facial expressions</div></div><div class="host u-font-sans">Psychophysiology, 19 (6) (1982), pp. 643-647</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1111/j.1469-8986.1982.tb02516.x" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0055"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0020360206&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0055"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20reactions%20to%20facial%20expressions&amp;publication_year=1982&amp;author=U.%20Dimberg" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0055"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0145" id="ref-id-bb0145" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Dimberg, 1997</span></span></a></span><span class="reference" id="rf0060"><div class="contribution"><div class="authors u-font-sans">U. Dimberg</div><div id="ref-id-rf0060" class="title text-m">Facial reactions: rapidly evoked emotional responses</div></div><div class="host u-font-sans">J. Psychophysiol., 11 (2) (1997), pp. 115-123</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0030954647&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0060"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20reactions%3A%20rapidly%20evoked%20emotional%20responses&amp;publication_year=1997&amp;author=U.%20Dimberg" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0060"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0155" id="ref-id-bb0155" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Dimberg and Petterson, 2000</span></span></a></span><span class="reference" id="rf0270"><div class="contribution"><div class="authors u-font-sans">U. Dimberg, M. Petterson</div><div id="ref-id-rf0270" class="title text-m">Facial reactions to happy and angry facial expressions: evidence for right hemisphere dominance</div></div><div class="host u-font-sans">Psychophysiology, 37 (5) (2000), pp. 693-696, <a class="anchor anchor-primary" href="https://doi.org/10.1017/S0048577200990759" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1017/S0048577200990759</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0033805792&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0270"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20reactions%20to%20happy%20and%20angry%20facial%20expressions%3A%20evidence%20for%20right%20hemisphere%20dominance&amp;publication_year=2000&amp;author=U.%20Dimberg&amp;author=M.%20Petterson" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0270"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0150" id="ref-id-bb0150" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Dimberg et al., 1998</span></span></a></span><span class="reference" id="rf0265"><div class="contribution"><div class="authors u-font-sans">U. Dimberg, G.Ö. Hansson, M. Thunberg</div><div id="ref-id-rf0265" class="title text-m">Fear of snakes and facial reactions: a case of rapid emotional responding</div></div><div class="host u-font-sans">Scand. J. Psychol., 39 (2) (1998), pp. 75-80, <a class="anchor anchor-primary" href="https://doi.org/10.1111/1467-9450.00059" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1111/1467-9450.00059</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0002001820&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0265"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Fear%20of%20snakes%20and%20facial%20reactions%3A%20a%20case%20of%20rapid%20emotional%20responding&amp;publication_year=1998&amp;author=U.%20Dimberg&amp;author=G.%C3%96.%20Hansson&amp;author=M.%20Thunberg" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0265"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0160" id="ref-id-bb0160" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Dimberg et al., 2000</span></span></a></span><span class="reference" id="rf0065"><div class="contribution"><div class="authors u-font-sans">U. Dimberg, M. Thunberg, K. Elmehed</div><div id="ref-id-rf0065" class="title text-m">Unconscious facial reactions to emotional facial expressions</div></div><div class="host u-font-sans">Psychol. Sci., 11 (1) (2000), pp. 86-89</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1111/1467-9280.00221" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0065"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0033754607&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0065"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Unconscious%20facial%20reactions%20to%20emotional%20facial%20expressions&amp;publication_year=2000&amp;author=U.%20Dimberg&amp;author=M.%20Thunberg&amp;author=K.%20Elmehed" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0065"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0165" id="ref-id-bb0165" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Dimberg et al., 2002</span></span></a></span><span class="reference" id="rf0275"><div class="contribution"><div class="authors u-font-sans">U. Dimberg, M. Thunberg, S. Grunedal</div><div id="ref-id-rf0275" class="title text-m">Facial reactions to emotional stimuli: automatically controlled emotional responses</div></div><div class="host u-font-sans">Cogn. Emot., 16 (4) (2002), pp. 449-471, <a class="anchor anchor-primary" href="https://doi.org/10.1080/02699930143000356" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1080/02699930143000356</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0036304682&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0275"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20reactions%20to%20emotional%20stimuli%3A%20automatically%20controlled%20emotional%20responses&amp;publication_year=2002&amp;author=U.%20Dimberg&amp;author=M.%20Thunberg&amp;author=S.%20Grunedal" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0275"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0170" id="ref-id-bb0170" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Donchin, 1978</span></span></a></span><span class="reference" id="rf0070"><div class="contribution"><div class="authors u-font-sans">E. Donchin</div><div id="ref-id-rf0070" class="title text-m">Use of scalp distribution as a dependent variable in event-related potential studies: excerpts of preconference correspondence</div></div><div class="host u-font-sans">D. Otto (Ed.), Multidisciplinary perspectives in event-related brain potentials research, US Government Printing Office, Washington, DC (1978), pp. 501-510</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Use%20of%20scalp%20distribution%20as%20a%20dependent%20variable%20in%20event-related%20potential%20studies%3A%20excerpts%20of%20preconference%20correspondence&amp;publication_year=1978&amp;author=E.%20Donchin" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0070"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0175" id="ref-id-bb0175" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Eger et al., 2003</span></span></a></span><span class="reference" id="rf0280"><div class="contribution"><div class="authors u-font-sans">E. Eger, A. Jedynak, T. Iwaki, W. Skrandies</div><div id="ref-id-rf0280" class="title text-m">Rapid extraction of emotional expression: evidence from evoked potential fields during brief presentation of face stimuli</div></div><div class="host u-font-sans">Neuropsychologia, 41 (7) (2003), pp. 808-817, <a class="anchor anchor-primary" href="https://doi.org/10.1016/S0028-3932(02)00287-7" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/S0028-3932(02)00287-7</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0028393202002877/pdfft?md5=048a0a3445bb923305287fbc3a8709d4&amp;pid=1-s2.0-S0028393202002877-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0280"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0028393202002877" aria-describedby="ref-id-rf0280"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0037215078&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0280"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Rapid%20extraction%20of%20emotional%20expression%3A%20evidence%20from%20evoked%20potential%20fields%20during%20brief%20presentation%20of%20face%20stimuli&amp;publication_year=2003&amp;author=E.%20Eger&amp;author=A.%20Jedynak&amp;author=T.%20Iwaki&amp;author=W.%20Skrandies" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0280"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0180" id="ref-id-bb0180" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Eimer and Holmes, 2002</span></span></a></span><span class="reference" id="rf0075"><div class="contribution"><div class="authors u-font-sans">M. Eimer, A. Holmes</div><div id="ref-id-rf0075" class="title text-m">An ERP study on the time course of emotional face processing</div></div><div class="host u-font-sans">Neuroreport, 13 (4) (2002), pp. 427-431</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0037171073&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0075"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=An%20ERP%20study%20on%20the%20time%20course%20of%20emotional%20face%20processing&amp;publication_year=2002&amp;author=M.%20Eimer&amp;author=A.%20Holmes" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0075"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0185" id="ref-id-bb0185" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Eisenbarth et al., 2011</span></span></a></span><span class="reference" id="rf0285"><div class="contribution"><div class="authors u-font-sans">H. Eisenbarth, A.B.M. Gerdes, G.W. Alpers</div><div id="ref-id-rf0285" class="title text-m">Motor-incompatibility of facial reactions: the influence of valence and stimulus content on voluntary facial reactions</div></div><div class="host u-font-sans">J. Psychophysiol., 25 (3) (2011), pp. 124-130, <a class="anchor anchor-primary" href="https://doi.org/10.1027/0269-8803/A000048" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1027/0269-8803/A000048</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-79960086982&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0285"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Motor-incompatibility%20of%20facial%20reactions%3A%20the%20influence%20of%20valence%20and%20stimulus%20content%20on%20voluntary%20facial%20reactions&amp;publication_year=2011&amp;author=H.%20Eisenbarth&amp;author=A.B.M.%20Gerdes&amp;author=G.W.%20Alpers" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0285"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0190" id="ref-id-bb0190" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Ekman and Friesen, 1978</span></span></a></span><span class="reference" id="rf0080"><div class="contribution"><div class="authors u-font-sans">P. Ekman, W.V. Friesen</div><div id="ref-id-rf0080" class="title text-m">Facial Action Coding System (FACS): a technique for the measurement of facial action</div></div><div class="host u-font-sans">Consulting Psychologists Press, Palo Alto, CA (1978)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20Action%20Coding%20System%20%3A%20a%20technique%20for%20the%20measurement%20of%20facial%20action&amp;publication_year=1978&amp;author=P.%20Ekman&amp;author=W.V.%20Friesen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0080"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0195" id="ref-id-bb0195" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Epstein et al., 2006</span></span></a></span><span class="reference" id="rf0085"><div class="contribution"><div class="authors u-font-sans">R.A. Epstein, J.S. Higgins, W. Parker, G.K. Aguirre, S. Coopermana</div><div id="ref-id-rf0085" class="title text-m">Cortical correlates of face and scene inversion: a comparison</div></div><div class="host u-font-sans">Neuropsychologia, 44 (2006), pp. 1145-1158</div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0028393205003349/pdfft?md5=6a5c183435046aa0bfda8dadce02c8b1&amp;pid=1-s2.0-S0028393205003349-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0085"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0028393205003349" aria-describedby="ref-id-rf0085"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-33646094470&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0085"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Cortical%20correlates%20of%20face%20and%20scene%20inversion%3A%20a%20comparison&amp;publication_year=2006&amp;author=R.A.%20Epstein&amp;author=J.S.%20Higgins&amp;author=W.%20Parker&amp;author=G.K.%20Aguirre&amp;author=S.%20Coopermana" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0085"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0200" id="ref-id-bb0200" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Esteves et al., 1994</span></span></a></span><span class="reference" id="rf0290"><div class="contribution"><div class="authors u-font-sans">F. Esteves, U. Dimberg, A. öhman</div><div id="ref-id-rf0290" class="title text-m">Automatically elicited fear: conditioned skin conductance responses to masked facial expressions</div></div><div class="host u-font-sans">Cogn. Emot., 8 (5) (1994), pp. 393-413, <a class="anchor anchor-primary" href="https://doi.org/10.1080/02699939408408949" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1080/02699939408408949</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-70350657808&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0290"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Automatically%20elicited%20fear%3A%20conditioned%20skin%20conductance%20responses%20to%20masked%20facial%20expressions&amp;publication_year=1994&amp;author=F.%20Esteves&amp;author=U.%20Dimberg&amp;author=A.%20%C3%B6hman" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0290"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0205" id="ref-id-bb0205" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Feng et al., 2014</span></span></a></span><span class="reference" id="rf0295"><div class="contribution"><div class="authors u-font-sans">C. Feng, W. Li, T. Tian, Y. Luo, R. Gu, C. Zhou, Y.J. Luo</div><div id="ref-id-rf0295" class="title text-m">Arousal modulates valence effects on both early and late stages of affective picture processing in a passive viewing task</div></div><div class="host u-font-sans">Soc. Neurosci., 9 (4) (2014), pp. 364-377, <a class="anchor anchor-primary" href="https://doi.org/10.1080/17470919.2014.896827" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1080/17470919.2014.896827</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84901613350&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0295"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Arousal%20modulates%20valence%20effects%20on%20both%20early%20and%20late%20stages%20of%20affective%20picture%20processing%20in%20a%20passive%20viewing%20task&amp;publication_year=2014&amp;author=C.%20Feng&amp;author=W.%20Li&amp;author=T.%20Tian&amp;author=Y.%20Luo&amp;author=R.%20Gu&amp;author=C.%20Zhou&amp;author=Y.J.%20Luo" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0295"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0210" id="ref-id-bb0210" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Ferri et al., 2012</span></span></a></span><span class="reference" id="rf0300"><div class="contribution"><div class="authors u-font-sans">J. Ferri, A. Weinberg, G. Hajcak</div><div id="ref-id-rf0300" class="title text-m">I see people: the presence of human faces impacts the processing of complex emotional stimuli</div></div><div class="host u-font-sans">Soc. Neurosci., 7 (4) (2012), pp. 436-443, <a class="anchor anchor-primary" href="https://doi.org/10.1080/17470919.2012.680492" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1080/17470919.2012.680492</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84866241577&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0300"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=I%20see%20people%3A%20the%20presence%20of%20human%20faces%20impacts%20the%20processing%20of%20complex%20emotional%20stimuli&amp;publication_year=2012&amp;author=J.%20Ferri&amp;author=A.%20Weinberg&amp;author=G.%20Hajcak" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0300"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0215" id="ref-id-bb0215" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Foroni and Semin, 2011</span></span></a></span><span class="reference" id="rf0305"><div class="contribution"><div class="authors u-font-sans">F. Foroni, G.R. Semin</div><div id="ref-id-rf0305" class="title text-m">When does mimicry affect evaluative judgment?</div></div><div class="host u-font-sans">Emotion, 11 (3) (2011), pp. 687-690, <a class="anchor anchor-primary" href="https://doi.org/10.1037/a0023163" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1037/a0023163</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-79959315364&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0305"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=When%20does%20mimicry%20affect%20evaluative%20judgment&amp;publication_year=2011&amp;author=F.%20Foroni&amp;author=G.R.%20Semin" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0305"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0220" id="ref-id-bb0220" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Foti et al., 2009</span></span></a></span><span class="reference" id="rf0090"><div class="contribution"><div class="authors u-font-sans">D. Foti, G. Hajcak, J. Dien</div><div id="ref-id-rf0090" class="title text-m">Differentiating neural responses to emotional pictures: evidence from temporal‐spatial PCA</div></div><div class="host u-font-sans">Psychophysiology, 46 (3) (2009), pp. 521-530</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1111/j.1469-8986.2009.00796.x" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0090"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-64149125679&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0090"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Differentiating%20neural%20responses%20to%20emotional%20pictures%3A%20evidence%20from%20temporalspatial%20PCA&amp;publication_year=2009&amp;author=D.%20Foti&amp;author=G.%20Hajcak&amp;author=J.%20Dien" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0090"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0225" id="ref-id-bb0225" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Franken et al., 2008</span></span></a></span><span class="reference" id="rf0310"><div class="contribution"><div class="authors u-font-sans">I.A. Franken, P. Muris, I. Nijs, J. van Strien</div><div id="ref-id-rf0310" class="title text-m">Processing of pleasant information can be as fast and strong as unpleasant information: implications for the negativity bias</div></div><div class="host u-font-sans">Neth. J. Psychol., 64 (4) (2008), pp. 168-176, <a class="anchor anchor-primary" href="https://doi.org/10.1007/BF03076419" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/BF03076419</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Processing%20of%20pleasant%20information%20can%20be%20as%20fast%20and%20strong%20as%20unpleasant%20information%3A%20implications%20for%20the%20negativity%20bias&amp;publication_year=2008&amp;author=I.A.%20Franken&amp;author=P.%20Muris&amp;author=I.%20Nijs&amp;author=J.%20van%20Strien" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0310"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0235" id="ref-id-bb0235" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Givre et al., 1994</span></span></a></span><span class="reference" id="rf0320"><div class="contribution"><div class="authors u-font-sans">S.J. Givre, C.E. Schroeder, J.C. Arezzo</div><div id="ref-id-rf0320" class="title text-m">Contribution of extrastriate area V4 to the surface-recorded flash VEP in the awake macaque</div></div><div class="host u-font-sans">Vis. Res., 34 (4) (1994), pp. 415-428, <a class="anchor anchor-primary" href="https://doi.org/10.1016/0042-6989(94)90156-2" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/0042-6989(94)90156-2</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/0042698994901562/pdf?md5=547f22b9bb8b4bc282e52a1fbd6f4d36&amp;pid=1-s2.0-0042698994901562-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0320"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/0042698994901562" aria-describedby="ref-id-rf0320"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0028089073&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0320"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Contribution%20of%20extrastriate%20area%20V4%20to%20the%20surface-recorded%20flash%20VEP%20in%20the%20awake%20macaque&amp;publication_year=1994&amp;author=S.J.%20Givre&amp;author=C.E.%20Schroeder&amp;author=J.C.%20Arezzo" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0320"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0240" id="ref-id-bb0240" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Grèzes et al., 2013</span></span></a></span><span class="reference" id="rf0325"><div class="contribution"><div class="authors u-font-sans">J. Grèzes, L. Philip, M. Chadwick, G. Dezecache, R. Soussignan, L. Conty</div><div id="ref-id-rf0325" class="title text-m">Self-Relevance Appraisal Influences Facial Reactions to Emotional Body Expressions</div></div><div class="host u-font-sans">PLoS One, 8 (2) (2013), p. e55885, <a class="anchor anchor-primary" href="https://doi.org/10.1371/journal.pone.0055885" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1371/journal.pone.0055885</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84873535893&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0325"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Self-Relevance%20Appraisal%20Influences%20Facial%20Reactions%20to%20Emotional%20Body%20Expressions&amp;publication_year=2013&amp;author=J.%20Gr%C3%A8zes&amp;author=L.%20Philip&amp;author=M.%20Chadwick&amp;author=G.%20Dezecache&amp;author=R.%20Soussignan&amp;author=L.%20Conty" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0325"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0250" id="ref-id-bb0250" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Hajcak and Nieuwenhuis, 2006</span></span></a></span><span class="reference" id="rf0330"><div class="contribution"><div class="authors u-font-sans">G. Hajcak, S. Nieuwenhuis</div><div id="ref-id-rf0330" class="title text-m">Reappraisal modulates the electrocortical response to unpleasant pictures</div></div><div class="host u-font-sans">Cogn. Affect. Behav. Neurosci., 6 (4) (2006), pp. 291-297, <a class="anchor anchor-primary" href="https://doi.org/10.3758/CABN.6.4.291" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.3758/CABN.6.4.291</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-34147218698&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0330"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Reappraisal%20modulates%20the%20electrocortical%20response%20to%20unpleasant%20pictures&amp;publication_year=2006&amp;author=G.%20Hajcak&amp;author=S.%20Nieuwenhuis" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0330"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0245" id="ref-id-bb0245" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Hansen and Hansen, 1988</span></span></a></span><span class="reference" id="rf0095"><div class="contribution"><div class="authors u-font-sans">C.H. Hansen, R.D. Hansen</div><div id="ref-id-rf0095" class="title text-m">Finding the face in the crowd: an anger superiority effect</div></div><div class="host u-font-sans">J. Pers. Soc. Psychol., 54 (6) (1988), pp. 917-924</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0024026854&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0095"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Finding%20the%20face%20in%20the%20crowd%3A%20an%20anger%20superiority%20effect&amp;publication_year=1988&amp;author=C.H.%20Hansen&amp;author=R.D.%20Hansen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0095"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0255" id="ref-id-bb0255" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Hariri et al., 2002</span></span></a></span><span class="reference" id="rf0335"><div class="contribution"><div class="authors u-font-sans">A.R. Hariri, A. Tessitore, V.S. Mattay, F. Fera, D.R. Weinberger</div><div id="ref-id-rf0335" class="title text-m">The amygdala response to emotional stimuli: a comparison of faces and scenes</div></div><div class="host u-font-sans">NeuroImage, 17 (1) (2002), pp. 317-323</div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1053811902911791/pdf?md5=867b4011da513ec708e79a2b4d21e746&amp;pid=1-s2.0-S1053811902911791-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0335"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1053811902911791" aria-describedby="ref-id-rf0335"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0036741356&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0335"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20amygdala%20response%20to%20emotional%20stimuli%3A%20a%20comparison%20of%20faces%20and%20scenes&amp;publication_year=2002&amp;author=A.R.%20Hariri&amp;author=A.%20Tessitore&amp;author=V.S.%20Mattay&amp;author=F.%20Fera&amp;author=D.R.%20Weinberger" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0335"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0260" id="ref-id-bb0260" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Herbert et al., 2013a</span></span></a></span><span class="reference" id="rf0340"><div class="contribution"><div class="authors u-font-sans">C. Herbert, R. Deutsch, P. Platte, P. Pauli</div><div id="ref-id-rf0340" class="title text-m">No fear, no panic: probing negation as a means for emotion regulation</div></div><div class="host u-font-sans">Soc. Cogn. Affect. Neurosci., 8 (6) (2013), pp. 654-661, <a class="anchor anchor-primary" href="https://doi.org/10.1093/scan/nss043" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1093/scan/nss043</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84882583189&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0340"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=No%20fear%2C%20no%20panic%3A%20probing%20negation%20as%20a%20means%20for%20emotion%20regulation&amp;publication_year=2013&amp;author=C.%20Herbert&amp;author=R.%20Deutsch&amp;author=P.%20Platte&amp;author=P.%20Pauli" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0340"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0265" id="ref-id-bb0265" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Herbert et al., 2013b</span></span></a></span><span class="reference" id="rf0345"><div class="contribution"><div class="authors u-font-sans">C. Herbert, A. Sfärlea, T. Blumenthal</div><div id="ref-id-rf0345" class="title text-m">Your emotion or mine: labeling feelings alters emotional face perception—an ERP study on automatic and intentional affect labeling</div></div><div class="host u-font-sans">Front. Hum. Neurosci., 7 (2013), <a class="anchor anchor-primary" href="https://doi.org/10.3389/fnhum.2013.00378" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.3389/fnhum.2013.00378</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Your%20emotion%20or%20mine%3A%20labeling%20feelings%20alters%20emotional%20face%20perceptionan%20ERP%20study%20on%20automatic%20and%20intentional%20affect%20labeling&amp;publication_year=2013&amp;author=C.%20Herbert&amp;author=A.%20Sf%C3%A4rlea&amp;author=T.%20Blumenthal" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0345"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb9030" id="ref-id-bb9030" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Hess et al., 2009</span></span></a></span><span class="reference" id="rf9030"><div class="contribution"><div class="authors u-font-sans">U. Hess, R.B. Adams, R.E. Kleck</div><div id="ref-id-rf9030" class="title text-m">The face is not an empty canvas: how facial expressions interact with facial appearance</div></div><div class="host u-font-sans">Philos. Trans. R. Soc. B, 364 (1535) (2009), pp. 3497-3504</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1098/rstb.2009.0165" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf9030"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-73549088326&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf9030"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20face%20is%20not%20an%20empty%20canvas%3A%20how%20facial%20expressions%20interact%20with%20facial%20appearance&amp;publication_year=2009&amp;author=U.%20Hess&amp;author=R.B.%20Adams&amp;author=R.E.%20Kleck" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf9030"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0270" id="ref-id-bb0270" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Hillyard and Anllo-Vento, 1998</span></span></a></span><span class="reference" id="rf0100"><div class="contribution"><div class="authors u-font-sans">S.A. Hillyard, L. Anllo-Vento</div><div id="ref-id-rf0100" class="title text-m">Event-related brain potentials in the study of visual selective attention</div></div><div class="host u-font-sans">Proc. Natl. Acad. Sci., 95 (3) (1998), pp. 781-787</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0032477915&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0100"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Event-related%20brain%20potentials%20in%20the%20study%20of%20visual%20selective%20attention&amp;publication_year=1998&amp;author=S.A.%20Hillyard&amp;author=L.%20Anllo-Vento" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0100"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0275" id="ref-id-bb0275" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Hirai et al., 2008</span></span></a></span><span class="reference" id="rf0350"><div class="contribution"><div class="authors u-font-sans">M. Hirai, S. Watanabe, Y. Honda, K. Miki, R. Kakigi</div><div id="ref-id-rf0350" class="title text-m">Emotional object and scene stimuli modulate subsequent face processing: an event-related potential study</div></div><div class="host u-font-sans">Brain Res. Bull., 77 (5) (2008), pp. 264-273, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.brainresbull.2008.08.011" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.brainresbull.2008.08.011</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0361923008002979/pdfft?md5=559086e2d42acc3200b1f285b1c80ae5&amp;pid=1-s2.0-S0361923008002979-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0350"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0361923008002979" aria-describedby="ref-id-rf0350"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-54549122768&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0350"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotional%20object%20and%20scene%20stimuli%20modulate%20subsequent%20face%20processing%3A%20an%20event-related%20potential%20study&amp;publication_year=2008&amp;author=M.%20Hirai&amp;author=S.%20Watanabe&amp;author=Y.%20Honda&amp;author=K.%20Miki&amp;author=R.%20Kakigi" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0350"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0285" id="ref-id-bb0285" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Holmes et al., 2003</span></span></a></span><span class="reference" id="rf0360"><div class="contribution"><div class="authors u-font-sans">A. Holmes, P. Vuilleumier, M. Eimer</div><div id="ref-id-rf0360" class="title text-m">The processing of emotional facial expression is gated by spatial attention: evidence from event-related brain potentials</div></div><div class="host u-font-sans">Cogn. Brain Res., 16 (2) (2003), pp. 174-184, <a class="anchor anchor-primary" href="https://doi.org/10.1016/S0926-6410(02)00268-9" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/S0926-6410(02)00268-9</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0926641002002689/pdfft?md5=1c7ac635c43c8cfa0aa8a3247a27e8bb&amp;pid=1-s2.0-S0926641002002689-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0360"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0926641002002689" aria-describedby="ref-id-rf0360"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0345103735&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0360"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20processing%20of%20emotional%20facial%20expression%20is%20gated%20by%20spatial%20attention%3A%20evidence%20from%20event-related%20brain%20potentials&amp;publication_year=2003&amp;author=A.%20Holmes&amp;author=P.%20Vuilleumier&amp;author=M.%20Eimer" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0360"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0280" id="ref-id-bb0280" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Holmes et al., 2005</span></span></a></span><span class="reference" id="rf0355"><div class="contribution"><div class="authors u-font-sans">A. Holmes, S. Green, P. Vuilleumier</div><div id="ref-id-rf0355" class="title text-m">The involvement of distinct visual channels in rapid attention towards fearful facial expressions</div></div><div class="host u-font-sans">Cogn. Emot., 19 (6) (2005), pp. 899-922, <a class="anchor anchor-primary" href="https://doi.org/10.1080/02699930441000454" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1080/02699930441000454</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-25844448138&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0355"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20involvement%20of%20distinct%20visual%20channels%20in%20rapid%20attention%20towards%20fearful%20facial%20expressions&amp;publication_year=2005&amp;author=A.%20Holmes&amp;author=S.%20Green&amp;author=P.%20Vuilleumier" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0355"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0290" id="ref-id-bb0290" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Junghöfer et al., 2001</span></span></a></span><span class="reference" id="rf0105"><div class="contribution"><div class="authors u-font-sans">M. Junghöfer, M.M. Bradley, T.R. Elbert, P.J. Lang</div><div id="ref-id-rf0105" class="title text-m">Fleeting images: A new look at early emotion discrimination</div></div><div class="host u-font-sans">Psychophysiology, 38 (2) (2001), pp. 175-178</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0035065245&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0105"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Fleeting%20images%3A%20A%20new%20look%20at%20early%20emotion%20discrimination&amp;publication_year=2001&amp;author=M.%20Jungh%C3%B6fer&amp;author=M.M.%20Bradley&amp;author=T.R.%20Elbert&amp;author=P.J.%20Lang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0105"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0295" id="ref-id-bb0295" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Keightley et al., 2011</span></span></a></span><span class="reference" id="rf0110"><div class="contribution"><div class="authors u-font-sans">M.L. Keightley, K.S. Chiew, J.A.E. Anderson, C.L. Grady</div><div id="ref-id-rf0110" class="title text-m">Neural correlates of recognition memory for emotional faces and scenes</div></div><div class="host u-font-sans">Soc. Cogn. Affect. Neurosci., 6 (1) (2011), pp. 24-37</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1093/scan/nsq003" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0110"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-77957305404&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0110"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Neural%20correlates%20of%20recognition%20memory%20for%20emotional%20faces%20and%20scenes&amp;publication_year=2011&amp;author=M.L.%20Keightley&amp;author=K.S.%20Chiew&amp;author=J.A.E.%20Anderson&amp;author=C.L.%20Grady" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0110"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0300" id="ref-id-bb0300" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Korb et al., 2010</span></span></a></span><span class="reference" id="rf0115"><div class="contribution"><div class="authors u-font-sans">S. Korb, D. Grandjean, K.R. Scherer</div><div id="ref-id-rf0115" class="title text-m">Timing and voluntary suppression of facial mimicry to smiling faces in a Go/NoGo task—an EMG study</div></div><div class="host u-font-sans">Biol. Psychol., 85 (2) (2010), pp. 347-349</div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0301051110002115/pdfft?md5=8e378928b5ab43eedb8f7171f39c695a&amp;pid=1-s2.0-S0301051110002115-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0115"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0301051110002115" aria-describedby="ref-id-rf0115"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-77956745262&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0115"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Timing%20and%20voluntary%20suppression%20of%20facial%20mimicry%20to%20smiling%20faces%20in%20a%20GoNoGo%20taskan%20EMG%20study&amp;publication_year=2010&amp;author=S.%20Korb&amp;author=D.%20Grandjean&amp;author=K.R.%20Scherer" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0115"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0305" id="ref-id-bb0305" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Korb et al., 2012</span></span></a></span><span class="reference" id="rf0365"><div class="contribution"><div class="authors u-font-sans">S. Korb, D. Grandjean, A.C. Samson, S. Delplanque, K.R. Scherer</div><div id="ref-id-rf0365" class="title text-m">Stop laughing! Humor perception with and without expressive suppression</div></div><div class="host u-font-sans">Soc. Neurosci., 7 (5) (2012), pp. 510-524, <a class="anchor anchor-primary" href="https://doi.org/10.1080/17470919.2012.667573" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1080/17470919.2012.667573</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84864648550&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0365"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Stop%20laughing%20Humor%20perception%20with%20and%20without%20expressive%20suppression&amp;publication_year=2012&amp;author=S.%20Korb&amp;author=D.%20Grandjean&amp;author=A.C.%20Samson&amp;author=S.%20Delplanque&amp;author=K.R.%20Scherer" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0365"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0310" id="ref-id-bb0310" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Kujawa et al., 2012</span></span></a></span><span class="reference" id="rf0370"><div class="contribution"><div class="authors u-font-sans">A. Kujawa, D.N. Klein, G. Hajcak</div><div id="ref-id-rf0370" class="title text-m">Electrocortical reactivity to emotional images and faces in middle childhood to early adolescence</div></div><div class="host u-font-sans">Dev. Cogn. Neurosci., 2 (4) (2012), pp. 458-467, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.dcn.2012.03.005" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.dcn.2012.03.005</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1878929312000400/pdfft?md5=e3ac48f780b15ed83b4b98a5370b94fe&amp;pid=1-s2.0-S1878929312000400-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0370"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1878929312000400" aria-describedby="ref-id-rf0370"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84865082627&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0370"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Electrocortical%20reactivity%20to%20emotional%20images%20and%20faces%20in%20middle%20childhood%20to%20early%20adolescence&amp;publication_year=2012&amp;author=A.%20Kujawa&amp;author=D.N.%20Klein&amp;author=G.%20Hajcak" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0370"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0315" id="ref-id-bb0315" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Künecke et al., 2014</span></span></a></span><span class="reference" id="rf0375"><div class="contribution"><div class="authors u-font-sans">J. Künecke, A. Hildebrandt, G. Recio, W. Sommer, O. Wilhelm</div><div id="ref-id-rf0375" class="title text-m">Facial EMG responses to emotional expressions are related to emotion perception ability</div></div><div class="host u-font-sans">PLoS One, 9 (1) (2014), p. e84053, <a class="anchor anchor-primary" href="https://doi.org/10.1371/journal.pone.0084053" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1371/journal.pone.0084053</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84900321502&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0375"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20EMG%20responses%20to%20emotional%20expressions%20are%20related%20to%20emotion%20perception%20ability&amp;publication_year=2014&amp;author=J.%20K%C3%BCnecke&amp;author=A.%20Hildebrandt&amp;author=G.%20Recio&amp;author=W.%20Sommer&amp;author=O.%20Wilhelm" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0375"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0325" id="ref-id-bb0325" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Lang et al., 1993</span></span></a></span><span class="reference" id="rf0385"><div class="contribution"><div class="authors u-font-sans">P. Lang, M. Greenwald, M. Bradley, A. Hamm</div><div id="ref-id-rf0385" class="title text-m">Looking at pictures: affective, facial, visceral, and behavioral reactions</div></div><div class="host u-font-sans">Psychophysiology, 30 (3) (1993), pp. 261-273, <a class="anchor anchor-primary" href="https://doi.org/10.1111/j.1469-8986.1993.tb03352.x" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1111/j.1469-8986.1993.tb03352.x</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0027523286&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0385"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Looking%20at%20pictures%3A%20affective%2C%20facial%2C%20visceral%2C%20and%20behavioral%20reactions&amp;publication_year=1993&amp;author=P.%20Lang&amp;author=M.%20Greenwald&amp;author=M.%20Bradley&amp;author=A.%20Hamm" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0385"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0320" id="ref-id-bb0320" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Lang et al., 2005</span></span></a></span><span class="reference" id="rf0380"><div class="contribution"><div class="authors u-font-sans">P. Lang, M. Bradley, B. Cuthbert</div><div id="ref-id-rf0380" class="title text-m">International affective picture system (IAPS): instruction manual and affective ratings</div></div><div class="host u-font-sans">The Center for Research in Psychophysiology, University of Florida (2005)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=International%20affective%20picture%20system%20%3A%20instruction%20manual%20and%20affective%20ratings&amp;publication_year=2005&amp;author=P.%20Lang&amp;author=M.%20Bradley&amp;author=B.%20Cuthbert" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0380"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb9040" id="ref-id-bb9040" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Lange et al., 2012</span></span></a></span><span class="reference" id="rf9040"><div class="contribution"><div class="authors u-font-sans">W-G. Lange, E. Allart, G.P.J. Keijsers, M. Rinck, E.S. Becker</div><div id="ref-id-rf9040" class="title text-m">A neutral face is not neutral even if you have not seen it: social anxiety disorder and affective priming with facial expressions</div></div><div class="host u-font-sans">Cogn. Behav. Ther., 1–11 (2012)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20neutral%20face%20is%20not%20neutral%20even%20if%20you%20have%20not%20seen%20it%3A%20social%20anxiety%20disorder%20and%20affective%20priming%20with%20facial%20expressions&amp;publication_year=2012&amp;author=W-G.%20Lange&amp;author=E.%20Allart&amp;author=G.P.J.%20Keijsers&amp;author=M.%20Rinck&amp;author=E.S.%20Becker" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf9040"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0330" id="ref-id-bb0330" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Langner et al., 2010</span></span></a></span><span class="reference" id="rf0390"><div class="contribution"><div class="authors u-font-sans">O. Langner, R. Dotsch, G. Bijlstra, D.H.J. Wigboldus, S.T. Hawk, A. Van Knippenberg</div><div id="ref-id-rf0390" class="title text-m">Presentation and validation of the Radboud Faces Database</div></div><div class="host u-font-sans">Cogn. Emot., 24 (8) (2010), pp. 1377-1388, <a class="anchor anchor-primary" href="https://doi.org/10.1080/02699930903485076" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1080/02699930903485076</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-78649717207&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0390"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Presentation%20and%20validation%20of%20the%20Radboud%20Faces%20Database&amp;publication_year=2010&amp;author=O.%20Langner&amp;author=R.%20Dotsch&amp;author=G.%20Bijlstra&amp;author=D.H.J.%20Wigboldus&amp;author=S.T.%20Hawk&amp;author=A.%20Van%20Knippenberg" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0390"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0335" id="ref-id-bb0335" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">LeDoux, 2003</span></span></a></span><span class="reference" id="rf0395"><div class="contribution"><div class="authors u-font-sans">J. LeDoux</div><div id="ref-id-rf0395" class="title text-m">The emotional brain, fear, and the amygdala</div></div><div class="host u-font-sans">Cell. Mol. Neurobiol., 23 (4-5) (2003), pp. 727-738, <a class="anchor anchor-primary" href="https://doi.org/10.1023/A:1025048802629" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1023/A:1025048802629</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0042317090&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0395"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20emotional%20brain%2C%20fear%2C%20and%20the%20amygdala&amp;publication_year=2003&amp;author=J.%20LeDoux" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0395"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0340" id="ref-id-bb0340" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Leppänen and Hietanen, 2004</span></span></a></span><span class="reference" id="rf0400"><div class="contribution"><div class="authors u-font-sans">J.M. Leppänen, J.K. Hietanen</div><div id="ref-id-rf0400" class="title text-m">Positive facial expressions are recognized faster than negative facial expressions, but why?</div></div><div class="host u-font-sans">Psychological Research, 69 (2004), pp. 22-29</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-11944259383&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0400"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Positive%20facial%20expressions%20are%20recognized%20faster%20than%20negative%20facial%20expressions%2C%20but%20why&amp;publication_year=2004&amp;author=J.M.%20Lepp%C3%A4nen&amp;author=J.K.%20Hietanen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0400"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0355" id="ref-id-bb0355" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Leppänen et al., 2008</span></span></a></span><span class="reference" id="rf0405"><div class="contribution"><div class="authors u-font-sans">J.M. Leppänen, J.K. Hietanen, K. Koskinen</div><div id="ref-id-rf0405" class="title text-m">Differential early ERPs to fearful versus neutral facial expressions: a response to the salience of the eyes?</div></div><div class="host u-font-sans">Biol. Psychol., 78 (2) (2008), pp. 150-158, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.biopsycho.2008.02.002" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.biopsycho.2008.02.002</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0301051108000367/pdfft?md5=69a3506d24381432eaeb82d9381515ee&amp;pid=1-s2.0-S0301051108000367-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0405"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0301051108000367" aria-describedby="ref-id-rf0405"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-42649120325&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0405"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Differential%20early%20ERPs%20to%20fearful%20versus%20neutral%20facial%20expressions%3A%20a%20response%20to%20the%20salience%20of%20the%20eyes&amp;publication_year=2008&amp;author=J.M.%20Lepp%C3%A4nen&amp;author=J.K.%20Hietanen&amp;author=K.%20Koskinen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0405"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0360" id="ref-id-bb0360" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Lundqvist et al., 1998</span></span></a></span><span class="reference" id="rf0130"><div class="contribution"><div class="authors u-font-sans">D. Lundqvist, A. Flykt, A. Öhman</div><div id="ref-id-rf0130" class="title text-m">The Karolinska Directed Emotional Faces — KDEF, CD ROM from Department of Clinical Neuroscience, Psychology section</div></div><div class="host u-font-sans">91-630-7164-9, Karolinska Institutet (1998)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20Karolinska%20Directed%20Emotional%20Faces%20%20KDEF%2C%20CD%20ROM%20from%20Department%20of%20Clinical%20Neuroscience%2C%20Psychology%20section&amp;publication_year=1998&amp;author=D.%20Lundqvist&amp;author=A.%20Flykt&amp;author=A.%20%C3%96hman" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0130"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0365" id="ref-id-bb0365" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Magnée et al., 2007</span></span></a></span><span class="reference" id="rf0410"><div class="contribution"><div class="authors u-font-sans">M.J.C.M. Magnée, J.J. Stekelenburg, C. Kemner, B. De Gelder</div><div id="ref-id-rf0410" class="title text-m">Similar facial electromyographic responses to faces, voices, and body expressions</div></div><div class="host u-font-sans">Neuroreport, 18 (4) (2007), pp. 369-372, <a class="anchor anchor-primary" href="https://doi.org/10.1097/WNR.0b013e32801776e6" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1097/WNR.0b013e32801776e6</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-34247609741&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0410"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Similar%20facial%20electromyographic%20responses%20to%20faces%2C%20voices%2C%20and%20body%20expressions&amp;publication_year=2007&amp;author=M.J.C.M.%20Magn%C3%A9e&amp;author=J.J.%20Stekelenburg&amp;author=C.%20Kemner&amp;author=B.%20De%20Gelder" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0410"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0370" id="ref-id-bb0370" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Mangun et al., 1993</span></span></a></span><span class="reference" id="rf0135"><div class="contribution"><div class="authors u-font-sans">G.R. Mangun, S.A. Hillyard, S.J. Luck</div><div id="ref-id-rf0135" class="title text-m">Electrocortical substrates of visual selective attention</div></div><div class="host u-font-sans">D.E.M.S. Kornblum (Ed.), Attention and Performance 14: Synergies in Experimental Psychology, Artificial Intelligence, and Cognitive Neuroscience, The MIT Press, Cambridge, MA, US (1993), pp. 219-243</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.7551/mitpress/1477.003.0018" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0135"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Electrocortical%20substrates%20of%20visual%20selective%20attention&amp;publication_year=1993&amp;author=G.R.%20Mangun&amp;author=S.A.%20Hillyard&amp;author=S.J.%20Luck" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0135"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0375" id="ref-id-bb0375" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Mikels et al., 2005</span></span></a></span><span class="reference" id="rf0140"><div class="contribution"><div class="authors u-font-sans">J.A. Mikels, B.L. Fredrickson, G.R. Larkin, C.M. Lindberg, S.J. Maglio, P.A. Reuter-Lorenz</div><div id="ref-id-rf0140" class="title text-m">Emotional category data on images from the International Affective Picture System</div></div><div class="host u-font-sans">Behav. Res. Methods, 37 (4) (2005), pp. 626-630</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-33646449046&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0140"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotional%20category%20data%20on%20images%20from%20the%20International%20Affective%20Picture%20System&amp;publication_year=2005&amp;author=J.A.%20Mikels&amp;author=B.L.%20Fredrickson&amp;author=G.R.%20Larkin&amp;author=C.M.%20Lindberg&amp;author=S.J.%20Maglio&amp;author=P.A.%20Reuter-Lorenz" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0140"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0380" id="ref-id-bb0380" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Moody et al., 2007</span></span></a></span><span class="reference" id="rf0145"><div class="contribution"><div class="authors u-font-sans">E.J. Moody, D.N. McIntosh, L.J. Mann, K.R. Weisser</div><div id="ref-id-rf0145" class="title text-m">More than mere mimicry? The influence of emotion on rapid facial reactions to faces</div></div><div class="host u-font-sans">Emotion, 7 (2) (2007), p. 447</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1037/1528-3542.7.2.447" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0145"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-34249083042&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0145"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=More%20than%20mere%20mimicry%20The%20influence%20of%20emotion%20on%20rapid%20facial%20reactions%20to%20faces&amp;publication_year=2007&amp;author=E.J.%20Moody&amp;author=D.N.%20McIntosh&amp;author=L.J.%20Mann&amp;author=K.R.%20Weisser" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0145"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0385" id="ref-id-bb0385" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Mühlberger et al., 2009</span></span></a></span><span class="reference" id="rf0415"><div class="contribution"><div class="authors u-font-sans">A. Mühlberger, M. Wieser, M. Herrmann, P. Weyers, C. Tröger, P. Pauli</div><div id="ref-id-rf0415" class="title text-m">Early cortical processing of natural and artificial emotional faces differs between lower and higher socially anxious persons</div></div><div class="host u-font-sans">J. Neural Transm., 116 (6) (2009), pp. 735-746, <a class="anchor anchor-primary" href="https://doi.org/10.1007/s00702-008-0108-6" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s00702-008-0108-6</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-67449131973&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0415"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Early%20cortical%20processing%20of%20natural%20and%20artificial%20emotional%20faces%20differs%20between%20lower%20and%20higher%20socially%20anxious%20persons&amp;publication_year=2009&amp;author=A.%20M%C3%BChlberger&amp;author=M.%20Wieser&amp;author=M.%20Herrmann&amp;author=P.%20Weyers&amp;author=C.%20Tr%C3%B6ger&amp;author=P.%20Pauli" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0415"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0390" id="ref-id-bb0390" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Narumoto et al., 2001</span></span></a></span><span class="reference" id="rf0150"><div class="contribution"><div class="authors u-font-sans">J. Narumoto, T. Okada, N. Sadato, K. Fukui, Y. Yonekura</div><div id="ref-id-rf0150" class="title text-m">Attention to emotion modulates fMRI activity in human right superior temporal sulcus</div></div><div class="host u-font-sans">Cogn. Brain Res., 12 (2) (2001), pp. 225-231</div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0926641001000532/pdfft?md5=000cbde6fd5874775c69a0cfbdcc9cee&amp;pid=1-s2.0-S0926641001000532-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0150"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0926641001000532" aria-describedby="ref-id-rf0150"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0034813989&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0150"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Attention%20to%20emotion%20modulates%20fMRI%20activity%20in%20human%20right%20superior%20temporal%20sulcus&amp;publication_year=2001&amp;author=J.%20Narumoto&amp;author=T.%20Okada&amp;author=N.%20Sadato&amp;author=K.%20Fukui&amp;author=Y.%20Yonekura" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0150"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0395" id="ref-id-bb0395" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Niedenthal, 2007</span></span></a></span><span class="reference" id="rf0420"><div class="contribution"><div class="authors u-font-sans">P.M. Niedenthal</div><div id="ref-id-rf0420" class="title text-m">Embodying emotion</div></div><div class="host u-font-sans">Science, 316 (5827) (2007), pp. 1002-1005, <a class="anchor anchor-primary" href="https://doi.org/10.1126/science.1136930" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1126/science.1136930</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-34249001070&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0420"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Embodying%20emotion&amp;publication_year=2007&amp;author=P.M.%20Niedenthal" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0420"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0400" id="ref-id-bb0400" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Niedenthal et al., 2001</span></span></a></span><span class="reference" id="rf0155"><div class="contribution"><div class="authors u-font-sans">P.M. Niedenthal, M. Brauer, J.B. Halberstadt, Å.H. Innes-Ker</div><div id="ref-id-rf0155" class="title text-m">When did her smile drop? Facial mimicry and the influences of emotional state on the detection of change in emotional expression</div></div><div class="host u-font-sans">Cogn. Emot., 15 (6) (2001), pp. 853-864</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0035168852&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0155"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=When%20did%20her%20smile%20drop%20Facial%20mimicry%20and%20the%20influences%20of%20emotional%20state%20on%20the%20detection%20of%20change%20in%20emotional%20expression&amp;publication_year=2001&amp;author=P.M.%20Niedenthal&amp;author=M.%20Brauer&amp;author=J.B.%20Halberstadt&amp;author=%C3%85.H.%20Innes-Ker" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0155"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0405" id="ref-id-bb0405" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Oberman et al., 2007</span></span></a></span><span class="reference" id="rf0425"><div class="contribution"><div class="authors u-font-sans">L.M. Oberman, P. Winkielman, V.S. Ramachandran</div><div id="ref-id-rf0425" class="title text-m">Face to face: blocking facial mimicry can selectively impair recognition of emotional expressions</div></div><div class="host u-font-sans">Soc. Neurosci., 2 (3-4) (2007), pp. 167-178, <a class="anchor anchor-primary" href="https://doi.org/10.1080/17470910701391943" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1080/17470910701391943</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-34547600602&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0425"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Face%20to%20face%3A%20blocking%20facial%20mimicry%20can%20selectively%20impair%20recognition%20of%20emotional%20expressions&amp;publication_year=2007&amp;author=L.M.%20Oberman&amp;author=P.%20Winkielman&amp;author=V.S.%20Ramachandran" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0425"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0410" id="ref-id-bb0410" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Öhman, 2005</span></span></a></span><span class="reference" id="rf0430"><div class="contribution"><div class="authors u-font-sans">A. Öhman</div><div id="ref-id-rf0430" class="title text-m">The role of the amygdala in human fear: automatic detection of threat</div></div><div class="host u-font-sans">Psychoneuroendocrinology, 30 (10) (2005), pp. 953-958, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.psyneuen.2005.03.019" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.psyneuen.2005.03.019</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0306453005001022/pdfft?md5=60365c6e227943debbdebe9b3a3b1573&amp;pid=1-s2.0-S0306453005001022-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0430"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0306453005001022" aria-describedby="ref-id-rf0430"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-22144456449&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0430"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20role%20of%20the%20amygdala%20in%20human%20fear%3A%20automatic%20detection%20of%20threat&amp;publication_year=2005&amp;author=A.%20%C3%96hman" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0430"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0415" id="ref-id-bb0415" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Öhman and Mineka, 2001</span></span></a></span><span class="reference" id="rf0435"><div class="contribution"><div class="authors u-font-sans">A. Öhman, S. Mineka</div><div id="ref-id-rf0435" class="title text-m">Fears, phobias, and preparedness: toward an evolved module of fear and fear learning</div></div><div class="host u-font-sans">Psychol. Rev., 108 (3) (2001), pp. 483-522, <a class="anchor anchor-primary" href="https://doi.org/10.1037/0033-295X.108.3.483" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1037/0033-295X.108.3.483</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85047683645&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0435"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Fears%2C%20phobias%2C%20and%20preparedness%3A%20toward%20an%20evolved%20module%20of%20fear%20and%20fear%20learning&amp;publication_year=2001&amp;author=A.%20%C3%96hman&amp;author=S.%20Mineka" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0435"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0420" id="ref-id-bb0420" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Olofsson et al., 2008</span></span></a></span><span class="reference" id="rf0440"><div class="contribution"><div class="authors u-font-sans">J.K. Olofsson, S. Nordin, H. Sequeira, J. Polich</div><div id="ref-id-rf0440" class="title text-m">Affective picture processing: an integrative review of ERP findings</div></div><div class="host u-font-sans">Biol. Psychol., 77 (3) (2008), pp. 247-265, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.biopsycho.2007.11.006" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.biopsycho.2007.11.006</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0301051107001913/pdfft?md5=63adf451ab0652400da9c052a8c0d5bf&amp;pid=1-s2.0-S0301051107001913-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0440"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0301051107001913" aria-describedby="ref-id-rf0440"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-39549116733&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0440"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Affective%20picture%20processing%3A%20an%20integrative%20review%20of%20ERP%20findings&amp;publication_year=2008&amp;author=J.K.%20Olofsson&amp;author=S.%20Nordin&amp;author=H.%20Sequeira&amp;author=J.%20Polich" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0440"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0425" id="ref-id-bb0425" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Pecchinenda, 1996</span></span></a></span><span class="reference" id="rf0445"><div class="contribution"><div class="authors u-font-sans">A. Pecchinenda</div><div id="ref-id-rf0445" class="title text-m">The affective significance of skin conductance activity during a difficult problem-solving task</div></div><div class="host u-font-sans">Cogn. Emot., 10 (5) (1996), pp. 481-504, <a class="anchor anchor-primary" href="https://doi.org/10.1080/026999396380123" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1080/026999396380123</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0000413484&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0445"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20affective%20significance%20of%20skin%20conductance%20activity%20during%20a%20difficult%20problem-solving%20task&amp;publication_year=1996&amp;author=A.%20Pecchinenda" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0445"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0430" id="ref-id-bb0430" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Pizzagalli et al., 1999</span></span></a></span><span class="reference" id="rf0450"><div class="contribution"><div class="authors u-font-sans">D. Pizzagalli, M. Regard, D. Lehmann</div><div id="ref-id-rf0450" class="title text-m">Rapid emotional face processing in the human right and left brain hemispheres: an ERP study</div></div><div class="host u-font-sans">Neuroreport, 10 (13) (1999), pp. 2691-2698, <a class="anchor anchor-primary" href="https://doi.org/10.1097/00001756-199909090-00001" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1097/00001756-199909090-00001</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0033538965&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0450"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Rapid%20emotional%20face%20processing%20in%20the%20human%20right%20and%20left%20brain%20hemispheres%3A%20an%20ERP%20study&amp;publication_year=1999&amp;author=D.%20Pizzagalli&amp;author=M.%20Regard&amp;author=D.%20Lehmann" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0450"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0435" id="ref-id-bb0435" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Ponari et al., 2012</span></span></a></span><span class="reference" id="rf0455"><div class="contribution"><div class="authors u-font-sans">M. Ponari, M. Conson, N.P. D'Amico, D. Grossi, L. Trojano</div><div id="ref-id-rf0455" class="title text-m">Mapping correspondence between facial mimicry and emotion recognition in healthy subjects</div></div><div class="host u-font-sans">Emotion, 12 (6) (2012), <a class="anchor anchor-primary" href="https://doi.org/10.1037/A0028588" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1037/A0028588</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">(Advance online, publication. doi: <a class="anchor anchor-primary" href="https://doi.org/10.1037/a0028588" target="_blank"><span class="anchor-text-container"><span class="anchor-text">http://dx.doi.org/10.1037/a0028588</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>)</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Mapping%20correspondence%20between%20facial%20mimicry%20and%20emotion%20recognition%20in%20healthy%20subjects&amp;publication_year=2012&amp;author=M.%20Ponari&amp;author=M.%20Conson&amp;author=N.P.%20D'Amico&amp;author=D.%20Grossi&amp;author=L.%20Trojano" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0455"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0440" id="ref-id-bb0440" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Pourtois et al., 2005</span></span></a></span><span class="reference" id="rf0460"><div class="contribution"><div class="authors u-font-sans">G. Pourtois, E.S. Dan, D. Grandjean, D. Sander, P. Vuilleumier</div><div id="ref-id-rf0460" class="title text-m">Enhanced extrastriate visual response to bandpass spatial frequency filtered fearful faces: time course and topographic evoked-potentials mapping</div></div><div class="host u-font-sans">Hum. Brain Mapp., 26 (1) (2005), pp. 65-79, <a class="anchor anchor-primary" href="https://doi.org/10.1002/hbm.20130" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1002/hbm.20130</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-24144449550&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0460"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Enhanced%20extrastriate%20visual%20response%20to%20bandpass%20spatial%20frequency%20filtered%20fearful%20faces%3A%20time%20course%20and%20topographic%20evoked-potentials%20mapping&amp;publication_year=2005&amp;author=G.%20Pourtois&amp;author=E.S.%20Dan&amp;author=D.%20Grandjean&amp;author=D.%20Sander&amp;author=P.%20Vuilleumier" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0460"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0445" id="ref-id-bb0445" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Proverbio et al., 2009</span></span></a></span><span class="reference" id="rf0465"><div class="contribution"><div class="authors u-font-sans">A.M. Proverbio, R. Adorni, A. Zani, L. Trestianu</div><div id="ref-id-rf0465" class="title text-m">Sex differences in the brain response to affective scenes with or without humans</div></div><div class="host u-font-sans">Neuropsychologia, 47 (12) (2009), pp. 2374-2388, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.neuropsychologia.2008.10.030" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.neuropsychologia.2008.10.030</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0028393208004314/pdfft?md5=ac5f468092989da46a9ce38a1e37aa4e&amp;pid=1-s2.0-S0028393208004314-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0465"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0028393208004314" aria-describedby="ref-id-rf0465"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-67650072507&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0465"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Sex%20differences%20in%20the%20brain%20response%20to%20affective%20scenes%20with%20or%20without%20humans&amp;publication_year=2009&amp;author=A.M.%20Proverbio&amp;author=R.%20Adorni&amp;author=A.%20Zani&amp;author=L.%20Trestianu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0465"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0450" id="ref-id-bb0450" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Reaz et al., 2006</span></span></a></span><span class="reference" id="rf0160"><div class="contribution"><div class="authors u-font-sans">M.B.I. Reaz, M.S. Hussain, F. Mohd-Yasin</div><div id="ref-id-rf0160" class="title text-m">Techniques of EMG signal analysis: detection, processing, classification and applications</div></div><div class="host u-font-sans">Biol. Proced. Online, 8 (1) (2006), pp. 11-35</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1251/bpo115" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0160"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-33645224070&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0160"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Techniques%20of%20EMG%20signal%20analysis%3A%20detection%2C%20processing%2C%20classification%20and%20applications&amp;publication_year=2006&amp;author=M.B.I.%20Reaz&amp;author=M.S.%20Hussain&amp;author=F.%20Mohd-Yasin" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0160"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0455" id="ref-id-bb0455" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Recio et al., 2014</span></span></a></span><span class="reference" id="rf0470"><div class="contribution"><div class="authors u-font-sans">G. Recio, A. Schacht, W. Sommer</div><div id="ref-id-rf0470" class="title text-m">Recognizing dynamic facial expressions of emotion: specificity and intensity effects in event-related brain potentials</div></div><div class="host u-font-sans">Biol. Psychol., 96 (2014), pp. 111-125, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.biopsycho.2013.12.003" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.biopsycho.2013.12.003</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0301051113002500/pdfft?md5=90bdef92bce55a047325d3830dadd8ff&amp;pid=1-s2.0-S0301051113002500-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0470"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0301051113002500" aria-describedby="ref-id-rf0470"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84891640777&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0470"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Recognizing%20dynamic%20facial%20expressions%20of%20emotion%3A%20specificity%20and%20intensity%20effects%20in%20event-related%20brain%20potentials&amp;publication_year=2014&amp;author=G.%20Recio&amp;author=A.%20Schacht&amp;author=W.%20Sommer" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0470"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0460" id="ref-id-bb0460" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Rossion et al., 2003</span></span></a></span><span class="reference" id="rf0475"><div class="contribution"><div class="authors u-font-sans">B. Rossion, C.A. Joyce, G.W. Cottrell, M.J. Tarr</div><div id="ref-id-rf0475" class="title text-m">Early lateralization and orientation tuning for face, word, and object processing in the visual cortex</div></div><div class="host u-font-sans">NeuroImage, 20 (3) (2003), pp. 1609-1624, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.neuroimage.2003.07.010" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.neuroimage.2003.07.010</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1053811903004609/pdfft?md5=b1f8831d47fb6c31315cded70780de21&amp;pid=1-s2.0-S1053811903004609-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0475"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1053811903004609" aria-describedby="ref-id-rf0475"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0344983777&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0475"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Early%20lateralization%20and%20orientation%20tuning%20for%20face%2C%20word%2C%20and%20object%20processing%20in%20the%20visual%20cortex&amp;publication_year=2003&amp;author=B.%20Rossion&amp;author=C.A.%20Joyce&amp;author=G.W.%20Cottrell&amp;author=M.J.%20Tarr" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0475"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0465" id="ref-id-bb0465" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Rugg et al., 1987</span></span></a></span><span class="reference" id="rf0480"><div class="contribution"><div class="authors u-font-sans">M.D. Rugg, A.D. Milner, C.R. Lines, R. Phalp</div><div id="ref-id-rf0480" class="title text-m">Modulation of visual event-related potentials by spatial and non-spatial visual selective attention</div></div><div class="host u-font-sans">Neuropsychologia, 25 (1 PART 1) (1987), pp. 85-96, <a class="anchor anchor-primary" href="https://doi.org/10.1016/0028-3932(87)90045-5" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/0028-3932(87)90045-5</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/0028393287900455/pdf?md5=8e1a06f6502db42510b5ce047575885b&amp;pid=1-s2.0-0028393287900455-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0480"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/0028393287900455" aria-describedby="ref-id-rf0480"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0023097832&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0480"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Modulation%20of%20visual%20event-related%20potentials%20by%20spatial%20and%20non-spatial%20visual%20selective%20attention&amp;publication_year=1987&amp;author=M.D.%20Rugg&amp;author=A.D.%20Milner&amp;author=C.R.%20Lines&amp;author=R.%20Phalp" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0480"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0470" id="ref-id-bb0470" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Sabatinelli et al., 2011</span></span></a></span><span class="reference" id="rf0485"><div class="contribution"><div class="authors u-font-sans">D. Sabatinelli, E.E. Fortune, Q. Li, A. Siddiqui, C. Krafft, W.T. Oliver, S. Beck, J. Jeffries</div><div id="ref-id-rf0485" class="title text-m">Emotional perception: meta-analyses of face and natural scene processing</div></div><div class="host u-font-sans">NeuroImage, 54 (3) (2011), pp. 2524-2533, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.neuroimage.2010.10.011" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.neuroimage.2010.10.011</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1053811910013030/pdfft?md5=45b61763b41ba78080962af1686ceed9&amp;pid=1-s2.0-S1053811910013030-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0485"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1053811910013030" aria-describedby="ref-id-rf0485"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-78650215212&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0485"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotional%20perception%3A%20meta-analyses%20of%20face%20and%20natural%20scene%20processing&amp;publication_year=2011&amp;author=D.%20Sabatinelli&amp;author=E.E.%20Fortune&amp;author=Q.%20Li&amp;author=A.%20Siddiqui&amp;author=C.%20Krafft&amp;author=W.T.%20Oliver&amp;author=S.%20Beck&amp;author=J.%20Jeffries" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0485"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0475" id="ref-id-bb0475" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Sakaki et al., 2012</span></span></a></span><span class="reference" id="rf0490"><div class="contribution"><div class="authors u-font-sans">M. Sakaki, K. Niki, M. Mather</div><div id="ref-id-rf0490" class="title text-m">Beyond arousal and valence: the importance of the biological versus social relevance of emotional stimuli</div></div><div class="host u-font-sans">Cogn. Affect. Behav. Neurosci., 12 (1) (2012), pp. 115-139, <a class="anchor anchor-primary" href="https://doi.org/10.3758/s13415-011-0062-x" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.3758/s13415-011-0062-x</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84857187152&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0490"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Beyond%20arousal%20and%20valence%3A%20the%20importance%20of%20the%20biological%20versus%20social%20relevance%20of%20emotional%20stimuli&amp;publication_year=2012&amp;author=M.%20Sakaki&amp;author=K.%20Niki&amp;author=M.%20Mather" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0490"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0490" id="ref-id-bb0490" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Schupp et al., 2003</span></span></a></span><span class="reference" id="rf0505"><div class="contribution"><div class="authors u-font-sans">H.T. Schupp, J. Markus, A.I. Weike, A.O. Hamm</div><div id="ref-id-rf0505" class="title text-m">Emotional Facilitation of Sensory Processing in the Visual Cortex</div></div><div class="host u-font-sans">Psychol. Sci., 14 (1) (2003), pp. 7-13, <a class="anchor anchor-primary" href="https://doi.org/10.1111/1467-9280.01411" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1111/1467-9280.01411</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0037270753&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0505"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotional%20Facilitation%20of%20Sensory%20Processing%20in%20the%20Visual%20Cortex&amp;publication_year=2003&amp;author=H.T.%20Schupp&amp;author=J.%20Markus&amp;author=A.I.%20Weike&amp;author=A.O.%20Hamm" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0505"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0485" id="ref-id-bb0485" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Schupp et al., 2004a</span></span></a></span><span class="reference" id="rf0500"><div class="contribution"><div class="authors u-font-sans">H.T. Schupp, M. Junghöfer, A.I. Weike, A.O. Hamm</div><div id="ref-id-rf0500" class="title text-m">The selective processing of briefly presented affective pictures: an ERP analysis</div></div><div class="host u-font-sans">Psychophysiology, 41 (3) (2004), pp. 441-449, <a class="anchor anchor-primary" href="https://doi.org/10.1111/j.1469-8986.2004.00174.x" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1111/j.1469-8986.2004.00174.x</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-2642512428&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0500"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20selective%20processing%20of%20briefly%20presented%20affective%20pictures%3A%20an%20ERP%20analysis&amp;publication_year=2004&amp;author=H.T.%20Schupp&amp;author=M.%20Jungh%C3%B6fer&amp;author=A.I.%20Weike&amp;author=A.O.%20Hamm" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0500"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0495" id="ref-id-bb0495" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Schupp et al., 2004b</span></span></a></span><span class="reference" id="rf0510"><div class="contribution"><div class="authors u-font-sans">H.T. Schupp, A. Öhman, M. Junghöfer, A.I. Weike, J. Stockburger, A.O. Hamm</div><div id="ref-id-rf0510" class="title text-m">The facilitated processing of threatening faces: an ERP analysis</div></div><div class="host u-font-sans">Emotion, 4 (2) (2004), p. 189, <a class="anchor anchor-primary" href="https://doi.org/10.1037/1528-3542.4.2.189" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1037/1528-3542.4.2.189</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-2942627716&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0510"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20facilitated%20processing%20of%20threatening%20faces%3A%20an%20ERP%20analysis&amp;publication_year=2004&amp;author=H.T.%20Schupp&amp;author=A.%20%C3%96hman&amp;author=M.%20Jungh%C3%B6fer&amp;author=A.I.%20Weike&amp;author=J.%20Stockburger&amp;author=A.O.%20Hamm" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0510"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0480" id="ref-id-bb0480" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Schupp et al., 2006a</span></span></a></span><span class="reference" id="rf0495"><div class="contribution"><div class="authors u-font-sans">H.T. Schupp, T. Flaisch, J. Stockburger, M. Junghöfer</div><div id="ref-id-rf0495" class="title text-m">Emotion and attention: event-related brain potential studies</div></div><div class="host u-font-sans">G.E.M.J.J.K.S. Anders, D. Wildgruber (Eds.), Progress in brain research, vol. 156, Elsevier (2006), pp. 31-51</div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0079612306560029/pdfft?md5=079642e3c787e99b229359062067aa2e&amp;pid=1-s2.0-S0079612306560029-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0495"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0079612306560029" aria-describedby="ref-id-rf0495"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-33749044591&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0495"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20and%20attention%3A%20event-related%20brain%20potential%20studies&amp;publication_year=2006&amp;author=H.T.%20Schupp&amp;author=T.%20Flaisch&amp;author=J.%20Stockburger&amp;author=M.%20Jungh%C3%B6fer" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0495"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0510" id="ref-id-bb0510" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Schupp et al., 2006b</span></span></a></span><span class="reference" id="rf0525"><div class="contribution"><div class="authors u-font-sans">H.T. Schupp, J. Stockburger, M. Codispoti, M. Junghöfer, A.I. Weike, A.O. Hamm</div><div id="ref-id-rf0525" class="title text-m">Stimulus novelty and emotion perception: the near absence of habituation in the visual cortex</div></div><div class="host u-font-sans">Neuroreport, 17 (4) (2006), pp. 365-369, <a class="anchor anchor-primary" href="https://doi.org/10.1097/01.wnr.0000203355.88061.c6" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1097/01.wnr.0000203355.88061.c6</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-33646710988&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0525"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Stimulus%20novelty%20and%20emotion%20perception%3A%20the%20near%20absence%20of%20habituation%20in%20the%20visual%20cortex&amp;publication_year=2006&amp;author=H.T.%20Schupp&amp;author=J.%20Stockburger&amp;author=M.%20Codispoti&amp;author=M.%20Jungh%C3%B6fer&amp;author=A.I.%20Weike&amp;author=A.O.%20Hamm" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0525"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0515" id="ref-id-bb0515" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Schupp et al., 2007</span></span></a></span><span class="reference" id="rf0530"><div class="contribution"><div class="authors u-font-sans">H.T. Schupp, J. Stockburger, M. Codispoti, M. Junghöfer, A.I. Weike, A.O. Hamm</div><div id="ref-id-rf0530" class="title text-m">Selective visual attention to emotion</div></div><div class="host u-font-sans">J. Neurosci., 27 (5) (2007), pp. 1082-1089, <a class="anchor anchor-primary" href="https://doi.org/10.1523/JNEUROSCI.3223-06.2007" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1523/JNEUROSCI.3223-06.2007</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-33846815034&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0530"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Selective%20visual%20attention%20to%20emotion&amp;publication_year=2007&amp;author=H.T.%20Schupp&amp;author=J.%20Stockburger&amp;author=M.%20Codispoti&amp;author=M.%20Jungh%C3%B6fer&amp;author=A.I.%20Weike&amp;author=A.O.%20Hamm" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0530"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0500" id="ref-id-bb0500" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Schupp et al., 2013a</span></span></a></span><span class="reference" id="rf0515"><div class="contribution"><div class="authors u-font-sans">H.T. Schupp, R. Schmälzle, T. Flaisch</div><div id="ref-id-rf0515" class="title text-m">Explicit semantic stimulus categorization interferes with implicit emotion processing</div></div><div class="host u-font-sans">Soc. Cogn. Affect. Neurosci. (2013), <a class="anchor anchor-primary" href="https://doi.org/10.1093/scan/nst171" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1093/scan/nst171</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Explicit%20semantic%20stimulus%20categorization%20interferes%20with%20implicit%20emotion%20processing&amp;publication_year=2013&amp;author=H.T.%20Schupp&amp;author=R.%20Schm%C3%A4lzle&amp;author=T.%20Flaisch" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0515"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0505" id="ref-id-bb0505" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Schupp et al., 2013b</span></span></a></span><span class="reference" id="rf0520"><div class="contribution"><div class="authors u-font-sans">H.T. Schupp, R. Schmälzle, T. Flaisch, A.I. Weike, A.O. Hamm</div><div id="ref-id-rf0520" class="title text-m">Reprint of “Affective picture processing as a function of preceding picture valence: an ERP analysis”</div></div><div class="host u-font-sans">Biol. Psychol., 92 (3) (2013), pp. 520-525, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.biopsycho.2013.02.002" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.biopsycho.2013.02.002</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0301051113000331/pdfft?md5=2664c2303e1fc8c2275edaa9470b4a3c&amp;pid=1-s2.0-S0301051113000331-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0520"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0301051113000331" aria-describedby="ref-id-rf0520"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84875092400&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0520"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Reprint%20of%20Affective%20picture%20processing%20as%20a%20function%20of%20preceding%20picture%20valence%3A%20an%20ERP%20analysis&amp;publication_year=2013&amp;author=H.T.%20Schupp&amp;author=R.%20Schm%C3%A4lzle&amp;author=T.%20Flaisch&amp;author=A.I.%20Weike&amp;author=A.O.%20Hamm" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0520"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0520" id="ref-id-bb0520" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Semlitsch et al., 1986</span></span></a></span><span class="reference" id="rf0165"><div class="contribution"><div class="authors u-font-sans">H.V. Semlitsch, P. Anderer, P. Schuster, O. Presslich</div><div id="ref-id-rf0165" class="title text-m">A solution for reliable and valid reduction of ocular artifacts, applied to the P300 ERP</div></div><div class="host u-font-sans">Psychophysiology, 23 (6) (1986), pp. 695-703</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1111/j.1469-8986.1986.tb00696.x" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0165"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0022871109&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0165"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20solution%20for%20reliable%20and%20valid%20reduction%20of%20ocular%20artifacts%2C%20applied%20to%20the%20P300%20ERP&amp;publication_year=1986&amp;author=H.V.%20Semlitsch&amp;author=P.%20Anderer&amp;author=P.%20Schuster&amp;author=O.%20Presslich" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0165"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0525" id="ref-id-bb0525" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Smith et al., 2013</span></span></a></span><span class="reference" id="rf0535"><div class="contribution"><div class="authors u-font-sans">E. Smith, A. Weinberg, T. Moran, G. Hajcak</div><div id="ref-id-rf0535" class="title text-m">Electrocortical responses to NIMSTIM facial expressions of emotion</div></div><div class="host u-font-sans">Int. J. Psychophysiol., 88 (1) (2013), pp. 17-25, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.ijpsycho.2012.12.004" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.ijpsycho.2012.12.004</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0167876012006782/pdfft?md5=e964d38c5dbbff92180a119c6be2ee5a&amp;pid=1-s2.0-S0167876012006782-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0535"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0167876012006782" aria-describedby="ref-id-rf0535"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84877612207&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0535"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Electrocortical%20responses%20to%20NIMSTIM%20facial%20expressions%20of%20emotion&amp;publication_year=2013&amp;author=E.%20Smith&amp;author=A.%20Weinberg&amp;author=T.%20Moran&amp;author=G.%20Hajcak" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0535"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0530" id="ref-id-bb0530" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Sonnby-Borgström, 2002</span></span></a></span><span class="reference" id="rf0540"><div class="contribution"><div class="authors u-font-sans">M. Sonnby-Borgström</div><div id="ref-id-rf0540" class="title text-m">Automatic mimicry reactions as related to differences in emotional empathy</div></div><div class="host u-font-sans">Scand. J. Psychol., 43 (5) (2002), pp. 433-443, <a class="anchor anchor-primary" href="https://doi.org/10.1111/1467-9450.00312" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1111/1467-9450.00312</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0036884832&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0540"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Automatic%20mimicry%20reactions%20as%20related%20to%20differences%20in%20emotional%20empathy&amp;publication_year=2002&amp;author=M.%20Sonnby-Borgstr%C3%B6m" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0540"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0535" id="ref-id-bb0535" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Stekelenburg and de Gelder, 2004</span></span></a></span><span class="reference" id="rf0170"><div class="contribution"><div class="authors u-font-sans">J.J. Stekelenburg, B. de Gelder</div><div id="ref-id-rf0170" class="title text-m">The neural correlates of perceiving human bodies: an ERP study on the body-inversion effect</div></div><div class="host u-font-sans">Neuroreport, 15 (5) (2004), pp. 777-780</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-1842504479&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0170"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20neural%20correlates%20of%20perceiving%20human%20bodies%3A%20an%20ERP%20study%20on%20the%20body-inversion%20effect&amp;publication_year=2004&amp;author=J.J.%20Stekelenburg&amp;author=B.%20de%20Gelder" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0170"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0540" id="ref-id-bb0540" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Streit et al., 2003</span></span></a></span><span class="reference" id="rf0545"><div class="contribution"><div class="authors u-font-sans">M. Streit, J. Dammers, S. Simsek-Kraues, J. Brinkmeyer, W. Wölwer, A. Ioannides</div><div id="ref-id-rf0545" class="title text-m">Time course of regional brain activations during facial emotion recognition in humans</div></div><div class="host u-font-sans">Neurosci. Lett., 342 (1–2) (2003), pp. 101-104, <a class="anchor anchor-primary" href="https://doi.org/10.1016/S0304-3940(03)00274-X" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/S0304-3940(03)00274-X</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S030439400300274X/pdfft?md5=8036818a9696142ffb60a8741c7295b5&amp;pid=1-s2.0-S030439400300274X-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0545"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S030439400300274X" aria-describedby="ref-id-rf0545"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0242584925&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0545"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Time%20course%20of%20regional%20brain%20activations%20during%20facial%20emotion%20recognition%20in%20humans&amp;publication_year=2003&amp;author=M.%20Streit&amp;author=J.%20Dammers&amp;author=S.%20Simsek-Kraues&amp;author=J.%20Brinkmeyer&amp;author=W.%20W%C3%B6lwer&amp;author=A.%20Ioannides" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0545"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0545" id="ref-id-bb0545" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Thom et al., 2014</span></span></a></span><span class="reference" id="rf0550"><div class="contribution"><div class="authors u-font-sans">N. Thom, J. Knight, R. Dishman, D. Sabatinelli, D.C. Johnson, B. Clementz</div><div id="ref-id-rf0550" class="title text-m">Emotional scenes elicit more pronounced self-reported emotional experience and greater EPN and LPP modulation when compared to emotional faces</div></div><div class="host u-font-sans">Cogn. Affect. Behav. Neurosci., 14 (2) (2014), pp. 849-860, <a class="anchor anchor-primary" href="https://doi.org/10.3758/s13415-013-0225-z" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.3758/s13415-013-0225-z</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84904300959&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0550"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotional%20scenes%20elicit%20more%20pronounced%20self-reported%20emotional%20experience%20and%20greater%20EPN%20and%20LPP%20modulation%20when%20compared%20to%20emotional%20faces&amp;publication_year=2014&amp;author=N.%20Thom&amp;author=J.%20Knight&amp;author=R.%20Dishman&amp;author=D.%20Sabatinelli&amp;author=D.C.%20Johnson&amp;author=B.%20Clementz" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0550"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0555" id="ref-id-bb0555" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">van Boxtel and Jessurun, 1993</span></span></a></span><span class="reference" id="rf0175"><div class="contribution"><div class="authors u-font-sans">A. van Boxtel, M. Jessurun</div><div id="ref-id-rf0175" class="title text-m">Amplitude and bilateral coherency of facial and jaw-elevator EMG activity as an index of effort during a two-choice serial reaction task</div></div><div class="host u-font-sans">Psychophysiology, 30 (6) (1993), pp. 589-604</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1111/j.1469-8986.1993.tb02085.x" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0175"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0027360362&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0175"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Amplitude%20and%20bilateral%20coherency%20of%20facial%20and%20jaw-elevator%20EMG%20activity%20as%20an%20index%20of%20effort%20during%20a%20two-choice%20serial%20reaction%20task&amp;publication_year=1993&amp;author=A.%20van%20Boxtel&amp;author=M.%20Jessurun" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0175"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0550" id="ref-id-bb0550" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Van Boxtel et al., 1996</span></span></a></span><span class="reference" id="rf0555"><div class="contribution"><div class="authors u-font-sans">A. Van Boxtel, E.J.P. Damen, C.H.M. Brunia</div><div id="ref-id-rf0555" class="title text-m">Anticipatory EMG responses of pericranial muscles in relation to heart rate during a warned simple reaction time task</div></div><div class="host u-font-sans">Psychophysiology, 33 (5) (1996), pp. 576-583, <a class="anchor anchor-primary" href="https://doi.org/10.1111/j.1469-8986.1996.tb02434.x" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1111/j.1469-8986.1996.tb02434.x</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0029810646&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0555"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Anticipatory%20EMG%20responses%20of%20pericranial%20muscles%20in%20relation%20to%20heart%20rate%20during%20a%20warned%20simple%20reaction%20time%20task&amp;publication_year=1996&amp;author=A.%20Van%20Boxtel&amp;author=E.J.P.%20Damen&amp;author=C.H.M.%20Brunia" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0555"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0560" id="ref-id-bb0560" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Vuilleumier and Pourtois, 2007</span></span></a></span><span class="reference" id="rf0560"><div class="contribution"><div class="authors u-font-sans">P. Vuilleumier, G. Pourtois</div><div id="ref-id-rf0560" class="title text-m">Distributed and interactive brain mechanisms during emotion face perception: evidence from functional neuroimaging</div></div><div class="host u-font-sans">Neuropsychologia, 45 (1) (2007), pp. 174-194, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.neuropsychologia.2006.06.003" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.neuropsychologia.2006.06.003</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0028393206002296/pdfft?md5=92363f3cd7d0f2faca0a32951945cea6&amp;pid=1-s2.0-S0028393206002296-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0560"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0028393206002296" aria-describedby="ref-id-rf0560"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-33751420802&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0560"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Distributed%20and%20interactive%20brain%20mechanisms%20during%20emotion%20face%20perception%3A%20evidence%20from%20functional%20neuroimaging&amp;publication_year=2007&amp;author=P.%20Vuilleumier&amp;author=G.%20Pourtois" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0560"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0565" id="ref-id-bb0565" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Walla and Panksepp, 2013</span></span></a></span><span class="reference" id="rf0565"><div class="contribution"><div class="authors u-font-sans">P. Walla, J. Panksepp</div><div id="ref-id-rf0565" class="title text-m">Neuroimaging helps to clarify brain affective processing without necessarily clarifying emotions</div></div><div class="host u-font-sans">K. Fountas (Ed.), Novel Frontiers of Advanced Neuroimaging, InTech (2013), pp. 93-118</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Neuroimaging%20helps%20to%20clarify%20brain%20affective%20processing%20without%20necessarily%20clarifying%20emotions&amp;publication_year=2013&amp;author=P.%20Walla&amp;author=J.%20Panksepp" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0565"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0570" id="ref-id-bb0570" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Wand, 2015</span></span></a></span><span class="reference" id="rf0570"><div class="contribution"><div class="authors u-font-sans">M. Wand</div><div id="ref-id-rf0570" class="title text-m">Advancing Electromyographic Continuous Speech Recognition: Signal Preprocessing and Modeling</div></div><div class="host u-font-sans">9783731502111, Verlag KIT Scientific Publishing, Karlsruhe (2015)</div><div class="comment">(226 pages)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Advancing%20Electromyographic%20Continuous%20Speech%20Recognition%3A%20Signal%20Preprocessing%20and%20Modeling&amp;publication_year=2015&amp;author=M.%20Wand" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0570"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0575" id="ref-id-bb0575" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Wangelin et al., 2012</span></span></a></span><span class="reference" id="rf0575"><div class="contribution"><div class="authors u-font-sans">B.C. Wangelin, M.M. Bradley, A. Kastner, P.J. Lang</div><div id="ref-id-rf0575" class="title text-m">Affective engagement for facial expressions and emotional scenes: the influence of social anxiety</div></div><div class="host u-font-sans">Biol. Psychol., 91 (1) (2012), pp. 103-110, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.biopsycho.2012.05.002" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.biopsycho.2012.05.002</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0301051112001093/pdfft?md5=8c9d32f67ba499666fda362e87fc45f0&amp;pid=1-s2.0-S0301051112001093-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf0575"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0301051112001093" aria-describedby="ref-id-rf0575"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84862313326&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0575"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Affective%20engagement%20for%20facial%20expressions%20and%20emotional%20scenes%3A%20the%20influence%20of%20social%20anxiety&amp;publication_year=2012&amp;author=B.C.%20Wangelin&amp;author=M.M.%20Bradley&amp;author=A.%20Kastner&amp;author=P.J.%20Lang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0575"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0580" id="ref-id-bb0580" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Weinberg and Hajcak, 2010</span></span></a></span><span class="reference" id="rf0580"><div class="contribution"><div class="authors u-font-sans">A. Weinberg, G. Hajcak</div><div id="ref-id-rf0580" class="title text-m">Beyond good and evil: the time-course of neural activity elicited by specific picture content</div></div><div class="host u-font-sans">Emotion, 10 (6) (2010), p. 767, <a class="anchor anchor-primary" href="https://doi.org/10.1037/a0020242" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1037/a0020242</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-78650682744&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0580"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Beyond%20good%20and%20evil%3A%20the%20time-course%20of%20neural%20activity%20elicited%20by%20specific%20picture%20content&amp;publication_year=2010&amp;author=A.%20Weinberg&amp;author=G.%20Hajcak" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0580"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0585" id="ref-id-bb0585" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Wieser and Brosch, 2012</span></span></a></span><span class="reference" id="rf0585"><div class="contribution"><div class="authors u-font-sans">M.J. Wieser, T. Brosch</div><div id="ref-id-rf0585" class="title text-m">Faces in context: a review and systematization of contextual influences on affective face processing</div></div><div class="host u-font-sans">Front. Psychol., 3 (NOV) (2012), p. 471, <a class="anchor anchor-primary" href="https://doi.org/10.3389/fpsyg.2012.00471" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.3389/fpsyg.2012.00471</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84870954762&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0585"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Faces%20in%20context%3A%20a%20review%20and%20systematization%20of%20contextual%20influences%20on%20affective%20face%20processing&amp;publication_year=2012&amp;author=M.J.%20Wieser&amp;author=T.%20Brosch" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0585"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb0590" id="ref-id-bb0590" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Winkielman et al., 2009</span></span></a></span><span class="reference" id="rf0590"><div class="contribution"><div class="authors u-font-sans">P. Winkielman, D.N. McIntosh, L. Oberman</div><div id="ref-id-rf0590" class="title text-m">Embodied and disembodied emotion processing: learning from and about typical and autistic individuals</div></div><div class="host u-font-sans">Emot. Rev., 1 (2) (2009), pp. 178-190, <a class="anchor anchor-primary" href="https://doi.org/10.1177/1754073908100442" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1177/1754073908100442</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-61849165161&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0590"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Embodied%20and%20disembodied%20emotion%20processing%3A%20learning%20from%20and%20about%20typical%20and%20autistic%20individuals&amp;publication_year=2009&amp;author=P.%20Winkielman&amp;author=D.N.%20McIntosh&amp;author=L.%20Oberman" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf0590"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbb9050" id="ref-id-bb9050" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">Wu et al., 2012</span></span></a></span><span class="reference" id="rf9050"><div class="contribution"><div class="authors u-font-sans">L. Wu, M.H. Winkler, M. Andreatta, G. Hajcak, P. Pauli</div><div id="ref-id-rf9050" class="title text-m">Appraisal frames of pleasant and unpleasant pictures alter emotional responses as reflected in self-report and facial electromyographic activity</div></div><div class="host u-font-sans">Int. J. Psychophysiol., 85 (2012), pp. 224-229</div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0167876012001377/pdfft?md5=47f4334ccbd773b93c133e75ceeaf6a6&amp;pid=1-s2.0-S0167876012001377-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-rf9050"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0167876012001377" aria-describedby="ref-id-rf9050"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84865372355&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf9050"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Appraisal%20frames%20of%20pleasant%20and%20unpleasant%20pictures%20alter%20emotional%20responses%20as%20reflected%20in%20self-report%20and%20facial%20electromyographic%20activity&amp;publication_year=2012&amp;author=L.%20Wu&amp;author=M.H.%20Winkler&amp;author=M.%20Andreatta&amp;author=G.%20Hajcak&amp;author=P.%20Pauli" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-rf9050"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li></ol></section></section></div><div id="section-cited-by"><section aria-label="Cited by" class="ListArticles preview"><div class="PageDivider"></div><header id="citing-articles-header"><h2 class="u-h4 u-margin-l-ver u-font-serif">Cited by (67)</h2></header><div aria-describedby="citing-articles-header"><div class="citing-articles u-margin-l-bottom"><ul><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-0-title"><a class="anchor anchor-primary" href="/science/article/pii/S0010945220302628"><span class="anchor-text-container"><span class="anchor-text">Attention and emotion: An integrative review of emotional face processing as a function of attention</span></span></a></h3><div>2020, Cortex</div><div class="CitedSection u-margin-s-top"><div class="u-margin-s-left"><div class="cite-header u-text-italic u-font-sans">Citation Excerpt :</div><p class="u-font-serif text-xs">Most studies observed pronounced EPN modulations for fearful and angry expressions. Though not often examined, also sad and disgusted expressions were found to enlarge EPN modulations, in contrast to happy faces for which no emotion effect was reported in most cases (Aguado et al., 2012; Calvo et al., 2013; Calvo &amp; Beltrán, 2014; Frühholz et al., 2009; Hammerschmidt et al., 2018; Itier &amp; Neath-Tavares, 2017; Jiang et al., 2014; Mavratzakis et al., 2016; Morel et al., 2014; Peschard et al., 2013; Recio et al., 2011; Rellecke et al., 2012; Smith et al., 2013; Valdés-Conroy et al., 2014; Wieser, Gerdes, et al., 2012; Wronka &amp; Walentowska, 2011). One study found effects only for patients with Parkinson disease (Wieser, Klupp, et al., 2012) and three studies including fearful and angry expressions found no emotional differentiation (Brenner et al., 2014; Keil et al., 2018; Kühnpast et al., 2012).</p></div></div></div><div class="buttons"><button class="button-link button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby="citing-articles-article-0-title" aria-controls="citing-articles-article-0" aria-expanded="false" type="button"><span class="button-link-text-container"><span class="button-link-text">Show abstract</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"><div class="u-margin-ver-m"><div class="u-margin-s-bottom" id="abspara0010">Paying attention to faces is of special interest for humans as well as for scientific research. The experimental manipulation of facial information offers an ecologically valid approach to investigate emotion, attention and social functioning. Humans are highly specialized in face perception and event-related brain potentials (ERP) provide insights into the temporal dynamics of involved neuronal mechanisms. Here, we summarize ERP research from the last decade, examining the processing of emotional compared to neutral facial expressions along the visual processing stream. A particular focus lies on exploring the impact of attention tasks on early (P1, N170), mid-latency (P2, EPN) and late (P3, LPP) stages of processing. This review systematizes facial emotion effects as a function of different attention tasks: 1) When faces serve as mere distractors, 2) during passive viewing designs, 3) directing attention at faces in general, and 4) paying attention to facial expressions. We find fearful and angry expressions to reliably modulate the N170, EPN, and LPP component, the latter benefiting from attention directed at the emotional facial expression.</div></div></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-1-title"><a class="anchor anchor-primary" href="/science/article/pii/S0301051117301187"><span class="anchor-text-container"><span class="anchor-text">Neural signals of selective attention are modulated by subjective preferences and buying decisions in a virtual shopping task</span></span></a></h3><div>2017, Biological Psychology</div><div class="CitedSection u-margin-s-top"><div class="u-margin-s-left"><div class="cite-header u-text-italic u-font-sans">Citation Excerpt :</div><p class="u-font-serif text-xs">As some of us explained in previous articles (Walker et al., 2011; Watts et al., 2014), ERPs to stimuli of higher motivational relevance (e.g. emotional stimuli, faces, affective stimuli) can be divided into three subtypes. First, pre–400 ms ERPs to motivationally relevant stimuli are thought to reflect a rapid and automatic orientation of attention triggered by evolutionary and/or motivationally relevant properties of external stimuli (Mavratzakis, Herbert, &amp; Walla, 2016; Olofsson et al., 2008; Schupp, Flaisch et al., 2006). Second, an ERP called “Late positive potential” has often been observed after 400 ms following the onset of a motivationally relevant stimulus.</p></div></div></div><div class="buttons"><button class="button-link button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby="citing-articles-article-1-title" aria-controls="citing-articles-article-1" aria-expanded="false" type="button"><span class="button-link-text-container"><span class="button-link-text">Show abstract</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"><div class="u-margin-ver-m"><div class="u-margin-s-bottom" id="spar0065">We investigated whether well-known neural markers of selective attention to motivationally-relevant stimuli were modulated by variations in subjective preference towards consumer goods in a virtual shopping task. Specifically, participants viewed and rated pictures of various goods on the extent to which they wanted each item, which they could potentially purchase afterwards. Using the event-related potentials (ERP) method, we found that variations in subjective preferences for consumer goods strongly modulated positive slow waves (PSW) from 800 to 3000 milliseconds after stimulus onset. We also found that subjective preferences modulated the N200 and the late positive potential (LPP). In addition, we found that both PSW and LPP were modulated by subsequent buying decisions. Overall, these findings show that well-known brain event-related potentials reflecting selective attention processes can reliably index preferences to consumer goods in a shopping environment. Based on a large body of previous research, we suggest that early ERPs (e.g. the N200) to consumer goods could be indicative of preferences driven by unconditional and automatic processes, whereas later ERPs such as the LPP and the PSW could reflect preferences built upon more elaborative and conscious cognitive processes.</div></div></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-2-title"><a class="anchor anchor-primary" href="https://doi.org/10.1007/s11031-019-09780-y" target="_blank"><span class="anchor-text-container"><span class="anchor-text">EmoMadrid: An emotional pictures database for affect research</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></h3><div>2019, Motivation and Emotion</div></div><div class="buttons"></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-3-title"><a class="anchor anchor-primary" href="https://doi.org/10.1073/pnas.1812250116" target="_blank"><span class="anchor-text-container"><span class="anchor-text">Tracking the affective state of unseen persons</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></h3><div>2019, Proceedings of the National Academy of Sciences of the United States of America</div></div><div class="buttons"></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-4-title"><a class="anchor anchor-primary" href="https://doi.org/10.3389/fpsyg.2017.01432" target="_blank"><span class="anchor-text-container"><span class="anchor-text">Basic emotions in human neuroscience: Neuroimaging and beyond</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></h3><div>2017, Frontiers in Psychology</div></div><div class="buttons"></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-5-title"><a class="anchor anchor-primary" href="https://doi.org/10.1080/23311908.2016.1157281" target="_blank"><span class="anchor-text-container"><span class="anchor-text">Carpe diem instead of losing your social mind: Beyond digital addiction and why we all suffer from digital overuse</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></h3><div>2016, Cogent Psychology</div></div><div class="buttons"></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"></div></div></li></ul><a class="button-alternative button-alternative-secondary large-alternative button-alternative-icon-left" href="http://www.scopus.com/scopus/inward/citedby.url?partnerID=10&amp;rel=3.0.0&amp;eid=2-s2.0-84945219325&amp;md5=88b4bf79848c655d2412d9d9ac9bec4a" target="_blank" id="citing-articles-view-all-btn"><svg focusable="false" viewBox="0 0 54 128" height="20" class="icon icon-navigate-right"><path d="M1 99l38-38L1 23l7-7 45 45-45 45z"></path></svg><span class="button-alternative-text-container"><span class="button-alternative-text">View all citing articles on Scopus</span></span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></a></div></div></section></div><div class="Footnotes text-xs"><dl class="footnote"><dt class="footnote-label"><a class="anchor u-padding-s-hor u-padding-xs u-display-inline-block anchor-primary" href="#bfn0005"><span class="anchor-text-container"><span class="anchor-text"><sup>1</sup></span></span></a></dt><dd class="footnote-detail u-padding-xs-top"><div class="u-margin-s-bottom" id="np0005">Given the critical nature of these findings, we reran the analysis using the 23 original participant data sets plus those for an additional three participants whose data were not included due to EMG recording issues. The analysis resulted in exactly the same interaction between Stimulus and Emotion even when applying a degrees of freedom correction (<em>p</em>&nbsp;=&nbsp;.005).</div></dd></dl><dl class="footnote"><dt class="footnote-label"><a class="anchor u-padding-s-hor u-padding-xs u-display-inline-block anchor-primary" href="#bfn0010"><span class="anchor-text-container"><span class="anchor-text"><sup>2</sup></span></span></a></dt><dd class="footnote-detail u-padding-xs-top"><div class="u-margin-s-bottom" id="np0010">We performed the ANOVA again using the 23 data sets plus the data for one participant who was excluded from the analysis due to having too few EEG trials. Results showed that the main effect of Stimulus appeared to strengthen (<em>F</em> (1, 23)&nbsp;=&nbsp;6.62, <em>p</em>&nbsp;=&nbsp;.017, <em>η2</em>&nbsp;=&nbsp;.22).</div></dd></dl><dl class="footnote"><dt class="footnote-label"><a class="anchor u-padding-s-hor u-padding-xs u-display-inline-block anchor-primary" href="#bfn0015"><span class="anchor-text-container"><span class="anchor-text"><sup>3</sup></span></span></a></dt><dd class="footnote-detail u-padding-xs-top"><div class="u-margin-s-bottom" id="np0015">Statistical values for the main effect of Emotion at time intervals 1000, 1250, and 1500&nbsp;ms are: At 1000&nbsp;ms, <em>F</em> (1.70, 37.41)&nbsp;=&nbsp;7.08, <em>p</em>&nbsp;=&nbsp;.004, <em>η</em><sup>2</sup>&nbsp;=&nbsp;.24, with sphericity corrections <em>χ<sup>2</sup></em>&nbsp;=&nbsp;6.14, <em>p</em>&nbsp;=&nbsp;.046, є&nbsp;=&nbsp;.85; at 1250&nbsp;ms, <em>F</em> (2, 44)&nbsp;=&nbsp;5.82, <em>p</em>&nbsp;=&nbsp;.006, <em>η</em><sup>2</sup>&nbsp;=&nbsp;.21; at 1500&nbsp;ms, <em>F</em> (2, 44)&nbsp;=&nbsp;4.30, <em>p</em>&nbsp;=&nbsp;.020, <em>η</em><sup>2</sup>&nbsp;=&nbsp;.16.</div></dd></dl><dl class="footnote"><dt class="footnote-label"><a class="anchor u-padding-s-hor u-padding-xs u-display-inline-block anchor-primary" href="#bfn0020"><span class="anchor-text-container"><span class="anchor-text"><sup>4</sup></span></span></a></dt><dd class="footnote-detail u-padding-xs-top"><div class="u-margin-s-bottom" id="np0020">Interestingly, neutral stimuli also tended to evoke enhanced corrugator activity, particularly neutral facial expressions. That both fearful and neutral stimuli evoked enhanced activity relative to baseline (remembering that corrugator muscles are typically tensed at baseline), suggests that neutral stimuli were perceived as being more negative rather than fearful stimuli being perceived as more neutral. This is in line with recent findings that neutral scenes evoke more negative rather than positive-related patterns of facial activity (<a class="anchor anchor-primary" href="#bb9050" name="bbb9050" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb9050"><span class="anchor-text-container"><span class="anchor-text">Wu et al., 2012</span></span></a>), and that neutral faces tend to be perceived in a negative context rather than simply representing an emotionally void canvas (<a class="anchor anchor-primary" href="#bb9000" name="bbb9000" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb9000"><span class="anchor-text-container"><span class="anchor-text">Adams et al., 2012</span></span></a>, <a class="anchor anchor-primary" href="#bb9020" name="bbb9020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb9020"><span class="anchor-text-container"><span class="anchor-text">Blasi et al., 2009</span></span></a>, <a class="anchor anchor-primary" href="#bb9030" name="bbb9030" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb9030"><span class="anchor-text-container"><span class="anchor-text">Hess et al., 2009</span></span></a>, <a class="anchor anchor-primary" href="#bb9040" name="bbb9040" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb9040"><span class="anchor-text-container"><span class="anchor-text">Lange et al., 2012</span></span></a>).</div></dd></dl><dl class="footnote"><dt class="footnote-label"><a class="anchor u-padding-s-hor u-padding-xs u-display-inline-block anchor-primary" href="#bfn0025"><span class="anchor-text-container"><span class="anchor-text"><sup>5</sup></span></span></a></dt><dd class="footnote-detail u-padding-xs-top"><div class="u-margin-s-bottom" id="np0025">We suspect that differences in skin conductance activity between faces and scenes in the current experiment were a residual product of factors relating to the S2 stimuli of each trial preceding the S1 stimulus presentations, particularly when considering that the emotion recognition tasks were probably easier for face-matching trials, and that increased task difficulty reduces the skin conductance response (<a class="anchor anchor-primary" href="#bb0425" name="bbb0425" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bb0425"><span class="anchor-text-container"><span class="anchor-text">Pecchinenda, 1996</span></span></a>). However, this remains speculative given that skin conductance responses would have also been affected by other experimental factors such as unforced response errors, for which the current analysis was not designed to cover.</div></dd></dl></div><div class="Copyright"><span class="copyright-line">Copyright © 2015 The Authors. Published by Elsevier Inc.</span></div></article><div class="u-display-block-from-md col-lg-6 col-md-8 pad-right u-padding-s-top"><aside class="RelatedContent u-clr-grey8" aria-label="Related content"><section aria-label="Substances" class="RelatedContentPanel u-margin-s-bottom substances-related-content"><header id="substances-header" class="related-content-panel-header u-margin-s-bottom"><button class="button-link button-link-secondary related-content-panel-toggle is-up button-link-icon-right button-link-has-colored-icon" aria-expanded="true" type="button"><span class="button-link-text-container"><span class="button-link-text"><h2 class="section-title u-h4"><span class="related-content-panel-title-text">Substances (1)</span></h2></span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class="" aria-hidden="false" aria-describedby="substances-header"><div class="substances"><div class="u-margin-xs-ver">Generated by <a class="anchor reaxys-link anchor-secondary u-display-inline anchor-icon-left anchor-with-icon" href="https://www.reaxys.com?pendo=-Bp7H6KseLNepmnxYNE0hJGAeaI" target="_blank"><svg class="u-valign-middle" height="24" aria-label="Reaxys" viewBox="0 0 98 33" width="58"><g fill="#f36d21"><path d="M10.8 14.2c2.5-0.4 4.6-2.6 4.6-5.5 0-4-2.9-5.7-7-5.7H3v20h2.6v-8.4h2.2c0.6 1 1.7 2.4 2.5 3.6 0.8 1.1 3.2 4.3 3.7 4.8h3.3c-0.9-0.8-3.1-3.8-4.2-5.4L10.8 14.2zM8 12.5H5.6V5.2H8c2.8 0 4.5 1 4.5 3.4C12.5 11.1 11.3 12.5 8 12.5z"></path><path d="M31.6 15.2c0-5.2-1.4-7.6-6-7.6 -3.9 0-6.7 2.8-6.7 7.9 0 4.9 3 7.8 6.7 7.8 2.8 0 4.4-0.6 5.7-1.4l-0.2-2.3c-0.9 0.8-2.9 1.6-4.7 1.6 -3.1 0-4.9-1.9-4.9-5.5v-0.5H31.6zM23.5 10.3c0.2-0.2 1.3-0.5 2-0.5 2.5 0 3.8 0.5 3.9 3.4h-7.5C22.1 11.4 23.3 10.5 23.5 10.3z"></path><path d="M47.1 10.2l0.2-2.1h-2l-0.6 1.2c-0.9-1.2-2.4-1.6-3.8-1.6 -4.7 0-7.2 3-7.2 8.2 0 4.9 2.8 7.6 6.4 7.6 1.9 0 3.4-0.6 4.6-2.5l0.1 0.6c0.1 0.4 0.2 0.9 0.4 1.4h2.5c-0.4-1.1-0.7-3.3-0.7-4.5V10.2zM44.6 17.8c-1 2.3-2.2 3.5-3.9 3.5 -2.1 0-4.1-1.5-4.1-5.7 0-3.3 1.4-4.7 1.8-5.1 0.2-0.2 1.5-0.6 2.7-0.6 2.2 0 3.5 1.5 3.5 4.8V17.8z"></path><polygon points="62.7 8.1 60.1 8.1 56.5 13.3 53.2 8.1 50.2 8.1 55 15.4 49.8 23 52.4 23 56.3 17.3 60 23 63 23 57.8 15.3 "></polygon><path d="M70.8 19.6L66.4 8.1h-2.7l5.8 15.1 -0.8 1.9c-0.9 2.1-2 4.6-2 4.6h2.7c0 0 1.1-2.7 2.2-5.9l5.5-15.6h-2.5L70.8 19.6z"></path><path d="M81.7 11.6c0-1.2 1.2-1.8 2.8-1.8 1.1 0 2.7 0.3 4 1.5l0.3-2.4c-1-0.6-2.3-1.1-4.3-1.1 -3.6 0-5.3 1.8-5.3 4.1 0 4.6 7.5 4.5 7.5 7.5 0 1.2-1 2.1-3.1 2.1 -1.9 0-3.1-0.8-4.2-1.5l-0.3 2.4c1.2 0.8 3 1.2 4.7 1.2 3.8 0 5.4-2.2 5.4-4.4C89.2 14.3 81.7 14.6 81.7 11.6z"></path><path d="M93 6.9c-1.1 0-1.9-0.8-1.9-1.9 0-1.1 0.9-1.9 1.9-1.9C94.1 3.1 95 3.9 95 5 95 6 94.1 6.9 93 6.9zM93 3.5c-0.8 0-1.5 0.7-1.5 1.5 0 0.8 0.7 1.5 1.5 1.5 0.8 0 1.5-0.7 1.5-1.5C94.5 4.1 93.9 3.5 93 3.5z"></path><path d="M93.5 6c-0.1-0.1-0.2-0.2-0.2-0.3l-0.4-0.6h-0.2V6h-0.4V3.9c0.2 0 0.5 0 0.6 0 0.4 0 0.8 0.2 0.8 0.6 0 0.3-0.2 0.5-0.5 0.6C93.5 5.4 93.7 5.7 94 6H93.5zM92.9 4.2l-0.2 0v0.6h0.2c0.3 0 0.4 0 0.4-0.3C93.4 4.3 93.2 4.2 92.9 4.2z"></path></g></svg><span class="anchor-text-container"><span class="anchor-text">​</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>, an expert-curated chemistry database.</div><div><ol id="substance-image-grid" class="u-margin-s-bottom"><li><button class="button-link button-link-secondary substance-image-button button-link-icon-only" aria-haspopup="dialog" data-sd-ui-side-panel-opener="true" type="button"><div class="substance-wrapper"><img alt="chemical structure for EPN" class="substance-image u-display-block" src="/sdfe/arp/pii/S1053811915008873/substance/2542580/image" width="200" height="150" title="chemical structure for EPN"></div></button></li></ol></div></div></div></section><section class="RelatedContentPanel u-margin-s-bottom"><header id="recommended-articles-header" class="related-content-panel-header u-margin-s-bottom"><button class="button-link button-link-secondary related-content-panel-toggle is-up button-link-icon-right button-link-has-colored-icon" aria-expanded="true" data-aa-button="sd:product:journal:article:location=recommended-articles:type=close" type="button"><span class="button-link-text-container"><span class="button-link-text"><h2 class="section-title u-h4"><span class="related-content-panel-title-text">Recommended articles</span></h2></span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class="" aria-hidden="false" aria-describedby="recommended-articles-header"><div id="recommended-articles" class="text-xs"><ul><li class="RelatedContentPanelItem u-display-block"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article0-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S1053811915008836" title="Modality-independent reduction mechanisms of primary sensory evoked fields in a one-back task"><span class="anchor-text-container"><span class="anchor-text"><span>Modality-independent reduction mechanisms of primary sensory evoked fields in a one-back task</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">NeuroImage, Volume 124, Part A, 2016, pp. 918-922</div></div><div class="authors"><span>David</span> <span>Hanke</span>, …, <span>Theresa</span> <span>Götz</span></div></div><div class="buttons"></div></li><li class="RelatedContentPanelItem u-display-block"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article1-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S1053811915008137" title="Genes influence the amplitude and timing of brain hemodynamic responses"><span class="anchor-text-container"><span class="anchor-text"><span>Genes influence the amplitude and timing of brain hemodynamic responses</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">NeuroImage, Volume 124, Part A, 2016, pp. 663-671</div></div><div class="authors"><span>Zuyao Y.</span> <span>Shan</span>, …, <span>David C.</span> <span>Reutens</span></div></div><div class="buttons"></div></li><li class="RelatedContentPanelItem u-display-block"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article2-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S1053811915008587" title="Reaching with the sixth sense: Vestibular contributions to voluntary motor control in the human right parietal cortex"><span class="anchor-text-container"><span class="anchor-text"><span>Reaching with the sixth sense: Vestibular contributions to voluntary motor control in the human right parietal cortex</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">NeuroImage, Volume 124, Part A, 2016, pp. 869-875</div></div><div class="authors"><span>Alexandra</span> <span>Reichenbach</span>, …, <span>Axel</span> <span>Thielscher</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1053811915008587/pdfft?md5=a147dd2ea451cacb98901fb503da7e8e&amp;pid=1-s2.0-S1053811915008587-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article2-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li><li class="RelatedContentPanelItem u-display-none"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article3-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0028393215302414" title="On dissociating the neural time course of the processing of positive emotions"><span class="anchor-text-container"><span class="anchor-text"><span>On dissociating the neural time course of the processing of positive emotions</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Neuropsychologia, Volume 83, 2016, pp. 123-137</div></div><div class="authors"><span>Elizabeth B.</span> <span>daSilva</span>, …, <span>Aina</span> <span>Puce</span></div></div><div class="buttons"></div></li><li class="RelatedContentPanelItem u-display-none"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article4-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0301051121001083" title="Embodied power: Specific facial muscles automatically respond to power-related information processing"><span class="anchor-text-container"><span class="anchor-text"><span>Embodied power: Specific facial muscles automatically respond to power-related information processing</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Biological Psychology, Volume 163, 2021, Article 108115</div></div><div class="authors"><span>Roland</span> <span>Neumann</span>, <span>Julia</span> <span>Kozlik</span></div></div><div class="buttons"></div></li><li class="RelatedContentPanelItem u-display-none"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article5-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0167865517303902" title="Deep spatial-temporal feature fusion for facial expression recognition in static images"><span class="anchor-text-container"><span class="anchor-text"><span>Deep spatial-temporal feature fusion for facial expression recognition in static images</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Pattern Recognition Letters, Volume 119, 2019, pp. 49-61</div></div><div class="authors"><span>Ning</span> <span>Sun</span>, …, <span>Guang</span> <span>Han</span></div></div><div class="buttons"></div></li></ul></div><button class="button-link more-recommendations-button u-margin-s-bottom button-link-primary button-link-icon-right" type="button"><span class="button-link-text-container"><span class="button-link-text">Show 3 more articles</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div></section><section class="RelatedContentPanel u-margin-s-bottom"><header id="metrics-header" class="related-content-panel-header u-margin-s-bottom"><button class="button-link button-link-secondary related-content-panel-toggle is-up button-link-icon-right button-link-has-colored-icon" aria-expanded="true" type="button"><span class="button-link-text-container"><span class="button-link-text"><h2 class="section-title u-h4"><span class="related-content-panel-title-text">Article Metrics</span></h2></span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class="" aria-hidden="false" aria-describedby="metrics-header"><div class="plumX-metrics"><h3 class="text-s metric-title metric-title-citations">Citations</h3><ul><li class="text-xs metrics"><span>Citation Indexes</span><span>67</span></li></ul><h3 class="text-s metric-title metric-title-captures">Captures</h3><ul><li class="text-xs metrics"><span>Readers</span><span>212</span></li></ul><h3 class="text-s metric-title metric-title-mentions">Mentions</h3><ul><li class="text-xs metrics"><span>References</span><span>1</span></li></ul><h3 class="text-s metric-title metric-title-social-media">Social Media</h3><ul><li class="text-xs metrics"><span>Shares, Likes &amp; Comments</span><span>20</span></li></ul><div class="metrics u-margin-m-top u-margin-s-bottom"><img src="https://cdn.plu.mx/3ba727faf225e19d2c759f6ebffc511d/plumx-logo.png" class="plumX-logo" alt="PlumX Metrics Logo"><a class="anchor text-xs anchor-primary" href="https://plu.mx/plum/a/?doi=10.1016%2Fj.neuroimage.2015.09.065&amp;theme=plum-sciencedirect-theme&amp;hideUsage=true" target="_blank"><span class="anchor-text-container"><span class="anchor-text">View details</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></div></div></section></aside></div></div></div></div><footer role="contentinfo" class="els-footer u-bg-white text-xs u-padding-s-hor u-padding-m-hor-from-sm u-padding-l-hor-from-md u-padding-l-ver u-margin-l-top u-margin-xl-top-from-sm u-margin-l-top-from-md"><div class="els-footer-elsevier u-margin-m-bottom u-margin-0-bottom-from-md u-margin-s-right u-margin-m-right-from-md u-margin-l-right-from-lg"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="https://www.elsevier.com/" target="_blank" aria-label="Elsevier home page (opens in a new tab)" rel="nofollow"><img class="footer-logo" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/47/images/elsevier-non-solus-new-with-wordmark.svg" alt="Elsevier logo with wordmark" height="64" width="58" loading="lazy"></a></div><div class="els-footer-content"><div class="u-remove-if-print"><ul class="els-footer-links u-margin-xs-bottom" style="list-style:none"><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsevier.com/solutions/sciencedirect" target="_blank" id="els-footer-about-science-direct" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">About ScienceDirect</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="/user/institution/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS1053811915008873" id="els-footer-remote-access" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Remote access</span></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://sd-cart.elsevier.com/?" target="_blank" id="els-footer-shopping-cart" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Shopping cart</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsmediakits.com" target="_blank" id="els-footer-advertise" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Advertise</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://service.elsevier.com/app/contact/supporthub/sciencedirect/" target="_blank" id="els-footer-contact-support" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Contact and support</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsevier.com/legal/elsevier-website-terms-and-conditions" target="_blank" id="els-footer-terms-condition" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Terms and conditions</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsevier.com/legal/privacy-policy" target="_blank" id="els-footer-privacy-policy" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Privacy policy</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li></ul></div><p id="els-footer-cookie-message" class="u-remove-if-print">Cookies are used by this site. <!-- --> <button class="button-link ot-sdk-show-settings cookie-btn button-link-primary button-link-small" id="ot-sdk-btn" type="button">Cookie Settings</button></p><p id="els-footer-copyright">All content on this site: Copyright © <!-- -->2024<!-- --> Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the Creative Commons licensing terms apply.</p></div><div class="els-footer-relx u-margin-0-top u-margin-m-top-from-xs u-margin-0-top-from-md"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="https://www.relx.com/" target="_blank" aria-label="RELX home page (opens in a new tab)" id="els-footer-relx" rel="nofollow"><img loading="lazy" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/60/images/logo-relx-tm.svg" width="93" height="20" alt="RELX group home page"></a></div></footer></div></div></div></div>
      <div id="floating-ui-node" class="floating-ui-node" data-sd-ui-floating-ui="true"></div>
      
      <script src="https://assets.adobedtm.com/4a848ae9611a/032db4f73473/launch-a6263b31083f.min.js" type="image/ot-performance" async=""></script>
      
<script type="text/javascript">
    window.pageData = {"content":[{"contentType":"JL","format":"MIME-XHTML","id":"sd:article:pii:S1053811915008873","type":"sd:article:JL:scope-full","detail":"sd:article:subtype:fla","publicationType":"journal","issn":"1053-8119","volumeNumber":"124","suppl":"PA","provider":"elsevier","entitlementType":"openaccess"}],"page":{"businessUnit":"ELS:RP:ST","language":"en","name":"product:journal:article","noTracking":"false","productAppVersion":"full-direct","productName":"SD","type":"CP-CA","environment":"prod","loadTimestamp":1734891649038,"loadTime":""},"visitor":{"accessType":"ae:ANON_GUEST","accountId":"ae:228598","accountName":"ae:ScienceDirect Guests","loginStatus":"anonymous","userId":"ae:12975512","ipAddress":"77.165.246.201","appSessionId":"7e7f2e46-8645-4693-866b-b7ad2764d028"}};
    window.pageData.page.loadTime = performance ? Math.round(performance.now()).toString() : '';

    try {
      appData.push({
      event: 'pageLoad',
      page: pageData.page,
      visitor: pageData.visitor,
      content: pageData.content
      })
    } catch(e) {
        console.warn("There was an error loading or running Adobe DTM: ", e);
    }
</script>
      <script nomodule="" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/73/js/core-js/3.20.2/core-js.es.minified.js" type="text/javascript"></script>
      <script src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/108/js/react/18.3.1/react.production.min.js" type="text/javascript"></script>
      <script src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/108/js/react-dom/18.3.1/react-dom.production.min.js" type="text/javascript"></script>
      <script async="" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/b64013ec63c69e3d916174cbebae89d65b2419e1/arp.js" type="text/javascript"></script>
      <script type="text/javascript">
    const pendoData = {"visitor":{"pageName":"SD:product:journal:article","pageType":"CP-CA","pageProduct":"SD","pageLanguage":"en","pageEnvironment":"prod","accessType":"ae:ANON_GUEST","countryCode":"NL"},"account":{"id":"ae:228598","name":"ae:ScienceDirect Guests"},"events":{}};;
    pendoData.events = {
      ready: function () {
        pendo.addAltText();
      },
    };
    function runPendo(data, options) {
  const {
    firstDelay,
    maxRetries,
    urlPrefix,
    urlSuffix,
    apiKey
  } = options;
  (function (apiKey) {
    (function (p, e, n, d, o) {
      var v, w, x, y, z;
      o = p[d] = p[d] || {};
      o._q = [];
      v = ['initialize', 'identify', 'updateOptions', 'pageLoad'];
      for (w = 0, x = v.length; w < x; ++w) (function (m) {
        o[m] = o[m] || function () {
          o._q[m === v[0] ? 'unshift' : 'push']([m].concat([].slice.call(arguments, 0)));
        };
      })(v[w]);
      y = e.createElement(n);
      y.async = !0;
      y.src = urlPrefix + apiKey + urlSuffix;
      z = e.getElementsByTagName(n)[0];
      z.parentNode.insertBefore(y, z);
    })(window, document, 'script', 'pendo');
    pendo.addAltText = function () {
      var target = document.querySelector('body');
      var observer = new MutationObserver(function (mutations) {
        mutations.forEach(function (mutation) {
          if (mutation?.addedNodes?.length) {
            if (mutation.addedNodes[0]?.className?.includes("_pendo-badge")) {
              const badge = mutation.addedNodes[0];
              const altText = badge?.attributes['aria-label'].value ? badge?.attributes['aria-label'].value : 'Feedback';
              const pendoBadgeImage = pendo.dom(`#${badge?.attributes?.id.value} img`);
              if (pendoBadgeImage.length) {
                pendoBadgeImage[0]?.setAttribute('alt', altText);
              }
            }
          }
        });
      });
      var config = {
        attributeFilter: ['data-layout'],
        attributes: true,
        childList: true,
        characterData: true,
        subtree: false
      };
      observer.observe(target, config);
    };
  })(apiKey);
  (function watchAndSetPendo(nextDelay, retryAttempt) {
    if (typeof pageDataTracker === 'object' && typeof pageDataTracker.getVisitorId === 'function' && pageDataTracker.getVisitorId()) {
      data.visitor.id = pageDataTracker.getVisitorId();
      console.debug(`initializing pendo`);
      pendo.initialize(data);
    } else {
      if (retryAttempt > 0) {
        return setTimeout(function () {
          watchAndSetPendo(nextDelay * 2, retryAttempt - 1);
        }, nextDelay);
      }
      pendo.initialize(data);
      console.debug(`gave up ... pendo initialized`);
    }
  })(firstDelay, maxRetries);
}
    runPendo(pendoData, {
      firstDelay: 100,
      maxRetries: 5,
      urlPrefix: 'https://cdn.pendo.io/agent/static/',
      urlSuffix: '/pendo.js',
      apiKey: 'd6c1d995-bc7e-4e53-77f1-2ea4ecbb9565',
    });
  </script>
      <span id="pendo-answer-rating"></span>
      <script type="text/x-mathjax-config;executed=true">
        MathJax.Hub.Config({
          displayAlign: 'left',
          "fast-preview": {
            disabled: true
          },
          CommonHTML: { linebreaks: { automatic: true } },
          PreviewHTML: { linebreaks: { automatic: true } },
          'HTML-CSS': { linebreaks: { automatic: true } },
          SVG: {
            scale: 90,
            linebreaks: { automatic: true }
          }
        });
      </script>
      <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=MML_SVG" type="text/javascript"></script>
      <script async="" src="https://www.googletagservices.com/tag/js/gpt.js" type="text/javascript"></script>
      <script async="" src="https://scholar.google.com/scholar_js/casa.js" type="text/javascript"></script>
      <script data-cfasync="false">
      (function initOneTrust()  {
        const monitor = {
  init: () => {},
  loaded: () => {},
};
        function enableGroup(group) {
  document.querySelectorAll(`script[type*="ot-${group}"]`).forEach(script => {
    script.type = 'text/javascript';
    document.head.appendChild(script);
  });
}
        function runOneTrustCookies(doClear, monitor) {
  const oneTrustConsentSdkId = 'onetrust-consent-sdk';
  const emptyNodeSelectors = 'h3.ot-host-name, h4.ot-host-desc, button.ot-host-box';
  const ariaLabelledByButtonNodes = 'div.ot-accordion-layout > button';
  const ariaAttribute = 'aria-labelledby';
  function adjustOneTrustDOM() {
    const oneTrustRoot = document.getElementById('onetrust-consent-sdk');

    /* remove empty nodes */
    [...(oneTrustRoot?.querySelectorAll(emptyNodeSelectors) ?? [])].filter(e => e.textContent === '').forEach(e => e.remove());

    /* remove invalid aria-labelledby values */
    oneTrustRoot?.querySelectorAll(ariaLabelledByButtonNodes).forEach(e => {
      const presentIdValue = e.getAttribute(ariaAttribute)?.split(' ').filter(label => document.getElementById(label)).join(' ');
      if (presentIdValue) {
        e.setAttribute(ariaAttribute, presentIdValue);
      }
    });
  }
  function observeOneTrustLoaded(shouldSetOTDefaults, isConsentPresent) {
    const cb = (mutationList, observer) => {
      const oneTrustRoot = mutationList.filter(mutationRecord => mutationRecord.type === 'childList' && mutationRecord.addedNodes.length).map(mutationRecord => [...mutationRecord.addedNodes]).flat().find(e => e.id === oneTrustConsentSdkId);
      if (oneTrustRoot && typeof OneTrust !== 'undefined') {
        monitor.loaded(true);
        OneTrust.OnConsentChanged(() => {
          const perfAllowed = decodeURIComponent(document.cookie.match('(^| )OptanonConsent=([^;]+)')?.[2])?.match('groups=([0-9:0|1,?]+)&?')?.[1]?.match('2:([0|1])')[1] === '1';
          if (perfAllowed) {
            enableGroup('performance');
          }
        });
        if (!isConsentPresent && (shouldSetOTDefaults || OneTrust.GetDomainData().ConsentModel.Name === 'implied consent')) {
          OneTrust.AllowAll();
        }
        document.dispatchEvent(new CustomEvent('@sdtech/onetrust/loaded', {}));
        observer.disconnect();
        adjustOneTrustDOM();
      }
    };
    const observer = new MutationObserver(cb);
    observer.observe(document.querySelector('body'), {
      childList: true
    });
  }
  if (doClear) {
    document.cookie = 'OptanonAlertBoxClosed=; expires=Thu, 01 Jan 1970 00:00:00 UTC; samesite=lax; path=/';
  }
  const isConsentPresent = !!decodeURIComponent(document.cookie.match('(^| )OptanonConsent=([^;]+)')?.[2])?.match('groups=([0-9:0|1,?]+)&?')?.[1];
  const shouldSetOTDefaults = 'true' === 'false' && !document.cookie?.match('OptanonAlertBoxClosed=');
  if (shouldSetOTDefaults) {
    const date = new Date();
    date.setFullYear(date.getFullYear() + 1);
    document.cookie = `OptanonAlertBoxClosed=${new Date().toISOString()}; expires=${date.toUTCString()}; samesite=lax; path=/; domain=sciencedirect.com`;
  }
  observeOneTrustLoaded(shouldSetOTDefaults, isConsentPresent, monitor);
  window.addOTScript = () => {
    const otSDK = document.createElement('script');
    otSDK.setAttribute('data-cfasync', 'false');
    otSDK.setAttribute('src', 'https://cdn.cookielaw.org/scripttemplates/otSDKStub.js');
    otSDK.setAttribute('data-document-language', 'true');
    otSDK.setAttribute('data-domain-script', '865ea198-88cc-4e41-8952-1df75d554d02');
    window.addOTScript = () => {};
    document.head.appendChild(otSDK);
    monitor.init();
  };
  window.addEventListener('load', () => window.addOTScript());
}
        if (document.location.host.match(/.sciencedirect.com$/)) {
          runOneTrustCookies(true, monitor);
        }
        else {
          window.addEventListener('load', (event) => {
            enableGroup('performance');
          });
        }
      }());
    </script>
    
  <iframe id="ps_zlprmper" src="https://service.seamlessaccess.org/ps/" style="display: none; position: absolute; top: -999px; left: -999px;"></iframe><div id="onetrust-consent-sdk"><div class="onetrust-pc-dark-filter ot-fade-in" style="z-index:2147483645;"></div><div id="onetrust-banner-sdk" class="otFlat bottom ot-wo-title ot-buttons-fw" tabindex="0" role="region" aria-label="Cookie banner"><div role="alertdialog" aria-describedby="onetrust-policy-text" aria-modal="true" aria-label="Privacy"><div class="ot-sdk-container"><div class="ot-sdk-row"><div id="onetrust-group-container" class="ot-sdk-eight ot-sdk-columns"><div class="banner_logo"></div><div id="onetrust-policy"><div id="onetrust-policy-text">We use cookies that are necessary to make our site work. We may also use additional cookies to analyze, improve, and personalize our content and your digital experience. For more information, see our<a class="ot-cookie-policy-link" href="https://www.elsevier.com/legal/cookienotice" target="_blank" aria-label=", opens in a new tab" rel="noopener">Cookie Policy</a></div></div></div><div id="onetrust-button-group-parent" class="ot-sdk-three ot-sdk-columns"><div id="onetrust-button-group"><button id="onetrust-pc-btn-handler" class="cookie-setting-link">Cookie Settings</button>  <button id="onetrust-accept-btn-handler">Accept all cookies</button></div></div></div></div><!-- Close Button --><div id="onetrust-close-btn-container"></div><!-- Close Button END--></div></div><div id="onetrust-pc-sdk" class="otPcCenter ot-hide ot-fade-in" lang="en" aria-label="Preference center" role="region"><div role="alertdialog" aria-modal="true" aria-describedby="ot-pc-desc" style="height: 100%;" aria-label="Cookie Preference Center"><!-- Close Button --><div class="ot-pc-header"><!-- Logo Tag --><div class="ot-pc-logo" role="img" aria-label="Company Logo"><img alt="Company Logo" src="https://cdn.cookielaw.org/logos/static/ot_company_logo.png"></div></div><!-- Close Button --><div id="ot-pc-content" class="ot-pc-scrollbar"><div class="ot-optout-signal ot-hide"><div class="ot-optout-icon"><svg xmlns="http://www.w3.org/2000/svg"><path class="ot-floating-button__svg-fill" d="M14.588 0l.445.328c1.807 1.303 3.961 2.533 6.461 3.688 2.015.93 4.576 1.746 7.682 2.446 0 14.178-4.73 24.133-14.19 29.864l-.398.236C4.863 30.87 0 20.837 0 6.462c3.107-.7 5.668-1.516 7.682-2.446 2.709-1.251 5.01-2.59 6.906-4.016zm5.87 13.88a.75.75 0 00-.974.159l-5.475 6.625-3.005-2.997-.077-.067a.75.75 0 00-.983 1.13l4.172 4.16 6.525-7.895.06-.083a.75.75 0 00-.16-.973z" fill="#FFF" fill-rule="evenodd"></path></svg></div><span></span></div><h2 id="ot-pc-title">Cookie Preference Center</h2><div id="ot-pc-desc">We use cookies which are necessary to make our site work. We may also use additional cookies to analyse, improve and personalise our content and your digital experience. For more information, see our <a href="https://www.elsevier.com/legal/cookienotice/_nocache" target="_blank">Cookie Policy</a> and the list of <a href="https://support.google.com/admanager/answer/9012903" target="_blank">Google Ad-Tech Vendors</a>.
<br>
<br>
You may choose not to allow some types of cookies. However, blocking some types may impact your experience of our site and the services we are able to offer. See the different category headings below to find out more or change your settings.
<br>
</div><button id="accept-recommended-btn-handler">Allow all</button><section class="ot-sdk-row ot-cat-grp"><h3 id="ot-category-title"> Manage Consent Preferences</h3><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="1"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-1" aria-labelledby="ot-header-id-1 ot-status-id-1"></button><!-- Accordion header --><div class="ot-acc-hdr ot-always-active-group"><div class="ot-plus-minus"><span></span><span></span></div><h4 class="ot-cat-header" id="ot-header-id-1">Strictly Necessary Cookies</h4><div id="ot-status-id-1" class="ot-always-active">Always active</div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-1">These cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.
<br><br></p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" aria-label="Cookie Details List" data-parent-id="1">Cookie Details List‎</button></div></div></div><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="3"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-3" aria-labelledby="ot-header-id-3"></button><!-- Accordion header --><div class="ot-acc-hdr"><div class="ot-plus-minus"><span></span><span></span></div><h4 class="ot-cat-header" id="ot-header-id-3">Functional Cookies</h4><div class="ot-tgl"><input type="checkbox" name="ot-group-id-3" id="ot-group-id-3" role="switch" class="category-switch-handler" data-optanongroupid="3" aria-labelledby="ot-header-id-3"> <label class="ot-switch" for="ot-group-id-3"><span class="ot-switch-nob" aria-checked="false" role="switch" aria-label="Functional Cookies"></span> <span class="ot-label-txt">Functional Cookies</span></label> </div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-3">These cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.</p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" aria-label="Cookie Details List" data-parent-id="3">Cookie Details List‎</button></div></div></div><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="2"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-2" aria-labelledby="ot-header-id-2"></button><!-- Accordion header --><div class="ot-acc-hdr"><div class="ot-plus-minus"><span></span><span></span></div><h4 class="ot-cat-header" id="ot-header-id-2">Performance Cookies</h4><div class="ot-tgl"><input type="checkbox" name="ot-group-id-2" id="ot-group-id-2" role="switch" class="category-switch-handler" data-optanongroupid="2" aria-labelledby="ot-header-id-2"> <label class="ot-switch" for="ot-group-id-2"><span class="ot-switch-nob" aria-checked="false" role="switch" aria-label="Performance Cookies"></span> <span class="ot-label-txt">Performance Cookies</span></label> </div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-2">These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.</p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" aria-label="Cookie Details List" data-parent-id="2">Cookie Details List‎</button></div></div></div><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="4"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-4" aria-labelledby="ot-header-id-4"></button><!-- Accordion header --><div class="ot-acc-hdr"><div class="ot-plus-minus"><span></span><span></span></div><h4 class="ot-cat-header" id="ot-header-id-4">Targeting Cookies</h4><div class="ot-tgl"><input type="checkbox" name="ot-group-id-4" id="ot-group-id-4" role="switch" class="category-switch-handler" data-optanongroupid="4" aria-labelledby="ot-header-id-4"> <label class="ot-switch" for="ot-group-id-4"><span class="ot-switch-nob" aria-checked="false" role="switch" aria-label="Targeting Cookies"></span> <span class="ot-label-txt">Targeting Cookies</span></label> </div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-4">These cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. If you do not allow these cookies, you will experience less targeted advertising.</p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" aria-label="Cookie Details List" data-parent-id="4">Cookie Details List‎</button></div></div></div><!-- Groups sections starts --><!-- Group section ends --><!-- Accordion Group section starts --><!-- Accordion Group section ends --></section></div><section id="ot-pc-lst" class="ot-hide ot-hosts-ui ot-pc-scrollbar"><div id="ot-pc-hdr"><div id="ot-lst-title"><button class="ot-link-btn back-btn-handler" aria-label="Back"><svg id="ot-back-arw" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 444.531 444.531" xml:space="preserve"><title>Back Button</title><g><path fill="#656565" d="M213.13,222.409L351.88,83.653c7.05-7.043,10.567-15.657,10.567-25.841c0-10.183-3.518-18.793-10.567-25.835
                    l-21.409-21.416C323.432,3.521,314.817,0,304.637,0s-18.791,3.521-25.841,10.561L92.649,196.425
                    c-7.044,7.043-10.566,15.656-10.566,25.841s3.521,18.791,10.566,25.837l186.146,185.864c7.05,7.043,15.66,10.564,25.841,10.564
                    s18.795-3.521,25.834-10.564l21.409-21.412c7.05-7.039,10.567-15.604,10.567-25.697c0-10.085-3.518-18.746-10.567-25.978
                    L213.13,222.409z"></path></g></svg></button><h3>Cookie List</h3></div><div class="ot-lst-subhdr"><div class="ot-search-cntr"><p role="status" class="ot-scrn-rdr"></p><input id="vendor-search-handler" type="text" name="vendor-search-handler" placeholder="Search…" aria-label="Cookie list search"> <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 -30 110 110" aria-hidden="true"><title>Search Icon</title><path fill="#2e3644" d="M55.146,51.887L41.588,37.786c3.486-4.144,5.396-9.358,5.396-14.786c0-12.682-10.318-23-23-23s-23,10.318-23,23
            s10.318,23,23,23c4.761,0,9.298-1.436,13.177-4.162l13.661,14.208c0.571,0.593,1.339,0.92,2.162,0.92
            c0.779,0,1.518-0.297,2.079-0.837C56.255,54.982,56.293,53.08,55.146,51.887z M23.984,6c9.374,0,17,7.626,17,17s-7.626,17-17,17
            s-17-7.626-17-17S14.61,6,23.984,6z"></path></svg></div><div class="ot-fltr-cntr"><button id="filter-btn-handler" aria-label="Filter" aria-haspopup="true"><svg role="presentation" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 402.577 402.577" xml:space="preserve"><title>Filter Icon</title><g><path fill="#fff" d="M400.858,11.427c-3.241-7.421-8.85-11.132-16.854-11.136H18.564c-7.993,0-13.61,3.715-16.846,11.136
      c-3.234,7.801-1.903,14.467,3.999,19.985l140.757,140.753v138.755c0,4.955,1.809,9.232,5.424,12.854l73.085,73.083
      c3.429,3.614,7.71,5.428,12.851,5.428c2.282,0,4.66-0.479,7.135-1.43c7.426-3.238,11.14-8.851,11.14-16.845V172.166L396.861,31.413
      C402.765,25.895,404.093,19.231,400.858,11.427z"></path></g></svg></button></div><div id="ot-anchor"></div><section id="ot-fltr-modal"><div id="ot-fltr-cnt"><button id="clear-filters-handler">Clear</button><div class="ot-fltr-scrlcnt ot-pc-scrollbar"><div class="ot-fltr-opts"><div class="ot-fltr-opt"><div class="ot-chkbox"><input id="chkbox-id" type="checkbox" class="category-filter-handler"> <label for="chkbox-id"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div></div></div><div class="ot-fltr-btns"><button id="filter-apply-handler">Apply</button> <button id="filter-cancel-handler">Cancel</button></div></div></div></section></div></div><section id="ot-lst-cnt" class="ot-host-cnt ot-pc-scrollbar"><div id="ot-sel-blk"><div class="ot-sel-all"><div class="ot-sel-all-hdr"><span class="ot-consent-hdr">Consent</span> <span class="ot-li-hdr">Leg.Interest</span></div><div class="ot-sel-all-chkbox"><div class="ot-chkbox" id="ot-selall-hostcntr"><input id="select-all-hosts-groups-handler" type="checkbox"> <label for="select-all-hosts-groups-handler"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div><div class="ot-chkbox" id="ot-selall-vencntr"><input id="select-all-vendor-groups-handler" type="checkbox"> <label for="select-all-vendor-groups-handler"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div><div class="ot-chkbox" id="ot-selall-licntr"><input id="select-all-vendor-leg-handler" type="checkbox"> <label for="select-all-vendor-leg-handler"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div></div></div></div><div class="ot-sdk-row"><div class="ot-sdk-column"><ul id="ot-host-lst"></ul></div></div></section></section><div class="ot-pc-footer ot-pc-scrollbar"><div class="ot-btn-container"> <button class="save-preference-btn-handler onetrust-close-btn-handler">Confirm my choices</button></div><!-- Footer logo --><div class="ot-pc-footer-logo"><a href="https://www.onetrust.com/products/cookie-consent/" target="_blank" rel="noopener noreferrer" aria-label="Powered by OneTrust Opens in a new Tab"><img alt="Powered by Onetrust" src="https://cdn.cookielaw.org/logos/static/powered_by_logo.svg" title="Powered by OneTrust Opens in a new Tab"></a></div></div><!-- Cookie subgroup container --><!-- Vendor list link --><!-- Cookie lost link --><!-- Toggle HTML element --><!-- Checkbox HTML --><!-- plus minus--><!-- Arrow SVG element --><!-- Accordion basic element --><span class="ot-scrn-rdr" aria-atomic="true" aria-live="polite"></span><!-- Vendor Service container and item template --></div><iframe class="ot-text-resize" sandbox="allow-same-origin" title="onetrust-text-resize" style="position: absolute; top: -50000px; width: 100em;" aria-hidden="true"></iframe></div></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div></body></html>