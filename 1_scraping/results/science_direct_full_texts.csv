,ID,Title,Authors,Year,Cited By,Detected_Dataset,Detected_Topic,Abstract,DOI,Journal,URL,Mentions_Accuracy,Mentions_F1,Mentions_Precision,Mentions_Recall,Mentions_Auc,Mentions_Roc,Mentions_Sensitivity,Mentions_Specificity,Mentions_Confusion_matrix,Mentions_Loss_function,Mentions_Cross-entropy,Mentions_Mean_squared_error,Mentions_Overfitting,Mentions_Underfitting,Mentions_Cross-validation,Mentions_Training_time,Mentions_Inference_time,Mentions_Statistical_significance,Mentions_P-value,Mentions_T-test,Mentions_Anova,Mentions_Correlation,Mentions_Regression,Mentions_Baseline_comparison,Mentions_Mae,Mentions_Rmse,Mentions_Bias,Full_Text
0,0,10 Automated Face Analysis for Affective Computing,"['JF Cohn', 'F De la Torre']",2015,170,Affective Faces Database,classifier,Differences in manual coding between databases may and do occur as well and can  contribute to impaired generalizability of classifiers from one database to another.,No DOI,The Oxford handbook of affective …,https://academic.oup.com/edited-volume/28057/chapter/212009519,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
1,1,3D facial expression recognition based on automatically selected features,"['H Tang', 'TS Huang']",2008,205,Binghamton University 3D Facial Expression,"classification, classifier, facial expression recognition",facial expression recognition from 3D facial shapes is inves distances between 83 facial  feature points in the 3D space. Using a  al. at Binghamton University. It was designed to sample,No DOI,… on computer vision and pattern recognition …,https://ieeexplore.ieee.org/document/4563052,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
2,2,3D facial expression recognition based on primitive surface feature distribution,"['J Wang', 'L Yin', 'X Wei', 'Y Sun']",2006,440,Binghamton University 3D Facial Expression,facial expression recognition,expressions using 3D facial expression range data. We propose a novel approach to extract  primitive 3D facial expression  distribution to classify the prototypic facial expressions. In,No DOI,… Vision and Pattern Recognition  …,https://ieeexplore.ieee.org/document/1640921,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
3,3,3D facial expression recognition based on properties of line segments connecting facial feature points,"['H Tang', 'TS Huang']",2008,153,Binghamton University 3D Facial Expression,"classification, classifier, facial expression recognition",Binghamton University have recently constructed a 3D facial expression database for facial   This is definitely the first attempt to make a publicly available 3D facial expression database,No DOI,… on Automatic Face & Gesture Recognition,https://ieeexplore.ieee.org/document/4813304,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
4,4,3D facial expression recognition using SIFT descriptors of automatically detected keypoints,"['S Berretti', 'B Ben Amor', 'M Daoudi', 'A Del Bimbo']",2011,184,Binghamton University 3D Facial Expression,"classification, classifier, facial expression recognition","at the Binghamton University (BU-3DFE database) [34], and at the Bogaziçi University (  facial expression recognition algorithms. This is due to the fact that, differently from other",No DOI,The Visual Computer,https://www.researchgate.net/publication/220068159_3D_facial_expression_recognition_using_SIFT_descriptors_of_automatically_detected_keypoints,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
5,5,A 3D facial expression database for facial behavior research,"['L Yin', 'X Wei', 'Y Sun', 'J Wang']",2006,1631,Binghamton University 3D Facial Expression,"classification, classifier, deep learning, facial expression recognition, machine learning, neural network",3D: Critical Issues and Limitations of 2D (1) 3D surface features exhibited in facial expressions  The common theme in the current research on face expression recognition is that the face,No DOI,… and gesture recognition  …,https://ieeexplore.ieee.org/document/1613022,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
6,6,A brief review of facial emotion recognition based on visual information,['BC Ko'],2018,762,Facial Expression Recognition 2013,facial expression recognition,"recognition and facial expression recognition. In this paper, the term FER refers to facial  emotion recognition  the general aspects of recognition of facial emotion expression.) has also",No DOI,sensors,https://www.mdpi.com/1424-8220/18/2/401,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
7,7,A compact deep learning model for robust facial expression recognition,"['CM Kuo', 'SH Lai', 'M Sarkis']",2018,193,Facial Expression Recognition 2013,facial expression recognition,"In this paper, we propose a compact frame-based facial expression recognition framework  for facial expression recognition which achieves very competitive performance with respect to",No DOI,… vision and pattern recognition …,https://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w41/Kuo_A_Compact_Deep_CVPR_2018_paper.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
8,8,A deep learning perspective on the origin of facial expressions,"['R Breuer', 'R Kimmel']",2017,140,"Acted Facial Expressions In The Wild, Extended Cohn-Kanade, Static Facial Expression in the Wild","CNN, FER, deep learning, machine learning","We verify our findings on the Extended Cohn-Kanade (CK+),  tasks and tests using transfer  learning, including cross-dataset  long-short-term-memory (LSTM) recurrent neural network (",No DOI,arXiv preprint arXiv:1705.01842,https://arxiv.org/abs/1705.01842,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
9,9,A deeper look at facial expression dataset bias,"['S Li', 'W Deng']",2020,127,"Affective Faces Database, MMI Facial Expression","FER, classification, classifier","As the skew class distribution across domains is a possible bottleneck for cross-domain FER,   Pantic, “Induced disgust, happiness and surprise: An addition to the MMI facial expression",No DOI,IEEE Transactions on Affective Computing,https://arxiv.org/abs/1904.11150,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
10,10,A facial expression recognition system using convolutional networks,"['AT Lopes', 'E De Aguiar']",2015,148,Facial Expression Recognition 2013,facial expression recognition,a simple solution for facial expression recognition that uses a  Abstract—Facial expression  recognition has been an active  The recognition of facial expressions is not an easy problem,No DOI,2015 28th SIBGRAPI …,https://ieeexplore.ieee.org/document/7314574,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
11,11,A high-resolution spontaneous 3d dynamic facial expression database,"['X Zhang', 'L Yin', 'JF Cohn', 'S Canavan']",2013,920,Binghamton University 3D Facial Expression,"classification, classifier, deep learning, facial expression recognition",3D video database of spontaneous facial expressions in a diverse group of young adults.  Well-validated emotion inductions were used to elicit expressions  units and emotion detection”,No DOI,… gesture recognition  …,https://www.sciencedirect.com/science/article/pii/S0262885614001012,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
12,12,A multimodal database for affect recognition and implicit tagging,"['M Soleymani', 'J Lichtenauer', 'T Pun']",2011,1719,Affective Faces Database,"classification, classifier","our database in Section 3. Section 4 explains the experimental setup. The first experiment  paradigm, some statistics and results of classifications  to face videos from 32 participants. The",No DOI,… on affective computing,https://ieeexplore.ieee.org/document/5975141,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
13,13,A natural visible and infrared facial expression database for expression recognition and emotion inference,"['S Wang', 'Z Liu', 'S Lv', 'Y Lv', 'G Wu', 'P Peng']",2010,458,Affective Faces Database,facial expression recognition,"database for expression recognition and emotion inference, we conduct visible facial  expression recognition  There are, however, a few thermal face databases (listed in Table II) that",No DOI,IEEE Transactions …,https://ieeexplore.ieee.org/document/5523955,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
14,14,A new facial expression recognition based on curvelet transform and online sequential extreme learning machine initialized with spherical clustering,"['A Uçar', 'Y Demir', 'C Güzeliş']",2016,158,Extended Cohn-Kanade,machine learning,"So, the learning machine is called as OSELM-SC. It is  Expression database and the  Cohn-Kanade database. The  of California at irvine repository of the machine learning database",No DOI,Neural Computing and Applications,https://link.springer.com/article/10.1007/s00521-014-1569-1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
15,15,A new method for facial expression recognition based on sparse representation plus LBP,"['MW Huang', 'Z Wang', 'ZL Ying']",2010,101,Japanese Female Facial Expression,classification,"algorithm for face recognition, notable for  facial expression recognition based on sparse  representation of LBP features. Extensive experiments on Japanese Female Facial Expression (",No DOI,2010 3rd International …,https://ieeexplore.ieee.org/document/5647898,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
16,16,A performance comparison of eight commercially available automatic classifiers for facial affect recognition,"['D Dupré', 'EG Krumhuber', 'D Küster', 'GJ McKeown']",2020,165,"Acted Facial Expressions In The Wild, Affective Faces Database, Static Facial Expression in the Wild","classification, classifier","classifiers, in particular when facial expressions are spontaneous rather than posed. In the  present work, we tested eight out-of-the-box automatic classifiers,  or acted facial behavior.",No DOI,Plos one,https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0231968,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
17,17,A review on automatic facial expression recognition systems assisted by multimodal sensor data,"['N Samadiani', 'G Huang', 'B Cai', 'W Luo', 'CH Chi', 'Y Xiang']",2019,206,"Binghamton University 3D Facial Expression, Facial Expression Recognition 2013, MMI Facial Expression","FER, facial expression recognition","facial expressions (eg, sadness and anger). An efficient facial expression system (FER) can   ; this method achieves the accuracies of 98.5% (CK+ dataset) and 81.18% (MMI dataset).",No DOI,Sensors,https://www.mdpi.com/1424-8220/19/8/1863,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
18,18,A survey of 2D face recognition techniques,"['M Chihaoui', 'A Elkefi', 'W Bellil', 'C Ben Amar']",2016,131,Toronto Face Database,classification,"Figure 6 summarizes the classification of face recognition approaches presented in this   Next, we presented face databases used by researchers in this field to test their approaches and",No DOI,Computers,https://www.mdpi.com/2073-431X/5/4/21,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
19,19,A survey on facial emotion recognition techniques: A state-of-the-art literature review,"['FZ Canal', 'TR Müller', 'JC Matias', 'GG Scotton']",2022,222,Affective Faces Database,facial expression recognition,"approaches for emotion recognition from facial expressions,  area of emotion expression  recognition captured from facial  areas such as face recognition and emotion recognition color",No DOI,Information …,https://www.sciencedirect.com/science/article/pii/S0020025521010136,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
20,20,A survey on human face expression recognition techniques,"['IM Revina', 'WRS Emmanuel']",2021,350,Facial Expression Recognition 2013,facial expression recognition,"Generally, face expressions are  Face expressions are the key characteristics of non-verbal  communication. This paper describes the survey of Face Expression Recognition (FER)",No DOI,Journal of King Saud University-Computer …,https://www.sciencedirect.com/science/article/pii/S1319157818303379,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: A Survey on Human Face Expression Recognition
Techniques
the features on this page.
Skip to main contentSkip to article
My account
Sign in
Search 
## Outline
1. Abstract
2. **Beta****Powered by GenAI** Questions answered in this article
3. Keywords
4. 1\. Introduction
5. 2\. Face expression recognition system
6. 3\. Performance comparison
7. 4\. Conclusion
8. References
Show full outline
## Cited by (178)
## Figures (7)
1. 2. 3. 4. 5. 6.
Show 1 more figure
## Tables (3)
1. Table 1
2. Table 2
3. Table 3
## Journal of King Saud University - Computer and Information Sciences
Volume 33, Issue 6, July 2021, Pages 619-628
# A Survey on Human Face Expression Recognition Techniques
Author links open overlay panelI.Michael Revina, W.R. Sam Emmanuel
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.jksuci.2018.09.002Get rights and content
Under a Creative Commons license
open access
Referred to by
Erratum regarding missing Declaration of Competing Interest statements in
previously published articles
Journal of King Saud University - Computer and Information Sciences, Volume
32, Issue 10, December 2020, Pages 1219
View PDF
## AbstractHuman Face expression Recognition is one of the most powerful and challenging
tasks in social communication. Generally, face expressions are natural and
direct means for human beings to communicate their emotions and intentions.
Face expressions are the key characteristics of non-verbal communication. This
paper describes the survey of Face Expression Recognition (FER) techniques
which include the three major stages such as preprocessing, feature extraction
and classification. This survey explains the various types of FER techniques
with its major contributions. The performance of various FER techniques is
compared based on the number of expressions recognized and complexity of
algorithms. Databases like JAFFE, CK, and some other variety of facial
expression databases are discussed in this survey. The study on classifiers
gather from recent papers reveals a more powerful and reliable understanding
of the peculiar characteristics of classifiers for research fellows.
## Questions answered in this article
**Beta****Powered by GenAI**
_This is generative AI content and the quality may vary. Learn more._
1. Which facial expressions are categorized in the classification process?
2. What does appearance based feature extraction include?
3. How does the Gabor filter extract features from face images?
4. What are the classifiers used in FER techniques?
5. What is the purpose of classification in FER?
* Previous article in issue
* Next article in issue
## Keywords
Classification
Face Expression Recognition (FER)
Feature extraction
Preprocessing
## 1\. Introduction
Human facial expressions are extremely essential in social communication.
Normally communication involves both verbal and nonverbal. Non-verbal
communications are expressed through facial expressions. Face expressions are
the delicate signals of the larger communication. Non-verbal communication
means communication between human and animals through eye contact, gesture,
facial expressions, body language, and paralanguage.
Eye contact is the important phase of communication which provides the mixture
of ideas. Eye contact controls the contribution, discussions and creates a
link with others. Face expressions include the smile, sad, anger, disgust,
surprise, and fear. A smile on human face shows their happiness and it
expresses eye with a curved shape. The sad expression is the feeling of
looseness which is normally expressed as rising skewed eyebrows and frown. The
anger on human face is related to unpleasant and irritating conditions. The
expression of anger is expressed with squeezed eyebrows, slender and stretched
eyelids. The disgust expressions are expressed with pull down eyebrows and
creased nose. The surprise or shock expression is expressed when some
unpredicted happens. This is expressed with eye-widening and mouth gaping and
this expression is an easily identified one. The expression of fear is related
with surprise expression which is expressed as growing skewed eyebrows.
FER has the important stage is feature extraction and classification. Feature
extraction includes two types and they are geometric based and appearance
based. The classification is also one of the important processes in which the
above-mentioned expressions such as smile, sad, anger, disgust, surprise, and
fear are categorized. The geometrically based feature extraction comprises
eye, mouth, nose, eyebrow, other facial components and the appearance basedfeature extraction comprises the exact section of the face (Zhao and Zhang,
2016).
Generally, the face offers three different types of signals such as static,
slow and rapid signals. The static signals are skin color which includes the
several lasting aspects of face skin pigmentation, greasy deposits, face
shapes, the constitution of bones, cartilage and shape, location and size of
facial features such as brows, eyes, nose, mouth. The slow signals are
permanent wrinkles which include the changes in facial appearance such as
muscle tone and skin texture changes that happen slowly with time.
The rapid signals are raising the eyebrows which include the face muscles
movement, impermanent face appearance changes, impermanent wrinkles and
changes in the location and shape of facial features. These flashes on the
face remain for a few seconds. These three signals are altered with individual
option while it is very hard to alter static and slow signals. Also, the face
is a multi-message system and it is not only a multi-signal system. Messages
are transmitted through a face which includes emotion, feel position, age,
quality, intelligence, attractiveness and almost certainly other substances as
well (Ekman and Friesen, 2003).
This paper mainly focuses on various FER techniques with three major steps
respectively preprocessing, feature extraction and classification. Also, this
paper shows the advantages of different FER techniques and the performance
analysis of different FER techniques. In this paper, only the image based FER
techniques are chosen for the literature review and the video based FER
techniques are not chosen. Mostly FER systems meet the problems of variation
in illumination, pose variation, lighting variations, skin tone variations.
Also this paper gives an essential research idea for future FER research.
Rest of the paper is structured as follows. Section 2 elaborates the detailed
description of face expression recognition system. Section 3 evaluates the
performance of FER techniques and through different table and charts. Section
4 provides suggestions along with the conclusion of this survey.
## 2\. Face expression recognition system
The overview of the FER system is illustrated in Fig. 1. The FER system
includes the major stages such as face image preprocessing, feature extraction
and classification.
1. Download: Download high-res image (185KB)
2. Download: Download full-size image
Fig. 1. Architecture of face expression recognition system.
### 2.1. Preprocessing
Preprocessing is a process which can be used to improve the performance of the
FER system and it can be carried out before feature extraction process
(Poursaberi et al., 2012). Image preprocessing includes different types of
processes such as image clarity and scaling, contrast adjustment, and
additional enhancement processes (Bashyal et al., 2008) to improve the
expression frames (Taylor et al., 2014).
The cropping and scaling processes were performed on the face image in which
the nose of the face is taken as midpoint and the other important facial
components are included physically (Zhang et al., 2011). Bessel down sampling
is used for face image size reduction but it protects the aspects and also the
perceptual worth of the original image (Owusu et al., 2014). The Gaussian
filter is used for resizing the input images which provides the smoothness to
the images (Biswas, 2015).
Normalization is the preprocessing method which can be designed for reduction
of illumination and variations of the face images (Ji and Idrissi, 2012) with
the median filter and to achieve an improved face image. The normalizationmethod also used for the extraction of eye positions which make more robust to
personality differences for the FER system and it provides more clarity to the
input images. Localization is a preprocessing method and it uses the Viola-
Jones algorithm (Noh et al., 2007, Demir, 2014, Zhang et al., 2014, Cossetin
et al., 2016, Salmam et al., 2016) to detect the facial images from the input
image. Detection of size and location of the face images using Adaboost
learning algorithm and haar like features (Happy et al., 2015, Mahersia and
Hamrouni, 2015). The localization is mainly used for spotting the size and
locations of the face from the image.
Face alignment is also the preprocessing method which can be performed by
using the SIFT (Scale Invariant Feature Transform) flow algorithm. For this,
first calculate reference image for each face expressions. After that all the
images are aligned through related reference images (Dahmane and Meunier,
2014). ROI (Region of Interest) segmentation is one of the important type of
preprocessing method which includes three important functions such as
regulating the face dimensions by dividing the color components and of face
image, eye or forehead and mouth regions segmentation (Hernandez-matamoros et
al., 2015). In FER, ROI segmentation is most popular because for convenient
segmentation of face organs from the face images.
The histogram equalization method is used to conquer the illumination
variations (Demir, 2014, Happy et al., 2015, Cossetin et al., 2016). This
method is mainly used for enhancing the contrast of the face images and for
exact lighting also used to improve the distinction between the intensities.
In FER, more preprocessing methods are used but the ROI segmentation process
is more suitable because it detects the face organs accurately which organs
are is mainly used for expression recognition. Next the histogram equalization
is also another one important preprocessing technique for FER because it
improves the image distinction.
### 2.2. Feature extraction
Feature extraction process is the next stage of FER system. Feature extraction
is finding and depicting of positive features of concern within an image for
further processing. In image processing computer vision feature extraction is
a significant stage, whereas it spots the move from graphic to implicit data
depiction. Then these data depiction can be used as an input to the
classification. The feature extraction methods are categorized into five types
such as texture feature-based method, edge based method, global and local
feature-based method, geometric feature-based method and patch-based method.
The descriptors which extract the features based on the texture feature-based
methods are described as follows. Gabor filter is a texture descriptor for
feature extraction and it includes the magnitude and phase information. The
Gabor filter with the magnitude feature confines the information about the
organization of the face image. The phase feature precincts the information
about the complete description of the magnitude features (Bashyal and
Venayagamoorthy, 2008, Owusu et al., 2014, Zhang et al., 2014, Hernandez-
matamoros et al., 2015, Hegde et al., 2016). Local Binary Pattern (LBP) is
also a texture descriptor and it can be used for feature extraction. Generally
LBP features are produced with the binary code and it can be obtained by using
thresholding between the center pixel and its locality pixels (Happy et al.,
2015, Cossetin et al., 2016). Also LBP with Three Orthogonal Planes (TOP)
features are extracted for multi resolution approaches and (Zhao and
Pietikäinen, 2009). It is used for extracting non dynamic appearance based on
features from the static face images (Ji and Idrissi, 2012). The facial
texture features are extracted using the Gaussian Laguerre (GL) function which
grants a steering pyramidal structure which extracts the texture features andthe facial related occurrence information. Comparing to Gabor function GL uses
the single filter instead of multiple filters (Poursaberi et al., 2012).
Moreover another descriptor which is used namely Vertical Time Backward (VTB)
which also extracts the texture features of face images. Moments descriptor
extracts the shape related features of significant facial components. Both VTB
and moments descriptors are effective on spatiotemporal planes (Ji and
Idrissi, 2012). Weber Local Descriptor (WLD) is a feature extraction technique
that extracts the high discriminant texture features from the segmented face
images (Cossetin et al., 2016). Feature extraction is performed with three
stages using Supervised Descent Method (SDM). At first, the facial main
positions are extracted. Next the related positions are selected. Finally it
estimates the distance between the various components of the face (Salmam et
al., 2016). Weighted Projection based LBP (WPLBP) is also a feature extraction
but based on the instructive regions which extracts the LBP features. After
that based on the significance of the instructive regions these features are
weighted (Kumar et al., 2016). Discrete Contourlet Transform (DCT) extracts
the texture features which can be performed by decomposition with two key
stages. The stages are Laplacian Pyramid (LP) and Directional Filter Bank
(DFB) which is used in the transformed domain. In LP stage, partitions the
image into low pass, band pass and confines the discontinuities position. The
DFB stage processes the band pass and forms the linear composition by
associating the discontinuities position (Biswas, 2015).
The descriptors which extract the features based on the edge based methods are
described as follows. Line Edge Map (LEM) descriptor is a facial expression
descriptor which improves the geometrical structural features by using the
dynamic two strip algorithm (Dyn2S) (Gao et al., 2003). Based on the motion
analysis two types of facial features are extracted such as non discriminative
and discriminative facial features (Noh et al., 2007). Graphics-processing
unit based Active Shape Model (GASM) is the feature extraction method which
can be performed with edge detection, enhancement, tone mapping and local
appearance model matching. After that the image ratio features are extracted
from the expressed face images (Song et al., 2010). Histogram of Oriented
Gradients (HOG) is a window supported feature descriptor which uses the
gradient filter. The extracted features are based on the edge information of
the registered face images. It extracts the visual features, for example a
smile expression means curvature shaped eyes (Dahmane and Meunier, 2014).
The descriptors which extract the features based on the global and local
feature-based methods are described as follows. Principal Component Analysis
(PCA) method is used for feature extraction. It extracts the global and low
dimensional features. Independent Component Analysis (ICA) is also a feature
extraction method which extracts the local features using the multichannel
observations (Taylor et al., 2014). Stepwise Linear Discriminant Analysis
(SWLDA) is the feature extraction technique which extracts the localized
features with backward and forward regression models. Depends on the class
labels the F-test values are estimated for both regression models (Siddiqi et
al., 2015).
The descriptors which extract the features based on the geometric feature-
based methods are described as follows. Local Curvelet Transform (LCT) is a
feature descriptor which extracts the geometric features which depends on
wrapping mechanism. The extracted geometric features are mean, entropy and
standard deviation (Demir, 2014). Addition to these geometrical features
energy, kurtosis are extracted by using three stage steerable pyramid
representation (Mahersia and Hamrouni, 2015).
The descriptors which extract the features based on patch-based methods aredescribed as follows. Facial movement features are extracted as patches
depending upon the distance characteristics. These are performed by using two
processes such as extracting the patches and patch matching. The patch
matching is performed by translating extracted patches into distance
characteristics (Zhang et al., 2011).
The texture feature based descriptors are more useful feature extraction
method than the others because it extracts the texture features like related
to the appearance which provides the important feature vectors for FER. Also
Local Directional Number (LDN) pattern (Rahul and Cherian, 2016), Local
Directional Ternary Pattern (LDTP) (Ryu et al., 2017), KL-transform Extended
LBP (K-ELBP) (Guo et al., 2016) and Discrete Wavelet Transform (DWT) (Nigam et
al., 2018) texture feature based descriptors are used as feature descriptors
in recent years FER.
Several extracted features have high dimensional vectors. Generally these
feature vectors are reduced by using various dimensionality reduction
algorithms such as PCA, Linear Discriminant Analysis, Whitened Principle
Component Analysis and the important features are also selected with different
algorithms such as Adaboost and similarity scores.
### 2.3. Classification
Classification is the final stage of FER system in which the classifier
categorizes the expression such as smile, sad, surprise, anger, fear, disgust
and neutral.
The directed Line segment Hausdorff Distance (dLHD) method is used for
recognition of expressions (Gao et al., 2003). Euclidean distance metric is
also used for classification purpose which uses the normalized score and
similarity score matrix for estimating Euclidean distance (Hegde et al.,
2016). Minimum Distance Classifier (MDC) is also one of the distance based
classifier used for classification which estimates the distance between the
feature vectors every sub image (Islam et al., 2018). The KNN (k ? Nearest
Neighbors) algorithm is a classification method in which the relationship
among the assessment models and the other models are estimated during the
training stage (Poursaberi et al., 2012).
Support Vector Machine (SVM) is one of the classification techniques in which
two types of approaches are involved. They are one against one and one against
all approaches. One against all classification means it constructs one sample
for each class (Zhao and Pietikäinen, 2009, Zhang et al., 2011, Zhang et al.,
2014, Biswas, 2015). One against one classification means it constructs one
class for each pair of classes (Happy et al., 2015, Kumar et al., 2016, Hegde
et al., 2016) and SVM is one of the strongest classification methods for
advanced dimensionality troubles (Dahmane and Meunier, 2014). SVM is the
supervised machine learning technique and it uses four types of kernels for
its better performance (Hernandez-matamoros et al., 2015). They are linear,
polynomial, Radial Basis Function (RBF) and sigmoid. The linear kernel maps
the high dimensional data and it is linearly separable (Zhang et al., 2014,
Kumar et al., 2016). The RBF kernel uses the function that maps the single
feature into the high dimensional data (Song et al., 2010, Wang et al., 2010,
Dahmane and Meunier, 2014, Happy et al., 2015, Hegde et al., 2016). The
polynomial kernel learns the nonlinear models and also resolves their
similarity (Zhao and Pietikäinen, 2009, Zhang et al., 2011, Ji and Idrissi,
2012, Biswas, 2015).
The Hidden Markov Model (HMM) classifier is the statistical model which
categorizes the expressions into different types (Taylor et al., 2014). Hidden
Conditional Random Fields (HCRF) representation is used for classification. It
uses the full covariance Gaussian distribution for superior classificationperformance (Siddiqi et al., 2015).
Online Sequential Extreme Learning Machine (OSELM) is a method that uses RBF
for classification. OSELM mainly contains two stages. They are initialization
and sequential learning stages. Initialization stage includes the training
samples (Demir, 2014). Pair wise classifiers are also used for expression
classification. It uses the one against one classification approach so
exacting separation is utilized (Cossetin et al., 2016).
ID3 Decision Tree (DT) classifier is a rule based classifier which extracts
the predefined rules to produce competent rules. The predefined rules are
generated from the decision tree and it was constructed by information gain
metrics. The classification is performed using the least Boolean evaluation
(Noh et al., 2007, Rashid, 2016). Classification and Regression Tree (CART) is
a machine learning algorithm for classification. The metric likely Decision
tree and Gini impurity are estimated. CART classifiers are signified by using
the distance vectors (Salmam et al., 2016).
Learning Vector Quantization (LVQ) is the unsupervised clustering algorithm
(Bashyal et al., 2008) which has two layers namely competitive and output
layers. The competitive layer has the neurons that are known as subclasses.
The neuron which is the greatest match in competitive layer then put high for
the class of exacting neuron in the output layer. Multi Layer Perceptron (MLP)
is also used for classification and it contains three layers such as input
layer, output layer and processing layer in which neurons are present (Rashid,
2016).
The Multilayer Feed Forward Neural Network (MFFNN) classifier uses three
layers such as input, hidden and output layers and back propagation algorithm
for classification. In the training stage the weights are initialized and the
activation units are estimated (Owusu et al., 2014). Bayesian neural network
classifier is the classification method which also includes three layers such
as input, hidden and output layers. The classical back propagation algorithm
is used with Bayesian classifier for its better accuracy (Mahersia and
Hamrouni, 2015). Convolution Neural Network (CNN) consists of two layers such
as convolutional layer and subsampling layer in which the two dimensional
images are taken as input. In convolutional layer the feature maps are
produced by intricate the convolution kernels with the two dimensional images
where as in the subsampling layer, pooling and redeployment are performed
(Shan et al., 2017). The CNN also contains two important perceptions likely
shared weight and sparse connectivity (Rashid, 2016). In FER, the CNN
classifier used as multiple classifiers for the different face regions. If CNN
is framed for entire face image then first frame the CNN for mouth area and
next for eye area likely for each other area CNNs are framed (Cui et al.,
2016).
Deep Neural Network (DNN) contains various hidden layers and the more
difficult functions are trained efficiently comparing with other neural
networks (Li and Lam, 2015). The Deep Belief Network (DBN) contains the hidden
variable resides of the various number of Restricted Boltzmann Machine (RBM)
which are the undirected generative pattern (Lv, 2015). DBN contains the Back
Propagation (BP) layer classifies the high-level features using classification
(Yang et al., 2016). DBN generally includes two phases such as pre-learning
and fine-tuning (Wu and Qiu, 2017) in which RBM are developed separately in
the first step whereas the BP are learning the input and output data in the
last phase.
According to several classifiers SVM classifier gives better recognition
accuracy and it provides better classification. The neural network based
classifier CNN gives better accuracy than the other neural network basedclassifiers. In FER, SVM classifier is more exploitable comparing with other
classifiers for recognition of expressions.
The various FER techniques with their algorithm is analyzed in Table1 which
includes the algorithms that are used for three important requirements such as
preprocessing, feature extraction and classification. The various
preprocessing methods used in this table are, face detection, image
enhancement, normalization, Gabor filter, localization, face acquisition, down
sampling, histogram equalization, face region detection, face alignment, ROI
segmentation and resizing. The different feature extraction methods used in
this table are LEM, Action based model, Gabor filter, LBP-TOP, GASM, Patch
based, GL wavelet, LBP, VTB, Moments, PCA, ICA, LCT, HOG, Steerable pyramid,
DCT, SWLDA, WLD, SDM, WPLBP, haar like features, LDN, LDTP, DWT, K-ELBP, 2DPCA
and eigenfaces. Classifiers used in this table are ID3 decision tree, LVQ,
SVM, KNN, HMM, MFFNN, OSLEM, Bayesian neural network, HCRF, pair wise, CART,
Euclidean distance, CNN, MDC, Chi square test and fisher discrimination
dictionary.
Table 1. Algorithm analysis of 2D FER Techniques.
Author, Year| Preprocessing method| Feature extraction method| Classification
method
---|---|---|---
Gao et al. (2003)| Not reported| LEM| dLHD
Noh et al. (2007)| Face detection| Action based model| ID3 decision tree
Bashyal et al. (2008)| Image enhancement| Gabor filter (GF)| LVQ
Zhao and Pietikäinen (2009)| Not reported| LBP ? TOP| SVM
Song et al. (2010)| Not specified| GASM| SVM
Wang et al. (2010)| Not reported| Not reported| SVM
Zhang et al. (2011)| Gabor filter| Patch based| SVM
Poursaberi et al. (2012)| Localization, Normalization| GL Wavelet| KNN
Ji and Idrissi (2012)| Face acquisition| LBP, VTB, Moments| SVM
Taylor et al. (2014)| Enhancement| PCA ICA| HMM
Owusu et al. (2014)| Down sampling| GF| MFFNN
Demir (2014)| Histogram equalization| LCT| OSLEM
Zhang et al. (2014)| Face region detection| GF| SVM
Dahmane and Meunier (2014)| Face alignment| HOG| SVM
Mahersia and Hamrouni (2015)| Normalization| Steerable pyramid| Bayesian
neural network
Hernandez-matamoros et al. (2015)| ROI segmentation| Gabor function| SVM
Happy et al. (2015)| Histogram equalization| LBP| SVM
Biswas (2015)| Histogram equalization| DCT| SVM
Siddiqi et al. (2015)| Not specified| SWLDA| HCRF
Cossetin et al. (2016)| Histogram equalization| LBP, WLD| Pairwise Classifiers
Salmam et al. (2016)| Face detection| SDM| CART
Kumar et al. (2016)| Not reported| WPLBP| SVM
Hegde et al. (2016)| Resizing| GF| Euclidean distance (ED), SVM
Rashid (2016)| Balancing data| Luxand Face SDK, ED| DT, MLP, CNN
Cui et al. (2016)| Face detection, normalization| Not reported| CNN
Jain et al. (2016)| Face detection| LBP| ED, SVM, Neural Network
Rahul and Cherian (2016)| Face region cropping| LDN| Chi square test
Guo et al. (2016)| Normalization| K-ELBP| SVM
Sharma and Rameshan, 2017| Face normalization| HOG, LBP, Eigen faces| Fisher
discrimination dictionary
Shan et al. (2017)| Histogram equalization| Haar like features| CNN
Nazir et al. (2017)| Face Detection| HOG, DCT| KNN
Chang (2017)| Face detection| DCT, GF| SVMZhang et al. (2017)| Localization| Not reported| CNN
Ryu et al. (2017)| Not reported| LDTP| SVM
Nigam et al. (2018)| Cropping, Normalization| DWT, HOG| SVM
Clawson et al. (2018)| Histogram equalization| Not reported| CNN
Islam et al. (2018)| Face detection| 2DPCA| MDC
In recent year papers, for preprocessing mostly the histogram equalization
method is used. For feature extraction, Gabor filter, WPLBP, SDM, WLD, HOG are
used. In feature extraction, the majority of the methods are based on the
texture descriptor such as LBP based which gives improved results. In modern
years, the classification uses the classifiers are SVM, Euclidean distance,
CART, Neural network based classifiers and pair wise classifiers. The SVM
classifier is highly used classifier in FER and it uses one- to ?one, one- to
all classification approach. Also, SVM with RBF kernel is most probably used
which gives the highest classification performance comparing to other
classifiers.
In 3D FER, the preprocessing of face images are performed by using the various
methods such as smoothing, cropping, face alignment. The facial expressions
are recognized from videos using the head gesticulation (Anisetti et al.,
2005). The features such as geometric features and appearance features are
extracted from 3D faces using the various descriptors likely 3D surface
descriptors (Yi Sun, 2008), texture filters (Gaeta and Gerardo Iovane, 2013),
and covariance region descriptors (Hariri et al., 2017). The facial landmarks
eye and nose areas are localized by extracting the principal curvatures and
shape index for efficient recognition of facial expressions (Vezzetti et al.,
2017). The lip based features are easily extracted by using the geometric
descriptors (Moos et al., 2014) and the mean, median, histogram (Vezzetti et
al., 2016) features also extracted for 3D FER. The descriptors also formed
from two facial components such as Basic Facial Shape Component (BFSC) and
Expressional Shape Component (ESC) (Gong et al., 2009). The classification is
performed in 3D FER using the different types of classifiers likely multi-SVM
(Hariri et al., 2017), HMM (Yi Sun, 2008), neural networks (Hamit Soyel,
2007), Deep Fusion ? CNN (Li et al., 2017), Naïve Bayes Classifier (NBC)
(Arman Savran, 2017). In 3D FER experimentation, mostly the Binghamton
University 3D Facial Expression (BU-3DFE) database and Bosphorus databases are
used.
### 2.4. Database description
Experiments are performed on FER by using various databases likely Japanese
Female Facial Expressions (JAFFE, 2017), Cohn ? Kanade (CK, 2017), Extended
Cohn ? Kanade (CK+), MMI (MMI, 2017), Multimedia Understanding Group (MUG,
2017), Taiwanese Facial Expression Image Database (TFEID, 2017), Yale (Yale,
2017), AR face database (AR, 2018), Real-time database (Zhao and Pietikäinen,
2009), Own database (Siddiqi et al., 2015) and Karolinska Directed Emotional
Faces (KDEF, 2018).
In most of the experiments, JAFFE database is used. JAFFE holds ten Japanese
female?s expressions with seven facial expressions and totally 213 images.
Each image in JAFFE database contains 256 × 256 pixel resolution. Some of the
sample images of JAFFE database are shown in Fig. 2.
1. Download: Download high-res image (113KB)
2. Download: Download full-size image
Fig. 2. Sample images from JAFFE database.
CK database also has seven expressions but it contains 132 subjects that are
posed with natural and smile. It contains totally 486 image sequences with 640
× 490 pixel resolution of gray images. Some of the sample images of the CK
database are shown in Fig. 3.1. Download: Download high-res image (120KB)
2. Download: Download full-size image
Fig. 3. Sample images from CK database.
Table 2 shows the origin, acquisition, expression types, number of images,
resolution details of the FER databases. The Real-time dataset is also used
for FER which contains nearly 2250 images for six expressions and another one
own dataset is used which contains 687 image pairs with 640 × 480 resolution.
Table 2. FER Databases description.
Database Name| Origin| Acquisition| Expressions| No. of images| Resolution
---|---|---|---|---|---
Japanese Female Facial Expressions (JAFFE)| Japan| Photos are taken from
Kyushu University| Smile, sad, surprise, anger, fear, disgust, neutral| 213|
256 × 256
Yale| California| Photos are taken from U.C. San Diego Computer vision
Laboratory| Happy, normal, sad, sleepy, surprised, wink.| 165| 168 × 192
Cohn Kanade (CK)| United States| Photos are taken by Panasonic WV3230 cameras|
Joy, surprise, anger, fear, disgust, sadness| 486| 640 × 490
Extended Cohn Kanade (CK+)| United States| Photos are taken by Panasonic
AG-7500 cameras| Neutral, sadness, surprise, happiness, fear, anger, contempt
and disgust| 593| 640 × 490
Multimedia Understanding Group (MUG)| Caucasian| High resolution and no
occlusion photos are taken| Neutral, sadness, surprise, happiness, fear,
anger, and disgust| 1462| 896 × 896
AR face database| Spain| Photos are taken by Sony 3CCD cameras| Neutral,
smile, anger, scream| 4000| 768 × 576
MMI| Netherlands| Photos are taken by JVC GR-D23E Mini-DV cameras| Disgust,
Happiness, surprise, neutral, surprise, sad, fear| 250| 720 × 576
Taiwanese Facial Expression Image Database (TFEID)| Taiwan| Photos are taken
by two CCD cameras simultaneously with different angles (0°,45°)| Neutral,
anger, contempt, disgust, fear, happiness, sadness, surprise| 7200| 600 × 480
Karolinska Directed Emotional Faces (KDEF)| Sweden| Photos are taken by Pentax
LX cameras| Angry, Fearful, Disgusted, Sad, Happy, Surprised, Neutral| 490|
762 × 562
## 3\. Performance comparison
The performance comparison of this survey is based on the complexity rate,
recognition accuracy on different databases, availability of preprocessing and
feature extraction methods, expression count analysis, major contribution and
advantages of the various FER techniques.
The complexity rates of the various FER techniques are shown in Fig. 4. The
x-axis indicates the complexity value of various FER techniques and the y-axis
indicates the name of the FER methods. The complexity value of each method is
calculated from its own papers which is categorized into three levels are
less, medium and high. In Fig. 4, the less complexity is denoted as 1, the
medium complexity is denoted as 2 and the high complexity is denoted as 3. The
complexity rates are less in Gabor functions, DCT, LBP and WLD comparing with
other methods.
1. Download: Download high-res image (360KB)
2. Download: Download full-size image
Fig. 4. Complexity rate of various FER techniques.
The Accuracy rates of the various FER techniques are plotted in Fig. 5 where
the x-axis indicates the name of the FER methods and the y-axis indicates the
percentage of accuracy acquired in FER techniques. The accuracy of each method
is analyzed from its own papers and difference databases are used in every
paper, so the mean of the accuracy rate is calculated. The methods such asGabor functions and DCT with SVM classifier give better accuracy. LBP and WLD
descriptors with the pair wise classifiers give better accuracy rate.
1. Download: Download high-res image (406KB)
2. Download: Download full-size image
Fig. 5. Accuracy rate of various FER techniques.
The availability of preprocessing and feature extraction is shown in Fig. 6.
The x-axis indicates the author name of the various FER techniques. The y-axis
denotes the availability of preprocessing and feature extraction methods in
survey papers. The availability of preprocessing and the feature extraction
calculation is based on the presence of the preprocessing and feature
extraction in FER papers. If the preprocessing is a presence in the paper then
it is denoted as 1 otherwise denoted as 0 and it is represented as 0.1 for
visible. Likewise the same procedure for calculating the availability of
feature extraction in FER papers.
1. Download: Download high-res image (416KB)
2. Download: Download full-size image
Fig. 6. Availability of preprocessing and feature extraction.
The expression count analysis of FER techniques are described in Fig. 7 here
the x-axis denotes the name of the FER methods and the y-axis denotes the
number of recognized expressions using the FER methods. The expression count
is analyzed from its own papers and the maximum of seven numbers of
expressions are recognized in most papers.
1. Download: Download high-res image (402KB)
2. Download: Download full-size image
Fig. 7. Expression count analysis.
The performance analysis of various FER techniques is described in Table 3. It
includes the various fields such as author name, year, FER method name,
database name, complexity rate, recognition accuracy, number of expressions
recognized, major contributions and advantage of FER techniques. The author
name and year field of the table denote the authors of various FER papers and
the year denotes the publishing year of the FER papers. The FER method name
field of the table describes the methods used for recognition of facial
expressions. The databases used in the FER papers are JAFFE, CK, CK+, MMI,
MUG, TFEID, AR, Yale, KDEF (Karolinska Directed Emotional Faces), Real-time
and own dataset. The complexity rate of various FER techniques are denoted as
less, medium, high and it is also illustrated in Fig. 4. The recognition
accuracy of the different techniques is from 75% to 99% and it is also
illustrated in Fig. 5. The number of expressions recognized in the FER survey
papers is 7. The LEM method recognizes only 3 expressions and the majority of
paper recognizes 6 or 7 expressions. The major contribution field of this
table describes the major work involved in the FER papers and the advantage
field indicates the benefits of the FER techniques.
Table 3. Performance analysis of FER techniques.
Author name, year| FER method name| Database name| Complexity| Recognition
accuracy (%)| No. of expressions recognized| Major contribution| Advantages
---|---|---|---|---|---|---|---
Gao et al. (2003)| LEM, dLHD| AR| Less| 86.6| 3| Oriented structural features
are extracted| Suitable for real time applications
Noh et al. (2007)| Action based, ID3 decision tree| JAFFE| Less| 75| 6| Facial
features are discriminative & non discriminative| Cost effective in speed and
accuracy
Bashyal et al. (2008)| GF, LVQ| JAFFE| Less| 88.86| Not reported| LVQ performs
better recognition for fear expressions| Better accuracy for fear expressions
Zhao and Pietikäinen (2009)| GASM, SVM| CK| High| 93.85| 6| Adaboost learningfor multi resolution features| Flexible feature selection
Song et al. (2010)| LBP-TOP, SVM| JAFFE, CK Realtime| Less| 86.85| 7|
Detection of facial features point motion & image ratio features| More robust
to lighting variations
Wang et al. (2010)| SVM| JAFFE| Less| 87.5| Not reported| DKFER for emotion
detection| More efficient emotion detection
Zhang et al. (2011)| Patch based, SVM| JAFFE, CK| Less| 82.5| 6| Capture
facial movement features based on distance features| Effective recognition
performance
Poursaberi et al. (2012)| GL Wavelet, KNN| JAFFE, CK, MMI| Medium| 91.9| 6|
Extraction of texture and geometric information| Wealthy capability for
texture analysis
Ji and Idrissi (2012)| LBP, VTB, Moments, SVM| CK, MMI| Medium| 95.84| 6|
Extraction of spatial temporal Features| Effective image based recognition
Taylor et al. (2014)| PCA, ICA, HMM| Own| Less| 98| 6| Multilayer scheme to
conquer similarity problems| High accuracy with own dataset
Owusu et al. (2014)| GF, MFFNN| JAFFE, Yale| High| 94.16| 7| Feature selection
based on Adaboost| Lowest computational cost
Demir (2014)| LCT, OSLEM| JAFFE, CK| High| 94.41| 7| Extraction of statistical
features mean, entropy and S.D| Reliable algorithm for recognition
Zhang et al. (2014)| GF, SVM| JAFFE, CK| Less| 82.5| 7| Template matching for
finding similar features| High robustness & fast processing speed
Dahmane and Meunier (2014)| HOG, SVM| JAFFE| High| 85| 7| SIFT flow algorithm
for face Alignment| Robust to rotation, occlusion & clutter
Mahersia and Hamrouni (2015)| Streerable pyramid, Bayesian NN| JAFFE, CK|
Less| 95.73| 7| Statistical features are extracted from the steerable
representation| Robust features & achieve good results
Hernandez-matamoros et al. (2015)| Gabor function, SVM| KDEF| Less| 99| Not
reported| Segmentation of face into two Regions| High performance with low
cost
Happy et al. (2015)| LBP, SVM| JAFFE, CK+| Less| 93.3| 6| Facial landmarks lip
and eyebrow corners are detected| Lower computational complexity
Biswas (2015)| DCT, SVM| JAFFE, CK| Less| 98.63| 6| Each image is decomposed
up to fourth level| Very fast & high accuracy
Siddiqi et al. (2015)| SWLDA, HCRF| JAFFE, CK+,MMI, Yale| High| 96.37| 6|
Expressions are categorized into 3 major categories| High accuracy
Cossetin et al. (2016)| LBP, WLD, Pairwise classifier| JAFFE, CK, TFEID| Less|
98.91| 7| Each pair wise classifier uses a particular subset| High accuracy &
less computation power
Salmam et al. (2016)| SDM, CART| JAFFE, CK| Less| 89.9| 6| Decision tree for
training| Improved recognition accuracy
Kumar et al. (2016)| WPLBP, SVM| JAFFE, CK+, MMI| Medium| 98.15| 7| Extraction
of discriminative features from informative face regions| Lower
misclassification
Hegde et al. (2016)| GF, ED, SVM| JAFFE, Yale| Less| 88.58| 6| Projects
feature vector space into low dimension space| Improves the recognition
efficiency
From this table clearly understand the combination of preprocessing method ROI
segmentation, feature extraction method GF and classification method SVM gives
better FER accuracy 99% and less complexity which are analyzed by using the
KDEF database. Comparing with other FER methods SVM classification is mostly
used which classifies the maximum 7 number of expressions. From this table
JAFFE, CK databases are frequently used in many papers and the Real-time
dataset is used with the SVM classifier which gives 86.85% accuracy.## 4\. Conclusion
The important future enhancements described from recent papers are FER for
side view faces using the subjective information of facial sub-regions and use
different parameters to represent the pose of the face for real-time
applications. FER is used in real-time applications such as driver sate
surveillance, medical, robotics interaction, forensic section, detecting
deceptions. This survey paper is useful for software developers to develop
algorithms based on their accuracy and complexity. Also, it is helpful for
hardware implementation to implement with low cost depends on their need. This
survey compares algorithms based on preprocessing, feature extraction,
classification and major contributions. The performance analysis is done based
on the database, complexity rate, recognition accuracy and major
contributions. This survey discusses the properties such as availability of
preprocessing and feature extraction and expression count. The power of
algorithms, advantages are discussed elaborately to reach the aim of this
survey. ROI segmentation method is used for preprocessing and it gives the
highest accuracy 99%. According to feature extraction GF have less complexity
which gives the accuracy always between 82.5% and 99%. The highest recognition
accuracy of 99% is provided by the SVM classifier and it recognizes the
several expressions such as disgust, sad, smile, surprise, anger, fear,
neutral effectively. In 2D FER, mostly JAFFE and CK database are used for
efficient performance than the other databases.
Recommended articles"
21,21,A survey: facial micro-expression recognition,"['M Takalkar', 'M Xu', 'Q Wu', 'Z Chaczko']",2018,128,Facial Expression Recognition 2013,facial expression recognition,"Facial expression recognition plays a crucial role in a wide  only be defined using a broader  classification scale. This paper is  In: Automatic face and gesture recognition (FG), 2013 10th",No DOI,Multimedia Tools and Applications,https://link.springer.com/article/10.1007/s11042-017-5317-2,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
22,22,"A systematic review on affective computing: Emotion models, databases, and recent advances","['Y Wang', 'W Song', 'W Tao', 'A Liotta', 'D Yang', 'X Li', 'S Gao']",2022,315,Affective Faces Database,"CNN, FER, classification, classifier, deep learning, machine learning, neural network","affective computing, we provide a systematical survey of important components: emotion  models, databases,  by five kinds of commonly used databases for affective computing. Next, we",No DOI,Information …,https://www.sciencedirect.com/science/article/pii/S1566253522000367,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
23,23,A-MobileNet: An approach of facial expression recognition,"['Y Nan', 'J Ju', 'Q Hua', 'H Zhang', 'B Wang']",2022,155,Facial Expression Recognition 2013,facial expression recognition,"Facial expression recognition (FER) is to separate the  to facial expression recognition tasks,  but the recognition results  a dense network for expression recognition. However, compared",No DOI,Alexandria Engineering Journal,https://www.sciencedirect.com/science/article/pii/S1110016821006682,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
24,24,Accurate periocular recognition under less constrained environment using semantics-assisted convolutional neural network,"['Z Zhao', 'A Kumar']",2016,136,Toronto Face Database,neural network,"v4 is the first publicly available long-range iris and face database acquired under NIR  illumination, which is released by the Center for Biometrics and Security Research (CBSR) from",No DOI,IEEE Transactions on Information Forensics …,http://ieeexplore.ieee.org/document/7775081,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
25,25,Acted facial expressions in the wild database,"['A Dhall', 'R Goecke', 'S Lucey', 'T Gedeon']",2011,140,"Acted Facial Expressions In The Wild, Static Facial Expression in the Wild","classification, facial expression recognition, machine learning, neural network","The Cohn-Kanade database [17] is widely used facial expression database. This database  formed a standard for facial expression recognition, it contains both static and dynamic data",No DOI,"… , Australia, Technical Report …",https://paperswithcode.com/dataset/acted-facial-expressions-in-the-wild-afew,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
26,26,Ad-corre: Adaptive correlation-based loss for facial expression recognition in the wild,"['AP Fard', 'MH Mahoor']",2022,113,"Affective Faces Database, Expression in-the-Wild","FER, classification, classifier, facial expression recognition","FER and then discuss the use of DML for general image classification tasks. Afterward, we  review the work used DML in FER expressional features from an input face image, as well as",No DOI,IEEE Access,https://ieeexplore.ieee.org/document/9727163,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
27,27,Adaptive deep metric learning for identity-aware facial expression recognition,"['X Liu', 'BVK Vijaya Kumar', 'J You']",2017,236,"CMU Multi-PIE, MMI Facial Expression",FER,"features which are not useful for FER. As shown in Fig. 1,  For FER, we desire that two  face images with the same  approaches in posed facial expression dataset (eg, CK+, MMI),",No DOI,Proceedings of the IEEE …,http://ieeexplore.ieee.org/document/8014813,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
28,28,Adaptive feature mapping for customizing deep learning based facial expression recognition model,"['BF Wu', 'CH Lin']",2018,106,"Extended Cohn-Kanade, Facial Expression Recognition 2013, Radboud Faces Database","deep learning, facial expression recognition, machine learning","progress in this artificial intelligence era. Many deep learning approaches have been applied  in  Compared to the competing deep learning architectures with the same training data, our",No DOI,IEEE access,https://ieeexplore.ieee.org/document/8291717,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
29,29,Addressing bias in machine learning algorithms: A pilot study on emotion recognition for intelligent systems,"['A Howard', 'C Zhang', 'E Horvitz']",2017,115,"Affective Faces Database, Radboud Faces Database",machine learning,"on 50% of the feature vectors classified as Fear or Surprise by the deep learning algorithm  and extracted from the Radboud Faces Database, the Dartmouth Database of Children’s",No DOI,2017 IEEE Workshop on …,https://ieeexplore.ieee.org/abstract/document/8025197,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
30,30,"Advances, challenges, and opportunities in automatic facial expression recognition","['B Martinez', 'MF Valstar']",2016,169,Facial Expression Recognition 2013,facial expression recognition,In this chapter we consider the problem of automatic facial expression analysis. Our take on  this is that the field has reached a point where it needs to move away from considering,No DOI,Advances in face detection and facial image …,https://link.springer.com/chapter/10.1007/978-3-319-25958-1_4,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
31,31,Aff-wild2: Extending the aff-wild database for affect recognition,"['D Kollias', 'S Zafeiriou']",2018,167,Affective Faces Database,"CNN, deep learning, neural network","the paper is that by training deep neural networks on the Aff- -art emotion recognition  performance on the other database,  of Aff-Wild2 database starts with the detection of faces [1] in",No DOI,arXiv preprint arXiv:1811.07770,https://arxiv.org/abs/1811.07770,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
32,32,Aff-wild: valence and arousal'In-the-Wild'challenge,"['S Zafeiriou', 'D Kollias', 'MA Nicolaou']",2017,373,Expression in-the-Wild,"deep learning, machine learning, neural network",”inthe-wild”. For a recent survey on facial behaviour analysis in-the-wild with an emphasis on  deep learning  come from movies and the annotation is limited to the universal expressions.,No DOI,Proceedings of the …,https://ieeexplore.ieee.org/document/8014982/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
33,33,"Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework","['D Kollias', 'S Zafeiriou']",2021,206,Expression in-the-Wild,"classification, deep learning","databases and ii) design and training of novel deep neural architectures  in-the-wild databases,  ie, Aff-Wild and Aff-Wild2 and presents the design of two classes of deep neural networks",No DOI,arXiv preprint arXiv:2103.15792,https://arxiv.org/abs/2103.15792,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
34,34,Affectiva-mit facial expression dataset (am-fed): Naturalistic and spontaneous facial expressions collected,"['D McDuff', 'R Kaliouby', 'T Senechal', 'M Amr']",2013,314,"Acted Facial Expressions In The Wild, Affective Faces Database",classifier,"Computer classification of facial expressions requires large amounts of data and this data  needs to  If consent is granted, the commercial is played in the browser whilst simultaneously",No DOI,Proceedings of the …,https://ieeexplore.ieee.org/document/6595975,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
35,35,Affective image classification using features inspired by psychology and art theory,"['J Machajdik', 'A Hanbury']",2010,1010,Affective Faces Database,classification,"as features, and the expected affective state in which the user is  that are specific to the task  of affective image classification.  face detection algorithm by Viola and Jones [28] to find faces",No DOI,Proceedings of the 18th ACM international …,https://dl.acm.org/doi/10.1145/1873951.1873965,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
36,36,"Affectnet: A database for facial expression, valence, and arousal computing in the wild","['A Mollahosseini', 'B Hasani']",2017,1965,"Acted Facial Expressions In The Wild, Affective Faces Database, Expression in-the-Wild, Static Facial Expression in the Wild","CNN, FER, classification, classifier, deep learning, facial expression recognition, machine learning, neural network",our deep neural network baselines can perform better than conventional machine learning   [18] released Acted Facial Expressions in the Wild (AFEW) from 54 movies by a recom,No DOI,IEEE Transactions on …,https://arxiv.org/abs/1708.03985,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
37,39,Appearance-based gender classification with Gaussian processes,"['HC Kim', 'D Kim', 'Z Ghahramani', 'SY Bang']",2006,102,Toronto Face Database,classification,"In Section 3, we introduce Gaussian process classification. In  classification. In Section 5,  we show experimental results on the PF01 database and compared with other classification",No DOI,Pattern Recognition Letters,https://www.sciencedirect.com/science/article/pii/S0167865505002801,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
38,40,Attention mechanism-based CNN for facial expression recognition,"['J Li', 'K Jin', 'D Zhou', 'N Kubota', 'Z Ju']",2020,284,Japanese Female Facial Expression,CNN,"The JAFFE dataset was collected from 10 Japanese females in a laboratory condition and  includes 213 images of posed expressions. For each subject, there are three or four images",No DOI,Neurocomputing,https://www.sciencedirect.com/science/article/pii/S0925231220309838,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Attention mechanism-based CNN for facial expression
recognition
the features on this page.
Skip to main contentSkip to article
My account
Sign in
* Access through **your organization**
* Purchase PDF
Search 
## Article preview
* Abstract
* Introduction
* Section snippets
* References (44)
* Cited by (263)
## Neurocomputing
Volume 411, 21 October 2020, Pages 340-350
# Attention mechanism-based CNN for facial expression recognition
Author links open overlay panelJing Li a, Kan Jin a, Dalin Zhou b, Naoyuki
Kubota c, Zhaojie Ju b
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.neucom.2020.06.014Get rights and content
## Abstract
Facial expression recognition is a hot research topic and can be applied in
many computer vision fields, such as human?computer interaction, affective
computing and so on. In this paper, we propose a novel end-to-end network with
attention mechanism for automatic facial expression recognition. The new
network architecture consists of four parts, i.e., the feature extraction
module, the attention module, the reconstruction module and the classification
module. The LBP features extract image texture information and then catch the
small movements of the faces, which can improve the network performance.
Attention mechanism can make the neural network pay more attention to useful
features. We combine LBP features and attention mechanism to enhance the
attention model to obtain better results. In addition, we collected and
labelled a new facial expression dataset of seven expressions from 35 subjects
aged from 20 to 25. For each subject, we captured both RGB images and depth
images with a Microsoft Kinect sensor. For each image type, there are 245
image sequences, each of which contains 110 images, resulting in 26,950 images
in total. We apply the newly proposed method to our own dataset and four
representative expression datasets, i.e., JAFFE, CK+, FER2013 and Oulu-CASIA.
The experimental results demonstrate the feasibility and effectiveness of the
proposed method.
## IntroductionFacial expression is one of the most direct signals to express inner feelings
in people's daily communication. The physical or mental state of a person at
one time can be obtained by analyzing facial expressions. Therefore, facial
expression recognition is of great significance in autopilot, human?computer
interaction, medical treatment and other fields related to facial expression,
and has gradually become a more and more important research direction. In
machine learning, a variety of facial expression recognition algorithms have
been proposed. Due to the complexity, diversity, occlusion, lighting and other
challenges in facial expression recognition, the recognition accuracy in
practical applications is still unsatisfactory.
In this paper, our goal is to design a recognition model that can
automatically and accurately recognize different expressions in various types
of images. Generally, the process of facial expression recognition consists of
the following steps: i) pre-processing of the facial expression data; ii)
feature extraction of facial expressions; and iii) classification of facial
expressions. The process is depicted in Fig. 1. We usually consider two kinds
of features, namely, facial features and face model features. The facial
features are specific points on the face, like eyes, mouth, and eyebrows; the
face model features are the features used to model the face. Therefore, there
are several ways for facial representation, like using the whole face to get
the holistic representation, using specific points for local representation,
and combining different points to get a hybrid approach. The final step is to
define some set of categories to which the expression belongs.
When dealing with expression recognition as a classification problem,
traditional methods often use hand-crafted features such as Local Binary
Patterns (LBP) and traditional machine learning algorithms such as Support
Vector Machine (SVM) to classify. These methods may work well on datasets
collected under laboratory conditions, but with the introduction of more
challenging expression datasets in uncontrollable environments (e.g.,
FER2013), they cannot effectively achieve this task. Fortunately, deep
learning has made a breakthrough in convenience and effectiveness since it has
been used to deal with the image classification problem.
The attention mechanism has been widely used in various computer vision tasks
such as saliency detection [15], crowd counting [16] and facial expression
recognition [38]. The operation can select the most useful features for
classification by learning an intermediate attention map and then applying
element-wise product on attention maps and source feature maps to weight the
importance of different features. For the task of facial expression
recognition, the features that are useful for recognition are mainly in some
key parts such as eyes, nose and mouth. The attention mechanism increases the
weights of these key features and helps improve the expression recognition
results.
In this paper, we design a novel Convolutional Neural Network with an
attention model for recognizing facial expressions. In [43], it showed that
using LBP features is better than using HOG and Gabor features because LBP can
achieve rotation invariance and grey-scale invariance and thus is suitable for
extracting texture features at different scales and can solve the imbalance of
displacement, rotation angles and illumination conditions in facial images. In
addition, LBP features can reflect fine facial changes in skin textures like
wrinkles and furrows, which shows the changes of expressions. In [38], an end-
to-end network with an attention model was presented for facial expression
recognition. The attention module makes the network focus more on useful
features which are vital for expression recognition by increasing the weights
of these features. This makes the network recognize expressions moreefficiently. Inspired by [38], [43], we combine LBP features with an attention
model for facial expression recognition. Embedding the attention model into
the network allows the network to pay different attention and weight to
different parts of the input data. This can make the neural network pay more
attention to useful features, which is vital to expression recognition.
Furthermore, we combine LBP features with convolution features to improve our
recognition results. The proposed method has been tested on five facial
expression datasets, which are CK+ [11], JAFFE [13], FER2013 [39], Oulu-CASIA
[25], and our self-collected Nanchang University Facial Expression (NCUFE).
To verify the effectiveness of our algorithm, we collected a new facial
expression dataset called NCUFE. The dataset consists of seven expressions
(i.e., anger, disgust, fear, happiness, sadness, surprise and neutral). We
collected these facial expression images from 35 graduate students (6 females
and 29 males) by a Microsoft Kinect sensor for acquiring both RGB images and
depth images. The sample images are shown in Fig. 2. For each student, the
size of these two types of images is 1280*1024 and 512*424, respectively. For
each image type, there are 245 image sequences, each of which contains 110
images, resulting in 26,950 images in total. During the process of image
capture, the students sat on a chair in front of the Kinect and faced the
camera. The distance between the face and the Kinect was about 100 cm. We
asked each student to look the expression examples printed on some pieces of
paper and then make seven expressions.
The main contributions of this paper are as follows:
* 1)
We introduce a novel facial expression recognition method with attention
mechanism. Not only raw images, but also LBP features are added to the
attention layers of the network. LBP features contain texture information and
can reflect fine facial changes in skin textures, which can help distinguish
expressions with subtle difference.
* 2)
We collected and labelled a new dataset named Nanchang University Facial
Expression (NCUFE) for facial expression recognition. The dataset includes 490
image sequence collected from 35 subjects labeled with seven facial
expressions (i.e., anger, disgust, fear, happiness, sadness, surprise and
neutral). For each subject, we captured both RGB images and depth images.
* 3)
We implement substantial experiments on five different datasets, as shown in
Fig. 3. There are not only datasets collected under laboratory conditions,
such as CK+, JAFFE, Oulu-CASIA and NCUFE, but also those collected in real
world like FER2013. We also compare the model performance with some state-of-
the-art expression recognition algorithms, and the results show that our model
is superior.
The remainder of this paper is as follows. In Section 2, we introduce the
related works in expression recognition and the existing algorithms. In
Section 3, we describe the proposed method in detail. We describe our
experimental process and results on different datasets and compare them with
the results of the state-of-the-art algorithms in Section 4. We give a
conclusion of the whole paper in Section 5.
## Section snippets
## Related works
Traditional facial feature extraction algorithms can be separated into two
categories: 1) geometric-based methods, such as Active Appearance Models (AAM)
[17]; and 2) appearance-based methods, such as LBP [9] and Gabor Wavelet
Representation [21]. After feature description, the features are fed into aclassifier, such as SVM [22] and K-nearest Neighbors (KNN) [24], for
recognizing different facial expressions. Therefore, the performance of the
classifier depends to a large extent on the quality
## Network architecture
In this section, we introduce our newly proposed convolutional neural network
with attention mechanism for automatically recognizing facial expressions. The
new network consists of four parts, i.e., the feature extraction module, the
attention module, the reconstruction module and the classification module. The
architecture starts from the feature extraction module composed of two
separate CNN processing streams: one is for raw images and the other is for
LBP feature maps. Our model uses pure
## Experimental results
In this work, we design a novel deep Convolutional Neural Network with an
attention model to automatically recognizing facial expressions. Except for
the famous facial expression datasets such as CK+ [11], JAFFE [13], Oulu-CASIA
[25] and FER2013 [39], we also evaluate our proposed method on our self-
collected dataset NCUFE. Because CK+, JAFFE, Oulu-CASIA and NCUFE do not
provide specified training and testing sets, we employ 5-fold cross-validation
protocol in these four datasets. The proposed
## Conclusions and future work
This paper presents a novel convolutional neural network with attention
mechanism for facial expression recognition. The method fuses LBP features and
convolution features, and then is combined with attention mechanism to improve
the performance of the network. In order to prevent overfitting and ensure the
generalization ability of the network, we apply data augmentation in the
datasets we used in the experiments. In addition, we collected a new dataset
called Nanchang University Facial
## CRediT authorship contribution statement
**Jing Li:** Conceptualization, Methodology, Writing - original draft. **Kan
Jin:** . **Dalin Zhou:** . **Naoyuki Kubota:** Writing - review & editing.
**Zhaojie Ju:** Conceptualization, Methodology, Writing - original draft.
## Declaration of Competing Interest
The authors declare that they have no known competing financial interests or
personal relationships that could have appeared to influence the work reported
in this paper.
## Acknowledgements
This work is supported by National Natural Science Foundation of China under
Grant 61963027, 61703198, and 51575412, Natural Science Foundation for
Distinguished Young Scholars of Jiangxi Province under Grant 2018ACB21014.
Jing Li received the B.E. degree in Electronic Information Engineering from
Nanchang University, China, in 2005, and obtained her PhD degree in Electronic
and Electrical Engineering from the University of Sheffield, UK, in 2011. From
2011 to 2012, she was a Research Associate with the Department of Computer
Science at the University of Sheffield. Currently, she is an Associate
Professor with the School of Information Engineering at Nanchang University.
Her research interests include visual
Recommended articles"
39,41,Au-aware deep networks for facial expression recognition,"['M Liu', 'S Li', 'S Shan', 'X Chen']",2013,318,MMI Facial Expression,facial expression recognition,"controlled expression database CK+ [21], MMI [22], and a wild  facial expression recognition.  All comparisons are performed on three datasets: CK+ [21], MMI [22], and a wild expression",No DOI,… face and gesture recognition (FG),https://ieeexplore.ieee.org/document/6553734,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
40,42,Au-inspired deep networks for facial expression feature learning,"['M Liu', 'S Li', 'S Shan', 'X Chen']",2015,265,"Acted Facial Expressions In The Wild, MMI Facial Expression","deep learning, neural network","Most existing technologies for facial expression recognition utilize off-the-shelf feature  extraction methods for classification. In this paper, aiming at learning better features specific for",No DOI,Neurocomputing,https://www.sciencedirect.com/science/article/pii/S0925231215001605,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
41,43,Automatic coding of facial expressions displayed during posed and genuine pain,"['GC Littlewort', 'MS Bartlett', 'K Lee']",2009,292,Toronto Face Database,machine learning,"a machine learning approach in a two-stage system. In the first stage, a set of 20 detectors  for facial actions from the Facial Action  Toronto. Mean accuracy of naïve human subjects for",No DOI,Image and Vision Computing,https://www.sciencedirect.com/science/article/pii/S0262885609000055,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
42,44,Automatic facial expression recognition based on a deep convolutional-neural-network structure,"['K Shan', 'J Guo', 'W You', 'D Lu']",2017,159,Japanese Female Facial Expression,"CNN, classification, deep learning, facial expression recognition, machine learning, neural network","in speech recognition, collaborative filtering, handwriting  We employ two standard facial  expression databases for the  10 Japanese women, while CK+ covers the expression images",No DOI,2017 IEEE 15th …,https://ieeexplore.ieee.org/abstract/document/7965717/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
43,45,Automatic facial expression recognition system using deep network-based data fusion,"['A Majumder', 'L Behera']",2016,192,MMI Facial Expression,"classification, classifier, deep learning, facial expression recognition, neural network","High level emotion recognition has been recently reported in the literature, such as  We  observed an average recognition accuracy of 89.3% for MMI DB and 92.54% for CK+ DB. The",No DOI,IEEE transactions on …,https://ieeexplore.ieee.org/document/7747479,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
44,46,Automatic facial expression recognition using DCNN,"['V Mayya', 'RM Pai', 'MMM Pai']",2016,136,Facial Expression Recognition 2013,facial expression recognition,". CNN is extensively used for facial feature extraction for determining age9, gender10 etc.   of facial expressions recognition for the proposed method for six universal expressions is 97%.",No DOI,Procedia Computer Science,https://www.sciencedirect.com/science/article/pii/S1877050916314752,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
45,47,Automatic facial expression recognition using features of salient facial patches,"['SL Happy', 'A Routray']",2014,686,"Affective Faces Database, Facial Expression Recognition 2013",facial expression recognition,of the salient patches on face images. This paper proposes a novel framework for expression  recognition by using appearance features of selected facial patches. A few prominent,No DOI,IEEE transactions on Affective …,https://ieeexplore.ieee.org/document/6998925/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
46,48,Automatic recognition of student engagement using deep learning and facial expression,"['O Mohamad Nezami', 'M Dras', 'L Hamey']",2020,134,Facial Expression Recognition 2013,facial expression recognition,"use the facial expression recognition 2013 (FER-2013) dataset [ 25,109 for training our  facial expression recognition model. To  evaluation of our facial expression recognition model.",No DOI,… on machine learning …,https://link.springer.com/chapter/10.1007/978-3-030-46133-1_17,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
47,49,Automaticity and the amygdala: Nonconscious responses to emotional faces,['A Öhman'],2002,540,Karolinska Directed Emotional Faces,neural network,"when exposed to emotionally expressive faces. Attention is  for responding to negative  emotional faces, and particularly to  emotional faces, but may instead respond to faces because",No DOI,Current directions in psychological science,https://journals.sagepub.com/doi/abs/10.1111/1467-8721.00169,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
48,50,BAUM-1: A spontaneous audio-visual face database of affective and mental states,"['S Zhalehpour', 'O Onder', 'Z Akhtar']",2016,224,Affective Faces Database,"FER, classification, classifier",Most databases available today are acted or do not contain audio data. We present a  -visual  affective face database of affective and mental states. The video clips in the database are,No DOI,… on Affective Computing,https://ieeexplore.ieee.org/document/7451244,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
49,51,Beyond emotion archetypes: Databases for emotion modelling using neural networks,"['R Cowie', 'E Douglas-Cowie', 'C Cox']",2005,179,Affective Faces Database,neural network,"such a database ideally cover quality, emotional content, emotion-related  database of  emotional faces remains for many people the example of what they expect an emotion database",No DOI,Neural networks,https://www.sciencedirect.com/science/article/pii/S0893608005000353,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
50,52,Bilinear models for 3-D face and facial expression recognition,"['I Mpiperis', 'S Malassiotis']",2008,230,Binghamton University 3D Facial Expression,"facial expression recognition, machine learning, neural network",of bilinear models on the Binghamton University 3-D Facial Expression (BU-3DFE)  ’s work  [1] on facial expression recognition and our previous work [2] on face recognition. BU-3DFE is,No DOI,IEEE Transactions on …,https://ieeexplore.ieee.org/abstract/document/4539275/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
51,53,Blended emotion in-the-wild: Multi-label facial expression recognition using crowdsourced annotations and deep locality feature learning,"['S Li', 'W Deng']",2019,127,"Expression in-the-Wild, Static Facial Expression in the Wild","CNN, classification, classifier, deep learning, facial expression recognition, machine learning","multi-label facial expression database, RAF-ML, along with a new deep learning algorithm,  to  To address this limitation, we propose a novel deep learning model, called DBM-CNN, to",No DOI,International Journal of Computer Vision,https://link.springer.com/article/10.1007/s11263-018-1131-1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
52,54,Categorization and evaluation of emotional faces in psychopathic women,"['H Eisenbarth', 'GW Alpers', 'D Segrè', 'A Calogero']",2008,117,Karolinska Directed Emotional Faces,classification,"detailed picture of the emotion decoding deficit through the dimensional rating. Furthermore,  the paradigm combines a quite obvious question on emotion in the valence dimension and",No DOI,Psychiatry …,https://www.sciencedirect.com/science/article/pii/S0165178107003265,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
53,55,Classifying affective states using thermal infrared imaging of the human face,"['BR Nhan', 'T Chau']",2009,221,Affective Faces Database,classifier,"For similar reasons, we also used adjusted accuracy to estimate the performance of the  linear classifier on the unseen test data in the external cross validation. This measure is",No DOI,IEEE Transactions on Biomedical …,https://pubmed.ncbi.nlm.nih.gov/19923040/,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
54,56,Classifying emotions and engagement in online learning based on a single facial expression recognition neural network,"['AV Savchenko', 'LV Savchenko']",2022,201,"Acted Facial Expressions In The Wild, Affective Faces Database, Static Facial Expression in the Wild","classifier, deep learning, machine learning, neural network","our models on EngageWild [8], AFEW (Acted Facial Expression In The Wild) [21] and VGAF  (Video- One of the first techniques that applied machine learning and FER to predict student’s",No DOI,IEEE Transactions on …,https://ieeexplore.ieee.org/document/9815154,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,
55,57,Classifying pretended and evoked facial expressions of positive and negative affective states using infrared measurement of skin temperature,"['MM Khan', 'RD Ward', 'M Ingleby']",2009,116,Affective Faces Database,"classification, classifier","Similarly, our evoked expression classifier (Table VI) could correctly classify 90% of the  sad faces and 70% faces of disgust. The facial expression of anger could not be",No DOI,ACM Transactions on Applied …,https://dl.acm.org/doi/10.1145/1462055.1462061,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
56,58,"Collecting large, richly annotated facial-expression databases from movies","['A Dhall', 'R Goecke', 'S Lucey', 'T Gedeon']",2012,690,Acted Facial Expressions In The Wild,classification,"facial expression database Acted Facial Expressions in the Wild (AFEW) and its static subset  Static Facial Expressions  and classification techniques. However, in the case of human",No DOI,IEEE multimedia,https://ieeexplore.ieee.org/document/6200254,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
57,59,Color face recognition for degraded face images,"['JY Choi', 'YM Ro', 'KN Plataniotis']",2009,140,Toronto Face Database,classification,", ON M5B 2K3, Canada (e-mail: kostas@comm.toronto.edu). Color versions of one or more   classification tasks [22], [25], [26]? To our knowledge, however, the color effect on face",No DOI,IEEE Transactions on …,https://ieeexplore.ieee.org/document/4804691,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
58,60,Color local texture features for color face recognition,"['JY Choi', 'YM Ro', 'KN Plataniotis']",2011,206,Toronto Face Database,classification,be used to enhance classification/recognition performance.  color texture methods for  classification tasks on texture images  information can improve classification performance obtained,No DOI,IEEE transactions on image …,https://ieeexplore.ieee.org/iel5/83/4358840/06020798.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
59,61,Combining modality specific deep neural networks for emotion recognition in video,"['SE Kahou', 'C Pal', 'X Bouthillier', 'P Froumenty']",2013,438,Toronto Face Database,"deep learning, neural network","neural network trained to predict emotions from static frames using two large data sets, the  Toronto Face Database and our own set of faces  profile results with deep learning, here we",No DOI,Proceedings of the 15th …,https://dl.acm.org/doi/10.1145/2522848.2531745,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
60,62,Convolutional neural networks based method for improving facial expression recognition,['TA Rashid'],2016,163,Facial Expression Recognition 2013,facial expression recognition,"In this paper, a technique for recognizing facial expressions using different  classification  to decide among five facial expressions [4]. In [5], in 2010, a recognition system for recognizing",No DOI,Intelligent Systems Technologies and Applications …,https://link.springer.com/chapter/10.1007/978-3-319-47952-1_6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
61,63,Covariance pooling for facial expression recognition,"['D Acharya', 'Z Huang', 'D Pani Paudel']",2018,216,"Affective Faces Database, Static Facial Expression in the Wild",facial expression recognition,methods on riemannian manifold for emotion recognition in the wild. In Proceedings of the   Image based static facial expression recognition with multiple deep network learning. In Pro,No DOI,… pattern recognition …,https://arxiv.org/abs/1805.04855,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
62,64,Decline or improvement?: Age-related differences in facial expression recognition,"['A Suzuki', 'T Hoshino', 'K Shigemasu', 'M Kawamura']",2007,173,Japanese Female Facial Expression,facial expression recognition,"emotion recognition. Currently, there is a growing emphasis on the inseparability between  emotional experiences and emotion recognition, particularly facial expression  the Japanese",No DOI,Biological psychology,https://www.sciencedirect.com/science/article/pii/S0301051106001669,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
63,65,"Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond","['D Kollias', 'P Tzirakis', 'MA Nicolaou']",2019,440,"Affective Faces Database, Expression in-the-Wild","CNN, classification, deep learning, machine learning",", we show that our network can be also used for other emotion  -Face network, pre-trained  for face recognition on the VGG- We note that all deep learning architectures have been",No DOI,International Journal of …,https://arxiv.org/abs/1804.10938,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
64,66,Deep convolution network based emotion analysis towards mental health care,"['Z Fei', 'E Yang', 'DDU Li', 'S Butler', 'W Ijomah', 'X Li', 'H Zhou']",2020,150,Karolinska Directed Emotional Faces,CNN,expression databases including the Karolinska Directed Emotional Faces (KDEF) Database  [25] CNN works as the deep feature extractor. The advantages of using a pre-trained CNN to,No DOI,Neurocomputing,https://www.sciencedirect.com/science/article/pii/S0925231220300783,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
65,67,Deep facial expression recognition: A survey,"['S Li', 'W Deng']",2020,1828,"Acted Facial Expressions In The Wild, Affective Faces Database, Binghamton University 3D Facial Expression, Facial Expression Recognition 2013, Japanese Female Facial Expression, MMI Facial Expression, Radboud Faces Database, Static Facial Expression in the Wild, Toronto Face Database","FER, deep learning, facial expression recognition, neural network","definition of facial expressions. In this survey, we limit our discussion on FER based on the  cat Pantic, “Induced disgust, happiness and surprise: an addition to the mmi facial expression",No DOI,IEEE transactions on affective computing,https://arxiv.org/abs/1804.08348,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
66,68,Deep imbalanced learning for face recognition and attribute prediction,"['C Huang', 'Y Li', 'CC Loy', 'X Tang']",2019,390,Toronto Face Database,"deep learning, machine learning","To mitigate this issue, contemporary deep learning methods typically follow classic strategies  such as class re- Given an imagery dataset with imbalanced class distribution, our goal is to",No DOI,… pattern analysis and machine …,https://arxiv.org/abs/1806.00194,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
67,69,Deep joint spatiotemporal network (DJSTN) for efficient facial expression recognition,"['D Jeong', 'BG Kim', 'SY Dong']",2020,105,MMI Facial Expression,"deep learning, facial expression recognition, neural network",facial emotion recognition systems with various signals. Most of them employ facial expression  recognition  : An addition to the mmi facial expression database. In Proceedings of the 3rd,No DOI,Sensors,https://www.mdpi.com/1424-8220/20/7/1936,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
68,70,Deep learning approaches for facial emotion recognition: A case study on FER-2013,"['P Giannopoulos', 'I Perikos', 'I Hatzilygeroudis']",2018,206,"Affective Faces Database, Facial Expression Recognition 2013","FER, facial expression recognition","the emotional behavior and the affective state of the human.  , for images of the Jaffe face  database with little noise and with  /non smiling expressions in the ORL database. In [32], a",No DOI,Advances in hybridization of …,https://link.springer.com/chapter/10.1007/978-3-319-66790-4_1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
69,71,Deep learning based facs action unit occurrence and intensity estimation,"['A Gudi', 'HE Tasli', 'TM Den Uyl']",2015,221,Toronto Face Database,deep learning,method on the SEMAINE dataset was much lower than on the BP4D dataset. One of the main  contributing factors to this observation is the low number of individual faces included in the,No DOI,… on automatic face and …,https://ieeexplore.ieee.org/document/7284873,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
70,72,Deep learning for biometrics: A survey,"['K Sundararajan', 'DL Woodard']",2018,314,Toronto Face Database,deep learning,"with the SVS2004 dataset, it can be observed that both deep learning and other approaches  seem to perform comparably. This could be attributed to smaller dataset size. However, for",No DOI,ACM Computing Surveys (CSUR),https://dl.acm.org/doi/10.1145/3190618,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
71,73,Deep learning for emotion recognition on small datasets using transfer learning,"['HW Ng', 'VD Nguyen', 'V Vonikakis']",2015,802,Affective Faces Database,deep learning,"dataset, when a sufficiently large face dataset such as FER-2013 is available, whether by  adding it to the auxiliary dataset  exploit deep neural networks such as CNN for face expression",No DOI,Proceedings of the 2015 …,https://dl.acm.org/doi/10.1145/2818346.2830593,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
72,74,Deep learning for human affect recognition: Insights and new developments,"['PV Rouast', 'MTP Adam', 'R Chiong']",2019,613,"Affective Faces Database, Toronto Face Database",deep learning,"deep neural networks, we comprehensively quantify their applications in this field. We find  that deep learning is used for learning  number of examples per dataset does not see a clear",No DOI,IEEE Transactions on …,https://arxiv.org/abs/1901.02884,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
73,75,Deep learning for real-time robust facial expression recognition on a smartphone,"['I Song', 'HJ Kim', 'PB Jeon']",2014,151,Facial Expression Recognition 2013,facial expression recognition,"In Section II, we outline the structure of the deep architecture and describe how to  face  datasets. In Section IV, we describe how to implement a real-time facial expression recognition",No DOI,2014 IEEE International Conference …,https://ieeexplore.ieee.org/document/6776135,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
74,76,Deep neural network augmentation: Generating faces for affect analysis,"['D Kollias', 'S Cheng', 'E Ververas', 'I Kotsia']",2020,135,"Affective Faces Database, Radboud Faces Database","CNN, classification, deep learning, neural network",train Deep Neural Networks over eight databases face database is annotated in terms of  valence and arousal and is then used for affect synthesis. The fact that this a temporal database,No DOI,International Journal of …,https://arxiv.org/abs/1811.05027,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
75,77,Deep neural networks with relativity learning for facial expression recognition,"['Y Guo', 'D Tao', 'J Yu', 'H Xiong', 'Y Li']",2016,127,"Acted Facial Expressions In The Wild, Facial Expression Recognition 2013","deep learning, facial expression recognition, neural network",Here we present a deep learning method termed Deep Neural Networks with Relativity   The SFEW 2.0 dataset was created from Acted Facial Expressions in the Wild (AFEW) [3] using,No DOI,2016 IEEE International …,https://ieeexplore.ieee.org/document/7574736,True,False,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
76,78,Deep spatial-temporal feature fusion for facial expression recognition in static images,"['N Sun', 'Q Li', 'R Huan', 'J Liu', 'G Han']",2019,116,MMI Facial Expression,"deep learning, facial expression recognition, neural network",MMI database holds 2885 videos and over 500 images of 88 subjects displaying various  facial expressions  output of the channel to recognize the facial expression. For MDSTFN with,No DOI,Pattern Recognition Letters,https://www.sciencedirect.com/science/article/pii/S0167865517303902,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
77,79,Deep-emotion: Facial expression recognition using attentional convolutional network,"['S Minaee', 'M Minaei', 'A Abdolrashidi']",2021,664,"Affective Faces Database, Extended Cohn-Kanade, Facial Expression Recognition 2013, Japanese Female Facial Expression, Toronto Face Database","FER, deep learning, facial expression recognition","FER dataset, and the images in the second and fourth rows belong to the extended Cohn-Kanade   images in the second and fourth rows belong to the extended Cohn-Kanade dataset.",No DOI,Sensors,https://www.mdpi.com/1424-8220/21/9/3046,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,
78,80,Deeply learning deformable facial action parts model for dynamic expression analysis,"['M Liu', 'S Li', 'S Shan', 'R Wang', 'X Chen']",2015,389,MMI Facial Expression,CNN,"evaluated on two posed expression datasets, CK+, MMI, and a  is how to represent different  facial expressions. In the past  is applying CNN or 3D CNN directly to expression analysis,",No DOI,Computer Vision--ACCV 2014: 12th …,https://link.springer.com/chapter/10.1007/978-3-319-16817-3_10,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
79,81,Development of a real-time emotion recognition system using facial expressions and EEG based on machine learning and deep neural network methods,"['A Hassouneh', 'AM Mutawa', 'M Murugappan']",2020,239,Affective Faces Database,machine learning,A convolutional neural network was used in our system to obtain improved facial emotion  detection as it is applied to other computer fields such as face recognition [25] and object,No DOI,Informatics in Medicine …,https://www.sciencedirect.com/science/article/pii/S235291482030201X,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
80,82,Dexpression: Deep convolutional neural network for expression recognition,"['P Burkert', 'F Trier', 'MZ Afzal', 'A Dengel']",2015,192,"Extended Cohn-Kanade, Facial Expression Recognition 2013, MMI Facial Expression","CNN, classification, deep learning, facial expression recognition, neural network",of different image classification approaches submitted by  evaluated on the Extended  Cohn-Kanade Dataset (Section 4.2 [12] have created the Extended CohnKanade dataset. This,No DOI,arXiv preprint arXiv …,https://arxiv.org/abs/1509.05371,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
81,83,Dfew: A large-scale database for recognizing dynamic facial expressions in the wild,"['X Jiang', 'Y Zong', 'W Zheng', 'C Tang', 'W Xia']",2020,126,"Acted Facial Expressions In The Wild, Affective Faces Database, Expression in-the-Wild, Static Facial Expression in the Wild","CNN, FER, classification, classifier, deep learning, facial expression recognition, machine learning, neural network","Recently, facial expression recognition (FER) in the wild has gained a lot of researchers’   a dynamic facial expression in the wild database, ie, acted facial expressions in the wild (AFEW)",No DOI,Proceedings of the 28th …,https://arxiv.org/abs/2008.05924,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
82,84,Diagnostic features of emotional expressions are processed preferentially,"['E Scheller', 'C Büchel', 'M Gamer']",2012,135,Karolinska Directed Emotional Faces,classifier,control group in an emotion classification task revealed a  faces that were shown during  the experiment were selected from several picture sets (The Karolinska directed emotional faces,No DOI,PloS one,https://pubmed.ncbi.nlm.nih.gov/22848607/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
83,85,Discovering hidden factors of variation in deep networks,"['B Cheung', 'JA Livezey', 'AK Bansal']",2014,225,Toronto Face Database,"classification, deep learning, machine learning, neural network","Deep learning has enjoyed a great deal of success because of its ability to  handwritten  digit database, the Toronto Faces Database (TFD) and the Multi-PIE dataset by generating",No DOI,arXiv preprint arXiv …,https://arxiv.org/abs/1412.6583,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
84,86,Disentangling factors of variation for facial expression recognition,"['S Rifai', 'Y Bengio', 'A Courville', 'P Vincent']",2012,264,Toronto Face Database,"classification, classifier, deep learning, facial expression recognition, machine learning, neural network","This system beats the state-of-the-art on a recently proposed dataset for facial expression  recognition, the Toronto Face Database, moving the state-of-art accuracy from 82.4% to 85.0%",No DOI,Computer Vision–ECCV …,https://link.springer.com/chapter/10.1007/978-3-642-33783-3_58,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
85,87,Disgust-specific impairment of facial expression recognition in Parkinson's disease,"['A Suzuki', 'T Hoshino', 'K Shigemasu', 'M Kawamura']",2006,259,Japanese Female Facial Expression,facial expression recognition,impair the recognition of facial expressions of disgust; this provides concrete evidence for  emotion- (eight photographs for each emotion; Japanese and Caucasian facial expressions of,No DOI,Brain,https://pubmed.ncbi.nlm.nih.gov/16415306/,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
86,88,Do deep neural networks learn facial action units when doing expression recognition?,"['P Khorrami', 'T Paine', 'T Huang']",2015,360,"Acted Facial Expressions In The Wild, Extended Cohn-Kanade, Toronto Face Database","CNN, classification, classifier, deep learning, facial expression recognition, machine learning, neural network","Cohn-Kanade (CK+) dataset [18], and the Multi-PIE dataset [10]. However, the recent success  of deep neural networks  in our experiments: the extended Cohn-Kanade database (CK+) [",No DOI,Proceedings of the IEEE …,http://arxiv.org/abs/1510.02969,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,
87,89,Driver's facial expression recognition in real-time for safe driving,"['M Jeong', 'BC Ko']",2018,158,MMI Facial Expression,"FER, classification, classifier, deep learning, facial expression recognition, machine learning",": An addition to the mmi facial expression database. In Proceedings of the 3rd International  Conference on Language Resources and Evaluation Workshop on EMOTION, Valletta, Malta",No DOI,Sensors,https://www.mdpi.com/1424-8220/18/12/4270,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
88,90,Dynamic facial expression recognition using longitudinal facial expression atlases,"['Y Guo', 'G Zhao', 'M Pietikäinen']",2012,116,MMI Facial Expression,facial expression recognition,"facial expression atlases of each expression, we are able to recognize expression of a new  facial expression  In this section, we evaluate the proposed method on the MMI database [24]",No DOI,"… Computer Vision, Florence, Italy, October 7 …",https://link.springer.com/chapter/10.1007/978-3-642-33709-3_45,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
89,91,EMPATH: A neural network that categorizes facial expressions,"['MN Dailey', 'GW Cottrell', 'C Padgett']",2002,349,Affective Faces Database,neural network,to which each face portrayed each basic emotion on a 1–5  rating’’ vectors for each face pair  as a measure of dissimilarity.  networks respond with emotion i when the intended emotion,No DOI,Journal of cognitive …,https://pubmed.ncbi.nlm.nih.gov/12495523/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
90,92,Early cortical processing of natural and artificial emotional faces differs between lower and higher socially anxious persons,"['A Mühlberger', 'MJ Wieser', 'MJ Herrmann']",2009,276,Karolinska Directed Emotional Faces,classification,", emotional faces should  emotional faces regarding their processing as reflected by ERP  components. On the one hand, natural faces contain a lot more information than the emotion of",No DOI,Journal of neural …,https://pubmed.ncbi.nlm.nih.gov/18784899/,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
91,93,Emonets: Multimodal deep learning approaches for emotion recognition in video,"['SE Kahou', 'X Bouthillier', 'P Lamblin', 'C Gulcehre']",2016,510,Toronto Face Database,deep learning,"learning several specialist models using deep learning techniques, each focusing on one  modality. Among these are a convolutional neural network,  is the Toronto Face Dataset (TFD) [",No DOI,Journal on Multimodal …,https://arxiv.org/abs/1503.01800,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
92,94,Emotion distribution recognition from facial expressions,"['Y Zhou', 'H Xue', 'X Geng']",2015,161,Toronto Face Database,"FER, facial expression recognition",facial expression recognition methods assume the availability of a single emotion for each  expression in  The s-JAFFE database contains 213 facial expression images. Each image was,No DOI,Proceedings of the 23rd ACM international …,https://dl.acm.org/doi/10.1145/2733373.2806328,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
93,95,Emotion recognition from facial expression using deep convolutional neural network,['DY Liliana'],2019,116,"Extended Cohn-Kanade, Facial Expression Recognition 2013","CNN, classification, deep learning, facial expression recognition, neural network",This paper extends the deep Convolutional Neural Network ( This research uses the  extended Cohn Kanade (CK+) dataset  In this paper we contribute a Deep Learning approaches,No DOI,Journal of physics: conference series,https://iopscience.iop.org/article/10.1088/1742-6596/1193/1/012004,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
94,96,Emotion recognition in the wild challenge 2013,"['A Dhall', 'R Goecke', 'J Joshi', 'M Wagner']",2013,241,"Acted Facial Expressions In The Wild, Facial Expression Recognition 2013","classification, classifier, facial expression recognition",emotion recognition methods in real-world conditions. The database in the 2013 challenge is  the Acted Facial Expression in the Wild  ] and Static Facial Expressions In The Wild (SFEW) [,No DOI,Proceedings of the 15th …,https://dl.acm.org/doi/10.1145/2522848.2531739,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
95,97,"Emotion recognition in the wild challenge 2014: Baseline, data and protocol","['A Dhall', 'R Goecke', 'J Joshi', 'K Sikka']",2014,269,"Acted Facial Expressions In The Wild, Expression in-the-Wild","classification, classifier, facial expression recognition, machine learning",The Second Emotion Recognition In The Wild Challenge 2014 provides a platform for   with their emotion recognition method on the Acted Facial Expressions In The Wild database.,No DOI,Proceedings of the 16th …,https://dl.acm.org/doi/10.1145/2663204.2666275,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
96,98,Emotion recognition in the wild via convolutional neural networks and mapped binary patterns,"['G Levi', 'T Hassner']",2015,413,"Acted Facial Expressions In The Wild, Expression in-the-Wild, Static Facial Expression in the Wild",neural network,"version 2.0 of the Static Facial Expression in the Wild benchmark [9]. It was assembled by  selecting frames from different videos of the Acted Facial Expressions in the Wild (AFEW), and",No DOI,Proceedings of the 2015 ACM on international …,https://dl.acm.org/doi/10.1145/2818346.2830587,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
97,99,Emotion recognition of affective speech based on multiple classifiers using acoustic-prosodic information and semantic labels,"['CH Wu', 'WB Liang']",2010,351,Affective Faces Database,classifier,to emotion recognition of affective speech using multiple classifiers with AP and SLs. The   using multiple classifiers and the MDT was used to select an appropriate classifier to output the,No DOI,IEEE Transactions on Affective Computing,https://ieeexplore.ieee.org/document/5674019,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
98,100,Emotion recognition using deep learning approach from audio–visual emotional big data,"['MS Hossain', 'G Muhammad']",2019,442,Affective Faces Database,deep learning,"The proposed system is evaluated using two audio–visual emotional databases, one of   The histograms are obtained from the cropped face images. If there was no face detected in a",No DOI,Information Fusion,https://www.sciencedirect.com/science/article/pii/S1566253517307066,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
99,101,Emotion recognition using facial expressions,"['P Tarnowski', 'M Kołodziej', 'A Majkowski']",2017,360,Facial Expression Recognition 2013,facial expression recognition,"For all users, we obtained classification accuracy of emotions  classification accuracy was  73% (for MLP classifier). In the same case, for the 3-NN classifier we obtained a classification",No DOI,Procedia Computer …,https://www.sciencedirect.com/science/article/pii/S1877050917305264,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
100,102,"Emotional facial expressions evoke faster orienting responses, but weaker emotional responses at neural and behavioural levels compared to scenes: A …","['A Mavratzakis', 'C Herbert', 'P Walla']",2016,118,Karolinska Directed Emotional Faces,neural network,"how affective neural activity during emotional face and scene perceptions translates into   of the analysis was to investigate differences in emotional face and scene processing, only the",No DOI,Neuroimage,https://www.sciencedirect.com/science/article/pii/S1053811915008873,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Emotional facial expressions evoke faster orienting
responses, but weaker emotional responses at neural and
behavioural levels compared to scenes: A simultaneous EEG and
facial EMG study
the features on this page.
Skip to main contentSkip to article
My account
Sign in
Search 
## Outline
1. Highlights
2. Abstract
3. 4. Keywords
5. Introduction
6. Methods
7. Results
8. Discussion
9. Acknowledgments
10. References
Show full outline
## Cited by (67)
## Figures (7)
1. 2. 3. 4. 5. 6.
Show 1 more figure
## Tables (4)
1. Table 1
2. Table 2
3. Table 3
4. Table 4
## NeuroImage
Volume 124, Part A, 1 January 2016, Pages 931-946
# Emotional facial expressions evoke faster orienting responses, but weaker
emotional responses at neural and behavioural levels compared to scenes: A
simultaneous EEG and facial EMG study
Author links open overlay panelAimee Mavratzakis a b, Cornelia Herbert c d,
Peter Walla a b e f
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.neuroimage.2015.09.065Get rights and contentUnder a Creative Commons license
open access
## Highlights
* ?
Faces and scenes elicit different emotion-related brain activities.
* ?
Faces and scenes elicit different facial expression-related muscle activities.
* ?
Faces and scenes are shown to be different emotion elicitors.
## Abstract
In the current study, electroencephalography (EEG) was recorded simultaneously
with facial electromyography (fEMG) to determine whether emotional faces and
emotional scenes are processed differently at the neural level. In addition,
it was investigated whether these differences can be observed at the
behavioural level via spontaneous facial muscle activity. Emotional content of
the stimuli did not affect early P1 activity. Emotional faces elicited
enhanced amplitudes of the face-sensitive N170 component, while its
counterpart, the scene-related N100, was not sensitive to emotional content of
scenes. At 220?280 ms, the early posterior negativity (EPN) was enhanced only
slightly for fearful as compared to neutral or happy faces. However, its
amplitudes were significantly enhanced during processing of scenes with
positive content, particularly over the right hemisphere. Scenes of positive
content also elicited enhanced spontaneous zygomatic activity from 500?750 ms
onwards, while happy faces elicited no such changes. Contrastingly, both
fearful faces and negative scenes elicited enhanced spontaneous corrugator
activity at 500?750 ms after stimulus onset. However, relative to baseline EMG
changes occurred earlier for faces (250 ms) than for scenes (500 ms) whereas
for scenes activity changes were more pronounced over the whole viewing
period. Taking into account all effects, the data suggests that emotional
facial expressions evoke faster attentional orienting, but weaker affective
neural activity and emotional behavioural responses compared to emotional
scenes.
* Previous article in issue
* Next article in issue
## Keywords
Emotion
Affective processing
Faces and scenes
Electroencephalography
Spontaneous facial EMG
N170
N100
Early posterior negativity
## Introduction
In emotion research two kinds of stimuli are frequently used: facial
expressions (e.g. a smiling or sad face) and emotionally evocative scenes
(e.g. snakes, erotic pictures). But, do these forms of emotional stimuli
undergo the same neural processing? Despite each being intrinsically
emotionally evocative, only a few studies exist that have compared affective
processing of faces and scenes in the same experiment and context.
A recent meta-analysis comparing 157 functional Magnetic Resonance Imaging
(fMRI) studies that used either emotional faces or emotional scenes
(Sabatinelli et al., 2011) revealed multiple clusters of brain activations
unique to these different forms of stimuli even after the subtraction ofneural activity related to basic visual processing. Although this suggests
that both types of stimuli might be processed differently in the brain, direct
comparisons of the time course of affective processing for faces and scenes
are lacking and little is known about whether both stimulus classes elicit
similar expressive behavioural reactions.
In other contexts it is obvious that faces and scenes are indeed quite
different. For example, facial expressions elicit mimicry and facial feedback
mechanisms might modify emotion-related processing (see Niedenthal et al.,
2001). Facial expressions can be understood as interpersonal, facilitating
social transactions, and require complex neural processing to translate these
emotional cues into social meaning. Emotional scenes on the other hand are
more intrapersonal and directly elicit motivational behaviours without needing
to translate their meaning beyond knowing whether to approach or avoid.
Different facial expressions are more similar to each other than different
scene pictures are. Various processing differences between face and scene
stimuli have been described. Hariri et al. (2002) found varying amygdala
activity depending on whether a fearful face or a threatening scene was
presented to their participants. In their fMRI study, Keightley et al. (2011)
reported about their conclusion that the contextual information in emotional
scenes may facilitate memory via additional visual processing, whereas memory
for emotional faces may rely more on cognitive control mediated by
rostrolateral prefrontal regions. Epstein et al. (2006) investigated
differences between face and scene inversion. Their results demonstrate that
both face and scene inversion cause a shift from specialised processing
streams towards generic object-processing mechanisms, but this shift only
leads to a reliable behavioural deficit in the case of face inversion.
The temporal characteristics of neural affective processing have been
relatively well documented for emotional faces (Vuilleumier and Pourtois,
2007, Wieser and Brosch, 2012) and emotional scenes (Olofsson et al., 2008,
Schupp et al., 2006a). Anatomically, visual information passes through the
extrastriate visual cortex where low-lying physical stimulus properties such
as luminance and spatial complexity determine which aspects of visual
information receive rapid attentional capture and further processing (Clark
and Hillyard, 1996, Givre et al., 1994, Hillyard and Anllo-Vento, 1998, Mangun
et al., 1993, Rugg et al., 1987). This rapid-attentional capture is seen in
scalp-recorded potentials as a prominent positively charged deflection in
amplitude over lateral posterior occipital sites at approximately 100 ms post-
stimulus (termed the P100 component, or P1 to represent the first positive
peak in neural activity), where the size of the amplitude deflection indexes
the degree of attentional capture of the related stimulus. Attended-to
information then undergoes object recognition processing in neural circuits
proceeding through bilateral ventral?lateral streams from the visual cortex
into the temporal cortices (Allison et al., 1999). Here, the fusiform gyrus, a
well-studied structure located in the inferior temporal lobes, facilitates
face recognition via a highly specialised process of collating local facial
features into a holistic global face representation (Rossion et al., 2003).
This activity is observed in scalp-recorded potentials as a strong negatively
charged deflection in amplitude over lateral temporal?occipital areas
approximately 170 ms post-stimulus onset (Bentin et al., 1996, Deffke et al.,
2007), hence the name N170. Other stimuli such as complex scenes also undergo
category-specific processing across more widely distributed hierarchically
organised circuits in the ventral?lateral streams, with this activity being
observed as a more modest negative deflection in amplitude at around 150?200
ms after stimulus onset over lateral temporal-occipital scalp locations(termed the N100). From here, it has been posited that affective information
of faces and scenes begins to influence neural activity, seen at
lateral?occipital scalp recordings as a more stable negative shift in polarity
when viewing emotionally-evocative relative to neutral stimuli. This posterior
negativity (i.e. the early posterior negativity or EPN) typically emerges at
the offset of the N100/N170, around 200 to 250 ms post-stimulus and has been
found to be modulated as a function of increased attentional allocation and
greater motivational relevance of the emotionally evocative stimuli (Bublatzky
and Schupp, 2011, Foti et al., 2009, Schupp et al., 2006a, Weinberg and
Hajcak, 2010).
There is however evidence that affective information can influence activity at
earlier stages of processing relative to the EPN. Several studies have
reported larger N170 amplitudes when viewing negatively-valanced facial
expressions such as fear and anger (e.g. Batty and Taylor, 2003, Leppänen et
al., 2008, Pourtois et al., 2005, Stekelenburg and de Gelder, 2004), which has
been interpreted as an innate attentional ?negativity bias? (Carretie et al.,
2009, Holmes et al., 2005). The same controversy exists for emotional scenes,
with some studies reporting a negativity bias for highly unpleasant
threatening or fearful scenes in the time window of the N100. Affective
modulation has even been reported as early as 100 ms post-stimulus (Batty and
Taylor, 2003, Eger et al., 2003, Eimer and Holmes, 2002, Holmes et al., 2003,
Pizzagalli et al., 1999, Pourtois et al., 2005, Recio et al., 2014, Smith et
al., 2013, Streit et al., 2003). Differences as well as similarities in
affective stimulus processing may be better understood by directly comparing
when these processes occur for emotional faces and scenes in a single
experimental framework. This would also allow a direct comparison of
behavioural reactions elicited by faces and scenes.
In the current study, we were interested in investigating how affective neural
activity during emotional face and scene perceptions translates into emotional
behaviour, building on the idea that emotional behaviour should be understood
as a consequence of subcortical affective neural activity (Walla and Panksepp,
2013). Spontaneous facial muscle activity is an emotion-related behavioural
phenomenon that is thought to play a crucial role in social emotion
recognition, whereby perceiving an emotional facial expression elicits a rapid
or spontaneous micro-simulation of the perceived facial expression by the
perceiver less than 1000 ms post onset (Achaibou et al., 2008, Dimberg et al.,
2000, Grèzes et al., 2013, Korb et al., 2010, Moody et al., 2007). By
utilising the excellent temporal resolution offered by electromyography to
measure facial muscle activity (fEMG), these studies have shown that
zygomaticus major ?cheek? muscles rapidly and spontaneously contract in
response to smiling faces while corrugator supercilii ?eyebrow? muscles
rapidly and spontaneously contract in response to angry or fearful faces. The
phenomenon is thought to facilitate emotion recognition by triggering the
reactivation of specific neural regions that are involved in producing that
same emotion in the perceiver, leading to a realisation of the other person's
emotional state (e.g. Barsalou, 2003, Barsalou et al., 2003, Clark et al.,
2008, Niedenthal, 2007). Moreover, empirical evidence suggests that
spontaneous facial reactions play a causal role in emotion recognition whereby
selectively preventing movement in facial muscle/s required to simulate an
expression leads to poor recognition ability for that facial expression in
another person (Foroni and Semin, 2011, Niedenthal et al., 2001, Oberman et
al., 2007, Ponari et al., 2012).
However, emotional scenes have also been shown to evoke spontaneous facial
reactions (Dimberg et al., 1998). In contrast to faces emotional scenes oftendo not contain any third-party emotion to recognise. This raises the question
of whether and in what ways spontaneous facial muscle activity may differ when
elicited by emotional faces compared to scenes, such as in latency or strength
of the response. To this extent, the objective of the current study was to
investigate differences in emotional responses evoked by happy, fearful and
neutral faces versus positive, neutral and negative scenes: (1) during early
visually-evoked stages of neural activity including the P1, N100/N170, and
EPN; and (2) in spontaneous zygomatic and corrugator facial reactions; and (3)
to examine correlations between affective neural activity and emotional
behaviour.
When considering motivationally-relevant emotion processing, arousal must be
taken into consideration, because stimuli that evoke heightened arousal have
been shown to modulate both neural and facial muscle activity independent of
emotional valence or stimulus type (Cacioppo et al., 1986, Cuthbert et al.,
2000, Feng et al., 2014, Lang et al., 1993). For this reason the face and
scene stimuli chosen for the current study were relatively low-arousing (see
Fig. 1 bottom right graph). However, it was still possible that face and scene
stimuli could evoke different degrees of arousal. Therefore, arousal responses
to pictures were also recorded via the skin conductance response (SCR), a
neurophysiological measure of sweat gland activity which is controlled by the
sympathetic part of the autonomic nervous system. SCRs could therefore be used
to differentiate neural and behavioural effects associated with enhanced
levels of arousal from those associated with emotional valence or stimulus
type.
1. Download: Download high-res image (193KB)
2. Download: Download full-size image
Fig. 1. Mean luminance and spatial frequency values (top) and pre-evaluated
pleasantness (bottom left) and arousal (bottom right) ratings for the final
collection of stimuli. Pleasantness was rated on a scale of 1 (very
unpleasant) to 9 (very pleasant). Arousal was rated on a scale of 1 (very
calm) to 9 (very arousing). Error bars represent one standard error of the
mean.* = The differences are significant at .05 alpha level.
A secondary aim of this study was to examine whether or not early emotion
processing is influenced by the depth of conceptual emotion processing, and,
if so, whether such effects might suppress or enhance spontaneous facial
reactions. Traditionally, the delayed match-to-sample task involves the
?passive? presentation of a first stimulus (e.g. an emotional facial
expression) followed by an ?active? presentation of a second stimulus, at
which point some judgement must be made regarding the second stimulus as a
function of the first, usually whether or not they express the same type of
emotion. In the current study, we varied the semantic format of emotion
recognition between three ?delayed match-to-sample? emotion-matching tasks. A
consistent presentation format was always used for the first emotional
stimulus (i.e. always an emotional picture) while for the second emotional
stimulus, the presentation format was varied across tasks to be either another
emotional picture, an emotional word or to freely label the depicted emotion
(see Fig. 2 for examples of each task). Hence, one version of the task was to
compare an emotional picture with another emotional picture; the second
version was to compare an emotional picture with an emotional word; and the
third task had no second emotional stimulus to compare the first emotional
picture with, instead participants had to freely label the depicted emotion.
1. Download: Download high-res image (389KB)
2. Download: Download full-size image
Fig. 2. An example of the trial structure for each emotion recognition task.The top figure illustrates examples for the versions using emotional faces,
and the bottom figure illustrates the same trial examples, but using emotional
scenes. From the left to right of each figure is an example of the
Picture?Picture matching task; the Picture?Word matching; and the Picture-
Labelling task. Notice that each task begins exactly the same, with a fixation
cross followed by a passively viewed picture (Stimulus 1; a happy, neutral or
fearful picture (face or scene, depending on the task version)). After this,
the trial structure changed according to type of task being completed. Neural
and facial muscle activity during the 3000 ms time window corresponding to
Stimulus 1 was analysed in this study to determine whether processing of
emotional information is differently influenced by the way that information
needs to be used.
Most research using the match-to-sample paradigm focuses on neural or
behavioural activity associated with the second ?active? stimulus (e.g. Hirai
et al., 2008, Narumoto et al., 2001). However the focus of the current study
was neural and behavioural activity associated with the first passively viewed
stimulus. This design specifically allowed us to examine whether, when
emotional pictures are viewed under exactly the same presentation conditions,
does passive emotion processing and responding vary as a function of the
semantic level of emotion recognition? Due to unresolved muscle-related
artefact issues in the picture-labelling task, we here focus on EEG and fEMG
effects associated with the ?picture?picture matching? and ?picture?word
matching? tasks.
## Methods
### Participants
Participants were 27 undergraduate students enrolled at the University of
Newcastle. Data of four participants were excluded from the analysis due to
technical issues with the EMG and skin conductance recording equipment (two
females and one male) and too few remaining EEG trials after artefact removal
(one female). The mean age of the remaining 23 participants is 21 years (SD =
1.72) (17 females). Participants were native speakers of English, right-
handed, non-smokers, had no known history of neuropathology and were not
taking central nervous system targeted medication such as antidepressants or
stimulants at the time of testing. Participants provided written informed
consent and the project was approved by the University of Newcastle Human
Research Ethics Committee [H-2012-0229].
### Stimuli
The 270 happy, fearful and neutral face stimuli were taken from the Radboud
Faces Database (RAFD; Langner et al., 2010) and Set A of the Karolinska
Directed Emotional Faces Database (KDEF; Lundqvist et al., 1998). For face
stimuli, each face was cropped to remove hair, ears, clothing etc. from the
image, leaving only the necessary elements of the face for distinguishing an
emotional expression. The 270 positive, negative and neutral scene stimuli
were taken from the Geneva Affective Picture Database (GAPED; Dan-Glauser and
Scherer, 2011) and the International Affective Picture System (IAPS; Lang et
al., 2005). Positive scenes included nature scenes, baby animals, appetising
food and erotic scenes depicting a male and female embrace. Negative scenes
were specifically selected based on a study by Mikels et al. (2005), which
categorised IAPS stimuli into discrete emotion categories including fear.
Obvious thematic characteristics of the discrete IAPS fear collection, such as
spiders and snakes, were then used as a basis for selecting negative (mainly
fearful) scenes from the GAPED database, for which no discrete emotional
categorisation exists. For neutral scenes, we specifically chose stimuli that
visually represented neutrality (e.g. a stair case, computer, light bulb),because valence ratings are not accurate predictors of emotional
categorisation (Blairy et al., 1999). Scene stimuli featuring a forward-facing
face were excluded from the scenes collection.
The stimuli collections were rated on levels of valence and arousal in a pilot
study of a larger pool of images using an independent group of 42 participants
(23 females) with a mean age of 25 years (_SD_ = 4.61). Participants rated
equal samples of face and scene stimuli using the Self-assessment manikin
(SAM; Bradley and Lang, 1994). Pictures with ratings that best balanced
valence and arousal levels across stimulus categories were then chosen to be
included in the study. Mean valence and arousal ratings in addition to
luminance and spatial frequency values for the final collection are displayed
in Fig. 1.
There is existing empirical evidence showing that low level physical features
of visual stimuli such as luminance and spatial frequency modify early brain
activities (e.g. Alorda et al., 2007, De Cesarei and Codispoti, 2012,
Delplanque et al., 2007), a phenomenon also known as exogenous brain activity
effects (Donchin, 1978). Since our motivation was focused on early brain
activity effects related to emotion-specific content of faces versus scenes it
is important to look at physical features of all visual stimuli, in particular
luminance and spatial frequency and to test whether or not potential
differences in luminance and spatial frequency across stimuli could
theoretically explain any early brain activity differences that are described
in the frame of this paper.
For this purpose we ran a spatial frequency analysis of all our images and
calculated analytic statistics to test whether or not spatial frequencies
differed between stimulus categories (faces and scenes) and also between
emotion categories. An ANOVA including all spatial frequency values was run
and revealed a highly significant main stimulus category effect (p < .001) and
a significant main emotion category effect (p = .009). However, the
interaction of both factors was not significant (p = .097) (all
Greenhouse?Geisser corrected). The pattern of these results is understood as
demonstrating that mean spatial frequencies of our images differ between faces
and scenes. In addition, spatial frequencies of our images differ as a
function of emotion category, but the way they differ between emotion
categories does not depend on stimulus category. Descriptive statistics shows
that scenes had overall higher spatial frequencies than faces (reflected in
the main stimulus category effect), which we interpret as a result of higher
complexity of scenes compared to faces. In both stimulus categories it can be
seen that neutral images are associated with lower spatial frequency values
compared to both positive and negative emotion categories (reflected in the
main emotion category effect and the not significant interaction of both
factors).
Further, t-tests revealed significant differences between spatial frequencies
of neutral and negative faces (p = .010; T = 2.618), also neutral and positive
faces (p = .047; T = ? 2.009), but not negative and positive faces (p = .557;
T = .590). Spatial frequencies of negative scenes don't differ from those of
neutral scenes (p = .312; T = 1.017), but they do differ between neutral and
positive scenes (p = .009; T = ? 2.679). No differences are found between
positive and negative scenes (p = .107; T = ? 1.626).
In summary, positive and negative stimuli are associated with higher spatial
frequencies than neutral stimuli, which is true for both stimulus categories.
In general, scenes are associated with higher spatial frequencies compared to
faces (see Fig. 1).
It also turned out that luminance differences exist. An ANOVA revealed asignificant main stimulus category effect on luminance values (F = 10.106; p =
.002). There is also a highly significant emotion category effect (F = 15.911;
p < .001), but a not significant interaction of those two factors (F = 2.931;
p = .058) (all Greenhouse?Geisser corrected). Similar to spatial frequency
data luminance data also demonstrate that differences exist between emotion
categories, but that these differences do not depend on stimulus category.
Paired-sample T-Tests revealed that image luminance differs significantly
between fearful faces and negative scenes (t = 4.177; p < .001). No other
differences were found to be significant (see Fig. 1).
Overall, we can summarise that both physical features show similar patterns in
terms of how they differ across stimulus and emotion categories. There are
emotion-specific differences in physical features, but those differences are
independent from stimulus category. In other words, any stimulus category
effect on brain activities can theoretically be explained by differences in
physical image features, but stimulus-specific emotion category effects
cannot.
For instance, taking a closer look at the present early EEG effects we notice
that the P1 stimulus category effects could theoretically be explained by
spatial frequency and/or luminance differences of our stimuli, but this is not
the case for the task-dependent effects and also not for all of the emotion-
specific brain activities that differ between faces and scenes as they were
found in later time windows.
### Tasks and procedure
During individual testing sessions, participants sat in a reclining chair
under dim lighting and positioned in front of a display monitor to allow 9.9°
× 8.5° of visual angle (300 × 399 pixels). After being connected to the
recording equipment, participants completed three delayed match-to-sample
emotion recognition tasks in random order. Each task was completed once with
face stimuli and once with scene stimuli (blocked sessions randomised within
tasks; Figs. 2a and b). At the beginning of the session participants were
informed of what each task involved so as to minimise potential practice
effects of task order bias. Then for each task, instructions for that task
were repeated and six practice trials were completed. Practice trials were not
included in the analyses.
Trials in each task always began with a fixation cross followed by a happy,
neutral or fearful picture (Stimulus 1; ?S1?) for 3000 ms, which required no
overt response from the participant other than to simply view the picture. S1
was proceeded by a second stimulus (Stimulus 2; ?S2?) which required an active
response. For one of the tasks (Picture?Picture matching; ?Pic?Pic?), the S2
was another happy, neutral or fearful picture, while for another one
(Picture?Word matching; ?Pic?Word?), the S2 was one of the three emotion
category labels ?fear?, ?neutral? or ?happy? presented in block white letters
against a black background. The active response for the Pic?Pic and Pic?Word
tasks was a forced choice (match/mismatch) judgement of whether S1 and S2
represented the same emotion (happy, fear or neutral) by pressing one of two
buttons (?Z? or ?/?) with the corresponding index finger as quickly as
possible without forgoing accuracy. For a third task (Picture labelling; ?Pic-
Label?), the S2 was the symbol ??? which cued the participant to say out loud
any one word that best described the emotion depicted in S1. For the Pic-Label
task, participants continued to the next trial by pressing the space bar. Key
responses at the end of each trial cued a 1000 ms inter-trial interval before
the next trial began. It should be emphasised that the S1 presentation
conditions were identical across the three tasks, i.e., the S1 was always a
passively viewed picture. Hence, the S2 event served as the experimentalmanipulation to examine whether emotional information (S1) is processed
differently depending on the emotion context of S2 and the task. Accordingly,
the event of interest for the analysis was the 3000 ms time window
corresponding to the S1 presentation.
There were 90 trials for each face and scene version of the tasks (30 fearful
(negative), 30 neutral and 30 happy (positive) S1 presentations). All S1
pictures were novel (i.e., no picture was presented more than once) and were
presented in colour (as were the S2 Pic?Pic pictures). S1 stimuli were
randomly presented within tasks, and for the Pic?Pic and Pic?Word tasks, S2
items were also randomised. Hence, each S1?S2 pairing was randomly generated,
however the frequency of match/mismatch and S1?S2 emotion category
combinations was balanced. Participants were given a short break midway and at
the end of each 90-trial task. The experiment took approximately one hour to
complete.
### Measures and data reduction
Due to the different nature and dynamics of biosignals recorded in the frame
of this study we chose different epoch lengths for the different measures.
Since we focus on early brain activity changes we set maximum epoch length to
1 s, but actually display ERPs only until 400 ms post stimulus, because during
this period early changes occur. Facial EMG epochs were set to 1.5 s to
potentially capture later effects and skin conductance epochs were set to 4 s,
because of their less dynamic nature and their delay (see more details below).
#### EEG recordings
Scalp EEG, measured in micro Volts (?V), was recorded using a 64 channel
Biosemi cap and amplifier (http://www.Biosemi.com) sampled continuously at
2048 Hz using an electrode layout corresponding to the 10-10-electrode
placement standard, and referenced to a common-mode signal. The data was down-
sampled offline to 256 Hz and filtered from 0.1 to 30 Hz using EEG Display
software. Eye blink artefacts were corrected using a set of linear regression
weights at each EEG electrode derived from an averaged eye blink (Semlitsch et
al., 1986). Segments of the EEG record containing gross artefact were detected
by an automated procedure that applied amplitude thresholds within a number of
frequency bands. For each S1 stimulus, EEG epochs were extracted from 100 ms
pre-stimulus to 1000 ms post-stimulus, and baseline corrected across the pre-
stimulus interval. Trials containing artefact exceeding ± 100 ?V were
excluded. Finally, trials were averaged to produce the ERP for each image
type.
Noisy channels were interpolated prior to data reduction for five
participants, involving no more than one electrode within each analysed
cluster. The first two trials completed during each face and scene recognition
task were removed. On average, 14% of trials were removed (4/30 trials per
condition for the 12 analysed conditions, SD = 2.92). Trials were then group-
averaged to create a single waveform per condition at each electrode location
and re-referenced to an average of all electrodes excluding the mastoid and
ocular sites.
The event-related components of interest were identified by visual inspection
of the electrode montage to identify clusters with prominent activity, and
then by software-facilitated comparisons to verify the exact electrode
locations of peak amplitude deflection. The high number of factors being
analysed increased the likelihood of generating false positive effects. To
reduce this risk, we employed procedures similar to those used by Schupp et
al. (2003) for calculating the grand mean of activity evoked during
experimental conditions for each component analysed. These included averaging
the activity of two electrode locations showing the greatest peak deflection,and then averaging over a time interval of at least 10 data samples (40 ms)
centred over the peak. The P1 component peaked over posterior?occipital
electrodes PO7/O1 and PO8/O2at 130 ms on average for face stimuli,
approximately 20 ms earlier than for scene stimuli, which had an average
latency of 150 ms. For each participant a single mean amplitude for each of
the 12 experimental conditions was calculated for the 120?160 ms time
interval. At temporal?occipital regions face stimuli elicited a prominent N170
component, while scene stimuli elicited only a small N100. These components
were immediately followed by a slower progressive negative shift, the so
called early posterior negativity (EPN). The N100/N170 and EPN were most
prominent over temporal?occipital electrode locations P7/P9 and P8/P10. For
each experimental condition, a single mean amplitude was again calculated for
each participant for the 150?190 ms time interval corresponding to the
N100/N170 and for the 220?280 ms time interval corresponding to the EPN.
#### fEMG recordings
The corrugator supercilii (CS) muscles, which furrow the eyebrows, were used
to reference muscle potential changes corresponding to face and scene stimuli
of negative content (see Ekman and Friesen (1978)). The zygomaticus major (ZM)
muscles, which lift the cheeks and lips were used to reference muscle
potential changes corresponding to face and scene of positive content. fEMG of
the CS and ZM, measured in micro Volts (?V), was recorded using a NeXus-10
wireless amplifier (http://www.Mindmedia.com) connected via Bluetooth to a PC
laptop, and output measurements were recorded using the NeXus-customised
Biotrace + Software. A NeXus Trigger Interface was used to synchronise the
onset of trial events between the continuous EEG and EMG recordings to within
less than 1 ms accuracy (http://www.Mindmedia.com).
Bipolar electromyography (EMG) was used to record muscle potential changes of
both muscles on both sides of the face. We used dual channel electrode cables
with carbon coating and active shielding technology for low noise and an
additional ground electrode cable attached to the back of the neck (see Reaz
et al., 2006, Wand, 2015). The EMG sampling rate was 2048 Hz. A band pass
filter from 20 Hz to 500 Hz was applied during online recording. Raw EMG data
were then recalculated by using the root mean square (RMS) method (epoch-size
= 1/16 s) to transform EMG signals into amplitudes.
The resulting amplitudes were then subject to statistical analysis. Using a
Matlab based program (www.mathworks.com), a single 1750 ms epoch time-locked
to 250 ms preceding the onset of each S1 stimulus presentation was then
extracted and divided into seven 250 ms time intervals by averaging across
data points. The first time window (? 250?0 ms) served as a baseline
correction for the six following intervals (0?250, 250?500, 500?750, 750?1000,
1000?1250, 1250?1500 ms) which were the subject of the analysis. After the
removal of gross artefacts, the time windows were baseline corrected and the
first two trials for each face and scene recognition task were removed from
analysis. An inspection of within-trial and across-trial variance was carried
out for each data set using an outlier criterion of 3.5 SD or greater. On
average, 18% of trials were removed from the ZM data (5/30 trials per
condition for the 12 analysed conditions, SD = 3.56) and 13% of trials were
removed from the CS data (4/30 trials per condition, SD = 3.35).
#### Skin conductance recordings
Skin conductance was recorded at a rate of 32 Hz with a Nexus-10-SC/GSR sensor
(Two finger sensor) connected to the Nexus-10 recording system with a 24 bit
resolution which is able to register changes of less than 0.0001 ?S. Because
the galvanic skin response is slow-changing, a 4250 ms epoch was extracted,
time-locked to 250 ms preceding the onset of S1, with ? 250 to 0 ms serving asthe baseline correction interval. The residual was divided into four 1000 ms
time intervals for further analysis (0?1000, 1000?2000, 2000?3000, 3000?4000
ms). Like with EMG data, after the removal of gross artefacts, the time
windows were baseline corrected and the first two trials for each face and
scene recognition task were removed from analysis. An inspection of within-
trial and across-trial variance was carried out for each data set using an
outlier criterion of 3.5 SD or greater. On average, 22% of trials were removed
(7/30 trials per condition, SD = 3.56).
### Statistical analyses
The analysis was a fully within-subjects design with three factors: Stimulus
type (Faces, Scenes) × Emotion (Fear (negative), Neutral, Happy (positive)) ×
Task (Pic?Pic, Pic?Word). Note again that the Pic-Label task was not included
in the current analysis. For each event-related potential (ERP) component of
interest, condition grand means for the extracted time intervals were subject
to a 4-way Stimulus type × Emotion × Task × Hemisphere (Left, Right) repeated
measures analysis of variance (RM ANOVA). For the ZM, CS and SCR analyses,
condition grand means for each time interval (six time intervals for ZM and
CS, and four for SCR) were subject to 3-way Stimulus type × Emotion × Task RM
ANOVAs. Significant interactions between Stimulus type and Emotion (_p_ < .05)
were further investigated where appropriate with secondary RM ANOVAs,
conducted separately for each stimulus type. All other significant
interactions involving the factor Stimulus type (_p_ < .05) were further
investigated with paired-samples _t_ -tests with bonferroni alpha corrections.
For Sphericity violations (_p_ < .05), Greenhouse?Geisser epsilon adjustments
were applied if ? < .75, otherwise Hyundt?Feldt. All main effects are
reported, however because the primary objective of the analysis was to
investigate differences in emotional face and scene processing, only the
interactions involving the factor Stimulus type are reported.
## Results
### EEG data
#### P1 component
Emotional content did not affect P1 amplitudes, however the type of stimulus
and task did. In addition to the different latencies at which the P1 emerged
for faces and scenes, significant main effects also emerged for the factors
Stimulus type (_F_ (1, 22) = 15.64, _p_ = .001, _?2_ = .42) and Task (_F_ (1,
22) = 24.89, _p_ < .001, _?2_ = .53). As can be seen in the top left and right
waveforms in Fig. 3, scenes evoked a larger mean P1 deflection compared to
faces, while pictures viewed during the Pic?Pic task produced larger mean P1
deflections compared to the Pic?Word task. The factors Stimulus type and Task
also interacted significantly (_F_ (1, 22) = 5.43, _p_ = .029, _?2_ = .20),
indicating that the effect of the Pic?Pic task on P1 amplitudes was greater
for scenes than for faces (see also topography maps in Fig. 3, far right).
1. Download: Download high-res image (292KB)
2. Download: Download full-size image
Fig. 3. Effects of the Pic?Pic and Pic?Word tasks on early visual processing
of emotional faces and scenes. For quick reference, examples of the tasks are
shown in the top right corner. Waveforms show grand-averaged ERPs collapsed
across emotion categories and time-locked to the onset of the passively viewed
Stimulus 1 (onset = 0 ms). Waveforms in the top panel represent brain activity
recorded at left and right posterior?occipital electrode regions. At the far
right are corresponding topographic maps of P1-related brain activity.
Waveforms in the bottom panel represent brain activity recorded at left and
right lateral occipital electrode regions. L = Left hemisphere. R = Right
hemisphere.#### N170 component (faces)/N100 component (scenes)
The factor Stimulus type produced a strongly significant main effect (_F_ (1,
22) = 324.22, _p_ < .001, _?2_ = .94) indicating that, in line with past
research, faces evoked a much larger negative deflection in the N100 time
window, the so called N170 component, compared to scenes. A significant main
effect of Task was also observed (_F_ (1, 22) = 33.60, _p_ < .001, _?2_ =
.60), indicating that pictures viewed during the Pic?Word task produced larger
mean N100/N170 deflections compared to the Pic?Pic task (Fig. 3 bottom left
and right waveforms).
Stimulus type also interacted separately with the factors Hemisphere (_F_ (1,
22) = 17.64, _p_ = < .001, _?2_ = .45) and Emotion (_F_ (2, 44) = 7.63, _p_ =
.001, _?2_ = .26). Effects of emotion content are shown in Fig. 4 top left and
right waveforms, and bar graphs depicting the mean activity across the
categories are also displayed at the bottom left. The Stimulus type ×
Hemisphere interaction indicated that regardless of task or emotional
expression, face stimuli produced greater activity over the right hemisphere
relative to left compared to scenes, which produced no observable lateralised
effects. However, note that when we followed up this effect by comparing the
total mean activity produced by faces over left and right hemispheres (i.e.
collapsing the means of emotion and task conditions), right hemispheric
activity was only marginally greater than left (_p_ = .059). More critically,
the Stimulus type × Emotion interaction was further investigated using
separate secondary ANOVAs for each stimulus type. These ANOVAs showed that the
N100 was not sensitive to the emotional content of scenes (_F_ (2, 44) = 1.64,
_p_ = .206, _?2_ = .07), but that the N170 was differently modulated depending
on the emotional facial expression (_F_ (2, 44) = 6.27, _p_ = .004, _?2_ =
.22). Contrasts confirmed that fearful faces evoked significantly (_p_ = .004;
_p_ = .043) more negative N170 amplitudes compared to neutral and happy faces,
respectively, while amplitudes for happy and neutral facial expressions were
not different (_p_ = .131).1
1. Download: Download high-res image (269KB)
2. Download: Download full-size image
Fig. 4. Effects of emotion category on early visual processing of emotional
faces and scenes. In the top panel, waveforms represent brain activity
recorded at left and right lateral occipital regions and show grand-averaged
ERPs collapsed across task categories and time-locked to the onset of the
passively viewed Stimulus 1 (onset = 0 ms). The bar graph at the bottom left
illustrates the mean N170-related activity averaged across left and right
hemispheres. The bar graph at the bottom right illustrates EPN-related
activity recorded over left and right hemispheres. Error bars represent one
standard error of the mean. * = The differences are significant after
Bonferroni corrections. L = Left hemisphere. R = Right hemisphere.
#### EPN time window
Waveforms corresponding to the EPN are also presented in Fig. 4 (top left and
right waveforms), and bar graphs depicting the mean activity across the
categories are displayed at the bottom right. Main effects of Stimulus type
(_F_ (1, 22) = 412.62, _p_ < .001, _?2_ = .95), Task (_F_ (1, 22) = 19.08, _p_
< .001, _?2_ = .46), and Emotion (_F_ (1.70, 37.38) = 11.60, _p_ < .001, _?_ 2
= .35, with sphericity corrections ?2 = 6.16, ? = .85, _p_ = .046) re-emerged,
as did the interaction between Stimulus type and Emotion (_F_ (2, 44) = 43.11,
_p_ < .001, _?2_ = .66). Additionally, a significant main effect of Hemisphere
emerged (_F_ (1, 22) = 5.70, _p_ = .026, _?2_ = .21), which led to a three-way
interaction between Stimulus type, Emotion, and Hemisphere (_F_ (2, 44) =
5.54, _p_ = .007, _?2_ = .20). Accordingly, secondary ANOVAs of each stimulustype were performed with Emotion and Hemisphere as within-subjects factors.
For faces, Emotion (_F_ (2, 44) = 11.63, _p_ < .001, _?2_ = .35) and
Hemisphere (_F_ (1, 22) = 4.53, _p_ = .045, _?2_ = .17) produced significant
main effects but did not interact (_p_ = .171). Contrasts confirmed that the
negative-going shift in activity during fearful face presentations was
significantly greater than during happy and (_p_ < .001) and neutral (_p_ =
.013) face presentations. The effect of Hemisphere further indicated greater
negative-going activity over the left hemisphere, but see below for a more
thorough interpretation. For scene stimuli, Emotion (_F_ (2, 44) = 41.96, _p_
< .001, _?2_ = .66) and Hemisphere (_F_ (1, 22) = 5.95, _p_ = .023, _?2_ =
.21) produced significant main effects and did interact significantly (_F_ (2,
44) = 3.74, _p_ = .032, _?2_ = .15). The EPN was differently sensitive to
emotional content in scenes than in faces however, with positive scenes
eliciting significantly greater negative-going activity compared to negative
and neutral scenes (both _p_ 's < .001). Critically, the interaction of
Emotion and Hemisphere evoked by scenes revealed that although scenes (and
faces) generated more negativity over the left hemisphere, there was greater
discrimination of positive from negative scenes over the right hemisphere (_p_
= .005), hence the EPN was more robust over the right hemisphere. In short,
fearful faces and positive scenes elicited a stronger EPN overall compared to
other emotional stimuli, but positive scenes also elicited a lateral
difference in EPN magnitude whereas fearful faces did not. See Table 1
summarising all EEG-related statistical significancies.
Table 1. Summary of significant factor main effects and/or significant factor
interactions related to EEG data.
EEG| P1| N170/N100| EPN
---|---|---|---
_Task_| | P < .001| P < .001
_Emotion_| | | P < .001
_Stimulus_| P < .001| P < .001| P < .001
_Hemisphere_| | | P = .026
_stimulus * task_| P = 0.29| |
_stimulus * hemisphere_| | P < .001|
_stimulus * emotion_| | P = .001| P < .001
_stimulus* emotion * hemisphere_| | | P = .007
### EMG data
#### Zygomatic recordings
No main effects emerged from the six ANOVAs, however a significant three-way
interaction between Stimulus type, Emotion and Task emerged at the third time
interval corresponding to the time between 500 and 750 ms post-stimulus (_F_
(2, 44) = 3.22, _p_ = .049, _?2_ = .13) followed by a sustained interaction
between Stimulus type and Emotion over the next three time intervals (750?1500
ms; _F_ (2, 44) = 4.54, 6.45, 4.85, _p_ = .016, .003, .012, _?2_ = .17, .23,
.18, respectively). As seen in Fig. 5, these interactions collectively
indicated that positive scenes evoked spontaneous ZM activity, while happy
faces did not. The initial three-way interaction between Stimulus type,
Emotion and Task also suggested that positive scenes elicited spontaneous
activity earlier during the Pic?Word task compared to the Pic?Pic task.
However, when the corresponding data for scene stimuli was submitted to a
secondary ANOVA with Task and Emotion as the within-subjects factors, the
modulatory effect of Task disappeared (_p_ = .202), and, consistent with the
effects at ensuing time intervals, was replaced with a significant Stimulus
type × Emotion interaction (_F_ (2, 44) = 3.62, _p_ = .035, _?2_ = .14). As
expected, the secondary ANOVA for face stimuli revealed no effects of task oremotion (all p-values > .2)
1. Download: Download high-res image (128KB)
2. Download: Download full-size image
Fig. 5. Mean zygomatic muscle EMG amplitudes (?V) and error bars for 1
standard error, time-locked to the onset of the passively viewed Stimulus 1
(onset = 0 ms). * = The differences are significant after Bonferroni
corrections.
Paired samples _t_ -tests were used to determine significant fluctuations in
ZM activity between emotional scene categories at each time interval for
intervals three to six (corrected significance threshold = .017). From
approximately 500?750 ms, positive scenes evoked significant differences in ZM
activity relative to negative scenes (_p_ = .034, .005, .019, .088 (trend),
respectively for intervals 3?6). Significant differences between positive and
neutral scenes did not emerge until 750?1000 ms, but were reliably strong
across the epoch (_p_ = .228, .004, .001, < .001, respectively for intervals
3?6). See Table 2 summarising all statistically significant zygomaticus
effects.
Table 2. Summary of significant factor main effects and/or significant factor
interactions related to _zygomaticus major_ EMG data.
EMG| Zygomaticus
---|---
Empty Cell| _0?250_| 250?500| 500?750| 750?1000| 1000?1250| 1250?1500
_Stimulus * Emotion_| | | | P = .016| P = .003| P = .012
_Stimulus* Emotion * Task_| | | P = .049| | |
#### Corrugator recordings
As shown in the top panel in Fig. 6, CS activity was characterised by a rapid
reduction in muscle activity from stimulus onset to 750 ms during all face and
scene S1 presentations, however the rate of this decline was faster when faces
were viewed compared to scenes, which led to a significant main effect of
Stimulus type in the 250?500 ms time window (_F_ (1, 22) = 5.58, _p_ = .027,
_?_ 2 = .20).2 This apparent relaxation of corrugator muscles at the point of
stimulus onset has been demonstrated by others (e.g. Achaibou et al., 2008,
Dimberg and Petterson, 2000, Dimberg et al., 2000, Dimberg et al., 2002), and
is thought to be the result of increased tension in corrugator muscles at
baseline due to anticipatory focus and attention towards an imminent visual
stimulus presentation (van Boxtel and Jessurun, 1993, Van Boxtel et al.,
1996).
1. Download: Download high-res image (193KB)
2. Download: Download full-size image
Fig. 6. Mean corrugator muscle EMG amplitudes (?V) and error bars for 1
standard error, time-locked to the onset of the passively viewed Stimulus 1
(onset = 0 ms). The top graph illustrates the mean amplitudes evoked by
emotional faces compared with scenes after collapsing across emotion and task
categories in order to highlight early latency differences involving
corrugator muscle relaxation between the stimuli, presumably reflecting
differences in the speed of early attentional orienting. The bottom graphs
illustrate the different patterns of spontaneous corrugator activity elicited
by emotional faces and scenes. Note that amplitude values differ across the
scales in the top and bottom panel graphs. * = The differences are significant
after Bonferroni corrections.
Then, from 500 to 750 ms, spontaneous muscle activity emerged as a function of
emotion category for both face and scene stimuli, seen via a significant main
effect of emotion (_F_ (2, 44) = 9.39, _p_ < .001, _?_ 2 = .30) that remained
reliably significant across the next three time windows.3 As seen in thebottom panel in Fig. 6, spontaneous activity tended to be greater for fearful
and neutral faces and negative and neutral scenes, while happy faces and
positive scenes led to greater relaxation of the CS muscles. Paired samples
_t_ -tests were again used to determine significant fluctuations in CS
activity between emotional categories, done separately for faces and scenes at
each time interval of interest i.e. intervals three to six (with a corrected
significance threshold of .017). For faces, spontaneous emotion-related
activity emerged only briefly at 500?750 ms as a trend (comparisons at all
other intervals, _p_ > .07). Here, fearful and neutral expressions evoked
significantly (_p_ = .017; _p_ = .061 (only trend)) greater CS activity
compared to happy expressions, respectively. Contrastingly, negative scenes
evoked a stronger, more enduring spontaneous effect from 500?750 ms onwards,
producing significantly greater activity relative to positive scenes across
most of the epoch (_p_ = .017, < .001, .091, .005, respectively for intervals
3-6). The generally stronger activity evoked by neutral compared to positive
scenes reached significance only at the fourth interval between 750 and 1000
ms (_p_ = .638, .014, .078, .186, respectively for intervals 3?6), and similar
to the effects observed in ZM activity, negative and neutral scenes evoked
very little difference in CS activity (_p_ = .043, .161, .801, .048,
respectively for intervals 3?6). See Table 3 summarising all statistically
significant corrugator effects.
Table 3. Summary of significant factor main effects and/or significant factor
interactions related to _corrugator supercilii_ EMG data.
EMG| Corrugator
---|---
Empty Cell| _0?250_| 250?500| 500?750| 750?1000| 1000?1250| 1250?1500
_Stimulus_| | P = .027| | | |
_Emotion_| | | P < .001| P = .004| P = .006| P = .020
_Stimulus* Emotion * Task_| | | | | |
### Skin conductance recordings
The ANOVA corresponding to the first 1000 ms post S1-onset showed a trend
towards a significant main effect of Stimulus type (_F_ (1, 22) = 3.97, _p_ =
.059, _?_ 2 = .15), followed by a significant effect between 1000 and 2000 ms
(_F_ (1, 22) = 5.22, _p_ = .032, _?_ 2 = .19), which then diminished from 2000
ms onwards (_p_ > .3). As seen in Fig. 7, skin conductance levels were greater
overall at an early post-stimulus stage and tended to decrease over the three
second presentation, with face stimuli evoking slightly greater activity
during the early stage. That the effect occurred at such an early stage
relative to a typical skin conductance response which emerges more slowly at
around 2 s post stimulus, suggests that differences between faces and scenes
were a residual effect related to the S2 active response stage of preceding
trials. Nevertheless, the significant effect indicates that faces and scenes
evoked different arousal-related activity which was independent of emotional
content. See Table 4 summarising all statistically significant skin
conductance effects.
1. Download: Download high-res image (133KB)
2. Download: Download full-size image
Fig. 7. Mean skin conductance amplitudes (?S) and error bars for 1 standard
error evoked by emotional faces and scenes time-locked to the onset of the
passively viewed Stimulus 1 (onset = 0 ms). Means were calculated by
collapsing across emotion and task categories. * = The differences are
significant after Bonferroni corrections.
Table 4. Summary of significant factor main effects and/or significant factor
interactions related to SC data.SC
---
Empty Cell| _0?1000_| 1000?2000| 2000?3000| 3000?4000
_Stimulus_| P = .059| P = .032| |
## Discussion
The aim of the current study was to determine the differences in emotional
face and scene processing at neural and behavioural (i.e. spontaneous facial
activity) levels. Using EEG to measure neural activity we found that the early
visually-evoked P1 component peaked earlier for faces than for scenes, and
that the type of task differently modulated the depth of this visual-related
processing for faces and scenes. For faces the N170 was sensitive to the
emotional content of the stimuli whereas the N100 for scenes was not. The EPN
was sensitive to the emotional content of both faces and scenes, but
differently so. For faces, the EPN was enhanced by fearful expressions as was
the N170, while for scenes, positive content elicited enhanced EPN amplitudes,
more prominent over the right hemisphere. Using fEMG we found that positive
scenes but not happy faces elicited enhanced spontaneous zygomatic activity,
whereas both fearful faces and negative scenes elicited enhanced spontaneous
corrugator activity, but again this emotion effect was more enduring for
scenes. Furthermore, prior to the influence of emotion, corrugator activity
was marked by a rapid orienting response that occurred faster for faces than
for scenes, which was akin to early P1 effects. Finally, skin conductance
responses revealed slightly greater arousal levels when viewing faces than
when viewing scenes. That the effect occurred at an early stage relative to a
typical skin conductance response which emerges more slowly at around 2 s post
stimulus, suggests that differences between faces and scenes were a residual
effect related to the S2 active response stage of preceding trials.
Nevertheless, the significant effect indicates that faces and scenes evoked
different arousal-related activity which was independent of emotional content.
### Early neural processing of emotional faces and scenes
#### P1 component
Neural activity at the early visually evoked P1 component showed that
stimulus-specific features of faces and scenes evoked different degrees of
rapid attentional processing irrespective of emotional content. Scenes
generated greater visually-evoked cortical activity compared to faces, most
likely because of their greater degree of complexity (Bradley et al., 2007).
Bradley et al. also found that picture complexity influences the magnitude of
evoked potentials proceeding the P1, which would explain the large differences
in visually evoked potentials between stimulus groups in the current study.
The Pic?Pic task also differently influenced the depth of P1-related visual
processing of scenes and faces in a manner that was proportional to the
complexity of the stimuli. In other words, processing of scene stimuli, which
were more complex, was considerably enhanced, whereas processing of face
stimuli, which were less complex, was only slightly enhanced.
It is important to mention that even though we can rule out that both
luminance and spatial frequency (see method section) explain later task- and
emotion-specific effects it is theoretically possible that those physical
features explain category-specific effects like the ones described above.
Beyond this stage, other early processes involved in face and object
perception, i.e. the N100/N170 and EPN components, were not differently
modulated by faces and scenes as a function of the recognition tasks.
#### N100/N170 component
A critical finding at the neural level was that affective information in faces
influenced neural activity earlier than affective information in scenes, asreflected by the enhanced activity of the N100/N170 for fearful faces, but not
for emotional compared to neutral scenes. Recently, Thom et al. (2014) also
compared neural activity generated by emotional faces and scenes in a single
experimental paradigm, and found that neural processes underlying the
N100/N170 component were sensitive to affective information for scenes. These
differences between Thom et al.'s and our findings may be explained by
methodological differences including that, in their study, some emotional
scene stimuli contained faces (fear, joy and angry stimuli, but not neutral,
which were all inanimate objects), while we specifically did not include
scenes with forward-facing faces. Also, their participants were all males
whereas in the current study participants were a mixture of males and females.
The latter distinction is important because males have been shown to generate
enhanced N100/N170 amplitudes compared to females (Proverbio et al., 2009),
and it has been shown that natural scenes with and without human faces can
evoke significantly different activity in the N100/N170 time window (Ferri et
al., 2012, Proverbio et al., 2009). It is possible that these factors,
particularly when combined, led to selective enhancement of amplitudes in the
N100/N170 time window during their emotional scene presentations.
The negativity bias of fearful facial expressions relative to neutral and
other emotional expressions has received a variety of interpretations in the
literature. Some have suggested that the early discrimination of fearful from
neutral faces is due to crude threat or signs of danger which rapidly activate
neural circuits specialised for detecting danger (e.g. Esteves et al., 1994,
Hansen and Hansen, 1988, LeDoux, 2003, Öhman, 2005, Öhman and Mineka, 2001).
However, this theory likely does not explain our results, as we would have
expected negative scenes, which included fearful components such as snakes and
spiders to evoke such activity as well, particularly considering that
detecting negative scenes is highly survival-relevant, and that negative
scenes in this study were rated as more unpleasant and more arousing than
fearful faces.
Vuilleumier and Pourtois (2007) instead reason that the anatomical regions
involved in facial expression recognition may be spatially organised according
to emotion categories, in that sub-regions associated with encoding facial
features more unique to one expression are spatially segregated from sub-
regions associated with encoding those that are more unique to another
expression. Hence, emotion category-specific modulation of the N170 component
may be reflecting these regional variations rather than motivational emotional
significance of the stimuli (Vuilleumier and Pourtois, 2007). In support of
this theory, they argue that activity in the N170 time window has been found
to be differently sensitive to a range of facial expressions in addition to
fear such as surprise and disgust. Along this line, faces with more similar
expressions such as anger and fear (both negative and with overlapping facial
muscle contraction) evoke more similar modulatory activity than when compared
with happy faces (Thom et al., 2014). On this basis, and when considering that
emotional content is identified faster for faces compared to scenes (Britton
et al., 2006, Dimberg et al., 2002, Eisenbarth et al., 2011), the current data
could be reflecting faster extraction of low level semantic affective
information from faces than from scenes, but not necessarily faster
identification of motivational emotional significance.
#### EPN time window
An EPN emerged at a similar latency for emotional faces and scenes, occurring
immediately following the offset of the N100/N170 components. For faces, the
EPN was pronounced only for fearful expressions, while for scenes, the EPN was
pronounced only for positive content. When considered separately, theseselective modulations of the EPN are in line with past research showing
enhanced negativity for fearful faces in the EPN time window compared to
neutral (Leppänen et al., 2008, Stekelenburg and de Gelder, 2004) and happy
expressions (Herbert et al., 2013b, Mühlberger et al., 2009, Schupp et al.,
2004b), and enhanced EPN negativity for positive scenes compared to negative
and neutral scenes (Bublatzky and Schupp, 2011, Franken et al., 2008, Schupp
et al., 2004a, Schupp et al., 2006b, Schupp et al., 2007, Schupp et al.,
2013b, Weinberg and Hajcak, 2010). Further research also supports our
observation that neural activity during the EPN time-frame was generally more
negative over the left hemisphere compared with the right (Schupp et al.,
2004b, Smith et al., 2013), while stronger emotion-specific modulation
occurred over the right hemisphere, and seemed to be an exclusive effect of
scene stimuli (Junghöfer et al., 2001, Schupp et al., 2007). These findings
suggest that EPN activity includes a commonality between processing of fearful
facial expressions and positive scenes.
Still, there seems to be no clear explanation in the literature addressing why
emotional faces and scenes evoke such different category-specific responses in
the EPN time window. Speculation has centred on the motivational significance
of affective cues, particularly in that erotica are highly arousing stimuli.
In the present study, stimuli with low or moderate arousal were chosen which
could have facilitated an arousal-driven processing bias for positive scenes
due to the erotica content. However skin conductance recordings during these
presentations do not support this interpretation, and instead, suggest that
all face stimuli evoked enhanced arousal levels compared to scenes. Findings
from Thom et al.'s study of faces and scenes (2014) also showed that despite
positive scenes (including erotica) being rated as more arousing than all
other emotional scene categories, these stimuli did not produce enhanced EPN
activity. Thus it seems that other factors are more likely.
Experiments involving EPN analyses have inevitably become more elaborate, and
there is now some evidence linking EPN activity to modulations of self-
reference or task relevance (e.g. Herbert et al., 2013a, Herbert et al.,
2013b, Stekelenburg and de Gelder, 2004), which could also be extended to
explain the differential effects of face and scene stimuli seen in the current
study. Evidence from several studies suggest that as stimuli become more
salient with respect to the complexity of required processing, typically as a
result of task demands, so too does the degree of EPN modulation during the
associated stimulus presentations, suggesting that EPN activity could be a
precursor to more conscious levels of stimulus evaluation (Herbert et al.,
2013a, Stekelenburg and de Gelder, 2004).
For example, in a study requiring participants to categorise fearful and
neutral faces as either upright or inverted, upright fearful faces predictably
evoked increased EPN activity compared to upright neutral faces. When faces
were inverted however, the EPN was enhanced for both fearful and neutral faces
compared to when the same faces were shown in the upright position
(Stekelenburg and de Gelder, 2004). This suggests that the EPN is sensitive to
task-induced changes in stimulus complexity.
These effects extend even to self-referential emotion processing whereby
emotional faces preceded by matched self-relevant word cues such as ?my fear?
or ?my happiness? evoked enhanced EPN activity compared to when the preceding
cues were meaningless letter strings (Herbert et al., 2013b), supporting the
view that the self-reference of affective stimuli facilitates motivated
attention capture to emotional stimuli as reflected by the EPN component. In
another study, participants were asked to use specific cue words (e.g. cues
like ?no fear?, ?no panic? etc.) to intentionally regulate their feelingsevoked by fearful and happy faces. Using these cue words that attenuated the
emotion described in the picture (e.g. no fear paired with a fearful face)
attenuated ERP amplitudes to fearful faces as early as in the EPN time window
(Herbert et al., 2013a).
In the frame of active versus passive tasks, similar effects have also been
documented during the EPN time interval. For example, Schupp et al. (2007b)
compared the effect of passively viewing emotional scenes versus the effect of
actively counting the number of times a specifically valanced scene was
presented. They found typical EPN modulation during the passive viewing task,
whereby erotica (pleasant stimuli) elicited enhanced EPN activity compared to
mutilation (unpleasant) and neutral stimuli, but when participants were
required to count the number of presentations occurring for each emotion
category, EPN activity was significantly modulated relative to when the same
category of emotional stimuli were passively viewed (Schupp et al., 2007b).
More recently, a study by Schupp et al. (2013a) comparing passive viewing to
active semantic categorisation also suggested a link between EPN activity and
higher cognitive evaluations. Here, emotional scenes were overlayed with
simple pictures of either animals or other non-animal scenes and objects. In
the active categorisation task, participants were required to judge whether
the foreground picture was either an animal or not, while no response was
required during passive viewing. Again, in the passive viewing task, stimuli
with pleasant scenes as the background image evoked stronger EPN activity than
did stimuli with unpleasant or neutral background scenes. However, active
categorisation led to the diminishment of emotion-specific modulation, and
instead, EPN activity was enhanced overall relative to the passive viewing
task regardless of the emotional background. Moreover, EPN activity was
significantly enhanced for foreground pictures of animals compared to non-
animals, suggesting that task difficulty and stimulus salience can override
emotional significance in the EPN time window.
In the current experiment, where the task was to passively view the S1
stimuli, but also to implicitly identify its emotional content, stimulus
salience, and thus EPN activity, should have been driven, at least partially,
by emotion recognition difficulty. Fearful facial expressions and positive
scenes, which evoked enhanced EPN activity, may have been more difficult to
recognise relative to other sub-categories, resulting in a call for more
sophisticated cognitive processing to accurately identify and categorise these
stimuli. This is exactly what past research predicts. Recio et al. (2014)
twice demonstrated that recognising fearful faces as expressing fear was more
difficult than recognising happiness from smiling faces and neutrality from
neutral faces, and found that negative expressions (i.e., anger, disgust,
fear, sadness and surprise) were most often confused, whereas happy faces
enjoy a recognition advantage in the EPN time window (Calvo and Beltran,
2013). As would be predicted by these findings, other research further shows
that happy faces are identified and responded to faster than angry faces
(Sonnby-Borgström, 2002; see also Leppänen and Hietanen, 2004, Calvo and
Lundqvist, 2008) supporting the view that happy faces are attention grabbing
due to their salience (i.e. the biologically determined social relevance of a
happy face for both interaction partners including the perceiver and the
receiver (Becker and Srinivasan, 2014)).
Speculatively, this could also explain why positive scenes evoked enhanced EPN
activity relative to other emotional scene categories. The negative, mainly
fearful (e.g. spiders and snakes) and neutral (e.g. a computer, a chair) scene
stimuli used in the current study were generally rather obvious and intuitive
to categorise. However, the content of positive scene stimuli was more varied(e.g. extreme sports, nature scenes, appetising foods and erotica), thereby
providing less-intuitive cues directly linked to ?happiness?. Hence, emotion-
category specific modulation of the EPN may have been related to differences
in recognition difficulty, resulting in a call for more sophisticated
cognitive processing to accurately categorise emotional stimuli which lacked
intuitive cues, thereby causing fearful facial expressions and positive scenes
to become more salient with respect to EPN-related brain activity.
### Spontaneous facial reactions to emotional faces and scenes
Emotion-related spontaneous activity of both the zygomatic and corrugator
muscles emerged between 500 and 1000 ms post stimulus, which is consistent
with other fEMG studies of emotional faces (Dimberg, 1982, Dimberg, 1997,
Moody et al., 2007) and scenes (Dimberg et al., 1998). It suggests that faces
and scenes trigger the same neural affective processes, or processes with
similar latencies leading to spontaneous facial reactions. This is also
consistent with the observation that emotional significance was detected in
neural activity at a similar latency for face and scene stimuli.
One difference between faces and scenes however, was that spontaneous
zygomatic activity was evoked by positive scenes but not by happy faces.
Künecke et al. (2014) also found no effect of happy faces on spontaneous
zygomatic activity, while fearful facial expressions evoked reliably enhanced
corrugator activity. Moreover, their experiment was also conducted in the
frame of an emotion recognition task which, similar to the current study,
required a delayed rather than immediate recognition judgement. As has been
discussed above, one possible explanation for why positive scenes but not
happy faces evoked spontaneous reactions is that, in the frame of an emotion
recognition task, smiling faces may simply be easy to recognise and
semantically categorise, whereas semantically categorising positive scenes
requires more effortful mental processing. If this is the case, we want to
speculate on these findings and point to two possible ideas: firstly, emotion-
related spontaneous facial activity is linked to emotion recognition
processes. Secondly, the mechanisms are triggered only when emotional
information is ambiguous, as has been suggested by others (Winkielman et al.,
2009). Moreover, basic motor-mimicry is not necessary for social-emotion
recognition, as has been demonstrated by others (Grèzes et al., 2013, Magnée
et al., 2007, Moody et al., 2007). The latter interpretation is particularly
supported by the observation that happy faces did not seem to evoke any
substantial change in zygomatic activity relative to baseline.
In addition to effects related to the content of the images, motivational
factors also seemed to contribute to the differential spontaneous reactions to
faces and scenes. Emotional scenes not only elicited spontaneous activity in
the emotion-appropriate muscles, but did so quite strongly compared to faces.
The disparity is particularly evident in the observed corrugator activity in
that fearful (and neutral4) faces evoked only a momentarily enhanced response,
whereas negative scenes evoked a strong stable response which seemed to become
even stronger at the same time that the effect of fearful faces diminished.
Alpers et al. (2011) also reported stronger orbicularis oculi (ring muscle
around the eyes) activity (an index of the Duchene smile) to pleasant scenes
than to smiling faces, and more broadly, demonstrated greater activation of
other behavioural indexes of motivation by emotional scenes compared to faces
including decreased heart rate acceleration, greater startle reflex modulation
and increased skin conductance levels, findings that were also replicated by
Wangelin et al. (2012).5 Indeed, stimulus arousal ratings in both our study
and Alpers' study also point to motivational influences in that emotional
scenes were rated as more arousing than emotional faces. It should also beconsidered that in such a task-primed context with little ?real-world? social-
motivational relevance, the veracity of motivational influences of facial
expressions on behaviour diminish, or perhaps strong stable
behavioural?emotional responses are not necessary during social interactions,
and instead could hinder one's ability to keep up with the naturally dynamic
exchange of affective signals in social?emotional interactions. Contrastingly,
emotionally evocative scenes more often involve an immediate
approach/avoidance overt physical response, and thus even at such an automated
stage of behaviour, it is logical to expect these stimuli will evoke more
stable and enduring motivational reactions.
### Consistencies between neural activity and spontaneous facial reactions
#### Stronger emotional responses to scenes
Consistent with the observation that emotional scenes evoked stronger and more
stable spontaneous behavioural activations compared to faces, neural processes
related to the EPN were also more strongly and stably activated by scenes than
by faces. These effects also emerged in Thom et al.'s comparison of faces and
scenes (2014), where they discussed this effect in terms of scenes activating
stronger motivationally relevant activity, particularly with respect to
theories pointing to increased amygdala activations as a contributing source
of EPN activity. Thom and colleagues interpretation is further supported by
functional MRI data showing that biologically relevant stimuli do activate
stronger functional connectivity between visual areas and the amygdala than do
socially relevant stimuli (Sakaki et al., 2012). Hence motivational factors
may influence how long evoked affective responses persist, rather than or in
addition to the strength of the response. Such a distinction may have gone
unnoticed in past facial EMG research, given that many studies focus on an
averaged amplitude for a block of time (some up to six seconds), rather than
shorter sequential averages.
The findings therefore draw on two similarities between EPN activity and
spontaneous facial reactions including stimulus salience, which is largely a
product of task demands, and the persistence of motivational responses, which
begs the question of whether EPN activity may be a precursor to motivated
behaviour. To test this theory, responses to positive scenes were analysed
(chosen because this stimuli evoked the strongest EPN activity as well as
reliably strong zygomatic activity, and thus would be of greatest interest and
least implicated by floor/ceiling effects) via a post hoc correlation of right
hemispheric EPN activity, and spontaneous zygomatic activity averaged over the
time frame of strongest emotional responding (750?1500 ms). If stronger EPN
activity is related to stronger spontaneous zygomatic responses, then we would
at least expect to see a negative linear correlation between these
physiological outputs indicating that the stronger the negative shift in EPN-
evoked potentials, the greater the amplitude of zygomatic muscle contractions.
Even without attention to potential outliers, the analysis of the 23 data sets
revealed a modest correlation in the predicted direction (_r_ = ? .29) which
was trending towards significance (_p_ = .091), while after removing one
outlier, the correlation strengthened considerably (_r_ = ? .47, _p_ = .014).
Of course, this correlation may represent a consistency in the absolute levels
of neural and behavioural activity within individuals. Therefore, further
research is necessary to determine the meaning of this relationship between
EPN activity and behavioural responses. In addition, it would also be
interesting to investigate the modulation of late ERP potentials such as the
LPP. The LPP has been shown to be influenced by emotional and cognitive
factors. In contrast to early ERP potentials such as the EPN, however, the LPP
is thought to reflect top-down controlled affective processing (Hajcak andNieuwenhuis, 2006; for review see Olofsson et al., 2008, Korb et al., 2012).
#### Faster attentional orienting to faces
Of course it would also be interesting to explore the relationship between
fEMG activity and LPP modulation. However, as argued above such a correlation
would not reveal if cortical or facial changes occurred first. In the present
study we show that EPN modulation is correlated with changes in fEMG activity,
which suggests that facilitated early cortical processing of emotional scenes
is a prerequisite for changes in facial muscle activity.
Early neural activity and behavioural responses to faces and scenes suggests
that viewing emotional faces engages attention processes faster than when
viewing emotionally evocative scenes. This was reflected in the earlier onset
latency of the P1 component and the faster release of tension in the
corrugator eyebrow muscles when viewing faces. Such differences can also be
seen in evoked potentials in other studies of emotional faces and scenes
(Kujawa et al., 2012, Thom et al., 2014). These findings could assist in
explaining why the valence of emotional faces is identified faster than the
valence of emotional scenes when measured via button press (Britton et al.,
2006) and voluntary mimicry latencies (Dimberg et al., 2002, Eisenbarth et
al., 2011), and may be a factor underlying why attention disengagement from
socially-relevant stimuli to a secondary target is faster than from
biologically-relevant stimuli (Sakaki et al., 2012).
### Conclusions
The findings in this study point to two key differences between emotional face
and scene stimuli during early visual processing, including rapid attentional
capture mechanisms and motivated response mechanisms. Broadly, there was a
logical chronology of stages of perception that could be seen in both neural
activity and behavioural output in that selective attention determined what
was attended to, which manifested into affectively-driven selective emotional
responses. However, not all stimulus percepts affecting neural activity were
apparent in behavioural activity. The picture- versus word-primed recognition
tasks elicited different neural activity at the P1 and N100/N170 components,
which broadly, reflects variations in basic object recognition processes.
However, the recognition tasks did not influence the nature of spontaneous
behavioural responses. This suggests that fast motivated emotional behaviour,
including spontaneous facial reactions, is relatively robust to variations in
basic object and face recognition pathways such as variations in semantic-
priming, and logically corroborates with the idea that behavioural responses
to emotional stimuli are grounded in motivational emotional significance.
Further research investigating consistencies between neural activity and
behavioural responses is needed to replicate and extend the current research.
Critically though, this will require careful sequential temporal analyses, as
opposed to amplitude-only analyses, particularly at very early time frames of
less than one second, which is quite rare in the facial EMG literature.
However, the fact that EMG techniques offer excellent temporal resolution
matching that of EEG means that such studies are viable and will be very
valuable.
## Acknowledgments
We thank Ross Fulham, Tony Kemp, Melinda Vardanega, Natalie Townsend, Jennifer
Gilchrist, Samantha Allen and Jacob Duffy for their assistance performing the
study.
Recommended articles"
101,103,Emotionet challenge: Recognition of facial expressions of emotion in the wild,"['CF Benitez-Quiroz', 'R Srinivasan', 'Q Feng']",2017,109,Expression in-the-Wild,machine learning,"recognition of facial expressions of emotion in the wild. Key  facial expressions, the  heterogeneity of expression in the wild in the wild by current computer vision and machine",No DOI,arXiv preprint arXiv …,https://arxiv.org/abs/1703.01210,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
102,104,"Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild","['C Fabian Benitez-Quiroz', 'R Srinivasan']",2016,685,"Affective Faces Database, Expression in-the-Wild","classification, classifier","in our set of a million face images in the wild, we  databases to successfully recognize AUs  and AU intensities on an independent database of images not used to train our classifiers. 2.",No DOI,Proceedings of the …,https://ieeexplore.ieee.org/document/7780969,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
103,105,Enhanced amygdala reactivity to emotional faces in adults reporting childhood emotional maltreatment,"['AL van Harmelen', 'MJ van Tol']",2013,302,Karolinska Directed Emotional Faces,"classification, neural network","to negative emotional faces are related to psychopathology, we investigated whether  abnormal amygdala (and/or mPFC functioning) was more apparent in emotionally maltreated",No DOI,Social cognitive and …,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3624946/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
104,106,Ensemble-based discriminant learning with boosting for face recognition,"['J Lu', 'KN Plataniotis']",2006,244,Toronto Face Database,"classifier, machine learning",ensemble-based methods as they are known in the machine learning literature [18]). Globally  nonlinear methods are not without problems. Approaches such as those based on kernel,No DOI,IEEE transactions on …,https://pubmed.ncbi.nlm.nih.gov/16526485/,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
105,107,Estimation of continuous valence and arousal levels from faces in naturalistic conditions,"['A Toisoul', 'J Kossaifi', 'A Bulat', 'G Tzimiropoulos']",2021,160,Affective Faces Database,"machine learning, neural network","For example, the ability to accurately extract emotional information from the face of the  person one is communicating with plays a major role in prosociality 24 and this capacity is often",No DOI,… Machine Intelligence,https://www.nature.com/articles/s42256-020-00280-0,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
106,108,Evidence and a computational explanation of cultural differences in facial expression recognition.,"['MN Dailey', 'C Joyce', 'MJ Lyons', 'M Kamachi', 'H Ishi']",2010,249,Japanese Female Facial Expression,"classification, classifier, facial expression recognition, machine learning, neural network",our interpretation of facial expressions of emotion is universal  EMPATH models to recognize  facial expressions in a variety  of facial expression images with different mixes of Japanese,No DOI,Emotion,https://pubmed.ncbi.nlm.nih.gov/21171759/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,
107,109,Excavating AI: The politics of images in machine learning training sets,"['K Crawford', 'T Paglen']",2021,442,Toronto Face Database,machine learning,"By looking at the politics of classification within machine learning systems, this article  demonstrates why the automated interpretation of images is an inherently social and political",No DOI,Ai & Society,https://link.springer.com/article/10.1007/s00146-021-01162-8,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
108,110,Exploiting multi-cnn features in cnn-rnn based dimensional emotion recognition on the omg in-the-wild dataset,"['D Kollias', 'S Zafeiriou']",2020,182,"Acted Facial Expressions In The Wild, Affective Faces Database, Expression in-the-Wild, Static Facial Expression in the Wild",CNN,"The Aff-Wild database served as benchmark for the Aff-Wild Challenge, organized in   the temporal dependencies of facial expressions in each utterance, we designed standard",No DOI,IEEE Transactions on Affective …,https://arxiv.org/abs/1910.01417,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
109,111,"Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface","['D Kollias', 'S Zafeiriou']",2019,345,Expression in-the-Wild,CNN,"expression classification). To address these, we substantially extend the largest available  in-the-wild  We conduct extensive experiments with CNN and CNN-RNN architectures that use",No DOI,arXiv preprint arXiv:1910.04855,https://arxiv.org/abs/1910.04855,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
110,112,Extended deep neural network for facial emotion recognition,"['DK Jain', 'P Shamsolmoali', 'P Sehdev']",2019,416,"Extended Cohn-Kanade, Japanese Female Facial Expression","classification, deep learning, facial expression recognition, neural network","This paper proposed a new deep learning model for the  size and variety of datasets, deep  neural network is the most  datasets are publicly available such as Cohn–Kanade (CK+) [7]",No DOI,Pattern Recognition Letters,https://www.sciencedirect.com/science/article/pii/S016786551930008X,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
111,113,"FACES—A database of facial expressions in young, middle-aged, and older women and men: Development and validation","['NC Ebner', 'M Riediger', 'U Lindenberger']",2010,1382,Karolinska Directed Emotional Faces,FER,"to neutral faces (Isaacowitz et al., 2006). These studies have almost exclusively used  emotional faces of young individuals and have not systematically varied the age of the face, even",No DOI,Behavior research methods,https://link.springer.com/article/10.3758/BRM.42.1.351,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
112,114,"Face behavior a la carte: Expressions, affect and action units in a single network","['D Kollias', 'V Sharmanska', 'S Zafeiriou']",2019,206,Acted Facial Expressions In The Wild,neural network,manifestation of complex facial expressions. The dataset  We train a multi-task neural  network model to jointly perform ( It served as benchmark for the ABAW Competition organized,No DOI,arXiv preprint arXiv:1910.11111,https://arxiv.org/abs/1910.11111,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
113,115,Face detection using mixtures of linear subspaces,"['MH Yang', 'N Abuja', 'D Kriegman']",2000,121,Toronto Face Database,classification,"in face recognition [2]. The reason for this is that FLD provides a better projection than PCA  for pattern classification. In the second proposed method, we decompose the training face",No DOI,… on Automatic Face and …,https://ieeexplore.ieee.org/document/840614,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
114,116,Face expression recognition with a 2-channel convolutional neural network,"['D Hamester', 'P Barros', 'S Wermter']",2015,164,Toronto Face Database,"facial expression recognition, neural network","CNN and train it with the Acted Facial Expression in the Wild (AFEW) dataset [17]. In the sec   with the Toronto Face Dataset. After that, a softmax layer is trained with the AFEW dataset.",No DOI,2015 international joint …,https://ieeexplore.ieee.org/document/7280539,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
115,117,Face recognition using kernel direct discriminant analysis algorithms,"['J Lu', 'KN Plataniotis']",2003,836,Toronto Face Database,"classification, classifier",") [20] have in pattern regression and classification tasks, we propose a new kernel discriminant  analysis algorithm for face recognition. The algorithm generalizes the strengths of the",No DOI,IEEE transactions on …,https://ieeexplore.ieee.org/document/1176132,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,
116,118,Face recognition with radial basis function (RBF) neural networks,"['MJ Er', 'S Wu', 'J Lu', 'HL Toh']",2002,975,Toronto Face Database,"classification, classifier, neural network",excellent performance both in terms of error rates of classification and learning efficiency.   Database Our experiments were performed on the face database which contains a set of face,No DOI,IEEE transactions on neural …,https://ieeexplore.ieee.org/document/1000134,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
117,119,Face recognition: A convolutional neural-network approach,"['S Lawrence', 'CL Giles', 'AC Tsoi']",1997,4525,Toronto Face Database,"classification, neural network",We are interested in rapid classification and hence we do not assume that time is  available for extensive preprocessing and normalization. Good algorithms for locating,No DOI,IEEE transactions on …,https://ieeexplore.ieee.org/document/554195,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
118,120,Facenet2expnet: Regularizing a deep face recognition net for expression recognition,"['H Ding', 'SK Zhou', 'R Chellappa']",2017,489,Toronto Face Database,"CNN, classification, deep learning, facial expression recognition, machine learning, neural network","adapt to the new domain task (facial expression recognition), we attach the fully- databases:  CK+ [27], Oulu-CASIA [28], Toronto Face Database (TFD) [29] and Static Facial Expression",No DOI,… face & gesture recognition (FG …,https://arxiv.org/abs/1609.06591,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
119,121,Faces of pain: automated measurement of spontaneousallfacial expressions of genuine and posed pain,"['GC Littlewort', 'MS Bartlett', 'K Lee']",2007,232,Toronto Face Database,machine learning,"machine learning approach, previously used successfully to categorize basic emotional facial   Here we applied machine learning on a 20-channel output stream of facial action detectors",No DOI,… of the 9th international conference on …,https://www.researchgate.net/publication/221052396_Faces_of_pain_automated_measurement_of_spontaneousallfacial_expressions_of_genuine_and_posed_pain,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
120,122,Facial Affect``In-The-Wild,"['S Zafeiriou', 'A Papaioannou', 'I Kotsia']",2016,122,"Acted Facial Expressions In The Wild, Expression in-the-Wild, Static Facial Expression in the Wild","CNN, FER, classification, classifier, deep learning, facial expression recognition, machine learning, neural network","of compound expressions [20]). A recent survey on facial expression recognition can be   In particular, the network exploits the fact that facial expressions can be decomposed to FAU.",No DOI,… Pattern Recognition …,https://openaccess.thecvf.com/content_cvpr_2016_workshops/w28/papers/Zafeiriou_Facial_Affect_In-The-Wild_CVPR_2016_paper.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
121,123,Facial emotion detection using deep learning,"['A Jaiswal', 'AK Raju', 'S Deb']",2020,150,"Facial Expression Recognition 2013, Japanese Female Facial Expression","deep learning, facial expression recognition","basically three main steps: face detection, features extraction,  a convolutional neural networks  (CNN) based deep learning  the Japanese Female Face Expression (JAFFE) [15], Facial",No DOI,2020 international conference for …,https://ieeexplore.ieee.org/document/9154121,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
122,124,Facial emotion processing in borderline personality disorder: a systematic review and meta-analysis,"['AE Mitchell', 'GL Dickens', 'MM Picchioni']",2014,220,Karolinska Directed Emotional Faces,FER,A body of work has developed over the last 20 years that explores facial emotion perception  in Borderline Personality Disorder (BPD). We identified 25 behavioural and functional,No DOI,Neuropsychology review,https://pubmed.ncbi.nlm.nih.gov/24574071/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
123,125,Facial emotion recognition using convolutional neural networks (FERC),['N Mehendale'],2020,328,Extended Cohn-Kanade,"FER, neural network","FERC was extensively tested with more than 750K images using extended Cohn–Kanade  expression, Caltech faces, CMU and NIST datasets. We expect the FERC emotion detection",No DOI,SN Applied Sciences,https://link.springer.com/article/10.1007/s42452-020-2234-1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
124,126,Facial emotion recognition: State of the art performance on FER2013,"['Y Khaireddin', 'Z Chen']",2021,223,Facial Expression Recognition 2013,facial expression recognition,Facial emotion recognition (FER) is significant for human-computer interaction such as   ) in 2013 and became a benchmark in comparing model performance in emotion recognition.,No DOI,arXiv preprint arXiv:2105.03588,https://arxiv.org/abs/2105.03588,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
125,127,Facial expression analysis under partial occlusion: A survey,"['L Zhang', 'B Verma', 'D Tjondronegoro']",2018,121,Toronto Face Database,facial expression recognition,"While face and facial expression recognition systems  database includes four typical complex  facial expressions: smile while hands obscure the face, anger while hand obscure the face",No DOI,ACM Computing Surveys …,https://arxiv.org/abs/1802.08784,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
126,128,Facial expression analysis with AFFDEX and FACET: A validation study,"['S Stöckli', 'M Schulte-Mecklenbeck', 'S Borer']",2018,309,"Affective Faces Database, Radboud Faces Database","classification, deep learning, facial expression recognition, machine learning, neural network",Picture Database (GAPED) and the Radboud Faces Database (RaFD) facial expression  analysis assumes that there is a direct link between emotion production and emotion recognition,No DOI,Behavior research …,https://pubmed.ncbi.nlm.nih.gov/29218587/,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
127,129,"Facial expression classification based on SVM, KNN and MLP classifiers","['HI Dino', 'MB Abdulrazzaq']",2019,160,"Extended Cohn-Kanade, MMI Facial Expression","classification, classifier",The Extended Cohn-Kanade (CK+) dataset used as good data recourse to exam the  classification of human Facial Expression. Principal component analysis (PCA) used to reduce the,No DOI,2019 International Conference on …,http://ieeexplore.ieee.org/document/8723728/,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
128,130,Facial expression recognition,"['Y Tian', 'T Kanade', 'JF Cohn']",2011,255,"Affective Faces Database, Binghamton University 3D Facial Expression, Extended Cohn-Kanade, Japanese Female Facial Expression",facial expression recognition,"This chapter introduces recent advances in facial expression analysis and recognition. The first part discusses general structure of AFEA systems. The second part describes the problem space for facial expression analysis. This space includes multiple dimensions: level of description, individual differences in subjects, transitions among expressions, intensity of facial expression, deliberate versus spontaneous expression, head orientation and scene complexity, image acquisition and resolution, reliability of ground truth, databases, and the",No DOI,Handbook of face recognition,https://paperswithcode.com/task/facial-expression-recognition,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
129,131,Facial expression recognition ability among women with borderline personality disorder: implications for emotion regulation?,"['AW Wagner', 'MM Linehan']",1999,459,Japanese Female Facial Expression,facial expression recognition,"examined recognition of facial expressions of emo tion among women  21), compared to a  group of women with histories of  Japanese males, 2 Japanese females) for each of 7 different",No DOI,Journal of personality disorders,https://pubmed.ncbi.nlm.nih.gov/10633314/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
130,132,Facial expression recognition and histograms of oriented gradients: a comprehensive study,"['P Carcagnì', 'M Del Coco', 'M Leo', 'C Distante']",2015,214,Radboud Faces Database,FER,"Automatic facial expression recognition (FER) is a topic of  (HOG) descriptor in the FER  problem, highlighting as this  with most commonly used FER frameworks was carried out. In the",No DOI,SpringerPlus,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4628009/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
131,133,Facial expression recognition based on 3D dynamic range model sequences,"['Y Sun', 'L Yin']",2008,172,Binghamton University 3D Facial Expression,"FER, facial expression recognition","3D facial expression database [21], we extend the facial expression analysis to a dynamic  3D  In this paper, we propose a spatio-temporal 3D facial expression analysis approach for",No DOI,Computer Vision–ECCV 2008: 10th European …,https://link.springer.com/chapter/10.1007/978-3-540-88688-4_5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
132,134,Facial expression recognition based on a mlp neural network using constructive training algorithm,"['H Boughrara', 'M Chtourou', 'C Ben Amar']",2016,124,Facial Expression Recognition 2013,facial expression recognition,"Images PFI has been applied to extract features from human face images. To evaluate, the   Cohn-Kanade facial expression and the facial expression recognition FER-2013 databases.",No DOI,Multimedia Tools and …,https://link.springer.com/article/10.1007/s11042-014-2322-6,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
133,135,Facial expression recognition based on deep learning,"['H Ge', 'Z Zhu', 'Y Dai', 'B Wang', 'X Wu']",2022,105,Facial Expression Recognition 2013,facial expression recognition,"In addition, [29] proposed a multi-stage fine-tuning strategy: in the first stage, additional  facial expression database FE R 2013 was used to fine-tune the existing pre-training model; In",No DOI,Computer Methods and Programs in …,https://www.sciencedirect.com/science/article/pii/S0169260722000062,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Facial expression recognition based on deep learning
the features on this page.
Skip to main contentSkip to article
My account
Sign in
* Access through **your organization**
* Purchase PDF
* * Patient Access
* Other access options
Search 
## Article preview
* Abstract
* Introduction
* Section snippets
* References (38)
* Cited by (86)
## Computer Methods and Programs in Biomedicine
Volume 215, March 2022, 106621
# Facial expression recognition based on deep learning
Author links open overlay panelHuilin Ge, Zhiyu Zhu, Yuewei Dai, Biao Wang,
Xuedong Wu
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.cmpb.2022.106621Get rights and content
## Highlights
* ?
Autonomous driving, virtual reality and all kinds of robots integrated into
our life rely on facial expression recognition technology.
* ?
Facial expression recognition and computer vision is based on deep learning
technology and convolutional neural network.
* ?
Whether it is two-stage target detection or single-stage target detection,
performance of algorithm is measured by detection speed and accuracy.
* ?
Large variety of training data with accurate expression tags can fundamentally
improve expression recognition rate.
## Abstract
### Background and objective
Facial expression recognition technology will play an increasingly important
role in our daily life. Autonomous driving, virtual reality and all kinds of
robots integrated into our life depend on the development of facial expression
recognition technology. Many tasks in the field of computer vision are based
on deep learning technology and convolutional neural network. The paper
proposes an occluded expression recognition model based on the generatedcountermeasure network. The model is divided into two modules, namely,
occluded face image restoration and face recognition.
### Methods
Firstly, this paper summarizes the research status of deep facial expression
recognition methods in recent ten years and the development of related facial
expression database. Then, the current facial expression recognition methods
based on deep learning are divided into two categories: Static facial
expression recognition and dynamic facial expression recognition. The two
methodswill be introduced and summarized respectively. Aiming at the advanced
deep expression recognition algorithms in the field, the performance of these
algorithms on common expression databases is compared, and the strengths and
weaknesses of these algorithms are analyzed in detail.
### Discussion and results
As the task of facial expression recognition is gradually transferred from the
controlled laboratory environment to the challenging real-world environment,
with the rapid development of deep learning technology, deep neural network
can learn discriminative features, and is gradually applied to automatic
facial expression recognition task. The current deep facial expression
recognition system is committed to solve the following two problems: (1)
Overfitting due to lack of sufficient training data; (2) In the real world
environment, other variables that have nothing to do with expression bring
interference problems.
### Conclusion
From the perspective of algorithm, combining other expression models, such as
facial action unit model and pleasure arousal dimension model, as well as
other multimodal models, such as audio mode, 3D face depth information and
human physiological information, can make expression recognition more
practical.
## Introduction
Image recognition is a technology that uses computer to process, analyze and
understand images to identify different patterns of targets and objects. It is
a main research direction in the field of computer vision and plays an
important role in intelligent data acquisition and processing based on image.
Image recognition technology can efficiently complete the detection and
recognition of specific target objects (such as handwritten characters,
products or faces), image classification and marking, and subjective image
quality evaluation. At present, image recognition technology has a broad
commercial market and optimistic application prospects in Internet application
products such as image retrieval, commodity recommendation, user behavior
analysis and face recognition. Moreover, it has long-term development
potential in high-tech industries such as UAV, autonomous driving and
intelligent robot, as well as many fields such as geology, medicine and
biology. Early image recognition systems mainly used directional gradient
histogram [1] and scale invariant feature transformation [2], and then input
the extracted features into the classifier for classification and recognition.
These functions are basically manually designed. For different recognition
problems, the extracted features will directly affect the performance of the
system. Therefore, researchers need to further study the unsolved problem
areas in order to design better adaptive features to improve the performance
of the system. The image recognition system in this period is often for a
specific recognition task, and the data scale is small, the generalization
ability is poor, so it is not easy to achieve the ideal recognition effect in
daily application.
Deep learning is a branch of representation learning based on data ofartificial neural network. Deep learning includes supervised learning, semi-
supervised learning and unsupervised learning. In deep learning, deep neural
network, deep belief network and recurrent neural network have been widely
used in speech recognition, computer vision, audio recognition, unmanned
driving and natural language processing. Rina dechter first introduced deep
learning in 1986. In addition, Igor aizenberg introduced the concept of
artificial neural network in 2000. In fact, Alexey ivakhnenko and Lapa
proposed supervised feedforward learning network as early as 1965. In 1986,
Geoffrey Hinton proposed the back-propagation algorithm of multilayer
perceptron (MLS), and used the SIGMOD activation function for nonlinear
transformation, so as to solve the problems of nonlinear learning and
classification. In 2006, Geoffrey Hinton et al. Pointed out that the pre-
training weight should be used to initialize the model and fine tune the model
according to the supervised training. With the further development of deep
learning, the proposal of lenet [3] in 1998 marked the emergence of
convolutional neural network (CNN).
However, due to the backward hardware at that time, convolutional neural
network is not in an advantage compared with other machine learning methods
(such as SVM [4]). With the further development of computing devices, the Alex
net that is proposed by Hinton et al. has made significant achievements in the
computer vision competition ILSVRC 2012. Convolutional neural networks have
made rapid progress in recent decades.
## Section snippets
## Review of face recognition technology
Face recognition has the characteristics of easy access, easy operation and
diversified features. In the last century, many scholars studied face
recognition. However, due to the underdeveloped network, limited resources of
face images and poor quality of photos, many scholars mostly studied it from
the perspective of algorithm, but the recognition accuracy is low, far from
the human eye recognition effect. With the gradual maturity of machine
learning technology, there are many powerful
## Depth target detection algorithm
In the realm of computer vision, object recognition technology is an algorithm
that can detect sample objects in videos and photos. Recent target recognition
algorithms mostly rely on high-performance GPU chips based on multilayer
neural network and deep learning software framework and built-in thousands of
stream processors. Therefore, it is also known as deep object detection (Deep
OD). Object detection has always been a vital issue in the realm of computer
vision. Before deep learning is
## Deep facial expression recognition based on static image
Due to the convenience and availability of network static data processing, a
large number of researches are based on static images without considering time
information for expression recognition. Direct training of deep network on
relatively small facial expression databases will inevitably result in
overfitting problem. In order to solve this problem, many related researches
use additional auxiliary data to pre-train and build their own network, or
directly fine tune based on an effective
## Discussion
This paper firstly recommends the background knowledge of facial expression
recognition, and summarizes the evolution and development of database and
algorithm in the field of facial expression recognition. It points out that
deep learning has become the mainstream framework in this field. Then, the
expression recognition algorithms based on deep learning are divided into two
categories (static expression recognition network and dynamic expressionrecognition network). By comparing the
## Conclusions
In the field of computer vision, based on the research status at home and
abroad, facial expression recognition technology has made great progress and
development. However, there are still many challenges and difficulties waiting
for researchers to solve. For example, the research on facial expression
recognition in real scenes, and there is a certain confusion between different
expressions. The expression of facial emotion may vary with region, culture,
and environment. There are differences
## Declaration of Competing Interest
The authors declare that there is no conflict of interest in this paper.
## Acknowledgment
This work is supported by the National Natural Science Foundation of China
(No. 62006102).
Special issue articlesRecommended articles"
134,136,Facial expression recognition based on facial components detection and hog features,"['J Chen', 'Z Chen', 'Z Chi', 'H Fu']",2014,133,Extended Cohn-Kanade,"classification, classifier",perform the facial expression classification. We evaluate our proposed method on the JAFFE  dataset and an extended Cohn-Kanade dataset. The average classification rate on the two,No DOI,International workshops on …,http://www.cedus.it/documents/SicurezzaUrbana/Videosorveglianza_e_altre_tecnologie_video_di_controllo/3ZChi_ACV-1.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
135,137,Facial expression recognition based on fusion feature of PCA and LBP with SVM,"['Y Luo', 'C Wu', 'Y Zhang']",2013,187,Facial Expression Recognition 2013,facial expression recognition,", to assist the global grayscale features of facial expression recognition. The support vector   of facial expression recognition system are feature extraction and expression classification.",No DOI,Optik-International Journal for Light and Electron …,https://www.sciencedirect.com/science/article/pii/S0030402612006560,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
136,138,Facial expression recognition by de-expression residue learning,"['H Yang', 'U Ciftci', 'L Yin']",2018,512,Oulu-CASIA,classification,that we target to exploit for expression classification.  The Oulu-CASIA database [33]  contains data captured under three  The Oulu-CASIA VIS has 480 video sequences taken from,No DOI,… of the IEEE conference on computer …,https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Facial_Expression_Recognition_CVPR_2018_paper.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
137,139,Facial expression recognition from world wild web,"['A Mollahosseini', 'B Hasani', 'MJ Salvador']",2016,103,"Acted Facial Expressions In The Wild, Expression in-the-Wild, Facial Expression Recognition 2013, Static Facial Expression in the Wild","FER, classification, classifier, deep learning, facial expression recognition, machine learning, neural network",networks can recognize wild facial expressions with an  of inthe-wild facial expressions by  querying different search en search engines for facial expression recognition. We trained two,No DOI,… pattern recognition …,https://arxiv.org/abs/1605.03639,True,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
138,140,Facial expression recognition in JAFFE dataset based on Gaussian process classification,"['F Cheng', 'J Yu', 'H Xiong']",2010,134,"Japanese Female Facial Expression, Toronto Face Database","classification, classifier, facial expression recognition","Here we propose a GP model and investigate it for the facial expression recognition in the  Japanese female facial expression dataset. By the strategy of leave-one-out cross validation,",No DOI,IEEE Transactions on Neural …,http://ieeexplore.ieee.org/document/5551215/,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
139,141,Facial expression recognition in image sequences using geometric deformation features and support vector machines,"['I Kotsia', 'I Pitas']",2006,900,Toronto Face Database,facial expression recognition,tion problems used for FAUs detection in the grid and for facial expression recognition.   The Cohn–Kanade database [2] was used for the facial expression recognition in six basic,No DOI,IEEE transactions on image processing,https://ieeexplore.ieee.org/document/4032815,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
140,142,Facial expression recognition in the wild via deep attentive center loss,"['AH Farzaneh', 'X Qi']",2021,289,Expression in-the-Wild,"CNN, FER, classification, classifier, deep learning, facial expression recognition, machine learning, neural network","Accordingly, for the task of FER in the wild, where  Face Database (RAFDB) [14]. Then, we  conduct extensive experiments on these two widely used wild Facial Expression Recognition (",No DOI,Proceedings of the IEEE/CVF winter …,https://openaccess.thecvf.com/content/WACV2021/papers/Farzaneh_Facial_Expression_Recognition_in_the_Wild_via_Deep_Attentive_Center_WACV_2021_paper.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
141,143,Facial expression recognition in video with multiple feature fusion,"['J Chen', 'Z Chen', 'Z Chi', 'H Fu']",2016,246,Acted Facial Expressions In The Wild,"classification, classifier, facial expression recognition",Acted Facial Expression in Wild (AFEW) 4.0 database show that our approach is robust in  dealing with video-based facial expression recognition  streams of facial expressions analysis,No DOI,IEEE Transactions on Affective …,https://ieeexplore.ieee.org/document/7518582,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
142,144,Facial expression recognition using a hybrid CNN–SIFT aggregator,"['T Connie', 'M Al-Shabi', 'WP Cheah', 'M Goh']",2017,184,Facial Expression Recognition 2013,facial expression recognition,facial expression recognition task. The proposed method is motivated by the success of  Convolutional Neural Networks (CNN) on the face recognition  FER-2013 and CK+ datasets.,No DOI,International workshop on multi …,https://arxiv.org/abs/1608.02833,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
143,145,Facial expression recognition using computer vision: A systematic review,"['D Canedo', 'AJR Neves']",2019,164,"Affective Faces Database, Binghamton University 3D Facial Expression, Radboud Faces Database","CNN, FER, facial expression recognition","Facial expressions are the main focus of this systematic review. Generally, an FER system  consists  Binghamton University 3D Facial Expression database (BU-3DFE) [52]: contains 606",No DOI,Applied Sciences,https://www.mdpi.com/2076-3417/9/21/4678,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
144,146,Facial expression recognition using convolutional neural networks: state of the art,"['C Pramerdorfer', 'M Kampel']",2016,331,Facial Expression Recognition 2013,facial expression recognition,"The ability to recognize facial expressions automatically enables novel  In this  paper, we review the state of the art in image-based facial expression recognition using CNNs",No DOI,arXiv preprint arXiv:1612.02903,https://arxiv.org/abs/1612.02903,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
145,147,Facial expression recognition using deep convolutional neural networks,"['DV Sang', 'N Van Dat']",2017,102,"Facial Expression Recognition 2013, Toronto Face Database","facial expression recognition, neural network","different effective CNNs to tackle the problem of facial expression recognition. Since  FERC-2013 dataset is much smaller than ImageNet dataset, we propose some changes to avoid",No DOI,2017 9th International Conference on …,https://ieeexplore.ieee.org/document/8119447,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
146,148,Facial expression recognition using deep neural networks,"['J Li', 'EY Lam']",2015,100,Facial Expression Recognition 2013,facial expression recognition,"In this paper, a facial expression recognition system based on feedforward deep neural  networks is built. The system consists of three major stages, which are image preprocessing,",No DOI,2015 IEEE International Conference on Imaging …,https://ieeexplore.ieee.org/document/10193866,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
147,149,Facial expression recognition using enhanced deep 3D convolutional neural networks,"['B Hasani', 'MH Mahoor']",2017,337,"Facial Expression Recognition 2013, MMI Facial Expression","facial expression recognition, neural network","facial expressions in a sequence (Figure 1). We evaluate our proposed method using four  well-known facial expression databases (CK+, MMI in recognition of facial expressions in cross",No DOI,… of the IEEE conference on computer …,https://arxiv.org/abs/1705.07871,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
148,150,Facial expression recognition using facial movement features,"['L Zhang', 'D Tjondronegoro']",2011,314,"Affective Faces Database, Japanese Female Facial Expression","FER, facial expression recognition","FER that considers facial movement features. In this paper, we aim for improving the  performance of FER by automatically capturing facial  on the Japanese female facial expression (",No DOI,IEEE transactions on affective …,https://ieeexplore.ieee.org/document/5871583,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
149,151,Facial expression recognition using hierarchical features with deep comprehensive multipatches aggregation convolutional neural networks,"['S Xie', 'H Hu']",2018,213,Japanese Female Facial Expression,"FER, neural network",on Facial Expression Recognition (FER) develop quickly as well. Applications based on FER   213 images of 7 facial expressions posed by 10 Japanese females. 183 images with six,No DOI,IEEE Transactions on Multimedia,https://ieeexplore.ieee.org/document/8371638,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
150,152,Facial expression recognition using kernel canonical correlation analysis (KCCA),"['W Zheng', 'X Zhou', 'C Zou', 'L Zhao']",2006,358,Japanese Female Facial Expression,"FER, classification","We will use the Japanese female facial expression (JAFFE) database [6]–[8] and Ekman’s  “Pictures of Facial Affect” database [22], respectively, to conduct the FER based on KCCA. In",No DOI,IEEE transactions on neural …,https://ieeexplore.ieee.org/document/1593706,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
151,153,Facial expression recognition using local gravitational force descriptor-based deep convolution neural networks,"['K Mohan', 'A Seal', 'O Krejcar']",2020,145,Japanese Female Facial Expression,neural network,"facial expression of a person. The proposed method consists of two parts. The former  one finds out local features from face  Budynek, “The Japanese female facial expression (JAFFE)",No DOI,IEEE Transactions on …,https://ieeexplore.ieee.org/document/9226437,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
152,154,Facial expression recognition via deep learning,"['X Zhao', 'X Shi', 'S Zhang']",2015,270,"Acted Facial Expressions In The Wild, Extended Cohn-Kanade, Facial Expression Recognition 2013, Japanese Female Facial Expression, Radboud Faces Database","CNN, deep learning, facial expression recognition","Experimental results on two benchmarking facial expression databases, ie, the JAFFE  database and the Cohn-Kanade database, demonstrate the promising performance of the",No DOI,IETE technical review,https://ieeexplore.ieee.org/document/8308363,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
153,155,Facial expression recognition via learning deep sparse autoencoders,"['N Zeng', 'H Zhang', 'B Song', 'W Liu', 'Y Li', 'AM Dobaie']",2018,574,Facial Expression Recognition 2013,facial expression recognition,to establish a DSAE-based deep learning framework for facial expression recognition to  classify the expressions with high accuracy by learning the useful features from the data set.,No DOI,Neurocomputing,https://www.sciencedirect.com/science/article/pii/S0925231217314649,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
154,156,Facial expression recognition with CNN ensemble,"['K Liu', 'M Zhang', 'Z Pan']",2016,216,Facial Expression Recognition 2013,facial expression recognition,"method, we use the newly released Facial Expression Recognition 2013 (FER2013) dataset.  The dataset was created using Google image search API with emotion-related keywords. It",No DOI,2016 international conference on …,https://ieeexplore.ieee.org/document/7756145,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
155,157,Facial expression recognition with FRR‐CNN,"['S Xie', 'H Hu']",2017,108,Japanese Female Facial Expression,CNN,"Japanese Female Facial Expression (JAFFE) database. CK+ database is a public expression   As for JAFFE, it consists of 213 expressional images of ten Japanese female subjects. We",No DOI,Electronics Letters,https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/el.2016.4328,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
156,158,Facial expression recognition with convolutional neural networks: coping with few data and the training sample order,"['AT Lopes', 'E De Aguiar', 'AF De Souza']",2017,923,"Binghamton University 3D Facial Expression, Facial Expression Recognition 2013","CNN, facial expression recognition, neural network","Hence, facial expression recognition is still a challenging problem in computer vision. In this   for facial expression recognition that uses a combination of Convolutional Neural Network",No DOI,Pattern recognition,https://www.sciencedirect.com/science/article/pii/S0031320316301753,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
157,159,Facial expression recognition with faster R-CNN,"['J Li', 'D Zhang', 'J Zhang', 'J Zhang', 'T Li', 'Y Xia']",2017,125,Facial Expression Recognition 2013,facial expression recognition,CNN was used to identify facial expression. There are the following advantages used in facial  expression recognition extraction in the traditional facial expression recognition is avoided.,No DOI,Procedia Computer …,https://www.sciencedirect.com/science/article/pii/S1877050917303447,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
158,160,Facial expression recognition with identity and emotion joint learning,"['M Li', 'H Xu', 'X Huang', 'Z Song', 'X Liu']",2018,120,"Affective Faces Database, Facial Expression Recognition 2013","FER, facial expression recognition","In this work, besides training deep-learned facial expression feature (emotional  latent face  identity feature such as the shape or appearance of face. We propose an identity and emotion",No DOI,… Transactions on affective …,https://ieeexplore.ieee.org/document/8528894,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
159,161,Facial expression recognition: A survey,"['Y Huang', 'F Chen', 'S Lv', 'X Wang']",2019,189,"Binghamton University 3D Facial Expression, Facial Expression Recognition 2013, Japanese Female Facial Expression, MMI Facial Expression","FER, facial expression recognition",-of-the-art FER approaches are presented and analysed.  FER datasets and summarise  four FER-related elements of datasets that may influence the choosing and processing of FER,No DOI,Symmetry,https://www.sciencedirect.com/science/article/pii/S1877050915021225,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
160,162,Facial expressions of emotion (KDEF): Identification under different display-duration conditions,"['MG Calvo', 'D Lundqvist']",2008,671,Karolinska Directed Emotional Faces,facial expression recognition,"expression of each model is identified. These data have provided researchers on the processing  of affective facial expressions  face database, the Karolinska Directed Emotional Faces (",No DOI,Behavior research methods,https://link.springer.com/article/10.3758/BRM.40.1.109,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
161,163,Fast facial emotion recognition using convolutional neural networks and Gabor filters,"['MMT Zadeh', 'M Imani', 'B Majidi']",2019,110,Affective Faces Database,neural network,face databases are not reliable for the real world applications. This article presents a new  database called RAF-DB that  [13] proposed to train a CNN network for the final face emotion,No DOI,2019 5th Conference on …,https://ieeexplore.ieee.org/document/8734943,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
162,164,Fera 2015-second facial expression recognition and analysis challenge,"['MF Valstar', 'T Almaev', 'JM Girard']",2015,326,Facial Expression Recognition 2013,facial expression recognition,"recognition of facial expressions, to be held in conjunction with the 11 IEEE conference on  Face and Gesture Recognition,  For a general overview of the field of expression recognition",No DOI,… Gesture Recognition …,https://ieeexplore.ieee.org/document/7284874/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
163,165,Former-dfer: Dynamic facial expression recognition transformer,"['Z Zhao', 'Q Liu']",2021,116,Static Facial Expression in the Wild,FER,"According to the data type, the FER can be divided into static FER (SFER) and dynamic  FER (DFER) • We propose a dynamic facial expression recognition transformer for the in-the-wild",No DOI,Proceedings of the 29th ACM International Conference …,https://github.com/zengqunzhao/Former-DFER,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
164,166,"Framework for reliable, real-time facial expression recognition for low resolution images","['RA Khan', 'A Meyer', 'H Konik', 'S Bouakaz']",2013,180,"Facial Expression Recognition 2013, MMI Facial Expression",facial expression recognition,"–Kanade (CK+) posed facial expression database, spontaneous expressions of MMI facial  expression database and FG-NET facial expressions and emotions database (FEED) and",No DOI,Pattern Recognition Letters,https://www.sciencedirect.com/science/article/pii/S0167865513001268,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Framework for reliable, real-time facial expression
recognition for low resolution images
the features on this page.
Skip to main contentSkip to article
Search 
## Outline
1. 2. Abstract
3. 4. Keywords
5. 1\. Introduction
6. 2\. Related work
7. 3\. Pyramid of local binary pattern
8. 4\. Psycho-visual experiment
9. 5\. Expression recognition framework
10. 6\. Experiment and results
11. 7\. Conclusions and future work
12. Acknowledgment
13. References
Show full outline
## Cited by (121)
## Figures (5)
1. 2. 3. 4. 5.
## Tables (3)
1. Table 1
2. Table 2
3. Table 3
## Pattern Recognition Letters
Volume 34, Issue 10, 15 July 2013, Pages 1159-1168
# Framework for reliable, real-time facial expression recognition for low
resolution images
Author links open overlay panelRizwan Ahmed Khan a b, Alexandre Meyer a b,
Hubert Konik a c, Saïda Bouakaz a b
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.patrec.2013.03.022Get rights and content
### Highlights
* ?
Facial expressions can be analyzed automatically by mimicking human visual
system.
* ?Proposed descriptor has strong discriminative ability.
* ?
Proposed framework is robust for low resolution images and spontaneous
expressions.
* ?
Proposed framework generalizes well on unseen data.
* ?
The proposed framework can be used for real-time applications.
## Abstract
Automatic recognition of facial expressions is a challenging problem specially
for low spatial resolution facial images. It has many potential applications
in human?computer interactions, social robots, deceit detection, interactive
video and behavior monitoring. In this study we present a novel framework that
can recognize facial expressions very efficiently and with high accuracy even
for very low resolution facial images. The proposed framework is memory and
time efficient as it extracts texture features in a pyramidal fashion only
from the perceptual salient regions of the face. We tested the framework on
different databases, which includes Cohn?Kanade (CK+) posed facial expression
database, spontaneous expressions of MMI facial expression database and FG-NET
facial expressions and emotions database (FEED) and obtained very good
results. Moreover, our proposed framework exceeds state-of-the-art methods for
expression recognition on low resolution images.
* Previous article in issue
* Next article in issue
## Keywords
Facial expression recognition
Low resolution images
Local binary pattern
Image pyramid
Salient facial regions
## 1\. Introduction
Communication in any form i.e. verbal or non-verbal is vital to complete
various routine tasks and plays a significant role in daily life. Facial
expression is the most effective form of non-verbal communication and it
provides a clue about emotional state, mindset and intention (Ekman, 2001).
Human visual system (HVS) decodes and analyzes facial expressions in real time
despite having limited neural resources. As an explanation for such
performance, it has been proposed that only some visual inputs are selected by
considering ?salient regions? (Zhaoping, 2006), where ?salient? means most
noticeable or most important.
For computer vision community it is a difficult task to automatically
recognize facial expressions in real-time with high reliability. Variability
in pose, illumination and the way people show expressions across cultures are
some of the parameters that make this task difficult. Low resolution input
images makes this task even harder. Smart meeting, video conferencing and
visual surveillance are some of the real world applications that require
facial expression recognition system that works adequately on low resolution
images. Another problem that hinders the development of such system for real
world application is the lack of databases with natural displays of
expressions (Valstar and Pantic, 2010). There are number of publicly available
benchmark databases with posed displays of the six basic emotions (Ekman,
1971) exist but there is no equivalent of this for spontaneous basic emotions.
While, it has been proved that Spontaneous facial expressions differ
substantially from posed expressions (Bartlett et al., 2002). In this work, wepropose a facial expression recognition system that caters for illumination
changes and works equally well for low resolution as well as for good
quality/high resolution images. We have tested our proposed system on
spontaneous facial expressions as well and recorded encouraging results.
We propose a novel descriptor for facial features analysis, pyramid of local
binary pattern (PLBP) (refer Section 3). PLBP is a spatial representation of
local binary pattern (LBP) (Ojala et al., 1996) and it represents stimuli by
its local texture (LBP) and the spatial layout of the texture. We combined
pyramidal approach with LBP descriptor for facial feature analysis as this
approach has already been proved to be very effective in a variety of image
processing tasks (Hadjidemetriou et al., 2004). Thus, the proposed descriptor
is a simple and computationally efficient extension of LBP image
representation, and it shows significantly improved performance for facial
expression recognition tasks for low resolution images. We base our framework
for automatic facial expression recognition (FER) on human visual system (HVS)
(refer Section 5), so it extracts PLBP features only from the salient regions
of the face. To determine which facial region(s) are the most important or
salient according to HVS, we conducted a psycho-visual experiment using an
eye-tracker (refer Section 4). We considered six universal facial expressions
for psycho-visual experimental study as these expressions are proved to be
consistent across cultures (Ekman, 1971). These six expressions are anger,
disgust, fear, happiness, sadness and surprise. The novelty of the proposed
framework is that, it is illumination invariant, reliable on low resolution
images and works adequately for both i.e. posed and spontaneous expressions.
Generally, facial expression recognition system consists of three steps: face
detection, feature extraction and expression classification. The same has been
shown in Fig. 1. In our framework we tracked face/salient facial regions using
Viola?Jones object detection algorithm (Viola and Jones, 2001) as it is the
most cited and considered the fastest and most accurate pattern recognition
method for face detection (Kolsch and Turk, 2004). The second step in the
framework is feature extraction, which is the area where this study
contributes. The optimal features should minimize within-class variations of
expressions, while maximize between class variations. If inadequate features
are used, even the best classifier could fail to achieve accurate recognition
(Shan et al., 2009). Section 3 presents the novel method for facial features
extraction which is based on human visual system (HVS). To study and
understand HVS we performed psycho-visual experiment. Psycho-visual
experimental study is briefly described in Section 4. Expression
classification or recognition is the last step in the pipeline. In literature
two different ways are prevalent to recognize expressions i.e. direct
recognition of prototypic expressions or recognition of expressions through
facial action coding system (FACS) action units (AUs) (Ekman and Friesen,
1978). In our proposed framework, which is described in Section 5 we directly
classify six universal prototypic expressions (Ekman, 1971). The performance
of the framework is evaluated for five different classifiers (from different
families i.e. classification tree, instance based learning, SVM, etc.) and
results are presented in Section 6. Next section presents the brief literature
review for facial features extraction methods.
1. Download: Download high-res image (83KB)
2. Download: Download full-size image
Fig. 1. Basic structure of facial expression recognition system pipeline.
## 2\. Related work
In the literature, various methods are employed to extract facial features and
these methods can be categorized either as appearance-based methods orgeometric feature-based methods.
_Appearance-based methods._ One of the widely studied method to extract
appearance information is based on Gabor wavelets (Littlewort et al., 2006,
Tian, 2004, Donato et al., 1999). Generally, the drawback of using Gabor
filters is that it produces extremely large number of features and it is both
time and memory intensive to convolve face images with a bank of Gabor filters
to extract multi-scale and multi-orientational coefficients. Another promising
approach to extract appearance information is by using Haar-like features, see
Yang et al. (2010). Recently, texture descriptors and classification methods
i.e. local binary pattern (LBP) (Ojala et al., 1996) and local phase
quantization (LPQ) (Ojansivu and Heikkilä, 2008) are also studied to extract
appearance-based facial features. Zhao and Pietikäinen (2007) proposed to
model texture using volume local binary patterns (VLBP) an extension to LBP,
for expression recognition.
_Geometric-based methods._ Geometric feature-based methods (Zhang and Ji,
2005, Pantic and Patras, 2006, Valstar et al., 2005, Bai et al., 2009)
extracts shapes and locations of facial components information to form a
feature vector. The problem with using geometric feature-based methods is that
they usually require accurate and reliable facial feature detection and
tracking which is difficult to achieve in many real world applications where
illumination changes with time and images are recorded in very low resolution.
Generally, we have found that all the reviewed methods for automatic facial
expression recognition are computationally expensive and usually requires
dimensionally large feature vector to complete the task. This explains their
inability for real-time applications. Secondly, in literature, very few
studies exist that tackles the issue of expressions recognition from low
resolution images, this adds to lack of applicability of expression
recognition system for real world applications. Lastly, all of the reviewed
methods, spend computational time on whole face image or divides the facial
image based on some mathematical or geometrical heuristic for features
extraction. We argue that the task of expression analysis and recognition
could be done in more conducive manner, if only some regions are selected for
further processing (i.e. salient regions) as it happens in human visual
system. Thus, our contributions in this study are:
* 1.
We propose a novel descriptor for facial expression analysis i.e. pyramid of
local binary pattern (PLBP), which outperforms state-of-the-art methods for
expression recognition on low resolution images (spatially degraded images).
It also performs better than other state-of-the-art methods for good
resolution images (with no degradation).
* 2.
As the proposed framework is based on human visual system it algorithmically
processes only salient facial regions which reduces the length of feature
vector. This reduction in feature vector length makes the proposed framework
suitable for real-time applications due to minimized computational complexity.
## 3\. Pyramid of local binary pattern
The proposed framework creates a novel feature space by extracting proposed
PLBP (pyramid of local binary pattern) features only from the visually salient
facial region (see Section 4 for psycho-visual experiment). PLBP is a
_pyramidal-based spatial_ representation of local binary pattern (LBP)
descriptor. PLBP represents stimuli by their local texture (LBP) and the
spatial layout of the texture. The spatial layout is acquired by tiling the
image into regions at multiple resolutions. The idea is illustrated in Fig. 2.
If only the coarsest level is used, then the descriptor reduces to a globalLBP histogram. Comparing to the multi-resolution LBP of Ojala et al. (2002),
our descriptor selects samples in a more uniformly distributed manner, whereas
Ojala?s LBP takes samples centered around a point leading to missing some
information in the case of face (which is different than a repetitive
texture).
1. Download: Download high-res image (187KB)
2. Download: Download full-size image
Fig. 2. Pyramid of local binary pattern. First row: stimuli at two different
pyramid levels, second row: histograms of LBP at two respective levels, and
third row: final descriptor.
LBP features were initially proposed for texture analysis (Ojala et al.,
1996), but recently they have been successfully used for facial expression
analysis (Zhao and Pietikäinen, 2007, Shan et al., 2009). The most important
property of LBP features are their tolerance against illumination changes and
their computational simplicity (Ojala and Pietikäinen, 1999, Ojala et al.,
1996, Ojala et al., 2002). The operator labels the pixels of an image by
thresholding the 3 × 3 neighborhood of each pixel with the center value and
considering the result as a binary number. Then the histogram of the labels
can be used as a texture descriptor. Formally, LBP operator takes the
form:(1)LBP(xc,yc)=?n=07s(in-ic)2nwhere in this case _n_ runs over the eight
neighbors of the central pixel c, ic and in are the gray level values at _c_
and _n_ and s(u) is 1 if u?0 or 0 otherwise.
Later, the LBP operator is extended to use neighborhood of different sizes
(Ojala et al., 2002) as the original operator uses 3 × 3 neighborhood. Using
circular neighborhoods and bilinearly interpolating the pixel values allow any
radius and number of pixels in the neighborhood. The LBP operator with _P_
sampling points on a circular neighborhood of radius _R_ is given
by:(2)LBPP,R=?p=0P-1s(gp-gc)2pwhere, gc is the gray value of the central
pixel, gp is the value of its neighbors, _P_ is the total number of involved
neighbors and _R_ is the radius of the neighborhood.
Another extension to the original operator is the definition of _uniform
patterns_ , which can be used to reduce the length of the feature vector and
implement a simple rotation-invariant descriptor. A local binary pattern is
called uniform if the binary pattern contains at most two bitwise transitions
from 0 to 1 or vice versa when the bit pattern is traversed circularly.
Accumulating the patterns which have more than two transitions into a single
bin yields an LBP operator, denoted LBPP,Ru2 patterns. These binary patterns
can be used to represent texture primitives such as spot, flat area, edge and
corner.
We extend LBP operator so that the stimuli can be represented by its local
texture and the spatial layout of the texture. We call this extended LBP
operator as pyramid of local binary pattern or PLBP. PLBP creates the spatial
pyramid by dividing the stimuli into finer spatial sub-regions by iteratively
doubling the number of divisions in each dimension. It can be observed from
Fig. 2 that the pyramid at level _l_ has 2 _l_ sub-regions along each
dimension (R0,?Rm-1). Histograms of LBP features at the same levels are
concatenated. Then, their concatenation at different pyramid levels gives
final PLBP descriptor (as shown in Fig. 2). It can be defined
as:(3)Hi,j=?l?xyI{fl(x,y)=i}I{(x,y)?Rl}where l=0,?,m-1, i=0,?,n-1. _n_ is the
number of different labels produced by the LBP operator and(4)I(A)=1ifAis
true0otherwiseWhile, the dimensionality of the descriptor can be calculated
by:(5)N?l4lWhere, in our experiment (see Section 6) l=1 and _N_ = 59 as we
created pyramid up to level 1 and extracted 59 LBP features using LBP8,2u2
operator, which denotes a uniform LBP operator with eight sampling pixels in alocal neighborhood region of radius 2. This pattern reduces the histogram from
256 to 59 bins. In our experiment we obtained 295 dimensional feature vector
from one facial region i.e. mouth region (59 dimensions/sub-region), since we
executed the experiment with the pyramid of level 1 (the same is shown in Fig.
2).
### 3.1. Novelty of the proposed descriptor
There exist some methods in literature that uses pyramid of LBP for different
applications and they look similar to our proposed descriptor, i.e. Wang et
al., 2011, Guo et al., 2010, Moore and Bowden, 2011. Our proposition is novel
and there exist differences in the methodology that creates differences in the
extracted information. Method for face recognition proposed in Wang et al.
(2011) creates pyramid before applying LBP operator by down sampling original
image i.e. scale-space representation, whereas we propose to create the
spatial pyramid by dividing the stimuli into finer spatial sub-regions by
iteratively doubling the number of divisions in each dimension. Secondly, our
approach reduces memory consumption (do not requires to store same image in
different resolutions) and is computationally more efficient. Guo et al.
(2010) proposed approach for face and palmprint recognition based on
multiscale LBP. Their proposed method seems similar to our method for
expression recognition but how multiscale analysis is achieved deviates our
approach. Approach proposed in Guo et al. (2010) achieves multiscale analysis
using different values of _P_ and _R_ , where LBP(P,R) denotes a neighborhood
of _P_ equally spaced sampling points on a circle of radius _R_ (discussed
earlier). Same approach has been applied by Moore and Bowden (2011) for facial
features analysis. Generally the drawback of using such approach is that it
increases the size of the feature histogram and increases the computational
cost. Moore and Bowden (2011) reports dimensionality of feature vector as high
as 30,208 for multiscale face expression analysis as compared to our
proposition which creates 590 dimensional feature vector (see Section 5) for
the same task. We achieve the task of multiscale analysis much more
efficiently than any other earlier proposed methods. By the virtue of
efficient multiscale analysis our framework can be used for real time
applications (see Table 1 for the time and memory consumption comparison)
which is not the case with other methods.
Table 1. Comparison of time and memory consumption.
Empty Cell| Shan et al. (2009)(a)| Shan et al. (2009)(b)| Bartlett et al.
(2003)| PLBP
---|---|---|---|---
Memory (feature dimension)| 2478| 42,650| 92,160| 590
Time (feature extraction time)| 0.03 s| 30 s| ?| 0.01 s
As mentioned earlier, we base our framework for facial expression recognition
on human visual system (HVS), which selects only few facial regions (salient)
to extract information. In order to determine the saliency of facial region(s)
for a particular expression, we conducted psycho-visual experiment with the
help of an eye-tracker. Next section briefly explains the psycho-visual
experimental study.
## 4\. Psycho-visual experiment
The aim of our experiment was to record the eye movement data of human
observers in free viewing conditions. The data were analyzed in order to find
which components of face are salient for specific displayed expression.
### 4.1. Participants, apparatus and stimuli
Eye movements of fifteen human observers were recorded using video based eye-
tracker (EyelinkII system, SR Research), as the subjects watched the
collection of 54 videos selected from the extended Cohn?Kanade (CK+) database(Lucey et al., 2010), showing one of the six universal facial expressions
(Ekman, 1971). Observers include both male and female aging from 20 to 45
years with normal or corrected to normal vision. All the observers were naïve
to the purpose of an experiment.
### 4.2. Eye movement recording
Eye position was tracked at 500 Hz with an average noise less than 0.01°. Head
mounted eye-tracker allows flexibility to perform the experiment in free
viewing conditions as the system is designed to compensate for small head
movements.
### 4.3. Psycho-visual experiment results
In order to statistically quantify which region is perceptually more
attractive for specific expression, we have calculated the average percentage
of trial time observers have fixated their gazes at specific region(s) in a
particular time period. As the stimuli used for the experiment is dynamic i.e.
video sequences, it would have been incorrect to average all the fixations
recorded during trial time (run length of the video) for the data analysis as
this could lead to biased analysis of the data. To meaningfully observe and
analyze the gaze trend across one video sequence we have divided each video
sequence in three mutually exclusive time periods. The first time period
correspond to initial frames of the video sequence i.e. neutral face. The last
time period encapsulates the frames where the expression is shown with full
intensity (apex frames). The second time period is a encapsulation of the
frames which has a transition of facial expression i.e. transition from
neutral face to the beginning of the desired expression (i.e. neutral to the
onset of the expression). Then the fixations recorded for a particular time
period are averaged across fifteen observers. For drawing the conclusions we
considered second and third time periods as they have the most significant
information in terms of specific displayed expression. Conclusions drawn are
summarized in Fig. 3. Refer Khan et al. (2012a) for the detailed explanation
of the psycho-visual experimental study.
1. Download: Download high-res image (235KB)
2. Download: Download full-size image
Fig. 3. Summary of the facial regions that emerged as salient for six
universal expressions. Salient regions are mentioned according to their
importance (for example facial expression of ?fear? has two salient regions
but mouth is the most important region according to HVS).
Conclusions drawn from this psycho-visual experimental study suggests that for
some expressions (i.e. happiness, sadness and surprise) only one facial region
is salient while for other expressions two facial regions are salient or
contain most discriminative information. We argue that the task of expression
analysis and recognition could be done in more conducive manner, if only same
perceptual salient regions are selected for further processing as it happens
in human visual system. By processing only perceptual salient regions the
proposed framework (refer Section 5) reduces the feature vector
dimensionality. This reduction in feature vector length makes the proposed
framework suitable for real-time applications due to minimized computational
complexity.
Machine learning research community has also proposed a mathematical model
called feature subset selection (FSS), to reduce feature vector
dimensionality. The objective of FSS is to reduce the number of features used
to characterize a dataset so as to improve a learning algorithm performance on
a given task (Aha and Bankert, 1994), and it has recently been used for facial
expression recognition application (Dornaika et al., 2011). Most FSS methods
involve evaluating different feature subsets, employ some criterion such asprobability of error (Jain and Zongker, 1997). One difficulty with this
approach when applied to real problems with large feature dimensionality, is
the high computational complexity involved in searching the exponential space
of feature subsets (Guo and Dyer, 2003). Jain and Zongker (1997) evaluated
different search algorithms for FSS and found that the sequential forward
floating selection (SFFS) algorithm proposed by Pudil et al. (1994) performed
best. However, SFFS is very time consuming when the number of features is
large. For example, Vailaya (2000) used the SFFS method to select 67 features
from 600 for a two-class problem and reported that SFFS required 12 days of
computation time.
As the proposed framework is based on psycho-visual experimental study and the
notion of saliency, it does not employ FSS technique to reduce feature vector
dimensionality. Rather reduction in feature vector dimensionality is inherited
due to the notion of saliency. Proposed framework (Section 5) extracts PLBP
features only from the perceptual salient facial region(s) which contains
highly discriminative information as suggested by Itti et al. (1998) and
proved by the recorded results (refer Table 1, Table 2). Specifically Table 1
proves that the dimensionality of the proposed descriptor is very low as
compared to other state-of-the-art descriptor. Thus, making it suitable for
real-time applications. In the perspective of this study, FSS still can be
utilized to reduce the dimensionality of proposed descriptor even futher (i.e.
to reduce dimensionality from hundreds to tens) on the cost of increasing
computational complexity.
Table 2. Comparison with the state-of-the-art methods for posed expressions.
Empty Cell| Sequence num.| Class num.| Performance measure| Recog. rate (%)
---|---|---|---|---
Littlewort et al. (2006)| 313| 7| Leave-one-out| 93.3
Zhao and Pietikäinen (2007)| 374| 6| 2-Fold| 95.19
Zhao and Pietikäinen (2007)| 374| 6| 10-Fold| 96.26
Kotsia et al. (2008)| 374| 6| 5-Fold| 94.5
Tian (2004)| 375| 6| ?| 93.8
Yang et al. (2010)(a)| 352| 6| 66% split| 92.3
Yang et al. (2010)(b)| 352| 6| 66% split| 80
**Ours**| **309**| **6**| **10-Fold**| **96.7**
**Ours**| **309**| **6**| **2-Fold**| **95.2**
## 5\. Expression recognition framework
Feature selection along with the region(s) from where these features are going
to be extracted is one of the most important step to recognize expressions. As
the proposed framework draws its inspiration from the human visual system
(HVS), it extracts proposed features i.e. PLBP, only from the perceptual
salient facial region(s) which were determined through psycho-visual
experiment. Schematic overview of the framework is presented in Fig. 4. Steps
of the proposed framework are as follows:
* 1.
The framework first localizes salient facial regions using Viola?Jones object
detection algorithm (Viola and Jones, 2001). We selected this algorithm as it
is the most cited and considered the fastest and most accurate pattern
recognition method for face and facial region detection (Kolsch and Turk,
2004).
* 2.
Then, the framework extracts PLBP features from the mouth region, feature
vector of 295 dimensions (f1,?,f295). The classification (?Classifier-a? in
Fig. 4) is carried out on the basis of extracted features in order to make two
groups of facial expressions. First group comprises of those expressions thathas one perceptual salient region i.e. happiness, sadness and surprise while
the second group is composed of those expressions that have two or more
perceptual salient regions i.e. anger, fear and disgust (see Section 4.3).
Purpose of making two groups of expressions is to reduce feature extraction
computational time.
* 3.
If the stimuli is classified in the first group, then it is classified either
as happiness, sadness or surprise by the ?Classifier-b? using already
extracted PLBP features from the mouth region.
* 4.
If the stimuli is classified in the second group, then the framework extracts
PLBP features from the eyes region and concatenates them with the already
extracted PLBP features from the mouth region, feature vector of 590
dimensions (f1,?,f295+f1,?,f295). Then, the concatenated feature vector is fed
to the classifier (?Classifier-c?) for the final classification. It is worth
mentioning here that for the expression of ?disgust? nose region emerged as
one of the salient regions but the framework do not explicitly extracts
features from this region. This is due to the fact that, the region of nose
that emerged as salient is the upper nose (wrinkles) area which is connected
and already included in the localization of the eyes region, refer Fig. 3.
1. Download: Download high-res image (182KB)
2. Download: Download full-size image
Fig. 4. Schematic overview of the framework.
## 6\. Experiment and results
We performed person-independent facial expression recognition using proposed
PLBP features.1 We performed four experiments to test different scenarios.
* 1.
First experiment was performed on the extended Cohn?Kanade (CK+) database
(Lucey et al., 2010). This database contains 593 sequences of posed universal
expressions.
* 2.
Second experiment was performed to test the performance of the proposed
framework on low resolution image sequences.
* 3.
Third experiment tests the robustness of the proposed framework when
generalizing on a new dataset.
* 4.
Fourth experiment was performed on the MMI facial expression database (parts
IV and V of the database) (Valstar and Pantic, 2010) which contains
spontaneous/natural expressions.
For the first two experiments we used all the 309 sequences from the CK+
database which have FACS coded expression label (Ekman and Friesen, 1978). The
experiment was carried out on the frames which covers the status of onset to
apex of the expression, as done by Yang et al. (2010). Region of interest was
obtained automatically by using Viola?Jones object detection algorithm (Viola
and Jones, 2001) and processed to obtain PLBP feature vector. We extracted LBP
features only from the salient region(s) using LBP8,2u2 operator which denotes
a uniform LBP operator with eight sampling pixels in a local neighborhood
region of radius 2. Only exception was in the second experiment, when we
adopted LBP4,1u2 operator when the spatial facial resolution gets smaller than
36 × 48.
In our framework we created image pyramid up to level 1, so in turn got five
sub-regions from one facial region i.e. mouth region (see Fig. 2). In total we
obtained 295 dimensional feature vector (59 dimensions/sub-region). Asmentioned earlier we adopted LBP4,1u2 operator when the spatial facial
resolution was 18 × 24. In this case we obtained 75 dimensional feature vector
(15 dimensions/sub-region).
We recorded correct classification accuracy in the range of 95% for image
pyramid level 1. We decided not to test framework with further image pyramid
levels as it would double the size of feature vector and thus increase the
feature extraction time and likely would add few percents in the accuracy of
the framework which will be insignificant for a framework holistically.
### 6.1. First experiment: posed expressions
This experiment measures the performance of the proposed framework on the
classical database i.e. extended Cohn?Kanade (CK+) database (Lucey et al.,
2010). Most of the methods in literature report their performance on this
database, so this experiment could be considered as the benchmark experiment
for facial expression recognition framework.
The performance of the framework was evaluated for five different classifiers:
* 1.
Support vector machine (SVM) with _?_ 2 kernel and ?=1
* 2.
C4.5 decision tree (DT) with reduced-error pruning
* 3.
Random forest (RF) of 10 trees
* 4.
2 Nearest neighbor (2NN) based on Euclidean distance
* 5.
Naive Bayes (NB) classifier
Above mentioned classifiers are briefly described below.
Support vector machine _(SVM)._ SVM performs an implicit mapping of data into
a higher dimensional feature space, and then finds a linear separating
hyperplane with the maximal margin to separate data in this higher dimensional
space (Vapnik, 1995). Given a training set of labeled examples
{(xi,yi),i=1,?,l} where xi?Rn and yi?{-1,1}, a new test example _x_ is
classified by the following function:(6)f(x)=sgn?i=1l?iyiK(xi,x)+bwhere ?i are
Langrange multipliers of a dual optimization problem that describe the
separating hyperplane, K(.,.) is a kernel function, and _b_ is the threshold
parameter of the hyperplane. We used Chi-Square kernel as it is best suited
for histograms. It is given by:(7)K(x,y)=1-?i2×(xi-yi)2(xi+yi)
_Classification Trees._ A classification tree is a classifier composed by
nodes and branches which break the set of samples into a set of covering
decision rules. In each node, a single test is made to obtain the partition.
The starting node is called the root of the tree. In the final nodes or
leaves, a decision about the classification of the case is made. In this work,
we have used C4.5 paradigm (Quinlan, 1993). Random forest (RFs) are
collections of decision trees (DTs) that have been constructed randomly. RFs
generally performs better than DT on unseen data.
_Instance Based Learning. k_ -NN classifiers are instance-based algorithms
taking a conceptually straightforward approach to approximate real or discrete
valued target functions. The learning process consists in simply storing the
presented data. All instances correspond to points in an _n_ -dimensional
space and the nearest neighbors of a given query are defined in terms of the
standard Euclidean distance. The probability of a query _q_ belonging to a
class _c_ can be calculated as
follows:(8)p(c|q)=?k?KWk·1(kc=c)?k?KWk(9)Wk=1d(k,q)_K_ is the set of nearest
neighbors, _kc_ the class of _k_ and d(k,q) the Euclidean distance of _k_ from
_q_._Naive Bayes classifiers._ The naive-Bayes (NB) classifier uses the Bayes
theorem to predict the class for each case, assuming that the predictive genes
are independent given the category. To classify a new sample characterized by
_d_ genes X=(X1,X2,?,Xd), the NB classifier applies the following
rule:(10)CN-B=argmaxcj?Cp(cj)?i=1dp(xi|cj)where CN-B denotes the class label
predicted by the naive-Bayes classifier and the possible classes of the
problem are grouped in C={c1,?,cl}.
#### 6.1.1. Results
The framework achieved average recognition rate of 96.7%, 97.9%, 96.2%, 94.7%
and 90.2% for SVM, 2 nearest neighbor (2NN), random forest (RF), C4.5 decision
tree (DT) and naive-Bayes (NB) respectively using 10-fold cross validation
technique. One of the most interesting aspects of our approach is that it
gives excellent results for a simple 2NN classifier which is a non-parametric
method. This points to the fact that framework do not need computationally
expensive methods such as SVM, random forests or decision trees to obtain good
results. In general, the proposed framework achieved high expression
recognition accuracies irrespective of the classifiers, proves the descriptive
strength of the extracted features (feature minimizes within-class variations
of expressions, while maximizes between class variations). For comparison and
reporting results, we have used the classification results obtained by the SVM
as it is the most cited method for classification in the literature.
#### 6.1.2. Comparisons
We chose to compare average recognition performance of our framework with the
framework proposed by Shan et al. (2009) with different SVM kernels. Our
choice was based on the fact that both have common underlying descriptor i.e.
local binary pattern (LBP), secondly framework proposed by Shan et al. (2009)
is highly cited in the literature. Our framework obtained average recognition
percentage of 93.5% for SVM linear kernel while for the same kernel Shan et
al. (2009) have reported 91.5%. For SVM with polynomial kernel and SVM with
RBF kernel our framework achieved recognition accuracy of 94.7% and 94.9%
respectively, as compared to 91.5% and 92.6%.
In terms of time and memory cost of feature extraction process, we have
measured and compared our descriptor with the frameworks proposed by Shan et
al. (2009) and Bartlett et al. (2003). The results are presented in Table 1.
Shan et al. (2009) have reported their results for two set of features i.e.
LBP and Gabor. In Table 1 ?Shan et al. (2009)(a)? corresponds to results
obtained with LBP features, while ?Shan et al. (2009)(b)? corresponds to Gabor
feature result. Table 1 shows the effectiveness of the proposed descriptor for
facial feature analysis i.e. PLBP, for real-time applications as it is memory
efficient and its extraction time is much lower than other compared descriptor
(see Section 5 for the dimensionality calculation). In Table 1 feature
dimension reported are stored in a data type ?float? and float occupies four
bytes. The proposed framework is compared with the other state-of the-art
frameworks using same database (i.e. Cohn?Kanade database) and the results are
presented in Table 2.
Table 2 shows the comparison of the achieved average recognition rate of the
proposed framework with the state-of-the-art methods using same database (i.e.
Cohn?Kanade database). Results from Yang et al. (2010) are presented for the
two configurations. ?Yang et al. (2010)(a)? shows the result when the method
was evaluated for the last three frames from the sequence while ?Yang et al.
(2010)(b)? presents the reported result for the frames which encompasses the
status from onset to apex of the expression. It can be observed from Table 2
that the proposed framework is comparable to any other state-of-the-art method
in terms of expression recognition accuracy. The method discussed in ?Yang etal. (2010)(b)? is directly comparable to our method (frames which covers the
status of onset to apex of the expression). In this configuration, our
framework is better in terms of average recognition accuracy.
In general, Table 1, Table 2 show that the framework is better than the state-
of-the-art frameworks in terms of average expression recognition performance,
time and memory costs of feature extraction processes. These results show that
the system could be used with the high degree of confidence for real-time
applications as its unoptimized Matlab implementation runs at more than 30
frames/second.
### 6.2. Second experiment: low resolution image sequences
Most of the existing state-of-the-art systems for expressions recognition
report their results on high resolution images without reporting results on
low resolution images. As mentioned earlier there are many real world
applications that require expression recognition system to work amicably on
low resolution images. Smart meeting, video conferencing and visual
surveillance are some examples of such applications. To compare with Tian?s
work (Tian, 2004), we tested our proposed framework on low resolution images
of four different facial resolutions (144 × 192, 72 × 96, 36 × 48, 18 × 24)
based on Cohn?Kanade database. Tian?s work can be considered as the pioneering
work for low resolution image facial expression recognition. Fig. 5 shows the
images at different spatial resolution along with the average recognition
accuracy achieved by the different methods. Low resolution image sequences
were obtained by down sampling the original sequences. All the other
experimental parameters i.e. descriptor, number of sequences and region of
interest, were same as mentioned earlier in the Section 6.
1. Download: Download high-res image (189KB)
2. Download: Download full-size image
Fig. 5. Robustness of different methods for facial expression recognition with
decreasing image resolution. PHOG[ICIP] corresponds to framework proposed by
Khan et al. (2012b), Gabor[CVPRW] corresponds to Tian?s work (Tian, 2004),
LBP[JIVC] and Gabor[JIVC] corresponds to results reported by Shan et al.
(2009)
Fig. 5 reports the recognition results of the proposed framework with the
state-of-the-art methods on four different low facial resolution images.
Reported results of our proposed method i.e. are obtained using support vector
machine (SVM) with _?_ 2 kernel and ?=1. In Fig. 5 recognition curve for our
proposed method is shown as _PLBP-SVM_ , recognition curves of LBP (Shan et
al., 2009) and Gabor (Shan et al., 2009) are shown as _LBP[JIVC]_ and
_Gabor[JIVC]_ respectively, curve for Tian?s work (Tian, 2004) is shown as
_Gabor[CVPRW]_ while Khan et al. (2012b) proposed system?s curve is shown as
_PHOG[ICIP]_. Results reports in LBP (Shan et al., 2009) and Gabor (Shan et
al., 2009), the different facial image resolution are 110 × 150, 55 × 75, 27 ×
37 and 14 × 19 which are comparable to the resolutions of 144 × 192, 72 × 96,
36 × 48, 18 × 24 pixels in our experiment. Referenced figure shows the
supremacy of the proposed framework for low resolution images. Specially for
the smallest tested facial image resolution (18 × 24) our framework performs
much better than any other compared state-of-the-art method.
Results from the first and second experiment show that the proposed framework
for facial expression recognition works amicably on classical dataset (CK
dataset) and its performance is not effected significantly for low resolution
images. Secondly, the framework has a very low memory requirement and thus it
can be utilized for real-time applications.
### 6.3. Third experiment: generalization on the new dataset
The aim of this experiment is to study how well the proposed frameworkgeneralizes on the new dataset. Valstar et al. (2005) have reported such data
earlier to show generalization ability of their expression recognition system.
Thus, this experiment helps to understand how the framework will behave when
it will be used to classify expressions in real life videos. We used image
sequences from CK+ dataset and FG-NET FEED (facial expressions and emotion
database) (Wallhoff, 2006). FG-NET FEED contains 399 video sequences across 18
different individuals showing seven facial expressions i.e. six universal
expression (Ekman, 1971) plus one neutral. In this dataset individuals were
not asked to act rather expressions were captured while showing them video
clips or still images i.e. natural expressions.
The experiment was carried out on the frames which covers the status of onset
to apex of the expression as done in the previous experiment. This experiment
was performed in two different scenarios, with the same classifier parameters
as the first experiment:
* (a)
In the first scenario samples from the CK+ database were used for the training
of different classifiers and samples from FG-NET FEED (Wallhoff, 2006) were
used for the testing. Obtained results are presented in Table 3.
Table 3. Average recognition accuracy (%).
Empty Cell| SVM| C4.5 DT| RF| 2NN
---|---|---|---|---
_Training on CK+ database and testing it with FG-NET FEED_
Training samples| 96.7| 94.7| 96.2| 97.9
Test samples| 81.9| 74.8| 79.5| 83.1
_Training on FG-NET FEED and testing it with CK+ database_
Training samples| 92.3| 91.2| 90.5| 93.3
Test samples| 80.5| 77.3| 79| 84.7
* (b)
In the second scenario we used samples from the FG-NET FEED for the training
and testing was carried out with the CK+ database samples. Results obtained
are presented in last two rows of Table 3.
This experiment simulates the real life situation when the framework would be
employed to recognize facial expressions on the unseen data. Obtained results
are presented in Table 3. Reported average recognition percentages for
training phase were calculated using 10-fold cross validation method. It can
be observed from the result that generally average recognition accuracy drops
by 15% across different classifiers during testing phase. This could be due to
the fact that the two databases used for training and testing phase have
inherent differences i.e. played emotions differ from the natural/spontaneous
ones. Obtained results can be probably further improved by training
classifiers on more than one dataset before using in real life scenario.
### 6.4. Fourth experiment: spontaneous expressions
Spontaneous/natural facial expressions differ substantially from posed
expressions (Bartlett et al., 2002). The same has also been proved by
psychophysical work (Ekman, 2001). To test the performance of the proposed
framework on the spontaneous facial expressions we used 392 video segments
from parts IV and V of the MMI facial expression database (Valstar and Pantic,
2010). Parts IV and V of the database contains spontaneous/naturalistic
expressions recorded from 25 participants aged between 20 and 32 years in two
different settings. Due to ethical concerns the database contains only the
video recording of the expressions of happiness, surprise and disgust (Valstar
and Pantic, 2010).
The framework achieved average recognition rate of 91%, 91.4%, 90.3% and 88%
for SVM, 2-nearest neighbor, random forest and C4.5 decision tree respectivelyusing 10-fold cross validation technique. Algorithm of Park and Kim (2008) for
spontaneous expression recognition achieved results for three expressions in
the range of 56?88% for four different configurations which is less than
recognition rate of our proposed algorithm, although results cannot be
compared directly as they used different database.
## 7\. Conclusions and future work
We presented a novel descriptor and framework for automatic and reliable
facial expression recognition. Framework is based on initial study of human
vision and works adequately on posed as well as on spontaneous expressions.
The key conclusions drawn from the study are:
* 1.
Facial expressions can be analyzed automatically by mimicking human visual
system i.e. extracting features only from the salient facial regions.
* 2.
Features extracted using proposed pyramidal local binary pattern (PLBP)
operator have strong discriminative ability as the recognition result for six
universal expressions is not effected by the choice of classifier.
* 3.
The proposed framework is robust for low resolution images, spontaneous
expressions and generalizes well on unseen data.
* 4.
The proposed framework can be used for real-time applications since its
unoptimized Matlab implementation run at more than 30 frames/second on a
Windows 64 bit machine with i7 processor running at 2.4 GHz having 6 GB of
RAM.
In future we plan to investigate the effect of occlusion as this parameter
could significantly impact the performance of the framework for real world
applications. Secondly, the notion of movement could improve the performance
of the proposed framework for real world applications as the experimental
study conducted by Bassili (1979) suggested that dynamic information is
important for facial expression recognition. Another parameter that needs to
be investigated is the variations of camera angle as for many applications
frontal facial pose is difficult to record. Lastly, in future we would also
like to evaluate the proposed framework on streaming artifacts i.e. ringing,
contouring, posterization, etc.
## Acknowledgment
This research work is funded by the Région Rhône-Alpes, France through ARC 6.
Recommended articles"
165,167,Fusing aligned and non-aligned face information for automatic affect recognition in the wild: a deep learning approach,"['BK Kim', 'SY Dong', 'J Roh', 'G Kim']",2016,139,Expression in-the-Wild,deep learning,We evaluate the proposed framework on the facial expression recognition 2013 (FER-2013)   Our final ensemble-based FER system achieves great performance on this in-the-wild,No DOI,Proceedings of the IEEE …,http://ieeexplore.ieee.org/abstract/document/7789677/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
166,168,Fusion of feature sets and classifiers for facial expression recognition,"['THH Zavaschi', 'AS Britto Jr', 'LES Oliveira']",2013,167,"Extended Cohn-Kanade, Facial Expression Recognition 2013","classifier, facial expression recognition",classifiers based on the under-pinning concept of “over-produce and choose”. The pool of  base classifiers  different databases (JAFFE and Cohn-Kanade) we demonstrate the efficiency,No DOI,Expert Systems with …,https://www.sciencedirect.com/science/article/pii/S095741741200930X,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
167,169,Generative moment matching networks,"['Y Li', 'K Swersky', 'R Zemel']",2015,1034,Toronto Face Database,machine learning,"On MNIST and the Toronto Face Dataset (TFD) we demonstrate improved results over  comparable baselines, including GANs. Source code for training GMMNs is available at https://",No DOI,… conference on machine learning,https://arxiv.org/abs/1502.02761,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
168,170,Geometric feature-based facial expression recognition in image sequences using multi-class adaboost and support vector machines,"['D Ghimire', 'J Lee']",2013,350,Facial Expression Recognition 2013,facial expression recognition,"accuracy, by using support vector machine (SVM) classifiers with boosted-LBP features.  An extension of the LBP operator, volume LBP (VLBP) and LBP on three orthogonal planes (LBP",No DOI,Sensors,https://www.mdpi.com/1424-8220/13/6/7714,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
169,171,Going deeper in facial expression recognition using deep neural networks,"['A Mollahosseini', 'D Chan']",2016,1245,"Acted Facial Expressions In The Wild, Affective Faces Database, Facial Expression Recognition 2013, MMI Facial Expression, Static Facial Expression in the Wild, Toronto Face Database","CNN, FER, classification, classifier, deep learning, facial expression recognition, machine learning, neural network","We evaluate the proposed method on well-known publicly available facial expression  databases: CMU MultiPIE [15], MMI [36], Denver Intensity of Spontaneous Facial Actions (DISFA) [",No DOI,2016 IEEE Winter …,https://arxiv.org/abs/1511.04110,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,
170,172,Hard negative generation for identity-disentangled facial expression recognition,"['X Liu', 'BVKV Kumar', 'P Jia', 'J You']",2019,107,"CMU Multi-PIE, MMI Facial Expression","FER, facial expression recognition","representations of RML achieve superior performance on the CK + , MMI and Oulu-CASIA   The performances of the FER systems usually depend heavily on facial expression",No DOI,Pattern Recognition,https://www.sciencedirect.com/science/article/pii/S0031320318303819,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
171,173,Head pose estimation in the wild using convolutional neural networks and adaptive gradient methods,"['M Patacchiola', 'A Cangelosi']",2017,255,Expression in-the-Wild,neural network,"an approach based on Convolutional Neural Networks (CNNs)  on recently released in-the-wild  datasets. Moreover, we  ) with MAE expressed in degrees and Accuracy expressed in",No DOI,Pattern Recognition,https://www.sciencedirect.com/science/article/pii/S0031320317302327,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False,"## Title: Head pose estimation in the wild using Convolutional Neural
Networks and adaptive gradient methods
the features on this page.
Skip to main contentSkip to article
My account
Sign in
* Access through **your organization**
* View Open Manuscript
* * Purchase PDF
* Other access options
Search 
## Article preview
* Abstract
* Introduction
* Section snippets
* References (47)
* Cited by (191)
## Pattern Recognition
Volume 71, November 2017, Pages 132-143
# Head pose estimation in the wild using Convolutional Neural Networks and
adaptive gradient methods
Author links open overlay panelMassimiliano Patacchiola, Angelo Cangelosi
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.patcog.2017.06.009Get rights and content
## Highlights
* ?
A convolutional neural network approach for head pose estimation is proposed.
* ?
The performance of different network architectures has been measured.
* ?
The use of adaptive gradient methods leads to the state-of-the-art in wild
datasets.
* ?
We release a library based on our work which is available under open source
licence.
## Abstract
Head pose estimation is an old problem that is recently receiving new
attention because of possible applications in human-robot interaction,
augmented reality and driving assistance. However, most of the existing work
has been tested in controlled environments and is not robust enough for real-
world applications. In order to handle these limitations we propose an
approach based on Convolutional Neural Networks (CNNs) supplemented with the
most recent techniques adopted from the deep learning community. We evaluatethe performance of four architectures on recently released in-the-wild
datasets. Moreover, we investigate the use of dropout and adaptive gradient
methods giving a contribution to their ongoing validation. The results show
that joining CNNs and adaptive gradient methods leads to the state-of-the-art
in unconstrained head pose estimation.
## Introduction
In the last few years major advancements in robotics, augmented reality and
driving assistance have highlighted the need for robust methods to estimate
the head pose in real-world scenarios. For instance, robots are gradually
leaving factories and becoming part of our lives as companions and as
assistants. It has been shown that in human-robot interaction a coarse pose
estimation of the head is a fundamental prerequisite for building trust with
users during joint-attention tasks [1]. In the context of autonomous cars a
driving assistance system could take advantage of head pose estimation for
decelerating the car when pedestrians do not notice the presence of the
vehicle [2]. Moreover a similar system can be installed inside the vehicle and
used to monitor the driver?s awareness. The need of a robust head pose
estimation is not limited to these domains. There have been significant
applications in surveillance and anomaly detection, human-computer interaction
and crowd behavioural dynamics analysis [3]. All of these unconstrained
scenarios need an estimator which is resistant to variable environmental
conditions, and which can evaluate the focus of attention in absence of more
accurate information such as the gaze. Here it is necessary to specify what we
consider as a wild environment. We define as taken in a wild environment those
face images exhibiting a large variety in appearance (pose, expression,
ethnicity, age, gender, etc.), environmental conditions (artificial light,
shadows, etc.), and containing relevant occlusions (sunglasses, masks,
scarves, etc.). We will show how Convolutional Neural Networks (CNNs) can be
considered one of the best algorithms for robust head pose estimation in a
wild environment.
We can summarise the main contribution of our work in three points:
* 1.
As far as we know this is the first work that has deeply investigated the use
of CNNs in head pose estimation. Our main contribution is a rigorous
evaluation of multiple CNN models and factors. The results are compared with
other algorithms, and show how an approach based on CNNs, dropout and adaptive
gradient methods represents the state of the art in head pose estimation.
* 2.
Deep learning is a rapidly growing field, which is bringing new techniques
that can significantly improve the performance of CNNs. Because these
techniques have been released in the last few years, there is still a
validation process for establishing their cross-domain usefulness. We explored
the role of adaptive gradient methods and we gave a valuable contribution to
their ongoing validation.
* 3.
The results obtained in this work have been used to implement a Python library
called Deepgaze. The library includes pre-trained CNNs based on Tensorflow [4]
which can run in real-time on GPUs and mobile devices. Deepgaze is released
under an open-source license and is available for both academic and commercial
purposes. The software is available on the author?s repository.1
## Section snippets
## Related work
The head pose estimation problem has been investigated from different points
of view and with different techniques. Devices such as laser pointers, cameraarrays, stereo-cameras, magnetic and inertial sensors, have been used to get a
stable estimation in controlled situations [5]. More recently some good
results have been obtained with commercial depth cameras [6]. However the use
of these devices is not always feasible due to space constraints and to
technical problems when operating outdoors.
## Convolutional Neural Networks
In recent years deep convolutional networks have showed their strength in
numerous pattern recognition contests. Some remarkable achievements have been
recently obtained in object detection [22], facial expression recognition [23]
and scene classification [24]. This technology is increasingly used in
commercial applications such as content filtering in social networks,
recommendation systems in e-commerce websites or image classifiers in web-
search engines. The deep learning revolution has been
## Experiments
In this section we report the results obtained using CNNs, dropout and
adaptive gradient methods on three public datasets: the Prima head-pose
dataset [32], the Annotated Facial Landmarks in the Wild (AFLW) dataset [33],
and the Annotated Face in the Wild (AFW) dataset [34]. The former is a well-
known dataset which has been around for more than ten years, and it is
considered a classic benchmark for head pose algorithms. The second is a
recently released in-the-wild dataset, and it has the
## Conclusions
In this article we introduced the use of dropout and adaptive gradient methods
for head pose estimation with CNNs. Our approach is significantly different
from previous research [17], [18], [20], [21], and show how using the most
recent deep learning techniques leads to the state-of-the-art in constrained
and unconstrained datasets. Our method should be considered as part of a
broader system, in particular it can be used in conjunction with a face
detector. We implemented the system in Python
## Acknowledgment
This material is based upon work supported by the Air Force Office of
Scientific Research grant number: A9550-15-1-0025, Air Force Materiel Command,
USAF under Award No. FA9550-15-1-0025.
We gratefully acknowledge the support of NVIDIA Corporation with the donation
of the Tesla K40 GPU used for this research.
**Massimiliano Patacchiola** attended his studies at La Sapienza University
(Rome). After his internship at the Laboratory of Artificial Life and Robotics
(Rome), in 2012 he started working as robotics engineer at Eurolink Systems
group (Rome), where he spent more than two years creating algorithms and
designing systems for the control of UGV (Unmanned Ground Vehicle) and UAV
(Unmanned Aerial Vehicle). In 2015 he started a PhD program in robotics and
computational modelling at Plymouth University.
Recommended articles"
172,174,Hierarchical committee of deep cnns with exponentially-weighted decision fusion for static facial expression recognition,"['BK Kim', 'H Lee', 'J Roh', 'SY Lee']",2015,167,Static Facial Expression in the Wild,facial expression recognition,We present a pattern recognition framework to improve  its application to static facial  expression recognition in the wild ( released for the 3rd Emotion Recognition in the Wild (EmotiW),No DOI,Proceedings of the 2015 ACM on …,https://dl.acm.org/doi/10.1145/2818346.2830590,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
173,175,Hierarchical committee of deep convolutional neural networks for robust facial expression recognition,"['BK Kim', 'J Roh', 'SY Dong', 'SY Lee']",2016,307,"Facial Expression Recognition 2013, Static Facial Expression in the Wild","facial expression recognition, neural network","-video emotion recognition (acted facial expression recognition in the wild, AFEW), whereas   : AFEW and image-based static facial expression recognition in the wild (SFEW). We",No DOI,Journal on Multimodal User Interfaces,https://link.springer.com/article/10.1007/s12193-015-0209-0,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
174,176,How we've taught algorithms to see identity: Constructing race and gender in image databases for facial analysis,"['MK Scheuerman', 'K Wade', 'C Lustig']",2020,211,Radboud Faces Database,"classifier, machine learning","to current database annotation approaches in machine learning  of machine learning model  building: the data it is limited to  For example, Radboud Faces Database (RAFD) contained",No DOI,Proceedings of the ACM …,https://dl.acm.org/doi/10.1145/3392866,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
175,177,Human behavior understanding in big multimedia data using CNN based facial expression recognition,"['M Sajjad', 'S Zahir', 'A Ullah', 'Z Akhtar']",2020,104,Facial Expression Recognition 2013,facial expression recognition,facial recognition and expressions. The subjective and objective experimental evaluations  prove better performance for both facial expression recognition  In: 2013 IEEE International,No DOI,Mobile networks and …,https://link.springer.com/article/10.1007/s11036-019-01366-9,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
176,178,Human-computer interaction using emotion recognition from facial expression,"['F Abdat', 'C Maaoui', 'A Pruski']",2011,137,Affective Faces Database,facial expression recognition,"Our facial expression recognition system is presented in section 3,  to emotion recognition  based on facial expression analysis.To detect the face in the image, we have used the face",No DOI,2011 UKSim 5th European …,https://ieeexplore.ieee.org/document/6131215,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
177,179,Hybrid deep neural networks for face emotion recognition,"['N Jain', 'S Kumar', 'A Kumar', 'P Shamsolmoali']",2018,312,"Facial Expression Recognition 2013, Toronto Face Database","deep learning, facial expression recognition, neural network","This paper proposed a deep learning technique in the  Now a day's due to quantity and  variety of datasets, deep learning  have just utilized the extra dataset for the training. Accordingly,",No DOI,Pattern Recognition …,https://www.sciencedirect.com/science/article/pii/S0167865518301302,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
178,180,Identity-adaptive facial expression recognition through expression regeneration using conditional generative adversarial networks,"['H Yang', 'Z Zhang', 'L Yin']",2018,132,"Binghamton University 3D Facial Expression, Oulu-CASIA","FER, classification","(FER-Net) is finetuned for expression classification. After the corresponding prototypic facial  expressions are regenerated from each facial image, we output the last FC layer of FER-Net",No DOI,… Conference on Automatic Face & …,http://ieeexplore.ieee.org/document/8373843/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
179,181,Identity-aware convolutional neural network for facial expression recognition,"['Z Meng', 'P Liu', 'J Cai', 'S Han']",2017,368,"Facial Expression Recognition 2013, MMI Facial Expression, Static Facial Expression in the Wild","CNN, facial expression recognition, neural network","of a neuron with probability of 0.6. In Eq. 8, λ2 is set to 5 for CK+/SFEW and 2 for MMI, while   Labeled subject IDs are provided in the CK+ and MMI databases and we manually labeled",No DOI,… on Automatic Face & …,https://ieeexplore.ieee.org/document/7961791,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
180,182,"Illusionary order: Online databases, optical character recognition, and Canadian history, 1997–2010",['I Milligan'],2013,127,Toronto Face Database,classification,", and the Toronto Star online. Yet the issues of poor ocr in these databases make this a very   , the decision was made to adhere to ProQuest's classification system (which is based upon",No DOI,Canadian Historical Review,https://www.utpjournals.press/doi/abs/10.3138/chr.694,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
181,183,Image based facial micro-expression recognition using deep learning on small datasets,"['MA Takalkar', 'M Xu']",2017,103,Affective Faces Database,deep learning,"It is not straightforward to recognise the genuine emotion shown on one’s face. Thus recognising   II database, we implemented the DLib face detector in OpenCV to detect and crop the",No DOI,2017 international conference on digital …,https://ieeexplore.ieee.org/document/8227443,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
182,184,Image based static facial expression recognition with multiple deep network learning,"['Z Yu', 'C Zhang']",2015,736,"Acted Facial Expressions In The Wild, CMU Multi-PIE, Facial Expression Recognition 2013, Static Facial Expression in the Wild, Toronto Face Database","CNN, FER, classification, classifier, deep learning, facial expression recognition, machine learning, neural network",Two large datasets: the Toronto Face Dataset and the Google dataset were combined to  train the CNN network. The Google dataset happens to be the very dataset provided to FER-,No DOI,Proceedings of the 2015 ACM on international …,https://dl.acm.org/doi/10.1145/2818346.2830595,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
183,185,Image ratio features for facial expression recognition application,"['M Song', 'D Tao', 'Z Liu', 'X Li']",2009,127,"Japanese Female Facial Expression, Toronto Face Database","classification, classifier, facial expression recognition","database, and the Japanese Female Facial Expression database.  asymmetric facial  expressions based on our own facial expression  of our combined expression recognition system.",No DOI,IEEE Transactions on …,https://pubmed.ncbi.nlm.nih.gov/19884092/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
184,186,Impact of deep learning approaches on facial expression recognition in healthcare industries,"['C Bisogni', 'A Castiglione', 'S Hossain']",2022,109,Static Facial Expression in the Wild,"deep learning, machine learning","For experimental purposes, three benchmark databases, static facial expressions in the  wild, Cohn-Kanade, and Karolinska directed emotional faces, are employed with some existing",No DOI,IEEE Transactions …,https://ieeexplore.ieee.org/document/9674818,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
185,187,"Induced disgust, happiness and surprise: an addition to the mmi facial expression database","['M Valstar', 'M Pantic']",2010,695,MMI Facial Expression,"deep learning, facial expression recognition, machine learning",notation is valuable for researchers who wish to build automatic basic emotion detection  systems. Not all affective states can be categorised into one of the six basic emotions.,No DOI,Proc. 3rd Intern. Workshop on EMOTION  …,https://www.researchgate.net/publication/284473673_Induced_disgust_happiness_and_surprise_An_addition_to_the_mmi_facial_expression_database,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
186,188,"Integrating faces, fingerprints, and soft biometric traits for user recognition","['AK Jain', 'K Nandakumar', 'X Lu', 'U Park']",2004,199,Toronto Face Database,classification,"an algorithm for age classification from facial images based on cranio-facial changes in  feature- However, they do not provide any accuracy estimates for their classification scheme.",No DOI,Biometric Authentication: ECCV …,https://link.springer.com/chapter/10.1007/978-3-540-25976-3_24,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
187,189,Intra-class variation reduction using training expression images for sparse representation based facial expression recognition,"['SH Lee', 'KN Plataniotis', 'YM Ro']",2014,141,"CMU Multi-PIE, Toronto Face Database","FER, classification, classifier, facial expression recognition",", “A 3D facial expression database for facial behavior research,”  Toronto, Toronto, ON,  Canada. His research interests include face detection/tracking, facial expression recognition, face",No DOI,IEEE Transactions on …,https://ieeexplore.ieee.org/document/6874505,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
188,190,Joint fine-tuning in deep neural networks for facial expression recognition,"['H Jung', 'S Lee', 'J Yim', 'S Park', 'J Kim']",2015,933,"Facial Expression Recognition 2013, MMI Facial Expression, Oulu-CASIA","deep learning, facial expression recognition, neural network",We designed our network with a moderate depth and a moderate number of parameters   facial expression recognition database is too small— there are only 205 sequences in the MMI,No DOI,Proceedings of the IEEE …,https://ieeexplore.ieee.org/document/7410698,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
189,191,Karolinska directed emotional faces,"['D Lundqvist', 'A Flykt', 'A Öhman']",1998,3821,Karolinska Directed Emotional Faces,facial expression recognition,"All the participants were instructed to try to evoke the emotion that was to be expressed  and to make the expression strong and clear. In a validation study (Goeleven et al., 2008), a",No DOI,Cognition and Emotion,https://kdef.se/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
190,192,Learning affective features with a hybrid deep model for audio–visual emotion recognition,"['S Zhang', 'S Zhang', 'T Huang', 'W Gao']",2017,346,Affective Faces Database,CNN,"tasks to initialize our CNN and 3D-CNN, respectively. Then,  For each frame in the video  segment, we run face detection,  audio-visual face database of affective and mental states,” IEEE",No DOI,IEEE transactions on …,https://ieeexplore.ieee.org/document/7956190,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
191,193,Learning affective video features for facial expression recognition via hybrid deep learning,"['S Zhang', 'X Pan', 'Y Cui', 'X Zhao', 'L Liu']",2019,147,MMI Facial Expression,"CNN, deep learning, machine learning","This deep fusion network is used to jointly learn  Vector Machine (SVM) is employed for  facial expression classification  -based facial expression datasets, ie, BAUM-1s, RML and MMI,",No DOI,IEEE Access,http://ieeexplore.ieee.org/document/8658192/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
192,194,Learning deep global multi-scale and local attention features for facial expression recognition in the wild,"['Z Zhao', 'Q Liu', 'S Wang']",2021,223,"Expression in-the-Wild, Static Facial Expression in the Wild","FER, deep learning, facial expression recognition","This paper aims at static FER in the wild, in which occlusion and pose are two key issues,   Fan, JC Lam, and VO Li, “Video-based emotion recognition using deeply-supervised neural",No DOI,IEEE Transactions on Image …,https://ieeexplore.ieee.org/document/9474949,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
193,195,Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition,"['M Liu', 'S Shan', 'R Wang', 'X Chen']",2014,455,MMI Facial Expression,facial expression recognition,"We demonstrate the proposed method for expression recognition on four databases: CK+,  MMI, Oulu-CASIA VIS, and AFEW. As shown in Table 1,2,3,4, we separate the results into",No DOI,… vision and pattern recognition,https://openaccess.thecvf.com/content_cvpr_2014/papers/Liu_Learning_Expressionlets_on_2014_CVPR_paper.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
194,196,Learning to disentangle factors of variation with manifold interaction,"['S Reed', 'K Sohn', 'Y Zhang']",2014,287,CMU Multi-PIE,"deep learning, machine learning",groups of hidden units that each learn to encode a distinct factor  disentangled features  learned on the CMU Multi-PIE dataset.  in pose estimation and face verification on CMU Multi-PIE.,No DOI,… on machine learning,http://proceedings.mlr.press/v32/reed14.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
195,197,Learning to optimize neural nets,"['K Li', 'J Malik']",2017,150,Toronto Face Database,neural network,"in stochasticity of gradients and the neural net architecture. More specifically, we  neural  net on MNIST generalizes to the problems of training neural nets on the Toronto Faces Dataset",No DOI,arXiv preprint arXiv:1703.00441,https://arxiv.org/abs/1703.00441,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
196,198,Local learning with deep and handcrafted features for facial expression recognition,"['MI Georgescu', 'RT Ionescu', 'M Popescu']",2019,346,Facial Expression Recognition 2013,facial expression recognition,"We perform a thorough experimental study on the 2013 Facial Expression Recognition (FER)  Challenge data set [27], the FER+ data set [1], and the AffectNet [28] data set, comparing",No DOI,IEEE Access,https://ieeexplore.ieee.org/abstract/document/8716652/,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
197,199,Machine learning methods for fully automatic recognition of facial expressions and facial actions,"['MS Bartlett', 'G Littlewort', 'C Lainscsek']",2004,216,Extended Cohn-Kanade,machine learning,"A second version of the system detects 18 action units of the Facial Action Coding  System (FAcs), we conducted cal investigations of machine learning methods applied to this",No DOI,"… on Systems, Man …",https://www.researchgate.net/publication/220755152_Machine_Learning_Methods_for_Fully_Automatic_Recognition_of_Facial_Expressions_and_Facial_Actions,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
198,200,Machine learning models that remember too much,"['C Song', 'T Ristenpart', 'V Shmatikov']",2017,622,Toronto Face Database,machine learning,a machine learning pipeline consists of several steps shown in Figure 1. The pipeline starts  with a set of labeled data  We exploit the fact that modern machine learning models have vast,No DOI,Proceedings of the 2017 ACM …,https://arxiv.org/abs/1709.07886,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
199,201,Mapping the development of facial expression recognition,"['H Rodger', 'L Vizioli', 'X Ouyang']",2015,194,Facial Expression Recognition 2013,facial expression recognition,"Generally, corresponding with our findings, children have been shown to perform well in  recognizing sadness (Herba & Phillips, 2004; Widen, 2013). Several studies have shown that",No DOI,Developmental …,https://pubmed.ncbi.nlm.nih.gov/25704672/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
200,202,Matsumoto and Ekman's Japanese and Caucasian Facial Expressions of Emotion (JACFEE): Reliability data and cross-national differences,"['M Biehl', 'D Matsumoto', 'P Ekman', 'V Hearn']",1997,734,Japanese Female Facial Expression,"classification, classifier","in both recognition data and intensity ratings. We, however, believe this classification to  be  Is there universal recognition of emotion from facial expression? A review of cross-cultural",No DOI,Journal of Nonverbal …,https://link.springer.com/article/10.1023/A:1024902500935,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
201,203,Meta-analysis of the first facial expression recognition challenge,"['MF Valstar', 'M Mehu', 'B Jiang', 'M Pantic']",2012,390,Affective Faces Database,facial expression recognition,IEEE conference on Face and Gesture Recognition 2011. It  : AU detection and classification  of facial expression imagery in  analysis of facial expressions consider facial affect (emotion),No DOI,IEEE Transactions on …,https://pubmed.ncbi.nlm.nih.gov/22736651/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
202,204,Morphed emotional faces: emotion detection and misinterpretation in social anxiety,"['K Heuer', 'WG Lange', 'L Isaac', 'M Rinck']",2010,136,Karolinska Directed Emotional Faces,classifier,"This resulted in three different types of emotional faces (anger, happy, disgust), but four  different interpretation categories (anger, happy, disgust, contempt). This way, the probability of",No DOI,Journal of behavior …,https://pubmed.ncbi.nlm.nih.gov/20511123/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
203,205,Multi angle optimal pattern-based deep learning for automatic facial expression recognition,"['DK Jain', 'Z Zhang', 'K Huang']",2020,115,"Facial Expression Recognition 2013, MMI Facial Expression","deep learning, facial expression recognition, machine learning",the required label for the facial expressions.The major key  the facial alignment. The  proposed MAOP-DL validates its effectiveness on two standard databases such as CK+ and MMI,No DOI,Pattern Recognition Letters,https://www.sciencedirect.com/science/article/pii/S0167865517302313,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
204,206,Multi-pie,"['R Gross', 'I Matthews', 'J Cohn', 'T Kanade']",2010,2679,CMU Multi-PIE,"classifier, facial expression recognition, machine learning","The range of facial expressions captured in Multi-PIE (neutral, smile, surprise, squint, disgust  In this paper we introduced the CMU Multi-PIE face database. Multi-PIE improves upon the",No DOI,Image and vision …,https://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
205,207,Multi-region ensemble convolutional neural network for facial expression recognition,"['Y Fan', 'JCK Lam', 'VOK Li']",2018,168,"Acted Facial Expressions In The Wild, Affective Faces Database","facial expression recognition, neural network","face on facial expression recognition. Our proposed method is evaluated based on two  well-known publicly available facial expression  database, Acted Facial Expressions in the Wild (",No DOI,… Conference on Artificial Neural Networks …,https://arxiv.org/abs/1807.10575,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
206,208,Multi-task convolutional neural network for pose-invariant face recognition,"['X Yin', 'X Liu']",2017,381,CMU Multi-PIE,"CNN, classification, neural network",classification error is more likely to happen compared to controlled datasets with discrete pose  angles. This work utilizes all data in the Multi-PIE  of variations on Multi-PIE. We also apply,No DOI,IEEE Transactions on Image Processing,https://ieeexplore.ieee.org/document/8080244,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
207,209,Multi-task pose-invariant face recognition,"['C Ding', 'C Xu', 'D Tao']",2015,303,CMU Multi-PIE,"classification, deep learning, machine learning, neural network","on FERET, CMU-PIE, and Multi-PIE databases shows that the  Multi-task learning (MTL) is  a machine learning technique  protocol for the pose problem on Multi-PIE, we adopt the three",No DOI,IEEE Transactions on image Processing,https://ieeexplore.ieee.org/document/7006757,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
208,210,Multi-view facial expression recognition,"['Y Hu', 'Z Zeng', 'L Yin', 'X Wei', 'X Zhou']",2008,167,Binghamton University 3D Facial Expression,facial expression recognition,"faculties from State University of New York at Binghamton. The  expressions, captured by a  3D face scanner. With the  in the multi-view emotion recognition experiment. The best average",No DOI,… & Gesture Recognition,https://ieeexplore.ieee.org/document/4813445,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
209,211,Multimodal 2D+ 3D facial expression recognition with deep fusion convolutional neural network,"['H Li', 'J Sun', 'Z Xu', 'L Chen']",2017,241,"Binghamton University 3D Facial Expression, Facial Expression Recognition 2013, Toronto Face Database","CNN, FER, classification, classifier, deep learning, facial expression recognition, machine learning, neural network",has never been used to learn 3D facial representations in 3D FER. This motivates us to fill   Expression) Database [59] has been the benchmarking for static 3D FER [12]. It includes 100,No DOI,IEEE Transactions on Multimedia,https://ieeexplore.ieee.org/document/7944639,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
210,212,Multimodal learning for facial expression recognition,"['W Zhang', 'Y Zhang', 'L Ma', 'J Guan', 'S Gong']",2015,133,"Facial Expression Recognition 2013, Toronto Face Database",facial expression recognition,"+ database as an example, C is defined as two to distinguish whether the inputs is the latent  facial expression we aim to recognize.  CK+ database, we find that the expression process is",No DOI,Pattern Recognition,https://www.sciencedirect.com/science/article/pii/S003132031500151X,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
211,213,Objective classes for micro-facial expression recognition,"['AK Davison', 'W Merghani', 'MH Yap']",2018,126,Facial Expression Recognition 2013,facial expression recognition,; Section 3 describes the methodology; Section 4 presents the results and discusses the  effects of applying objective classification to a micro-expression recognition task; Section 5,No DOI,Journal of imaging,https://arxiv.org/abs/1708.07549,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
212,214,Occlusion aware facial expression recognition using CNN with attention mechanism,"['Y Li', 'J Zeng', 'S Shan', 'X Chen']",2018,852,"Acted Facial Expressions In The Wild, Affective Faces Database, Expression in-the-Wild, Static Facial Expression in the Wild",CNN,"that are common in the wild. In this paper, we propose a convolution neutral network (CNN)  with attention mechanism (ACNN) that can perceive the occlusion regions of the face and",No DOI,IEEE Transactions on Image …,https://ieeexplore.ieee.org/document/8576656,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
213,215,On solving the face recognition problem with one training sample per subject,"['J Wang', 'KN Plataniotis', 'J Lu', 'AN Venetsanopoulos']",2006,132,Toronto Face Database,"classification, classifier",to be recognized using a generic training database which consists of images from subjects  other than those under consideration. Many state-of-the-art face recognition solutions can be,No DOI,Pattern recognition,https://www.sciencedirect.com/science/article/pii/S0031320306001233,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
214,216,Patch-gated CNN for occlusion-aware facial expression recognition,"['Y Li', 'J Zeng', 'S Shan', 'X Chen']",2018,177,Expression in-the-Wild,CNN,"Then, via a proposed Patch-Gated Unit, PG-CNN reweighs each patch by the unobstructed-  The proposed PG-CNN is evaluated on two largest in-the-wild facial expression datasets (",No DOI,2018 24th international …,https://ieeexplore.ieee.org/document/8545853/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
215,217,Peak-piloted deep network for facial expression recognition,"['X Zhao', 'X Liang', 'L Liu', 'T Li', 'Y Han']",2016,369,CMU Multi-PIE,FER,"Multi-PIE  FER datasets available, we pre-trained GoogLeNet [24] on a large-scale face  recognition dataset, the CASIA Webface dataset [32]. This network was then fine-tuned for FER.",No DOI,Computer Vision–ECCV …,https://arxiv.org/abs/1607.06997,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
216,218,Perceptual and affective mechanisms in facial expression recognition: An integrative review,"['MG Calvo', 'L Nummenmaa']",2016,315,"Affective Faces Database, Karolinska Directed Emotional Faces","classification, classifier, facial expression recognition","We conclude that facial expression recognition, as it has been investigated in conventional   One such method is the affective priming paradigm, whereby an emotional face is briefly",No DOI,Cognition and Emotion,https://pubmed.ncbi.nlm.nih.gov/26212348/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
217,219,Practical emotional neural networks,"['E Lotfi', 'MR Akbarzadeh-T']",2014,136,Affective Faces Database,neural network,"is associated with motivation from emotion to improve or  emotional neural networks (Khashman,  2010). This paper aims to review and develop neural networks motivated from emotion",No DOI,Neural Networks,https://www.sciencedirect.com/science/article/pii/S0893608014001488,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
218,220,Predicting long-term outcome of Internet-delivered cognitive behavior therapy for social anxiety disorder using fMRI and support vector machine learning,"['KNT Månsson', 'A Frick', 'CJ Boraxbekk']",2015,170,Karolinska Directed Emotional Faces,machine learning,"demonstrated that initial activations of the visual cortex, in response to emotional face  stimuli, predicted symptom improvement with CBT, and that brain measures vastly improved",No DOI,Translational …,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4354352/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
219,221,Presentation and validation of the Radboud Faces Database,"['O Langner', 'R Dotsch', 'G Bijlstra']",2010,2954,"Affective Faces Database, Radboud Faces Database","classification, facial expression recognition","available Radboud Faces Database (RaFD), a face database containing Caucasian face   We asked participants to pick the emotion label that best fitted the shown facial expression.",No DOI,… and emotion,https://www.tandfonline.com/doi/full/10.1080/02699930903485076,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,
220,222,Pyramid with super resolution for in-the-wild facial expression recognition,"['TH Vo', 'GS Lee', 'HJ Yang', 'SH Kim']",2020,138,Expression in-the-Wild,"FER, facial expression recognition","Wang, “Video facial emotion recognition based on local enhanced motion history image and   Xia, “Bi-modality fusion for emotion recognition in the wild,” ICMI 2019 Proceedings of the",No DOI,IEEE Access,http://ieeexplore.ieee.org/document/9143068/,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
221,223,Real time emotion recognition from facial expressions using CNN architecture,"['MA Ozdemir', 'B Elagoz', 'A Alaybeyoglu']",2019,107,Karolinska Directed Emotional Faces,CNN,"In this study, we proposed CNN based LeNet architecture for facial expression recognition  to estimate emotion states of human. We merged 3 different datasets (KDEF, JAFFE and our",No DOI,2019 medical …,https://ieeexplore.ieee.org/document/8895215,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
222,224,Real-time face detection and lip feature extraction using field-programmable gate arrays,"['D Nguyen', 'D Halupka', 'P Aarabi']",2006,147,Toronto Face Database,classifier,This paper proposes a new face detection technique that utilizes a naive Bayes classifier  to detect faces in an image based on only image edge direction information. This technique,No DOI,IEEE Transactions on …,https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=ccb214f4273b8eb7e2c155beb89f9318c347d60f,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
223,225,Real-time mobile facial expression recognition system-a case study,"['M Suk', 'B Prabhakaran']",2014,178,Facial Expression Recognition 2013,facial expression recognition,"facial expression recognition running on mobile devices. The proposed system recognizing  a user’s emotion  2013 IEEE International Symposium on Multimedia, 0:84–87, 2012. [16] S.",No DOI,… computer vision and pattern recognition …,http://ieeexplore.ieee.org/document/6909970,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
224,226,Recognition of affect in the wild using deep neural networks,"['D Kollias', 'MA Nicolaou', 'I Kotsia']",2017,161,"Acted Facial Expressions In The Wild, Affective Faces Database, Expression in-the-Wild","CNN, classification, deep learning, neural network",(such as subjects reacting to an unexpected development in a  end-to-end deep CNN and  CNN-RNN architectures (Section 3)  that a major challenge in facial expression and emotion,No DOI,Proceedings of the …,https://ieeexplore.ieee.org/document/8014981,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
225,227,Recognition of facial expressions of emotion is related to their frequency in everyday life,"['MG Calvo', 'A Gutiérrez-García']",2014,134,Karolinska Directed Emotional Faces,facial expression recognition,"on facial expression recognition has consistently found that happy expressions are   Consistent with this, in the current study, nearly one-third (29 %) of all the observed emotional faces",No DOI,Journal of Nonverbal …,https://link.springer.com/article/10.1007/s10919-014-0191-3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
226,228,Recognition thresholds for static and dynamic emotional faces.,"['MG Calvo', 'P Avero', 'A Fernández-Martín', 'G Recio']",2016,113,Karolinska Directed Emotional Faces,"classification, classifier","To this end, we varied the degree of intensity of emotional expressions unfolding from a  neutral face, by means of graphics morphing software. The resulting face stimuli (photographs",No DOI,Emotion,https://pubmed.ncbi.nlm.nih.gov/27359222/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
227,229,Recognizing action units for facial expression analysis,"['YI Tian', 'T Kanade', 'JF Cohn']",2001,2384,Extended Cohn-Kanade,"classification, classifier",We classify each of the wrinkles into one of two states:  of seven subjects from the Cohn-Kanade  database. Of the 72  of 46 subjects from the CohnKanade database and tested on 50,No DOI,IEEE Transactions on pattern …,https://ieeexplore.ieee.org/document/908962,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
228,230,Recognizing facial expression: machine learning and application to spontaneous behavior,"['MS Bartlett', 'G Littlewort', 'M Frank']",2005,912,Extended Cohn-Kanade,machine learning,"We first report performance for generalization to novel subjects within the Cohn-Kanade and   Shown are 4 subjects from the Cohn-Kanade dataset posing disgust containing AU’s 4,7",No DOI,2005 IEEE Computer …,https://ieeexplore.ieee.org/document/1467492/1000,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
229,231,Recover canonical-view faces in the wild with deep neural networks,"['Z Zhu', 'P Luo', 'X Wang', 'X Tang']",2014,152,Expression in-the-Wild,neural network,"Thc first term in Equation (1) measures the face's symmetry, which is the difference between  the left half and the right half of the face, and the second term measures the rank of the face.",No DOI,arXiv preprint arXiv:1404.3543,https://arxiv.org/abs/1404.3543,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
230,232,Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild,"['S Li', 'W Deng', 'JP Du']",2017,1700,Expression in-the-Wild,"CNN, classification, classifier, deep learning, machine learning","For all we know, RAF-DB is the first database that contains compound expressions in the wild.  Our  So in this paper, we directly trained our deep learning system on the big enough self-",No DOI,… of the IEEE conference on computer …,https://ieeexplore.ieee.org/document/8099760,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
231,233,Robust facial expression recognition based on local directional pattern,"['T Jabid', 'MH Kabir', 'O Chae']",2010,448,Japanese Female Facial Expression,"classification, classifier, facial expression recognition, machine learning","Two well-known machine learning methods, template matching and support vector machine,  are used for classification using the Cohn-Kanade and Japanese female facial expression",No DOI,ETRI journal,https://onlinelibrary.wiley.com/doi/abs/10.4218/etrij.10.1510.0132,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
232,234,Shape analysis of local facial patches for 3D facial expression recognition,"['A Maalej', 'BB Amor', 'M Daoudi', 'A Srivastava']",2011,107,Binghamton University 3D Facial Expression,"classification, classifier, deep learning, facial expression recognition, machine learning, neural network",► We address the 3D facial expression recognition problem using Riemannian geometry.  ► We propose local shape  [12] at Binghamton University. It was designed for research on,No DOI,Pattern Recognition,https://www.sciencedirect.com/science/article/pii/S0031320311000756,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Shape analysis of local facial patches for 3D facial
expression recognition
the features on this page.
Skip to main contentSkip to article
My account
Sign in
* Access through **your organization**
* Purchase PDF
Search 
## Article preview
* Abstract
* Introduction
* Section snippets
* References (20)
* Cited by (74)
## Pattern Recognition
Volume 44, Issue 8, August 2011, Pages 1581-1589
# Shape analysis of local facial patches for 3D facial expression recognition
Author links open overlay panelAhmed Maalej a b, Boulbaba Ben Amor a b,
Mohamed Daoudi a b, Anuj Srivastava c, Stefano Berretti d
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.patcog.2011.02.012Get rights and content
## Abstract
In this paper we address the problem of 3D facial expression recognition. We
propose a local geometric shape analysis of facial surfaces coupled with
machine learning techniques for expression classification. A computation of
the length of the geodesic path between corresponding patches, using a
Riemannian framework, in a shape space provides a quantitative information
about their similarities. These measures are then used as inputs to several
classification methods. The experimental results demonstrate the effectiveness
of the proposed approach. Using multiboosting and support vector machines
(SVM) classifiers, we achieved 98.81% and 97.75% recognition average rates,
respectively, for recognition of the six prototypical facial expressions on
BU-3DFE database. A comparative study using the same experimental setting
shows that the suggested approach outperforms previous work.
### Highlights
? We address the 3D facial expression recognition problem using Riemannian
geometry. ? We propose local shape analysis of faces coupled with machine-
learning techniques. ? A comparative study shows that the suggested approach
outperforms previous work.
## Introduction
In recent years, 3D facial expression recognition has received growing
attention. It has become an active research topic in computer vision andpattern recognition community, impacting important applications in fields
related to human?machine interaction (e.g., interactive computer games) and
psychological research. Increasing attention has been given to 3D acquisition
systems due to the natural fascination induced by 3D objects visualization and
rendering. In addition 3D data have advantages over the 2D data, in that 3D
facial data have high resolution and convey valuable information that
overcomes the problem of pose/lighting variations and the detail concealment
of low resolution acquisition.
In this paper we present a novel approach for 3D identity-independent facial
expression recognition based on a local shape analysis. Unlike the identity
recognition task that has been the subject of many papers, only few works have
addressed 3D facial expression recognition. This could be explained through
the challenge imposed by the demanding security and surveillance requirements.
Besides, there has long been a shortage of publicly available 3D facial
expression databases that serve the researchers exploring 3D information to
understand human behaviors and emotions. The main task is to classify the
facial expression of a given 3D model, into one of the six prototypical
expressions, namely _Happiness_ , _Anger_ , _Fear_ , _Disgust_ , _Sadness_ and
_Surprise_. It is stated that these expressions are universal among human
ethnicity as described in [1], [2].
The remainder of this paper is organized as follows. First, a brief overview
of related work is presented in Section 2. In Section 3 we describe the
BU-3DFE database designed to explore 3D information and improve facial
expression recognition. In Section 4, we summarize the shape analysis
framework applied earlier for 3D curves matching by Joshi et al. [3], and
discuss its use to perform 3D patches analysis. This framework is further
expounded in Section 5, so as to define methods for shapes analysis and
matching. In Section 6 a description of the feature vector and used
classifiers is given. In Section 7, experiments and results of our approach
are reported, and the average recognition rate over 97% is achieved using
machine-learning algorithms for the recognition of facial expressions such as
multiboosting and SVM. Finally, discussion and conclusion are given in Section
8.
## Section snippets
## Related work
Facial expression recognition has been extensively studied over the past
decades especially in 2D domain (e.g., images and videos) resulting in a
valuable enhancement. Existing approaches that address facial expression
recognition can be divided into three categories: (1) _static_ versus
_dynamic_ ; (2) _global_ versus _local_ ; (3) _2D_ versus _3D_. Most of the
approaches are based on feature extraction/detection as a mean to represent
and understand facial expressions. Pantic and Rothkrantz [4] and Samal
## Database description
BU-3DFE is one of the very few publicly available databases of annotated 3D
facial expressions, collected by Yin et al. [12] at Binghamton University. It
was designed for research on 3D human face and facial expression and to
develop a general understanding of the human behavior. Thus the BU-3DFE
database is beneficial for several fields and applications dealing with human
computer interaction, security, communication, psychology, etc. There are a
total of 100 subjects in the database, 56
## 3D facial patches-based representation
Most of the earlier work in 3D shape analysis use shape descriptors such as
curvature, crest lines, shape index (e.g., ridge, saddle, rut, dome, etc.).
These descriptors are defined based on the geometric and topologicalproperties of the 3D object, and are used as features to simplify the
representation and thus the comparison for 3D shape matching and recognition
tasks. Despite their rigorous definition, such features are computed based on
numerical approximation that involves second
## Framework for 3D shape analysis
Once the patches are extracted, we aim at studying their shape and design a
similarity measure between corresponding ones on different scans under
different expressions. This is motivated by the common belief that people
smile, or convey any other expression, the same way, or more appropriately
certain regions taking part in a specific expression undergo practically the
same dynamical deformation process. We expect that certain corresponding
patches associated with the same given expression
## Feature vector generation for classification
In order to classify expressions, we build a feature vector for each facial
scan. Given a candidate facial scan of a person _j_ , facial patches are
extracted around facial landmarks. For a facial patch _P_ _j_ _i_ , a set of
level curves {c?}ji are extracted centered on the _i_ th landmark. Similarly,
a patch _P_ _ref_ _i_ is extracted in correspondence to landmarks of a
reference scans _ref_. The length of the geodesic path between each level
curve and its corresponding curve on the reference scan is computed using a
## Recognition experiments
To investigate facial expression recognition, we have applied our proposed
approach on a dataset that is appropriate for this task. In this section, we
describe the experiments, obtained results and comparisons with related work.
## Conclusions
In this paper we presented a novel approach for identity-independent facial
expression recognition from 3D facial shapes. Our idea was to describe the
change in facial expression as a deformation in the vicinity of facial patches
in 3D shape scan. An automatic extraction of local curve-based patches within
the 3D facial surfaces was proposed. These patches were used as local shape
descriptors for facial expression representation. A Riemannian framework was
applied to compute the geodesic path
**Ahmed Maalej** is currently a Ph.D. candidate within the Fundamental
Computer Science laboratory of Lille (LIFL UMR 8022), France. He obtained the
M.S. degree in Telecommunications from the Higher School of Communications of
Tunis (SUP?COM), Tunisia, in 2008, and the electrical engineering degree from
the National Engineering School of Monastir (ENIM), Tunisia, in 2005. His main
research interests focus on 3D facial expression recognition.
Recommended articles"
233,235,Simultaneous facial feature tracking and facial expression recognition,"['Y Li', 'S Wang', 'Y Zhao', 'Q Ji']",2013,257,"Facial Expression Recognition 2013, MMI Facial Expression",facial expression recognition,"the extended CohnKanade database and test on the MMI facial expression database [53].  Since most of the image sequences on the MMI database have only single AU active, we only",No DOI,IEEE Transactions on image …,https://pubmed.ncbi.nlm.nih.gov/23529088/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
234,236,Single sample face recognition via learning deep supervised autoencoders,"['S Gao', 'Y Zhang', 'K Jia', 'J Lu']",2015,265,Toronto Face Database,"deep learning, machine learning","based deep neural networks, and driven by the SSPP face  a supervised auto-encoder to  build the deep neural network.  the Toronto Face Database, which is a very large dataset to",No DOI,IEEE transactions on …,https://ieeexplore.ieee.org/document/7124463,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
235,237,Spatio-temporal convolutional features with nested LSTM for facial expression recognition,"['Z Yu', 'G Liu', 'Q Liu', 'J Deng']",2018,158,Oulu-CASIA,classification,"Oulu-CASIA: We also consider for experiments the Oulu-CASIA dataset, which is a little bit  more  This dataset is more difficult to classify than CK+. In the cases of disgust, fear, happiness",No DOI,Neurocomputing,https://www.sciencedirect.com/science/article/pii/S0925231218308634,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Spatio-temporal convolutional features with nested LSTM for
facial expression recognition
the features on this page.
Skip to main contentSkip to article
Search 
## Outline
1. Abstract
2. 3. Keywords
4. 1\. Introduction
5. 2\. Related work
6. 3\. Spatio-Temporal convolutional features with nested LSTM
7. 4\. Experiments and results
8. 5\. Conclusion
9. Acknowledgements
10. References
11. Vitae
Show full outline
## Cited by (122)
## Figures (10)
1. 2. 3. 4. 5. 6.
Show 4 more figures
## Tables (9)
1. Table 1
2. Table 2
3. Table 3
4. Table 4
5. Table 5
6. Table 6
Show all tables
## Neurocomputing
Volume 317, 23 November 2018, Pages 50-57
# Spatio-temporal convolutional features with nested LSTM for facial
expression recognition
Author links open overlay panelZhenbo Yu a, Guangcan Liu a, Qingshan Liu a,
Jiankang Deng b
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.neucom.2018.07.028Get rights and content
## AbstractIn this paper, we propose a novel end-to-end architecture termed Spatio-
Temporal Convolutional features with Nested LSTM (STC-NLSTM), which learns the
muti-level appearance features and temporal dynamics of facial expressions in
a joint fashion. More precisely, 3DCNN is used to extract spatio-temporal
convolutional features from the image sequences that represent facial
expressions, and the dynamics of expressions are modeled by Nested LSTM, which
is actually coupled by two sub-LSTMs, saying T-LSTM and C-LSTM. Namely, T-LSTM
is used to model the temporal dynamics of the spatio-temporal features in each
convolutional layer, and C-LSTM is adopted to integrate the outputs of all
T-LSTMs together so as to encode the multi-level features encoded in the
intermediate layers of the network. We conduct experiments on four benchmark
databases, CK+, Oulu-CASIA, MMI and BP4D, and the results show that the
proposed method achieves a performance superior to the state-of-the-art
methods.
* Previous article in issue
* Next article in issue
## Keywords
Facial expression recognition
LSTM
3DCNN
Multi-level features
## 1\. Introduction
Facial Expression Recognition (FER) [1], in general, is to automatically group
various kinds of facial muscle motions into similar emotion categories purely
based on the visual information in images or videos. Due to its potentials in
a broad range of applications such as face recognition [2], [3], [4], [5],
face alignment [6], [7], [8], [9], [10], [11], [12], [13], [14] and human-
computer interface [15], FER has received extensive attentions in the
literatures, e.g., [16], [17], [18]. Essentially, facial expression is a
dynamic process consisting of multiple stages, mainly including neutral,
onset, apex and offset [19], so how to learn the dynamics of facial
expressions is a key issue in FER [20].
Early FER methods [21], [22] are often built upon some pre-defined features
such as the Gabor filters, haar-like features and Local Binary Patterns (LBP).
These methods may work well only on limited occasions, as the pre-defined
features are incapable of fitting well with the data from a wide range of
applications. To overcome this issue, it would be natural to consider the deep
learning methods such as Convolutional Neural Network (CNN) [23], which can
seamlessly integrate feature extraction and expression classification into a
unified procedure. Extensive experiments demonstrate that CNN achieves
substantial improvement in recognition accuracy over conventional methods
[16], [24], [25], [26], but most of CNN-based methods consider a video as a
collection of multiple static images, and thus they may not handle well the
dynamic nature of facial expressions.
In order to make better use of the features that capture the motion of facial
muscles, sequence-based methods [25], [27], [28], [29], which represent an
expression by a sequence of images with known time stamp, have emerged as a
preferable choice. To analyze sequential data, the deep learning community has
also established several tools, e.g., Recurrent Neural Network (RNN) [30],
Long Short-Term Memory (LSTM) [31], [32], [33], [34] and 3D Convolutional
Neural Network (3DCNN) [35]. Especially, the CNN-RNN (or CNN-LSTM) framework
attracts much attention [25], [27], [28], in which RNN (or LSTM) takes the
appearance features extracted by CNN over individual frames as inputs and
encodes the temporal dynamics for later use, because it can combine theadvantages of CNN and RNN to model both the appearance features and temporal
dynamics simultaneously. Recently, researchers have investigated the framework
of 3DCNN-RNN (or 3DCNN-LSTM) [36], [37]. Unlike CNN, which only deals with 2D
inputs, 3DCNN takes image sequences as inputs and can therefore extract
directly the spatio-temporal features underlying the image sequences. Despite
of the considerable improvement attained with the help of deep learning in
recent few years, existing methods often use only the outputs of the last
fully-connected layer as features for classification, discarding much useful
information encoded in the intermediate layers of the network. As can be seen
from Fig. 1, early convolutional layers extract fine-grained details (e.g.,
local boundaries or illuminations) of faces, while later layers capture more
detailed information, e.g., the appearance patterns of mouths and eyes. It can
be seen that the features from all layers of 3DCNN indeed provide FER with a
hierarchical representation of multi-level features from fine to coarse. Such
a hierarchical representation, intuitively, would be more effective than the
features contained in the last layer only.
1. Download: Download high-res image (258KB)
2. Download: Download full-size image
Fig. 1. Visualization of the convolutional features extracted from different
layers of 3DCNN. The blue and red points correspond to the low and high
response values, respectively. The emotion label for the input image sequence
is surprise.
In this work, we propose an end-to-end FER method that can involve various
visual clues, including the multi-level appearance features and the temporal
dynamics of facial expressions. To this end, we propose a novel architecture
termed Spatio-Temporal Convolutional features with Nested LSTM (STC-NLSTM),
which is illustrated in Fig. 2. In general, our STC-NLSTM contains three major
components: (1) a 3DCNN module consisting of multiple convolutional layers,
(2) multiple temporal-LSTM (T-LSTM) modules each of which corresponds to one
layer of the 3DCNN, and (3) a convolutional-LSTM (C-LSTM) module that takes
the outputs of T-LSTMs as inputs1. Given a sequence of images that represent
an emotion class, first, the 3DCNN module extracts the spatio-temporal
convolutional features of the expression for later use. Second, T-LSMT takes
the spatio-temporal features as inputs and produces compact features that
encode the appearance features as well as the temporal dynamics. Third, C-LSTM
plays the role of integrating the outputs of all T-LSTMs together and encoding
the multi-level features contained in each convolutional layer. Finally, the
softmax classifier is used to categorize the given sequence into one of the
six basic emotion classes. In contrast to the existing sequence-based methods
[25], [28], our STC-NLSTM can utilize not only the appearance features as well
as the temporal dynamics of facial expressions, but also the multi-level
semantics encoded in the individual layers of the network, so as to attain
more reliable classification results. Experiments on CK+ [38], Oulu-CASIA
[39], MMI [19] and BP4D [40] show that the proposed STC-NLSTM is superior to
the state-of-the-art methods.
1. Download: Download high-res image (226KB)
2. Download: Download full-size image
Fig. 2. Architecture of the proposed STC-NLSTM, which consists of 3DCNN and
Nested LSTM, and which is coupled by temporal-LSTM (T-LSTM) and convolutional-
LSTM (C-LSTM). In the figure above, the term ?ST-Convs? standards for the
spatio-temporal convolutional features.
The rest of this paper is organized as follows. Section 2 provides a brief
survey for FER. Section 3 introduces the proposed STC-NLSTM method. Section 4
shows some empirical results and Section 5 concludes this paper.## 2\. Related work
Deep learning methods have exhibited superior performance for FER, showing
dramatic improvement in accuracy and robustness over the conventional methods
based on pre-defined features [16], [24], [25], [26], [28], [40], [41], [42],
[43]. According to how an expression is represented, existing methods can be
roughly divided into two categories: image-based and sequence-based methods.
In general, FER is a special pattern recognition problem, and thus the
techniques for generic classification can be naturally applied to FER. Yu et
al. [44] proposed a FER method that combines together an ensemble of multiple
CNNs by minimizing a mixture of the log likelihood loss and the hinge loss.
Kim et al. [45] devised a recognition framework by combining multiple CNNs to
form a hierarchical network, and they won the first place of EmotiW 2015, an
international competition of FER. Bargal et al. [41] established a hybrid
network that combines VGG16 [46] with Residual Network [47] to learn the
appearance features of expressions, and they used SVM to produce the final
classification results. Yao et al. [48] proposed a deeper and wider network
consisting of three inception modules, and Zhao et al. [26] built a novel
peak-piloted feature transformation network to capture the intrinsic
correlations between the peak and weak expressions.
Different from the image-based methods, the sequence-based methods attempted
to well capture the temporal variations of the appearance features, which are
better for FER. Liu et al. [49] proposed a FER method termed 3DCNN-DAP (DAP
standards for deformable action part), in which 3DCNN is used to extract the
spatio-temporal features and the strong spatial structural constraints among
the dynamic action parts as well. Jung et al. [16] studied an integrated
network with joint fine-tuning to infer the appearance features and temporal
dynamics of facial expressions. Jaiswal et al [25] utilized CNN in combination
with Bi-directional LSTM (BiLSTM) for FER, achieving a performance better than
the winner of FERA 2015 [40]. Fan et al. [28] established a novel hybrid
network that combines 3DCNN and RNN in a late-fusion fashion and won the first
place in EmotoW2016 [42]. The proposed STC-NLSTM method belongs to the
sequence-based methods. Comparing to the previous works, our STC-NLSTM
provides a fine grained approach for modeling multi-level features encoded in
the intermediate layers of the network so as to achieve more accurate FER.
## 3\. Spatio-Temporal convolutional features with nested LSTM
This section details the proposed STC-NLSTM. As shown in Fig. 2, our STC-NLSTM
has three main components: (1) a 3DCNN module is used to extracted the spatio-
temporal convolutional features (ST-Convs) of facial expressions, (2) multiple
T-LSTM modules are adopted to capture the temporal dynamics of facial muscle
motions, and (3) a C-LSTM module aims to seize the multi-level features
encoded in the individual layers of the 3DCNN.
### 3.1. Spatio-temporal convolutional features
Since facial expression is essentially a dynamic process, we attempt to
extract directly the spatio-temporal features of facial expressions by a more
straightforward approach, that is, the well recognized 3DCNN, which has been
widely used in the fields of activity recognition, lip reading recognition,
gesture recognition, and so on [35]. Different from the traditional CNN that
can only deal with 2D inputs, 3DCNN takes directly the image sequences as
inputs and can therefore capture literally the spatio-temporal features of
image sequences.
Fig. 3 illustrated the architecture of the 3DCNN. Given a sequence of images
with known time stamp that represent a facial emotion class, 3DCNN processes
the sequence by multiple convolutional and pooling layers, producing a
collection of spatio-temporal features that characterize the expression.1. Download: Download high-res image (110KB)
2. Download: Download full-size image
Fig. 3. Architecture of the adopted 3DCNN.
### 3.2. Nested LSTM
To capture the multi-level features encoded in the intermediate layers of the
network, we propose the so-called Nested LSTM shown in Fig. 4, which is
composed of MSPP-norm, T-LSTM and C-LSTM. MSSP-norm aims to normalize the
spatio-temporal features of different sizes to the same dimension, while
T-LSTM and C-LSTM can capture the temporal dynamics and seize the multi-level
features encoded in the individual convolutional layers of the network
respectively.
1. Download: Download high-res image (291KB)
2. Download: Download full-size image
Fig. 4. Architecture of the proposed Nested LSTM.
#### 3.2.1. MSPP-norm
Because the spatio-temporal features extracted by different layers of 3DCNN
have different dimensions, it is impossible to input them directly to the LSTM
units, which generally require the inputs to have the same dimension. To fill
this gap, we design the Multi-dimensional Spatial Pyramid Pooling
normalization (MSPP-norm) operation, which is inspired by the Spatial Pyramid
Pooling network (SPP-net) proposed by He et al. [50]. The purpose of the MSSP-
norm is to normalize the spatio-temporal features of different sizes to the
same dimension. Given a 3D feature map of size _N_ × _a_ × _a_ , we
partition it into _N_ × _n_ × _n_ (with n=2,4,8) sub-regions and summarize
the responses within each sub-region via max pooling, resulting in a feature
vector with a fixed dimension determined by the parameter _n_.
#### 3.2.2. T-LSTM and C-LSTM
After the process of MSPP-norm, the spatio-temporal features in each layer of
3DCNN are transferred to feature vectors of the same dimension. Thus, it is
suitable to further analyze the spatio-temporal features by LSTM, which is an
advanced RNN architecture for sequential data analysis including in FER [27],
[51]. The commonly used LSTM can model the temporal information by
transforming a sequence of inputs to a sequence of outputs; this, in general,
can partially capture the correlations among the spatio-temporal features
extracted by 3DCNN. However, few conventional methods based on LSTM make full
use of the information encoded in all the convolutional layers, because it is
hard to involve all of the appearance features, temporal dynamics and multi-
level features by simply combining 3DCNN with LSTM.
To deal with the above issues, we adopt two LSTM modules, saying T-LSTM and
C-LSTM to cope with the spatio-temporal features extracted by 3DCNN. For each
feature vector corresponding to a certain convolution layer, a T-LSTM is
constructed by stacked LSTM units, which models the temporal dynamics of
facial expressions. After that, a C-LSTM is constructed to take the outputs of
T-LSTMs as inputs, so the desired multi-level features can be modeled in a
seamless way.
Suppose that there are in total _l_ convolutional layers in 3DCNN. Then the
procedure of our STC-NLSTM method can be summarized as follows:
fj=3DCNN(x),j=1,?,l,fjmspp=MSPP-
norm(fj),j=1,?,l,hj=T-LSTMj(fjmspp),j=1,?,l,h={h1,?,hl},o=C?LSTM(h),where _x_
denotes an image sequence, _f j_ is the 3D feature map produced by the _j_ th
convolutional layer of 3DCNN, _h j_ is the feature vector from the _j_ th
T-LSTM module, and _o_ denotes the final feature vector used for
classification.
## 4\. Experiments and results### 4.1. Experimental data
To verify the effectiveness of the proposed STC-NLSTM, we experiment with four
benchmark datasets, CK+ [38], Oulu-CASIA [39], MMI [19] and BP4D [40].
**CK+:** This dataset has six basic emotion classes, including anger(An),
disgust(Di), fear(Fe), happiness(Ha), sadness(Sa) and surprise(Su). In
addition, there is another special expression called ?contempt?. The dataset
contains in total 593 image sequences from 123 subjects, but only 309
sequences are annotated with the six basic expression labels. We divide these
309 sequences into 10 groups, of which 9 groups are used for training and the
rest for testing. In this way, we can run various FER methods multiple times
and obtain an averaged accuracy for evaluation.
_Oulu-CASIA_ : We also consider for experiments the Oulu-CASIA dataset, which
is a little bit more challenging than CK+. The dataset contains 480 image
sequences of six basic emotion classes (including An, Di, Fe, Ha, Sa and Su)
under normal illumination conditions. Each sequence begins with a neutral
expression and ends with the peak expression. The same as in CK+, a 10-fold
cross validation is performed to evaluate various FER methods.
_MMI_ : The third dataset used for experiments, MMI, is consist of 205 image
sequences of the six basic emotion classes. Unlike CK+ and Oulu-CASIA, in
which each sequence ends at the peak expression, the peak expressions in MMI
are located in the middle of the sequences. The location of the peak frame is
not provided as a prior information, which is usually the case for real-world
videos. To obtain unbiased evaluation results, we perform a 10-fold cross
validation in the same way as in CK+ and Oulu-CASIA.
_BP4D_ : The last dataset used for experiments, BP4D, is divided into a fixed
set of training, development and test data. In total, the training partition
contains 75,586 images, the development contains 71,261 images and the test
contains 75,726 images. Each of these images in BP4D are annotated with 11
Action Units. Unlike above datasets, BP4D contains large number of annotated
images which benefits deep learning algorithms and provides a good platform
for a fair evaluation due to a fixed training and test set.
Fig. 5 shows some examples sampled from the above four datasets, and Table 1
summarizes the number of sequences in each of the six emotion classes. To
better perform FER, we need to use several pre-processing techniques, mainly
including video normalization, face detection and data augmentation.
1. Download: Download high-res image (254KB)
2. Download: Download full-size image
Fig. 5. Examples of the images used in our experiments. Top: CK+; Middle:
Oulu-CASIA; Bottom: MMI.
Table 1. The number of image sequences in each of the six basic emotion
classes: anger(An),disgust(Di), fear(Fe), happiness(Ha), sadness(Sa) and
surprise(Su).
Empty Cell| An| Di| Fe| Ha| Sa| Su| All
---|---|---|---|---|---|---|---
CK+| 45| 59| 25| 69| 28| 83| 309
Oulu| 80| 80| 80| 80| 80| 80| 480
MMI| 32| 31| 28| 42| 32| 40| 205
_Video normalization_ : Since the length of the image sequences is variable,
but the dimension of the inputs for a neural network is usually fixed, the
normalization along the time axis is required as input for neural networks.
For the sequences in CK+ and Oulu-CASIA , which have respectively averaged
lengths of 18 and 22, we make each sequence into the average length via either
uniform sampling (for the sequences longer than the average) or replicating
the last frame (for the sequences shorter than the average). Regarding the MMIdataset, which is based on video, we convert the videos into the image
sequences by uniformly selecting 10 frames per second, and normalize the
sequences into a fixed length of 22 in the same way as in CK+ and Oulu-CASIA.
_Face detection_ : We utilize the Multi-Task Cascaded Convolutional Network
(MTCCN) [52] to obtain the coordinates of two eyes at first, then determine
the final rectangular face by keeping the distances between two eyes
invariable. Finally, we turn a rectangle into a square through zero padding
and resize the square to 64 × 64 (see Fig. 6).
1. Download: Download high-res image (210KB)
2. Download: Download full-size image
Fig. 6. Some examples of the face detection results in CK+ (top), Oulu-CASIA
(middle) and MMI (bottom).
_Data augmentation_ : Facial expression datasets, e.g., CK+, Oulu-CASIA and
MMI, often contain only hundreds of image sequences. However, a typical deep
neural network has many parameters, and this will make a deep network prone to
overfitting. To handle this issue, we first flip each image sequence
horizontally so as to double the number of sequences. Then we rotate each
image by an angle in {?7.5°, ?5°, ?2.5°, 2.5°, 5°, 7.5°}, resulting in a new
dataset which is 14 times as big as the original one. Such a data augmentation
process can not only make the learnt model robust against the slight
rotational changes of the input images, but also broaden the number of
training samples so as to avoid overfitting.
### 4.2. Experimental data
To evaluate the performance of the proposed STC-NLSTM, we compare it with 12
prevalent FER methods, including 3DCNN-DAP [49], 3DSIFT [53], ARDfee [54],
CSPL [20], DTAGN [16], FN2EN [55], IDT+FV [56], LOmo [43], PPDN [26], DCPN
[57], STM-ExpLet [17] and ST-RBM [58].
In addition, to further investigate the effectiveness of the proposed Nested
LSTM, we design four baselines by amending the architecture of STC-NLSTM:
* \- _STC (i.e., 3DCNN)_ : This baseline is created by simply removing the T-LSTM and C-LSTM modules
from the architecture of STC-NLSTM, and the outputs of the last convolutional layer of 3DCNN are taken as
inputs to the softmax classifier so as to obtain the final classification results.
* \- _STC-LSTM_ : This baseline is constructed by replacing the T-LSTM and C-LSTM modules with a
traditional LSTM. Namely, the outputs of the last convolutional layer are taken as inputs to LSTM, which
produces the final feature vectors for classification.
* \- _STC-SLSTM_ : This baseline is similar to STC-LSTM. The only difference is that the outputs of all
convolutional layer are taken as inputs to LSTM by sum fusion, which computes the sum of two feature maps
at the same spatial locations and channels.
* \- _DenseNet_ : This baseline [59] is a simpler and more efficient network compared to Inception networks,
which also utilizes the middle latent representation. We compared the DenseNet to the other FER methods
under the standard setting [26], which uses the strong expressions in each sequence(e.g., the last one to
three frames) for training and testing. Because the DenseNet method is only based on the static image, we
train this baseline following to [57].
For the 12 previously proposed baselines, their results are directly quoted
from the original reports. For STC, STC-LSTM and STC-SLSTM, we obtain their
classification results using the same parametric configuration as STC-NLSTM.
As usual, we denote a deep network by a sequence of letters and numbers, e.g.,
I(64,64,22)-C(3,64)-BN-P2-FC18-S6, where I(64,64,22) means the 64 × 64 × 22
input image sequences, C(3,64) is a convolutional layer with 64 filters of 3 ×
3, BN standards for the operation of batch normalization, P2 is a 2 × 2 max
pooling layer, FC refers to a fully connected layer, and S6 denotes a softmax
layer with six outputs. For simplicity, the architecture of the 3DCNN used in
our STC-NLSTM is configured as
I(64,64,18)-C(3,64)-BN-P2-C(3,64)-BN-P2-C(3,64)-C(3,64)-P2-C(3,64)-FC18, and atwo-level LSTM architecture is used to construct the T-LSTM and C-LSTM
modules. The stride of each layer is 1 with the exception of the pooling
layer, which has a stride value of 2. Table 2 details the configurations of
the network architecture.
Table 2. Detailed configurations of the network for CK+.
Type| Patch size/stride| Input size
---|---|---
conv1| 3 × 3 × 3/1 × 1 × 1| 18 × 64 × 64 × 3
mspp1| [8,4,2]| 18 × 5376
pool1| 1 × 2 × 2/1 × 2 × 2| 18 × 32 × 32 × 64
conv2| 3 × 3 × 3/1 × 1 × 1| 18 × 32 × 32 × 64
mspp2| [8,4,2]| 18 × 5376
pool2| 1 × 2 × 2/1 × 2 × 2| 18 × 16 × 16 × 64
conv3| 3 × 3 × 3/1 × 1 × 1| 18 × 16 × 16 × 64
mspp3| [8,4,2]| 18 × 5376
conv4| 3 × 3 × 3/1 × 1 × 1| 18 × 16 × 16 × 64
mspp4| [8,4,2]| 18 × 5376
pool4| 1 × 2 × 2/1 × 2 × 2| 18 × 8 × 8 × 64
conv5| 3 × 3 × 3/1 × 1 × 1| 18 × 8 × 8 × 64
mspp5| [8,4,2]| 18 × 5376
Our model is implemented based on the TensorFlow library [60] and trained on
four GeForce Titan X (pascal) GPU with 12GB memory. The weights of the network
are initialized randomly using the ?xaiver? procedure [61]. We first set the
learning rate as 0.0025 and train the network until 300 iterations, and then
fine-tune the network by setting the learning rate to 0.000025 and running 200
iterations. In all the experiments, the weight decay parameter is consistently
set as 0.0015.
### 4.3. Experimental results
#### 4.3.1. Results on CK+
For fair comparison, we follow [20] to use 10-fold cross-validation and repeat
the procedure 4 times, resulting in 40 trials in total. Table 3 shows the
comparison results of various FER methods. Since the expressions in this
dataset are easy to classify, several methods obtain superior classification
results. In particular, as we can see from Table 4, our STC-NLSTM can achieve
an accuracy near 100%. It performs well in anger and surprise, but for sadness
, it is easy to be confused with disgust and fear. Fig. 7 compares STC-NLSTM
with STC, STC-LSTM and STC-SLSTM on each of the six basic emotion classes. It
can be seen that STC-NLSTM performs consistently better than STC, STC-LSTM and
STC-SLSTM, which confirms the effectiveness of our Nested LSTM.
Table 3. Classification accuracies on CK+. The numbers for STC, STC-LSTM, STC-
SLSTM and STC-NLSTM are averaged from 40 trails.
Method| Average accuracy
---|---
3DCNN-DAP [49]| 92.4
STM-ExpLet [17]| 94.2
LOmo [43]| 95.1
IDT+FV [56]| 95.8
FN2EN [55]| 96.9
DTAGN [16]| 97.3
ARDfee [54]| 98.7
PPDN [26]| 99.3
DCPN [57]| 99.6
STC| 98.9
STC-LSTM| 99.3STC-SLSTM| 99.4
DenseNet| 97.6
STC-NLSTM| **99.8(** ± **0.2)**
Table 4. Confusion matrix of STC-NLSTM for CK+. The labels in the leftmost and
topmost columns denote the ground truth and prediction results, respectively.
Empty Cell| An| Di| Fe| Ha| Sa| Su
---|---|---|---|---|---|---
An| 100| 0| 0| 0| 0| 0
Di| 0.15| 99.68| 0| 0| 0.17| 0
Fe| 0| 0| 99.71| 0| 0.29| 0
Ha| 0| 0| 0.11| 99.89| 0| 0
Sa| 0| 0.29| 0.57| 0| 99.14| 0
Su| 0| 0| 0| 0| 0| 100
1. Download: Download high-res image (202KB)
2. Download: Download full-size image
Fig. 7. Comparing STC-NLSTM with STC, STC-LSTM and STC-SLSTM on each of the
six emotion classes in CK+.
#### 4.3.2. Results on Oulu-CASIA
Table 5 shows the comparison results, and Table 6 gives the confusion matrix
produced by our STC-NLSTM method on the Oulu-CASIA dataset. This dataset is
more difficult to classify than CK+. In the cases of disgust, fear, happiness
and surprise, the performance is good, but the performance for anger and
sadness is slightly poor. As we can see, STC-NLSTM achieves an averaged
accuracy of 93.45%, which is 4.2% higher than the 89.6% accuracy produced by
the most close baseline, ARDfee. This illustrates the superiorities of STC-
NLSTM over the state-of-the-art FER methods. Fig. 8 shows that STC-NLSTM is
distinctly better than STC, STC-LSTM and STC-SLSTM on all expression classes
except the class ?happy?. It is shown that that FER can benefit a lot from the
modeling of the multi-level features encoded in each convolutional layer.
Table 5. Classification accuracies on Oulu-CASIA. The numbers for STC, STC-
LSTM, STC-SLSTM and STC-NLSTM are obtained by averaging the accuracies from 40
trails.
Method| Average accuracy
---|---
STM-ExpLet [17]| 74.59
DTAGN [16]| 81.64
LOmo [43]| 82.10
PPDN [26]| 84.59
DCPN [57]| 86.23
FN2EN [55]| 87.71
ARDfee [54]| 89.60
STC| 84.72
STC-LSTM| 88.98
STC-SLSTM| 90.12
DenseNet| 87.28
STC-NLSTM| **93.45(** ± **0.43)**
Table 6. Confusion matrix of STC-NLSTM for Oulu-CASIA. The labels in the
leftmost and topmost columns denote the ground truth and prediction results,
respectively.
Empty Cell| An| Di| Fe| Ha| Sa| Su
---|---|---|---|---|---|---
An| 89.82| 6.20| 0.75| 0| 3.23| 0
Di| 1.38| 95.20| 0.30| 0.95| 2.17| 0
Fe| 0| 0| 96.14| 0.50| 0.65| 2.71Ha| 0| 0.90| 3.83| 94.78| 0.49| 0
Sa| 4.4| 2.38| 0.56| 0| 92.66| 0
Su| 0| 0| 3.95| 0| 0| 96.05
1. Download: Download high-res image (188KB)
2. Download: Download full-size image
Fig. 8. Comparing STC-NLSTM with STC, STC-LSTM and STC-SLSTM on each of the
six emotion classes in Oulu-CASIA.
#### 4.3.3. Results on MMI
As shown in Table 7, the STC-NLSTM method can distinctly outperform previous
state-of-the-art methods on the MMI dataset. Table 8 shows the confusion
matrix produced by STC-NLSTM. Actually, the averaged accuracy by STC-NLSTM
reaches 84.53% (see Table 7), which is 2.9% better than ST-RBM, and which
archives the best performance among the previous reports. Fig. 9 shows that
STC-NLSTM is distinctly better than STC-SLSTM on all the emotion classes,
which is same as the results on the other three datasets.
Table 7. Classification accuracies on MMI. The numbers for STC, STC-LSTM, STC-
SLSTM and STC-NLSTM are averaged from 40 trails.
Method| Average accuracy
---|---
3DCNN-DAP [49]| 63.4
3DSIFT [53]| 64.39
DTAGN [16]| 70.24
CSPL [20]| 73.53
STM-ExpLet [17]| 75.12
ST-RBM [58]| 81.63
STC| 74.84
STC-LSTM| 80.39
STC-SLSTM| 81.92
DenseNet| 77.68
STC-NLSTM| **84.53(** ± **0.67)**
Table 8. Confusion matrix of STC-NLSTM for MMI. The labels in the leftmost and
topmost columns denote the ground truth and prediction results, respectively.
Empty Cell| An| Di| Fe| Ha| Sa| Su
---|---|---|---|---|---|---
An| 83.24| 9.04| 0| 5.36| 1.24| 1.12
Di| 6.72| 88.21| 0| 2.74| 2.33| 0
Fe| 4.34| 0| 81.24| 1.23| 1.56| 11.63
Ha| 3.62| 0| 3.16| 93.22| 0| 0
Sa| 1.55| 1.12| 9.18| 1.18| 85.77| 1.20
Su| 2.64| 0| 8.66| 3.41| 0| 85.29
1. Download: Download high-res image (184KB)
2. Download: Download full-size image
Fig. 9. Comparing STC-NLSTM with STC, STC-LSTM and STC-SLSTM on each of the
six emotion classes in MMI.
#### 4.3.4. Results on BP4D
Since the BP4D dataset has the training set and testing set, we do not need to
use 10-fold cross-validation. Different from the above three datasets, the
BP4D dataset is bigger than them. Table 9 shows the experimental results. It
can be seen that the STC-NLSTM obviously outperforms previous the state-of-
the-art methods. This result supports the conclusion that our STC-NLSTM can
also achieve the good performance on a large scale datasets.
Table 9. Performance (F1 scores) comparison on BP4D Test set.
Method| F1 Scores
---|---LGBP [40]| 0.44
GDNN [25]| 0.48
DLE [62]| 0.51
CNN+BLSTM [25]| 0.52
STC-NLSTM| **0.58**
#### 4.3.5. Influences of the number of layers
The above results illustrate that Nested LSTM plays a crucial role in our
proposed method. To be more clear, we shall investigate the influences of the
number of the convolutional layers contained in the architecture of STC-NLSTM.
Fig. 10 shows the results. It can be seen that the classification accuracy
gradually increases as the enlargement of the layer number, reaching the
maximum at 5 convolutional layers. Since the datasets are not large, the
performance drops while the number of layers exceeds 5. Regarding why the CK+
dataset is not so sensitive to the number of layers, the reason is that the
dataset is easy to classify (see Table 3).
1. Download: Download high-res image (146KB)
2. Download: Download full-size image
Fig. 10. Plotting the classification accuracy as a function of the number of
convolutional layers in STC-NLSTM.
## 5\. Conclusion
In this paper, we proposed a novel method termed STC-NLSTM for FER. Unlike
most of the existing deep learning based methods, which obtain the
classification results based on the outputs of the last fully-connected layer,
STC-NLSTM aims to taken into account the multi-level features encoded in the
intermediate layers of the network. To achieve this, the architecture of STC-
NLSTM is designed to involve three major components: 3DCNN, T-LSTMs and
C-LSTM. Each component is devised carefully to own a specific ability. The
3DCNN module plays the role of extracting the spatio-temporal convolutional
features of facial expressions. The T-LSTM modules take charge of capturing
the temporal dynamics that depict the facial appearance variations in temporal
domain, and the C-LSTM is responsible for seizing the multi-level features
encoded in the individual convolutional layers of the network. All the three
components are integrated into an end-to-end network so as to cooperate
seamlessly with each other. Experiments on four public datasets demonstrated
that STC-NLSTM is superior to the state-of-the-art methods.
## Acknowledgements
The work of Qingshan Liu is supported by National Natural Science Foundation
of China (NSFC) under Grant61532009. The work of Guangcan Liu is supported in
part by NSFC under grants 61622305 and 61502238, and in part by the Natural
Science Foundation of Jiangsu Province of China (NSFJPC) under Grant
BK20160040.
Recommended articles"
236,238,Spatio-temporal facial expression recognition using convolutional neural networks and conditional random fields,"['B Hasani', 'MH Mahoor']",2017,116,"MMI Facial Expression, Static Facial Expression in the Wild, Toronto Face Database","FER, facial expression recognition, neural network","a network for the task of facial expression recognition in videos. The first part of our network  is a Deep Neural Network  are used to evaluate the network: CK+, MMI, and FERA. We show",No DOI,… Conference on Automatic Face & …,https://ieeexplore.ieee.org/document/7961822,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
237,239,"Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark","['A Dhall', 'R Goecke', 'S Lucey']",2011,706,"Acted Facial Expressions In The Wild, Static Facial Expression in the Wild","FER, classification, classifier, facial expression recognition, machine learning","and testing protocol for expression recognition as part of the BEFIT work Acted Facial  Expressions in the Wild (AFEW) [9]. Therefore, we name it the Static Facial Expressions in the Wild (",No DOI,2011 IEEE international …,https://ieeexplore.ieee.org/document/6130508,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
238,240,Static topographic modeling for facial expression recognition and analysis,"['J Wang', 'L Yin']",2007,119,MMI Facial Expression,facial expression recognition,Maja Pantic’s group at Imperial College London for providing the MMI facial expression  database. This work is supported in part by the National Science Foundation under Grants IIS-,No DOI,Computer Vision and Image Understanding,https://www.sciencedirect.com/science/article/pii/S107731420600227X,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
239,241,Subject independent facial expression recognition with robust face detection using a convolutional neural network,"['M Matsugu', 'K Mori', 'Y Mitari', 'Y Kaneda']",2003,975,Affective Faces Database,neural network,"convolutional network architecture for robust face detection and  in the CNN, between neutral  and emotional faces. We show that the  neural networks for facial expression recognition.",No DOI,Neural networks,https://www.sciencedirect.com/science/article/pii/S0893608003001151,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
240,242,Supervised committee of convolutional neural networks in automated facial expression analysis,"['G Pons', 'D Masip']",2017,119,"Affective Faces Database, MMI Facial Expression","CNN, classification, neural network",Learning could improve facial expression classification tasks.  application of deep CNNs to  facial expression classification [6].  In this experiment we describe this configuration as MMI+,No DOI,IEEE Transactions on Affective Computing,https://www.computer.org/csdl/journal/ta/2018/03/08039231/13rRUx0xPgA,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
241,243,Temporal multimodal fusion for video emotion classification in the wild,"['V Vielzeuf', 'S Pateux', 'F Jurie']",2017,209,Expression in-the-Wild,classification,EmotioNet Challenge: Recognition of facial expressions of emotion in the wild. arXiv preprint  arXiv: Emotion recognition in the wild with feature fusion and multiple kernel learning. In,No DOI,Proceedings of the 19th ACM International …,https://arxiv.org/abs/1709.07200,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
242,244,The Dartmouth Database of Children's Faces: Acquisition and validation of a new face stimulus set,"['KA Dalrymple', 'J Gomez', 'B Duchaine']",2013,165,Radboud Faces Database,classification,is comparable to rates from other published face databases [7]. Happy and Content were  the most accurately identified expressions; raters correctly classified 97.8% of the Happy (teeth,No DOI,PloS one,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3828408/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
243,245,The Indian spontaneous expression database for emotion recognition,"['SL Happy', 'P Patnaik', 'A Routray']",2015,120,Affective Faces Database,"classifier, facial expression recognition","facial expression recognition algorithms. In this paper, we propose and establish a new facial  expression database  face database including facial expression videos elicited by watching",No DOI,… on Affective Computing,https://arxiv.org/abs/1512.00932,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
244,246,The Karolinska directed emotional faces: a validation study,"['E Goeleven', 'R De Raedt', 'L Leyman']",2008,963,Karolinska Directed Emotional Faces,"classification, facial expression recognition","In this study, 490 pictures of human facial expressions from the Karolinska Directed Emotional  Faces database (KDEF; Lundqvist et al., Citation1998) were validated. The pictures were",No DOI,… and emotion,https://www.tandfonline.com/doi/abs/10.1080/02699930701626582,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
245,247,The MUG facial expression database,"['N Aifanti', 'C Papachristou']",2010,479,Affective Faces Database,facial expression recognition,face in human communication it is natural that much research is conducted on facial expression  recognition the six facial expressions are performed according to the ’emotion prototypes,No DOI,… Workshop on Image …,https://mug.ee.auth.gr/fed/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
246,248,The Toronto paper matching system: an automated paper-reviewer assignment system,"['L Charlin', 'R Zemel']",2013,170,Toronto Face Database,machine learning,We thank members of the Machine Learning group at the University of Toronto for their  help in developing the system (notably Hugo Larochelle and Amit Gruber) and for valuable,No DOI,,https://www.cs.toronto.edu/~lcharlin/papers/tpms.pdf,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
247,249,The elements of end-to-end deep face recognition: A survey of recent advances,"['H Du', 'H Shi', 'D Zeng', 'XP Zhang', 'T Mei']",2022,127,Toronto Face Database,deep learning,"In Section 5, we provide a review of deep learning-based methods for discriminative face   The images in PASCAL faces dataset [280] are taken from the Pascal person layout dataset [",No DOI,ACM Computing Surveys (CSUR …,https://arxiv.org/abs/2009.13290,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
248,250,The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression,"['P Lucey', 'JF Cohn', 'T Kanade', 'J Saragih']",2010,5135,Extended Cohn-Kanade,"classification, classifier, deep learning, machine learning, neural network","It involves computer vision, machine learning and behavioral sciences, and can be used  for many applications such as security [20], human-computer-interaction [23], driver safety [24],",No DOI,2010 ieee computer …,https://ieeexplore.ieee.org/document/5543262,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
249,251,The facial emotion recognition (FER-2013) dataset for prediction system of micro-expressions face using the convolutional neural network (CNN) algorithm based …,"['L Zahara', 'P Musa', 'EP Wibowo', 'I Karim']",2020,147,Facial Expression Recognition 2013,facial expression recognition,Training process using dataset FER-2013 The data training process in this study utilizing  the FER2013 dataset has been preprocessed and called the dataset with the file name fer2013.,No DOI,2020 Fifth international …,https://ieeexplore.ieee.org/document/9288560/,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
250,252,The first facial expression recognition and analysis challenge,"['MF Valstar', 'B Jiang', 'M Mehu', 'M Pantic']",2011,452,Affective Faces Database,facial expression recognition,In section III we describe the challenge protocol for both the AU detection and emotion detection  sub-challenges. Section IV then describes the baseline method and the baseline results,No DOI,… & gesture recognition …,https://ieeexplore.ieee.org/document/5771374,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
251,253,The role of the amygdala in atypical gaze on emotional faces in autism spectrum disorders,"['D Kliemann', 'I Dziobek', 'A Hatri', 'J Baudewig']",2012,237,Karolinska Directed Emotional Faces,classification,"gaze in ASD we applied a facial emotion classification task, using eye tracking during  fMRI, varying the initial fixation position on faces. We hypothesized that individuals with ASD",No DOI,Journal of …,https://pubmed.ncbi.nlm.nih.gov/22787032/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
252,254,Three convolutional neural network models for facial expression recognition in the wild,"['J Shao', 'Y Qian']",2019,181,"Acted Facial Expressions In The Wild, Expression in-the-Wild, Facial Expression Recognition 2013, Static Facial Expression in the Wild","CNN, FER, classification, deep learning, facial expression recognition, machine learning, neural network",Facial expressions in the wild have hundreds of thousands of variations referring to different   expressions are relatively easier to be recognized from an acted face than from a face in the,No DOI,Neurocomputing,https://www.sciencedirect.com/science/article/pii/S0925231219306137,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
253,255,Tied factor analysis for face recognition across large pose differences,"['SJD Prince', 'JH Elder', 'J Warrell']",2008,272,Toronto Face Database,machine learning,in face data across different poses. Our model was applied to both face identification and   The system described here is a pure machine learning approach that knows very little about,No DOI,… machine intelligence,https://ieeexplore.ieee.org/document/4459336,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
254,256,Torontocity: Seeing the world with a million eyes,"['S Wang', 'M Bai', 'G Mattyus', 'H Chu', 'W Luo']",2016,210,Toronto Face Database,"classification, neural network",", which covers the full greater Toronto area (GTA) with 712.5 semantic labeling and scene  type classification (recognition).  , tree detection and tree species classification as well as traffic",No DOI,arXiv preprint arXiv …,https://arxiv.org/abs/1612.00423,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
255,257,Training deep neural networks on noisy labels with bootstrapping,"['S Reed', 'H Lee', 'D Anguelov', 'C Szegedy']",2014,1181,Toronto Face Database,"deep learning, neural network","On the Toronto Face Database, we show that our model  can also benefit from unlabeled  face images with no modification  we train a deep neural network with our proposed consistency",No DOI,arXiv preprint arXiv …,https://arxiv.org/abs/1412.6596,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
256,258,Unsupervised face normalization with extreme pose and expression in the wild,"['Y Qian', 'W Deng', 'J Hu']",2019,112,Expression in-the-Wild,machine learning,"in the wild with unpaired data. To this end, we propose a Face Normalization Model (FNM) to  generate a frontal, neutral expression Our FNM is an end-to-end deep learning model. FNM",No DOI,… of the IEEE/CVF Conference on …,https://openaccess.thecvf.com/content_CVPR_2019/papers/Qian_Unsupervised_Face_Normalization_With_Extreme_Pose_and_Expression_in_the_CVPR_2019_paper.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
257,259,Using CNN for facial expression recognition: a study of the effects of kernel size and number of filters on accuracy,"['A Agrawal', 'N Mittal']",2020,300,Facial Expression Recognition 2013,facial expression recognition,problem of facial expression recognition using  facial expression recognition problem are  influenced by these architectures. This work tries to overcome this limitation by using FER-2013,No DOI,The Visual Computer,https://link.springer.com/article/10.1007/s00371-019-01630-9,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
258,260,Using colour local binary pattern features for face recognition,"['JY Choi', 'KN Plataniotis', 'YM Ro']",2010,103,Toronto Face Database,classification,"of a face image for FR purpose. We evaluate the proposed feature using three public face  databases: CMU-PIE,  As in any classification task, feature extraction is of prime importance in",No DOI,2010 IEEE International …,https://ieeexplore.ieee.org/document/5653653,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
259,261,Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing,"['TP Jung', 'TJ Sejnowski']",2019,228,Affective Faces Database,"deep learning, machine learning","For each dataset, we first individually evaluate the emotion-classification performance   , we utilized these networks on face-images using a deep network pretrained on VGG-faces",No DOI,IEEE Transactions on Affective …,https://arxiv.org/abs/1905.07039,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,
260,262,Video and image based emotion recognition challenges in the wild: Emotiw 2015,"['A Dhall', 'OV Ramana Murthy', 'R Goecke']",2015,384,"Acted Facial Expressions In The Wild, Expression in-the-Wild, Static Facial Expression in the Wild","classification, classifier, deep learning, facial expression recognition, machine learning","For the VReco sub-challenge, the facial features should  emotion recognition method on  the Acted Facial Expressions in the Wild database 5.0 and single image based facial expression",No DOI,Proceedings of the …,https://dl.acm.org/doi/10.1145/2818346.2829994,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
