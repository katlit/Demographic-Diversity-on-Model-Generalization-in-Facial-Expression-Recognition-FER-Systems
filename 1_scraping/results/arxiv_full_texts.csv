,Unnamed: 0,Title,Authors,Year,Cited By,Detected_Dataset,Detected_Topic,Abstract,DOI,Journal,URL,Mentions_Accuracy,Mentions_F1,Mentions_Precision,Mentions_Recall,Mentions_Auc,Mentions_Roc,Mentions_Sensitivity,Mentions_Specificity,Mentions_Confusion_matrix,Mentions_Loss_function,Mentions_Cross-entropy,Mentions_Mean_squared_error,Mentions_Overfitting,Mentions_Underfitting,Mentions_Cross-validation,Mentions_Training_time,Mentions_Inference_time,Mentions_Statistical_significance,Mentions_P-value,Mentions_T-test,Mentions_Anova,Mentions_Correlation,Mentions_Regression,Mentions_Baseline_comparison,Mentions_Mae,Mentions_Rmse,Mentions_Bias,Base_Domain,Full_Text
0,0,10 Automated Face Analysis for Affective Computing,"['JF Cohn', 'F De la Torre']",2015,170,Affective Faces Database,classifier,Differences in manual coding between databases may and do occur as well and can  contribute to impaired generalizability of classifiers from one database to another.,No DOI,The Oxford handbook of affective …,https://academic.oup.com/edited-volume/28057/chapter/212009519,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,oup.com,
1,1,3D facial expression recognition based on automatically selected features,"['H Tang', 'TS Huang']",2008,205,Binghamton University 3D Facial Expression,"classification, classifier, facial expression recognition",facial expression recognition from 3D facial shapes is inves distances between 83 facial  feature points in the 3D space. Using a  al. at Binghamton University. It was designed to sample,No DOI,… on computer vision and pattern recognition …,https://ieeexplore.ieee.org/document/4563052,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
2,2,3D facial expression recognition based on primitive surface feature distribution,"['J Wang', 'L Yin', 'X Wei', 'Y Sun']",2006,440,Binghamton University 3D Facial Expression,facial expression recognition,expressions using 3D facial expression range data. We propose a novel approach to extract  primitive 3D facial expression  distribution to classify the prototypic facial expressions. In,No DOI,… Vision and Pattern Recognition  …,https://ieeexplore.ieee.org/document/1640921,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
3,3,3D facial expression recognition based on properties of line segments connecting facial feature points,"['H Tang', 'TS Huang']",2008,153,Binghamton University 3D Facial Expression,"classification, classifier, facial expression recognition",Binghamton University have recently constructed a 3D facial expression database for facial   This is definitely the first attempt to make a publicly available 3D facial expression database,No DOI,… on Automatic Face & Gesture Recognition,https://ieeexplore.ieee.org/document/4813304,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
4,4,3D facial expression recognition using SIFT descriptors of automatically detected keypoints,"['S Berretti', 'B Ben Amor', 'M Daoudi', 'A Del Bimbo']",2011,184,Binghamton University 3D Facial Expression,"classification, classifier, facial expression recognition","at the Binghamton University (BU-3DFE database) [34], and at the Bogaziçi University (  facial expression recognition algorithms. This is due to the fact that, differently from other",No DOI,The Visual Computer,https://www.researchgate.net/publication/220068159_3D_facial_expression_recognition_using_SIFT_descriptors_of_automatically_detected_keypoints,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,researchgate.net,
5,5,A 3D facial expression database for facial behavior research,"['L Yin', 'X Wei', 'Y Sun', 'J Wang']",2006,1631,Binghamton University 3D Facial Expression,"classification, classifier, deep learning, facial expression recognition, machine learning, neural network",3D: Critical Issues and Limitations of 2D (1) 3D surface features exhibited in facial expressions  The common theme in the current research on face expression recognition is that the face,No DOI,… and gesture recognition  …,https://ieeexplore.ieee.org/document/1613022,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
6,6,A deep learning perspective on the origin of facial expressions,"['R Breuer', 'R Kimmel']",2017,140,"Acted Facial Expressions In The Wild, Extended Cohn-Kanade, Static Facial Expression in the Wild","CNN, FER, deep learning, machine learning","We verify our findings on the Extended Cohn-Kanade (CK+),  tasks and tests using transfer  learning, including cross-dataset  long-short-term-memory (LSTM) recurrent neural network (",No DOI,arXiv preprint arXiv:1705.01842,https://arxiv.org/abs/1705.01842,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"BREUER,KIMMEL:ADEEPLEARNINGPERSPECTIVEONFACIALEXPRESSIONS 1
A Deep Learning Perspective on the Origin
of Facial Expressions
RanBreuer DepartmentofComputerScience
rbreuer@cs.technion.ac.il Technion-IsraelInstituteofTechnology
RonKimmel TechnionCity,Haifa,Israel
ron@cs.technion.ac.il
Figure1: Demonstrationofthefiltervisualizationprocess.
Abstract
Facial expressions play a significant role in human communication and behavior.
Psychologists have long studied the relationship between facial expressions and emo-
tions. PaulEkmanetal.[17,20],devisedtheFacialActionCodingSystem(FACS)to
taxonomize human facial expressions and model their behavior. The ability to recog-
nize facial expressions automatically, enables novel applications in fields like human-
computer interaction, social gaming, and psychological research. There has been a
tremendouslyactiveresearchinthisfield,withseveralrecentpapersutilizingconvolu-
tionalneuralnetworks(CNN)forfeatureextractionandinference.Inthispaper,weem-
ployCNNunderstandingmethodstostudytherelationbetweenthefeaturesthesecom-
putationalnetworksareusing,theFACSandActionUnits(AU).Weverifyourfindings
ontheExtendedCohn-Kanade(CK+),NovaEmotionsandFER2013datasets.Weapply
these models tovarious tasks and tests usingtransfer learning, including cross-dataset
validationandcross-taskperformance. Finally,weexploitthenatureoftheFERbased
CNNmodelsforthedetectionofmicro-expressionsandachievestate-of-the-artaccuracy
usingasimplelong-short-term-memory(LSTM)recurrentneuralnetwork(RNN).
(cid:13)c 2017.Thecopyrightofthisdocumentresideswithitsauthors.
Itmaybedistributedunchangedfreelyinprintorelectronicforms.
7102
yaM
01
]VC.sc[
2v24810.5071:viXra2 BREUER,KIMMEL:ADEEPLEARNINGPERSPECTIVEONFACIALEXPRESSIONS
Figure 2: Example of primary universal emotions. From left to right: disgust, fear, happi-
ness,surprise,sadness,andanger.2
1 Introduction
Humancommunicationconsistsofmuchmorethanverbalelements, wordsandsentences.
Facialexpressions(FE)playasignificantroleininter-personinteraction. Theyconveyemo-
tional state, truthfulness and add context to the verbal channel. Automatic FE recognition
(AFER) is an interdisciplinary domain standing at the crossing of behavioral science, psy-
chology,neurology,andartificialintelligence.
1.1 FacialExpressionAnalysis
Theanalysisofhumanemotionsthroughfacialexpressionsisamajorpartinpsychological
research. Darwin’sworkinthelate1800’s[13]placedhumanfacialexpressionswithinan
evolutionary context. Darwin suggested that facial expressions are the residual actions of
more complete behavioral responses to environmental challenges. When in disgust, con-
strictingthenostrilsservedtoreduceinhalationofnoxiousorharmfulsubstances. Widening
oftheeyesinsurpriseincreasedthevisualfieldtobetterseeanunexpectedstimulus.
Inspired by Darwin’s evolutionary basis for expressions, Ekman et al. [20] introduced
their seminal study about facial expressions. They identified seven primary, universal ex-
pressionswhereuniversalityrelatedtothefactthattheseexpressionsremainthesameacross
differentcultures[18]. Ekmanlabeledthembytheircorrespondingemotionalstates,thatis,
happiness, sadness, surprise, fear, disgust, anger, and contempt , see Figure 2. Due to its
simplicityandclaimforuniversality,theprimaryemotionshypothesishasbeenextensively
exploitedincognitivecomputing.
Inordertofurtherinvestigateemotionsandtheircorrespondingfacialexpressions, Ek-
man devised the facial action coding system (FACS) [17]. FACS is an anatomically based
systemfordescribingallobservablefacialmovementsforeachemotion,seeFigure3.Using
FACSasamethodologicalmeasuringsystem,onecandescribeanyexpressionbytheaction
units(AU)oneactivatesanditsactivationintensity. Eachactionunitdescribesaclusterof
facialmusclesthatacttogethertoformaspecificmovement.AccordingtoEkman,thereare
44 facial AUs, describing actions such as “open mouth”, “squint eyes” etc., and 20 other
AUs were addedin a 2002 revisionof the FACS manual [21], to account for headand eye
movement.
2Imagestakenfrom[42](cid:13)cJeffreyCohnBREUER,KIMMEL:ADEEPLEARNINGPERSPECTIVEONFACIALEXPRESSIONS 3
Figure3: ExpressiveimagesandtheiractiveAUcoding.Thisdemonstratesthecomposition
ofdescribingone’sfacialexpressionusingacollectionofFACSbaseddescriptors.
1.2 FacialExpressionRecognitionandAnalysis
Theabilitytoautomaticallyrecognizefacialexpressionsandinfertheemotionalstatehasa
widerangeofapplications. Theseincludedemotionallyandsociallyawaresystems[14,16,
51],improvedgamingexperience[7],driverdrowsinessdetection[52],anddetectingpainin
patients[38]aswellasdistress[28]. Recentpapershaveevenintegratedautomaticanalysis
ofviewers’reactionfortheeffectivenessofadvertisements[1,2,3].
Various methods have been used for automatic facial expression recognition (FER or
AFER)tasks. Earlypapersusedgeometricrepresentations,forexample,vectorsdescriptors
for the motion of the face [10], active contours for mouth and eye shape retrieval [6], and
using2Ddeformablemeshmodels[31]. Otherusedappearancerepresentationbasedmeth-
ods,suchasGaborfilters[34],orlocalbinarypatterns(LBP)[43]. Thesefeatureextraction
methodsusuallywerecombinedwithoneofseveralregressorstotranslatethesefeaturevec-
torstoemotionclassificationoractionunitdetection. Themostpopularregressorsusedin
this context were support vector machines (SVM) and random forests. For further reading
onthemethodsusedinFER,wereferthereaderto[11,39,55,58]
1.3 UnderstandingConvolutionalNeuralNetworks
Overthelastpartofthispastdecade, convolutionalneuralnetworks(CNN)[33]anddeep
beliefnetworks(DBN)havebeenusedforfeatureextraction,classificationandrecognition
tasks. TheseCNNshaveachievedstate-of-the-artresultsinvariousfields,includingobject
recognition[32],facerecognition[47],andsceneunderstanding[60]. Leadingchallengesin
FER[15,49,50]havealsobeenledbymethodsusingCNNs[9,22,25].
Convolutional neural networks, as first proposed by LeCun in 1998 [33], employ con-
ceptsofreceptivefieldsandweightsharing. Thenumberoftrainableparametersisgreatly
reducedandthepropagationofinformationthroughthelayersofthenetworkcanbesimply
calculatedbyconvolution. Theinput,likeanimageorasignal,isconvolvedthroughafilter
collection (or map) in the convolution layers to produce a feature map. Each feature map
detectsthepresenceofasinglefeatureatallpossibleinputlocations.
In the effort of improving CNN performance, researchers have developed methods of
exploringandunderstandingthemodelslearnedbythesemethods. [45]demonstratedhow4 BREUER,KIMMEL:ADEEPLEARNINGPERSPECTIVEONFACIALEXPRESSIONS
saliencymapscanbeobtainedfromaConvNetbyprojectingbackfromthefullyconnected
layersofthenetwork. [23]showedvisualizationsthatidentifypatcheswithinadatasetthat
areresponsibleforstrongactivationsathigherlayersinthemodel.
Zeiler et al. [56, 57] describe using deconvolutional networks as a way to visualize a
singleunitinafeaturemapofagivenCNN,trainedonthesamedata. Themainideaisto
visualizetheinputpixelsthatcauseacertainneuron,likeafilterfromaconvolutionallayer,
to maximize its output. This process involves a feed forward step, where we stream the
inputthroughthenetwork,whilerecordingtheconsequentactivationsinthemiddlelayers.
Afterwards, one fixes the desired filter’s (or neuron) output, and sets all other elements to
the neutral elements (usually 0). Then, one “back-propagates” through the network all the
way to the input layer, where we would get a neutral image with only a few pixels set -
those are the pixels responsible for max activation in the fixed neuron. Zeiler et al. found
that while the first layers in the CNN model seemed to learn Gabor-like filters, the deeper
layers were learning high level representations of the objects the network was trained to
recognize.Byfindingthemaximalactivationforeachneuron,andback-propagatingthrough
thedeconvolutionlayers,onecouldactuallyviewthelocationsthatcausedaspecificneuron
toreact.
FurthereffortstounderstandthefeaturesintheCNNmodel,weredonebySpringenberg
et al. who devised guided back-propagation [46]. With some minor modifications to the
deconvolutionalnetworkapproach,theywereabletoproducemoreunderstandableoutputs,
whichprovidedbetterinsightintothemodel’sbehavior.Theabilitytovisualizefiltermapsin
CNNsimprovedthecapabilityofunderstandingwhatthenetworklearnsduringthetraining
stage.
Themaincontributionsofthispaperareasfollows.
• We employ CNN visualization techniques to understand the model learned by cur-
rentstate-of-the-artmethodsinFERonvariousdatasets. Weprovideacomputational
justification for Ekman’s FACS[17] as a leading model in the study of human facial
expressions.
• Weshowthegeneralizationcapabilityofnetworkstrainedonemotiondetection,both
acrossdatasetsandacrossvariousFERrelatedtasks.
• We discuss various applications of FACS based feature representation produced by
CNN-basedFERmethods.
2 Experiments
Ourgoalistoexploretheknowledge(ormodels)aslearnedbystate-of-the-artmethodsfor
FER,similartotheworksof[29]. WeuseCNN-basedmethodsonvariousdatasetstogeta
senseofacommonmodelstructure,andstudytherelationofthesemodelstoEkman’sFACS
[17]. To inspect the learned models ability to generalize, we use the method of transfer
learning [54] to see how these models perform on other datasets. We also measure the
models’abilitytoperformonotherFERrelatedtasks, oneswhichtheywerenotexplicitly
trainedfor.
InordertogetasenseofthecommonpropertiesofCNN-basedstate-of-the-artmodels
in FER, we employ these methods on numerous datasets. Below are brief descriptions of
datasetsusedinourexperiments. SeeFigure4forexamples.BREUER,KIMMEL:ADEEPLEARNINGPERSPECTIVEONFACIALEXPRESSIONS 5
Figure4: ImagesfromCK+(top),NovaEmotions(middle)andFER2013(bottom)datasets.
ExtendedCohn-KanadeTheExtendedCohn-Kanadedataset(CK+)[37],iscomprised
ofvideosequencesdescribingthefacialbehaviorof210adults. Participantagesrangefrom
18to50.69%arefemale,91%Euro-American,13%Afro-American,and6%belongtoother
groups.Thedatasetiscomposedof593sequencesfrom123subjectscontainingposedfacial
expressions. Another107sequenceswereaddedaftertheinitialdatasetwasreleased. These
sequencescapturedspontaneousexpressionsperformedbetweenformalsessionsduringthe
initialrecordings,thatis,non-posedfacialexpressions.
Data from the Cohn-Kanade dataset is labeled for emotional classes (of the 7 primary
emotionsbyEkman[20])atpeakframes.Inaddition,AUlabelingwasdonebytwocertified
FACScoders. Inter-coderagreementverificationwasperformedforallreleaseddata.
NovaEmotions NovaEmotions [40, 48], aim to represent facial expressions and emo-
tional state as captured in a non-controlled environment. The data is collected in a crowd-
sourcingmanner,wheresubjectswereputinfrontofagamingdevice,whichcapturedtheir
response to scenes and challenges in the game itself. The game, in time, reacted to the
player’s response as well. This allowed collecting spontaneous expressions from a large
poolofvariations.
TheNovaEmotionsdatasetconsistsofover42,000imagestakenfrom40differentpeo-
ple. Majority of the participants were college students with ages ranges between 18 and
25. Datapresentsavarietyofposesandillumination. Inthispaperweusecroppedimages
containing only the face regions. Images were aligned such that eyes are presented on the
same horizontal line across all images in the dataset. Each frame was annotated by mul-
tiple sources, both certified professionals as well as random individuals. A consensus was
collectedfortheannotationoftheframes,resultinginthefinallabeling.
FER 2013 The FER 2013 challenge [24] was created using Google image search API
with184 emotionrelated keywords, like blissful, enraged. Keywordswere combinedwith
phrases for gender, age and ethnicity in order to obtain up to 600 different search queries.
Image data was collected for the first 1000 images for each query. Collected images were
passed through post-processing, that involved face region cropping and image alignment.
Images were then grouped into the corresponding fine-grained emotion classes, rejecting
wrongfullylabeledframesandadjustingcroppedregions. Theresultingdatacontainsnearly
36,000images,dividedinto8classes(7effectiveexpressionsandaneutralclass),witheach6 BREUER,KIMMEL:ADEEPLEARNINGPERSPECTIVEONFACIALEXPRESSIONS
emotionclasscontainingafewthousandimages(disgustbeingtheexceptionwithonly547
frames).
2.1 NetworkArchitectureandTraining
Forallexperimentsdescribedinthispaper,weimplementedasimple,classicfeed-forward
convolutionalneuralnetwork. Eachnetworkisstructuredasfollows. Aninputlayer,receiv-
ing a gray-level or RGB image. The input is passed through 3 convolutional layer blocks,
each block consists of a filter map layer, a non-linearity (or activation) and a max pooling
layer. Ourimplementationiscomprisedof3convolutionalblocks,eachwitharectifiedlin-
earunit(ReLU[12])activationandapoolinglayerwith2×2poolsize. Theconvolutional
layershavefiltermapswithincreasingfilter(neuron)countthedeeperthelayeris,resulting
in a 64, 128 and 256 filter map sizes, respectively. Each filter in our experiments supports
5×5pixels.
Theconvolutionalblocksarefollowedbyafully-connectedlayerwith512hiddenneu-
rons. Thehiddenlayer’soutputistransferredtotheoutputlayer, whichsizeisaffectedby
thetaskinhand,8foremotionclassification,andupto50forAUlabeling. Theoutputlayer
canvaryinactivation,forexample,forclassificationtasksweprefersoftmax.
To reduce over-fitting, we used dropout [12]. We apply the dropout after the last con-
volutional layer and between the fully-connected layers, with probabilities of 0.25 and 0.5
respectively. Adropoutprobability pmeansthateachneuron’soutputissetto0withprob-
ability p.
WetrainedournetworkusingADAM[30]optimizerwithalearningrateof1e−3and
a decay rate of 1e−5. To maximize generalization of the model, we use methods of data
augmentation. We use combinations of random flips and affine transforms, e.g. rotation,
translation,scaling,sheer,ontheimagestogeneratesyntheticdataandenlargethetraining
set. OurimplementationisbasedontheKeras[8]librarywithTensorFlow[5]back-end. We
useOpenCV[4]forallimageoperations.
3 Results and Analysis
Weverifytheperformanceofournetworksonthedatasetsmentionedin2usinga10-fold
cross validation technique. For comparison, we use the frameworks of [24, 34, 35, 43].
We analyze the networks’ ability to classify facial expression images into the 7 primary
emotions or as a neutral pose. Accuracy is measured as the average score of the 10-fold
crossvalidation. Ourmodelperformsatstate-of-the-artlevelwhencomparedtotheleading
methodsinAFER,SeeTables1,2.
3.1 VisualizingtheCNNFilters
After establishing a sound classification framework for emotions, we move to analyze the
modelsthatwerelearnedbythesuggestednetwork. WeemployZeileretal.andSpringen-
berg’s [46, 56] methods for visualizing the filters trained by the proposed networks on the
differentemotionclassificationtasks,seeFigure1.
Asshownby[56],thelowerlayersprovidelowlevelGabor-likefilterswhereasthemid
andhigherlayers,thatareclosertotheoutput,providehighlevel,humanreadablefeatures.BREUER,KIMMEL:ADEEPLEARNINGPERSPECTIVEONFACIALEXPRESSIONS 7
Method Accuracy
Method Accuracy
HumanAccuracy 68%±5%
Gabor+SVM[34] 89.8%
RBM 71.162%
LBPSVM[43] 95.1%
VGGCNN[44] 72.7%
AUDN[35] 93.70%
ResNetCNN[26] 72.4%
BDBN[36] 96.7%
Ours 72.1%±0.5%
Ours 98.62%±0.11%
Table 2: Accuracy evaluation of
Table 1: Accuracy evaluation of
emotion classification on the FER
emotion classification on the CK+
2013challenge. Methodsandscores
dataset.
aredocumentedin[24,39].
Byusingthemethodsabove,wevisualizethefeaturesofthetrainednetwork. Featurevisu-
alization is shown in Figure 5 through input that maximized activation of the desired filter
alongside the pixels that are responsible for the said response. From analyzing the trained
models,onecannoticegreatsimilaritybetweenournetworks’featuremapsandspecificfa-
cial regions and motions. Further investigation shows that these regions and motions have
significantcorrelationtothoseusedbyEkmantodefinetheFACSActionUnits,seeFigure
6.
Figure5: Featurevisualizationforthetrainednetworkmodel. Foreachfeatureweoverlay
thedeconvolutionoutputontopofitsoriginalinputimage. Onecaneasilyseetheregionsto
whicheachfeaturerefers.
We matched a filter’s suspected AU representation with the actual CK+ AU labeling,
usingthefollowingmethod.
1. Givenaconvolutionallayerlandfilter j,theactivationoutputismarkedasF .
l,j
2. WeextractedthetopN inputimagesthatmaximized,i=arg maxF (i).
i l,j
3. For each input i, the manually annotated AU labeling is A44×1. A is 1 if AU u is
i i,u
presentini.
4. Thecorrelationoffilter jwithAUu’spresenceisP andisdefinedbyP =
∑Ai,u.
j,u j,u N
SinceweusedasmallN,werejectedcorrelationswithP <1.Outof50activeneurons
j,u
froma256filtersmaptrainedonCK+,only7wererejected. Thisshowsanamazinglyhigh
correlation between a CNN-based model, trained with no prior knowledge, and Ekman’s
facialactioncodingsystem(FACS).
Inaddition,wefoundthateventhoughsomeAU-inspiredfilterswerecreatedmorethan
just once, a large amount of neurons in the highest layers were found “dead”, that is, they8 BREUER,KIMMEL:ADEEPLEARNINGPERSPECTIVEONFACIALEXPRESSIONS
werenotproducingeffectiveoutputforanyinput. Theamountofactiveneuronsinthelast
convolutionallayerwasabout30%ofthefeaturemapsize(60outof256). Thenumberof
effectiveneuronsissimilartothesizeofEkman’svocabularyofactionunitsbywhichfacial
expressionscanbeidentified.
AU4:Browlowerer AU5:Upperlidraiser AU9:Nosewrinkler
AU10:Upperlipraiser AU12:LipCornerPuller AU25:LipsPart
Figure6: SeveralfeaturemapsandtheircorrespondingFACSActionUnit.
3.2 ModelGeneralityandTransferLearning
AftercomputationallydemonstratingthestrongcorrelationbetweenEkman’sFACSandthe
modellearnedbytheproposedcomputationalneuralnetwork,westudythemodel’sability
togeneralizeandsolveotherproblemsrelatedtoexpressionrecognitiononvariousdatasets.
Weusethetransferlearningtrainingmethodology[54]andapplyittodifferenttasks.
Transfer learning, or knowledge transfer, aims to use models that were pre-trained on
differentdatafornewtasks. Neuralnetworkmodelsoftenrequirelargetrainingsets. How-
ever,insomescenariosthesizeofthetrainingsetisinsufficientforpropertraining. Transfer
learning allows using the convolutional layers as pre-trained feature extractors, with only
theoutputlayersbeingreplacedormodifiedaccordingtothetaskathand. Thatis,thefirst
layersaretreatedaspre-definedfeatures, whilethelastlayers, thatdefinethetaskathand,
areadaptedbylearningbasedontheavailabletrainingset.
We tested our models on both cross-dataset and cross-task capabilities. In most FER
related tasks, AU detection is done as a leave-one-out manner. Given an input (image or
video)thesystemwouldpredicttheprobabilityofaspecificAUtobeactive. Thismethod
is proven to be more accurate than training against the detection of all AU activations at
the same time, mostly due to the sizes of the training datasets. When testing our models
against detection of a single AU, we recorded high accuracy scores with most AUs.Some
action units, like AU11: nasolabial deepener, were not predicted properly in some cases
whenusingthesuggestedmodel. AbetterpredictionmodelfortheseAUswouldrequirea
dedicated set of features that focus on the relevant region in the face, since they signify a
minorfacialmovement.
Theleave-one-outapproachiscommonlyusedsincethetrainingsetisnotlargeenough
to train a classifier for all AUs simultaneously (all-against-all). In our case, predicting all
AUactivationssimultaneouslyforasingleimage,requiresalargerdatasetthantheoneweBREUER,KIMMEL:ADEEPLEARNINGPERSPECTIVEONFACIALEXPRESSIONS 9
Test
CK+ FER2013 NovaEmotions
Train
CK+ 98.62% 69.3% 67.2%
FER2013 92.0% 72.1% 78.0%
NovaEmotions 93.75% 71.8% 81.3%
Table3: Crossdatasetapplicationofemotiondetectionmodels.
used. Havingtrainedourmodeltopredictonlyeightclasses,weverifyourmodelonanall-
against-allapproachandobtainedresultthatcompetewiththeleave-one-outclassifiers. In
ordertoincreaseaccuracy,weapplyasparsityinducinglossfunctionontheoutputlayerby
combiningbothL andL terms. ThisresultedinasparseFACScodingoftheinputframe.
2 1
Whentestingforbinaryrepresentation,thatis,onlyanactive/nonactivepredictionperAU,
werecordedanaccuracyrateof97.54%. WhenpredictingAUintensity,anintegerofrange
0to5,werecordedanaccuracyrateof96.1%withameansquareerror(MSE)of0.2045.
When testing emotion detection capabilities across datasets, we found that the trained
modelshadveryhighscores. Thisshows,onceagain,thattheFACS-likefeaturestrainedon
onedatasetcanbeappliedalmostdirectlytoanother,seeTable3.
4 Micro-Expression Detection
Micro-expressions(ME)areamorespontaneousandsubtlefacialmovementsthathappenin-
voluntarily,thusrevelingone’sgenuine,underlyingemotion[19]. Thesemicro-expressions
arecomprisedofthesamefacialmovementsthatdefineFACSactionunitsanddifferinin-
tensity. MEtendtolastupto0.5sec,makingdetectionachallengingtaskforanun-trained
individual. Each ME is broken down to 3 steps: Onset, apex, and offset, describing the
beginning,peek,andtheendofthemotion,respectively.
Similar to AFER, a significant effort was invested in the last years to train computers
inordertoautomaticallydetectmicro-expressionsandemotions. Duetoitslowmovement
intensity,automaticdetectionofmicro-expressionsrequiresatemporalsequence,asopposed
to a single frame. Moreover, since micro-expressions tend to last for just a short time and
occurinabriefofamoment,ahighspeedcameraisusuallyusedforcapturingtheframes.
WeapplyourFACS-likefeatureextractorstothetaskofautomaticallydetectingmicro-
expressions. To that end, we use the CASME II dataset [53]. CASME II includes 256
spontaneousmicro-expressionsfilmedat200fps. Allvideosaretaggedforonset,apex,and
offsettimes,aswellastheexpressionconveyed. AUcodingwasaddedfortheapexframe.
Expressions were captured by showing a subject video segments that triggered the desired
response.
Toimplementourmicro-expressionsdetectionnetwork,wefirsttrainedthenetworkon
selected frames from the training data sequences. For each video, we took only the onset,
apex, and offset frames, as well as the first and last frames of the sequence, to account for
neutralposes. SimilartoSection3.2,wefirsttrainedourCNNtodetectemotions. Wethen
combinedtheconvolutionallayersfromthetrainednetwork,withalong-short-tern-memory
[27]recurrentneuralnetwork(RNN),whoseinputisconnectedtothefirstfullyconnected
layerofthefeatureextractorCNN.TheLSTMweusedisaveryshallownetwork,withonly
aLSTMlayerandanoutputlayer. RecurrentdropoutwasusedaftertheLSTMlayer.10 BREUER,KIMMEL:ADEEPLEARNINGPERSPECTIVEONFACIALEXPRESSIONS
Method Accuracy
LBP-TOP[59] 44.12%
LBP-TOPwithadaptivemagnification[41] 51.91%
Ours 59.47%
Table 4: Micro-expression detection and analysis accuracy. Comparison with reported
state-of-the-artmethods.
Wetestedournetworkwithaleave-one-outstrategy,whereonesubjectwasdesignated
astestandwasleftoutoftraining. Ourmethodperformsatstate-of-the-artlevel(Table4).
5 Conclusions
We provided a computational justification of Ekman’s facial action units (FACS) which is
thecoreofhisfacialexpressionanalysisaxiomatic/observationalframework.Westudiedthe
modelslearnedbystate-of-the-artCNNs,andusedCNNvisualizationtechniquestounder-
standthefeaturemapsthatareobtainedbytrainingforemotiondetectionofsevenuniversal
expressions. We demonstrated a strong correlation between the features generated by an
unsupervisedlearningprocessandEkman’sactionunitsusedastheatomsinhisleadingfa-
cialexpressionsanalysismethods. TheFACS-basedfeatures’abilitytogeneralizewasthen
verifiedoncross-dataandcross-taskaspectsthatprovidedhighaccuracyscores. Equipped
with refined computationally-learned action units that align with Ekman’s theory, we ap-
plied our models to the task of micro-expression detection and obtained recognition rates
thatoutperformedstate-of-the-artmethods.
TheFACSbasedmodelscanbefurtherappliedtootherFERrelatedtasks. Embedding
emotionormicro-expressionrecognitionandanalysisaspartofreal-timeapplicationscanbe
usefulinseveralfields,forexample,liedetection,gaming,andmarketinganalysis. Analyz-
ingcomputergeneratedrecognitionmodelscanhelprefineEkman’stheoryofreadingfacial
expressionsandemotionsandprovideanevenbettersupportforitsvalidityandaccuracy.
References
[1] http://www.affectiva.com.
[2] http://www.emotient.com.
[3] http://www.realeyesit.com.
[4] Opensourcecomputervisionlibrary. https://www.opencv.org,2015.
[5] MartínAbadi,PaulBarham,JianminChen,ZhifengChen,AndyDavis,JeffreyDean,
Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kud-
lur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner,
Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiao-
qiang Zheng. Tensorflow: A system for large-scale machine learning. In Proceed-
ings of the 12th USENIX Conference on Operating Systems Design and Implemen-
tation, OSDI’16, pages 265–283, Berkeley, CA, USA, 2016. USENIX Association.BREUER,KIMMEL:ADEEPLEARNINGPERSPECTIVEONFACIALEXPRESSIONS 11
ISBN 978-1-931971-33-1. URL http://dl.acm.org/citation.cfm?id=
3026877.3026899.
[6] PetarSAleksicandAggelosKKatsaggelos. Automaticfacialexpressionrecognition
using facial animation parameters and multistream hmms. IEEE Transactions on In-
formationForensicsandSecurity,1(1):3–11,2006.
[7] Sander Bakkes, Chek Tien Tan, and Yusuf Pisan. Personalised gaming: a motivation
and overview of literature. In Proceedings of The 8th Australasian Conference on
InteractiveEntertainment: PlayingtheSystem,page4.ACM,2012.
[8] FrançoisChollet. Keras. https://github.com/fchollet/keras,2015.
[9] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively,
with application to face verification. In 2005 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (CVPR’05), volume 1, pages 539–546 vol.
1,June2005. doi: 10.1109/CVPR.2005.202.
[10] IraCohen,NicuSebe,AshutoshGarg,LawrenceSChen,andThomasSHuang. Facial
expressionrecognitionfromvideosequences:temporalandstaticmodeling.Computer
Visionandimageunderstanding,91(1):160–187,2003.
[11] C. A. Corneanu, M. O. SimÃs¸n, J. F. Cohn, and S. E. Guerrero. Survey on rgb, 3d,
thermal,andmultimodalapproachesforfacialexpressionrecognition: History,trends,
and affect-related applications. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 38(8):1548–1568, Aug 2016. ISSN 0162-8828. doi: 10.1109/TPAMI.
2016.2515606.
[12] G.E.Dahl,T.N.Sainath,andG.E.Hinton. Improvingdeepneuralnetworksforlvcsr
using rectified linear units and dropout. In 2013 IEEE International Conference on
Acoustics,SpeechandSignalProcessing,pages8609–8613,May2013. doi: 10.1109/
ICASSP.2013.6639346.
[13] Charles Darwin. The expression of the emotions in man and ani-
mals / by Charles Darwin. New York ;D. Appleton and Co.„ 1916.
URL http://www.biodiversitylibrary.org/item/24064.
http://www.biodiversitylibrary.org/bibliography/4820—Includesindex.
[14] DavidDeVault,RonArtstein,GraceBenn,TeresaDey,EdFast,AlesiaGainer,Kallirroi
Georgila, Jon Gratch, Arno Hartholt, Margaux Lhommet, et al. Simsensei kiosk: A
virtualhumaninterviewerforhealthcaredecisionsupport. InProceedingsofthe2014
internationalconferenceonAutonomousagentsandmulti-agentsystems,pages1061–
1068.InternationalFoundationforAutonomousAgentsandMultiagentSystems,2014.
[15] AbhinavDhall,RolandGoecke,JyotiJoshi,MichaelWagner,andTomGedeon. Emo-
tionrecognitioninthewildchallenge2013. InProceedingsofthe15thACMonInter-
nationalconferenceonmultimodalinteraction,pages509–516.ACM,2013.
[16] Zoran Duric, Wayne D Gray, Ric Heishman, Fayin Li, Azriel Rosenfeld, Michael J
Schoelles,ChristianSchunn,andHarryWechsler. Integratingperceptualandcognitive
modelingforadaptiveandintelligenthuman-computerinteraction. Proceedingsofthe
IEEE,90(7):1272–1289,2002.12 BREUER,KIMMEL:ADEEPLEARNINGPERSPECTIVEONFACIALEXPRESSIONS
[17] P.EkmanandW.Friesen. FacialActionCodingSystem: ATechniquefortheMeasure-
mentofFacialMovement. ConsultingPsychologistsPress,PaloAlto,1978.
[18] PaulEkman. Strongevidenceforuniversalsinfacialexpressions: areplytoRussell’s
mistakencritique. PsychologyBulletin,115(2):268–287,1994.
[19] PaulEkmanandWallaceVFriesen. Nonverballeakageandcluestodeception. Psy-
chiatry,32(1):88–106,1969.
[20] PaulEkmanandDacherKeltner. Universalfacialexpressionsofemotion. California
MentalHealthResearchDigest,8(4):151–158,1970.
[21] PaulEkmanandErikaL.Rosenberg,editors. Whatthefacereveals: basicandapplied
studies of spontaneous expression using the facial action coding system(FACS) 2nd
Edition. Seriesinaffectivescience.OxfordUniversityPress,firstedition,2005.
[22] SayanGhosh,EugeneLaksana,StefanScherer,andLouis-PhilippeMorency. Amulti-
labelconvolutionalneuralnetworkapproachtocross-domainactionunitdetection. In
AffectiveComputingandIntelligentInteraction(ACII),2015InternationalConference
on,pages609–615.IEEE,2015.
[23] RossGirshick,JeffDonahue,TrevorDarrell,andJitendraMalik. Richfeaturehierar-
chies for accurate object detection and semantic segmentation. In Proceedings of the
IEEEconferenceoncomputervisionandpatternrecognition,pages580–587,2014.
[24] IanJGoodfellow,DumitruErhan,PierreLucCarrier,AaronCourville,MehdiMirza,
BenHamner,WillCukierski,YichuanTang,DavidThaler,Dong-HyunLee,etal.Chal-
lengesinrepresentationlearning: Areportonthreemachinelearningcontests. InIn-
ternational Conference on Neural Information Processing, pages 117–124. Springer,
2013.
[25] AmoghGudi,H.EmrahTasli,TimM.denUyl,andAndreasMaroulis. Deeplearning
basedFACSactionunitoccurrenceandintensityestimation.In11thIEEEInternational
Conference and Workshops on Automatic Face and Gesture Recognition, FG 2015,
Ljubljana, Slovenia, May 4-8, 2015, pages 1–5. IEEE Computer Society, 2015. doi:
10.1109/FG.2015.7284873. URL http://dx.doi.org/10.1109/FG.2015.
7284873.
[26] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningfor
image recognition. In Proceedings of the IEEE Conference on Computer Vision and
PatternRecognition,pages770–778,2016.
[27] SeppHochreiterandJürgenSchmidhuber. Longshort-termmemory. NeuralComput.,
9(8):1735–1780,November1997.ISSN0899-7667.doi:10.1162/neco.1997.9.8.1735.
URLhttp://dx.doi.org/10.1162/neco.1997.9.8.1735.
[28] JyotiJoshi, AbhinavDhall, RolandGoecke, MichaelBreakspear, andGordonParker.
Neural-net classification for spatio-temporal descriptor based depression analysis. In
PatternRecognition(ICPR),201221stInternationalConferenceon,pages2634–2638.
IEEE,2012.BREUER,KIMMEL:ADEEPLEARNINGPERSPECTIVEONFACIALEXPRESSIONS 13
[29] PooyaKhorrami,ThomasPaine,andThomasHuang. Dodeepneuralnetworkslearn
facial action units when doing expression recognition? In Proceedings of the IEEE
InternationalConferenceonComputerVisionWorkshops,pages19–27,2015.
[30] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.
CoRR,abs/1412.6980,2014. URLhttp://arxiv.org/abs/1412.6980.
[31] IreneKotsiaandIoannisPitas. Facialexpressionrecognitioninimagesequencesusing
geometric deformation features and support vector machines. IEEE transactions on
imageprocessing,16(1):172–187,2007.
[32] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classifica-
tion with deep convolutional neural networks. In Peter L. Bartlett, Fernando
C. N. Pereira, Christopher J. C. Burges, Léon Bottou, and Kilian Q. Wein-
berger, editors, Advances in Neural Information Processing Systems 25: 26th
Annual Conference on Neural Information Processing Systems 2012. Proceed-
ings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United
States., pages 1106–1114, 2012. URL http://papers.nips.cc/paper/
4824-imagenet-classification-with-deep-convolutional-neural-networks.
[33] Yann Lecun, LÃl’on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based
learning applied to document recognition. In Proceedings of the IEEE, pages 2278–
2324,1998.
[34] Gwen Littlewort, Marian Stewart Bartlett, Ian Fasel, Joshua Susskind, and Javier
Movellan. Dynamicsof facial expressionextracted automatically from video. Image
andVisionComputing,24(6):615–625,2006.
[35] Mengyi Liu, Shaoxin Li, Shiguang Shan, and Xilin Chen. Au-inspired deep
networks for facial expression feature learning. Neurocomputing, 159:126 –
136, 2015. ISSN 0925-2312. doi: http://dx.doi.org/10.1016/j.neucom.2015.02.
011. URL http://www.sciencedirect.com/science/article/pii/
S0925231215001605.
[36] PingLiu,ShizhongHan,ZiboMeng,andYanTong. Facialexpressionrecognitionvia
aboosteddeepbeliefnetwork. InProceedingsoftheIEEEConferenceonComputer
VisionandPatternRecognition,pages1805–1812,2014.
[37] PatrickLucey, JeffreyF.Cohn, TakeoKanade, JasonM.Saragih, ZaraAmbadar, and
IainA.Matthews. Theextendedcohn-kanadedataset(CK+): Acompletedatasetfor
action unit and emotion-specified expression. In IEEE Conference on Computer Vi-
sionandPatternRecognition,CVPRWorkshops2010,SanFrancisco,CA,USA,13-18
June,2010,pages94–101.IEEEComputerSociety,2010.doi:10.1109/CVPRW.2010.
5543262. URLhttp://dx.doi.org/10.1109/CVPRW.2010.5543262.
[38] Patrick Lucey, Jeffrey F Cohn, Iain Matthews, Simon Lucey, Sridha Sridharan, Jes-
sicaHowlett,andKennethMPrkachin. Automaticallydetectingpaininvideothrough
facialactionunits. IEEETransactionsonSystems,Man,andCybernetics,PartB(Cy-
bernetics),41(3):664–674,2011.14 BREUER,KIMMEL:ADEEPLEARNINGPERSPECTIVEONFACIALEXPRESSIONS
[39] BraisMartinezandMichelFValstar. Advances,challenges,andopportunitiesinauto-
maticfacialexpressionrecognition. InAdvancesinFaceDetectionandFacialImage
Analysis,pages63–100.Springer,2016.
[40] André Mourão and João Magalhães. Competitive affective gaming: Winning with a
smile. InProceedingsofthe21stACMInternationalConferenceonMultimedia,MM
’13,pages83–92,NewYork,NY,USA,2013.ACM. ISBN978-1-4503-2404-5. doi:
10.1145/2502081.2502115. URLhttp://doi.acm.org/10.1145/2502081.
2502115.
[41] SungYeongPark, SeungHoLee, andYongManRo. Subtlefacialexpressionrecog-
nitionusingadaptivemagnificationofdiscriminativefacialmotion. InProceedingsof
the23rdACMinternationalconferenceonMultimedia,pages911–914.ACM,2015.
[42] KarenLSchmidtandJeffreyFCohn. Humanfacialexpressionsasadaptations: Evo-
lutionaryquestionsinfacialexpressionresearch. Americanjournalofphysicalanthro-
pology,116(S33):3–24,2001.
[43] CaifengShan,ShaogangGong,andPeterWMcOwan. Facialexpressionrecognition
basedonlocalbinarypatterns: Acomprehensivestudy. ImageandVisionComputing,
27(6):803–816,2009.
[44] KarenSimonyanandAndrewZisserman. Verydeepconvolutionalnetworksforlarge-
scaleimagerecognition. arXivpreprintarXiv:1409.1556,2014.
[45] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolu-
tional networks: Visualising image classification models and saliency maps. CoRR,
abs/1312.6034,2013. URLhttp://arxiv.org/abs/1312.6034.
[46] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin A. Ried-
miller. Strivingforsimplicity: Theallconvolutionalnet. CoRR,abs/1412.6806,2014.
URLhttp://arxiv.org/abs/1412.6806.
[47] YanivTaigman,MingYang,Marc’AurelioRanzato,andLiorWolf. Deepface:Closing
the gap to human-level performance in face verification. In 2014 IEEE Conference
onComputerVisionandPatternRecognition,CVPR2014,Columbus,OH,USA,June
23-28, 2014, pages 1701–1708. IEEE Computer Society, 2014. doi: 10.1109/CVPR.
2014.220. URLhttp://dx.doi.org/10.1109/CVPR.2014.220.
[48] GonÃg˘alo Tavares, AndrÃl’ MourÃcˇo, and JoÃcˇo MagalhÃcˇes. Crowdsourcing fa-
cialexpressionsforaffective-interaction. ComputerVisionandImageUnderstanding,
147:102–113,2016. ISSN1077-3142. doi: http://dx.doi.org/10.1016/j.cviu.2016.02.
001. URL http://www.sciencedirect.com/science/article/pii/
S1077314216000461. SpontaneousFacialBehaviourAnalysis.
[49] Michel Valstar, Jonathan Gratch, Björn Schuller, Fabien Ringeval, Dennis Lalanne,
MercedesTorresTorres,StefanScherer,GiotaStratou,RoddyCowie,andMajaPantic.
Avec 2016: Depression, mood, and emotion recognition workshop and challenge. In
Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge,
pages3–10.ACM,2016.BREUER,KIMMEL:ADEEPLEARNINGPERSPECTIVEONFACIALEXPRESSIONS 15
[50] MichelFValstar,TimurAlmaev,JeffreyMGirard,GaryMcKeown,MarcMehu,Lijun
Yin,MajaPantic,andJeffreyFCohn. Fera2015-secondfacialexpressionrecognition
andanalysischallenge. InAutomaticFaceandGestureRecognition(FG),201511th
IEEEInternationalConferenceandWorkshopson,volume6,pages1–8.IEEE,2015.
[51] Alessandro Vinciarelli, Maja Pantic, and Hervé Bourlard. Social signal processing:
Surveyofanemergingdomain.Imageandvisioncomputing,27(12):1743–1759,2009.
[52] Esra Vural, Mujdat Cetin, Aytul Ercil, Gwen Littlewort, Marian Bartlett, and Javier
Movellan. Drowsydriverdetectionthroughfacialmovementanalysis. InInternational
WorkshoponHuman-ComputerInteraction,pages6–18.Springer,2007.
[53] Wen-JingYan,XiaobaiLi,Su-JingWang,GuoyingZhao,Yong-JinLiu,Yu-HsinChen,
and Xiaolan Fu. Casme ii: An improved spontaneous micro-expression database
and the baseline evaluation. PLOS ONE, 9(1):1–8, 01 2014. doi: 10.1371/journal.
pone.0086041. URL http://dx.doi.org/10.1371%2Fjournal.pone.
0086041.
[54] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable
are features in deep neural networks? In Zoubin Ghahramani, Max Welling,
Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger, editors, Advances
in Neural Information Processing Systems 27: Annual Conference on Neural
Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec,
Canada, pages 3320–3328, 2014. URL http://papers.nips.cc/paper/
5347-how-transferable-are-features-in-deep-neural-networks.
[55] Stefanos Zafeiriou, Athanasios Papaioannou, Irene Kotsia, Mihalis A. Nicolaou, and
GuoyingZhao.Facialaffect""in-the-wild"":Asurveyandanewdatabase.In2016IEEE
ConferenceonComputerVisionandPatternRecognitionWorkshops,CVPRWorkshops
2016,LasVegas,NV,USA,June26-July1,2016,pages1487–1498.IEEEComputer
Society,2016. doi: 10.1109/CVPRW.2016.186. URLhttp://dx.doi.org/10.
1109/CVPRW.2016.186.
[56] MatthewD.ZeilerandRobFergus.VisualizingandUnderstandingConvolutionalNet-
works,pages818–833. SpringerInternationalPublishing,Cham,2014. ISBN978-3-
319-10590-1. doi: 10.1007/978-3-319-10590-1_53. URLhttp://dx.doi.org/
10.1007/978-3-319-10590-1_53.
[57] Matthew D. Zeiler, Dilip Krishnan, Graham W. Taylor, and Rob Fergus. Deconvolu-
tionalnetworks. InInCVPR.IEEEComputerSociety,2010. ISBN978-1-4244-6984-
0. URL http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?
punumber=5521876.
[58] Zhihong Zeng, Maja Pantic, Glenn I. Roisman, and Thomas S. Huang. A survey of
affectrecognitionmethods: Audio,visual,andspontaneousexpressions. IEEETrans.
Pattern Anal. Mach. Intell., 31(1):39–58, 2009. doi: 10.1109/TPAMI.2008.52. URL
http://dx.doi.org/10.1109/TPAMI.2008.52.
[59] GuoyingZhaoandMattiPietikainen. Dynamictexturerecognitionusinglocalbinary
patternswithanapplicationtofacialexpressions. IEEEtransactionsonpatternanaly-
sisandmachineintelligence,29(6),2007.16 BREUER,KIMMEL:ADEEPLEARNINGPERSPECTIVEONFACIALEXPRESSIONS
[60] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva.
Learning deep features for scene recognition using places database. In Advances in
neuralinformationprocessingsystems,pages487–495,2014."
7,7,A deeper look at facial expression dataset bias,"['S Li', 'W Deng']",2020,127,"Affective Faces Database, MMI Facial Expression","FER, classification, classifier","As the skew class distribution across domains is a possible bottleneck for cross-domain FER,   Pantic, “Induced disgust, happiness and surprise: An addition to the MMI facial expression",No DOI,IEEE Transactions on Affective Computing,https://arxiv.org/abs/1904.11150,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"1
A Deeper Look at Facial Expression Dataset Bias
Shan Li, and Weihong Deng∗, Member, IEEE
Abstract—Datasets play an important role in the progress of
facial expression recognition algorithms, but they may suffer AffectNet
from obvious biases caused by different cultures and collection
conditions. To look deeper into this bias, we first conduct
CK+
comprehensive experiments on dataset recognition and cross-
dataset generalization tasks, and for the first time explore
the intrinsic causes of the dataset discrepancy. The results
FER2013
quantitatively verify that current datasets have a strong build-
in bias and corresponding analyses indicate that the conditional
probability distributions between source and target datasets are
JAFFE
different. However, previous researches are mainly based on
shallow features with limited discriminative ability under the
assumption that the conditional distribution remains unchanged MMI
across domains. To address these issues, we further propose a
novel deep Emotion-Conditional Adaption Network (ECAN) to
learn domain-invariant and discriminative feature representa- Oulu-CASIA
tions, which can match both the marginal and the conditional
distributions across domains simultaneously. In addition, the
largelyignoredexpressionclassdistributionbiasisalsoaddressed RAF-DB 2.0
by a learnable re-weighting parameter, so that the training and
testing domains can share similar class distribution. Extensive
cross-databaseexperimentsonbothlab-controlleddatasets(CK+, SFEW 2.0
JAFFE, MMI and Oulu-CASIA) and real-world databases (Af-
fectNet,FER2013,RAF-DB2.0andSFEW2.0)demonstratethat Anger Disgust Fear Happiness Neutral Sadness Surprise
our ECAN can yield competitive performances across various Fig. 1: Already aligned images from different facial expres-
facial expression transfer tasks and outperform the state-of-the-
sion datasets. Expression label, from left to right, is Anger,
art methods.
Disgust, Fear, Happiness, Neutral, Sadness, Surprise. Even
Index Terms—Cross Dataset, Facial Expression Recognition after aligning and eliminating background variations, domain
(FER), Dataset Bias, Domain Adaption.
discrepancystilllingersamongthesefacialexpressiondatasets
duetodifferentsettingsonpeople’sagerange,gender,culture,
I. INTRODUCTION
pose and the level of expressiveness.
AUTOMATIC facial expression recognition (FER) has
been one of the research hotspots in computer vision
and machine learning for its huge potential applications in
test (target) dataset share an identical distribution. Although
human-computer interface (HCI). Early researches in facial
currentresearchhasrealizedtheexistenceoffacialexpression
expression recognition mainly evaluate within a single facial
dataset bias, it still lacks in-depth study on how exactly these
expression database and can achieve promising performances
facial expression datasets differ from each other and in what
in controlled laboratory environment [1], [2]. However, lab-
aspectsthebiasactuallyaffectstheclassificationperformance.
controlled databases with posed expressions are too uniform
For a deeper look at dataset bias, we conduct a series
to reflect complex scenarios in our real life. In this context,
of comprehensive experiments on popular facial expression
more and more datasets have been collected from real world
datasets and providea reliable measure to evaluatethe dataset
for conducting facial expression analysis under unconstrained
biasquantitatively.Inthedatarecognitionexperiment,thebest
and challenging conditions, which contain more factors un-
classification performance on seven facial expression datasets
related to facial expressions, such as registration errors and
reaches79%despitethesmalltrainingsamplesize(thechance
variations in pose, occlusion, illumination, background and
is 1/7=14%), and intriguingly, there is no evidence of satura-
subject identity [3], [4]. With this development of diverse
tion as more training data is added. And the cross-dataset
facial expression datasets, the dataset bias problem becomes
generalization experiment on specific expression categories
obvious,especiallybetweenlab-controlledandreal-worlddata
also reveals that facial expression datasets appear to have a
collections. Moreover, in practical applications testing face
strong build-in bias and even the deep models are insufficient
imagescouldbetakenfromvaryingdomains.Henceitishard
to generalize well across various datasets.
tosatisfytheassumptionthatthetraining(source)datasetand
Focusingontheabovechallenges,weproposeanoveldeep
The authors are with the Pattern Recognition and Intelligent System Emotion-ConditionalAdaptionNetwork(ECAN)forunsuper-
Laboratory,SchoolofInformationandCommunicationEngineering,Beijing
vised cross-dataset facial expression recognition. Following
UniversityofPostsandTelecommunications,Beijing,100876,China.
E-mail:{ls1995,whdeng}@bupt.edu.cn. the work in [5], we first plug the multi-kernel maximum
9102
rpA
52
]VC.sc[
1v05111.4091:viXra2
mean discrepancy (MK-MMD) into the original deep network extension of domain adaptation method, as well as the cor-
architecturetoeffectivelymeasurethedivergencebetweenthe responding improved performances on various databases. The
source and target domain. Different from earlier approaches rest of paper is organized as follows. Section II reviews
that assume the conditional distributions across domains to be related work. In Section III, we present a deeper look at
unchangedandlearntheinvariantrepresentationsbymatching bias among different facial expression datasets. Section IV
the marginal distribution, the ECAN minimizes the discrep- introduces our approach ECAN that learns domain invariant
ancy of both marginal and conditional distributions across and discriminative features. The results of our experimental
domains, through making the most use of underlying label evaluation and experimental analysis are presented in Section
information on target data. Moreover, we find that the skew V, and conclusions are drawn in Section VI.
class distribution between the source and target domain is a
possible bottleneck for cross-domain expression recognition.
II. RELATEDWORK
To address this class distribution bias problem, we introduce
a learnable class-wise weighting parameter to the original Numerous approaches of domain adaption have been pro-
MMD, in which case, the re-sampled source data can share posed in the last years to address adaption problems that
similar class distribution with the target set. arise in different computer visual scenarios. However, few
Jointly trained with the classical softmax loss which learns significant interest until now has been gained in the cross-
distinguishable features under the supervision of the labeled domain learning for facial expression recognition. So besides
source domain and a variety of MMD regularization terms cross-dataset facial expression recognition, in this section we
whichimprovethegeneralizationabilitytotheunseendataby will also investigate recent works that is closely related to
exploring the latent semantic meaning behind the unlabeled it including cross-domain facial action units detection, micro-
target domain, the ECAN thus excels in the field of cross- expressionrecognitionandpainrecognition.Generally,wecan
domain FER. The contributions of this paper are as follows: divide these domain adaption methods into three categories:
(1) Our work makes a first step to analyze the facial instance-, model-, and feature-based methods.
expression dataset bias quantitatively and explicitly indicates Instancebasedmethodsre-weightsourcesamplestoreduce
the intrinsic causes of the observed effect. Specifically, our the distribution differences among the source and target do-
dataset recognition experimental results demonstrate that each mains.In[7],asupervisedextensionofKernelMeanMatching
datasettendstohaveitsspecificcharacteristicsandpreferences isproposedtore-weightthesourcedatasuchthatthedistribu-
during the construction process. And this capture bias will tions between source and target domains can be minimized in
lead to the discrepancy of the marginal distribution. Our aclass-to-classmanner.However,alimitednumberoflabeled
cross-dataset generalization experiment further reveals that data in target domain is required in this case to ensure the
annotators in each dataset tend to have inconsistent percep- discriminative information between classes. Likewise, Chen
tions on the expression categories. And due to this category et al. [8] employ the transductive transfer learning algorithm
bias, the assumption that the conditional distribution remains to re-weight each source sample according to the probability
unchanged across domains fails to hold in FER applications. ratio between the marginal distributions of the source and the
(2) Different from previous deep learning methods, e.g., targetdata,andthetargetlabelsisnotrequiredinthisprocess.
fine-tuning techniques, that still require a certain number of In [9], Chu et al. propose Selective Transfer Machine to re-
labels in test set, the unsupervised transfer learning method weight the source training samples that are most relevant to
ECAN is more practical since it is applicable without labeled the target users based on an iterative minimization procedure
data in target domain. Moreover, ECAN can learn domain to form a distribution closer to the target data. However,
invariant and class discriminative feature representations by this instance re-weighting method requires highly expensive
effectively aligning the marginal distribution globally and computationsandisespeciallytimeconsumingattrainingtime
also matching the conditional distributions across domains duetotheproposedoptimizationupdatestrategy.Ontheother
end-to-end. Furthermore, ECAN takes the expression class hand, user-specific adaptation algorithms are required to be
distribution bias into account, and embeds a learnable class- computationallyefficienttobeusedinrealworldapplications.
wise weighted parameter into the adversarial learning process Model based methods construct an adaptive classifier that
sothatthere-sampledsourcedataandthetargetdatacanabide have parameters or priors shared between the source and the
by similar class distribution statistics. targetdata.Sanginetoetal.[10]proposeapersonalizedmodel
(3)Extensiveexperimentshavebeenconductedondifferent that uses a regression model to learn the mapping from each
well-established facial expression databases in the literature, source subject’s data distribution into the associated classifier,
including lab-controlled datasets (CK+, JAFFE, MMI and then transfers the learned parameters to the unseen target
Oulu-CASIA)andreal-worlddatabases(AffectNet,FER2013, individual. Based on this work, Zen et al. [11] further extend
RAF-DB 2.0 and SFEW 2.0). Comparing with other state-of- the framework to other classification models and use support
the-artmethodsforcross-datasetfacialexpressionrecognition, vectorsforparametertransfer.In[12],aGaussianprocess(GP)
ourECANyieldscomparableandevensuperiorperformances. domain expert is proposed to facilitate the adaption of the
This journal paper is an extension of our conference work classifierbyconditioningthetargetGPonthepredictionsfrom
[6]. In the paper, the new contents include the extensive multiple source GPs. And the predictions from the individual
experiments that take a deeper look at expression biases GP are combined to form the final prediction. However, it
on several widely-used datasets, and expression-conditional still requires a small amount of labeled data of the target3
subject.Huangetal.[13]alsoconstructmultipleweakgeneric is available.
classifiers based on a subset of the source subjects’ data and To the best of our knowledge, our approach is among the
use the auxiliary target labeled data to evaluate and re-weight verystate-of-the-artonestoderivefeaturerepresentationswith
each of the weak generic classifiers. And this method also both generalization and discrimination ability for the cross-
requires annotated target data, which is typically not easily domain facial expression recognition problem based on the
obtainedinhumancomputerinteractionscenarios.Besides,the deep learning technique, where no labeled data are required
subject identity information, i.e., the annotation of the person in the target domain. And for the first time, conditional
specific data, is required in the above mentioned methods, distributionbiasandexpressionclassimbalanceproblemhave
which is usually not available in the real-world applications. been taken into consideration in this case to further boost the
Feature based methods concentrate on learning features final classification performance.
that are domain invariant between source training data and
target test data, hence the classifier trained on the source III. MEASURINGFACIALEXPRESSIONDATASETBIAS
samples can be well fitted into target data. In [14], the author
Facial expression recognition is a data-driven task. Many
investigates the cross-dataset FER problem and proposes a
databases have been established to evaluate affect recognition
transfersubspacelearningmethodtoobtainafeaturesubspace
systems. However, there exists inevitable bias between these
that transfers the knowledge gained from the source set to the
facial expression datasets. Some example face images from
targetdata.In[15],adiscriminativefeatureadaptationmethod
different databases have been displayed in Fig. 1. Previous
is proposed to obtain a feature space that can minimize the
across-database experiments have also shown that it’s hard
mismatch between and source and target distribution. In [16],
to learn a classifier which can take into account of all
unsupervised domain adaptive dictionary learning is proposed
possible variations of the data [20]. For example, Shan et.
tolearnnewrepresentationsoforiginalsourceandtargetsam-
al. [1] trained the Boosted-LBP based SVM classifier on the
plesthatreducethedistributionmismatchbetweensourceand CohnâA˘S¸Kanade database, and then evaluated it on the MMI
targetdomains.Huangetal.[13]proposedtoalignthefeature
database and the JAFFE database, respectively. The general-
boundaries of different subjects to the same points in the
ization performance across databases has reduced by around
normalizedfeaturespacewithrespecttotheneutralexpression
36% for MMI and 40% for JAFFE. In [2], Littlewort et. al.
and the boundary values to remove subject-dependent bias.
havealsoevaluatedtheirsystems’generalizationabilityacross
In [17], a transductive transfer subspace learning method is
databases, which achieved strong performance results within
proposed to deal with cross-pose and cross-database FER.
the database (more than 90%) but decreased to 50–60% when
Specifically, an auxiliary image set from unlabeled target
testedonanotherdifferentdatabase.Similarobservationshave
domain is leveraged into the labeled source domain data set
been also reported in [21], [22], [23]. They all demonstrate
forjointlylearningadiscriminativesubspacetoreducethedis-
that dataset bias is very common among different facial ex-
similarityofthemarginalprobabilitydistributionsbetweenthe
pressiondatabases,andevaluatingmethodswithintra-database
source and target data. Most recently, a domain regeneration
protocol would render them lack generalization capability on
frameworkhasbeenproposedin[18]tore-generatesourceand
unseen samples at test time.
target samples sharing the same or similar feature distribution
Although previous work has demonstrated that dataset bias
for the cross-database micro-expression recognition problem.
problem causes a noticeable deterioration in cross-dataset
However, most of these methods neglect the conditional dis-
facial expression expression performance, there have been
tribution bias between the source and target domains, which
rare discussions in the literature regarding how these facial
is a common phenomenon in realistic conditions.
expressiondatasetsdifferamongeachother.Sointhissection,
Since annotating abundant facial expression images from
we conduct a series of comprehensive experiments includ-
diverse domains is a difficult and time consuming work and
ing dataset recognition tasks and class-specific cross-dataset
labeling samples in target domain on-the-fly is impossible,
generalization tests to evaluate this largely neglected issue
supervised approach that requires labeled instances from the
quantitatively and widen the attention on concrete aspects
target domain is impractical for real-world applications. So in
in which bias sneaks into these datasets and affects classi-
this paper, we will hinge on unsupervised domain adaption
fication performances. Specifically, a variety of well known
technique for cross-dataset FER. As a few related studies
lab-controlled & in-the-wild facial expression datasets have
[7], [8], [12], [13] have demonstrated that the supervised
beenevaluatedinourexperiments,includingAffectNet,CK+,
information on target data is necessary to boost the adaption
FER2013, MMI, Oulu-CASIA, RAF-DB 2.0 and SFEW 2.0.
performance and enhance the discriminative ability of the
learned model, we further explore the underlying semantic
A. Dataset Recognition
meaning behind target data using the pseudo label. And
both marginal and conditional distributions will be taken into We first evaluate to what extent these facial expression
accounttoalleviatethedomaindiscrepancy.Moreover,asdeep datasets vary across each other, and dig out the main cause
learninghasbeengraduallyleveragedtodealwithchallenging of this discrepancy. During the image pre-processing stage,
variety in the wild [4], [19], we also employ CNN to learn facial images from all datasets are first aligned by an affine
features with more discriminative ability. Hence our approach transformation defined by the centers of the two eyes and the
is unsupervised and can learn discriminative features that center of the two corners of the mouth and then normalized
generalize well to new domains in which no prior knowledge to the size of 100×100. Then for image representations, we4
80
70
60
50
40
30
20
10
0 10 20 50 100 200 500
Number of training samples per dataset
)%(
ecnamrofreP
noitingoceR
thedatasetswiththehighestconfusiondegree,whichismainly
AffectNet CK+ FER2013 MMI Oulu-CASIA RAF-DB SFEW 2.0 due to the diverse and extensive image samples they contain.
AffectNet 57.40 1.00 6.75 1.80 0.75 24.75 7.55 In summary, there appears to have obvious distinctions
CK+ 0.80 96.69 0.10 0.00 0.30 1.56 0.55 among these expression datasets. And the main consequences
of this discrepancy can be accounted for the capture bias, that FER2013 8.05 0.50 78.05 0.50 1.05 9.55 2.30
is, each dataset tends to have its own preference during the
chance MMI 9.86 0.15 1.10 76.16 2.14 10.35 0.25
H VGO GG Face2 construction processing. For example, images can be acquired
Oulu-CASIA 0.47 0.00 0.78 0.05 96.30 2.34 0.05
withdifferentdevices(e.g.,professionalpicturesphotographed
RAF-DB 23.70 1.90 8.65 1.75 2.10 58.40 3.50
using the laboratory camera vs. amateur snapshots and selfies
SFEW 2.0 3.60 0.00 0.80 0.05 0.15 1.70 93.70
collected from the Internet), and various collection envi-
(a) Datasetrecognitionrate (b) Confusionmatrix ronments (e.g., different lighting conditions and occlusions,
certain types of background, post-processing elaboration such
Fig. 2: Dataset recognition experiment on seven different
asfiltering).Inaddition,participants’identityandcharacterin
datasets. (a) Classification performance as a function of train-
thesedatasetsarealsodifferent.Forinstance,personsinMMI
ingsetsizefortwodifferentdescriptors.(b)Confusionmatrix
dataset carry more accessories, e.g., glasses and mustache,
for the HOG descriptor.
when compared to other lab-controlled datasets. And datasets
retrievedfromtheInternetcontainsubjectswithamuchwider
range of age and ethnicity.
extract two different features for these already aligned face The capture bias makes these datasets following different
images: the handcrafted HOG features and the deep learned marginaldistributions,whichisthepublicacknowledgedissue
features. For HOG [24] features, we first divide the images to be solved in recent studies on domain adaption. And in
into 10×10 pixel blocks of four 5×5 pixel cells with no the following subsection, we will look into another generally
overlapping.Bysetting10binsforeachhistogram,weobtaina ignored but equally important issue, i.e., the conditional dis-
4,000-dimensional feature vector per aligned image. For deep tribution of each dataset.
features, we first resize the aligned face images so that the
shorter side is 256 pixels, then we crop a 224×224 pixels B. Cross-dataset Generalization
regionofeachresizedimageasthenetworkinput.Wechoose
Inthisexperiment,weinvestigatehowwelldoesanexpres-
the ResNet-50 Convolutional Neural Network [25] that are
sionclassmodeltrainedononedatasetgeneralizewhentested
first pre-trained on the MS-Celeb-1M face dataset [26] and
on other datasets, and give an insight on how the expression
then fine-tuned on the VGGFace2 face dataset [27] to extract
classes in each dataset are associated with each other.
deepfeatures.Finallya2048-dimensionaldescriptorperimage
We study the class-specific cross-dataset performances on
is obtained from the layer adjacent to the classifier layer.
these datasets with two testing regimes: facial expression
To name the dataset, we randomly sample 500 training detectionthatdetectsimageswithonespecificexpressionclass
images from each of the seven datasets and apply a seven- from all images; and facial expression classification that finds
way linear SVM implemented by LibSVM [28]. The penalty all imagescontaining the desiredexpression class.Notice that
parameter C of SVM is set to the default value 1. To ensure the classification task is basically the same as the detection
person independence, 200 images sampled from each of the task if there are only two classes. And we follow the image
remaining sets with person identities that do not appear in representations (HOG and deep feature) and classifier (linear
the training set are chosen for classification. We repeat it 10 SVM) used in the above data recognition experiment.
timeswithdifferentdatasplitsandreporttheobtainedaverage For the expression detection task, we use all images with
results.Figure2(a)showsdatasetrecognitionperformancefor the target expression label in each dataset as the positive
two feature representations. Figure 2(b) shows the detailed samples and the others as the negative samples. In the in-
confusion matrix using the HOG feature. database setting, we follow the person-independent rule and
From the plot on the left, we can see that the best clas- conduct five-fold cross-validation experiment on each dataset.
sification performance reaches 79% (the chance is 1/7=14%) In the cross-database setting, we train our model using all
despitethesmallsamplesize,andthereisnoevidenttendency images in one dataset and test it on another dataset. And we
of saturation as more training data is added. When comparing focus on two expressions that are representative and common
the performances of two different features, it indicates that in all the datasets: happiness and sadness. Likewise, for the
HOG feature shows an advantage in separating these datasets expressionclassificationtask,wealsoadoptallimagesineach
than deep feature. As the deep network is pre-trained on dataset and follow the person-independent protocol. Table I
large-scale face datasets, features extracted from it are more present results for each task. Note that since each dataset
generalized and universal. Hence deep learning can learn containsdifferentnumberofimagesperclassandhasdifferent
common representations and achieve higher generalizability difficultydegreeonexpressionrecognition,theactualaccuracy
across domains. From the confusion matrix of HOG feature valuesforeachdatasetdonotmeanalottothegeneralization
ontheright,weseethatitiseasytodistinguishlab-controlled performance.Anditisthepercentdropbetweenthein-dataset
datasets, especially CK+ and Oulu-CASIA datasets, from all performance and the average cross-dataset performance over
the others. And there is a confusion grouping among real- othertestdatasetsthatbearsmuchmoreimportantinformation
world datasets. Furthermore, AffectNet and RAF-DB 2.0 are on the evaluation for cross-dataset generalization.5
TABLE I: Cross-dataset generalization on different tasks with two different features. Each matrix contains the expression
detection or classification performance when training on one dataset (rows) and testing on another (columns). The diagonal
bold elements correspond to the in-dataset results, i.e., training and testing on the same dataset. The “Mean Others” refers the
average cross-dataset performance on all the other test datasets except self. For short, A = AffectNet, C = CK+, F = FER2013,
M = MMI, O = Oulu-CASIA, R = RAF-DB 2.0, S = SFEW 2.0.
Mean Percent Mean Percent
HOG Deep feature
Others Drop Others Drop
ssenippaH
Train Test A C F M O R S Train Test A C F M O R S
AffectNet 89.14 95.55 80.41 91.90 90.42 75.38 84.64 86.38 3% AffectNet 89.51 93.61 87.03 92.33 93.59 81.77 84.64 88.83 1%
CK+ 88.69 98.96 79.53 89.77 92.40 72.06 83.23 84.28 15% CK+ 85.23 94.18 84.82 91.19 93.02 79.97 81.68 85.99 9%
FER2013 86.23 81.63 87.05 90.91 90.68 82.40 77.82 84.94 2% FER2013 90.51 94.90 91.56 90.34 92.03 84.64 87.24 89.95 2%
MMI 88.80 97.01 81.74 94.65 94.90 76.71 83.01 87.03 8% MMI 83.77 90.94 82.44 88.20 90.78 77.78 78.56 84.05 5%
Oulu-CASIA 86.60 90.29 77.71 87.50 96.98 70.37 81.01 82.25 15% Oulu-CASIA 86.34 91.67 82.89 92.90 94.64 76.67 78.26 84.79 10%
RAF-DB2.0 83.40 91.02 82.03 91.62 86.62 83.45 78.26 85.49 -2% RAF-DB2.0 82.71 89.48 82.79 84.38 81.93 90.15 82.20 83.91 7%
SFEW2.0 87.23 91.58 80.05 90.62 89.69 75.55 88.80 85.79 3% SFEW2.0 84.57 87.54 81.80 87.22 86.15 76.33 89.17 83.93 6%
ssendaS
Train Test A C F M O R S Train Test A C F M O R S
AffectNet 80.97 86.97 77.61 85.65 73.02 82.24 76.33 80.31 1% AffectNet 81.23 81.88 79.77 78.27 74.37 78.51 76.11 78.15 4%
CK+ 84.97 96.63 84.08 88.21 87.92 85.99 81.53 85.45 12% CK+ 83.06 94.18 82.67 88.35 87.50 84.07 80.93 84.43 10%
FER2013 80.29 89.89 81.96 85.08 80.10 82.79 71.74 81.65 0% FER2013 83.63 91.02 85.19 87.07 87.19 86.72 78.19 85.64 -1%
MMI 84.00 93.45 83.11 89.71 85.62 85.24 78.71 85.02 5% MMI 80.66 89.81 79.85 85.15 82.45 80.98 76.93 81.78 4%
Oulu-CASIA 79.83 91.99 77.57 83.52 91.20 80.78 79.01 82.12 10% Oulu-CASIA 68.66 79.37 71.88 76.85 82.08 69.37 73.89 73.34 11%
RAF-DB2.0 80.89 89.00 77.83 78.27 80.99 83.02 79.67 81.11 2% RAF-DB2.0 85.86 91.10 84.32 87.36 86.35 89.52 81.08 86.01 4%
SFEW2.0 73.86 86.41 71.51 84.80 76.15 74.12 83.98 77.81 7% SFEW2.0 79.23 87.22 79.10 76.85 79.58 80.05 84.64 80.34 5%
snoisserpxe
7
Train Test A C F M O R S Train Test A C F M O R S
AffectNet 48.83 58.58 28.29 42.19 32.50 33.33 27.74 36.92 24% AffectNet 58.43 51.38 39.00 45.60 37.76 42.30 29.67 40.95 30%
CK+ 34.09 93.92 31.07 56.68 58.85 34.96 25.59 40.21 57% CK+ 32.28 85.40 41.75 52.84 53.49 40.76 30.41 41.92 51%
FER2013 30.14 44.58 52.28 45.60 41.30 46.44 33.98 40.34 23% FER2013 36.54 58.25 61.74 46.87 43.12 52.70 34.72 45.37 27%
MMI 31.83 74.11 32.73 65.11 53.85 36.34 26.71 42.59 35% MMI 29.11 51.94 32.08 53.19 40.68 32.80 24.03 35.11 34%
Oulu-CASIA 24.69 74.68 27.01 47.30 73.59 25.80 18.55 36.34 51% Oulu-CASIA 25.57 58.58 30.96 47.02 61.25 30.16 21.37 35.61 42%
RAF-DB2.0 32.71 59.87 39.05 51.70 44.84 55.36 30.49 43.11 22% RAF-DB2.0 38.29 58.09 49.87 48.44 45.52 68.69 38.13 46.39 32%
SFEW2.0 26.17 40.86 30.94 35.80 31.87 33.33 55.05 33.16 40% SFEW2.0 27.23 36.00 33.67 23.15 29.58 33.22 58.09 30.48 48%
First,thereisasignificantdropofperformancewhentesting lab-controlled datasets, especially CK+, are easier across all
on other different datasets, on the whole, in all tasks and tasks (column average). And real-world datasets, especially
features. For instance, the average in-dataset performance of AffectNet and RAF-DB 2.0, are more diverse and generalized
all datasets is 63.83% in the seven expression classification (rowaverage),whichevenachievehighercross-datasetperfor-
taskusingdeepfeature,anditdropsto39.40%fortheaverage mance on lab-controlled datasets than the in-dataset one for
cross-dataset performance. This observation is coherent with several tasks.
what we deduce from the data recognition experiment, which
An important factor that intrinsically induces this dec-
verifies the distinct bias across these datasets again. Second,
lination on class-specific cross-dataset generalization is the
comparing different features on the in-database performance,
categorybias:annotatorsineachdataset,includinghumanand
the deep feature shows the superiority on real-world datasets,
machine,mayhavedifferentinterpretationsandperceptionsof
which mainly benefits from the large-scale in-the-wild dataset
theemotionconveyedinfacialimages,andmanyimagestend
the network pre-trained on. However, as a byproduct trained
to express more than one expression category which further
on face recognition dataset, the deep feature performs worse
enhances the difficulty and uncertainty of annotation. Due to
on the lab-controlled dataset with person-independent setting.
this fact, the conditional distributions in these datasets are
Andforthecross-databaseperformance,deepfeatureachieves
indeed different. However, many researches assume that the
better results on the average mean other accuracy and percent
conditional probability distributions remain the same across
drop, which indicates that the deep feature captures general-
different datasets before conducting domain adaption, which
ized information and is more suitable for domain adaption.
would prevent good domain invariant feature representations
Third, focusing on the difficulty and diversity of each dataset,
from being learned.6
TABLE II: Summary of Major Notations Used in the Paper
where E and E denote the population expectations under
ps pt
distribution p and p , respectively. The MMD function class
s t
Notations Description
F is the unit ball in a reproducing kernel Hilbert space H.
D /D the source/target domain We then map the data into H using feature space mapping
s t
xs/xt feature vector of sample in the source/target domain function φ(·) and get an empirical estimate of the MMD:
i i
N /N number of source/target samples
s t (cid:12)(cid:12) (cid:12)(cid:12)2
ys expression label of source sample xs (cid:12)(cid:12) 1 (cid:88)Ns 1 (cid:88)Nt (cid:12)(cid:12)
i i MMD2[H,D ,D ]=(cid:12)(cid:12) φ(xs)− φ(xt)(cid:12)(cid:12) ,
φ(·) feature map function with kernel map k(x,·) s t (cid:12)(cid:12)N i N i (cid:12)(cid:12)
(cid:12)(cid:12) s t (cid:12)(cid:12)
P(X) marginal probability distribution of domain data X i=1 i=1 H (2)
P(Y|X) conditional probability distribution of Y given X where φ(·) denotes the feature map function associated with
α l weight ratio to sample with label l thekernelmapk(xs,xt)=(cid:104)φ(xs),φ(xt)(cid:105) .Oneofthemost
H
xs i,l feature vector of source data with true label l used kernel corresponding to an infinite-dimensional H is the
xt i,ˆl feature vector of target data with pseudo label ˆl Gaussian kernel k(xs,xt) = exp(−||xs −xt||2/(2σ2)). To
Nl number of source samples with true label l maximizethetestpowerandminimizethetesterror,thekernel
s
Nˆl number of target samples with pseudo label ˆl map K is defined as a linear combination of base kernels
t
Θ network parameters to be learned {k u}d u=1:
γ,λ trade-off hyper-parameters (cid:40) d d (cid:41)
(cid:88) (cid:88)
K= k = β k , β =1,β (cid:62)0,∀u . (3)
u u u u
u=1 u=1
IV. EMOTION-CONDITIONALADAPTIONNETWORK
Given the statistical tests defined in MMD, we can have
Although the above experiments demonstrate the general-
P = P if and only if MMD = 0. Hence we can use the
s t
ization ability of deep feature, the dataset bias still remains
MMD function based on the feature map φ in Eq. 2 to detect
evident. So in this section, we will focus on this problem
any marginal distribution discrepancy between D and D . To
s t
and present our approach for dealing with cross-dataset facial
adapt the MMD function to the stochastic gradient descent
expression recognition. For the convenience of reading, we
(SGD) in CNN-based domain adaption, we further adopt an
list some of the major notations that are used throughout this unbiased empirical estimator [30] of MMD2[H,D ,D ] with
s t
paper in Table II.
linear time complexity for efficiency:
A. Problem Formulation 1
(cid:88)Ns
MMD2[H,D ,D ]= k(xs,xs)
Wefirststartfromtheproblemdefinition.LetX
s
∈RD×Ns u s t N s(N s−1)
i(cid:54)=j
i j
be the source domain data with marginal probability dis-
tribution Ps(X) and X t ∈ RD×Nt be the target domain + 1
(cid:88)Nt
k(xt,xt) (4)
with marginal probability distribution Pt(X), where D is the N t(N t−1) i j
i(cid:54)=j
dimension of the data instance, N and N are number of
s t
2
(cid:88)Ns (cid:88)Nt
images in source and target domain respectively. − k(xs,xt).
In unsupervised domain adaption, we refer to the train- N sN t i j
i=1 i=1
ing dataset with labeled data as the source domain D =
s Actually,manydomainadaptionapproacheshaveemployed
{(xs,ys)}Ns where ys ∈ {1,2,3,4,5,6,7} is the corre-
i i i=1 i MMD to reduce the marginal distribution distance between
sponding expression class label of source data xs, and the
i source and target domains. However, additional important
test dataset without labeled data as the target domain D =
t issues have been ignored:
{(xt)}Nt . Both the source and target data pertain to seven
i i=1 (1) Conditional distribution discrepancy: As has been men-
expressionclassesinthiscase.Andinthisscenario,thesource
tioned in Sec. III-B, the conditional distributions in facial
and target set are assumed to be different but related due to
expressiondatasetsareindeeddifferent.Therefore,wepropose
different acquisition conditions. Our goal is to learn a deep
to match not only the marginal distributions but also the
neuralnetworkf :X →Y thatcandecreasethecross-domain
conditional distributions across domains by optimizing their
discrepancy so as to minimize the target error just using the
empirical MMD distance, respectively.
supervision information from source data.
(2) Class discriminative capacity: Domain invariance can
effectively transfer knowledge from the source data to the
B. Preliminary and Motivation
target data. However, it can not guarantee the discriminability
We first introduce Maximum Mean Discrepancy (MMD) of learned features. For example, features with fear label in
which is a widely-used and efficient non-parametric metric the target domain can be mapped near features with surprise
on the embeddings of probability distributions to measure the label in the source domain while also satisfying the condition
divergence between different domains [29]. The MMD and of global domain-invariance.
its empirical estimate in the reproducing kernel Hilbert space (3) Class imbalanced problem: the biased distribution is
(RKHS) can be defined as: quite frequent in facial expression datasets and exists in most
realistic settings. This can be justified by the practicalities
MMD[F,P ,P ]= sup(E [f(xs)]−E [f(xt)]), (1)
s t ps pt of data acquisition: eliciting and annotating a smile is very
f∈F7
easy, however, capturing information for disgust, anger or require the target domain to have the same distribution with
less common expressions can be very challenging. Samples the source domain, then part of samples in the target domain
in one category dominate but lack in another can mislead will be incorrectly classified as fear under the supervision of
judgment on the source domain’s relevance on the target task. the original MMD algorithm.
This phenomenon that has been neglected by most previous In order to weaken the impact of the class distribution bias,
research would be a significant bottleneck for cross-dataset we add a weight ratio α to re-sample the class distribution in
facial expression analysis. thesourcedomain.Thenthere-weightedmarginaldistribution
Sointhefollowingparagraph,wewillpresentourapproach of the source domain can be formulated as:
to deal with these outstanding issues. 7
P (xs)=(cid:88) P(Xs|Ys =l)P(Yt =ˆl)
α
C. Emotion-Conditional Adaption Network (ECAN) l=1 (6)
7
The existing methods propose to match the marginal dis- =(cid:88) P(Xs|Ys =l)P(Ys =l)α
l
tribution P(X) across different domains. However, it cannot
l=1
guarantee that the discrepancy in the conditional distribution
P(Y|X) is also decreased. Our goal is to learn a deep where the weight ratio α l = PP (( YY st= =ˆl l) ). For the target domain
networkthatcanensuretheinvariantofboththemarginaland we do not know the true labels, we use the pseudo labels ˆl
the conditional distribution so that the efficiency of domain predicted by the classifier trained on the source domain. By
adaption across different facial expression datasets can be amending the original MMD with the weighted ratio α, the
improved. source domain thus can share the same class distribution with
As the label variable Y is in a low-dimensional and the target domain. Combining Eq. 2, Eq. 4 and Eq. 6, the
discrete space, it is more efficient to match label distri- weighted empirical estimate MMD of two domains and its
butions P(Y) compared with its conditional variant. Ac- unbiased estimate can be formulated as:
cording to the Bayes rule P(X|Y)P(Y) ∝ P(Y|X), we
can then correct the changes in conditional distribution by
(cid:12)(cid:12) (cid:12)(cid:12)2
matching P(Xs|Ys)P(Ys) and P(Xt|Yt)P(Yt). And for (cid:12)(cid:12) 1 (cid:88)Ns 1 (cid:88)Nt (cid:12)(cid:12)
the probability distribution of X given Y, we propose to
MMD2 α[D s,D t]=(cid:12)
(cid:12)
(cid:12)(cid:12)
(cid:12) (cid:12)N
s
α
y
isφ(xs i)−
N
t
φ(xt i)(cid:12)
(cid:12)
(cid:12)(cid:12)
(cid:12)
(cid:12)
i=1 i=1
learn a conditional invariant feature transformation such that (7)
P(Xs|Ys)=P(Xt|Yt).Consideringthatthereisnolabeled
data in the target domain for unsupervised domain adaption,
1
(cid:88)Ns
w ple orf eur at ch ce er ssre ibs lo ert seto math ne ticps ie nu fod ro ml aa tb ioel no of ft th he et da ir sg te rit bd ua tit oa nt so
.
Wex e- MMD2 α,u[D s,D t]=
N s(N s−1)
α
y
isα
y
jsk(xs i,xs j)
i(cid:54)=j
then present this two additional regularization terms that can
1
(cid:88)Nt
be intuitively implemented in the deep learning framework. + k(xt,xt) (8)
N (N −1) i j
(1) Learning Class Re-weighted Label Space: As has been t t
i(cid:54)=j
mentioned before, class imbalance is very common in facial
2
N (cid:88)s,Nt
expression databases, especially for the real-world scenarios. − α k(xs,xt)
During experiments, we found that, in some cases, simply
N sN
t
y is i j
i,j=1
using MMD for cross-domain facial expression recognition
(2)LearningClass-ConditionalInvariantFeatures:Oncon-
would even degrade performances. To remedy this problem,
dition that the invariance of the class prior probability is
we propose to apply a class re-weighted term to the MMD
guaranteed, we propose to learn the conditional invariant
distance of distribution. Note that the marginal probability
transformation by applying the MMD in the class level. With
distribution of both domains can be written as:
thetruelabelsinsourcedomainandthepseudolabelsintarget
7 7
(cid:88) (cid:88) domain, we can then match the distributions of each centroid
P(X)= P(X,Y)= P(X|Y =l)P(Y =l), (5)
in the same class across different domains as follow:
l=1 l=1
w g thih v ee e ar ne ssY uP m( a pX n td io|Y nP t( hY= at= tl h) eli ) cs lait s sh se th de ic so tc rn l id a bsi ut si to iopn nra i ,l o ir .d ei p .s ,rt or thi bb eau bt cii llo ain t sy s.o pAf rin oX d
r
MMD2 c[D s,D t]=(cid:88) l=7 1(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)N1 sl(cid:88) iN =sl 1φ(cid:0) xs i,l(cid:1) − N1 tˆl(cid:88) iN =tlˆ 1φ(cid:16) xt i,ˆl(cid:17)(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)2 ,
(9)
probability, is the same in both domains is no longer valid in
where xs and xt are the i-th instance with class label l in
our case. When P(Y) is different across domains, P(X) can i,l i,ˆl
still vary even if P(X|Y) remains unchanged. In this case, the source and target domains respectively. Nl and Nˆl are
s t
commonlyrestrictingthediscrepancyofP(X)mayleadtothe numbers of source and target instances in class l. Through
effectofmisleadinginformation,thatis,predictionsoncertain explicitly minimizing the distance of the distributions across
classes would be easily misguided by differences between domainsforeachclassseparately,wecanensuretheinvariance
label occurrences in both domains. A special example can of class-conditional distribution between different domains.
intuitively explain this phenomenon: Assuming that samples Besides, the additional benefit of this class-wise adaption
with label fear only exist in the source domain, if we strictly is two fold. First, one of the open issues for the pseudo label8
Labeled Source Data Labeled Source Data
Classified by softmaxloss
On the source data
Classified by softmaxloss
Based on source data
Traditional (1) (2)
MMD Loss Re-weighted Class conditional
MMD Loss MMD Loss
Class re-weighed source samples
Happy and angry source samples with label
Happy and angry target samples without label Happy and angry source samples with label
Unlabeled Target Data Misclassified target samples Unlabeled Target Data H Ma isp cp lay sa sn ifd ie a dn tg ar ry g eta t r sg ae mt psa lem s p l e s without label
(a) Original MMD (b) ECAN
Fig.3:(a)IntraditionalMMD,onlythediscrepancyofmarginaldistributionsbetweensourceandtargetdomainsareconsidered
to be restricted. Since domain-invariance does not mean discriminativeness and class distribution bias exists across domains,
samplesintargetdomainsarestillpronetobemisclassifiedevenifthemarginaldistributionisinvariant.Asthegraphillustrates,
the most part of target domain are misclassified as “happy” which are dominate in the source domain. (b) In our method,
we explore the underlying label information of target data, and match both the marginal and class conditional distributions to
mitigate the discrepancy across domains. With the Re-weighed MMD redistributing the class distribution of the source domain
and the class conditional MMD learning the conditional invariant transformation, the discriminative separating hyperplane thus
can generalize well on the target data.
is the validity of the predicted result based on other different output features can be generalized and discriminated enough
domains. And the wrong information in the pseudo-labeled on both domains. As related research [31] has suggested, the
target domain may deteriorate the adaption performance. In network becomes more task-aimed as the layer goes deeper
our method, by aligning the centroid of each class, the and hence it will become difficult to directly transfer the
effects of these correctly and wrongly labeled samples are learned feature to the target domain. Therefore, we utilize the
neutralized together so that the negative influences conveyed output of the last feature extraction layer as the RKHS for the
by the false pseudo-labeled samples can be suppressed by MMD layer so as to regularize the learned representation to
the correct labeled ones. Second, by calculating the MMD be invariable to domain shift.
distances across domains for each class independently, our
In addition, as the ground truth label Yt of the target
method can guarantee that samples with the same label can
domain is unavailable in this unsupervised circumstance, we
bemapped nearbyinthe learnedfeaturespace. Thustheclass
then replace the ground truth with pseudo labels
Yˆt
learned
discriminative of learned features can be enhanced.
by the network in a fixed number of iterations. Different
Likewise, combining Eq. 4 and Eq. 9, the unbiased approx-
from previous methods that all pseudo labels of the target
imation to MMD can be formulated as:
c
data have been directly used to calculate the weight ratio, we

MMD2 c,u[D s,D t]=(cid:88)7  Nl(N1 l−1)(cid:88)N sl k(cid:0) xs i,l,xs j,l(cid:1) f ou frt ah se sr igi nn it nr god lu ac se aa tp ea nr ta am tive ete lr abδ e(l l) toto xi tn .d Aica mte int ih me uc mon efi nd te ron pc ye
l=1 s s i(cid:54)=j regularization method [32] has been adopted to learn δ (l)
i
+
1 (cid:88)N tlˆ k(cid:16)
xt ,xt
(cid:17) f no er twe oa rc kh px art i a: mδ ei t( el r) s=
in
tp h( el|x ktt i h;θ ik te) r, atw ioh ne .re Inθ tk hisis wt ah ye
,
wle earn cae nd
Nˆl(Nˆl−1) i,ˆl j,ˆl
take advantage of the unlabeled target data while regulating
t t i(cid:54)=j
Nl,Nlˆ  theircontributiontoproviderobustnesstothelearningscheme.
2 (cid:88)s t (cid:16) (cid:17) We then estimate the weigh ratio α as:
−
N slN tˆl i,j=1
k xs i,l,xt
j,ˆl
.
α = P(Yˆt =l) = (cid:80)N i=t 1δ i(l)/N t, (11)
(10) l P(Ys =l) Nl/N
s s
whereNl arethenumberofsampleswithclasslinthesource
D. Optimization of the Network s
domain.Byaddingtheclassre-weightedMMDlosslayerand
Withoutanylabelinformationinthetargetdomain,directly the class-conditional MMD loss layer into the network, the
adapting deep CNN to the target domain via fine-tuning objective function can be formulated as:
remains impossible and utilizing CNN learned on the source
domain to classify the target domain is prone to over-fitting. 1
(cid:88)Ns
L= L (Θ(xs),ys)+γMMD2 [D ,D ]
So we embed the MMD metrics proposed in Sec. IV-C N s i i α,u s t (12)
s
i=1
to the CNN architecture, in which way the deep network
+λMMD2 [D ,D ],
can leverage the labeled-source data and also explore the c,u s t
underlying information of the unlabeled-target data, then the where L denotes the softmax loss of the source domain,
s9
Θ is the network parameters to be learned, γ and λ is the Algorithm 1 Optimization algorithm of ECAN.
hyper-parameter to weight against these loss functions. In Input: Source data {(xs,ys)}ns , Target data {xt}nt ,
i i i=1 i i=1
addition, we suppress the noisy information of pseudo labels n and n is the size of mini-batch
s t
by assigning a small weight to γ and λ in the early training Output: Network layer parameters Θ
phase. The gradients of the re-weighted MMD loss with 1: Initialize:Thenumberofiterationk←0,pseudolabel’supdate
interval N , Network learning rate µ, hyper parameter γ and λ,
respect to source feature xs and target feature xt can be p
i i Network layer parameters Θ.
computed as: 2: while not converge do
3: k←k+1
∂MMD2 α,u[D s,D t] = α y isα y js (cid:88)Ns ∂k(xs i,xs j) 4: if k mod N p =0 then
∂xs
i
N s(N s−1)
i(cid:54)=j
∂xs
i
(13)
5: tU arp gd ea tte dat th ae xp ts (ke )u .do label yˆ it(k) and parameter δ i(l)(k) of
−
N2α sNy is
t
N i(cid:88) ,s j, =N 1t ∂k( ∂x xs i,
s
ixt j)
,
6 7:
:
e
C
bn yod
m
Ei
p
qf
u .t 1e 1t .he
wei
ight ratio α lk for the re-weighted MMD loss
8: Compute the joint loss by Eq. 12:
∂MMD ∂2 α, xu t[D s,D t] =
N
(N1 −1)(cid:88)Nt ∂k( ∂x xt i, txt j) Lk =Lk s +γLα M, Mu D(k) +λLc M, Mu( Dk)
i t t i(cid:54)=j i
(14)
−
2α
y is
N (cid:88)t,Ns ∂k(xs j,xt i)
.
9: C Eqo .m 1p 3u –t 1e 6t :he back propagation error for each samples x i by
N N ∂xt
s t i,j=1 i ∂Lk ∂Lk ∂Lα,u(k) ∂Lc,u(k)
= s +γ MMD +λ MMD
And the gradients of the class-conditional MMD loss with ∂xs(k) ∂xs(k) ∂xs(k) ∂xs(k)
respect to source feature xs and target feature xt can be i i i i
i i ∂Lk ∂Lα,u(k) ∂Lc,u(k)
computed as: =γ MMD +λ MMD
∂xt(k) ∂xt(k) ∂xt(k)
 (cid:16) (cid:17) i i i
∂MMD2 c,u[D s,D t] =(cid:88)7

1 (cid:88)N sl ∂k xs i,l,xs j,l
10: Update the network layer parameters Θ:
∂xs Nl(Nl−1) ∂xs
i l=1 s s i(cid:54)=j i ∂Lk
Θk+1 =Θk−µk
−
N
sl2
N tˆl
N i(cid:88) ,s jl, =N 1tlˆ ∂k(cid:16) x ∂s i x,l,
s
ixt j,ˆl(cid:17) , =Θk−µk(∂ (cid:88) inΘ =s 1k ∂∂ xL
s
i(k
k)
∂ ∂x Θs i( kk) +(cid:88) in =t
1
∂∂ xL
t
i(k
k)
∂ ∂x Θt i( kk)
)
(15)
11: end while
∂MMD2 c,u[D s,D t] =(cid:88)7 

1 (cid:88)N tlˆ ∂k(cid:16) xt i,ˆl,xt j,ˆl(cid:17)
V. EXPERIMENTSANDRESULTS
∂xt Nˆl(Nˆl−1) ∂xt
i l=1 t t i(cid:54)=j i In this section, we will conduct extensive experiments to
2
N (cid:88)tlˆ,N sl
∂k(cid:16)
xs j,l,xt
i,ˆl(cid:17)
evaluatetheproposedunsupervisedcross-datasetfacialexpres-
− . sion recognition method. Four lab-controlled datasets (CK+,
NlNˆl ∂xt
s t i,j=1 i JAFFE, MMI and Oulu-CASIA) and four real-world datasets
(16) (AffectNet, FER2013, RAF-DB 2.0 and SFEW 2.0) are used
to examine the adaption power of our method.
Given the Gaussian multi-kernel defined in Eq. (3), we typi-
cally take ∂k(xs,xt)/xs for example:
i j i
A. Databases and Setups
∂k(xs i,xt j) =−(cid:88)d β
uk (xs,xt)∗(xs−xt). (17) AffectNet, CK+, FER2013, JAFFE, MMI, Oulu-CASIA,
∂xs σ2 u i j i j
i u=1 u RAF-DB and SFEW 2.0 are widely-used facial expression
datasets. Fig. 1 shows the example samples of these datasets.
By updating the network parameters with mini-batch SGD, And the class distributions of these datasets used in the
the deep emotion-conditional adaption network (ECAN) can experiments are shown in Fig. 4.
learn discriminative representations by utilizing information (1) AffectNet Database [33]: The AffectNet database con-
from the source domain, and in the meantime, matching both tains around one million facial images downloaded from the
the marginal distribution and the conditional distribution be- Internetbyqueryingdifferentsearchenginesusing1,250emo-
tween source and target domains so that the good separability tion related tags in different languages. During experiment,
among different expressions can also generalize well to the around280,000imagesfromthetrainingsetand3,500images
target data. What’s more, by minimizing the differences in from the validation set with neutral and six basic expression
prior class probabilities, the ECAN can remedy the imbal- labels are chosen.
anced problem in facial expression recognition. Algorithm 1 (2) CK+ Database [34]: The lab-controlled database CK+
summarizes the learning process in the proposed ECAN. contains 593 video sequences from 123 subjects. Only 30910
14
12 10
8 6 4 2 0 1 2 3Emotion4 Classes5 6 7
segamI
fo rebmuN
104 350
300 250
200 150 100 50 0 1 2 3Emotion4 Classes5 6 7
(a) AffectNet
segamI
fo rebmuN
9000
8000 7000 6000
5000 4000 3000 2000 1000 0 1 2 3Emotion4 Classes5 6 7
(b) CK+
segamI
fo rebmuN
35
30 25
20 15 10 5 0 1 2 3Emotion4 Classes5 6 7
(c) FER2013
segamI
fo rebmuN
(d) JAFFE
180 160 140 120 100 80 60 40 20
0 1 2 3Emotion4 Classes5 6 7
segamI fo rebmuN 500 450 400 350 300 250 200 150 100 50
0 1 2 3Emotion4 Classes5 6 7
(e) MMI
segamI fo rebmuN 3000 2500 2000 1500 1000 500
0 1 2 3Emotion4 Classes5 6 7
(f) Oulu-CASIA
segamI fo rebmuN 300 250 200 150 100 50
0 1 2 3Emotion4 Classes5 6 7
(g) RAF-DB 2.0
segamI fo rebmuN
distribution of expression categories, we augment the dataset
with more samples in rare expression classes. Following the
previous collection criteria, 642 images with anger label, 886 images with disgust label and 1,010 images with fear label
have been supplemented into the original datasets to form a
morebalanceddataset,RAF-DB2.0.Wethenchooseasubset of the database with basic emotions for the experiments, in total 14,216 images.
(8)SFEW2.0Database[3]:Thein-the-wilddatabaseSFEW
(h) SFEW 2.0
contains dynamic images selected from different films with
Fig. 4: Distribution of seven expressions for seven represen- spontaneousexpressions,variousheadpose,agerange,occlu-
tative databases. 1, 2, 3, 4, 5, 6, 7 stand for Anger, Disgust, sions and illuminations. The challenging database is divided
Fear, Happiness, Neutral, Sadness, Surprise respectively. We into three sets for training, validation and testing. During
canseethatclassimbalanceproblemisverycommoninfacial experiments, we use 921 images in the training set and 427
expression datasets, especially for real-world scenarios. Note images in the validation part provided with labels, in total
thatforthelab-controlleddatasetJAFFEandOulu-CASIA,the 1,358 images.
numberofeachcategoryisintentionallysettobethesameby As RAF-DB 2.0 dataset achieves the best cross-dataset
experimenters. performance on the seven expressions classification task (see
themeanothersvaluesinTableI),wechooseRAF-DB2.0as
the source domain in our single-source domain adaption ex-
sequences have been labeled with six basic expression labels periments. In Figure 5, we further visualize the other datasets
(excluding Neutral). We then extract the final three frames look-alike images from RAF-DB 2.0 by picking out samples
of each sequence with peak formation, and in the meanwhile that are closed to the decision boundary, to see how RAF-DB
select the first frame (neutral face) from 309 sequences, 2.0 can resemble other different datasets.
resulting in 1,236 images.
(3) FER2013 Database [35]: The large-scale and uncon- B. Implementation Details
strained database FER2013 was created and labeled automati-
In our experiments, we finetune our model based on the
callybytheGoogleimagesearchAPI.AllimagesinFER2013
VGG-Face architecture [40] that pre-trained on a large-scale
have been registered and resized to 48×48 pixels. We use
face dataset with 2.6M images. Two MMD-based regulariza-
35,887 images with expression labels during experiments.
tion layers are appended to the second to last fully-connected
(4) JAFFE Database [36]: The JAFFE is a laboratory-
layer and the dimension of the last label prediction layer is
controlled database which contains only 213 samples from
amended to 7 for facial expression recognition. The already
10 Japanese females with posed expressions. Each person has
alignedgray-scaleimagesofeachdatasetarefirstlyresizedto
3ËIJ4imageswitheachofsixbasicfacialexpressionsandone
256×256 and then random 224×224 pixel patches cropped
image with a neutral expression. All images have been used
from these images are fed into the network. We also augment
in our experiments.
the data by flipping it horizontally with 50% probability.
(5) MMI Database [37]: The MMI is a lab-controlled
For hype-parameters settings, no labeled target samples are
database which includes 2,900 video sequences from 75
referred in our unsupervised domain adaptation experiments.
subjects with non-uniformly posed expressions and various
And we tune the trade-off parameters γ and λ from the sets
accessories. We select the first frame (neutral face) and the
{0,0.01,0.03,0.05,0.1,0.3,0.5,1} and {0,0.001,0.01,0.1}
three peak frames in each sequence with expression labels,
by grid searching, respectively. To suppress the noisy signal
resulting in 704 images.
from the pseudo label of target data, we further assign a rela-
(6) Oulu-CASIA Database [38]: The lab-controlled Oulu-
tivelysmallweightwtothesetwoparametersattheearlystage
CASIA database includes 2,880 image sequences collected of the training process. Specifically, w = 2 −1,
from 80 subjects labeled with six basic emotion labels. Each 1+exp(−10·p)
where p is the training progress linearly changing from 0
of the videos is captured with one of two imaging systems,
to 1. During training, stochastic gradient descent with 0.9
i.e., near-infrared (NIR) or visible light (VIS), under three
momentum is used. And the based learning rate is set to
different illumination conditions. Similar to CK+, we select
0.001. As the last classifier layer is trained from scratch, we
the last three peak frames and the first frame (neutral face)
set its learning rate to be 10 times that of the lower layers.
fromthe480videoswiththeVISSystemundernormalindoor
All experiments are implemented by Caffe Toolbox [41], and
illumination, resulting in 1,920 images.
run on a PC with a NVIDIA GTX 1080 GPU.
(7) RAF-DB 2.0 Database 1: The RAF-DB 2.0 is an exten-
sion to the current RAF-DB database [4], [39]. RAF-DB is a
C. Experimental Results
large-scale dataset which contains about 30,000 great diverse
facial images from thousands of individuals downloaded from In this subsection, we comprehensively evaluate our deep
the Internet. To address the concerns on the imbalanced domain adaption method on cross-dataset facial expression
recognition tasks in terms of the classification accuracy to
1http://www.whdeng.cn/RAF/model1.html demonstrate the effectiveness of our proposed algorithm. The11
(a) AffectNetlook-alikesfromRAF-DB2.0 (b) CK+look-alikesfromRAF-DB2.0 (c) FER2013look-alikesfromRAF-DB2.0
(d) MMIlook-alikesfromRAF-DB2.0 (e) Oulu-CASIAlook-alikesfromRAF-DB (f) SFEW 2.0 look-alikes from RAF-DB
2.0 2.0
Fig. 5: Other datasets Look-alike from RAF-DB 2.0. Samples from RAF-DB 2.0 that are closest to the decision boundaries
of SVM trained in the dataset recognition experiment are displayed.
TABLE III: Comparison of our methods with other results in
cross-domain results on four lab-controlled databases and
the literature on CK+ dataset.
comparisonswithotherstate-of-the-artsareshowninTable.III
– Table. VI. The comparison results of the other three real-
Methods Source Dataset Accuracy
world databases are shown in Table. VII. Note that “CNN”
refers to the VGG-Face network directly fine-tuned on the Da et al. [42] BOSPHORUS 57.60%
source dataset and “CNN+MMD” refers to the original MMD Zhang et al. [43] MMI 61.20%
Shallow
that only match the marginal distribution across domain. And Miao et al. [7] MMI + JAFFE 65.0%
Models
we list their best results after parameter tuning. Lee et al. [44] MMI 65.47%
On the whole, despite that different methods may employ Mayer et al. [45] MMI 66.20%
different datasets as the source domain, we can still observe Mollahosseini [46] 6 Datasets† 64.2%
thatournetworkoutperformsthecomparisonmethodsonmost Zavarez et al. [47] 6 Datasets‡ 88.58%
Deep
datasets,andcanachievecompetitiveperformancesondifficult Hasani et al. [48] MMI + JAFFE 73.91%
Models
transfer tasks, especially when source and target domains are Wen et al. [49] FER2013 76.05%
much more different. We then analyze the results for each
Wang et al. [50] FER2013 76.58%
dataset in detail.
CNN RAF-DB 2.0 78.00%
For CK+ dataset, Our ECAN achieves much better perfor- Our CNN + MMD RAF-DB 2.0 82.44%
mance than most other methods except [47] that conducted Methods ECAN RAF-DB 2.0 86.49%
cross-dataset facial expression recognition combining six lab- ECAN 4 Datasets(cid:63) 89.69%
controlled datasets as the source domain. It is worth noting
† MultiPIE,MMI,DISFA,FERA,SFEW,andFER2013.
that CK+ and these lab-controlled datasets are very similar in
‡ JAFFE,MMI,RaFD,KDEF,BU3DFEandARFace.
many respects, such as the controlled collection environment, (cid:63) RAF-DB2.0,JAFFE,MMI,Oulu-CASIA
subject characters, illumination condition and head postures.
Hence methods in the literature that used these databases
forcross-domainexpressionrecognitioncanachieverelatively a slight performance improvement, which indicates that our
good performance on CK+. However, our method only adopt method can mitigate the discrepancy across different datasets.
an unconstrained dataset which is collected from the Internet For JAFFE dataset, which is a highly biased dataset in
and much more diverse than the lab-controlled ones for the respect of gender and ethnicity, i.e., it only contains ten
transfer tasks. So we further evaluate our method using RAF- Japanesefemales,thefine-tuningtechniqueusedin[47]which
DB 2.0 with other three lab-controlled datasets as our multi- reported high accuracy on CK+ dataset is no longer effective
sourcedomain,andachievethebestcross-datasetperformance in this context. In contrast, by matching the marginal and
89.69%. On the other hand, when compared to the CK+ in- conditional distribution and also the class distribution across
dataset results in Table III-B (85.40%), ECAN also gains domains, our method yields the best performance and is12
TABLE IV: Comparison of our methods with other results in TABLE VI: Cross-dataset results on Oulu-CASIA dataset.
the literature on JAFFE dataset.
Target Methods Source Accuracy
Methods Source Dataset Accuracy CNN 59.39%
Oulu-CASIA CNN + MMD RAF-DB 2.0 60.14%
Shan et al. [1] CK 41.30%
ECAN 63.97%
El et al. [21] Bu-3DFE 41.96%
Shallow
Da et al. [42] CK+ 42.30%
Models TABLE VII: Comparison of our methods with other results in
Zhou et al. [51] CK 45.71%
the literature on AffectNet, FER2013 and SFEW 2.0 dataset.
Gu et al. [22] CK+ 55.87%
Wen et al. [49] FER2013 50.70% Target Method Source Accuracy
Deep
Ali et al. [52] RaFD 48.67% CNN RAF-DB 2.0 49.29%
Models
Zavarez et al. [47] 6 datasets(cid:63) 44.32% AffectNet CNN + MMD RAF-DB 2.0 48.76%
CNN RAF-DB 2.0 54.26% ECAN RAF-DB 2.0 51.84%
Our
CNN + MMD RAF-DB 2.0 58.64%
Methods El et al. [21] Bu-3DFE 20.57%
ECAN RAF-DB 2.0 61.94%
Liu et al. [55] CK+ 29.43%
(cid:63) CK+,MMI,RaFD,KDEF,BU3DFEandARFace. Mollahosseini [46] 6 datasets† 39.8%
SFEW CNN RAF-DB 2.0 52.67%
TABLE V: Comparison of our methods with other results in
CNN + MMD RAF-DB 2.0 52.81%
the literature on MMI dataset.
ECAN RAF-DB 2.0 54.34%
Methods Source Dataset Accuracy Mollahosseini [46] 6 datasets‡ 34.0%
Shan et al. [1] CK 51.10% CNN RAF-DB 2.0 55.38%
FER2013
Shallow Cruz et al. [53] CK+ 57.6% CNN + MMD RAF-DB 2.0 56.54%
Models Mayer et al. [45] CK 60.30% ECAN RAF-DB 2.0 58.21%
Zhang et al. [43] CK+ 66.90% † MultiPIE,CK+,DISFA,FERA,MMI,andFER2013.
Zavarez et al. [47] 6 datasets† 67.03% ‡ MultiPIE,CK+,DISFA,FERA,MMI,andSFEW.
Mollahosseini [46] 6 datasets‡ 55.6%
Deep
Hasani et al. [54] CK+ 54.76%
Models For AffectNet dataset, we find that the performance of the
Wang et al. [50] FER2013 61.86%
original MMD is slightly degraded due to the very different
Hasani et al. [48] CK+ 68.51%
class distribution. However, our ECAN can mitigate this
CNN RAF-DB 2.0 64.13% discrepancy and thus help boost the performance. And For
Our
CNN + MMD RAF-DB 2.0 65.80% the other two in-the-wild datasets FER2013 and SFEW 2.0,
Methods
ECAN RAF-DB 2.0 69.89% ourmethodsignificantlyoutperformsthecomparisonmethods.
† CK+,JAFFE,RaFD,KDEF,BU3DFEandARFace. Firstly, because most previous methods have chosen lab-
‡ MultiPIE,CK+,DISFA,FERA,SFEW,andFER2013. controlled databases that deviate from these two target sets as
the source domain, our method which uses the similar in-the-
wildRAF-DB2.0canperformbetter.Secondly,fromtheother
superiortothehighestaccuracyoftheliterature[22]by6.07%. perspective, as the domain discrepancy may not be distinct in
ForMMIdataset,ourECANachieves69.89%cross-dataset our case and the original MMD provides limited improve-
accuracy, which outperforms all the other compared methods ments over CNN, our ECAN can further help enhance the
and also the in-dataset performance shown in Table III-B. performance by a certain margin, which demonstrates that the
Comparing with the baselines, we can find that the original ECANisabletotransferdeepmodelsacrossvariousdomains.
CNN structure is inferior than some previous methods that Inaddition,resultsonthelarge-scaletestsetFER2013reveals
use more similar source dataset (such as CK+) with target that our method can also boost the recognition rate when the
MMIdatasetandtheoriginalMMD(CNN+MMD)onlygains targetdomainsizeismuchlargerthanthesourcedomainsize.
a negligible improvement. However, with the help of the re- Looking deeper into the experimental results, we can make
weighted MMD and the class-conditional MMD, the ECAN the following observations: (1) In consistent with the conclu-
can ameliorate the effect of the class distribution bias and sion in Sec. III, well-trained deep learning based methods
learn features with more discriminative ability, thus achieves generally outperform most traditional shallow models for
superior results. cross-database facial expression recognition, which is mainly
For Oulu-CASIA dataset, we can observe similar results: due to their generalization characteristic and the capacity for
With the supervision of the two MMD regularizations, which learning discriminative representations. (2) Among the deep
match the class distributions and close both the marginal and learning methods, the performances of fine-tuning technique
conditional distribution distance across domains, our method suffer a large decline when domain discrepancy becomes
can therefore achieve better performances when compared obvious, which indicates that the challenge of domain dis-
with the baselines and the in-dataset recognition performance. crepancy cannot be simply settled by fine-tuning techniques13
and mitigating the domain shift between different source
and target datasets is of great significance for cross-dataset 0.88 0.86
facial expression recognition. (3) In some cases that domain 0.84 0.82 discrepancy is relatively small or the class distribution bias 0.8 0.78 is obvious, the original MMD can yield only a small gain 0.76 0.74 or even worse performance. Nevertheless, our ECAN that 0.01 0.03 0.05 0.1 0.3 0.5 1
considers both marginal and conditional distributions and also
the class distribution bias can help improve the cross-dataset
recognition accuracy by learning class discriminative feature
representations and alleviating the class weight bias, which
reveals its effectiveness in emotion transfer learning task
varying different kinds of domains.
D. Empirical Analysis
In this subsection, we further analyze the effectiveness of
ECAN from several different aspects.
1) Parameter sensitivity: The tunable hype-parameters γ
and λ in ECAN represent the domain transfer degree.
To examine the sensitiveness of these two parameters, we
demonstrateparticularcross-domainresultsonthesmall-scale
lab-controlled database CK+ and the large-scale in-the-wild
database FER2013 in terms of different value of each hype-
parameter,whichcanbeseeninFig.6.Thebestresultsofthe
baseline CNN corresponding to the case γ =0 and λ=0 are
reported in Fig. 6 as the dashed lines.
We first conduct experiment as γ varying from 0.01 to
1, with λ fixed. In Fig. 6(a), it can be observed that for
CK+ the accuracy reaches the maximum when λ = 0.3
and then decreases as λ continues to increase. Similarly, the
performance on FER2013 peaks at λ=0.1 and then falls off.
Thisbell-shapedcurveexemplifiesthepromotingeffectofthe
jointly supervision of the softmax loss and the MMD-based
regularizers when a proper trade-off is chosen. Furthermore,
when comparing the performances of our ECAN with the
original MMD, our method behaves better than the original
MMD as λ varies. This suggests that the class imbalance
indeed causes a bottleneck for facial expression recognition
and the ECAN can effectively mitigate this problem by
learning suitable re-sampling weights for the source domain.
We then fix γ and vary λ from 0 to 0.1, and the results
are shown in Fig. 6(b). Note that λ = 0 refers to the case
of only using the re-weighted MMD term. And the further
improvements as the increase of λ indicates the importance
and necessity of the conditional MMD term that matches the
classconditionaldistributionsandenhancesthediscriminative
ability of the learned feature representations across domains
in the meanwhile.
2) Ablation study: To look more deeply into the proposed
method, we compare several variants of ECAN: “ECAN with
re-weight”thatonlyconsidersthere-weightedMMD,“ECAN
with condition” that only considers the conditional MMD
and “ECAN with re-weight and condition” that considers
both of two MMD regularizations. We then evaluate these
variantalgorithmson“RAF-DB2.0→CK+”cross-datasettask
followingthesameexperimentsettings.Theresultsareshown
in Table VIII. From the ablation results, we can notice that
all these variant methods of ECAN perform better than the
ycaruccA
CCDNNETNNN+MMD 0 0.5 .58 85
0.575 0.57 0.565 0.56 0.555 0.55 0.545 0.54 0.5350.01 0.03 0.05 0.1 0.3 0.5 1 ycaruccA
0.9 CCDNNETNNN+MMD 0. 08 .5
8 0.75 0.7 0.65 0.6 0.55 0.50 0.001 0.01 0.1
(a) γ onCK+(left)/FER2013(right)
ycaruccA CK+ FER2013
(b) λ
Fig. 6: Parameter sensitivity study on target datasets CK+ and
FER2013 w.r.t γ and λ. Results of the best baseline CNN are
shown as the dashed lines.
TABLE VIII: Ablation study of ECAN on “RAF-DB
2.0→CK+” cross-dataset facial expression recognition task.
Target Methods Source Accuracy
ECAN (re-weight) 85.24%
RAF-DB
CK+ ECAN (condition) 83.56%
2.0
ECAN (re-weight+condition) 86.49%
baselines. And our ECAN that considers both marginal and
conditional distributions and also the class distribution bias
achieves the best result in these variants, which indicates that
bothtwoMMDregularizationtermscooperatewitheachother
to achieve desirable adaption behaviors.
3) Feature visualization: To investigate how the ECAN
works on cross-database facial expression recognition, we
employed the t-SNE dimensionality reduction technique [56]
to visualize the learned features of CK+ on a 2-dimensional
embedding.FromFig.7(a)wecanfindthatsampleswithanger
and fear labels in CK+ tend to be misclassified into sadness
and surprise respectively, which are two dominant categories
in the source dataset RAF-DB 2.0. However, in Fig. 7(b), our
ECAN effectively alleviates this misguided decision caused
byclassdistributionbiasanddecreasestheclassificationerror
rate. What’s more, with the help of class-conditional MMD
term,sampleswithdifferentlabelscanbepulledfartheraway,
thustheseparabilityanddiscriminativeabilitycanbeensured.
VI. CONCLUSION
In this paper, a deep Emotion-Conditional Adaption Net-
work (ECAN) has been proposed to conduct unsupervised
cross-database facial expression recognition. The strength of
theECANliesinitsabilitytomakethemostofthebeneficial
knowledge from the target domain to simultaneously bridge
the discrepancy of both marginal and conditional distribution
betweensourceandtargetdomains,andalsothediscriminative
powerbroughtaboutbythelearningprocessofdeepnetwork.
Besides,classimbalanceproblemhas beentakenintoaccount
andare-weightingparameterisintroducedtobalancetheclass
bias between source and target domains. All these optimal
goals associate with each other and then effectively boost the
recognition rate of cross-database facial expression recogni-
tion. Extensive experiments on widely-used facial expression
datasets show that the proposed ECAN achieves excellent
performance in a series of transfer tasks and outperforms
previous cross-dataset facial expression recognition results,
demonstrating the effectiveness of the proposed method.14
IEEE transactions on Affective Computing, vol. 9, no. 1, pp. 21–37,
80 80 2018.
60 60 [18] Y. Zong, W. Zheng, X. Huang, J. Shi, Z. Cui, and G. Zhao, “Domain
40 40 regeneration for cross-database micro-expression recognition,” IEEE
20 20 TransactionsonImageProcessing,vol.27,no.5,pp.2484–2498,2018.
0 0 [19] Y.Li,J.Zeng,S.Shan,andX.Chen,“Occlusionawarefacialexpression
-20 1 -20 1 recognitionusingcnnwithattentionmechanism,”IEEETransactionson
-40 2 3 -40 2 3 ImageProcessing,vol.28,no.5,pp.2439–2450,May2019.
4 4
-60 5 6 -60 5 6 [20] “Chapter 19 - affective facial computing: Generalizability across do-
7 7
-80-80 -60 -40 -20 0 20 40 60 -80-80 -60 -40 -20 0 20 40 60 mains,” in Multimodal Behavior Analysis in the Wild, X. Alameda-
Pineda,E.Ricci,andN.Sebe,Eds. AcademicPress,2019,pp.407–
(a) CNN+MMD (b) ECAN 441.
[21] M.K.A.ElMeguidandM.D.Levine,“Fullyautomatedrecognitionof
Fig. 7: Feature visualization results of different methods on spontaneousfacialexpressionsinvideosusingrandomforestclassifiers,”
CK+ database. 1, 2, 3, 4, 5, 6, 7 stand for Anger, Disgust, IEEETransactionsonAffectiveComputing,vol.5,no.2,pp.141–154,
2014.
Fear, Happiness, Neutral, Sadness, Surprise respectively.
[22] W. Gu, C. Xiang, Y. Venkatesh, D. Huang, and H. Lin, “Facial
expressionrecognitionusingradialencodingoflocalgaborfeaturesand
classifiersynthesis,”PatternRecognition,vol.45,no.1,pp.80–91,2012.
[23] J.Whitehill,G.Littlewort,I.Fasel,M.Bartlett,andJ.Movellan,“To-
REFERENCES
ward practical smile detection,” IEEE transactions on pattern analysis
andmachineintelligence,vol.31,no.11,pp.2106–2111,2009.
[1] C. Shan, S. Gong, and P. W. McOwan, “Facial expression recognition [24] N. Dalal and B. Triggs, “Histograms of oriented gradients for human
based on local binary patterns: A comprehensive study,” Image and detection,” in Computer Vision and Pattern Recognition, 2005. CVPR
VisionComputing,vol.27,no.6,pp.803–816,2009. 2005.IEEEComputerSocietyConferenceon,vol.1. IEEE,2005,pp.
[2] G. Littlewort, M. S. Bartlett, I. Fasel, J. Susskind, and J. Movellan, 886–893.
“Dynamics of facial expression extracted automatically from video,” [25] K.He,X.Zhang,S.Ren,andJ.Sun,“Deepresiduallearningforimage
ImageandVisionComputing,vol.24,no.6,pp.615–625,2006. recognition,”inProceedingsoftheIEEEconferenceoncomputervision
[3] A.Dhall,R.Goecke,S.Lucey,andT.Gedeon,“Staticfacialexpression andpatternrecognition,2016,pp.770–778.
analysisintoughconditions:Data,evaluationprotocolandbenchmark,” [26] Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao, “Ms-celeb-1m: A
inComputerVisionWorkshops(ICCVWorkshops),2011IEEEInterna- dataset and benchmark for large-scale face recognition,” in European
tionalConferenceon. IEEE,2011,pp.2106–2112. ConferenceonComputerVision. Springer,2016,pp.87–102.
[4] S.Li,W.Deng,andJ.Du,“Reliablecrowdsourcinganddeeplocality- [27] Q.Cao,L.Shen,W.Xie,O.M.Parkhi,andA.Zisserman,“Vggface2:
preserving learning for expression recognition in the wild,” in 2017 A dataset for recognising faces across pose and age,” in 2018 13th
IEEEConferenceonComputerVisionandPatternRecognition(CVPR). IEEE International Conference on Automatic Face &amp; Gesture
IEEE,2017,pp.2584–2593. Recognition(FG2018). IEEE,2018,pp.67–74.
[5] M. Long, Y. Cao, J. Wang, and M. Jordan, “Learning transferable [28] C.-C. Chang and C.-J. Lin, “LIBSVM: A library for support
features with deep adaptation networks,” in International Conference vector machines,” ACM Transactions on Intelligent Systems and
onMachineLearning,2015,pp.97–105. Technology, vol. 2, pp. 27:1–27:27, 2011, software available at
http://www.csie.ntu.edu.tw/cjlin/libsvm.
[6] S.LiandW.Deng,“Deepemotiontransfernetworkforcross-database
[29] K.M.Borgwardt,A.Gretton,M.J.Rasch,H.-P.Kriegel,B.Schölkopf,
facial expression recognition,” in 24th International Conference on
and A. J. Smola, “Integrating structured biological data by kernel
PatternRecognition,ICPR,2018,pp.3092–3099.
maximummeandiscrepancy,”Bioinformatics,vol.22,no.14,pp.e49–
[7] Y.-Q.Miao,R.Araujo,andM.S.Kamel,“Cross-domainfacialexpres-
e57,2006.
sion recognition using supervised kernel mean matching,” in Machine
[30] A.Gretton,K.M.Borgwardt,M.J.Rasch,B.Schölkopf,andA.Smola,
Learning and Applications (ICMLA), 2012 11th International Confer-
“A kernel two-sample test,” Journal of Machine Learning Research,
enceon,vol.2. IEEE,2012,pp.326–332.
vol.13,no.Mar,pp.723–773,2012.
[8] J.Chen,X.Liu,P.Tu,andA.Aragones,“Learningperson-specificmod-
[31] J.Yosinski,J.Clune,Y.Bengio,andH.Lipson,“Howtransferableare
elsforfacialexpressionandactionunitrecognition,”PatternRecognition
features in deep neural networks?” in Advances in neural information
Letters,vol.34,no.15,pp.1964–1970,2013.
processingsystems,2014,pp.3320–3328.
[9] W.-S.Chu,F.DelaTorre,andJ.F.Cohn,“Selectivetransfermachinefor
[32] Y. Grandvalet and Y. Bengio, “Semi-supervised learning by entropy
personalized facial expression analysis,” IEEE transactions on pattern
minimization,” in Advances in neural information processing systems,
analysisandmachineintelligence,vol.39,no.3,pp.529–545,2017.
2005,pp.529–536.
[10] E. Sangineto, G. Zen, E. Ricci, and N. Sebe, “We are not all equal:
[33] A.Mollahosseini,B.Hasani,andM.H.Mahoor,“Affectnet:Adatabase
Personalizing models for facial expression analysis with transductive
forfacialexpression,valence,andarousalcomputinginthewild,”IEEE
parameter transfer,” in Proceedings of the 22nd ACM international
TransactionsonAffectiveComputing,vol.PP,no.99,pp.1–1,2017.
conferenceonMultimedia. ACM,2014,pp.357–366.
[34] P.Lucey,J.F.Cohn,T.Kanade,J.Saragih,Z.Ambadar,andI.Matthews,
[11] G. Zen, L. Porzi, E. Sangineto, E. Ricci, and N. Sebe, “Learning per-
“The extended cohn-kanade dataset (ck+): A complete dataset for
sonalizedmodelsforfacialexpressionanalysisandgesturerecognition,” actionunitandemotion-specifiedexpression,”inComputerVisionand
IEEETransactionsonMultimedia,vol.18,no.4,pp.775–788,2016.
PatternRecognitionWorkshops(CVPRW),2010IEEEComputerSociety
[12] S.Eleftheriadis,O.Rudovic,M.P.Deisenroth,andM.Pantic,“Gaussian Conferenceon. IEEE,2010,pp.94–101.
processdomainexpertsformodelingoffacialaffect,”IEEEtransactions [35] I. J. Goodfellow, D. Erhan, P. L. Carrier, A. Courville, M. Mirza,
onimageprocessing,vol.26,no.10,pp.4697–4711,2017. B. Hamner, W. Cukierski, Y. Tang, D. Thaler, D.-H. Lee et al.,
[13] M. X. Huang, J. Li, G. Ngai, H. V. Leong, and K. A. Hua, “Fast- “Challenges in representation learning: A report on three machine
padma:Rapidlyadaptingfacialaffectmodelfromsimilarindividuals,” learning contests,” in International Conference on Neural Information
IEEETransactionsonMultimedia,vol.20,no.7,pp.1901–1915,2018. Processing. Springer,2013,pp.117–124.
[14] H.Yan,“Transfersubspacelearningforcross-datasetfacialexpression [36] M.J.Lyons,S.Akamatsu,M.Kamachi,J.Gyoba,andJ.Budynek,“The
recognition,”Neurocomputing,vol.208,pp.165–173,2016. japanesefemalefacialexpression(jaffe)database,”1998.
[15] R. Zhu, G. Sang, and Q. Zhao, “Discriminative feature adaptation for [37] M.ValstarandM.Pantic,“Induceddisgust,happinessandsurprise:an
cross-domainfacialexpressionrecognition,”inBiometrics(ICB),2016 addition to the mmi facial expression database,” in Proc. 3rd Intern.
InternationalConferenceon. IEEE,2016,pp.1–7. WorkshoponEMOTION(satelliteofLREC):CorporaforResearchon
[16] K. Yan, W. Zheng, Z. Cui, and Y. Zong, “Cross-database facial ex- EmotionandAffect,2010,p.65.
pressionrecognitionviaunsuperviseddomainadaptivedictionarylearn- [38] G. Zhao, X. Huang, M. Taini, S. Z. Li, and M. PietikäInen, “Facial
ing,” in International Conference on Neural Information Processing. expression recognition from near-infrared videos,” Image and Vision
Springer,2016,pp.427–434. Computing,vol.29,no.9,pp.607–619,2011.
[17] W.Zheng,Y.Zong,X.Zhou,andM.Xin,“Cross-domaincolorfacial [39] S. Li and W. Deng, “Reliable crowdsourcing and deep locality-
expression recognition using transductive transfer subspace learning,” preserving learning for unconstrained facial expression recognition,”15
IEEE Transactions on Image Processing, vol. 28, no. 1, pp. 356–370,
2019.
[40] K.SimonyanandA.Zisserman,“Verydeepconvolutionalnetworksfor
large-scaleimagerecognition,”arXivpreprintarXiv:1409.1556,2014.
[41] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for
fastfeatureembedding,”arXivpreprintarXiv:1408.5093,2014.
[42] F. A. M. da Silva and H. Pedrini, “Effects of cultural characteristics
on building an emotion classifier through facial expression analysis,”
JournalofElectronicImaging,vol.24,no.2,pp.023015–023015,2015.
[43] X. Zhang, M. H. Mahoor, and S. M. Mavadati, “Facial expression
recognitionusing{l}_{p}-normmklmulticlass-svm,”MachineVision
andApplications,vol.26,no.4,pp.467–483,2015.
[44] S. H. Lee, K. N. K. Plataniotis, and Y. M. Ro, “Intra-class variation
reduction using training expression images for sparse representation
based facial expression recognition,” IEEE Transactions on Affective
Computing,vol.5,no.3,pp.340–351,2014.
[45] C.Mayer,M.Eggers,andB.Radig,“Cross-databaseevaluationforfacial
expressionrecognition,”Patternrecognitionandimageanalysis,vol.24,
no.1,pp.124–132,2014.
[46] A.Mollahosseini,D.Chan,andM.H.Mahoor,“Goingdeeperinfacial
expressionrecognitionusingdeepneuralnetworks,”inApplicationsof
Computer Vision (WACV), 2016 IEEE Winter Conference on. IEEE,
2016,pp.1–10.
[47] M. V. Zavarez, R. F. Berriel, and T. Oliveira-Santos, “Cross-database
facial expression recognition based on fine-tuned deep convolutional
network,” in Graphics, Patterns and Images (SIBGRAPI), 2017 30th
SIBGRAPIConferenceon. IEEE,2017,pp.405–412.
[48] B. Hasani and M. H. Mahoor, “Spatio-temporal facial expression
recognitionusingconvolutionalneuralnetworksandconditionalrandom
fields,”arXivpreprintarXiv:1703.06995,2017.
[49] G. Wen, Z. Hou, H. Li, D. Li, L. Jiang, and E. Xun, “Ensemble of
deepneuralnetworkswithprobability-basedfusionforfacialexpression
recognition,”CognitiveComputation,pp.1–14,2017.
[50] X. Wang, X. Wang, and Y. Ni, “Unsupervised domain adaptation for
facial expression recognition using generative adversarial networks,”
Computationalintelligenceandneuroscience,vol.2018,2018.
[51] J.Zhou,T.Xu,andJ.Gan,“Featureextractionbasedonlocaldirectional
patternwithsvmdecision-levelfusionforfacialexpressionrecognition,”
InternationalJournalofBio-scienceandBio-technology,vol.5,no.2,
pp.101–110,2013.
[52] G. Ali, M. A. Iqbal, and T.-S. Choi, “Boosted nne collections for
multiculturalfacialexpressionrecognition,”PatternRecognition,vol.55,
pp.14–27,2016.
[53] A. C. Cruz, B. Bhanu, and N. S. Thakoor, “One shot emotion scores
forfacialemotionrecognition,”inImageProcessing(ICIP),2014IEEE
InternationalConferenceon. IEEE,2014,pp.1376–1380.
[54] B. Hasani and M. H. Mahoor, “Facial expression recognition us-
ing enhanced deep 3d convolutional neural networks,” arXiv preprint
arXiv:1705.07871,2017.
[55] M. Liu, S. Li, S. Shan, and X. Chen, “Au-inspired deep networks for
facialexpressionfeaturelearning,”Neurocomputing,vol.159,pp.126–
136,2015.
[56] L. Van Der Maaten, “Accelerating t-sne using tree-based algorithms.”
Journal of machine learning research, vol. 15, no. 1, pp. 3221–3245,
2014."
8,8,A high-resolution spontaneous 3d dynamic facial expression database,"['X Zhang', 'L Yin', 'JF Cohn', 'S Canavan']",2013,920,Binghamton University 3D Facial Expression,"classification, classifier, deep learning, facial expression recognition",3D video database of spontaneous facial expressions in a diverse group of young adults.  Well-validated emotion inductions were used to elicit expressions  units and emotion detection”,No DOI,… gesture recognition  …,https://www.sciencedirect.com/science/article/pii/S0262885614001012,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
9,9,A multimodal database for affect recognition and implicit tagging,"['M Soleymani', 'J Lichtenauer', 'T Pun']",2011,1719,Affective Faces Database,"classification, classifier","our database in Section 3. Section 4 explains the experimental setup. The first experiment  paradigm, some statistics and results of classifications  to face videos from 32 participants. The",No DOI,… on affective computing,https://ieeexplore.ieee.org/document/5975141,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
10,10,A natural visible and infrared facial expression database for expression recognition and emotion inference,"['S Wang', 'Z Liu', 'S Lv', 'Y Lv', 'G Wu', 'P Peng']",2010,458,Affective Faces Database,facial expression recognition,"database for expression recognition and emotion inference, we conduct visible facial  expression recognition  There are, however, a few thermal face databases (listed in Table II) that",No DOI,IEEE Transactions …,https://ieeexplore.ieee.org/document/5523955,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
11,11,A new facial expression recognition based on curvelet transform and online sequential extreme learning machine initialized with spherical clustering,"['A Uçar', 'Y Demir', 'C Güzeliş']",2016,158,Extended Cohn-Kanade,machine learning,"So, the learning machine is called as OSELM-SC. It is  Expression database and the  Cohn-Kanade database. The  of California at irvine repository of the machine learning database",No DOI,Neural Computing and Applications,https://link.springer.com/article/10.1007/s00521-014-1569-1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,springer.com,
12,12,A new method for facial expression recognition based on sparse representation plus LBP,"['MW Huang', 'Z Wang', 'ZL Ying']",2010,101,Japanese Female Facial Expression,classification,"algorithm for face recognition, notable for  facial expression recognition based on sparse  representation of LBP features. Extensive experiments on Japanese Female Facial Expression (",No DOI,2010 3rd International …,https://ieeexplore.ieee.org/document/5647898,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
13,13,A performance comparison of eight commercially available automatic classifiers for facial affect recognition,"['D Dupré', 'EG Krumhuber', 'D Küster', 'GJ McKeown']",2020,165,"Acted Facial Expressions In The Wild, Affective Faces Database, Static Facial Expression in the Wild","classification, classifier","classifiers, in particular when facial expressions are spontaneous rather than posed. In the  present work, we tested eight out-of-the-box automatic classifiers,  or acted facial behavior.",No DOI,Plos one,https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0231968,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,plos.org,
14,14,A review on automatic facial expression recognition systems assisted by multimodal sensor data,"['N Samadiani', 'G Huang', 'B Cai', 'W Luo', 'CH Chi', 'Y Xiang']",2019,206,"Binghamton University 3D Facial Expression, MMI Facial Expression",FER,"facial expressions (eg, sadness and anger). An efficient facial expression system (FER) can   ; this method achieves the accuracies of 98.5% (CK+ dataset) and 81.18% (MMI dataset).",No DOI,Sensors,https://www.mdpi.com/1424-8220/19/8/1863,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,mdpi.com,
15,15,A survey of 2D face recognition techniques,"['M Chihaoui', 'A Elkefi', 'W Bellil', 'C Ben Amar']",2016,131,Toronto Face Database,classification,"Figure 6 summarizes the classification of face recognition approaches presented in this   Next, we presented face databases used by researchers in this field to test their approaches and",No DOI,Computers,https://www.mdpi.com/2073-431X/5/4/21,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,mdpi.com,
16,16,A survey on facial emotion recognition techniques: A state-of-the-art literature review,"['FZ Canal', 'TR Müller', 'JC Matias', 'GG Scotton']",2022,222,Affective Faces Database,facial expression recognition,"approaches for emotion recognition from facial expressions,  area of emotion expression  recognition captured from facial  areas such as face recognition and emotion recognition color",No DOI,Information …,https://www.sciencedirect.com/science/article/pii/S0020025521010136,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
17,17,"A systematic review on affective computing: Emotion models, databases, and recent advances","['Y Wang', 'W Song', 'W Tao', 'A Liotta', 'D Yang', 'X Li', 'S Gao']",2022,315,Affective Faces Database,"CNN, FER, classification, classifier, deep learning, machine learning, neural network","affective computing, we provide a systematical survey of important components: emotion  models, databases,  by five kinds of commonly used databases for affective computing. Next, we",No DOI,Information …,https://www.sciencedirect.com/science/article/pii/S1566253522000367,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
18,18,Accurate periocular recognition under less constrained environment using semantics-assisted convolutional neural network,"['Z Zhao', 'A Kumar']",2016,136,Toronto Face Database,neural network,"v4 is the first publicly available long-range iris and face database acquired under NIR  illumination, which is released by the Center for Biometrics and Security Research (CBSR) from",No DOI,IEEE Transactions on Information Forensics …,http://ieeexplore.ieee.org/document/7775081,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
19,19,Acted facial expressions in the wild database,"['A Dhall', 'R Goecke', 'S Lucey', 'T Gedeon']",2011,140,"Acted Facial Expressions In The Wild, Static Facial Expression in the Wild","classification, facial expression recognition, machine learning, neural network","The Cohn-Kanade database [17] is widely used facial expression database. This database  formed a standard for facial expression recognition, it contains both static and dynamic data",No DOI,"… , Australia, Technical Report …",https://paperswithcode.com/dataset/acted-facial-expressions-in-the-wild-afew,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,paperswithcode.com,
20,20,Ad-corre: Adaptive correlation-based loss for facial expression recognition in the wild,"['AP Fard', 'MH Mahoor']",2022,113,"Affective Faces Database, Expression in-the-Wild","FER, classification, classifier, facial expression recognition","FER and then discuss the use of DML for general image classification tasks. Afterward, we  review the work used DML in FER expressional features from an input face image, as well as",No DOI,IEEE Access,https://ieeexplore.ieee.org/document/9727163,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
21,21,Adaptive deep metric learning for identity-aware facial expression recognition,"['X Liu', 'BVK Vijaya Kumar', 'J You']",2017,236,"CMU Multi-PIE, MMI Facial Expression",FER,"features which are not useful for FER. As shown in Fig. 1,  For FER, we desire that two  face images with the same  approaches in posed facial expression dataset (eg, CK+, MMI),",No DOI,Proceedings of the IEEE …,http://ieeexplore.ieee.org/document/8014813,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
22,22,Adaptive feature mapping for customizing deep learning based facial expression recognition model,"['BF Wu', 'CH Lin']",2018,106,"Extended Cohn-Kanade, Radboud Faces Database","deep learning, machine learning","progress in this artificial intelligence era. Many deep learning approaches have been applied  in  Compared to the competing deep learning architectures with the same training data, our",No DOI,IEEE access,https://ieeexplore.ieee.org/document/8291717,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
23,23,Addressing bias in machine learning algorithms: A pilot study on emotion recognition for intelligent systems,"['A Howard', 'C Zhang', 'E Horvitz']",2017,115,"Affective Faces Database, Radboud Faces Database",machine learning,"on 50% of the feature vectors classified as Fear or Surprise by the deep learning algorithm  and extracted from the Radboud Faces Database, the Dartmouth Database of Children’s",No DOI,2017 IEEE Workshop on …,https://ieeexplore.ieee.org/abstract/document/8025197,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
24,24,Aff-wild2: Extending the aff-wild database for affect recognition,"['D Kollias', 'S Zafeiriou']",2018,167,Affective Faces Database,"CNN, deep learning, neural network","the paper is that by training deep neural networks on the Aff- -art emotion recognition  performance on the other database,  of Aff-Wild2 database starts with the detection of faces [1] in",No DOI,arXiv preprint arXiv:1811.07770,https://arxiv.org/abs/1811.07770,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"Aff-Wild2: Extending the Aff-Wild Database for Affect Recognition
Dimitrios Kollias1 and Stefanos Zafeiriou1,2
1 Department of Computing, Imperial College London, UK
2 Centre for Machine Vision and Signal Analysis, University of Oulu, Finland
Abstract—Automatic understanding of human affect using poral dynamics caused by different expression intensities.
visualsignalsisaproblemthathasattractedsignificantinterest In reality, expressions can vary greatly in intensity, which
over the past 20 years. However, human emotional states
constitutesacrucialcuefortheinterpretationofexpressions.
are quite complex. To appraise such states displayed in real-
Thatiswhy,morerecently,affectivecomputingresearches
world settings, we need expressive emotional descriptors that
are capable of capturing and describing this complexity. The are shifting towards dimensional emotion analysis for bet-
circumplex model of affect, which is described in terms of ter understanding of human emotions. Dimensional theory
valence(i.e.,howpositiveornegativeisanemotion)andarousal [45],[38] considers an emotional state as a point in a
(i.e., power of the activation of the emotion), can be used
continuous space, the 2D Emotion Wheel [35], shown in
for this purpose. Recent progress in the emotion recognition Figure 1. The dimensions represent arousal, indicating the
domain has been achieved through the development of deep
neural architectures and the availability of very large training levelofaffectiveactivation,andvalence,measuringthelevel
databases. To this end, Aff-Wild has been the first large-scale of pleasure. Hence, dimensional theory can model subtle,
”in-the-wild” database, containing around 1,200,000 frames. complicated, and continuous affective behaviors.
In this paper, we build upon this database, extending it with
260 more subjects and 1,413,000 new video frames. We call
the union of Aff-Wild with the additional data, Aff-Wild2.
The videos are downloaded from Youtube and have large
variations in pose, age, illumination conditions, ethnicity and
profession. Both database-specific as well as cross-database
experiments are performed in this paper, by utilizing the
Aff-Wild2, along with the RECOLA database. The developed
deep neural architectures are based on the joint training of
state-of-the-art convolutional and recurrent neural networks
with attention mechanism; thus exploiting both the invariant
properties of convolutional features, while modeling temporal
dynamics that arise in human behaviour via the recurrent
layers. The obtained results show premise for utilization of
the extended Aff-Wild, as well as of the developed deep neural
architectures for visual analysis of human behaviour in terms
of continuous emotion dimensions.
I. INTRODUCTION Fig.1. The2DEmotionWheel,orinotherwords,the2DValence-Arousal
Space
The field of Automatic Facial Expression Analysis has
grownrapidlyinrecentyears,withapplicationsspreadacross
Various emotion recognition databases have been devel-
a variety of fields, such as medicine [41], [42], health[19],
oped utilizing the dimensional emotion representation. Ex-
monitoring,entertainment,liedetection[50],[22].Thereare
amples are the SAL [7], SEMAINE [32], MAHNOB-HCI
two major emotion computing models according to theories
[40],Belfastnaturalistic1,Belfastinduced[39],DEAP[13],
inpsychologyresearch[30]:discreteanddimensionaltheory.
RECOLA [37], SEWA 2 databases. Nevertheless, the major-
Most of the past research has revolved around the recog-
ity of collected data this far, although containing naturalistic
nition of the so-called six universal expressions [6], [3],
emotional states, have been captured in highly controlled
[25], [16], [18], which are intuitive and simple, but cannot
recording conditions and have rather small size. Moreover,
express complex affective states. Also, some Facial Ex-
they display very low diversity in terms of the total number
pression Recognition and Analysis systems proposed in the
of subjects they contain; they include a limited amount of
literaturefocusedonthebinaryoccurrenceofexpressionsas
head pose variations and occlusions, static backgrounds and
FACS Action Units (AUs) [8], assuming that the expression
uniform illuminations. Recently, Aff-Wild was created [49],
intensity is fixed. Some representative datasets developed in
[21], constituting the first large-scale ”in-the-wild” database,
labs, which are still used in many recent works [12], are
with over 60 hours of video data, annotated in terms of the
the Cohn-Kanade database [43], [29], the MMI database
valence-arousal dimensions.
[33],[44],theMulti-PIEdatabase[9]andtheBU-3D/BU-4D
To this end, in this paper, our contribution is a significant
databases [47], [46]. Most lab-controlled databases focused
on detecting the occurrence of expressions, regardless of 1https://belfast-naturalistic-db.sspnet.eu/
the significant differences in appearance, shape, and tem- 2http://sewaproject.eu
9102
ceD
31
]VC.sc[
2v07770.1181:viXraFig. 2. Some frames from the additional data that formed Aff-Wild2, showing subjects of different ethnicities and age groups, in different emotional
states,inavarietyofheadposes,illuminationconditionsandocclusions.
extension of the Aff-Wild database, by approximately dou- collectedusingtheYoutubevideosharingweb-site.Themain
bling the number of included video frames and the number keyword that was used to retrieve the videos was reaction.
of subjects; thus, improving the variability of the included
B. Additional dataset and properties
behaviors and of the involved persons. We call the union of
We collected a new dataset consisting of 260 videos,
theexistingAff-Wildwiththeadditionaldata,Aff-Wild2.Let
having 1,413,000 frames and a total length of 13 hours and
us note here that this constitutes a preliminary work of [27],
5minutes.11outofthose260videosdisplayedtwosubjects,
[24]. We then present end-to-end deep neural architectures
all of which have been annotated. In total, the new dataset
trained over Aff-Wild2 and used for estimation of the va-
shows 258 subjects, with 149 being male and 109 female.
lence and arousal emotion dimensions. The proposed deep
Table I shows the general attributes of the additional data
architectures are composed of convolutional and recurrent
added to the Aff-Wild, forming the Aff-Wild2.
neural network (CNN-RNN) layers, along with an attention
The aim of adding these new videos in the existing Aff-
mechanism.
Wild has been to extend the spontaneous facial behaviors
Another contribution of the paper is that by training deep
in arbitrary recording conditions met in Aff-Wild, also sig-
neural networks on the Aff-Wild2 and by fine-tuning them
nificantly increasing the number of different subjects in it.
onanotherexistingdatabases,suchasRECOLA,weachieve
All these additional videos have wide range in subjects’:
state-of-the-artemotionrecognitionperformanceontheother
age (from babies to young children to elderly people);
database, substantially exceeding the performance of other
ethnicity (subjects are caucasian, hispanic or latino, asian,
networks trained with that database.
black, or african american); profession (e.g. actors, athletes,
All of the obtained results illustrate the very good per-
politicians, journalists); head pose; illumination conditions;
formance of the presented approach and its potential for
occlussions; emotions.
effective visual analysis of human behavior in terms of
continuous emotion dimensions. TABLEI
Section II of the paper presents the Aff-Wild database GENERALATTRIBUTESOFTHEADDITIONALDATAADDEDTOTHE
and then continues with a detailed description of the ad- AFF-WILD,FORMINGAFF-WILD2
ditional dataset that was collected, illustrating the related
Attribute Description
attributes, the pre-processing and annotation steps. Section
Noofframes 1,413,000
IIIdescribesthefacialandemotionanalysis,focusingonthe
Noofvideos 260
deepneuralarchitecturesandtheevaluationmetricsthatwere
Noofsubjects 258(149male;109female)
used. Section IV presents the experimental study and the Noofannotators 4
obtainedresults.Conclusionsandfurtherworkaredescribed Lengthofvideos 3secs 15mins4secs
−
Videoformat MP4
in Section V of the paper.
MeanImageResolution 1454 890
×
II. THEAFF-WILD2DATABASE Figure 2 shows some frames from the additional data,
verifyingthein-the-wildnatureofthisset,inwhichpeopleof
A. The existing Aff-Wild database
differentethnicitiesandagegroupsdisplayvariousemotions,
The Aff-Wild has been the first large scale captured in- with different head poses and facial occlusions and under
the-wild database, consisting of 298 videos and displaying different illumination conditions.
reactionsof200subjects,withatotalvideodurationofmore ThevideoswerecollectedusingtheYoutubevideosharing
than30hours.Thetotalnumberofframesinthisdatabaseis web-site. All of the collected videos were provided under
1,180,000.Regardingsubjects’gender,130ofthemaremale Creative Commons licence. The keywords that were used to
and 70 female. This database has been annotated by 8 lay retrievethevideosweremainly”reaction”andotheremotion
experts with regards to valence and arousal. The Aff-Wild relatedwordsfromthe2-DEmotionWheel,showninFigure
database served as benchmark for the Aff-Wild Challenge, 1.
organized in conjunction with CVPR 2017. The aim for The videos show subjects who: react on positive and
this database was to collect spontaneous facial behaviors in unexpected surprise (i.e., receiving gifts); are stand-up co-
arbitrary recording conditions. To this end, the videos were medians; give a really interesting speech in ceremonies (i.e.oscar awards, united nations); participate in interviews (i.e.,
job or football or news ones); are taking an oral exam;
react to flirt, or rejection; are in a car that runs really
fast; are (movie) stars and react to mean, or funny tweets;
are in a talk show; are giving an audition for a role; are
giving lectures on depression, or other serious disorders;
reactonsomethingthatbringsthemhappiness,orfulfillment
(i.e., seeing their newborn baby for the first time); react
on important political issues; are performing activities that
are either passive, boring, apathetic, or intense (i.e., yoga,
anxiety and anger control, roller-coaster); are watching a
highly anticipated trailer, or horror movie; are teaching a
new language.
AllthevideosareinMP4formatandhavebeenannotated
intermsofvalenceandarousal.Foursubjectshaveannotated
the videos using the method proposed in [4]. That is, an
on line annotation procedure was used, according to which
annotators were watching each video and provided their
annotations through a joystick. Valence and arousal values
ranged continuously in [ 1, +1].
−
Figure 3 shows an example of annotated valence and
arousal values over a part of a video in the additional data,
together with some corresponding frames. This illustrates Fig.4. Histogramofvalenceandarousalannotationsoftheadditionaldata
the in-the-wild nature of the database, namely, including addedinAff-WilddatabaseandshapingAff-Wild2.
manydifferentemotionalstates,rapidemotionalchangesand
occlusions in the facial areas.
content (i.e., advertisements, scenes with no human face, or
with only captions).
After that, some videos were split into smaller videos
because: (i) they included individual scenes with different
people,or(ii)intermsofthetimedimension,theydisplayed
a person into different, non-continuous time instances. An
example of case (i) is a video showing the reactions of 7
people inside a car, showing a different scene for each per-
son;thisvideowasseparatedinto7videos,independentfrom
each other. An example of case (ii) is a video consisting of
two parts, with the first recorded in day light and the second
recorded at night. This video was split in two new videos,
since the person appearing in them was not continuously
expressing an emotion.
It is worth mentioning that when two people appear
simultaneously in the video (e.g., having a discussion), this
Fig.3. Valenceandarousalannotationsoverapartofavideo,alongwith
corresponding frames, illustrating the in-the-wild nature of the additional video was not split in two new ones, but has been retained
dataset(differentemotionalstates,rapidemotionalchanges,occlusions) as it was. We allowed such cases, so that the emotion
recognition system can learn from the interaction between
Figure 4 provides a histogram of the annotated values for those two persons. In our database, we had 11 such cases
valenceandarousalinthegeneratedadditionaldataset.Since andallsubjectsappearinginthevideoshavebeenannotated.
we have focused on users’ reactions, arousal is generally After this step, as already mentioned, the generated dataset
positive; positive reactions were the majority, resulting in included 260 videos in total.
more positive than negative valence values. 2) Frame-rate conversion: Each of those 260 videos had
a different frame per second (fps) value. As can be seen in
C. Data pre-processing
TableII,themajorityofthevideoshadafpsratearound30.
1) Pre-processingatvideolevel: Atfirst,241videoshave
To have the same frame rate over all videos, we converted
been downloaded from Youtube selecting the best video
them to have the same fps value, equal to 30.
(imageandaudio)quality.Thesevideoswerethenconverted
D. Data annotation
into MP4 format using FormatFactory. Using VirtualDub
[28], the videos were processed and trimmed, especially 1) The annotation procedure: For data annotation, we
at the beginning and ending, so as to remove unnecessary usedthesametoolas[48]thatbuildsonotherexistingones,TABLEII
did not examine higher values, to avoid severely changing
NUMBERANDDURATIONOFVIDEOSFOUNDINTHEADDITIONALDATA,
the annotators’ values. The corresponding MIAC values for
ALONGWITHTHEIRCORRESPONDINGFPSRATES.
valence-arousalwere(0.603,0.582),(0.62,0.594)and(0.63,
Fpsvalue 30 29 25 60 15 17 0.60). We selected the latter approach, since it provided a
Noofvideos 168 10 60 15 5 2 slightlybetterMAICvalue.Anexamplesetofannotationsis
Duration 8:25:00 0:35:00 2:10:00 1:35:00 0:10:00 0:05:00
showninFigure5,inanefforttoclarifytheobtainedMAIC
values. It shows the four annotations in a video segment
for valence, with MAIC value of 0.64 (similar to the value
specifically Feeltrace [4] and Gtrace [5]. A time-continuous
obtained over all additional data).
annotation is performed for each affective dimension.
Itshouldbementionedthattheannotationtoolalsohasthe
1
abilitytoshowtheinsertedvalenceandarousalvalueswhile
displaying a respective video. This was used for annotation 0.8
verification, as described below, in the annotation post-
0.6
processing step.
Four experts were chosen to perform the annotation task. 0.4
All annotators were computer scientists who had worked on
0.2
face analysis problems and all had a working understanding
of facial expressions. Each annotator was instructed orally 0
and through a multi-page document on the procedure to
0.2
follow for the task. This document included a list of some −
well identified emotional cues for both arousal and valence, 0.4
− 0 200 400 600 800 1,000 1,200 1,400 1,600 1,800 2,000
providing a common basis for the annotation task. On top
Frames
of that the experts used their own appraisal of the subjects’
emotional states for creating the annotations. Before starting
the annotation of each video, the experts watched the whole
video, so as to know what to expect regarding the emotions
being displayed in the video.
2) Annotation post-processing: After the annotators had
completed the insertion of the selected valence and arousal
values in each video, a post-processing annotation verifica-
tion step was performed.
Every expert-annotator watched all videos for a second
time, in order to verify that the recorded annotations were
in accordance with the shown emotions in the videos, or
decide to change the annotations accordingly. In this way,
a further validation of annotations was achieved. After the
annotations have been validated by the annotators, a final
annotation selection step followed.
Twodifferentapproacheswereconsideredfordetermining
the final label values. Those were: (i) computing the mean
of the 4 annotators, (ii) performing median filtering for each
video, on each expert annotation and then computing the
mean. To select one of them, we performed, independently
for valence and arousal the following procedure: At first,
we computed the inter-annotator correlations, i.e., the cor-
relations of each one of the four annotators with all other
annotators, over all videos; this resulted in three correlation
valuesperannotator.Then,wecomputedforeachannotator,
his/her average inter-annotator correlations, resulting in one
pairof(valence,arousal)valuesperannotator.Afterthat,we
computed the mean of those pairs of values; let us denote
theresultingmeanasMAIC,meaningmeanofaverageinter
annotation correlation. In approach (i), MAIC was (0.60,
0.58) for valence-arousal. In approach (ii), we experimented
with different values of the median filter size, in particular
5, 15 and 30 frames (0.1, 0.5 and 1 sec, respectively). We
noitatonnAecnelaV
Annotator1
Annotator2
Annotator3
Annotator4
Fig.5. Allfourvalenceannotationsinavideosegment.ThevalueofMAIC
(mean of average inter annotation correlation) is 0.64 which is similar to
themeanMAICobtainedoveralladditionaldata.
E. The properties of Aff-Wild2 database
Since all additional videos are in MP4 format and have a
framerateof30,thevideosoftheoriginalAff-Wilddatabase
havealsobeenconvertedtoMP4formatwithafpsrateequal
to 30. Next, we concatenated the additional dataset with the
Aff-Wild database, creating a new database, which we call
Aff-Wild2. In total, Aff-Wild2 consists of 558 videos with
2,786,201frames.Thetotalnumberofsubjectsis458,with
279 of them being male and 179 being female.
F. Database partition sets
The Aff-Wild2 database was also split into three sets:
training, validation and test sets. The partitioning was done
in a subject independent manner, in the sense that a person
could appear only in one of those three sets and not in
all, or in any combination of them. The resulting training,
validationandtestsetsconsistedof350,70,138videos,with
1,601,000 , 405,000 and 780,201 frames, respectively.
G. Other Databases: RECOLA
The REmote COLlaborative and Affective (RECOLA)
dataset is a corpus that monitors subjects collaborating in
dyadic teams. This corpus includes multimodal data, i.e.
audio, video, ECG and EDA (physiological data). It consists
of 46 French speaking subjects being recorded for 9.5h
recordings in total. The recordings have been annotated, in
termsofvalenceandarousal,for5minuteseachby6French-
speaking annotators (three male, three female). The dataset
is divided into three parts, namely, training (16 subjects),validation (15 subjects) and test (15 subjects), in such a way layers in the RNN, each having 128 hidden units, as in [23],
thatthegender,ageandmothertonguearebalanced.Valence [26].
and arousal range continuously in [ 1, +1]. 3) Best performing architecture/model: Figure 6 shows
−
the architecture/model that achieved the best results in Aff-
III. FACIALANDEMOTIONANALYSIS
Wild2,asshowninSectionIV.TheCNNpartisbasedonthe
A. Face detection VGGFACE, the RNN part is based on GRUs taking as input
the output of the first FC layer of VGGFACE, an attention
The analysis of Aff-Wild2 database starts with the de-
layer is stacked on top of the RNN and before the output
tection of faces [1] in all video frames. We followed the
layer that gives the valence and arousal predictions.
same methodology described in [31], which was also per-
formed in [48] for generating facial bounding boxes in Aff-
Wild database. The resulting facial images were resized to
resolution of 96 96 3, with their intensity values being
× ×
normalized to the range [ 1, +1].
−
B. The CNN-RNN Deep Neural Architectures
SincetheAff-Wild2databaseconsistsofvideos,wedevel-
oped CNN-RNN architectures that can exploit the temporal
dynamics of the data. The following strategies have been
adopted for training all CNN-RNN networks: (i) keep the
CNN weights fixed to their pre-trained values and train only
the RNN layers; (ii) randomly initialize the RNN part and
trainthewholeCNN-RNN;(iii)initializetheRNNpartwith
the best performing model of (i) and then train the whole
CNN-RNN. In the following, we present details about the
CNN and RNN parts of the developed architectures:
Fig.6. Thearchitecture/modelthatachievedthebestresultsinAff-Wild2
1) CNN part of the architectures: For CNN models,
we experimented with the VGGFACE[34], ResNet-50[10]
and DenseNet-121[11] structures, which can provide rich C. The evaluation criterion
representations for the visual emotion analysis task. In the
The criterion that is considered for evaluating the per-
VGGFACE and ResNet-50 cases, we considered three dif-
formance of the networks is Concordance Correlation Co-
ferent setting, where networks are: a) pre-trained on the
efficient (CCC), which is used in all related Challenges
VGGFACE database[34], b) pre-trained on the VGGFACE2 (AVEC [36], Affect-in-the-Wild3). CCC takes values in the
database[2], and c) pre-trained on the VGGFACE and then
range [ 1,+1], with +1 indicating perfect concordance and
on the VGGFACE2 datasets. In the DenseNet-121 case we −
1 perfect discordance. The highest the value of the CCC
considered a setting in which the network is pre-trained on −
the better the fit between annotations and predictions, and
the ImageNet database. Those network settings are shown
thereforehighvaluesaredesired.CCCisdefinedasfollows:
in Table III, together with the denoting used for referencing
these networks in the experimental study. 2s xy
ρ = (1)
c s2+s2+(x¯ y¯)2
x y
TABLEIII −
CNNARCHITECTURESWITHCORRESPONDINGPRE-TRAINEDMODEL where s x and s y are the variances of the ground truth and
predicted values respectively, x¯ and y¯ are the corresponding
Network Databasepre-trainedon Denote mean values and s is the respective covariance value.
xy
VGGFace VGGFace VGGFace1/ResNet1
The loss function used for training all the networks was:
or VGGFace2 VGGFace2/ResNet2
ResNet-50 VGGFace+VGGFace2 VGGFace12/ResNet12
ρ +ρ
DenseNet-121 ImageNet DenseNet L =1 a v , (2)
total
− 2
2) RNN part of the architectures: For RNN models, we where ρ and ρ are the CCC (defined in eq.(1) ) for the
a v
experimented with Long Short Term Memories (LSTM), arousal and valence, respectively.
Gated Recurrent Units (GRU) and independent RNNs (in-
dRNN). We also tested using an attention layer on top of
IV. EXPERIMENTALSTUDY
the developed CNN-RNN models. In the VGGFACE case, A. Training implementation details
theRNNpartwasfedwiththeoutputofeitherthefirstfully
In order to train all deep neural architectures, we utilized
connected (FC) layer or the last pooling layer after global
the Adam optimizer algorithm and used exponential decay
average pooling. In all other CNN-RNN architectures, the
forthelearningrateinfixednumberofepochs;thebatchsize
RNN part was fed with the output of the last convolution or
pooling layer of the respective CNN part. We used 2 hidden 3https://ibug.doc.ic.ac.uk/resources/first-affect-wild-challenge/TABLEV
was set to to 320 (consisting of 4 different sequences, each
OBTAINEDCCCVALUESFORVALENCEANDAROUSALFORCNN-RNN
one having 80 consecutive frames), the attention length was
ARCHITECTURESTHATAREBASEDONTHERESNETANDUNDER
chosen to be 32, the initial learning rate was set to 0.001,
DIFFERENTSETTINGSDESCRIBEDINSECTIONIII-B.INPARENTHESIS
the decay steps for the learning rate were 400 and its decay
ARETHEPERFORMANCESOBTAINEDONTHEVALIDATIONSET.
factor was 0.97. The platform used for this implementation
was Tensorflow. Networks CCC
Valence Arousal
B. Experiments on Aff-Wild2 ResNet1-LSTM 0.44(0.47) 0.39(0.41)
ResNet1-GRU 0.45(0.49) 0.39(0.39)
In the experimental study, we evaluate the performance
ResNet1-indRNN 0.42(0.46) 0.38(0.39)
of all deep neural architectures described in Section III- ResNet2-LSTM 0.46(0.49) 0.38(0.42)
B. Table IV shows the CCC performance for estimation of ResNet2-GRU 0.50(0.53) 0.42(0.44)
valenceandarousalvalues,providedbytheabove-mentioned ResNet2-indRNN 0.46(0.50) 0.39(0.41)
ResNet12-LSTM 0.44(0.48) 0.39(0.41)
CNN-RNN networks in which: a) the CNN part is based
ResNet12-GRU 0.48(0.51) 0.40(0.43)
on the VGGFACE model and is pre-trained on either the ResNet12-indRNN 0.47(0.49) 0.36(0.39)
VGGFACE, or the VGGFACE2, or first the VGGFACE and
thentheVGGFACE2databases;b)theRNNpartuseseither TABLEVI
LSTM, or GRU, or indRNN neuron models. OBTAINEDCCCVALUESFORVALENCEANDAROUSALFORCNN-RNN
It should be mentioned that the case in which the output ARCHITECTURESTHATAREBASEDONTHEDENSENET.INPARENTHESIS
of the FC layer of VGGFACE is fed to the RNN had better ARETHEPERFORMANCESOBTAINEDONTHEVALIDATIONSET.
performance compared to the case where the RNN was fed
Networks CCC
with the output of the last pooling layer after global average Valence Arousal
pooling. We do not present the performance of the latter DenseNet-LSTM 0.43(0.48) 0.38(0.41)
DenseNet-GRU 0.48(0.51) 0.39(0.43)
case in order not to clutter the results. It can be seen that
DenseNet-indRNN 0.43(0.47) 0.37(0.41)
the VGGFACE1-GRU network (pretrained on VGGFACE
database) provided the best performance.
ResNet2-GRU and DenseNet-GRU networks with the same
TABLEIV
networks when they have an attention mechanism on top of
OBTAINEDCCCVALUESFORVALENCEANDAROUSALFORCNN-RNN
their GRU part. It can be easily seen that the VGGFACE1-
ARCHITECTURESTHATAREBASEDONTHEVGGFACEANDUNDER
GRU-attention network (that was pre-trained on VGGFACE
DIFFERENTSETTINGSDESCRIBEDINSECTIONIII-B.INPARENTHESIS
database and included an attention layer) had the best per-
ARETHEPERFORMANCESOBTAINEDONTHEVALIDATIONSET.
formance on the test set, providing a 0.55 value for valence
Networks CCC and 0.45 for arousal.
Valence Arousal
VGGFACE1-LSTM 0.44(0.49) 0.34(0.37) TABLEVII
VGGFACE1-GRU 0.52(0.55) 0.41(0.45)
OBTAINEDCCCVALUESBETWEENTHEBESTPERFORMINGCNN-RNN
VGGFACE1-indRNN 0.48(0.52) 0.35(0.39)
VGGFACE2-LSTM 0.42(0.47) 0.33(0.36) ARCHITECTURES’CONFIGURATIONSANDSAMEARCHITECTURESWITH
VGGFACE2-GRU 0.46(0.51) 0.37(0.40) ANATTENTIONLAYERONTOPOFTHEIRRNNPART.INPARENTHESIS
VGGFACE2-indRNN 0.44(0.48) 0.34(0.37) ARETHEPERFORMANCESOBTAINEDONTHEVALIDATIONSET.
VGGFACE12-LSTM 0.44(0.48) 0.33(0.37)
VGGFACE12-GRU 0.47(0.53) 0.35(0.42) Networks CCC
VGGFACE12-indRNN 0.46(0.51) 0.35(0.39) Valence Arousal
VGGFACE1-GRU 0.52(0.55) 0.41(0.45)
SimilarresultsareshowninTableV.Thedifference,here, ResNet2-GRU 0.50(0.53) 0.42(0.44)
DenseNet-GRU 0.48(0.51) 0.39(0.43)
is that the CNN part of the CNN-RNN architecture is based
VGGFACE1-GRU-attention 0.55(0.58) 0.45(0.48)
on the ResNet-50 model. It can be seen that the ResNet2- ResNet2-GRU-attention 0.53(0.57) 0.43(0.45)
GRUnetwork(pretrainedonVGGFACE2database)provided DenseNet-GRU-attention 0.52(0.57) 0.41(0.44)
the best performance.
The use of DenseNet-121 in the CNN part of the above-
C. Experiments on RECOLA
examined CNN-RNN architectures was also investigated.
Table VI compares the performance between models that Inthefollowing,weevaluatetheabilityofthedeepneural
their CNN part is based on the DenseNet-121 and the RNN architecture developed above, trained on the Aff-Wild2, to
is based on either LSTM, GRU or indRNN neuron models. adapt and provide highly accurate estimation of valence and
It can be seen that the DenseNet-GRU network provided the arousal in the RECOLA database.
best performance. Forthisreason,wefine-tunedthebestperformingnetwork
We then examined the improvement provided by adding on Aff-Wild2, namely the VGGFACE1-GRU-attention, on
an attention mechanism on top of the best performing theRECOLAandcompareditsperformancewiththeoneof
networks in the above three Tables. Table VII compares two other deep neural networks that have achieved state-of-
the performance of the best performing VGGFACE1-GRU, the-art performance in this database.The first network was an architecture comprised of a [4] R.Cowie,E.Douglas-Cowie,S.Savvidou*,E.McMahon,M.Sawey,
ResNet-50 and a 2-layer GRU stacked on top (let us call and M. Schro¨der. ’feeltrace’: An instrument for recording perceived
emotioninrealtime. InISCAtutorialandresearchworkshop(ITRW)
it ResNet-50-GRU network) as stated in [20]. The second
onspeechandemotion,2000.
was the best performing network in the original Aff-Wild [5] R.Cowie,G.McKeown,andE.Douglas-Cowie. Tracingemotion:an
database, namely AffWildNet, as stated in [17], [21]. overview. InternationalJournalofSyntheticEmotions(IJSE),3(1):1–
17,2012.
Table VIII shows the obtained results. It is clear that the [6] T.DalgleishandM.Power.Handbookofcognitionandemotion.John
performance-forbotharousalandvalenceestimation-ofthe Wiley&Sons,2000.
[7] E. Douglas-Cowie, R. Cowie, C. Cox, N. Amier, and D. K. Heylen.
fine-tuned VGGFACE1-GRU-attention architecture is higher
The sensitive artificial listner: an induction technique for generating
thantheperformanceoftheothertwonetworks.Itshouldbe emotionally coloured conversation. In LREC Workshop on Corpora
mentionedthatnoon-the-flyoroff-the-flydataaugmentation forResearchonEmotionandAffect.ELRA,2008.
[8] P.Ekman. Facialactioncodingsystem(facs). Ahumanface,2002.
[14], [15], or post-processing techniques, were conducted [9] R.Gross,I.Matthews,J.Cohn,T.Kanade,andS.Baker. Multi-pie.
when training these networks. ImageandVisionComputing,28(5):807–813,2010.
[10] K.He,X.Zhang,S.Ren,andJ.Sun.Deepresiduallearningforimage
recognition. In Proceedings of the IEEE Conference on Computer
TABLEVIII VisionandPatternRecognition,pages770–778,2016.
OBTAINEDCCCVALUESFORVALENCEANDAROUSALFORTHE [11] F. Iandola, M. Moskewicz, S. Karayev, R. Girshick, T. Darrell, and
K. Keutzer. Densenet: Implementing efficient convnet descriptor
FINE-TUNEDONRECOLA,BESTPERFORMINGONAFF-WILD2,
pyramids. arXivpreprintarXiv:1404.1869,2014.
VGGFACE1-GRU-ATTENTIONANDFORTHEFINE-TUNEDON [12] H.Jung,S.Lee,J.Yim,S.Park,andJ.Kim.Jointfine-tuningindeep
RECOLA,AFFWILDNETANDTHERESNET-GRUTRAINEDONTHE neural networks for facial expression recognition. In Proceedings of
theIEEEInternationalConferenceonComputerVision,pages2983–
RECOLA.
2991,2015.
[13] S. Koelstra, C. Muhl, M. Soleymani, J.-S. Lee, A. Yazdani,
CCC T. Ebrahimi, T. Pun, A. Nijholt, and I. Patras. Deap: A database
Valence Arousal foremotionanalysis;usingphysiologicalsignals. IEEETransactions
Fine-tunedVGGFACE1-GRU-attention 0.547 0.304 onAffectiveComputing,3(1):18–31,2012.
Fine-tunedAffWildNet[17],[21] 0.526 0.273 [14] D. Kollias, S. Cheng, M. Pantic, and S. Zafeiriou. Photorealistic
ResNet-GRU[20] 0.462 0.209 facial synthesis in the dimensional affect space. In Proceedings of
the European Conference on Computer Vision (ECCV), pages 0–0,
2018.
[15] D. Kollias, S. Cheng, E. Ververas, I. Kotsia, and S. Zafeiriou. Gen-
V. CONCLUSIONSANDFUTUREWORK erating faces for affect analysis. arXiv preprint arXiv:1811.05027,
2018.
The creation of a significant extension of the recently [16] D. Kollias, G. Marandianos, A. Raouzaiou, and A.-G. Stafylopatis.
Interweaving deep learning and semantic techniques for emotion
developedAff-Wilddatabase,thatmanagestoalmostdouble analysis in human-machine interaction. In 2015 10th International
both the included videos and video frames, as well as Workshop on Semantic and Social Media Adaptation and Personal-
the involved subjects, is presented in this paper. Various
ization(SMAP),pages1–6.IEEE,2015.
[17] D. Kollias, M. A. Nicolaou, I. Kotsia, G. Zhao, and S. Zafeiriou.
deep neural architectures, of the CNN-RNN type have been Recognition of affect in the wild using deep neural networks. In
developedandtestedontheAff-Wild2database.Bestperfor- ProceedingsoftheIEEEConferenceonComputerVisionandPattern
RecognitionWorkshops,pages26–33,2017.
mance, in valence and arousal estimation, has been obtained
[18] D.Kollias,A.Tagaris,andA.Stafylopatis. Onlineemotiondetection
using a VGGFace-GRU-attention architecture, which is a using retrainable deep neural networks. In 2016 IEEE Symposium
CNN-RNN network followed by an attention layer. SeriesonComputationalIntelligence(SSCI),pages1–8.IEEE,2016.
[19] D. Kollias, A. Tagaris, A. Stafylopatis, S. Kollias, and G. Tagaris.
This architecture, that was trained with a very rich Deep neural architectures for prediction in healthcare. Complex &
database, when fine-tuned on other existing emotion IntelligentSystems,4(2):119–131,2018.
[20] D. Kollias, P. Tzirakis, M. A. Nicolaou, A. Papaioannou, G. Zhao,
databases, such as RECOLA, has been shown to outperform
B.Schuller,I.Kotsia,andS.Zafeiriou. Deepaffectpredictionin-the-
state-of-the-art networks trained on this database. wild:Aff-wilddatabaseandchallenge,deeparchitectures,andbeyond,
Ourfutureworkincludesextendingtheexaminedemotion 2018.
[21] D. Kollias, P. Tzirakis, M. A. Nicolaou, A. Papaioannou, G. Zhao,
analysis framework, by including different data modalities,
B.Schuller,I.Kotsia,andS.Zafeiriou. Deepaffectpredictionin-the-
such as audio and text, different emotion representations, wild:Aff-wilddatabaseandchallenge,deeparchitectures,andbeyond.
suchasactionunitsandcomplexemotioncategories,anden-
InternationalJournalofComputerVision,127(6-7):907–929,2019.
[22] D. Kollias, M. Yu, A. Tagaris, G. Leontidis, A. Stafylopatis, and
sembles of deep neural networks that are capable of dealing S. Kollias. Adaptation and contextualization of deep neural network
with the different types of data and emotion representations. models. In 2017 IEEE Symposium Series on Computational Intelli-
gence(SSCI),pages1–8.IEEE,2017.
[23] D. Kollias and S. Zafeiriou. A multi-component cnn-rnn approach
REFERENCES for dimensional emotion recognition in-the-wild. arXiv preprint
arXiv:1805.01452,2018.
[1] Y. Avrithis, N. Tsapatsoulis, and S. Kollias. Broadcast news parsing [24] D. Kollias and S. Zafeiriou. A multi-task learning & generation
using visual cues: A robust face detection approach. In 2000 IEEE framework:Valence-arousal,actionunits&primaryexpressions.arXiv
International Conference on Multimedia and Expo. ICME2000. Pro- preprintarXiv:1811.07771,2018.
ceedings.LatestAdvancesintheFastChangingWorldofMultimedia [25] D. Kollias and S. Zafeiriou. Training deep neural networks with
(Cat.No.00TH8532),volume3,pages1469–1472.IEEE,2000. different datasets in-the-wild: The emotion recognition paradigm. In
[2] Q.Cao,L.Shen,W.Xie,O.M.Parkhi,andA.Zisserman. Vggface2: 2018 International Joint Conference on Neural Networks (IJCNN),
A dataset for recognising faces across pose and age. In Automatic pages1–8.IEEE,2018.
Face&GestureRecognition(FG2018),201813thIEEEInternational [26] D.KolliasandS.Zafeiriou. Exploitingmulti-cnnfeaturesincnn-rnn
Conferenceon,pages67–74.IEEE,2018. baseddimensionalemotionrecognitionontheomgin-the-wilddataset.
[3] R. Cowie and R. R. Cornelius. Describing the emotional states that arXivpreprintarXiv:1910.01417,2019.
areexpressedinspeech. Speechcommunication,40(1):5–32,2003. [27] D. Kollias and S. Zafeiriou. Expression, affect, action unit recog-nition: Aff-wild2, multi-task learning and arcface. arXiv preprint enceonIntelligentComputing,pages189–196.Springer,2015.
arXiv:1910.04855,2019.
[28] A.Lee. Welcometovirtualdub.org!-virtualdub.org,2002.
[29] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and
I. Matthews. The extended cohn-kanade dataset (ck+): A complete
datasetforactionunitandemotion-specifiedexpression. InComputer
Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE
ComputerSocietyConferenceon,pages94–101.IEEE,2010.
[30] S.MarsellaandJ.Gratch.Computationallymodelinghumanemotion.
CommunicationsoftheACM,57(12):56–67,2014.
[31] M. Mathias, R. Benenson, M. Pedersoli, and L. Van Gool. Face
detection without bells and whistles. In European Conference on
ComputerVision,pages720–735.Springer,2014.
[32] G. McKeown, M. Valstar, R. Cowie, M. Pantic, and M. Schro¨der.
Thesemainedatabase:Annotatedmultimodalrecordsofemotionally
coloredconversationsbetweenapersonandalimitedagent. Affective
Computing,IEEETransactionson,3(1):5–17,2012.
[33] M. Pantic, M. Valstar, R. Rademaker, and L. Maat. Web-based
databaseforfacialexpressionanalysis.InMultimediaandExpo,2005.
ICME 2005. IEEE International Conference on, pages 5–pp. IEEE,
2005.
[34] O.M.Parkhi,A.Vedaldi,andA.Zisserman. Deepfacerecognition.
InBMVC,volume1,page6,2015.
[35] R.Plutchik. Emotion:Apsychoevolutionarysynthesis. Harpercollins
CollegeDivision,1980.
[36] F.Ringeval,B.Schuller,M.Valstar,J.Gratch,R.Cowie,S.Scherer,
S.Mozgai,N.Cummins,M.Schmi,andM.Pantic. Avec2017–real-
lifedepression,andaectrecognitionworkshopandchallenge. 2017.
[37] F. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Introducing
the recola multimodal corpus of remote collaborative and affective
interactions. InAutomaticFaceandGestureRecognition(FG),2013
10th IEEE International Conference and Workshops on, pages 1–8.
IEEE,2013.
[38] J. A. Russell. Evidence of convergent validity on the dimensions of
affect. Journal of personality and social psychology, 36(10):1152,
1978.
[39] I.Sneddon,M.McRorie,G.McKeown,andJ.Hanratty. Thebelfast
induced natural emotion database. IEEE Transactions on Affective
Computing,3(1):32–41,2012.
[40] M.Soleymani,J.Lichtenauer,T.Pun,andM.Pantic. Amultimodal
databaseforaffectrecognitionandimplicittagging.IEEETransactions
onAffectiveComputing,3(1):42–55,2012.
[41] A.Tagaris,D.Kollias,andA.Stafylopatis. Assessmentofparkinsons
disease based on deep neural networks. In International Conference
on Engineering Applications of Neural Networks, pages 391–403.
Springer,2017.
[42] A. Tagaris, D. Kollias, A. Stafylopatis, G. Tagaris, and S. Kollias.
Machine learning for neurodegenerative disorder diagnosissurvey of
practicesandlaunchofbenchmarkdataset. InternationalJournalon
ArtificialIntelligenceTools,27(03):1850011,2018.
[43] Y.-l. Tian, T. Kanade, and J. F. Cohn. Recognizing action units for
facialexpressionanalysis. PatternAnalysisandMachineIntelligence,
IEEETransactionson,23(2):97–115,2001.
[44] M.ValstarandM.Pantic. Induceddisgust,happinessandsurprise:an
addition to the mmi facial expression database. In Proc. 3rd Intern.
Workshop on EMOTION (satellite of LREC): Corpora for Research
onEmotionandAffect,page65,2010.
[45] C. Whissel. The dictionary of affect in language, emotion: Theory,
research and experience: vol. 4, the measurement of emotions, r.
PlutchikandH.Kellerman,Eds.,NewYork:Academic,1989.
[46] L.Yin,X.Chen,Y.Sun,T.Worm,andM.Reale. Ahigh-resolution
3ddynamicfacialexpressiondatabase. InAutomaticFace&Gesture
Recognition, 2008. FG’08. 8th IEEE International Conference On,
pages1–6.IEEE,2008.
[47] L. Yin, X. Wei, Y. Sun, J. Wang, and M. J. Rosato. A 3d facial
expression database for facial behavior research. In Automatic face
andgesturerecognition,2006.FGR2006.7thinternationalconference
on,pages211–216.IEEE,2006.
[48] S. Zafeiriou, D. Kollias, M. A. Nicolaou, A. Papaioannou, G. Zhao,
andI.Kotsia. Aff-wild:Valenceandarousalin-the-wildchallenge. In
2017IEEEConferenceonComputerVisionandPatternRecognition
Workshops(CVPRW),pages1980–1987.IEEE,2017.
[49] S. Zafeiriou, D. Kollias, M. A. Nicolaou, A. Papaioannou, G. Zhao,
andI.Kotsia. Aff-wild:Valenceandarousal’in-the-wild’challenge. In
ProceedingsoftheIEEEConferenceonComputerVisionandPattern
RecognitionWorkshops,pages34–41,2017.
[50] Y. Zhou, H. Zhao, and X. Pan. Lie detection from speech analysis
basedonk–svddeepbeliefnetworkmodel. InInternationalConfer-"
25,25,Aff-wild: valence and arousal'In-the-Wild'challenge,"['S Zafeiriou', 'D Kollias', 'MA Nicolaou']",2017,373,Expression in-the-Wild,"deep learning, machine learning, neural network",”inthe-wild”. For a recent survey on facial behaviour analysis in-the-wild with an emphasis on  deep learning  come from movies and the annotation is limited to the universal expressions.,No DOI,Proceedings of the …,https://ieeexplore.ieee.org/document/8014982/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
26,26,"Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework","['D Kollias', 'S Zafeiriou']",2021,206,Expression in-the-Wild,"classification, deep learning","databases and ii) design and training of novel deep neural architectures  in-the-wild databases,  ie, Aff-Wild and Aff-Wild2 and presents the design of two classes of deep neural networks",No DOI,arXiv preprint arXiv:2103.15792,https://arxiv.org/abs/2103.15792,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"1
Affect Analysis in-the-wild: Valence-Arousal,
Expressions, Action Units and a Unified Framework
Dimitrios Kollias, Member, IEEE, Stefanos Zafeiriou
Abstract
Affect recognition based on subjects’ facial expressions has been a topic of major research in the attempt to
generate machines that can understand the way subjects feel, act and react. In the past, due to the unavailability
of large amounts of data captured in real-life situations, research has mainly focused on controlled environments.
However, recently, social media and platforms have been widely used. Moreover, deep learning has emerged as a
meanstosolvevisualanalysisandrecognitionproblems.Thispaperexploitstheseadvancesandpresentssignificant
contributions for affect analysis and recognition in-the-wild. Affect analysis and recognition can be seen as a
dual knowledge generation problem, involving: i) creation of new, large and rich in-the-wild databases and ii)
design and training of novel deep neural architectures that are able to analyse affect over these databases and to
successfully generalise their performance on other datasets. The paper focuses on large in-the-wild databases, i.e.,
Aff-WildandAff-Wild2andpresentsthedesignoftwoclassesofdeepneuralnetworkstrainedwiththesedatabases.
The first class refers to uni-task affect recognition, focusing on prediction of the valence and arousal dimensional
variables.Thesecondclassreferstoestimationofallmainbehaviortasks,i.e.valence-arousalprediction;categorical
emotionclassificationinsevenbasicfacialexpressions;facialActionUnitdetection.Anovelmulti-taskandholistic
framework is presented which is able to jointly learn and effectively generalize and perform affect recognition over
all existing in-the-wild databases. Large experimental studies illustrate the achieved performance improvement over
the existing state-of-the-art in affect recognition.
Index Terms
Affect recognition in-the-wild, categorical model, basic emotions, dimensional model, valence, arousal, action
units, Aff-Wild, Aff-Wild2, deep neural networks, AffWildNet, multi-component architecture, multi-task learning,
holistic learning, FaceBehaviorNet, unified affect recognition
I. INTRODUCTION
THIS paper presents recent developments and research directions in affective behavior analysis in-the-wild,
which is a major targeted characteristic of human computer interaction systems in real life applications. Such
systems, machines and robots, should be able to automatically sense and interpret facial and audio-visual signals
relevant to emotions, appraisals and intentions; thus, being able to interact in a ’human-centered’ and engaging
manner with people, as their digital assistants in the home, work, operational or industrial environment.
Through human affect recognition, the reactions of the machine, or robot, will be consistent with people’s
expectationsandemotions;theirverbalandnon-verbalinteractionswillbepositivelyreceivedbyhumans.Moreover,
this interaction should not be dependent on the respective context, nor the human’s age, sex, ethnicity, educational
level, profession, or social position. As a consequence, the development of intelligent systems able to analyze
human behavior in-the-wild can contribute to generation of trust, understanding and closeness between humans and
machines in real life environments.
In this paper we mainly focus on facial affect analysis, which constitutes a difficult problem, because emotion
patterns in faces are complex, time varying, user and context dependent. We deal with all three main emotion
models, i.e., facial expressions and categorical affect, facial action units and dimensional affect representations.
Representinghumanemotionshasbeenabasictopicofresearchinpsychology.Themostfrequentlyusedemotion
representation is the categorical one, including the seven basic categories, i.e., Anger, Disgust, Fear, Happiness,
Sadness, Surprise and Neutral [1]. Discrete emotion representation can also be described in terms of the Facial
D. Kollias is with the School of Computing and Mathematical Sciences, University of Greenwich, London, UK, e-mail:
D.Kollias@greenwich.ac.uk
S. Zafeiriou is with the Department of Computing, Imperial College London, UK, e-mail: s.zafeiriou@imperial.ac.uk
1202
raM
92
]VC.sc[
1v29751.3012:viXra2
Action Coding System (FACS) model, in which all possible facial actions are described in terms of Action Units
(AUs) [1]. Finally, the dimensional model of affect [2], [3] has been proposed as a means to distinguish between
subtly different displays of affect and encode small changes in the intensity of each emotion on a continuous scale.
The 2-D Valence and Arousal Space (VA-Space) is the most usual dimensional emotion representation; valence
shows how positive or negative an emotional state is, whilst arousal shows how passive or active it is.
Emotion recognition is a very important topic in human computer interaction [4]. Furthermore, since real-world
settings entail uncontrolled conditions, where subjects operate in a diversity of contexts and environments, systems
that perform automatic analysis of human behavior and emotion recognition should be robust to video recording
conditions, diversity of contexts and timing of display. These goals are scientifically and technically challenging.
We wouldn’t be able to create emotion recognition systems [5] that work on real life situations if we did not have
large emotion databases that simulate behaviors in-the-wild.
Researchinfaceperceptionandemotionanalysishas,therefore,beengreatlyassistedbythecreationofdatabases
[6] of images and video sequences annotated in terms of persons’ behaviors, their facial expressions and underlying
emotions. To this end, some in-the-wild datasets have been generated and used for recognition of facial expressions,
for detection of facial action units and for estimation of valence and arousal. However: i) their size is small, ii)
they are not audiovisual, iii) only a small part of them is manually annotated, iv) they contain a small number of
subjects, v) they are not annotated for all the tasks. In this paper, we focus and use a class of new in-the-wild
databases, Aff-Wild [7] [8] and Aff-Wild2 [9], [10] that overcome the above limitations.
Current state-of-the-art affect recognition methods are generally based on Deep Neural Networks (DNNs) [11]
that are able to analyze large amounts of data and recognise persons’ emotional states. In this paper we present
new DNN systems that can be trained with the above-mentioned large in-the-wild databases and improve the
state-of-the-art in affect recognition, considering either the dimensional, categorical, or AU based emotion models.
Furthermore, we develop multitask architectures that can jointly learn to recognize all three emotion representa-
tions in-the-wild. We present a novel holistic approach that achieves excellent performance, by coupling all three
recognition tasks during training [12]. To achieve this, we utilise all publicly available datasets (including over 5
million images) that study facial behavior tasks in-the-wild.
The rest of the paper is organized as follows. Section II briefly describes the three models of affect that are
adopted and used for the development of affect recognition methods. Section III presents the existing databases
and their properties, focusing on the Aff-Wild and Aff-Wild2 databases. The signal pr-processing steps and the
performance evaluation criteria are described in Section IV. Section V examines uni-task affect analysis, focusing
on dimensional affect; it presents single-component and multiple-component architectures which improve the state-
of-the-art in dimensional affect recognition. The unified affect analysis methodology, over all three emotion models,
is described in Section VI. It first presents the multitask affect analysis approach and then the unifying holistic
approach. Section VII presents the conclusions and further prospects of the presented approaches.
II. MODELS OF AFFECT
A. Categorical Affect
Ekman defined the six basic emotions, i.e., Anger, Disgust, Fear, Happiness, Sadness, Surprise and the neutral
state, based on a cross-culture study [1], which indicated that humans perceive certain basic emotions in the same
way regardless of culture. Nevertheless, advanced research on neuroscience and psychology argued that the model
of six basic emotions are culture-specific and not universal. Additionally, the affect model based on basic emotions
is limited in the ability to represent the complexity and subtlety of our daily affective displays. Despite these
findings, the categorical model that describes emotions in terms of discrete basic emotions is still the most popular
perspective for Facial Expression Recognition (FER), due to its pioneering investigations along with the direct and
intuitive definition of facial expressions.
B. Action Units
Detection of Facial Action Units (AUs) has also attained large attention. The Facial Action Coding System
(FACS) [1] provides a standardised taxonomy of facial muscles’ movements and has been widely adopted as a
common standard towards systematically categorising physical manifestation of complex facial expressions. Since
any facial expression can be represented as a combination of action units, they constitute a natural physiological3
Fig. 1: The 2D Valence-Arousal Space & Some facial Action Units
basis for face analysis. The existence of such a basis is a rare boon for computer vision, as it allows focusing
on the essential atoms of the problem and, by virtue of their exponentially large possible combinations, opens the
door for studying a wide range of applications beyond prototypical emotion classification. Consequently, in the last
years, there has been a shift of related research towards the detection of action units. The presence of action units
is typically brief and unconscious, and their detection requires analyzing subtle appearance changes in the human
face. Furthermore, action units do not appear in isolation, but as elemental units of facial expressions, and hence
some AUs co-occur frequently, while others are mutually exclusive. Fig. 1 shows the most common action units
and the corresponding facial action movement that defines them.
C. Dimensional Affect
The dimensional model of affect, which is appropriate to represent not only extreme, but also subtle emotions
appearing in everyday human-computer interactions, has also attracted significant attention over the last years.
According to the dimensional approach [3] [2], affective behavior is described by a number of latent continuous
dimensions.Themostcommonlyuseddimensionsincludevalence(indicatinghowpositiveornegativeanemotional
state is) and arousal (measuring the power of emotion activation). Valence and arousal readily relate to specific
functions of regions of the brain; the parietal region of the right hemisphere appears to play a special role in the
mediation of arousal, whereas the frontal regions appear to play a special role in emotional valence. Fig. 1 shows
the 2D Valence-Arousal Space, in which the horizontal axis denotes valence that ranges from very positive to very
negative and the vertical one denotes arousal that ranges from very active to very passive.
III. EXISTING DATASETS WITH AFFECT ANNOTATION
Current research in automatic analysis of facial affect aims at developing systems, such as robots and virtual
humans, that will interact with humans in a naturalistic way under real-world settings. To this end, such systems
should automatically sense and interpret facial signals relevant to emotions, appraisals and intentions. Moreover,
since real-world settings entail uncontrolled conditions, where subjects operate in a diversity of contexts and
environments, systems that perform automatic analysis of human behavior should be robust to video recording
conditions, the diversity of contexts and the timing of display.
For the past twenty years research in automatic analysis of facial behavior was mainly limited to posed behaviors
which were captured in highly controlled recording conditions. Nevertheless, it is now accepted by the community4
TABLE I: Existing Databases along with their properties; ’static’ means images, ’dynamic’ means image sequences
(video without audio), ’A/V’ means audiovisual sequences (video with audio)
DBs DB Type Model of Affect Condition DB Size
RAF-DB [17] static 6 Basic, Neutral & 11 Compound in-the-wild 15,339 & 3,954
6 Basic, Neutral + Contempt 291,651 manual & 400,000 automatic annotations
AffectNet [16] static in-the-wild
valence-arousal 325,000 manual & 460,300 automatic annotations
DISFA [13] dynamic 12 action units controlled 54 videos: 261,630 frames
BP4DS [14] dynamic 27 action units controlled 1,640 videos: 222,573 frames
BP4D+ [15] dynamic 34 action units controlled 5,463 videos: 967,570 frames
EmotioNet [18] static 11 action units in-the-wild 50,000 manual & 950,000 automatic annotations
AFEW-VA [21] dynamic valence-arousal in-the-wild 600 videos: 30,050 frames
SEWA [19] A/V valence-arousal in-the-wild 538 videos
OMG-Emotion [20] A/V valence-arousal in-the-wild 495 videos: 5,288 utterances
that facial expressions of naturalistic behaviors can be radically different from posed ones. Hence, efforts have
been made in order to collect subjects displaying naturalistic behavior. Examples include the DISFA [13], BP4D-
Spontaneous (BP4DS) [14], BP4D+ [15] databases.
However,withthedevelopmentoflargeanddiversedatasetsinthefieldofcomputervision(andtheaccompanying
performance gains), it has become apparent that the diversity of human participants and spontaneous expressions
have to become the prerogatives in deployment of the affective computing models in practice. Hence, it is now
widely accepted, in both the computer vision and machine learning communities, that progress in a particular
application domain is significantly catalyzed when large datasets are collected in unconstrained conditions (also
referred as ""in-the-wild"" data). Therefore, facial analysis could not only focus on spontaneous behaviors, but also
on behaviors captured in unconstrained conditions.
Some datasets with in-the-wild settings have been recently collected to study: i) facial expression analysis, such
as the static AffectNet [16] and the static RAF-DB [17]; ii) facial action units, such as the static EmotioNet [18];
and iii) continuous emotions of valence and arousal in-the-wild, such as the audiovisual SEWA [19], the audiovisual
OMG-Emotion [20], the static AffectNet [16] and the static AFEW-VA [21] datasets. Let us note that the term
’static’meansthatthedatasetcontainsonly(static)images,neithervideonoraudio.TableIshowssomein-the-wild,
databases that exist in literature (and are being utilised in our experiments in the next Sections) and are annotated
in terms of either facial expressions, or action units, or valence-arousal. The in-the-wild databases are captured
under different illumination conditions in uncluttered backgrounds and contexts, in which people have different
head poses and there exist occlusions in the facial area.
Aff-Wild database Back in 2017, there existed some databases for dimensional emotion recognition. However,
theywerecapturedinlaboratorysettingsandnotin-the-wild.Thisurgedustocollectthefirstlargescalein-the-wild
databaseandannotateitintermsofvalenceandarousal.Todoso,wecapitalizedontheabundanceofdataavailable
in video-sharing websites, such as YouTube and selected videos that display the affective behavior of people.
To this end we collected 298 videos displaying reactions of 200 subjects, with a total video duration of more than
30 hours, consisting of 1,224,100 total number of frames. This database has been annotated by 8 lay experts in a
per-frame basis with regard to valence and arousal; the annotated values were continuous and ranged in [-1,1]. We
organised the Aff-Wild Challenge based on the Aff-Wild database, in conjunction with CVPR 2017. Aff-Wild was
developed as the first large in-the-wild database - with a big variety of: (1) emotional states, (2) rapid emotional
changes, (3) ethnicities, (4) head poses, (5) illumination conditions and (6) occlusions - used for affect recognition.
Aff-Wild2 database Up to the present, there was no database that contain annotations for all main behavior
tasks (valence-arousal estimation, action unit detection, expression classification). Most of the existing databases
contain annotations for only one task (AffectNet is the exception, that contains annotations for two tasks). Also
the existing corpora have a number of other limitations; just to name a few: non in-the-wild nature; small total
number of annotations (making it impossible to train deep neural networks and generalise to other databases);
automatic or semi-automatic annotation (which is error prone and makes the annotations noisy); small number of
expert annotators (making the annotations biased).
This urged and led us to create the Aff-Wild2 database; the first and only database annotated in terms of valence
and arousal, action units and expressions. Aff-Wild2 is a significant extension of Aff-Wild, through augmentation5
with 260 more YouTube videosof a total duration of 13 hours and 5 minutes. The new videos have wide range
in subjects’: age (from babies to elderly people); ethnicity (caucasian/hispanic/latino/asian/black/african american);
profession (e.g. actors, athletes, politicians, journalists); head pose; illumination conditions; occlusions; emotions.
Intotal,Aff-Wild2consistsof558videosof458subjects,witharound2,800,000frames,showingbothsubtleand
extremehumanbehaviorsinreal-worldsettings.Fourexpertsannotatedthedatabaseintermsofvalenceandarousal;
three very experienced annotators annotated 63 videos, with 398,835 frames in terms of AUs 1,2,4,6,12,15,20,25;
seven experts annotated 539 videos consisting of 2,595,572 frames in terms of the 7 basic expressions. All
annotations have been performed in a frame-by-frame basis. Finally, we organised the Affective Behavior Analysis
in-the-wild (ABAW) Competition that utilised the Aff-Wild2 database, in conjunction with IEEE International
Conference on Automatic Face and Gesture Recognition (FG) 2020.
IV. INTELLIGENT SIGNAL PRE-PROCESSING & PERFORMANCE EVALUATION METRICS
Data pre-processing consists of the steps required for facilitating extraction of meaningful features from the data.
For the visual modality, the usual steps are face detection and alignment, image resizing and image normalization.
The pre-processing steps described below have been utilized in all developments presented in this paper. Let us
note that no data augmentation, either on-the-fly [22]–[24] or off-the-fly has been performed.
TheSSHdetector[25]basedonResNetwasusedtoextractfaceboundingboxesinallimages.Forfacealignment,
at first we extracted facial landmarks. Facial landmarks are defined as distinctive face locations, such as corners of
the eyes, centre of the bottom lip, tip of the nose. If they are aggregated in sufficient numbers, they can effectively
describe the face shape. In our implementations, we used the facial landmark detector in the dlib library to locate
68 facial landmarks in all frames. We focused on 5 of them - corresponding to the location of the left eye, right
eye, nose and mouth in a prototypical frontal face -as rigid, anchor points. Then, for every frame, we extracted the
respective5faciallandmarksandcomputedtheaffinitytransformationbetweenthecoordinatesofthese5landmarks
and the coordinates of the 5 landmarks of the frontal face; we imposed next this transformation to the whole new
frame to perform the alignment. All cropped and aligned images were then resized to 96×96×3 pixel resolution
and their intensity values were normalized to the range [−1,1].
In some experiments, next in this paper, the audio modality was also utilized. In these cases the audio (mono)
signal was sampled at 44,100Hz. Then spectrograms were extracted; spectrogram frames were computed over a
33ms window with 11ms overlap. The resulting intensity values were normalized in [−1,1] to be consistent with
the visual modality.
Regarding evaluation, the metric used for measuring models’ performance on valence and arousal estimation
is the Concordance Correlation Coefficient (CCC). CCC evaluates the agreement between two time series (e.g.,
annotations and predictions) by scaling their correlation coefficient with their mean square difference. In this way,
predictions that are well correlated with the annotations, but shifted in value, are penalized in proportion to the
deviation. CCC takes values in the range [−1,1], where +1 indicates perfect concordance and −1 denotes perfect
discordance. The highest the value of the CCC the better the fit between annotations and predictions, and therefore
high values are desired. CCC is defined as follows:
2s 2s s ρ
xy x y xy
ρ = = , (1)
c s2 +s2+(x¯−y¯)2 s2 +s2+(x¯−y¯)2
x y x y
where ρ is the Pearson Correlation Coefficient, s and s are the variances of all valence/arousal annotations and
xy x y
predicted values, x¯ and y¯ are the corresponding mean values and s is the corresponding covariance value.
xy
The evaluation metric used for measuring models’ performance on 7 basic expression classification is either
the F1 Score (harmonic mean of precision and recall), or the mean diagonal value of the confusion matrix. The
evaluation metric used for measuring models’ performance on action unit detection is either the F1 Score, or an
average of the F1 Score and the Accuracy metrics.
V. UNI-TASK AFFECT ANALYSIS: DIMENSIONAL AFFECT RECOGNITION
In this Section we treat affect recognition as Uni-task, i.e., with reference to the adopted emotion model. In
particular, we adopt the dimensional emotion model. Adopting the expression, or the AU model, can be similarly
treated [26], [27].6
A. Single-Component Architectures: CNN plus RNN network
ByutilisingtheAff-Wilddatabase,wedevelopedtheAffWildNetnetworkthatsuccessfullycapturedthedynamics
and the in-the-wild nature of the database, providing the best performance over Aff-Wild.
Atfirst,AffWildNetisaCNN-RNNnetwork.TheCNNpartisbasedontheVGG-FACE,orResNet-50network’s
convolutional and pooling layers. Since low- and middle-level facial features are common in both face and facial
affect recognition, we adopted the VGG-FACE network, pre-trained with a large human faces’ dataset for face
recognition. The outputs of the last pooling layer of this CNN part, concatenated with the facial landmarks, are fed
to a Fully Connected (FC) layer with 4096 or 1500 hidden units (depending on whether VGG-FACE or ResNet-50
is used). This FC layer has the role to map its two types of inputs to the same feature space, before forwarding
them to the RNN part. The facial landmarks, which are provided as additional input to the network, in this way,
are able to boost the performance of the main CNN part of our model. The output of the FC layer is then fed to
the RNN part. The RNN is used in order to model the contextual information in the data, taking into account the
temporal variations. The RNN is a 2-layered GRU with 128 units in each layer; the first layer processes the FC
layer outputs; the second layer is followed by the output layer that gives the final estimates of valence and arousal.
GRU units have been chosen instead of LSTM ones as they are less complex, more efficient and - as shown in the
experimental evaluation - provide best results.
Three novel characteristics of AffWildNet mainly contribute to its achieving state-of-the-art performance. First,
the fusion of facial landmarks and features extracted from the CNN part, provides the higher part of the network
with the ability to fuse the extracted by the network representations with these robust ’anchor’ features. Next, the
adopted loss function was based on the powerful Concordance Correlation Coefficient (CCC), which had also been
the main evaluation criterion used in the Aff-Wild Challenge. In particular, the defined loss function was:
L = 1−0.5×(ρ +ρ ), (2)
total a v
where ρ and ρ is the CCC for arousal and valence, respectively.
a v
Furthermore, the AffWildNet was trained as an end-to-end architecture, by jointly training its CNN and RNN
parts; in contrast to previous works which used to either train separately the CNN and the RNN parts, or used fixed
pre-trained weights for the CNN and trained only the RNN part.
Training implementation details: Adam optimizer; batch size of 4 and sequence length of 80; initial learning rate
of 0.0001, exponentially decaying after 10 epochs; dropout probability value of 0.5.
AffWildNet Performance Evaluation
Next, we illustrate the performance of AffWildNet, comparing it to the performance of other CNN and standard
CNN-RNNarchitectures,aswellastothewinneroftheAff-WildChallenge.FortheCNNarchitectures,weconsider
the ResNet-50, VGG-16 and VGG-FACE networks. For the CNN-RNN architectures, we consider a VGG-FACE-
LSTM,inwhichtheLSTMisa2-layeredRNNfedwiththeoutputsofthefirstfullyconnectedlayerofVGG-FACE.
Table II summarizes the CCC and MSE values obtained when applying all the above architectures, to the Aff-
Wild test set. It shows the improvement in CCC and MSE values obtained when using the AffWildNet compared to
all other developed architectures. This improvement clearly indicates the ability of the AffWildNet to better capture
the dynamics in Aff-Wild. Table II also compares the performance of AffWildNet to that of the FATAUVA-NET,
which was the winner of the Aff-Wild Challenge. AffWildNet outperformed this network, as well.
Additionally, it can be shown that AffWildNet successfully generalizes its knowledge when used in other emotion
recognition datasets and contexts. By learning complex and emotionally rich features of the AffWild database,
AffWildNet constitutes a robust prior for both dimensional and categorical emotion recognition. It is the first time
that such a state-of-the-art performance has been achieved. We refer the interested reader to [27].
B. Multi-Component Architectures: CNN plus Multi-RNN networks
Next we present novel multi-component architectures that are able to achieve excellent performance in emotion
recognition. This is illustrated through dimensional affect analysis over the One-Minute-Gradual Emotion (OMG-
Emotion) Dataset, when using visual information. It should be mentioned that the submissions we made to the
OMG-Emotion Challenge were ranked at second position for valence estimation [28], [29].7
TABLE II: CCC and MSE based evaluation of valence & arousal predictions provided by AffWildNet, FATAUVA-
Net (the winner of Aff-Wild Challange) and other state-of-the-art networks. A higher CCC and a lower MSE value
indicate a better performance.
CCC MSE
Valence Arousal Mean Value Valence Arousal Mean Value
FATAUVA-Net 0.40 0.28 0.34 0.12 0.10 0.11
VGG-16 0.40 0.30 0.35 0.13 0.11 0.12
ResNet-50 0.43 0.30 0.37 0.11 0.11 0.11
VGG-FACE 0.51 0.33 0.42 0.10 0.08 0.09
VGG-FACE-LSTM 0.52 0.38 0.45 0.10 0.09 0.10
AffWildNet 0.57 0.43 0.50 0.08 0.06 0.07
Ingeneral,featuresextractedfromCNNlowerlayerscontainrich,completeandtimevaryinginformation,whilst,
high-level features extracted from CNN higher layers, are more specific and characteristic of the studied problem.
Taking this into account, we have developed CNN plus Multi-RNN networks; these networks extract low-, mid-
and high- level features from different layers of the CNN and feed them as inputs to multiple RNNs. The best
performing networks fall into two different types, based on the adopted methodology: the first, referred as CNN-
3RNN, feeds the features extracted from three CNN layers to three respective RNN subnets, whereas the other,
referred as CNN-1RNN, concatenates these features and processes them through a single RNN net. An ensemble
methodology, involving both types of networks is also described.
1) CNN-3RNN networks
The CNN-3RNN networks include, first, the convolutional and pooling layers of VGG-FACE, followed by a fully
connected layer of 4096 units. The 68 facial landmarks are concatenated with the features extracted from the last
pooling layer of VGG-FACE and are fed to this FC layer. Low-, mid- and high-level feature sets are extracted from
this network and each set is fed to a 2-layer RNN (GRU) network that provides an estimate of the targeted valence
and arousal values. Each RNN layer comprises 128 GRU units. The CNN-3RNN networks are provided with an
input sequence of frames (and the corresponding landmarks of each frame) and predict, for each frame, estimates
of the valence-arousal values. Their median constitute the final estimates.
Fig.2 (on the right hand side) presents such a CNN-3RNN network, named CNN-3RNN-2nd-pool_last-pool_fc.
In this network: i) the features extracted from the FC layer are fed, as input, to a RNN network, denoted RNN ; ii)
1
the features extracted from the last pooling layer (before being concatenated with the landmarks) are fed, as input,
to a second RNN network, denoted RNN ; iii) the features extracted from the second pooling layer (following
2
the fourth convolutional layer) are fed, as input, to another RNN network, denoted RNN . Fig.2 depicts the exact
3
structure of the afore-mentioned RNN, i ∈ {1,2,3}, networks. All networks have the same structure, i.e., a 2-layer
i
GRU network, with each layer having 128 units. Next, the outputs of the 3 RNNs are concatenated and fed to the
output layer that performs the valence-arousal prediction.
2) CNN-1RNN networks
The CNN-1RNN type of networks also consists of the convolutional and pooling layers of VGG-FACE, followed by
a FC layer of 4096 units. The 68 facial landmarks are concatenated with the features extracted from the last pooling
layer of VGG-FACE and are fed to the FC layer. Low-, mid- and high-level features are extracted, concatenated
and fed to a single 2-layer GRU that predicts the valence and arousal values. Each GRU layer comprises 128 units.
The CNN-1RNN networks are also provided with an input sequence of frames (and the corresponding landmarks
of each frame), predicting, for each frame, the valence-arousal values; their median values are the final estimates.
Fig.2 (on the left hand side) presents such a CNN-1RNN network, named CNN-1RNN-2nd-pool_last-pool_fc.
In this network, the features extracted from: i) the second pooling layer (following the 4th convolutional), ii) the
last pooling layer (following the 13th convolutional and before being concatenated with the landmarks) and iii) the
FC layer are concatenated and fed to the RNN.
3) Ensemble Methodology
Next we describe an ensemble approach which fuses the above-described networks, either at Model-level, or at
Decision-level. Model-level fusion is based on concatenating the high level features extracted by the different8
Fig. 2: The CNN-1RNN-2nd-pool_last-pool_fc architecture (on the left hand side). The CNN-3RNN-2nd-pool_last-
pool_fc (on the right hand side). The latter architecture provided the best results. Each architecture provides a
valence-arousal (V-A) estimate per input sequence of consecutive frames. The ’68 landmarks’ are concatenated
with the features of the last ’pool’ layer and passed as input to the ’fc’ layer.
networks, while Decision-level fusion is based on computing a weighted average of the predictions [30] provided
by the different networks. On the one hand side, Model-level fusion takes advantage of the mutual information in
the data. On the other hand side, the averaging procedure in Decision-level fusion reduces variance in the ensemble
regressor (thus achieving higher robustness), while preserving the relative importance of each individual model.
- Model-level Fusion: Let us consider the CNN-1RNN and CNN-3RNN networks described above. We concatenate
the outputs of all the RNNs in the above networks and provide them, as input, either: i) to another single RNN9
layer with 128 GRU units, or ii) to a fully connected layer with 128 units; the output layer follows. We denote the
resulting networks as Model-level Fusion + RNN and Model-level Fusion + FC, respectively. For each frame in the
input sequence of frames, this model-level fusion network predicts the valence-arousal values and then computes
their median values as final estimates.
- Decision-level Fusion: Let us consider the CNN-1RNN and CNN-3RNN networks. The final valence (arousal)
estimateOdec.−level (Odec.−level), is computed as a weighted average of the final valence (arousal) estimates, on(on),
v a v a
of these networks; each weight is proportional to the corresponding network performance on the validation set:
O dec.−level = 1 ∑ tn·on, (3)
i ∑t in
n
i i
n
where i ∈ {v,a} (v stands for valence, a for arousal), tn is equal to Concordance Correlation Coefficient (defined
i
in eq. 1), for valence or arousal, computed on the validation set, with n denoting the CNN-1RNN or CNN-3RNN
network.
The presented multi-component networks differ from networks that either: i) use standard CNN-RNNs in which
the output of the CNN is fed to the RNN, or ii) apply ensemble methodologies, using features extracted from
many CNNs (but not using features from multiple layers of the same network) and fusing them. Additionally, in
model-level fusion, our approach performs fusion through an RNN instead of a typical FC layer.
In addition, we performed adaptation [31] of the developed architectures to the specific OMG-Emotion dataset
characteristics and in particular to the dataset’s annotation at utterance level. To deal with this, we split each
utterance into sequences, which were individually processed by the above architectures. The median values of the
predicted valence-arousal values were first computed at sequence level. Then, the median values were averaged at
utterance level so as to provide the final valence and arousal estimates. This procedure deviates from related works
that uniformly (or randomly) sample a constant number of frames from each utterance, assign to each of them the
annotation value of the utterance and compute the prediction per frame.
Furthermore, we should mention the pre-training of the proposed architectures with the large-scale emotionally
rich Aff-Wild2 database. Other works use networks that are not pre-trained on the same task (i.e., valence-arousal
estimation), but on other tasks (face recognition, object detection). The pre-training on Aff-Wild2 helped our
developed architectures achieve excellent performance.
Training implementation details: Adam optimizer; loss of eq. 2; end-to-end training with a learning rate of either
10−4 or 10−5; batch size of 4 and sequence length of 80; dropout with 0.5 probability value in fully connected
layers and dropout with 0.8 probability value in the first GRU layer of the RNNs.
Finally, for all developed architectures, a chain of post-processing steps was applied. These steps included: i)
median filtering of the - per frame - predictions within each sequence and ii) smoothing of the - per utterance -
predictions. Any of these post-processing steps was kept when an improvement was observed on the CCC over the
validation set, and applied then, with the same configuration to the test partition.
Multi-Component Network Performance Evaluation
Next,wecomparetheCNNplusMulti-RNNarchitecturestothestate-of-the-artCNNnetwork,ResNet-50,andits
CNN plus RNN counterpart, ResNet-RNN, as well as the state-of-the-art methods submitted to the OMG-Emotion
Challenge. Table III illustrates the performance of all these models.
OnecannotethatbothCNN-1RNN-2nd-pool_last-pool_fcandCNN-3RNN-2nd-pool_last-pool_fcexhibitamuch
improved performance (around 6% and 9% on average) when compared to ResNet-RNN. This validates our essence
that low-level CNN features together with high-level ones provide useful information for our task. Additionally,
CNN-3RNN-2nd-pool_last-pool_fc outperformed CNN-1RNN-2nd-pool_last-pool_fc showing that it is better to
exploit the low- and high-level features’ time variations via RNNs, independently, and then concatenate them,
rather than concatenate them first and process them through the use of a single RNN.
Table III validates that using the ensemble methodology is better than using a single network. This is because
different networks produce quite different features; fusing them exploits all these representations that include rich
information. It can also be observed that Model-level fusion has a superior performance compared to that of the
Decision-level one, since the features from different networks that are concatenated, contain richer information
about the raw data than the final decision. In particular, in Model-level fusion, we concatenate these features and10
feed them to an RNN and the whole ensemble is trained end-to-end and optimised so that the concatenation of
features can provide the best overall result. Moreover, in Model-level fusion, a better performance is achieved
when a RNN, instead of a fully connected layer, is used for the fusion. Table III also shows that our Model-level
Fusion + RNN method outperforms, on both valence and arousal estimation, the winning methodologies [15] of
the OMG-Emotion Challenge, which have been trained additionally with the audio modality.
Another observation is that the performance of models that only used the visual modality in arousal estimation
was worse than their performance in valence estimation. This was expected because, for arousal estimation, the
audio cues appear to include more discriminating capabilities than facial features in terms of correlation coefficient.
TABLE III: CCC based evaluation of valence and arousal predictions provided by the developed CNN plus Multi-
RNN architectures, the state-of-the-art CNN and CNN plus RNN architectures and the winning methodologies of
the OMG-Emotion Challenge. A higher CCC value indicates a better performance.
Methods Modality CCC
Valence Arousal
ResNet-50 V,A: visual 0.359 0.195
ResNet-RNN V,A: visual 0.409 0.224
Single Multi-Modal [15] V,A: audio + visual 0.484 0.345
Ensemble I [15] V,A: audio + visual 0.496 0.356
Ensemble II [15] V,A: audio + visual 0.499 0.361
CNN-1RNN-2nd-pool_last-pool_fc V,A: visual 0.449 0.303
CNN-3RNN-2nd-pool_last-pool_fc V,A: visual 0.472 0.329
Decision-Level Fusion V,A: visual 0.501 0.332
Model-Level Fusion + FC V,A: visual 0.518 0.348
Model-Level Fusion + RNN V,A: visual 0.535 0.365
VI. UNIFIED AFFECT ANALYSIS
A. A Multi-Task Approach to Affect Recognition
As has previously be mentioned, there are three main behavior tasks: i) valence-arousal estimation, ii) action unit
detection and iii) basic expression classification. Up to the present, these three tasks have been generally tackled
individually from each other, despite the fact that they are interconnected. Multi-task learning (MTL) is an approach
that can be used to jointly learn all three behavior analysis tasks. MTL was first studied to jointly learn parallel
tasks that share a common representation and to transfer part of the knowledge - learned to solve one task - to
improve learning of the other related task. Several approaches have adopted MTL for solving different problems
in computer vision and machine learning. In the face analysis domain, the use of MTL is somewhat limited. In the
following we develop a new MTL approach to affect recognition in-the-wild, by using Aff-Wild2 which includes
annotations for all three tasks.
1) MT-VGG
At first, we developed a multi-task CNN network based on the VGG-FACE. We kept the convolutional and pooling
layers of VGG-FACE, discarded its fully connected layers and added on top of them 2 fully connected layers,
each containing 4096 units. A (linear) output layer followed that provided final estimates for valence and arousal;
it also produced 7 basic expression logits that were passed through a softmax function to get the final 7 basic
expression predictions; lastly, it produced 8 AU logits that were passed through a sigmoid function to get the final
AU predictions.
2) MT-VGG-GRU
Next we extended the above described MT-VGG so as to effectively model contextual information in the data,
taking into account temporal affect variations. Thus we constructed a MT CNN-RNN network. In more detail, a
2-layer GRU with 128 units per layer was stacked on top of the first FC layer of MT-VGG; the output layer was
on top of the GRU, being the same as in MT-VGG. The prediction for each task was pooled from the same feature
space, taking advantage of the correlation between the three different tasks.11
3) A/V-MT-VGG-GRU
Since Aff-Wild2 is an audiovisual (A/V) database, we additionally developed a network for handling both the video
and audio modalities. A/V-MT-VGG-GRU consisted of two identical streams that extracted features directly from
raw input images (extracted from the video) and spectrograms (extracted from the audio). Each stream consisted of
a MT-VGG-GRU, without an output layer. The features from the two streams were concatenated, forming a 256-
dimensional feature vector that was fed to a 2-layer GRU with 128 units in each layer, so as to fuse the information
of the audio and visual streams. The output layer followed on top; it was exactly the same as in MT-VGG-GRU.
A/V-MT-VGG-GRU is a multi-modal and multi-task network and is illustrated in Fig. 3. Let us note here that it is
the first time that audio is taken into account for action unit detection.
Fig. 3: A/V-MT-AffWildNet: the Multi-Modal and Multi-Task developed model
The loss function minimized during training of the multi-task networks was the sum of the individual task losses:
epp
L = E[−log ] (4)
CCE ∑7 i=1ep
i
L =
E[−∑8
(t ·log p +(1−t )·log (1− p ))] (5)
BCE i=1 i i i i
L = 1−0.5×(ρ +ρ ) (6)
CCC a v
where L is the categorical cross entropy loss, L is the binary cross entropy loss, p is the prediction of
CCE BCE p
positive class, p is the prediction of AU or Expr , t ∈ {0,1} is the label of AU, ρ is the Concordance
i i i i i a,v
Correlation Coefficient (CCC) of arousal/valence.
Training implementation details: MT-VGG was first pre-trained on the Aff-Wild, then the output layer was
discarded and substituted by a new one for MTL. The CNN part of the MT-VGG-GRU was initialized with
the weights of the MT-VGG. Training of A/V-MT-VGG-GRU was divided in two phases: first the audio/visual
streams were trained independently and then the audiovisual network was trained end-to-end. To train each stream
individually, we followed the same procedure as in the MT-VGG-GRU case. Once the single streams were trained,
they were used for initializing the corresponding streams in the multi-stream architecture. After pre-training, all
developednetworksweretrainedend-to-endonAff-Wild2.WeutilizedtheAdamoptimizer;learningratewaseither
10−4 or 10−5; batch size was set to 10 and sequence length to 90 for MT-VGG-GRU and A/V-MT-VGG-GRU
networks; batch size was set to 256 for MT-VGG; dropout with 0.5 probability value was applied.
MT Approach Performance Evaluation
Next, we compared our developed networks’ performance, on a cross database experimental study, to that of
state-of-the-art methods developed for these databases. Our networks were first trained on the Aff-Wild2 database
and then fine-tuned to each of 9 different databases. This study illustrates that the MT networks provide the best
pre-trained framework for a large variety of affect recognition settings.12
Table IV compares the performance, in all tasks, of the developed MT-VGG and MT-VGG-GRU (in two settings:
i) when trained only with video frames; ii) when trained only with spectrograms); it also compares A/V-MT-VGG-
GRUtostate-of-the-artmethodsinAff-Wild2,developedbythetop-2performingteamsoftheABAWCompetition.
It can be observed that the MT-VGG-GRU, either when trained with the audio, or visual modality, outperformed,
in all tasks, all other methods. The same happened with the best performing A/V-MT-VGG-GRU.The MT-VGG
displayed a slightly worse, or similar performance in all tasks, compared to the best performing methods, which
was expected given that these methods used either an ensemble methodology of CNN-RNNs, or fused the visual
and audio modalities.
TableIValsopresentsacross-databasecomparisonon8databases,betweenthestate-of-the-artinthesedatabases
and our developed networks. Let us also note that AffectNet, RAF-DB and EmotioNet are static databases, meaning
that they contain only images and thus we could only test the MT-VGG on them. AFEW-VA, DISFA, BP4DS and
BP4D+ databases do not contain audio and thus we could not test the A/V-MT-VGG-GRU on them.
TABLEIV:Cross-databaseevaluationforthethreetaskson9databases,betweenthestate-of-the-artofeachdatabase
and our developed networks; VA evaluation is shown as CCCV-CCCA; the mean diagonal value of the confusion
matrix (denoted as ’Diag.’) was the evaluation criterion for RAF-DB; ’-’ means that either the database did not
containaudioorthedatabaseisastaticoneconsistingofonlyimagesorthenetworkwasnottrainedonthisdatabase
or the network was not trained for this task; EExpr = 0.67×F +0.33∗T Acc; EAU = 0.5×AF +0.5∗T Acc
total 1 total 1
Network Aff-Wild2 Aff-Wild AFEW-VA AffectNet RAF-DB EmotioNet DISFA BP4DS BP4D+
CCC EExpr EAU CCC CCC CCC F1 Diag. F1 F1 F1 F1
total total
CNN-RNNEnsemble[32] 0.44-0.45 0.41 0.61 - - - - - - - - -
TSAV[33] 0.45-0.42 0.51 0.6 - - - - - - - - -
AffWildNet[8],[27] - - - 0.57-0.43 0.52-0.56 - - - - - - -
AlexNet[16] - - - - - 0.6-0.34 0.58 - - - - -
VGG-FACE-mSVM[17] - - - - - - - 0.58 - - - -
ResNet-34[34] - - - - - - - - 0.51 - - -
R-T1[35] - - - - - - - - - 0.6 - -
DLEextension[36] - - - - - - - - - - 0.54 -
VGG+SVM[37] - - - - -- - - - - - 0.51
MT-VGG 0.43-0.42 0.5 0.6 0.56-0.35 0.58-0.53 0.61-0.46 0.54 0.61 0.52 0.61 0.66 0.49
MT-VGG-GRU
0.44-0.51 0.51 0.62 0.54-0.47 - - - - - - - -
(audiomodality)
MT-VGG-GRU
0.46-0.45 0.52 0.62 0.6-0.45 0.6-0.6 - - - - 0.63 0.67 0.52
(visualmodality)
A/V-MT-VGG-GRU 0.47-0.52 0.53 0.63 0.62-0.49 - - - - - - - -
The A/V-MT-VGG-GRU achieved the best performance in Aff-Wild for both valence and arousal estimation,
outperforming the existing state-of-the-art AffWildNet. Moreover, it can be seen that MT-VGG-GRU performed
best for valence estimation when trained, on Aff-Wild, with the visual modality, whilst performed best for arousal
whentrainedwiththeaudiomodality.Thisisbecauseaudiotendstohavethematicconstancy.Consider,forexample,
two fight sequences in a movie, one being a flashy fight scene and the other a one-sided fight with a person being
injured. In both cases, arousal can be high due to loud and pronounced music, but valence will be positive in the
former and negative in the latter sequence.
TableIVshowsthattheMT-VGG-GRU,trainedwiththevisualmodality,outperformedthefine-tunedAffWildNet
in AFEW-VA database. It can also be observed that the MT-VGG outperformed: i) the state-of-the-art AlexNet [16]
on AffectNet both in valence and arousal estimation, ii) the state-of-the-art VGG-FACE-mSVM [17] in RAF-DB,
iii) the winner [34] of Emotionet 2017 Challenge, ResNet-34. Only, in expression recognition in AffectNet, the
obtained performance of the MT-VGG is lower to the state-of-the-art. Finally, the MT-VGG-GRU trained with the
visual modality outperformed: i) the R-TI method [35] in DISFA, ii) the winner of FERA 2015 Challenge, DLE
extension [36] and iii) the winner of FERA 2017 Challenge, VGG+SVM [37].
B. A Holistic Approach to Affect Recognition in-the-wild
In the previous subsection we developed multi-task networks trained on Aff-Wild2, which are able to provide
affect recognition in terms of all three emotion models. We achieved this, by exploiting the fact that Aff-Wild213
contains annotations for all three affect recognition tasks. It should be, however, mentioned that the other existing
databases contain annotations for only one, or two of the tasks and not for all three of them.
Inthefollowingwepresentthefirstholisticframeworkforaffectanalysisin-the-wild,inwhichdifferentemotional
states, such as binary action unit activations, basic categorical emotions and continuous dimensions of valence and
arousal constitute interconnected tasks that are explicable by the human’s affective state. What makes it different
from the approach described before is the exploration of the idea of task-relatedness, given explicitly, either from
external expert knowledge, or from empirical evidence. It should be mentioned that classical multi-task literature
explores feature sharing and task relatedness during training. However in such multi-task settings, one typically
assumes homogeneity of the tasks, i.e. that tasks are of the same type, e.g., object classifiers, or attribute detectors.
The main difference here is that the proposed holistic framework: (i) explores the relatedness of non-homogeneous
tasks, i.e., tasks for (expression) classification, (AU) detection, (V-A) regression; (ii) operates over datasets with
partial, or non-overlapping annotations of the tasks; (iii) encodes explicit relationships between tasks to improve
transparency and to enable expert input. In the following:
• A flexible holistic framework is presented, which can accommodate non-homogeneous tasks, by encoding prior
knowledgeoftaskrelatedness.Intheexperimentstwoeffectivestrategiesoftaskrelatednessareevaluated:a)based
on a cognitive and psychological study, which defines how action units are related to basic emotion categories
[38], and b) inferred empirically from external dataset annotations.
• An effective algorithmic approach is presented, by coupling the tasks via co-annotation and distribution matching;
its effectiveness for facial behavior analysis is illustrated with experimental studies.
• The first holistic network for facial behavior analysis (FaceBehaviorNet) is developed, trained end-to-end to
simultaneously predict 7 basic expressions, 17 action units and continuous valence-arousal, in-the-wild. The
network is trained with all publicly available in-the-wild databases that, in total, consist of over 5M images -
with partial and/or non-overlapping annotations for different tasks.
• Experimental studies illustrate that FaceBehaviorNet greatly outperforms each of the single-task networks, vali-
datingthatthenetwork’saffectrecognitioncapabilitiesareenhancedwhenitisjointlytrainedforallrelatedtasks.
By further exploring feature representations learned during joint training, it is shown that a good generalisation is
achieved on the task of compound expression recognition, when no, or little, training data is available (zero-shot
and few-shot learning).
Let us start with the multi-task formulation of the facial behavior model. In this model we have three objectives:
(1) learning seven basic emotions, (2) detecting activations of 17 binary facial action units, (3) learning the intensity
of the valence and arousal continuous affect dimensions. Our target is to train a multi-task network model to jointly
achieve objectives (1)-(3). However, now we assume that for a given image x ∈ X, we can have a single type of
label annotations; i.e., in terms of either the seven basic emotions y ∈ {1,2,...,7}, or the 171 binary action
emo
unit activations y ∈ {0,1}17, or the two continuous affect dimensions, valence and arousal, y ∈ [−1,1]2. For
au va
simplicity of presentation, we use the same notation x for all images leaving the context to be explained by the
label notations. We train the multi-task model by minimizing the following total objective, which is similar to eq.
4 - 6, with a slight change in the symbols and notations used, so as to fit the following developments:
L = L +λ L +λ L (7)
MT Emo 1 AU 2 VA
L = E [−log p(y |x)]
Emo x,yemo emo
L = E [−log p(y |x)]
AU x,yau au
(cid:104) 17 17 (cid:105)
= E −[∑ δ ]−1· ∑ δ ·[yi log p(yi |x)+(1−yi )log (1− p(yi |x))
x,yau k i au au au au
k=1 i=1
L = 1−0.5×(ρ +ρ ),
VA a v
where: the first term is the cross entropy loss computed over images with a basic emotion label; the second term
is the binary cross entropy loss computed over images with 17 AU activations, δ ∈ {0,1} indicating whether the
i
image contains annotation for AU; the third term measures the CCC loss as in eq. 6.
i
117 is an aggregate of action units in all datasets; typically each dataset has from 10 to 12 AUs labelled by purposely trained annotators.14
TABLE V: Relatedness between: i) basic emotions and their prototypical and observational AUs from [38]: the
weights w in brackets correspond to the fraction of annotators that observed the AU activation; ii) basic emotions
and AUs, inferred from Aff-Wild2: the weights w in brackets correspond to the percentage of images annotated
with the specific expression in which the AU was activated.
Cognitive-Psychological Study [38] Empirical Evidences, Aff-Wild2
Emotion Prototypical AUs Observational AUs (with weights w) AUs (with weights w)
happiness 12, 25 6 (0.51) 12 (0.82), 25 (0.7), 6 (0.57), 7 (0.83), 10 (0.63)
sadness 4, 15 1 (0.6), 6 (0.5), 11 (0.26), 17 (0.67) (0.53), 15 (0.42), 1 (0.31), 7 (0.13), 17 (0.1)
fear 1, 4, 20, 25 2 (0.57), 5 (0.63), 26 (0.33) 1 (0.52), 4 (0.4), 25 (0.85), 5 (0.38), 7 (0.57), 10 (0.57)
anger 4, 7, 24 10 (0.26), 17 (0.52), 23 (0.29) 4 (0.65), 7 (0.45), 25 (0.4), 10 (0.33), 9 (0.15)
surprise 1, 2, 25, 26 5 (0.66) 1 (0.38), 2 (0.37), 25 (0.85), 26 (0.3), 5 (0.5), 7 (0.2)
disgust 9, 10, 17 4 (0.31), 24 (0.26) 9 (0.21), 10 (0.85), 17 (0.23), 4 (0.6), 7 (0.75), 25 (0.8)
Inferring Task-Relatedness: In the seminal work of [38], the authors conduct a study on the relationship between
emotions (basic and compound) and facial action unit activations. The summary of the study is a Table of the
emotions and their prototypical and observational action units, which we include in Table V for completeness.
Prototypical action units are ones that are labelled as activated across all annotators’ responses, observational are
action units that are labelled as activated by a fraction of annotators. For example, in emotion happiness the
prototypical are AU12 and AU25, the observational is AU6 with weight 0.51 (observed by 51% of the annotators).
Table V provides the relatedness between emotion categories and action units obtained from this cognitive and
psychological study with human participants.
Alternatively we inferred empirically the task relatedness from external dataset annotations. In particular, we
used the Aff-Wild2 database, which is the first in-the-wild database that contains annotations for all three behavior
tasks that we are dealing with. At first, we trained a network for AU detection on the union of Aff-Wild2 and
GFT databases [39]. Next, this network was used to automatically annotate all Aff-Wild2 videos with AUs. Table
V also shows the distribution of AUs for each basic expression. In parenthesis next to each AU is the percentage
of images annotated with the specific expression in which this AU was activated.
In the following, we describe and explain the developed losses used for coupling the tasks, mentioning the case
where task relatedness is inferred from the cognitive and psychological study [38].
1) CouplingofbasicemotionsandAUsviaco-annotation: Weproposeaco-annotationstrategytocoupletraining
of emotions and action unit predictions. Given an image x with the ground truth basic emotion y , we enforce the
emo
prototypical and observational AUs of this emotion to be activated. We co-annotate the image (x,y ) with y ;
emo au
this image contributes to both L and L 2 in eq. 7. We re-weight the contributions of the observational AUs
Emo AU
with the annotators’ agreement score (from Table V). Similarly, for an image x with ground truth action units y ,
au
we check whether we can co-annotate it with an emotion label. For an emotion to be present, all its prototypical
and observational AUs have to be present. In cases when more than one emotion is possible, we assign the label
y of the emotion with the largest requirement of prototypical and observational AUs. The image (x,y ) that
emo au
is co-annotated with the emotion label y contributes to both L and L in eq. 7. We use this approach to
emo AU Emo
develop FaceBehaviorNet with co-annotation.
2) Coupling of basic emotions and AUs via distribution matching: The aim here is to align the predictions of
emotions and action units tasks during training. For each sample x we have the predictions of emotions p(y |x)
emo
as the softmax scores over 7 basic emotions and we have the prediction of AU activations p(yi |x), i = 1,...,17
au
as the sigmoid scores over 17 AUs. The distribution matching idea is the following: we match the distribution over
AU predictions p(yi |x) with the distribution q(yi |x), where the AUs are modeled as a mixture over the basic
au au
emotion categories:
q(yi |x) = ∑ p(y |x)p(yi |y ), (8)
au emo au emo
yemo∈{1,...,7}
where p(yi |y ) is defined in a deterministic way from Table V and is equal to 1 for prototypical/observational
au emo
action units, or to 0 otherwise. For example, AU2 is prototypical for emotion surprise and observational for emotion
2Here we overload slightly notations; for co-annotated images, yau has variable length and only contains prototypical/observational AUs.15
Fig. 4: The holistic (multi-task, multi-domain, multi-label) FaceBehaviorNet; ’VA/AU/EXPR-BATCH’ refers to
batches annotated in terms of VA/AU/7 basic expressions
fear and thus q(y |x) = p(y |x)+ p(y |x). 3. This matching aims to make the network’s predicted
AU2 surprise fear
AUs consistent with the prototypical and observational AUs of the network’s predicted emotions. So if, e.g., the
network predicts the emotion happiness with probability 1, i.e., p(y |x) = 1, then the prototypical and
happiness
observational AUs of happiness, i.e., AUs 12, 25 and 6- need to be activated in the distribution q: q(y |x) = 1;
AU12
q(y |x) = 1; q(y |x) = 1; q(yi |x) = 0, i ∈ {1,..,14}. In spirit of the distillation approach, we match the
AU25 AU6 au
distributions p(yi |x) and q(yi |x) by minimizing the cross entropy with the soft targets loss term4:
au au
17
L = E ∑ [−p(yi |x)log q(yi |x)], (9)
DM x au au
i=1
where all available training samples are used to match the predictions. We use this approach to develop FaceBe-
haviorNet with distr-matching.
3) A mix of the two strategies, co-annotation and distribution matching: Given an image x with the ground
truth annotation of the action units y , we can first co-annotate it with a soft label in form of the distribution
au
over emotions and then match it with the predictions of emotions p(y |x). More specifically, for each basic
emo
emotion, we compute the score of its prototypical and observational AUs being present. For example, for emotion
happiness, we compute (y +y +0.51·y )/(1+1+0.51), or set all weights to be equal to 1, when
AU12 AU25 AU6
no reweighting is used. We take a softmax over the scores to produce the probabilities over emotion categories. In
this variant, every single image that has ground truth annotation of AUs will have a soft emotion label assigned to
it. Finally we match the predictions p(y |x) and the soft label by minimizing the cross entropy with soft targets
emo
similarly to eq. 9. We use this approach to develop FaceBehaviorNet with soft co-annotation.
4) Coupling of categorical emotions and AUs with continuous affect: In our work, continuous affect (valence
and arousal) is implicitly coupled with the basic expressions and action units via a joint training procedure. Also
one of the datasets we used has annotations for categorical and continuous emotions (AffectNet).
FaceBehaviorNetstructure&Trainingimplementationdetails: Fig.4showsthestructureoftheholistic(multi-task,
multi-domain and multi-label) FaceBehaviorNet, which is an adapted version of the MT-VGG described previously.
The difference is that the output layer predicts valence and arousal, 7 basic expressions and 17 action units. The
predictions for all tasks are pooled from the same feature space.
At this point let us describe the strategy that was used for feeding images from different databases to FaceBe-
haviorNet. At first, the training set was split into three different sets, each of which contained images that were
3We also tried a variant with reweighting for observational AUs, i.e. p(yi au|yemo)=w
4This can be seen as minimizing the KL-divergence KL(p||q) across the 17 action units.16
annotated in terms of either valence-arousal, or action units, or seven basic expressions; let us denote these sets as
VA-Set, AU-Set and EXPR-Set, respectively. During training, at each iteration, three batches, one from each set
(as can be seen in Fig.4), were concatenated and fed to FaceBehaviorNet. This step was important for network
training, because: i) the network minimizes the objective function of eq. 7; at each iteration, the network has seen
images from all categories and thus all loss terms contribute to the objective function, ii) since the network sees an
adequate number of images from all categories, the weight updates (during gradient descent) are not based on noisy
gradients; this in turn prevents poor convergence behaviors; otherwise, we would need to tackle these problems,
e.g. do asynchronous SGD to make the task parameter updates decoupled, iii) the CCC cost function needs an
adequate sequence of predictions.
Since VA-Set, AU-Set and EXPR-Set had different sizes, they needed to be ’aligned’. To do so, we selected the
batches of these sets in such a manner, so that after one epoch we have sampled all images in the sets. In particular,
we chose batches of size 401, 247 and 103 for the VA-Set, AU-Set and EXPR-Set, respectively. The training of
FaceBehaviorNet was performed in an end-to-end manner, with a learning rate of 10−4. A 0.5 Dropout value was
used in the fully connected layers.
FaceBehaviorNet Performance Evaluation
Next, we trained a VGG-FACE network on all dimensionally annotated databases to predict valence and arousal;
we also trained another VGG-FACE network on all categorically annotated databases, to perform seven basic
expression classification; finally we trained a third VGG-FACE network on all databases annotated with action
units, so as to perform AU detection. For brevity these three single-task networks are denoted as ’(3 ×) VGG-
FACE single-task’ in one row of Table VI. We compared these networks’ performance to the performance of
FaceBehaviorNet when trained with and without the coupling losses. We also compared them to the performance
of state-of-the-art methodologies developed for each utilised database, that we described previously. Table VI
displays the performance of all these networks.
It might be argued that the more data used for network training (even if they contain partial or non-overlapping
annotations), the better network performance will be in all tasks. However this may not be true, as the three studied
tasks are non-homogeneous and each one of them contains ambiguous cases: i) there is, in general, discrepancy
in the perception of the disgust, fear, sadness and (negative) surprise emotions across different people and across
databases; ii) the exact valence and arousal value for a particular affect is not consistent among databases; iii)
the AU annotation process is a hard to do and error prone one. Nevertheless, from Table VI, it can be verified
that FaceBehaviorNet achieved a better performance on all databases than the independently trained VGG-FACE
single-task models. This illustrates that all described facial behavior understanding tasks are coherently correlated
to each other. Thus, simultaneously training an end-to-end architecture, with heterogeneous databases, leads to
improved performance.
In Table VI, it can be observed that FaceBehaviorNet trained with no coupling loss: i) ouperforms the state-
of-the-art by 3.5% (average CCC) on Aff-Wild, 4% (average CCC) on AffectNet, 9% on RAF-DB and 2% on
BP4DS; ii) shows inferior performance by 4% on AffectNet and 1% on EmotioNet, 1% on BP4D+. However,
when FaceBehaviorNet is trained with soft co-annotation and distr-matching losses (either when task relatedness is
inferred from Aff-Wild2 or from [38]), it shows superior performance to all state-of-the-art methods. The fact that
it outperforms these methods and the single-task networks, in both task relatedness settings, verifies the generality
of the proposed losses; network performance is boosted independently of the Table of task relatedness used.
Zero-Shot and Few-Shot Learning In order to further prove and validate that FaceBehaviorNet learned good
features encapsulating all aspects of facial behavior, we conducted zero-shot learning experiments for classifying
compound expressions. Given that there exist only 2 datasets (EmotioNet and RAF-DB) annotated with compound
expressions and that they do not contain a lot of samples (less than 3,000 each), at first, we used the predictions
of FaceBehaviorNet together with the rules from [38] to generate compound emotion predictions. Additionally,
to demonstrate the superiority of FaceBehaviorNet, we used it as a pre-trained network in a few-shot learning
experiment. We took advantage of the fact that the network has learned good features and used them as priors for
fine-tuning the network to perform compound emotion classification.
a) RAF-DB database: At first, we performed zero-shot experiments on the 11 compound categories of RAF-
DB. We computed a candidate score, C (y ), for each class y :
s emo emo17
TABLE VI: Performance evaluation of valence-arousal, seven basic expression, compound expression and action
unitspredictionsonallutiliseddatabasesprovidedbytheFaceBehaviorNetandthestate-of-the-artmethods;CCCis
shown as CCCV-CCCA; ’Diag.’ is the mean diagonal value of the confusion matrix; ’AFA’ is the Average between
the mean F1 score and the mean Accuracy; ’UAR’ is the Unweighted Average Recall
Databases Aff-Wild AffectNet RAF-DB EmotioNet DISFA BP4DS BP4D+
Basic Basic Compound Compound
VA VA AU AU AU AU
Expr Expr Expr Expr
CCC CCC F1 Diag. Diag. F1 UAR AFA F1 F1 F1
VGG-FACE[8],[27] 0.51-0.33 - - - - - - - - - -
AlexNet[16] - 0.60-0.34 0.58 - - - - - - - -
VGG-FACE-mSVM[17] - - - 0.58 0.32 - - - - - -
DLP-CNN[17] - - - - 0.45 - - - - - -
NTechLab[18] - - - - - 0.26 0.24 - - - -
ResNet-34[34] - - - - - - - 0.73 - - -
DLEextension[36] - - - - - - - - - 0.59 -
[37] - - - - - - - - - - 0.58
(3 ×)VGG-FACEsingle-task 0.52-0.31 0.53-0.43 0.51 0.59 - - - 0.67 0.47 0.56 0.54
FaceBehaviorNet
0.55-0.36 0.56-0.46 0.54 0.67 - - - 0.72 0.52 0.61 0.57
nocouplingloss
FaceBehaviorNet soft co-annotation
0.59-0.41 0.59-0.50 0.60 0.70 - - - 0.73 0.57 0.67 0.60
& distr-matching, [38]
FaceBehaviorNet soft co-annotation
0.60-0.40 0.61-0.51 0.60 0.71 - - - 0.74 0.60 0.66 0.60
& distr-matching, Aff-Wild2
zero-shotFaceBehaviorNet
- - - - 0.35 0.25 0.27 - - - -
nocouplingloss
zero-shot FaceBehaviorNet
soft co-annotation - - - - 0.37 0.32 0.34 - - - -
& distr-matching [38]
fine-tunedFaceBehaviorNet
- - - - 0.46 - - - - - -
nocouplingloss
fine-tuned FaceBehaviorNet
soft co-annotation - - - - 0.49 - - - - - -
& distr-matching [38]
∑17 p(yk |y ) p(y |x)
C (y ) = k=1 au emo +p(y )+ p(y )+0.5·( v +1),p(y |x) (cid:54)= 0 (10)
s emo ∑1 k=7
1
p(yk au|x)p(yk au|y emo) emo1 emo2 |p(y v|x)| v
where:i)thefirsttermofthesumisFaceBehaviorNet’spredictionsofonlytheprototypical(andobservational)AUs
that are associated with this compound class according to [38]; in this manner, every AU acts as an indicator for
this particular emotion class; ii) p(y ) and p(y ) are FaceBehaviorNet’s predictions of the basic expression
emo1 emo2
classesemo1andemo2thataremixedandformthecompoundclass(e.g.,ifthecompoundclassishappilysurprised
then emo1 is happy and emo2 is surprised); iii) the last term of the sum is added only to the happily surprised
and happily disgusted classes and is either 0 or 1 depending on whether FaceBehaviorNet’s valence prediction
is negative or positive, respectively; the rationale is that only happily surprised and (maybe) happily disgusted
classes have positive valence; all other classes are expected to have negative valence as they correspond to negative
emotions. Our final prediction was the class that had the maximum candidate score.
Table VI shows the results of this approach when we used the predictions of FaceBehaviorNet trained with and
without the soft co-annotation and distr-matching losses. Best results have been obtained when the network was
trained with the coupling losses. One can observe, that this approach outperformed by 5% the VGG-FACE-mSVM
[17] which has the same architecture as this network and has been trained for compound emotion classification.
Next, we target few-shot learning. In particular, we fine-tune the FaceBehaviorNet (trained with and without the
soft co-annotation and distr-matching losses) on the small training set of RAF-DB. In Table VI we compared its
performance to a state-of-the-art network. It can be seen that the fine-tuned FaceBehaviorNet, trained with and
without the coupling losses, outperformed by 1% and 4%, respectively, the best performing network, DLP-CNN,
that was trained with a loss designed for this specific task.18
b) EmotioNet database: Next, we performed zero-shot experiments on the EmotioNet basic and compound set
that was released for the related Challenge. This set includes 6 basic plus 10 compound categories. The zero-shot
methodology we followed was similar to the one described above for the RAF-DB database. The results of this
experiment can be found in Table VI. Best results have also been obtained when the network was trained with
the two coupling losses. It can be observed that this approach outperformed by 6% and 10% in F1 score and
Unweighted Average Recall (UAR), respectively, the state-of-the-art NTechLab’s [18] approach, which used the
Emotionet’s images with compound annotation.
VII. CONCLUSIONS
This paper presented the state-of-the-art in affect recognition in-the-wild. This involved large databases annotated
in terms of: dimensional emotion variables, i.e., valence and arousal; seven basic expression categories; facial action
units. This also involved deep neural architectures [40] that are trained with these databases, providing state-of-the-
art performance on them, or constituting robust priors for dimensional, and/or categorical recognition over other
datasets in-the-wild.
In this framework, we presented the state-of-the-art affect recognition DNN, AffWildNet, trained on the Aff-Wild
database. We also presented a DNN methodology by extracting low-, mid- and high-level latent information and
analysing this by multiple RNN subnets. Moreover, we showed that an ensemble approach, based on model-level
fusion,producedexcellentresultsforvisualaffectrecognitionontheOMG-EmotionChallenge.Wefurtherexplored
the use of Aff-Wild2, the largest in-the-wild, audiovisual, database, being annotated in terms of valence-arousal
dimensions, seven basic expressions and facial action units. We presented multi-task DNNs, being trained on Aff-
Wild2, illustrating that they outperform the state-of-the-art for affect recognition. We additionally presented the
FaceBehaviorNet, which is the first holistic framework for joint: basic expression recognition, action unit detection
and valence-arousal estimation.
Future directions include the development of new scalable architectures that can learn to analyse affect, by
extracting coarse-to-fine information from visual inputs in-the-wild. Moreover, focusing on specific types of affect,
for example, related to negative or reluctant behaviours, or on compound emotions, will require extending the
databases, and/or performing domain adaptation of the developed architectures in these frameworks. Unsupervised
learning will be the main direction in the future for handling non-annotated data cases, while focusing on related
problems, such as uncertainty of the estimation procedure. Moreover, although it has been shown that DNNs are
capable of analysing large datasets for affect recognition, they lack transparency in their decision making, in the
sense that it is not straightforward to justify their prediction. Extraction of latent variables, such as low-, medium-
and high-level features and further unsupervised exploration of them in multiple contexts and in low-shot learning
frameworks can be further investigated.
REFERENCES
[1] P. Ekman, “Facial action coding system (facs),” A human face, 2002.
[2] C. Whissel, “The dictionary of affect in language, emotion: Theory, research and experience: vol. 4, the measurement of emotions, r,”
Plutchik and H. Kellerman, Eds., New York: Academic, 1989.
[3] J. A. Russell, “Evidence of convergent validity on the dimensions of affect.” Journal of personality and social psychology, vol. 36,
no. 10, p. 1152, 1978.
[4] D. Kollias, G. Marandianos, A. Raouzaiou, and A.-G. Stafylopatis, “Interweaving deep learning and semantic techniques for emotion
analysis in human-machine interaction,” in 2015 10th International Workshop on Semantic and Social Media Adaptation and
Personalization (SMAP). IEEE, 2015, pp. 1–6.
[5] D.Kollias,A.Tagaris,andA.Stafylopatis,“Onlineemotiondetectionusingretrainabledeepneuralnetworks,”in2016IEEESymposium
Series on Computational Intelligence (SSCI). IEEE, 2016, pp. 1–8.
[6] A.Tagaris,D.Kollias,A.Stafylopatis,G.Tagaris,andS.Kollias,“Machinelearningforneurodegenerativedisorderdiagnosis—survey
ofpracticesandlaunchofbenchmarkdataset,”InternationalJournalonArtificialIntelligenceTools,vol.27,no.03,p.1850011,2018.
[7] S.Zafeiriou,D.Kollias,M.A.Nicolaou,A.Papaioannou,G.Zhao,andI.Kotsia,“Aff-wild:Valenceandarousal‘in-the-wild’challenge,”
in Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on. IEEE, 2017, pp. 1980–1987.
[8] D. Kollias, M. A. Nicolaou, I. Kotsia, G. Zhao, and S. Zafeiriou, “Recognition of affect in the wild using deep neural networks,” in
Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on. IEEE, 2017, pp. 1972–1979.
[9] D. Kollias and S. Zafeiriou, “Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface,” arXiv preprint
arXiv:1910.04855, 2019.
[10] D. Kollias, A. Schulc, E. Hajiyev, and S. Zafeiriou, “Analysing affective behavior in the first abaw 2020 competition,” in 2020 15th
IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG). IEEE Computer Society, pp. 794–800.19
[11] A. Tagaris, D. Kollias, and A. Stafylopatis, “Assessment of parkinson’s disease based on deep neural networks,” in International
Conference on Engineering Applications of Neural Networks. Springer, 2017, pp. 391–403.
[12] D.Kollias,V.Sharmanska,andS.Zafeiriou,“Facebehavior\alacarte:Expressions,affectandactionunitsinasinglenetwork,”arXiv
preprint arXiv:1910.11111, 2019.
[13] S.M.Mavadati,M.H.Mahoor,K.Bartlett,P.Trinh,andJ.F.Cohn,“Disfa:Aspontaneousfacialactionintensitydatabase,”Affective
Computing, IEEE Transactions on, vol. 4, no. 2, pp. 151–160, 2013.
[14] X. Zhang, L. Yin, J. F. Cohn, S. Canavan, M. Reale, A. Horowitz, P. Liu, and J. M. Girard, “Bp4d-spontaneous: a high-resolution
spontaneous 3d dynamic facial expression database,” Image and Vision Computing, vol. 32, no. 10, pp. 692–706, 2014.
[15] Z. Zheng, C. Cao, X. Chen, and G. Xu, “Multimodal emotion recognition for one-minute-gradual emotion challenge,” arXiv preprint
arXiv:1805.01060, 2018.
[16] A. Mollahosseini, B. Hasani, and M. H. Mahoor, “Affectnet: A database for facial expression, valence, and arousal computing in the
wild,” arXiv preprint arXiv:1708.03985, 2017.
[17] S. Li, W. Deng, and J. Du, “Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 2852–2861.
[18] C. F. Benitez-Quiroz, R. Srinivasan, Q. Feng, Y. Wang, and A. M. Martinez, “Emotionet challenge: Recognition of facial expressions
of emotion in the wild,” arXiv preprint arXiv:1703.01210, 2017.
[19] J. Kossaifi, R. Walecki, Y. Panagakis, J. Shen, M. Schmitt, F. Ringeval, J. Han, V. Pandit, A. Toisoul, B. W. Schuller et al., “Sewa
db:Arichdatabaseforaudio-visualemotionandsentimentresearchinthewild,”IEEETransactionsonPatternAnalysisandMachine
Intelligence, 2019.
[20] P.Barros,N.Churamani,E.Lakomkin,H.Siqueira,A.Sutherland,andS.Wermter,“Theomg-emotionbehaviordataset,”arXivpreprint
arXiv:1803.05434, 2018.
[21] J. Kossaifi, G. Tzimiropoulos, S. Todorovic, and M. Pantic, “Afew-va database for valence and arousal estimation in-the-wild,” Image
and Vision Computing, 2017.
[22] D. Kollias, S. Cheng, M. Pantic, and S. Zafeiriou, “Photorealistic facial synthesis in the dimensional affect space,” in Proceedings of
the European Conference on Computer Vision (ECCV), 2018, pp. 0–0.
[23] D.Kollias,S.Cheng,E.Ververas,I.Kotsia,andS.Zafeiriou,“Deepneuralnetworkaugmentation:Generatingfacesforaffectanalysis,”
International Journal of Computer Vision, pp. 1–30, 2020.
[24] D.KolliasandS.Zafeiriou,“Va-stargan:Continuousaffectgeneration,”inInternationalConferenceonAdvancedConceptsforIntelligent
Vision Systems. Springer, 2020, pp. 227–238.
[25] M.Najibi,P.Samangouei,R.Chellappa,andL.Davis,“SSH:Singlestageheadlessfacedetector,”inTheIEEEInternationalConference
on Computer Vision (ICCV), 2017.
[26] D.KolliasandS.Zafeiriou,“Trainingdeepneuralnetworkswithdifferentdatasetsin-the-wild:Theemotionrecognitionparadigm,”in
2018 International Joint Conference on Neural Networks (IJCNN). IEEE, 2018, pp. 1–8.
[27] D. Kollias, P. Tzirakis, M. A. Nicolaou, A. Papaioannou, G. Zhao, B. Schuller, I. Kotsia, and S. Zafeiriou, “Deep affect prediction
in-the-wild:Aff-wilddatabaseandchallenge,deeparchitectures,andbeyond,”InternationalJournalofComputerVision,vol.127,no.
6-7, pp. 907–929, 2019.
[28] D. Kollias and S. Zafeiriou, “A multi-component cnn-rnn approach for dimensional emotion recognition in-the-wild,” arXiv preprint
arXiv:1805.01452, 2018.
[29] D.KolliasandS.P.Zafeiriou,“Exploitingmulti-cnnfeaturesincnn-rnnbaseddimensionalemotionrecognitionontheomgin-the-wild
dataset,” IEEE Transactions on Affective Computing, pp. 1–1, 2020.
[30] M. Yu, D. Kollias, J. Wingate, N. Siriwardena, and S. Kollias, “Machine learning for predictive modelling of ambulance calls,”
Electronics, vol. 10, no. 4, p. 482, 2021.
[31] D. Kollias, M. Yu, A. Tagaris, G. Leontidis, A. Stafylopatis, and S. Kollias, “Adaptation and contextualization of deep neural network
models,” in 2017 IEEE symposium series on computational intelligence (SSCI). IEEE, pp. 1–8.
[32] D.Deng,Z.Chen,andB.E.Shi,“Fau,facialexpressions,valenceandarousal:Amulti-tasksolution,”arXivpreprintarXiv:2002.03557,
2020.
[33] F. Kuhnke, L. Rumberg, and J. Ostermann, “Two-stream aural-visual affect analysis in the wild,” in 2020 15th IEEE International
Conference on Automatic Face and Gesture Recognition (FG 2020)(FG). IEEE Computer Society, pp. 366–371.
[34] W. Ding, D.-Y. Huang, Z. Chen, X. Yu, and W. Lin, “Facial action recognition using very deep networks for highly imbalanced
classdistribution,”in2017Asia-PacificSignalandInformationProcessingAssociationAnnualSummitandConference(APSIPAASC).
IEEE, 2017, pp. 1368–1372.
[35] W. Li, F. Abtahi, and Z. Zhu, “Action unit detection with region adaptation, multi-labeling learning and optimal temporal fusing,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 1841–1850.
[36] A.Yüce,H.Gao,andJ.-P.Thiran,“Discriminantmulti-labelmanifoldembeddingforfacialactionunitdetection,”in201511thIEEE
International Conference and Workshops on Automatic Face and Gesture Recognition (FG), vol. 6. IEEE, 2015, pp. 1–6.
[37] C. Tang, W. Zheng, J. Yan, Q. Li, Y. Li, T. Zhang, and Z. Cui, “View-independent facial action unit detection,” in 2017 12th IEEE
International Conference on Automatic Face & Gesture Recognition (FG 2017). IEEE, 2017, pp. 878–882.
[38] S.Du,Y.Tao,andA.M.Martinez,“Compoundfacialexpressionsofemotion,”ProceedingsoftheNationalAcademyofSciences,vol.
111, no. 15, pp. E1454–E1462, 2014.
[39] J. M. Girard, W.-S. Chu, L. A. Jeni, and J. F. Cohn, “Sayette group formation task (gft) spontaneous facial expression database,” in
2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017). IEEE, 2017, pp. 581–588.
[40] D.Kollias,A.Tagaris,A.Stafylopatis,S.Kollias,andG.Tagaris,“Deepneuralarchitecturesforpredictioninhealthcare,”Complex&
Intelligent Systems, vol. 4, no. 2, pp. 119–131, 2018.20
DimitriosKollias,FellowoftheHigherEducationAcademy,holderofaPost-GraduateCertificateandmemberofthe
IEEE, is currently aSenior Lecturerin ComputerScience withthe Schoolof Computingand MathematicalSciences,
UniversityofGreenwich.HehasbeentherecipientoftheprestigiousTeachingFellowshipofImperialCollegeLondon.
He has obtained the Ph.D. from the Department of Computing, Imperial College London, where he was a member
of the iBUG group. Prior to this, he received the Diploma/M.Sc. in Electrical and Computer Engineering from the
ECE School of the National Technical University of Athens, Greece, and the M.Sc. in Advanced Computing from
the Department of Computing of Imperial College London. He has published his research in the top journals and
conferencesonmachinelearning,perceptionandcomputervisionsuchasIJCV,CVPR,ECCV,BMVC,IJCNN,ECAI
and SSCI. He is a reviewer in top journals and conferences, such as CVPR, ECCV, ICCV, AAAI, TIP, TNNL, TAC,
Neurocomputing, Pattern Recognition and Neural Networks. He has been Competition Chair and Workshop Chair in IEEE FG 2020. He
has won many grants and awards, such as from the City and Guilds College Association, the Imperial College Trust and the Complex &
Intelligent Systems Journal. He has h-index 17 and i10-index 19. His research interests span the areas of machine and deep learning, deep
neural networks, computer vision, affective computing and medical imaging.
Stefanos Zafeiriou is currently a Professor in Machine Learning and Computer Vision with the Department of
Computing, Imperial College London. He also holds an EPSRC Fellowship. He received the Prestigious Junior
Research Fellowships from Imperial College London in 2011 to start his own independent research group. He
received the President’s Medal for Excellence in Research Supervision for 2016. He received various awards during
his doctoral and postdoctoral studies. He has been a Guest Editor of more than 6 journal special issues and co-
organized more than 15 workshops/special sessions on specialized computer vision topics in top venues, such as
CVPR/FG/ICCV/ECCV (including three very successfully challenges run in ICCV’13, ICCV’15 and CVPR’17 on
facial landmark localisation/tracking). He has coauthored more than 70 journal papers mainly on novel statistical
machine learning methodologies applied to computer vision problems, such as 2-D/3-D face analysis, deformable
object fitting and tracking, shape from shading, and human behavior analysis, published in the most prestigious journals in his field of
research, such as TPAMI, IJCV, TIP, TNNLS and many papers in top conferences, such as CVPR, ICCV, ECCV, ICML. His students are
frequentrecipientsofveryprestigiousandhighlycompetitivefellowships,suchastheGoogle,IntelandQualcommones.Hehasmorethan
12000 citations to his work, h-index 54, i10-index 159. He was the General Chair of BMVC 2017."
27,27,Affectiva-mit facial expression dataset (am-fed): Naturalistic and spontaneous facial expressions collected,"['D McDuff', 'R Kaliouby', 'T Senechal', 'M Amr']",2013,314,"Acted Facial Expressions In The Wild, Affective Faces Database",classifier,"Computer classification of facial expressions requires large amounts of data and this data  needs to  If consent is granted, the commercial is played in the browser whilst simultaneously",No DOI,Proceedings of the …,https://ieeexplore.ieee.org/document/6595975,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
28,28,Affective image classification using features inspired by psychology and art theory,"['J Machajdik', 'A Hanbury']",2010,1010,Affective Faces Database,classification,"as features, and the expected affective state in which the user is  that are specific to the task  of affective image classification.  face detection algorithm by Viola and Jones [28] to find faces",No DOI,Proceedings of the 18th ACM international …,https://dl.acm.org/doi/10.1145/1873951.1873965,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,acm.org,
29,29,"Affectnet: A database for facial expression, valence, and arousal computing in the wild","['A Mollahosseini', 'B Hasani']",2017,1965,"Acted Facial Expressions In The Wild, Affective Faces Database, Expression in-the-Wild, Static Facial Expression in the Wild","CNN, FER, classification, classifier, deep learning, facial expression recognition, machine learning, neural network",our deep neural network baselines can perform better than conventional machine learning   [18] released Acted Facial Expressions in the Wild (AFEW) from 54 movies by a recom,No DOI,IEEE Transactions on …,https://arxiv.org/abs/1708.03985,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"IEEETRANSACTIONSONAFFECTIVECOMPUTING 1
AffectNet: A Database for Facial Expression,
Valence, and Arousal Computing in the Wild
Ali Mollahosseini, Student Member, IEEE, Behzad Hasani, Student Member, IEEE,
and Mohammad H. Mahoor, Senior Member, IEEE
Abstract—Automatedaffectivecomputinginthewildsettingisachallengingproblemincomputervision.Existingannotated
databasesoffacialexpressionsinthewildaresmallandmostlycoverdiscreteemotions(akathecategoricalmodel).Therearevery
limitedannotatedfacialdatabasesforaffectivecomputinginthecontinuousdimensionalmodel(e.g.,valenceandarousal).Tomeet
thisneed,wecollected,annotated,andpreparedforpublicdistributionanewdatabaseoffacialemotionsinthewild(calledAffectNet).
AffectNetcontainsmorethan1,000,000facialimagesfromtheInternetbyqueryingthreemajorsearchenginesusing1250emotion
relatedkeywordsinsixdifferentlanguages.Abouthalfoftheretrievedimagesweremanuallyannotatedforthepresenceofseven
discretefacialexpressionsandtheintensityofvalenceandarousal.AffectNetisbyfarthelargestdatabaseoffacialexpression,
valence,andarousalinthewildenablingresearchinautomatedfacialexpressionrecognitionintwodifferentemotionmodels.Two
baselinedeepneuralnetworksareusedtoclassifyimagesinthecategoricalmodelandpredicttheintensityofvalenceandarousal.
Variousevaluationmetricsshowthatourdeepneuralnetworkbaselinescanperformbetterthanconventionalmachinelearning
methodsandoff-the-shelffacialexpressionrecognitionsystems.
IndexTerms—Affectivecomputinginthewild,facialexpressions,continuousdimensionalspace,valence,arousal.
(cid:70)
1 INTRODUCTION
AFFECT is a psychological term used to describe the all possible facial actions are described in terms of Action
outwardexpressionofemotionandfeelings.Affective Units (AUs) [4]. FACS model explains facial movements
computing seeks to develop systems and devices that can and does not describe the affective state directly. There
recognize, interpret, and simulate human affects through are several methods to convert AUs to affect space (e.g.,
various channels such as face, voice, and biological sig- EMFACS[5]statesthattheoccurrenceofAU6andAU12isa
nals[1].Faceandfacialexpressionsareundoubtedlyoneof signofhappiness).Inthecategoricalmodel,mixedemotions
themostimportantnonverbalchannelsusedbythehuman cannotadequatelybetranscribedintoalimitedsetofwords.
beingtoconveyinternalemotion. Someresearcherstriedtodefinemultipledistinctcompound
There have been tremendous efforts to develop reliable emotioncategories(e.g.,happilysurprised,sadlyfearful)[6]
automatedFacialExpressionRecognition(FER)systemsfor toovercomethislimitation.However,stillthesetislimited,
useinaffect-awaremachinesanddevices.Suchsystemscan and the intensity of the emotion cannot be defined in the
understand human emotion and interact with users more categorical model. In contrast, the dimensional model of
naturally. However, current systems have yet to reach the affect can distinguish between subtly different displays of
fullemotionalandsocialcapabilitiesnecessaryforbuilding affect and encode small changes in the intensity of each
rich and robust Human Machine Interaction (HMI). This is emotiononacontinuousscale,suchasvalenceandarousal.
mainly due to the fact that HMI systems need to interact Valence refers to how positive or negative an event is, and
with humans in an uncontrolled environment (aka wild arousal reflects whether an event is exciting/agitating or
setting) where the scene lighting, camera view, image res- calm/soothing[3].Figure1showssamplesoffacialexpres-
olution,background,usersheadpose,gender,andethnicity sionsrepresentedinthe2Dspaceofvalenceandarousal.As
canvarysignificantly.Moreimportantly,thedatathatdrives it is shown, there are several different kinds of affect and
thedevelopmentofaffectivecomputingsystemsandpartic- small changes in the same emotion that cannot be easily
ularlyFERsystemslacksufficientvariationsandannotated mappedintoalimitedsetoftermsexistinginthecategorical
samplesthatcanbeusedinbuildingsuchsystems. model.
There are several models in the literature to quantify The dimensional model of affect covers both intensity
affective facial behaviors: 1) categorical model, where the anddifferentemotioncategoriesinthecontinuousdomain.
emotion/affect is chosen from a list of affective-related Nevertheless, there are relatively fewer studies on devel-
categories such as six basic emotions defined by Ekman et oping automated algorithms in measuring affect using the
al.[2],2)dimensionalmodel,whereavalueischosenovera continuous dimensional model (e.g., valence and arousal).
continuousemotionalscale,suchasvalenceandarousal[3] Oneofthemainreasonsisthatcreatingalargedatabaseto
and 3) Facial Action Coding System (FACS) model, where cover the entire continuous space of valence and arousal
is expensive and there are very limited annotated face
• AuthorsarewiththeDepartmentofElectricalandComputerEngineering, databasesinthecontinuousdomain.Thispapercontributes
UniversityofDenver,Denver,CO,80210. to the field of affective computing by providing a large
E-mail:amollah,bhasani,mmahoor@du.edu
annotated face database of the dimensional as well as the
7102
tcO
9
]VC.sc[
4v58930.8071:viXraIEEETRANSACTIONSONAFFECTIVECOMPUTING 2
and dimensional (valence and arousal) models and tagged
the images that have any occlusion on the face. Figure 1
shows sample images from AffectNet and their valence and
arousalannotations.
To calculate the agreement level between the human
labelers,36,000imageswereannotatedbytwohumanlabel-
ers.AffectNetisbyfarthelargestdatabaseoffacialaffectin
still images which covers both categorical and dimensional
models. The cropped region of the facial images, the facial
landmarkpoints,andtheaffectlabelswillbepubliclyavail-
abletotheresearchcommunity1.Consideringthelackofin-
the-wild large facial expressions datasets and more specif-
ically annotated face datasets in the continuous domain of
valenceandarousal,AffectNetisagreatresourcewhichwill
enable further progress in developing automated methods
for facial behavior computing in both the categorical and
continuousdimensionalspaces.
The rest of this paper is organized as follows. Section 2
reviewstheexistingdatabasesandstate-of-the-artmethods
for facial expression recognition with emphasis on the di-
mensionalmodelandinthewildsettingdatabases.Section3
explainstheprocessofcollectingAffectNetimagesfromthe
Fig.1.SampleimagesinValenceArousalcircumplex Internet and annotating the categorical and dimensional
models. Section 4 presents two different baselines for au-
tomatic recognition of categorical emotions and prediction
categoricalmodelsofaffect. ofdimensionalvalenceandarousalinthecontinuousspace
The majority of the techniques for automated affective using AffecNet images. Finally Section 5 concludes the
computingandFERarebasedonsupervisedmachinelearn- paper.
ingmethodologies.Thesesystemsrequireannotatedimage
samples for training. Researchers have created databases
2 RELATED WORK
of human actors/subjects portraying basic emotions [7],
[8], [9], [10], [11]. Most of these databases mainly contain 2.1 Existingdatabases
posed expressions acquired in a controlled lab environ-
Early databases of facial expressions such as JAFFE [7],
ment. However, studies show that posed expressions can
Cohn-Kanade [8], [9], MMI [10], and MultiPie [11] were
be different from unposed expressions in configuration,
captured in a lab-controlled environment where the sub-
intensity, and timing [12], [13]. Some researchers captured
jects portrayed different facial expressions. This approach
unposed facial behavior while the subject is watching a
resultedinacleanandhigh-qualitydatabaseofposedfacial
shortvideo[14],[15],engagedinlaboratory-basedemotion
expressions. However, posed expressions may differ from
inducingtasks[16],orinteractedwithacomputer-mediated
daily life unposed (aka spontaneous) facial expressions.
tutoring system [17]. Although a large number of frames
Thus, capturing spontaneous expression became a trend in
can be obtained by these approaches, the diversity of these
theaffectivecomputingcommunity.Examplesoftheseenvi-
databases is limited due to the number of subjects, head
ronments are recording the responses of participants’ faces
position,andenvironmentalconditions.
whilewatchingastimuli(e.g.,DISFA[14],AM-FED[15])or
Recently,databasesoffacialexpressionandaffectinthe
performing laboratory-based emotion inducing tasks (e.g.,
wild received much attention. These databases are either
Belfast [16]). These databases often capture multi-modal
captured from movies or the Internet, and annotated with
affects such as voice, biological signals, etc. and usually a
categorical model [18], [19], [20], dimensional model [21],
seriesofframesarecapturedthatenableresearcherstowork
andFACSmodel[22].However,theyonlycoveronemodel
ontemporalanddynamicaspectsofexpressions.However,
of affect, have a limited number of subjects, or contain few
thediversityofthesedatabasesislimitedduetothenumber
samples of certain emotions such as disgust. Therefore, a
of subjects, head pose variation, and environmental condi-
large database, with a large amount of subject variations
tions.
in the wild condition that covers multiple models of affect
Hence there is a demand to develop systems that
(especiallythedimensionalmodel)isaneed.
are based on natural, unposed facial expressions. To ad-
To address this need, we created a database of facial
dress this demand, recently researchers paid attention to
Affect from the InterNet (called AffectNet) by querying
databases in the wild. Dhall et al. [18] released Acted Facial
different search engines (Google, Bing, and Yahoo) using
Expressions in the Wild (AFEW) from 54 movies by a rec-
1250emotionrelatedtagsinsixdifferentlanguages(English,
ommender system based on subtitles. The video clips were
Spanish, Portuguese, German, Arabic, and Farsi). AffectNet
annotated with six basic expressions plus neutral. AFEW
contains more than one million images with faces and ex-
tractedfaciallandmarkpoints.Twelvehumanexpertsman-
1.InterestedresearchercandownloadacopyofAffectNetfrom:http:
ually annotated 450,000 of these images in both categorical //mohammadmahoor.com/databases-codes/IEEETRANSACTIONSONAFFECTIVECOMPUTING 3
TABLE1
TheSummaryandCharacteristicsofReviewedDatabasesinAffectRecognition
Database Databaseinformation #ofSubjects Condition AffectModeling
-Controlled -30AUs
CK+[9] -Frontaland30degreeimages -123
-Posed -7emotioncategories
-Around750,000images -Controlled
MultiPie[11] -337 -7emotioncategories
-Undermultipleviewpointsandilluminations -Posed
-Controlled
-Subjectsportrayed79seriesoffacialexpressions -31AUs
MMI[10] -25 -Posed
-Imagesequenceoffrontalandsideviewarecaptured -Sixbasicexpression
&Spontaneous
-Videoofsubjectswhilewatchingafourminutesvideo -Controlled
DISFA[14] -27 -12AUs
-Cliparerecordedbyastereocamera -Spontaneous
-SAL -Valence
-Controlled
SALDB[23],[24] -Audiovisual(facialexpression,shoulder,audiocues) -4 -Quantized[23]
-Spontaneous
-20facialfeaturepoints,5shoulderpointsforvideo -Continuous[24]
-Valenceandarousal
-Controlled
RELOCA[25] -Multi-modalaudio,video,ECGandEDA -46 (continuous)
-Spontaneous
-Selfassessment
AM-FED[15] -242facialvideos -242 -Spontaneous -14AUs
-Valenceandarousal
-40one-minutelongvideosshowntosubjects -Controlled
DEAP[26] -32 (continuous)
-EEGsignalsrecorded -Spontaneous
-Selfassessment
AFEW[18] -Videos -330 -Wild -7emotioncategories
FER-2013[19] -Imagesqueriedfromweb -∼35,887 -Wild -7emotioncategories
-Imagesqueriedfromweb -12AUsannotated
EmotioNet[22] -100,000imagesannotatedmanually -∼100,000 -Wild -23emotioncategories
-900,000imagesannotatedautomatically basedonAUs
-Valenceandarousal
Aff-Wild[21] -500videosfromYouTube -500 -Wild
(continuous)
FER-Wild[20] -24,000imagesfromweb -∼24,000 -Wild -7emotioncategories
-8emotioncategories
AffectNet -1,000,000imageswithfaciallandmarks
-∼450,000 -Wild -Valenceandarousal
(Thiswork) -450,000imagesannotatedmanually
(continuous)
contains 330 subjects aged 1-77 years and addresses the ent illumination and contrast. The database was annotated
issue of temporal facial expressions in the wild. A static frame-by-frame for the presence of 14 FACS action units,
subset (SFEW [27]) is created by selecting some frames head movements, and automatically detected landmark
of AFEW. SFEW covers unconstrained facial expressions, points.AM-FEDisagreatresourcetolearnAUsinthewild.
differentheadposes,agerange,occlusions,andclosetoreal However,thereisnotahugevarianceinheadpose(limited
world illuminations. However, it contains only 700 images, profiles),andthereareonlyafewsubjectsinthedatabase.
andthereareonly95subjectsinthedatabase.
TheFER-Wild[20]databasecontains24,000imagesthat
The Facial Expression Recognition 2013 (FER-2013) are obtained by querying emotion-related terms from three
database was introduced in the ICML 2013 Challenges in search engines. The OpenCV face recognition was used to
Representation Learning [19]. The database was created detect faces in the images, and 66 landmark points were
usingtheGoogleimagesearchAPIthatmatchedasetof184 found using Active Appearance Model (AAM) [28] and a
emotion-related keywords to capture the six basic expres- face alignment algorithm via regression local binary fea-
sionsaswellastheneutralexpression.Imageswereresized tures [29], [30]. Two human labelers annotated the images
to48x48pixelsandconvertedtograyscale.Humanlabelers intosixbasicexpressionsandneutral.ComparingwithFER-
rejected incorrectly labeled images, corrected the cropping 2013,FER-Wildimageshaveahigherresolutionwithfacial
if necessary, and filtered out some duplicate images. The landmarkpointsnecessarytoregistertheimages.However,
resulting database contains 35,887 images most of which stillafewsamplesportraysomeexpressionssuchasdisgust
are in the wild settings. FER-2013 is currently the biggest andfearandonlythecategoricalmodelofaffectisprovided
publiclyavailablefacialexpressiondatabaseinthewildset- withFER-Wild.
tings, enabling many researchers to train machine learning
The EmotioNet [22] consists of one million images of fa-
methods such as Deep Neural Networks (DNNs) where
cial expressions downloaded from the Internet by selecting
largeamountsofdataareneeded.InFER-2013,thefacesare
all the words derived from the word “feeling” in Word-
not registered, a small number of images portray disgust
Net[31].Facedetector[32]wasusedtodetectfacesinthese
(547 images), and unfortunately most of facial landmark
images and the authors visually inspected the resultant
detectors fail to extract facial landmarks at this resolution
images. These images were then automatically annotated
andquality.Inaddition,onlythecategoricalmodelofaffect
with AUs and AU intensities by an approach based on
isprovidedwithFER-2013.
Kernel Subclass Discriminant Analysis (KSDA) [33]. The
The Affectiva-MIT Facial Expression Dataset (AM-FED) KSDA-based approach was trained with Gabor features
database [15] contains 242 facial videos (160K frames) of centered on facial landmark with a Radial Basis Function
people watching Super Bowl commercials using their we- (RBF) kernel. Images were labeled as one of the 23 (basic
bcam. The recording conditions were arbitrary with differ- or compound) emotion categories defined in [6] based onIEEETRANSACTIONSONAFFECTIVECOMPUTING 4
AUs.Forexample,ifanimagehasbeenannotatedashaving subjectsandthevideos werecapturedinthelabcontrolled
AUs 1, 2, 12 and 25, it is labeled as happily surprised. A settings.
totalof100,000images(10%ofthedatabase)weremanually Audio-Visual Emotion recognition Challenge (AVEC)
annotated with AUs by experienced coders. The proposed series of competitions [36], [37], [38], [39], [40], [41] pro-
AUdetectionapproachwastrainedonCK+[9],DISFA[14], vided a benchmark of automatic audio, video and audio-
andCFEE[34]databases,andtheaccuracyoftheautomated visual emotion analysis in continuous affect recognition.
annotatedAUswasreportedabout80%onthemanuallyan- AVEC 2011, 2012, 2013, and 2014 used videos from the
notatedset.EmotioNetisanovelresourceofFACSmodelin SEMAINE [42] database videos. Each video is annotated
thewildwithalargeamountofsubjectvariation.However, by a single rater for every dimension using a two-axis
it lacks the dimensional model of affect, and the emotion joystick.AVEC2015and2016usedtheRECOLAbenchmark
categories are defined based on annotated AUs and not intheircompetitions.Variouscontinuousaffectrecognition
manuallylabeled. dimensions were explored in each challenge year such as
On the other hand, some researchers developed valence,arousal,expectation,power,anddominance,where
databases of the dimensional model in the continuous the prediction of valence and arousal are studied in all
domain. These databases, however, are limited since the challenges.
annotationofcontinuousdimensionsismoreexpensiveand TheAff-WildDatabase[21]isbyfarthelargestdatabase
necessitatetrainedannotators.Examplesofthesedatabases formeasuringcontinuousaffectinthevalence-arousalspace
are Belfast [16], RECOLA [25], Affectiva-MIT Facial Expres- “in-the-wild”. More than 500 videos from YouTube were
sionDataset(AM-FED)[15],andrecentlypublishedAff-Wild collected. Subjects in the videos displayed a number of
Database [21] which is the only database of dimensional spontaneous emotions while watching a particular video,
modelinthewild. performing an activity, and reacting to a practical joke.
The Belfast database [16] contains recordings (5s to The videos have been annotated frame-by-frame by three
60s in length) of mild to moderate emotional responses humanraters,utilizingajoystick-basedtooltoratevalence
of 60 participants to a series of laboratory-based emotion and arousal. Aff-Wild is a great database of dimensional
inducing tasks (e.g., surprise response by setting off a modeling in the wild that considers the temporal changes
loud noise when the participant is asked to find some- oftheaffect,however,ithasasmallsubjectvariance,i.e.,it
thing in a black box). The recordings were labeled by onlycontains500subjects.
information on self-report of emotion, the gender of the
Table 1 summarizes the characteristics of the reviewed
participant/experimenter,andthevalenceinthecontinuous
databases in all three models of affect, i.e., categorical
domain.ThearousaldimensionwasnotannotatedinBelfast
model, dimensional model, and Facial Action Coding Sys-
database. While the portrayed emotions are natural and
tem(FACS).
spontaneous, the tasks have taken place in a relatively
artificial setting of a laboratory where there was a control
onlightingconditions,headposes,etc.
2.2 EvaluationMetrics
The Database for Emotion Analysis using Physiological Sig-
nals(DEAP)[26]consistsofspontaneousreactionsof32par-
There are various evaluation metrics in the literature to
ticipants in response to one-minute long music video clip.
measure the reliability of annotation and automated affec-
The EEG, peripheral physiological signals, and frontal face
tive computing systems. Accuracy, F1-score [49], Cohens
videos of participants were recorded, and the participants
kappa [50], Krippendorfs Alpha [51], ICC [52], area under
rated each video in terms of valence, arousal, like/dislike,
the ROC curve (AUC), and area under Precision-Recall
dominance, and familiarity. Correlations between the EEG
curve (AUC-PR) [53] are well-defined widely used metrics
signal frequencies and the participants ratings were inves-
for evaluation of the categorical and FACS-based models.
tigated, and three different modalities, i.e., EEG signals,
Since, the dimensional model of affect is usually evaluated
peripheral physiological signals, and multimedia features
in a continuous domain, different evaluation metrics are
onvideoclips(suchaslightingkey,colorvariance,etc.)were
necessary. In the following, we review several metrics that
used for binary classification of low/high arousal, valence,
are used in the literature for evaluation of dimensional
andliking.DEAPisagreatdatabasetostudytherelationof
model.
biological signals and dimensional affect, however, it has
Root Mean Square Error (RMSE) is the most common
only a few subjects and the videos are captured in lab
evaluationmetricinacontinuousdomainwhichisdefined
controlledsettings.
as:
The RECOLA benchmark [25] contains videos of 23 (cid:118)
d coy na fd ei rc ente ca em cs om(46 plp eta ir nt gicip aan tat ss k) th wa ht icp hart ri ec qip ua irt ee dd i cn ola lav bi od re ao - RMSE =(cid:117) (cid:117) (cid:116) n1 (cid:88)n (θˆ i−θ i)2 (1)
tion. Different multi-modal data of the first five minutes i=1
of interaction, i.e., audio, video, ECG and EDA) were
recorded continuously and synchronously. Six annotators where θˆ i and θ i are the prediction and the ground truth
measured arousal and valence. The participants reported of ith sample, and n is the number of samples in the
their arousal and valence through the Self-Assessment evaluation set. RMSE-based evaluation can heavily weigh
Manikin(SAM)[35]questionnairebeforeandafterthetask. theoutliers[54],anditisnotabletoprovidethecovariance
RECOLAisagreatdatabaseofthedimensionalmodelwith of prediction and ground-truth to show how they change
multiple cues and modalities, however, it contains only 46 with respect to each other. Pearsons correlation coefficientIEEETRANSACTIONSONAFFECTIVECOMPUTING 5
TABLE2
State-of-the-artAlgorithmsandTheirPerformanceontheDatabasesListedinTable1.
Work Database Method Results
Mollahosseini CK+ -InceptionbasedConvolutionalNeuralNetwork(CNN) -93.2%accuracyonCK+
etal.[43] MultiPie -Subject-independentandcross-databaseexperiments -94.7%accuracyonMultiPie
-DifferentSVMkernelstrainedwithLBPfeatures
Shanetal.[44] MMI -86.9%accuracyonMMI
-Subject-independentandcross-databaseexperiments
Zhangetal.[45] DISFA
-lpnormmulti-taskmultiplekernellearning -0.70F1-scoreonDISFA
-learningsharedkernelsfromagivensetofbasekernels -0.93recognitionrateonDISFA
-Leave-one-sequence-out
-BidirectionalLSTM
Nicolaou -BLSTM-NNoutperformSVR
SALDB -Trainedonmultipleengineeredfeaturesextracted
etal.[24] -Valence(RMSE=0.15andCC=0.796)
fromaudio,facialgeometry,andshoulder
-Arousal(RMSE=0.21andCC=0.642)
-MultiplestackofbidirectionalLSTM(DBLSTM-RNN) -WinnerofAVEC2015challenge
Heetal.[46] RECOLA -Trainedonengineeredfeaturesextractedfromaudio(LLDs), -Valence(RMSE=0.104andCC=0.616)
video(LPQ-TOP),52ECGfeatures,and22EDAfeatures -Arousal(RMSE=0.121andCC=0.753)
-HOGfeaturesextracted -AUC0.90,0.72and0.70forsmile,
McDuffetal.[15] AM-FED
-SVMwithRBFkernel AU2andAU4respectively
-GaussiannaiveBayesclassifier -0.39F1-scoreonArousal
Koelstraetal.[26] DEAP -EEG,physiologicalsignals,andmultimediafeatures -0.37F1-scoreonValence
-Binaryclassificationoflow/higharousal,valence,andliking -0.40F1-scoreonLiking
-Trainedonbothvideoandaudio.
-WinnerofEmotiW2016challenge
Fanetal.[47] AFEW -VGGnetworkarefollowedbyLSTMsandcombinedwith
-56.16%accuracyonAFEW
3Dconvolution
-WinneroftheFERchallenge
Tangetal.[48] FER-2013 -CNNwithlinearone-vs-allSVMatthetop
-71.2%accuracyontestset
-NewfacefeatureextractionmethodusingGaborfilters
Benitez-Quiroz
EmotioNet -KSDAclassification -∼80%AUdetectiononEmotioNet
etal.[22]
-Subject-independentandcross-databaseexperiments
Mollahosseini -TrainedonAlexNet
FER-Wild -82.12%accuracyonFER-Wild
etal.[20] -Noiseestimationmethodsused
is therefore proposed in some literature [24], [36], [37] to The above discussed metrics are used to evaluate the
overcomethislimitation: categoricalanddimensionalbaselinesonAffectNetinSec.4.
COV{θˆ,θ} E[(θˆ−µ )(θ−µ )]
CC = =
θˆ θ
(2)
σ σ σ σ 2.3 ExistingAlgorithms
θˆ θ θˆ θ
Concordance Correlation Coefficient (CCC) is another Affective computing is now a well-established field, and
metric [40], [41] which combines the Pearsons correlation therearemanyalgorithmsanddatabasesfordevelopingau-
coefficient (CC) with the square difference between the tomatedaffectperceptionsystems.Sinceitisnotpossibleto
meansoftwocomparedtimeseries: includeallthosegreatworks,weonlygiveabriefoverview
and cover the state-of-the-art methods that are applied on
2ρσ σ
ρ c =
σ2+σ2+(θˆ µθ
−µ )2 (3) thedatabasesexplainedinSec.2.1.
θˆ θ θˆ θ Conventional algorithms of affective computing from
whereρisthePearsoncorrelationcoefficient(CC)between facesusehand-craftedfeaturessuchaspixelintensities[55],
two time-series (e.g., prediction and ground-truth), σ2 and Gabor filters [56], Local Binary Patterns (LBP) [44], and
θˆ
σ θ2 are the variance of each time series, and µ θˆ and µ θ are Histogram of Oriented Gradients (HOG) [14]. These hand-
themeanvalueofeach.UnlikeCC,thepredictionsthatare crafted features often lack enough generalizability in the
well correlated with the ground-truth but shifted in value wild settings where there is a high variation in scene light-
arepenalizedinproportiontothedeviationinCCC. ing, camera view, image resolution, background, subjects
The value of valence and arousal are [-1,+1] and their headposeandethnicity.
signsareessentialinmanyemotion-predictionapplications. AnalternativeapproachistouseDeepNeuralNetworks
Forexample,iftheground-truthvalenceis+0.3,prediction (DNN) to learn the most appropriate feature abstractions
of +0.7 is far better than prediction of -0.1, since +0.7 indi- directly from the data and handle the limitations of hand-
catesapositiveemotionsimilartotheground-truth(despite crafted features. DNNs have been a recent successful ap-
both predictions have the same RMSE). Sign Agreement proachinvisualobjectrecognition[57],humanposeestima-
Metric (SAGR) is another metric that is proposed in [24] tion[58],faceverification[59]andmanymore.Thissuccess
to evaluate the performance of a valence and arousal pre- is mainly due to the availability of computing power and
dictionsystem.SAGRisdefinedas: existing big databases that allow DNNs to extract highly
n discriminative features from the data samples. There have
SAGR= n1 (cid:88) δ(sign(θˆ i),sign(θ i)) (4) been enormous attempts on using DNNs in automated
facial expression recognition and affective computing [20],
i=1
whereδ istheKroneckerdeltafunction,definedas: [43],[46],[47],[48]thatareespeciallyverysuccessfulinthe
(cid:40) wildsettings.
1, a=b Table 2 shows a list of the state-of-the-art algorithms
δ(a,b)= (5)
0, a(cid:54)=b and their performance on the databases listed in Table 1.IEEETRANSACTIONSONAFFECTIVECOMPUTING 6
Fig. 2. A screen-shot of the software application used to annotate categorical and dimensional (valence and arousal) models of affect and the
osculationtagifexisting.Onlyonedetectedfaceineachimageisannotated(showninthegreenboundingbox).
As shown in the table, the majority of these approaches neutral facial images, no individual query was performed
have used DNNs to learn a better representation of affect, toobtainadditionalneutralface.
especiallyinthewildsettings.Evensomeoftheapproaches, Three search engines (Google, Bing, and Yahoo) were
suchasthewinneroftheAVEC2015challenge[46],trained queried with these 1250 emotion related tags. Other search
a DNN with hand-crafted features and still could improve engines such as Baidu and Yandex were considered. How-
thepredictionaccuracy. ever, they either did not produce a large number of facial
images with intended expressions or they did not have
3 AFFECTNET available APIs for automatically querying and pulling im-
age URLs into the database. Additionally, queries were
AffectNet(AffectfromtheInterNet)isthelargestdatabase
combined with negative terms (e.g., “drawing”, “cartoon”,
of the categorical and dimensional models of affect in the
“animation”, “birthday”, etc.) to avoid non-human objects
wild(asshowninTable1).Thedatabaseiscreatedbyquery-
as much as possible. Furthermore, since the images of
ing emotion related keywords from three search engines
stock photo websites are posed unnaturally and contain
and annotated by expert human labelers. In this section,
watermarks mostly, a list of popular stock photo websites
the process of querying the Internet, processing facial im-
wascompiledandtheresultsreturnedfromthestockphoto
ages and extracting facial landmarks, and annotating facial
websiteswerefilteredout.
expression,valence,andarousalofaffectarediscussed.
A total of ∼1,800,000 distinct URLs returned for each
querywerestoredinthedatabase.TheOpenCVfacerecog-
3.1 FacialImagesfromtheWeb nition was used to obtain bounding boxes around each
Emotion-related keywords were combined with words re- face. A face alignment algorithm via regression local bi-
latedtogender,age,orethnicity,toobtainnearly362strings nary features [29], [30] was used to extract 66 facial land-
intheEnglishlanguagesuchas“joyfulgirl”,“blissfulSpan- mark points. The facial landmark localization technique
ishman”,“furiousyounglady”,“astonishedsenior”.These wastrainedusingtheannotationsprovidedfromthe300W
keywords are then translated into five other languages: competition [60]. More than 1M images containing at least
Spanish, Portuguese, German, Arabic and Farsi. The direct onefacewithextractedfaciallandmarkpointswerekeptfor
translation of queries in English to other languages did furtherprocessing.
not accurately result in the intended emotions since each The average image resolution of faces in AffectNet are
language and culture has differing words and expressions 425×425withSTDof349×349pixels.WeusedMicrosoft
fordifferentemotions.Therefore,thelistofEnglishqueries cognitivefaceAPItoextractthesefacialattributeson50,000
was provided to native non-English speakers who were randomly selected images from the database. According
proficient in English, and they created a list of queries to MS face API, 49% of the faces are men. The average
for each emotion in their native language and inspected estimated age of the faces is 33.01 years with the standard
the quality of the results visually. The criteria for high- deviationof16.96years.Inparticular,10.85,3.9,30.19,26.86,
quality queries were those that returned a high percentage 14.46,and13.75percentofthefacesareinageranges[0,10),
of human faces showing the intended queried emotions [10,20),[20,30),[30,40),[40,50)and[50,-),respectively.MS
rather than drawings, graphics, or non-human objects. A face API detected forehead, mouth, and eye occlusions in
totalof1250searchquerieswerecompiledandusedtocrawl 4.5, 1.08, and 0.49 percent of the images, respectively. Also,
thesearchenginesinourdatabase.Sinceahighpercentage 9.63%ofthefaceswearglasses,51.07and41.4%ofthefaces
of results returned by our query terms already contained have eye and lip make-ups, respectively. In terms of headIEEETRANSACTIONSONAFFECTIVECOMPUTING 7
pose,theaverageestimatedpitch,yaw,rollare0.0,-0.7,and TABLE3
-1.19degrees,respectively. NumberofAnnotatedImagesinEachCategory
Expression Number
3.2 Annotation Neutral 80,276
Happy 146,198
Crowd-sourcing serviceslike Amazon Mechanical Turkare Sad 29,487
fast,cheapandeasyapproachesforlabelinglargedatabases. Surprise 16,288
Thequalityoflabelsobtainedfromcrowd-sourcingservices, Fear 8,191
Disgust 5,264
however,variesconsiderablyamongtheannotators.Dueto
Anger 28,130
these issues and the fact that annotating the valence and Contempt 5,135
arousal requires a deep understanding of the concept, we None 35,322
avoidedcrowd-sourcingfacilitiesandinsteadhired12full- Uncertain 13,163
Non-Face 88,895
time and part-time annotators at the University of Denver
to label the database. A total of 450,000 images were given
to these expert annotators to label the face in the images TABLE4
PercentageofAnnotatedCategoriesforQueriedEmotionTerms(%)
into both discrete categorical and continuous dimensional
(valence and arousal) models. Due to time and budget QueryExpression
constraintseachimagewasannotatedbyoneannotator. HA SA SU FE DI AN CO
A software application was developed to annotate the
categoricalanddimensional(valenceandarousal)modelsof
affect.Figure2showsascreen-shotoftheannotationappli-
cation.Acomprehensivetutorialincludingthedefinitionof
thecategoricalanddimensionalmodelsofaffectwithsome
examples of each category, valence and arousal was given
totheannotators. Threetrainingsessionswereprovidedto
eachannotator,inwhichtheannotatorlabeledtheemotion
category,valenceandarousalof200imagesandtheresults
werereviewedwiththeannotators.Necessaryfeedbackwas
given on both the categorical and dimensional labels. In
addition, the annotators tagged the images that have any
occlusion on the face. The occlusion criterion was defined
asifanypartofthefacewasnotvisible.Ifthepersoninthe
imagesworeglasses,buttheeyeswerevisiblewithoutany
shadow,itwasnotconsideredasocclusion.
3.2.1 CategoricalModelAnnotation
Eleven discrete categories were defined in the categorical
model of AffectNet as: Neutral, Happy, Sad, Surprise, Fear,
Anger, Disgust, Contempt, None, Uncertain, and Non-face.
The None (“None of the eight emotions”) category is the
type of expression/emotions (such as sleepy, bored, tired,
seducing, confuse, shame, focused, etc.) that could not be
assigned by annotators to any of the six basic emotions,
contempt or neutral. However, valence and arousal could
be assigned to these images. The Non-face category was
definedasimagesthat:1)Donotcontainafaceintheimage;
2) Contain a watermark on the face; 3) The face detection
algorithmfailsandtheboundingboxisnotaroundtheface;
4) The face is a drawing, animation, or painted; and 5) The
face is distorted beyond a natural or normal shape, even
if an expression could be inferred. If the annotators were
uncertain about any of the facial expressions, images were
taggedasuncertain.WhenanimagewasannotatedasNon-
face or uncertain, valence and arousal were not assigned to
theimage.
The annotators were instructed to select the proper
expression category of the face, where the intensity is not
importantaslongasthefacedepictstheintendedemotion.
Table 3 shows the number of images in each category.
Table 4 indicates the percentage of annotated categories
for queried emotion terms. As shown, the happy emotion
noisserpxEdetatonnA
NE* 17.3 16.3 13.9 17.8 17.8 16.1 20.1
HA 48.9 27.2 30.4 28.6 33 29.5 30.1
SA 2.6 15.7 4.8 5.8 4.5 5.4 4.6
SU 2.7 3.1 16 4.4 3.6 3.4 4.1
FE 0.7 1.2 4.2 4 1.5 1.4 1.3
DI 0.6 0.7 0.7 0.9 2.7 1.1 1
AN 2.8 4.5 3.8 5.6 6 12.2 6.1
CO 1.3 0.9 0.4 1.1 1.1 1.2 2.4
NO 5.4 8.7 4.8 8.1 8.8 9.3 11.2
UN 1.3 3.1 4.3 3.1 4.1 3.7 2.7
NF 16.3 18.6 16.7 20.6 16.9 16.8 16.3
*NE,HA,SA,SU,FE,DI,AN,CO,NO,UN,andNFstandforNeutral,
Happy,Sad,Surprise,Fear,Anger,Disgust,Contempt,None,Uncertain,
andNon-facecategories,respectively.
had the highest hit-rate (48%), and the rest of the emotions
had hit-rates less than 20%. About 15% of all query results
were in the No-Face category, as many images from the
web contain watermarks, drawings, etc. About 15% of all
queried emotions resulted in neutral faces. Among other
expressions,disgust,fear,andcontempthadthelowesthit-
ratewithonly2.7%,4%,and2.4%hit-rates,respectively.As
one can see, the majority of the returned images from the
searchengineswerehappyorneutralfaces.Theauthorsbe-
lievethatthisisbecausepeopletendtopublishtheirimages
with positive expressions rather than negative expressions.
Figure 3 shows a sample image in each category and its
intendedqueries(inparentheses).
3.2.2 Dimensional(Valence&Arousal)Annotation
The definition of valence and arousal dimensions was
adaptedfrom[3]andwasgiventoannotatorsinourtutorial
as: “Valence refers to how positive or negative an event is,
and arousal reflects whether an event is exciting/agitating
or calm/soothing”. A sample circumplex with estimated
positions of several expressions, borrowed from [61], was
providedinthetutorialasareferencefortheannotators.The
providedcircumplexinthetutorialcontainedmorethan34
complex emotions categories such as suspicious, insulted,
impressed,etc.,andusedtotrainannotators.Theannotators
were instructed to consider the intensity of valence and
arousal during the annotation. During the annotation pro-
cess, the annotators were supervised closely and constant
necessaryfeedbackwasprovidedwhentheywereuncertain
aboutsomeimages.IEEETRANSACTIONSONAFFECTIVECOMPUTING 8
1
10000
0.8
3162
0.6
1000
0.4
Neutral(Angry) Happy(Happy) Sad(Angry) Surprise(Fear) 0.2 316
0 100
−0.2
32
−0.4
10
−0.6
Fear(Fear) Disgust(Disgust) Angry(Angry) Contempt(Happy)
3
−0.8
−1 1
−1 −0.5 0 0.5 1
Fig.4.Histogram(numberofframesineachrange/area)ofvalenceand
arousalannotations(Bestviewedincolor).
Non-face(Surprise) Uncertain(Sad) None(Fear) None(Happy)
TABLE5
Fig.3. Samplesofqueriedimagesfromthewebandtheirannotated
Annotators’AgreementinDimensionalModelofAffect
tags.Thequeriedexpressioniswritteninparentheses.
SameCategory All
Valence Arousal Valence Arousal
RMSE 0.190 0.261 0.340 0.362
Tomodelthedimensionalaffectofvalenceandarousal,a CORR 0.951 0.766 0.823 0.567
2D Cartesian coordinate system was used where the x-axis SAGR 0.906 0.709 0.815 0.667
CCC 0.951 0.746 0.821 0.551
and y-axis represent the valence and arousal, respectively.
SimilartoRussell’scircumplexspacemodel[3],ourannota-
tionsoftwaredidnotallowthevalueofvalenceandarousal
3.3 AnnotationAgreement
outside of the circumplex. This allows us to convert the
Cartesian coordinates to polar coordinates with 0 ≤ r ≤ 1 Inordertomeasuretheagreementbetweentheannotators,
and0≤θ <360.Theannotationsoftwareshowedthevalue 36,000 images were annotated by two annotators. The an-
ofvalenceandarousaltotheannotatorswhentheyselected notations were performed fully blind and independently,
apointinthecircumplex.Thishelpedtheannotatorstopick i.e., the annotators were not aware of the intended query
morepreciselocationsofvalenceandarousalwithahigher or other annotator’s response. The results showed that the
confidence. annotators agreed on 60.7% of the images. Table 6 shows
the agreement between two annotators for different cate-
A predefined estimated region of valence and arousal
gories. As it is shown, the annotators highly agreed on the
was defined for each categorical emotion in the annotation
HappyandNoFacecategories,andthehighestdisagreement
software(e.g.,forhappyemotionthevalenceisin(0.0,1.0],
occurred in the None category. Visually inspecting some of
and the arousal is in [-0.2, 0.5] ). If the annotators select
theimagesintheNonecategory,theauthorsbelievethatthe
avalueofvalenceandarousaloutsideoftheselectedemo-
images in this category contain very subtle emotions and
tion’sregion,thesoftwareindicatesawarningmessage.The
they can be easily confused with other categories (the last
annotators were able to proceed, and they were instructed
twoexampleofFig.3showimagesintheNonecategory).
to do so, if they were confident about the value of valence
Table 5 shows various evaluation metrics between the
and arousal. The images with the warning messages were
two annotators in the continuous dimensional model of
marked in the database, for further review by the authors.
affect. These metrics are defined in Sec. 2.2. We calculated
This helped to avoid mistakes in the annotation of the
these metrics in two scenarios: 1) the annotators agreed on
dimensionalmodelofaffect.
thecategoryoftheimage;2)onallimagesthatareannotated
Figure 4 shows the histogram (number of samples in
by two annotators. As Table 5 shows, when the annotators
each range/area) of annotated images in a 2D Cartesian
agreedonthecategoryoftheimage,theannotationshavea
coordinate system. As illustrated, there are more samples
high correlation and sign agreement (SAGR). According to
in the center and the right middle (positive valence and
Table6,thisoccurredononly60.7%images.However,there
small positive arousal) of the circumplex, which confirms
is less correlation and SAGR on overall images, since the
the higher number of Neutral and Happy images in the
annotatorshadadifferentperceptionofemotionsexpressed
database compared to other categories in the categorical
intheimages.Itcanalsobeseenthattheannotatorsagreed
model.2
on valence more than arousal. The authors believe that
this is because the perception of valence (how positive or
negative the emotion is) is easier and less subjective than
arousal(howexcitedorcalmthesubjectis)especiallyinstill
2.A numerical representation of annotated images in each
range/areaofvalenceandarousalisprovidedintheAppendix. images. Comparing the metrics in the existing dimensionalIEEETRANSACTIONSONAFFECTIVECOMPUTING 9
TABLE6
AgreementBetweenTwoAnnotatorsinCategoricalModelofAffect(%)
Neutral Happy Sad Surprise Fear Disgust Anger Contempt None Uncertain Non-Face
Neutral 50.8 7.0 9.1 2.8 1.1 1.0 4.8 5.3 11.1 1.9 5.1
Happy 6.3 79.6 0.6 1.7 0.3 0.4 0.5 3.0 4.6 1.0 2.2
Sad 11.8 0.9 69.7 1.2 3.4 1.3 4.0 0.3 3.5 1.2 2.6
Surprise 2.0 3.8 1.6 66.5 14.0 0.8 1.9 0.6 4.2 1.9 2.7
Fear 3.1 1.5 3.8 15.3 61.1 2.5 7.2 0.0 1.9 0.4 3.3
Disgust 1.5 0.8 3.6 1.2 3.5 67.6 13.1 1.7 2.7 2.3 2.1
Anger 8.1 1.2 7.5 1.7 2.9 4.4 62.3 1.3 5.5 1.9 3.3
Contempt 10.2 7.5 2.1 0.5 0.5 4.4 2.1 66.9 3.7 1.5 0.6
None 22.6 12.0 14.5 8.0 6.0 2.3 16.9 1.3 9.6 4.3 2.6
Uncertain 13.5 12.1 7.8 7.3 4.0 4.5 6.2 2.6 12.3 20.6 8.9
Non-Face 3.7 3.8 1.7 1.1 0.9 0.4 1.7 0.4 1.2 1.4 83.9
databases(showninTable2)withtheagreementofhuman 4.2 CategoricalModelBaseline
labelers on AffectNet, suggest that AffectNet is a very chal-
lenging database and even human annotations have more Facial expression data is usually highly skewed. This form
RMSEthanautomatedmethodsonexistingdatabases. of imbalance is commonly referred to as intrinsic variation,
i.e.,itisadirectresultofthenatureofexpressionsinthereal
world.Thishappensinboththecategoricalanddimensional
4 BASELINE
models of affect. For instance, Caridakis et al. [64] reported
Inthissection,twobaselinesareproposedtoclassifyimages that a bias toward quadrant 1 (positive arousal, positive
inthecategoricalmodelandpredictthevalueofvalenceand valence)existsintheSALdatabase.Theproblemoflearning
arousal in the continuous domain of dimensional model. fromimbalanceddatasetshastwochallenges.First,training
Since deep Convolutional Neural Networks (CNNs) have data with an imbalanced distribution often causes learning
beenasuccessfulapproachtolearnappropriatefeatureab- algorithms to perform poorly on the minority class [65].
stractionsdirectlyfromtheimageandtherearemanysam- Second, the imbalance in the test/validation data distribu-
plesinAffectNetnecessarytotrainCNNs,weproposedtwo tioncanaffecttheperformancemetricsdramatically.Jeniet
simpleCNNbaselinesforbothcategoricalanddimensional al.[53]studiedtheinfluenceofskewonimbalancedvalida-
models. We also compared the proposed baselines with tionset.Thestudyshowedthatwithexceptionofareaunder
conventionalapproaches(SupportVectorMachines[62]and theROCcurve(AUC),allotherstudiedevaluationmetrics,
SupportVectorRegressions[63])learnedfromhand-crafted i.e., Accuracy, F1-score, Cohens kappa [50], Krippendorfs
features(HOG).Inthefollowingsections,wefirstintroduce Alpha [51], and area under Precision-Recall curve (AUC-
our training, validation and test sets, and then show the PR)areaffectedbyskeweddistributionsdramatically.While
performanceofeachproposedbaselines. AUC is unaffected by skew, precision-recall curves sug-
gested that AUC may mask poor performance. To avoid or
4.1 Test,Validation,andTrainingSets minimize skew-biased estimates of performance, the study
suggested to report both skew-normalized scores and the
Test set: The subset of the annotated images that are an-
originalevaluation.
notated by two annotators is reserved for the test set. To
determine the value of valence and arousal in the test set, We used AlexNet [57] architecture as our deep CNN
since there are two responses for one image in the contin- baseline. AlexNet consists of five convolution layers, fol-
uous domain, one of the annotations is picked randomly. lowed by max-pooling and normalization layers, and three
To select the category of image in the categorical model, if fully-connected layers. To train our baseline with an im-
therewasadisagreement,afavorwasgiventotheintended balanced training set, four approaches are studied in this
query,i.e.,ifoneoftheannotatorslabeledtheimageasthe paper as Imbalanced learning, Down-Sampling, Up-Sampling,
intended query, the image was labeled with the intended and Weighted-Loss. The imbalanced learning approach was
queryinthetestset.Thishappenedin29.5%oftheimages trainedwiththeimbalancedtrainingsetwithoutanychange
with disagreement between the annotators. On the rest of in the skew of the dataset. To train the down-sampling
the images with disagreement, one of the annotations was approach, we selected a maximum of 15,000 samples from
assigned to the image randomly. Since the test set is a eachclass.Sincetherearelessthan15,000samplesforsome
random sampling of all images, it is heavily imbalanced. classes such as Disgust, Contempt, and Fear, the resulting
In other words, there are more than 11,000 images with training set is semi-balanced. To train the up-sampling
happy expression while it contains only 1,000 images with approach, we heavily up-sampled the under-represented
contemptuousexpression. classes by replicating their samples so that all classes had
Validation set: Five hundred samples of each category the same number of samples as the class with maximum
is selected randomly as a validation set. The validation set samples,i.e.,Happyclass.
isusedforhyper-parametertuning,andsinceitisbalanced, The weighted-loss approach weighted the loss function
thereisnoneedforanyskewnormalization. for each of the classes by their relative proportion in the
Training set:Therestofimagesareconsideredastrain- training dataset. In other words, the loss function heavily
ing examples. The training examples, as shown in Table 3, penalizes the networks for misclassifying examples from
areheavilyimbalanced. under-represented classes, while penalizing networks lessIEEETRANSACTIONSONAFFECTIVECOMPUTING 10
TABLE7
F1-ScoresoffourdifferentapproachesoftrainingAlexNet
Imbalanced Down-Sampling Up-Sampling Weighted-Loss
Top-1 Top-2 Top-1 Top-2 Top-1 Top-2 Top-1 Top-2
Orig* Norm* Orig Norm Orig Norm Orig Norm Orig Norm Orig Norm Orig Norm Orig Norm
Neutral 0.63 0.49 0.82 0.66 0.58 0.49 0.78 0.70 0.61 0.50 0.81 0.64 0.57 0.52 0.81 0.77
Happy 0.88 0.65 0.95 0.80 0.85 0.68 0.92 0.85 0.85 0.71 0.95 0.80 0.82 0.73 0.92 0.88
Sad 0.63 0.60 0.84 0.81 0.64 0.60 0.81 0.78 0.6 0.57 0.81 0.77 0.63 0.61 0.83 0.81
Surprise 0.61 0.64 0.84 0.86 0.53 0.63 0.75 0.83 0.57 0.66 0.80 0.81 0.51 0.63 0.77 0.86
Fear 0.52 0.54 0.78 0.79 0.54 0.57 0.80 0.82 0.56 0.58 0.75 0.76 0.56 0.66 0.79 0.86
Disgust 0.52 0.55 0.76 0.78 0.53 0.64 0.74 0.81 0.53 0.59 0.70 0.72 0.48 0.66 0.69 0.83
Anger 0.65 0.59 0.83 0.80 0.62 0.60 0.79 0.78 0.63 0.59 0.81 0.77 0.60 0.60 0.81 0.81
Contempt 0.08 0.08 0.49 0.49 0.22 0.32 0.60 0.70 0.15 0.18 0.42 0.42 0.27 0.59 0.58 0.79
*OrigandNormstandforOriginalandskew-Normalized,respectively.
for misclassifying examples from well-represented classes. performed at this baseline. To augment the data, five crops
The entropy loss formulation for a training example (X,l) of 224×224 and their horizontal flips were extracted from
isdefinedas: the four corners and the center of the image at random
during the training phase. The networks were trained for
K
(cid:88) 20 epochs using a batch size of 256. The base learning rate
E =− H l,ilog(pˆ i) (6)
was set to 0.01, and decreased step-wise by a factor of 0.1
i=1
every10,000iterations.Weusedamomentumof0.9.
where H l,i denotes row l penalization factor of class i, K Table 7 shows the top-1 and top-2 F1-Scores for the
is the number of classes, and pˆ i is the predictive softmax imbalanced learning, down-sampling, up-sampling, and
withvalues[0,1]indicatingthepredictedprobabilityofeach weighted-loss approaches on the test set. Since the test set
classas: is imbalanced, both the skew-normalized and the original
exp(x )
pˆ i = (cid:80)K j=1expi
(x j)
(7) s bc yor re as ndar oe mre up no drt ee rd -s. aT mh pe lis nk gew ofn to hr em ca ll aiz ssa eti son inis thp eer tf eo sr tm se ed
t.
Thisprocessisrepeated200times,andtheskew-normalized
Equation(6)canbere-writtenas:
score is the average of the score on multiple trials. As
E =
−(cid:88)
H log(
exp(x i)
)
it is shown, the weighted-loss approach performed better
l,i (cid:80) exp(x ) thanotherapproachesintheskew-normalizedfashion.The
i j j
improvementissignificantinunder-representedclasses,i.e.,
(cid:88) (cid:88) (cid:88)
=− H l,ix i+ H l,ilog( exp(x j)) (8) Contempt, Fear, and Disgust. The imbalanced approach
i i j performed worst in the Contempt and Disgust categories
(cid:88) (cid:88) (cid:88)
=log( exp(x )) H − H x sincetherewereafewtrainingsamplesoftheseclassescom-
j l,i l,i i
pared with other classes. The up-sampling approach also
j i i
did not classify the Contempt and Disgust categories well,
Thederivatewithrespecttothepredictionx k is: sincethetrainingsamplesoftheseclasseswereheavilyup-
∂E ∂ (cid:88) (cid:88) ∂ (cid:88) sampled(almost20times),andthenetworkwasover-fitted
= [log( exp(x )) H ]− [ H x ]
∂x ∂x j l,i ∂x l,i i to these samples. Hence the network lost its generalization
k k k
j i i andperformedpoorlyontheseclassesofthetestset.
=((cid:88) H ) 1 ∂ (cid:88) exp(x )−H The confusion matrix of the weighted-loss approaches
i
l,i (cid:80) jexp(x j)∂x k
j
j l,k is shown in Table 8. The weighted-loss approach classified
the samples of Contempt and Disgust categories with an
=((cid:88) H l,i) (cid:80)ex ep x( px (k x)
)
−H
l,k
acceptable accuracy but did not perform well in Happy
i j j andNeutral.Thisisbecausethenetworkwasnotpenalized
(cid:88)
=( H )pˆ −H enough for misclassifying examples from these classes. We
l,i k l,k
believe that a better formulation of the weight matrix H
i
(9) basedonthenumberofsamplesinthemini-batchesorother
When H = I, the identity, the proposed weighted-loss data-drivenapproachescanimprovetherecognitionofwell-
approach gives the traditional cross-entropy loss function. representedclasses.
WeusedtheimplementedInfogainlossinCaffe[66]forthis Table 9 shows accuracy, F1-score, Cohens kappa, Krip-
purpose.Forsimplicity,weusedadiagonalmatrixdefined pendorfsAlpha,areaundertheROCcurve(AUC),andarea
as: underthePrecision-Recallcurve(AUC-PR)onthetestsets.
(cid:40)
fi , ifi=j Except for the accuracy, all the metrics are calculated in a
H ij = 0f ,min otherwise (10) binary-class manner where the positive class contains the
samples labeled by the given category, and the negative
wheref iisthenumberofsamplesoftheithclassandf minis class contains the rest. The reported result in Table 9 is the
thenumberofsamplesinthemostunder-representedclass, average of these metrics over eight classes. The accuracy
i.e.,Disgustclassinthissituation. is defined in a multi-class manner in which the number of
Before training the network, the faces were cropped correctpredictionsisdividedbythetotalnumberofsamples
and resized to 256×256 pixels. No facial registration was in the test set. The skew-normalization is performed byIEEETRANSACTIONSONAFFECTIVECOMPUTING 11
TABLE8
ConfusionMatrixofWeighted-LossApproachontheTestSet
Predicted
NE HA SA SU FE DI AN CO
lautcA
NE 53.3 2.8 9.8 8.7 1.7 2.5 10.4 10.9
HA 4.5 72.8 1.1 6.0 0.6 1.7 1.0 12.2
SA 13.0 1.3 61.7 3.6 5.8 4.4 9.2 1.2
SU 3.4 1.2 1.7 69.9 18.9 1.7 2.8 0.5 Angry(Disgust) Disgust(Angry) Fear(Sad) Angry(Sad)
FE 1.5 1.5 4.6 13.5 70.4 4.2 4.3 0.2
DI 2.0 2.2 5.8 3.3 6.2 68.6 10.6 1.3
AN 6.2 1.2 5.0 3.2 5.8 11.1 65.8 1.9
CO 16.2 13.1 3.5 3.1 0.5 4.3 5.7 53.8
balancing the distribution of classes in the test set using Happy(Surprise) Fear(Surprise) Surprise(Fear) Angry(Fear)
randomunder-samplingandaveragingover200trials.Since
the validation set is balanced, there is no need for skew-
normalization.
We compared the performance of CNN baseline with a
SupportVectorMachine(SVM)[62].TotrainSVM,thefaces
in the images were cropped and resized to 256×256 pixels. Angry(Disgust) Happy(Neutral) Sad(Angry) Happy(Contempt)
HOG[67]featureswereextractedwiththecellsizeof8.We
Fig.5. Samplesofmiss-classifiedimages.Theircorrespondingground-
applied PCA retaining 95% of the variance to reduce the
truthisgiveninparentheses.
HOGfeaturesdimensionalityfrom36,864to6,697features.
We used a linear kernel SVM in Liblinear package [68]
(which is optimized for large-scale linear classification and 0.25
regression). Table 9 shows the evaluation metrics of SVM.
Reported AUC and AUCPR values for SVM are calculated
using the LibLinear’s resulting decision values. We calcu- 0.2
lated the scores of predictions using a posterior-probability
transformation sigmoid function. Comparing the perfor-
mance of SVM with the CNN baselines on AffectNet, in-
0.15
dicatesthatCNNmodelsperformbetterthanconventional
SVMandHOGfeaturesinallmetrics.
Wealsocomparedthebaselinewithanavailableoff-the-
0.1
shelf expression recognition system (Microsoft Cognitive 0 2000 4000 6000 8000 10000 12000 14000 16000
iteration
Services emotion API [69]). The MS cognitive system had
anexcellentperformanceonNeutralandHappycategories
withanaccuracyof0.94and0.85,respectively.However,it
performedpoorlyonotherclasseswithanaccuracyof0.25,
0.27and0.04intheFear,DisgustandContemptcategories.
Table 9 shows the evaluation metrics on the MS cognitive
system. Comparing the performance of the MS cognitive
withthesimplebaselinesonAffectNetindicatesthatAffect-
Netisachallengingdatabaseandagreatresourcetofurther
improve the performance of facial expression recognition
systems.
Figure 5 shows nine samples of randomly selected mis-
classified images of the weighted-loss approach and their
correspondingground-truth.Asthefigureshows,itisreally
difficulttoassignsomeoftheemotionstoasinglecategory.
Someofthefaceshavepartialsimilaritiesinfacialfeaturesto
the misclassified images, such as nose wrinkled in disgust,
oreyebrowsraisedinsurprise.Thisemphasizesthefactthat
classifying facial expressions in the wild is a challenging
task and, as mentioned before, even human annotators
agreedononly60.7%oftheimages.
4.3 DimensionalModel(ValenceandArousal)Baseline
Predicting dimensional model in the continuous domain
is a real-valued regression problem. We used AlexNet [57]
architecture as our deep CNN baseline to predict the value
rorre
Training
Validation
Fig.6.Euclideanerroroftrainingvalenceandarousal.
of valence and arousal. Particularly, two separate AlexNets
were trained where the last fully-connected layer was re-
placed with a linear regression layer containing only one
neuron. The output of the neuron predicted the value of
valence/arousal in continuous domain [-1,1]. A Euclidean
(L2) loss was used to measure the distance between the
predicted value (yˆ n) and actual value of valence/arousal
(y n)as:
N
1 (cid:88) E =
2N
||yˆ n−y n||2
2
(11)
n=1
The faces were cropped and resized to 256×256 pixels.
Thebaselearningratewasfixedandsetto0.001duringthe
trainingprocess.Weusedamomentumof0.9.Trainingwas
continueduntilaplateauwasreachedintheEuclideanerror
ofthevalidationset(approximately16epochswithamini-
batchsizeof256).Figure6showsthevalueoftrainingand
validationlossesover16Kiterations(about16epochs).
WealsocomparedSupportVectorRegression(SVR)[63]
with our DNN baseline for predicting valence and arousal
inAffectNet.Inourexperiments,first,thefacesintheimages
were cropped and resized to 256×256 pixels. Histogram of
OrientedGradient(HOG)[67]featureswereextractedwith
thecellsizeof8.Afterward,weappliedPCAretaining95%IEEETRANSACTIONSONAFFECTIVECOMPUTING 12
TABLE9
EvaluationMetricsandComparisonofCNNbaselines,SVMandMSCognitiveonCategoricalModelofAffect.
CNNBaselines
SVM MSCognitive
Imbalanced Down-Sampling Up-Sampling Weighted-Loss
Orig Norm Orig Norm Orig Norm Orig Norm Orig Norm Orig Norm
Accuracy 0.72 0.54 0.68 0.58 0.68 0.57 0.64 0.63 0.60 0.37 0.68 0.48
F1-Score 0.57 0.52 0.56 0.57 0.56 0.55 0.55 0.62 0.37 0.31 0.51 0.45
Kappa 0.53 0.46 0.51 0.51 0.52 0.49 0.5 0.57 0.32 0.25 0.46 0.40
Alpha 0.52 0.45 0.51 0.51 0.51 0.48 0.5 0.57 0.31 0.22 0.46 0.37
AUC 0.85 0.80 0.82 0.85 0.82 0.84 0.86 0.86 0.77 0.70 0.83 0.77
AUCPR 0.56 0.55 0.54 0.57 0.55 0.56 0.58 0.64 0.39 0.37 0.52 0.50
TABLE10 machine learning methodologies, and their performance
Baselines’PerformancesofPredictingValenceandArousalonTestSet highly depends on the amount and diversity of annotated
CNN(AlexNet) SVR training samples. Recently, databases of facial expression
Valence Arousal Valence Arousal and affect in the wild received much attention. However,
RMSE 0.394 0.402 0.494 0.400 existing databases of facial affect in the wild only cover
CORR 0.602 0.539 0.429 0.360
one model of affect, have a limited number of subjects, or
SAGR 0.728 0.670 0.619 0.748
CCC 0.541 0.450 0.340 0.199 containfewsamplesofcertainemotions.
The Internet is a vast source of facial images, most of
which are captured in uncontrolled conditions. These im-
agesareoftentakeninthewildundernaturalconditions.In
thispaper,weintroducedanewpubliclyavailabledatabase
of a facial Affect from the InterNet (called AffectNet) by
querying different search engines using emotion related
tags in six different languages. AffectNet contains more
than 1M images with faces and extracted landmark points.
Twelvehumanexpertsmanuallyannotated450,000ofthese
images in both the categorical and dimensional (valence
and arousal) models and tagged the images that have any
Fig. 7. RMSE of predicted valence and arousal using AlexNet and occlusionontheface.
Euclidean(L2)loss(Bestviewedincolor). The agreement level of human labelers on a subset of
AffectNetshowedthatexpressionrecognitionandpredicting
valence and arousal in the wild is a challenging task. The
of the variance of these features to reduce the dimension-
two annotators agreed on 60.7% of the category of facial
ality. Two separate SVRs were trained to predict the value
expressions, and there was a large disagreement on the
of valence and arousal. Liblinear [68] package was used to
valueofvalenceandarousal(RMSE=0.34and0.36)between
implementSVRbaseline.
thetwoannotators.
Table 10 shows the performances of the proposed base-
Two simple deep neural network baselines were exam-
line and SVR on the test set. As shown, the CNN baseline
inedtoclassifythefacialexpressionimagesandpredictthe
can predict the value of valence and arousal better than
value of valence and arousal in the continuous domain of
SVR.ThisisbecausethehighvarietyofsamplesinAffectNet
dimensionalmodel.Evaluationmetricsshowedthatsimple
allows the CNN to extract more discriminative features
deep neural network baselines trained on AffectNet can
than hand-crafted HOG, and therefore it learned a better
performbetterthanconventionalmachinelearningmethods
representationofdimensionalaffect.
and available off-the-shelf expression recognition systems.
TheRMSEofCNNbaseline(AlexNet)betweenthepre-
AffectNet is by far the largest database of facial expression,
dictedvalenceandarousalandtheground-truthareshown
valenceandarousalinthewild,enablingfurtherprogressin
inFig.7.Asillustrated,theCNNbaselinehasalowererror
the automatic understanding of facial behavior in both cat-
rate in the center of circumplex. In particular, predicting
egorical and continuous dimensional space. The interested
low-valence mid-arousal and low-arousal mid-valence ar-
investigatorscanstudycategoricalanddimensionalmodels
eas were more challenging. These areas correspond to the
in the same corpus, and possibly co-train them to improve
expressions of contempt, bored, and sleepy. It should be
the performance of their affective computing systems. It is
mentioned that predicting valence and arousal in the wild
highly anticipated that the availability of this database for
is a challenging task, and as discussed in Sec. 3.3, the dis-
the research community, along with the recent advances
agreementbetweentwohumanannotatorshasRMSE=0.367
in deep neural networks, can improve the performance
andRMSE=0.481forvalenceandarousal,respectively.
of automated affective computing systems in recognizing
facialexpressionsandpredictingvalenceandarousal.
5 CONCLUSION
ACKNOWLEDGMENTS
The analysis of human facial behavior is a very complex
and challenging problem. The majority of the techniques This work is partially supported by the NSF grants IIS-
for automated facial affect analysis are mainly based on 1111568 and CNS-1427872. We gratefully acknowledge theIEEETRANSACTIONSONAFFECTIVECOMPUTING 13
support of NVIDIA Corporation with the donation of the [21] S. Zafeiriou, A. Papaioannou, I. Kotsia, M. A. Nicolaou, and
TeslaK40GPUsusedforthisresearch. G. Zhao, “Facial affect “in-the-wild”: A survey and a new
database,”inInternationalConferenceonComputerVisionandPattern
Recognition(CVPR)Workshops,Affect”in-the-wild”Workshop,June
2016. 2,3,4
REFERENCES
[22] C. F. Benitez-Quiroz, R. Srinivasan, and A. M. Martinez, “Emo-
tionet:Anaccurate,real-timealgorithmfortheautomaticannota-
[1] J. Tao and T. Tan, “Affective computing: A review,” in Interna-
tionofamillionfacialexpressionsinthewild,”inProceedingsof
tional Conference on Affective Computing and Intelligent Interaction.
IEEEInternationalConferenceonComputerVision&PatternRecogni-
Springer,2005,pp.981–995. 1
tion(CVPR16),LasVegas,NV,USA,2016. 2,3,5
[2] P.EkmanandW.V.Friesen,“Constantsacrossculturesintheface
[23] M.A.Nicolaou,H.Gunes,andM.Pantic,“Audio-visualclassifica-
and emotion.” Journal of personality and social psychology, vol. 17,
tionandfusionofspontaneousaffectivedatainlikelihoodspace,”
no.2,p.124,1971. 1
inPatternRecognition(ICPR),201020thInternationalConferenceon.
[3] J.A.Russell,“Acircumplexmodelofaffect,”JournalofPersonality
IEEE,2010,pp.3695–3699. 3
andSocialPsychology,vol.39,no.6,pp.1161–1178,1980. 1,7,8
[24] ——,“Continuouspredictionofspontaneousaffectfrommultiple
[4] P.EkmanandW.V.Friesen,“Facialactioncodingsystem,”1977.
cues and modalities in valence-arousal space,” IEEE Transactions
1
onAffectiveComputing,vol.2,no.2,pp.92–105,2011. 3,5
[5] W. V. Friesen and P. Ekman, “Emfacs-7: Emotional facial action
[25] F.Ringeval,A.Sonderegger,J.Sauer,andD.Lalanne,“Introducing
codingsystem,”Unpublishedmanuscript,UniversityofCaliforniaat
therecolamultimodalcorpusofremotecollaborativeandaffective
SanFrancisco,vol.2,p.36,1983. 1
interactions,”inAutomaticFaceandGestureRecognition(FG),2013
[6] S.Du,Y.Tao,andA.M.Martinez,“Compoundfacialexpressions
10thIEEEInternationalConferenceandWorkshopson. IEEE,2013,
of emotion,” Proceedings of the National Academy of Sciences, vol.
pp.1–8. 3,4
111,no.15,pp.E1454–E1462,2014. 1,3
[26] S. Koelstra, C. Muhl, M. Soleymani, J.-S. Lee, A. Yazdani,
[7] M.Lyons,S.Akamatsu,M.Kamachi,andJ.Gyoba,“Codingfacial
T.Ebrahimi,T.Pun,A.Nijholt,andI.Patras,“Deap:Adatabasefor
expressions with gabor wavelets,” in Automatic Face and Gesture
emotion analysis; using physiological signals,” IEEE Transactions
Recognition, 1998. Proceedings. Third IEEE International Conference
onAffectiveComputing,vol.3,no.1,pp.18–31,2012. 3,4,5
on. IEEE,1998,pp.200–205. 2
[27] A.Dhall,R.Goecke,S.Lucey,andT.Gedeon,“Staticfacialexpres-
[8] Y.-I.Tian,T.Kanade,andJ.F.Cohn,“Recognizingactionunitsfor
sion analysis in tough conditions: Data, evaluation protocol and
facialexpressionanalysis,”IEEETrans.PatternAnal.Mach.Intell.,
benchmark,”inComputerVisionWorkshops(ICCVWorkshops),2011
vol.23,no.2,pp.97–115,2001. 2
IEEEInternationalConferenceon. IEEE,2011,pp.2106–2112. 3
[9] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and
[28] A.MollahosseiniandM.H.Mahoor,“Bidirectionalwarpingofac-
I. Matthews, “The extended cohn-kanade dataset (ck+): A com-
tiveappearancemodel,”inComputerVisionandPatternRecognition
pletedatasetforactionunitandemotion-specifiedexpression,”in
Workshops (CVPRW), 2013 IEEE Conference on. IEEE, 2013, pp.
ComputerVisionandPatternRecognitionWorkshops(CVPRW),2010
875–880. 3
IEEEComputerSocietyConferenceon. IEEE,2010,pp.94–101. 2,3,
[29] S. Ren, X. Cao, Y. Wei, and J. Sun, “Face alignment at 3000 fps
4
via regressing local binary features,” in Proceedings of the IEEE
[10] M. Pantic, M. Valstar, R. Rademaker, and L. Maat, “Web-based
Conference on Computer Vision and Pattern Recognition, 2014, pp.
database for facial expression analysis,” in Multimedia and Expo,
1685–1692. 3,6
2005.ICME2005.IEEEInternationalConferenceon. IEEE,2005,pp.
[30] L. Yu, “face-alignment-in-3000fps,” https://github.com/
5–pp. 2,3
yulequan/face-alignment-in-3000fps,2016. 3,6
[11] R.Gross,I.Matthews,J.Cohn,T.Kanade,andS.Baker,“Multi-
pie,”ImageandVisionComputing,vol.28,no.5,pp.807–813,2010. [31] G.A.Miller,“Wordnet:alexicaldatabaseforenglish,”Communi-
2,3 cationsoftheACM,vol.38,no.11,pp.39–41,1995. 3
[12] J. F. Cohn and K. L. Schmidt, “The timing of facial motion in [32] P.ViolaandM.J.Jones,“Robustreal-timefacedetection,”Inter-
posed and spontaneous smiles,” International Journal of Wavelets, nationaljournalofcomputervision,vol.57,no.2,pp.137–154,2004.
Multiresolution and Information Processing, vol. 2, no. 02, pp. 121– 3
132,2004. 2 [33] D.You,O.C.Hamsici,andA.M.Martinez,“Kerneloptimization
[13] M.F.Valstar,M.Pantic,Z.Ambadar,andJ.F.Cohn,“Spontaneous in discriminant analysis,” IEEE Trans. Pattern Anal. Mach. Intell.,
vs.posedfacialbehavior:automaticanalysisofbrowactions,”in vol.33,no.3,pp.631–638,2011. 3
Proceedingsofthe8thinternationalconferenceonMultimodalinterfaces. [34] P. Lucey, J. F. Cohn, K. M. Prkachin, P. E. Solomon, and
ACM,2006,pp.162–170. 2 I. Matthews, “Painful data: The unbc-mcmaster shoulder pain
[14] S.M.Mavadati,M.H.Mahoor,K.Bartlett,P.Trinh,andJ.F.Cohn, expression archive database,” in Automatic Face & Gesture Recog-
“Disfa: A spontaneous facial action intensity database,” Affective nitionandWorkshops(FG2011),2011IEEEInternationalConference
Computing,IEEETransactionson,vol.4,no.2,pp.151–160,2013. 2, on. IEEE,2011,pp.57–64. 4
3,4,5 [35] M. M. Bradley and P. J. Lang, “Measuring emotion: the self-
[15] D.McDuff,R.Kaliouby,T.Senechal,M.Amr,J.Cohn,andR.Pi- assessment manikin and the semantic differential,” Journal of be-
card,“Affectiva-mitfacialexpressiondataset(am-fed):Naturalis- haviortherapyandexperimentalpsychiatry,vol.25,no.1,pp.49–59,
tic and spontaneous facial expressions collected,” in Proceedings 1994. 4
of the IEEE Conference on Computer Vision and Pattern Recognition [36] B. Schuller, M. Valstar, F. Eyben, G. McKeown, R. Cowie, and
Workshops,2013,pp.881–888. 2,3,4,5 M.Pantic,“Avec2011–thefirstinternationalaudio/visualemotion
[16] I. Sneddon, M. McRorie, G. McKeown, and J. Hanratty, “The challenge,” in International Conference on Affective Computing and
belfast induced natural emotion database,” IEEE Transactions on IntelligentInteraction. Springer,2011,pp.415–424. 4,5
AffectiveComputing,vol.3,no.1,pp.32–41,2012. 2,4 [37] B.Schuller,M.Valster,F.Eyben,R.Cowie,andM.Pantic,“Avec
[17] J.F.Grafsgaard,J.B.Wiggins,K.E.Boyer,E.N.Wiebe,andJ.C. 2012: the continuous audio/visual emotion challenge,” in Pro-
Lester, “Automatically recognizing facial expression: Predicting ceedings of the 14th ACM international conference on Multimodal
engagementandfrustration.”inEDM,2013,pp.43–50. 2 interaction. ACM,2012,pp.449–456. 4,5
[18] A.Dhall,R.Goecke,J.Joshi,M.Wagner,andT.Gedeon,“Emotion [38] M. Valstar, B. Schuller, K. Smith, F. Eyben, B. Jiang, S. Bilakhia,
recognitioninthewildchallenge2013,”inProceedingsofthe15th S.Schnieder,R.Cowie,andM.Pantic,“Avec2013:thecontinuous
ACMonInternationalconferenceonmultimodalinteraction. ACM, audio/visual emotion and depression recognition challenge,” in
2013,pp.509–516. 2,3 Proceedings of the 3rd ACM international workshop on Audio/visual
[19] I. J. Goodfellow, D. Erhan, P. L. Carrier, A. Courville, M. Mirza, emotionchallenge. ACM,2013,pp.3–10. 4
B. Hamner, W. Cukierski, Y. Tang, D. Thaler, D.-H. Lee et al., [39] M.Valstar,B.Schuller,K.Smith,T.Almaev,F.Eyben,J.Krajewski,
“Challengesinrepresentationlearning:Areportonthreemachine R.Cowie,andM.Pantic,“Avec2014:3ddimensionalaffectand
learningcontests,”NeuralNetworks,vol.64,pp.59–63,2015. 2,3 depression recognition challenge,” in Proceedings of the 4th Inter-
[20] A.Mollahosseini,B.Hasani,M.J.Salvador,H.Abdollahi,D.Chan, nationalWorkshoponAudio/VisualEmotionChallenge. ACM,2014,
and M. H. Mahoor, “Facial expression recognition from world pp.3–10. 4
wildweb,”inTheIEEEConferenceonComputerVisionandPattern [40] F.Ringeval,B.Schuller,M.Valstar,R.Cowie,andM.Pantic,“Avec
Recognition(CVPR)Workshops,June2016. 2,3,5 2015: The 5th international audio/visual emotion challenge andIEEETRANSACTIONSONAFFECTIVECOMPUTING 14
workshop,”inProceedingsofthe23rdACMinternationalconference [62] C. Cortes and V. Vapnik, “Support-vector networks,” Machine
onMultimedia. ACM,2015,pp.1335–1336. 4,5 learning,vol.20,no.3,pp.273–297,1995. 9,11
[41] M. Valstar, J. Gratch, B. Schuller, F. Ringeval, D. Lalanne, M. T. [63] A. Smola and V. Vapnik, “Support vector regression machines,”
Torres, S. Scherer, G. Stratou, R. Cowie, and M. Pantic, “Avec Advances in neural information processing systems, vol. 9, pp. 155–
2016-depression, mood, and emotion recognition workshop and 161,1997. 9,11
challenge,”arXivpreprintarXiv:1605.01600,2016. 4,5 [64] G. Caridakis, K. Karpouzis, and S. Kollias, “User and context
[42] G.McKeown,M.Valstar,R.Cowie,M.Pantic,andM.Schroder, adaptiveneuralnetworksforemotionrecognition,”Neurocomput-
“The semaine database: Annotated multimodal records of emo- ing,vol.71,no.13,pp.2553–2562,2008. 9
tionally colored conversations between a person and a limited [65] H.HeandE.A.Garcia,“Learningfromimbalanceddata,”IEEE
agent,”IEEETransactionsonAffectiveComputing,vol.3,no.1,pp. Trans.Knowl.DataEng.,vol.21,no.9,pp.1263–1284,2009. 9
5–17,2012. 4 [66] Y.Jia,E.Shelhamer,J.Donahue,S.Karayev,J.Long,R.Girshick,
[43] A.Mollahosseini,D.Chan,andM.H.Mahoor,“Goingdeeperin S.Guadarrama,andT.Darrell,“Caffe:Convolutionalarchitecture
facialexpressionrecognitionusingdeepneuralnetworks,”IEEE forfastfeatureembedding,”inProceedingsofthe22ndACMinter-
WinterConferenceonApplicationsofComputerVision(WACV),2016. nationalconferenceonMultimedia. ACM,2014,pp.675–678. 10
5 [67] N. Dalal and B. Triggs, “Histograms of oriented gradients for
human detection,” in 2005 IEEE Computer Society Conference on
[44] C. Shan, S. Gong, and P. W. McOwan, “Facial expression recog-
ComputerVisionandPatternRecognition(CVPR’05),vol.1. IEEE,
nition based on local binary patterns: A comprehensive study,”
2005,pp.886–893. 11
ImageandVisionComputing,vol.27,no.6,pp.803–816,2009. 5
[68] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin,
[45] X.Zhang,M.H.Mahoor,andS.M.Mavadati,“Facialexpression
“Liblinear: A library for large linear classification,” Journal of
recognition using {l} {p}-norm mkl multiclass-svm,” Machine
machinelearningresearch,vol.9,no.Aug,pp.1871–1874,2008. 11,
VisionandApplications,pp.1–17,2015. 5
12
[46] L.He,D.Jiang,L.Yang,E.Pei,P.Wu,andH.Sahli,“Multimodal
[69] “Microsoft cognitive services - emotion api,” https://www.
affectivedimensionpredictionusingdeepbidirectionallongshort-
microsoft.com/cognitive-services/en-us/emotion-api, (Accessed
termmemoryrecurrentneuralnetworks,”inProceedingsofthe5th
on12/01/2016). 11
InternationalWorkshoponAudio/VisualEmotionChallenge. ACM,
2015,pp.73–80. 5,6
[47] Y.Fan,X.Lu,D.Li,andY.Liu,“Video-basedemotionrecognition
usingcnn-rnnandc3dhybridnetworks,”inProceedingsofthe18th
ACM International Conference on Multimodal Interaction. ACM,
Ali Mollahosseini received the BSc degree in
2016,pp.445–450. 5
computer software engineering from the Iran
[48] Y. Tang, “Deep learning using linear support vector machines,”
University of Science and Technology, Iran, in
arXivpreprintarXiv:1306.0239,2013. 5
2006, and the MSc degree in computer engi-
[49] M.Sokolova,N.Japkowicz,andS.Szpakowicz,“Beyondaccuracy, neering - artificial intelligence from AmirKabir
f-scoreandroc:afamilyofdiscriminantmeasuresforperformance University,Iran,in2010.Heiscurrentlyworking
evaluation,”inAustralasianJointConferenceonArtificialIntelligence.
toward the Ph.D. degree and is a graduate re-
Springer,2006,pp.1015–1021. 4 searchassistantintheDepartmentofElectrical
[50] J.Cohen,“Acoefficientofagreementfornominalscales,”Educa- and Computer Engineering at the University of
tionalandPsychologicalMeasurement,vol.20,no.1,p.37,1960. 4, Denver.Hisresearchinterestsincludedeepneu-
9 ralnetworksfortheanalysisoffacialexpression,
[51] K.Krippendorff,“Estimatingthereliability,systematicerrorand developinghumanoidsocialrobotsandcomputervision.
randomerrorofintervaldata,”EducationalandPsychologicalMea-
surement,vol.30,no.1,pp.61–70,1970. 4,9
[52] P. E. Shrout and J. L. Fleiss, “Intraclass correlations: uses in Behzad Hasani received the BSc degree in
assessing rater reliability.” Psychological bulletin, vol. 86, no. 2, p. computer hardware engineering from Khaje
420,1979. 4 Nasir Toosi University of Technology, Tehran,
[53] L. A. Jeni, J. F. Cohn, and F. De La Torre, “Facing imbalanced Iran,in2013,andtheMScdegreeincomputer
data–recommendations for the use of performance metrics,” in engineering-artificialintelligencefromIranUni-
AffectiveComputingandIntelligentInteraction(ACII),2013Humaine versityofScienceandTechnology,Tehran,Iran,
AssociationConferenceon. IEEE,2013,pp.245–251. 4,9 in 2015. He is currently pursuing his Ph.D. de-
[54] S.BermejoandJ.Cabestany,“Orientedprincipalcomponentanal- greeinelectrical&computerengineeringandis
ysisforlargemarginclassifiers,”NeuralNetworks,vol.14,no.10, agraduateresearchassistantintheDepartment
pp.1447–1461,2001. 4 of Electrical and Computer Engineering at the
University of Denver. His research interests in-
[55] M.Mohammadi,E.Fatemizadeh,andM.H.Mahoor,“Pca-based
cludeComputerVision,MachineLearning,andDeepNeuralNetworks,
dictionarybuildingforaccuratefacialexpressionrecognitionvia
especiallyonfacialexpressionanalysis.
sparserepresentation,”JournalofVisualCommunicationandImage
Representation,vol.25,no.5,pp.1082–1092,2014. 5
MohammadH.MahoorreceivedtheBSdegree
[56] C.LiuandH.Wechsler,“Gaborfeaturebasedclassificationusing
inelectronicsfromtheAbadanInstituteofTech-
the enhanced fisher linear discriminant model for face recogni-
nology,Iran,in1996,theMSdegreeinbiomed-
tion,”IEEETrans.ImageProcess.,vol.11,no.4,pp.467–476,2002.
ical engineering from the Sharif University of
5
Technology,Iran,in1998,andthePh.D.degree
[57] A.Krizhevsky,I.Sutskever,andG.E.Hinton,“Imagenetclassifi- inelectricalandcomputerengineeringfromthe
cationwithdeepconvolutionalneuralnetworks,”inAdvancesin University of Miami, Florida, in 2007. He is an
neuralinformationprocessingsystems,2012,pp.1097–1105. 5,9,11 AssociateProfessorofElectricalandComputer
[58] A.ToshevandC.Szegedy,“Deeppose:Humanposeestimationvia EngineeringatDU.Hedoesresearchinthearea
deepneuralnetworks,”inComputerVisionandPatternRecognition ofcomputervisionandmachinelearninginclud-
(CVPR),2014IEEEConferenceon. IEEE,2014,pp.1653–1660. 5 ingvisualobjectrecognition,objecttrackingand
[59] Y.Taigman,M.Yang,M.Ranzato,andL.Wolf,“Deepface:Closing pose estimation, motion estimation, 3D reconstruction, and human-
thegaptohuman-levelperformanceinfaceverification,”inCom- robot interaction (HRI) such as humanoid social robots for interaction
puterVisionandPatternRecognition(CVPR),2014IEEEConference and intervention with children with special needs (e.g., autism) and
on. IEEE,2014,pp.1701–1708. 5 elderly with depression and dementia. He has received over $3M of
[60] C. Sagonas, E. Antonakos, G. Tzimiropoulos, S. Zafeiriou, and researchfundingfromstateandfederalagenciesincludingtheNational
M.Pantic,“300facesin-the-wildchallenge:Databaseandresults,” ScienceFoundation.HeisaSeniorMemberofIEEEandhaspublished
ImageandVisionComputing,vol.47,pp.3–18,2016. 6 about100conferenceandjournalpapers.
[61] G.PaltoglouandM.Thelwall,“Seeingstarsofvalenceandarousal
in blog posts,” IEEE Transactions on Affective Computing, vol. 4,
no.1,pp.116–123,2013. 7IEEETRANSACTIONSONAFFECTIVECOMPUTING 15
APPENDIX A
TABLE11
SamplesofAnnotatedCategoriesforQueriedEmotionTerms
QueriedExpression
Happy Sad Surprise Fear Disgust Anger Contempt
noisserpxEdetatonnA
Neutral
Happy
Sad
Surprise
Fear
Disgust
Anger
Contempt
None
Uncertain
Non-FaceIEEETRANSACTIONSONAFFECTIVECOMPUTING 16
TABLE12
SamplesofAnnotatedImagesbyTwoAnnotators(Randomlyselected)
Annotator1
Neutral Happy Sad Surprise Fear Disgust Anger Contempt None Uncertain Non-Face
2rotatonnA
Neutral
Happy
Sad
Surprise
Fear
Disgust
Anger
Contempt
None
Uncertain
Non-Face
TABLE13
AgreementpercentagebetweenTwoAnnotatorsinCategoricalModelofAffect(%)
A1* A2 A3 A4 A5 A6 A7 A8 A9 A10 A11 A12
A1 0.0** 69 70 68 0 0 0 0 0 0 0 0
A2 69 0 64.9 68.3 0 0 0 64.7 0 0 0 0
A3 70 64.9 0 70.6 67.4 69.9 63 62.3 0 48.1 0 0
A4 68 68.3 70.6 0 70.4 70.8 64.3 67.5 0 27.5 0 0
A5 0 0 67.4 70.4 0 70.6 0 0 0 0 0 0
A6 0 0 69.9 70.8 70.6 0 0 0 0 0 0 0
A7 0 0 63 64.3 0 0 0 0 0 75.8 0 0
A8 0 64.7 62.3 67.5 0 0 0 0 51.1 0 0 0
A9 0 0 0 0 0 0 0 51.1 0 0 54.4 0
A10 0 0 48.1 27.5 0 0 75.8 0 0 87.5 0 61.9
A11 0 0 0 0 0 0 0 0 54.4 0 0 0
A12 0 0 0 0 0 0 0 0 0 61.9 0 0
*A1toA12indicateAnnotators1to12
**ZeromeansthattherewerenocommonimagesbetweenthetwoannotatorsIEEETRANSACTIONSONAFFECTIVECOMPUTING 17
Arousal
V: -0.2 A: 0.87
V: 0.2 A: 0.85
V: -0.6 A: 0.75
V: 0.3 A: 0.62
V: -0.4 A: 0.6 V: 0.68 A: 0.66
V: -0.77 A: 0.45 V: 0.1 A: 0.46
V: -0.35 A: 0.28
V: 0.35 A: 0.3
e
c V: -0.95 A: 0.19
n V: 0.95 A: 0.17
e
la
V
V: 0.0 A: 0.0
V: -0.55 A: -0.1
V: 0.97 A: -0.1
V: -0.3 A: -0.35 V: 0.4 A: -0.41
V: -0.85 A: -0.38
V: -0.55 A: -0.6 V: 0.23 A: -0.65
V: 0.7 A: -0.73
V: -0.47 A: -0.73 V: -0.12 A: -0.75
V: 0.1 A: -0.85
V: 0.0 A: -0.98
Fig.8.SampleimagesinValenceArousalcircumplexwiththeircorrespondingValenceandArousalvalues(V:Valence,A:Arousal).
TABLE14
Numberofannotatedimagesineachrange/areaofvalenceandarousal
Valence
[-1,-.8] [-.8,-.6] [-.6,-.4] [-.4,-.2] [-.2,0] [0,.2] [.2,.4] [.4,.6] [.6,.8] [.8,1]
lasuorA
[.8,1] 0 0 21 674 1021 521 60 57 0 0
[.6,.8] 0 74 161 561 706 1006 432 738 530 0
[.4,.6] 638 720 312 505 2689 1905 1228 992 3891 957
[.2,.4] 6770 9283 3884 2473 5530 2296 3506 1824 2667 1125
[0,.2] 3331 1286 2971 4854 14083 15300 4104 9998 13842 9884
[-.2,0] 395 577 5422 3675 9024 23201 6237 42219 23281 21040
[-.4,-.2] 787 1364 3700 6344 2804 1745 821 5241 10619 9934
[-.6,-.4] 610 7800 2645 3571 2042 2517 1993 467 1271 921
[-.8,-.6] 0 3537 8004 4374 5066 3379 4169 944 873 0
[-1,-.8] 0 0 4123 1759 4836 1845 1672 739 0 0IEEETRANSACTIONSONAFFECTIVECOMPUTING 18
TABLE15
EvaluationMetricsandComparisonofCNNbaselines,SVMandMSCognitiveonCategoricalModelofAffectontheValidationSet.
CNNBaselines
SVM MSCognitive
Imbalanced Down-Sampling Up-Sampling Weighted-Loss
Accuracy 0.40 0.50 0.47 0.58 0.30 0.37
F 1-Score 0.34 0.49 0.44 0.58 0.24 0.33
Kappa 0.32 0.42 0.38 0.51 0.18 0.27
Alpha 0.39 0.42 0.37 0.51 0.13 0.23
AUCPR 0.42 0.48 0.44 0.56 0.30 0.38
AUC 0.74 0.47 0.75 0.82 0.68 0.70
TABLE16
Baselines’PerformancesofPredictingValenceandArousalontheValidationSet
CNN(AlexNet) SVR
Valence Arousal Valence Arousal
RMSE 0.37 0.41 0.55 0.42
CORR 0.66 0.54 0.35 0.31
SAGR 0.74 0.65 0.57 0.68
CCC 0.60 0.34 0.30 0.18"
30,32,Appearance-based gender classification with Gaussian processes,"['HC Kim', 'D Kim', 'Z Ghahramani', 'SY Bang']",2006,102,Toronto Face Database,classification,"In Section 3, we introduce Gaussian process classification. In  classification. In Section 5,  we show experimental results on the PF01 database and compared with other classification",No DOI,Pattern Recognition Letters,https://www.sciencedirect.com/science/article/pii/S0167865505002801,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
31,33,Attention mechanism-based CNN for facial expression recognition,"['J Li', 'K Jin', 'D Zhou', 'N Kubota', 'Z Ju']",2020,284,Japanese Female Facial Expression,CNN,"The JAFFE dataset was collected from 10 Japanese females in a laboratory condition and  includes 213 images of posed expressions. For each subject, there are three or four images",No DOI,Neurocomputing,https://www.sciencedirect.com/science/article/pii/S0925231220309838,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
32,34,Au-aware deep networks for facial expression recognition,"['M Liu', 'S Li', 'S Shan', 'X Chen']",2013,318,MMI Facial Expression,facial expression recognition,"controlled expression database CK+ [21], MMI [22], and a wild  facial expression recognition.  All comparisons are performed on three datasets: CK+ [21], MMI [22], and a wild expression",No DOI,… face and gesture recognition (FG),https://ieeexplore.ieee.org/document/6553734,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
33,35,Au-inspired deep networks for facial expression feature learning,"['M Liu', 'S Li', 'S Shan', 'X Chen']",2015,265,"Acted Facial Expressions In The Wild, MMI Facial Expression","deep learning, neural network","Most existing technologies for facial expression recognition utilize off-the-shelf feature  extraction methods for classification. In this paper, aiming at learning better features specific for",No DOI,Neurocomputing,https://www.sciencedirect.com/science/article/pii/S0925231215001605,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
34,36,Automatic coding of facial expressions displayed during posed and genuine pain,"['GC Littlewort', 'MS Bartlett', 'K Lee']",2009,292,Toronto Face Database,machine learning,"a machine learning approach in a two-stage system. In the first stage, a set of 20 detectors  for facial actions from the Facial Action  Toronto. Mean accuracy of naïve human subjects for",No DOI,Image and Vision Computing,https://www.sciencedirect.com/science/article/pii/S0262885609000055,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
35,37,Automatic facial expression recognition based on a deep convolutional-neural-network structure,"['K Shan', 'J Guo', 'W You', 'D Lu']",2017,159,Japanese Female Facial Expression,"CNN, classification, deep learning, facial expression recognition, machine learning, neural network","in speech recognition, collaborative filtering, handwriting  We employ two standard facial  expression databases for the  10 Japanese women, while CK+ covers the expression images",No DOI,2017 IEEE 15th …,https://ieeexplore.ieee.org/abstract/document/7965717/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
36,38,Automatic facial expression recognition system using deep network-based data fusion,"['A Majumder', 'L Behera']",2016,192,MMI Facial Expression,"classification, classifier, deep learning, facial expression recognition, neural network","High level emotion recognition has been recently reported in the literature, such as  We  observed an average recognition accuracy of 89.3% for MMI DB and 92.54% for CK+ DB. The",No DOI,IEEE transactions on …,https://ieeexplore.ieee.org/document/7747479,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
37,39,Automatic facial expression recognition using features of salient facial patches,"['SL Happy', 'A Routray']",2014,686,Affective Faces Database,facial expression recognition,of the salient patches on face images. This paper proposes a novel framework for expression  recognition by using appearance features of selected facial patches. A few prominent,No DOI,IEEE transactions on Affective …,https://ieeexplore.ieee.org/document/6998925/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
38,40,Automaticity and the amygdala: Nonconscious responses to emotional faces,['A Öhman'],2002,540,Karolinska Directed Emotional Faces,neural network,"when exposed to emotionally expressive faces. Attention is  for responding to negative  emotional faces, and particularly to  emotional faces, but may instead respond to faces because",No DOI,Current directions in psychological science,https://journals.sagepub.com/doi/abs/10.1111/1467-8721.00169,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sagepub.com,
39,41,BAUM-1: A spontaneous audio-visual face database of affective and mental states,"['S Zhalehpour', 'O Onder', 'Z Akhtar']",2016,224,Affective Faces Database,"FER, classification, classifier",Most databases available today are acted or do not contain audio data. We present a  -visual  affective face database of affective and mental states. The video clips in the database are,No DOI,… on Affective Computing,https://ieeexplore.ieee.org/document/7451244,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
40,42,Beyond emotion archetypes: Databases for emotion modelling using neural networks,"['R Cowie', 'E Douglas-Cowie', 'C Cox']",2005,179,Affective Faces Database,neural network,"such a database ideally cover quality, emotional content, emotion-related  database of  emotional faces remains for many people the example of what they expect an emotion database",No DOI,Neural networks,https://www.sciencedirect.com/science/article/pii/S0893608005000353,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
41,43,Bilinear models for 3-D face and facial expression recognition,"['I Mpiperis', 'S Malassiotis']",2008,230,Binghamton University 3D Facial Expression,"facial expression recognition, machine learning, neural network",of bilinear models on the Binghamton University 3-D Facial Expression (BU-3DFE)  ’s work  [1] on facial expression recognition and our previous work [2] on face recognition. BU-3DFE is,No DOI,IEEE Transactions on …,https://ieeexplore.ieee.org/abstract/document/4539275/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
42,44,Blended emotion in-the-wild: Multi-label facial expression recognition using crowdsourced annotations and deep locality feature learning,"['S Li', 'W Deng']",2019,127,"Expression in-the-Wild, Static Facial Expression in the Wild","CNN, classification, classifier, deep learning, facial expression recognition, machine learning","multi-label facial expression database, RAF-ML, along with a new deep learning algorithm,  to  To address this limitation, we propose a novel deep learning model, called DBM-CNN, to",No DOI,International Journal of Computer Vision,https://link.springer.com/article/10.1007/s11263-018-1131-1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,springer.com,
43,45,Categorization and evaluation of emotional faces in psychopathic women,"['H Eisenbarth', 'GW Alpers', 'D Segrè', 'A Calogero']",2008,117,Karolinska Directed Emotional Faces,classification,"detailed picture of the emotion decoding deficit through the dimensional rating. Furthermore,  the paradigm combines a quite obvious question on emotion in the valence dimension and",No DOI,Psychiatry …,https://www.sciencedirect.com/science/article/pii/S0165178107003265,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
44,46,Classifying affective states using thermal infrared imaging of the human face,"['BR Nhan', 'T Chau']",2009,221,Affective Faces Database,classifier,"For similar reasons, we also used adjusted accuracy to estimate the performance of the  linear classifier on the unseen test data in the external cross validation. This measure is",No DOI,IEEE Transactions on Biomedical …,https://pubmed.ncbi.nlm.nih.gov/19923040/,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,nih.gov,
45,47,Classifying emotions and engagement in online learning based on a single facial expression recognition neural network,"['AV Savchenko', 'LV Savchenko']",2022,201,"Acted Facial Expressions In The Wild, Affective Faces Database, Static Facial Expression in the Wild","classifier, deep learning, machine learning, neural network","our models on EngageWild [8], AFEW (Acted Facial Expression In The Wild) [21] and VGAF  (Video- One of the first techniques that applied machine learning and FER to predict student’s",No DOI,IEEE Transactions on …,https://ieeexplore.ieee.org/document/9815154,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,ieee.org,
46,48,Classifying pretended and evoked facial expressions of positive and negative affective states using infrared measurement of skin temperature,"['MM Khan', 'RD Ward', 'M Ingleby']",2009,116,Affective Faces Database,"classification, classifier","Similarly, our evoked expression classifier (Table VI) could correctly classify 90% of the  sad faces and 70% faces of disgust. The facial expression of anger could not be",No DOI,ACM Transactions on Applied …,https://dl.acm.org/doi/10.1145/1462055.1462061,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,acm.org,
47,49,"Collecting large, richly annotated facial-expression databases from movies","['A Dhall', 'R Goecke', 'S Lucey', 'T Gedeon']",2012,690,Acted Facial Expressions In The Wild,classification,"facial expression database Acted Facial Expressions in the Wild (AFEW) and its static subset  Static Facial Expressions  and classification techniques. However, in the case of human",No DOI,IEEE multimedia,https://ieeexplore.ieee.org/document/6200254,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
48,50,Color face recognition for degraded face images,"['JY Choi', 'YM Ro', 'KN Plataniotis']",2009,140,Toronto Face Database,classification,", ON M5B 2K3, Canada (e-mail: kostas@comm.toronto.edu). Color versions of one or more   classification tasks [22], [25], [26]? To our knowledge, however, the color effect on face",No DOI,IEEE Transactions on …,https://ieeexplore.ieee.org/document/4804691,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
49,51,Color local texture features for color face recognition,"['JY Choi', 'YM Ro', 'KN Plataniotis']",2011,206,Toronto Face Database,classification,be used to enhance classification/recognition performance.  color texture methods for  classification tasks on texture images  information can improve classification performance obtained,No DOI,IEEE transactions on image …,https://ieeexplore.ieee.org/iel5/83/4358840/06020798.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
50,52,Combining modality specific deep neural networks for emotion recognition in video,"['SE Kahou', 'C Pal', 'X Bouthillier', 'P Froumenty']",2013,438,Toronto Face Database,"deep learning, neural network","neural network trained to predict emotions from static frames using two large data sets, the  Toronto Face Database and our own set of faces  profile results with deep learning, here we",No DOI,Proceedings of the 15th …,https://dl.acm.org/doi/10.1145/2522848.2531745,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,acm.org,
51,53,Covariance pooling for facial expression recognition,"['D Acharya', 'Z Huang', 'D Pani Paudel']",2018,216,"Affective Faces Database, Static Facial Expression in the Wild",facial expression recognition,methods on riemannian manifold for emotion recognition in the wild. In Proceedings of the   Image based static facial expression recognition with multiple deep network learning. In Pro,No DOI,… pattern recognition …,https://arxiv.org/abs/1805.04855,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"Covariance Pooling for Facial Expression Recognition
DineshAcharya†,ZhiwuHuang†,DandaPaniPaudel†,LucVanGool†‡
†ComputerVisionLab,ETHZurich,Switzerland ‡VISICS,KULeuven,Belgium
{acharyad, zhiwu.huang, paudel, vangool}@vision.ee.ethz.ch
Abstract
Classifying facial expressions into different categories
requirescapturingregionaldistortionsoffaciallandmarks.
Webelievethatsecond-orderstatisticssuchascovarianceis
betterabletocapturesuchdistortionsinregionalfacialfea- Figure1.Top:sampleimagesofdifferentfacialexpressionclasses
tures. Inthiswork,weexplorethebenefitsofusingaman- fromtheSFEWdataset.Bottom:distortionofregionbetweentwo
ifold network structure for covariance pooling to improve eyebrowsinthecorrespondingfacialimages.
facialexpressionrecognition. Inparticular,wefirstemploy
such kind of manifold networks in conjunction with tradi-
in Figure 1, facial expression recognition is more directly
tionalconvolutionalnetworksforspatialpoolingwithinin-
related to how facial landmarks are distorted rather than
dividualimagefeaturemapsinanend-to-enddeeplearning
presenceorabsenceofspecificlandmarks. Webelievethat
manner. Bydoingso,weareabletoachievearecognition
second-orderstatisticsismoresuitedtocapturesuchdistor-
accuracy of 58.14% on the validation set of Static Facial
tionsthanfirst-orderstatistics. Tolearnsecond-orderinfor-
ExpressionsintheWild(SFEW2.0)and87.0%onthevali-
mation deeply, we introduce covariance pooling after final
dationsetofReal-WorldAffectiveFaces(RAF)Database1.
convolutional layers. For further dimensionality reduction
Both of these results are the best results we are aware of.
weborrowtheconceptsfromthemanifoldnetwork[11]and
Besides,weleveragecovariancepoolingtocapturethetem-
train it together with conventional CNNs in an end-to-end
poralevolutionofper-framefeaturesforvideo-basedfacial
fashion. It is important to point out this is not a first work
expression recognition. Our reported results demonstrate
tointroducesecond-orderpoolingtotraditionalCNNs. Co-
the advantage of pooling image-set features temporally by
variance pooling was initially used in [13] for pooling co-
stackingthedesignedmanifoldnetworkofcovariancepool-
variance matrix from the outputs of CNNs. [25] proposed
ingontopofconvolutionalnetworklayers.
analternativetocomputesecond-orderstatisticsintheset-
tingofCNNs. However, suchtwoworksdonotuseeither
dimensionality reduction layers or non-linear rectification
1.Introduction
layersforsecond-orderstatistics. Inthispaper, wepresent
Facial expressions play an important role in communi- astrongmotivationforexploringtheminthecontextoffa-
cating the state of our mind. Both humans and computer cialexpressionrecognition.
algorithms can greatly benefit from being able to classify In addition to being better able to capture distortions in
facial expressions. Possible applications of automatic fa- regionalfacialfeatures,covariancepoolingcanalsobeused
cial expression recognition include better transcription of tocapturetemporalevolutionofper-framefeatures.Covari-
videos, movie or advertisement recommendations, detec- ance matrix has been employed before to summarize per-
tionofpainintelemedicineetc. framefeatures[17]. Inthiswork,weexperimentwithusing
Traditional convolutional neural networks (CNNs) that manifoldnetworksforpoolingper-framefeatures.
useconvolutionallayers,maxoraveragepoolingandfully Insummary,thecontributionofthispaperistwo-fold:
connected layers are considered to capture only first-order
• End-to-endpoolingofsecond-orderstatisticsforboth
statistics [25]. Second-order statistics such as covariance
videos and images in the context of facial expression
are considered to be better regional descriptors than first-
recognition
orderstatisticssuchasmeanormaximum[20]. Asshown
1The code of this paper will be eventually released on https:// • State-of-art result on image-based facial expression
github.com/d-acharya/CovPoolFER recognition
1
8102
yaM
31
]VC.sc[
1v55840.5081:viXra2.RelatedWorks use various techniques to capture the temporal evolution
of the per-features. For example, LSTMs have been suc-
Though facial expression recognition from both images
cessfullyemployedwithvariousnamessuchasCNN-RNN,
and videos are closely related, they each have their own
CNN-BRNN etc [8][9][23]. 3D convolutional neural net-
challenges. Videos contain dynamic information which a
workshavealsobeenusedforfacialexpressionrecognition.
singleimagelacks. Withthisadditionaldynamicinforma-
However, performance of a single 3D-ConvNet was worse
tion, we should theoretically be able to improve facial ex-
than applying LSTMs on per-frame features [9]. State-of-
pression accuracy. However, extracting information from
art result reported in [9] was obtained by score fusion of
videos has its own challenges. In following sub-sections,
multiplemodelsof3D-ConvNetsandCNN-RNNs.
webrieflyreviewstandardapproachestofacialexpressions
onbothimageandvideo-basedapproaches. Covariancematrixrepresentationwasusedasoneofthe
summary statistics of per-frame features in [17]. Kernel-
2.1.FacialExpressionRecognitionfromImages basedpartialleastsquares(PLS)werethenusedforrecog-
nition.Here,weusethemethodsin[17]asbaselineanduse
Mostoftherecentapproachesinfacialexpressionrecog-
theSPDRiemanniannetworksinsteadofkernelbasedPLS
nitionfromimagesusevariousstandardarchitecturessuch
forrecognitionandobtainslightimprovement.
asVGGnetworks,Inceptionnetworks,Residualnetworks,
Inception-ResidualNetworksetc[3][7][21]. Manyofthese
workscarryoutpretrainingonFER-2013,facerecognition
3. Facial Expression Recognition and Covari-
datasetsorsimilardatasetsandeitheruseoutputsfromfully
ancePooling
connectedlayersasfeaturestotrainclassifiersorfine-tune
thewholenetwork. UseofensembleofmultipleCNNsand
3.1.Overview
fusionofthepredictedscoresisalsowidelyusedandfound
to be successful. For example, in Emotiw2015 sub chal-
Facial expression is localized in the facial region
lenge on image-based facial expression recognition, both
whereas images in the wild contain large irrelevant infor-
winners and runner up teams [15][26] employed ensemble
mation. Due to this, face detection is performed first and
of CNNs to achieve the best reported score. There, pre-
thenalignedbasedonfaciallandmarklocations. Next, we
training was done on FER-2013 dataset. Recently, in [3],
feed the normalized faces into a deep CNN. To pool the
authors reported validation accuracy of 54.82% which is a
feature maps spatially from the CNN, we propose to use
state-of-art result for a single network. The accuracy was
covariancepooling,andthenemploythemanifoldnetwork
achieved using VGG-VD-16. The authors carried out pre-
[11]todeeplylearnthesecond-orderstatistics.Thepipeline
trainingonVGGFacesandFER-2013.
of our proposed model for image-based facial expression
All such networks discussed above employ traditional
recognitionisshowninFigure2.
neural network layers. These architectures can be consid-
eredtocaptureonlyfirst-orderstatistics. Covariancepool- As the case of image-based facial expression recogni-
ing,ontheotherhandcapturessecond-orderstatistics. One tion,videosinthewildcontainlargeirrelevantinformation.
oftheearliestworksemployingcovariancepoolingforfea- First, all the frames are extracted from a video. Face de-
ture extraction used it as regional descriptor [6][20]. In tectionandalignmentisthenperformedoneachindividual
[25], authors propose various architectures based on VGG frame. Depending on the feature extraction algorithm, ei-
network to employ covariance pooling. In [11], authors therimagefeaturesareextractedfromthenormalizedfaces
present a deep learning architecture for learning on Rie- or the normalized faces are concatenated and 3d convolu-
mannian manifold which can be employed for covariance tions are applied to the concatenated frames. Intuitively,
pooling. as the temporal convariance can capture the useful facial
motion pattern, we propose to pool the frames over time.
2.2.FacialExpressionRecognitionfromVideos Todeeplylearnthetemporalsecond-orderinformation,we
also employ the manifold network [11] for dimensionality
Traditionally, video-based recognition problems used
reduction and non-linearity on covariance matrices. The
per-framefeaturessuchasSIFT,dense-SIFT,HOG[17]and
overviewofourpresentedmodelforvideo-basedfacialex-
recentlydeepfeaturesextractedwithCNNshavebeenused
pressionrecognitionisillustratedinFigure3.
[9][4]. Theper-framefeaturesarethenusedtoassignscore
to each individual frame. Summary statistics of such per- Accordingly, the core techniques of the two proposed
frame features are then used for facial expression recog- models are spatial/temporal covariance pooling and the
nition. In [24], authors propose modification of Inception manifold network for learning the second-order features
architecture to capture action unit activation which can be deeply. In the following we will introduce the two crucial
beneficial for facial expression recognition. Other works techniques.Figure2.Inordertoleveragecovariancepoolingonimage-based
Figure 3. In case of video-based facial expression recognition
facial expression recognition problem, output of convolutional
problems,outputoffullyconnectedlayersareconsideredasimage
layerisflattenedasillustrated.Thecovariancematrixiscomputed
setfeatures. Thecovariancematrixiscomputedfromsuchimage
formresultingvectors.
setfeatures.
3.2.CovariancePooling
covarianceasinEqn1andregularizingthuscomputedma-
As discussed earlier, traditional CNNs that consist of trixusingEqn.2.
fully connected layers, max or average pooling and con-
volutional layers only capture first-order information [25].
ReLUintroducesnon-linearitybutdoessoonlyatindivid- Covariance Matrix for Temporal Pooling: As illus-
ual pixel level. Covariance matrices computed from fea- trated in Figure 3, covariance pooling can be employed in
turesarebelievedtobebetterabletocaptureregionalfea- [17] to pool temporal features. If f 1,f 2,...,f n ∈ Rd be
turesthanfirst-orderstatistics[20]. per-framefeaturesextractedfromimages,wecancompute
Givenasetoffeatures,covariancematrixcanbeusedto covariance matrix using the Eqn. 1 and regularize it using
compactly summarize the second-order information in the Eqn.2.
set. Iff ,f ,...,f ∈Rd bethesetoffeatures,thecovari-
1 2 n
ancematrixcanbecomputedas: 3.3.SPDManifoldNetwork(SPDNet)Layers
n The covariance matrices thus obtained typically reside
C= 1 (cid:88) (f −¯f)(f −¯f)T, (1) ontheRiemannianmanifoldofSPDmatrices.Directlyflat-
n−1 i i
i=1 tening and applying fully connected layers directly causes
where¯f = 1 (cid:80)n f . lossofgeometricinformation.Standardmethodsapplylog-
n i=1 i arithmoperationtoflattentheRiemannianmanifoldstruc-
Thematricesthusobtainedaresymmetricpositivedefi-
turetobeabletoapplystandardlossfunctionsofEuclidean
nite (SPD) only if number of linearly independent compo-
space [6][20]. The covariance matrices thus obtained are
nents in {f ,f ,...,f } is greater than d. In order to em-
1 2 n
oftenlargeandtheirdimensionneedstobereducedwithout
ploy the geometric structure preserving layers of the SPD
losing geometric structure. In [11], authors introduce spe-
manifoldnetwork[11],thecovariancematricesarerequired
cial layers for reducing dimension of SPD matrices and to
tobeSPD.However,evenifthematricesareonlypositive
flattentheRiemannianmanifoldtobeabletoapplystandard
semidefinite,theycanberegularizedbyaddingamultiple
lossfunctions.
oftracetodiagonalentriesofthecovariancematrix:
In this subsection, we briefly discuss the layers intro-
C+ =C+λtrace(C)I, (2) ducedin[11]forlearningonRiemannianManifold.
whereλisaregularizationparameterandIisidentityma-
trix. Bilinear Mapping Layer (BiMap) Covariance matrices
computedfromfeaturescanbelargeanditmaynotbefea-
CovarianceMatrixforSpatialPooling: Inordertoap- sibletodirectlyapplyfullyconnectedlayersafterflattening
ply covariance pooling to image-based facial expression them. Furthermore,itisalsoimportanttopreservegeomet-
recognition problem, as shown in Figure 2, outputs from ric structure while reducing dimension. The BiMap layer
finalconvolutionallayerscanbeflattenedandusedtocom- accomplishes both of these conditions and plays the same
pute covariance matrix. Let X ∈ Rw×h×d be the output roleastraditionalfullyconnectedlayers. IfX k−1 beinput
obtained after several convolutional layers, where w,h,d SPDmatrix,W k ∈Rd ∗k×dk−1 beweightmatrixinthespace
standforwidth,heightandnumberofchannelsintheoutput of full rank matrices and X k ∈ Rdk×dk be output matrix,
respectively. XcanbeflattenedasanelementX(cid:48) ∈ Rn×d thenk-ththebilinearmappingfk isdefinedas
b
wheren = w×h. Iff ,f ,...,f ∈ Rd becolumnsofX(cid:48),
1 2 n
wecancapturethevariationacrosschannelsbycomputing X =fk(X ;W )=W X WT. (3)
k b k−1 k k k−1 kpreparedbyselectingframesfromvideosofAFEWdataset.
Facial landmark points provided by the authors were de-
tected using mixture-of-parts based model [28]. The land-
marks thus obtained were then used for alignment. The
RAF dataset [16] was prepared by collecting images from
various search engines and the facial landmarks were an-
notated manually by 40 independent labelers. The dataset
Figure 4. Illustration of SPD Manifold Network (SPDNet) with
contains 15331 images labeled with seven basic emotion
2-BiRelayers.
categories of which 3068 are to be used for validation and
12271fortraining.
Eigenvalue Rectification (ReEig) ReEig layer can be It is worth pointing out that there exist several other
used to introduce non-linearity in the similar way as Rec- image-based datasets such as EmotioNet [5] and FER-
tified Linear Unit (ReLU) layers in traditional neural net- 2013 [10]. However, they have their own downsides.
works. If X k−1 be input SPD matrix, X k be output and (cid:15) ThoughEmotioNetisthelargestexistingdatasetforfacial
be eigenvalue rectification threshold, k-th ReEig Layer f rk expression recognition, the images were automatically an-
isdefinedas: notatedandthelabelsareincomplete. FER-2013,contains
relativelysmallimagesizeanddoesnotcontainRGBinfor-
X =fk(X )=U max((cid:15)I,σ )UT , (4)
k r k−1 k−1 k−1 k−1 mation. Most other databases either contain too few sam-
plesoraretakeninwellposedlaboratorysetting.
where U and Σ are defined by eigenvalue decom-
k−1 k−1
positionX = U Σ UT . Themaxoperationis
k−1 k−1 k−1 k−1
element-wisematrixoperation. Video-basedFacialExpressionRecognition Forvideo-
based facial expression recognition, we use Acted Facial
Log Eigenvalue Layer (LogEig) As discussed earlier, Expressions in the Wild (AFEW) dataset to compare our
SPD matrices lie on Riemannian manifold. The final Lo- methodswithexistingmethods. Thisdatasetwasprepared
gEig layer endows elements in Riemannian manifold with by selecting videos from movies. It contains about 1156
aLieGroupstructuresothatmatricescanbeflattenedand publicly available labeled videos of which 773 videos are
standard euclidean operations can be applied. If X be used for training and 383 for validation. Just as in case
k−1
inputmatrix,X beoutputmatrix,theLogEiglayerapplied of SFEW 2.0 dataset, the landmarks and aligned images
k
ink-thlayerfk isdefinedas provided by authors were obtained using mixture-of-parts
l
basedmodel.
X =fk(X )=log(X )=U log(Σ )UT , Thoughthereexistseveralotherfacialexpressionrecog-
k l k−1 k−1 k−1 k−1 k−1
(5) nition databases for videos such as Cohn-Kanade/Cohn-
whereX = U Σ UT isaneigenvaluedecompo- Kanade+(CK/CK+)[14][18],mostofthemareeithersam-
k k−1 k−1 k−1
sitionandlogisanelement-wisematrixoperation. pled in well controlled laboratory environment or are la-
BiMapandReEiglayerscanbeusedtogetherasablock beled with action unit encoding rather than seven basic
and is abbreviated as BiRe. The architecture of a SPDNet classesoffacialexpressions.
with2-BiRelayersisshowninFigure4.
4.2.FaceDetectionandAlignment
4.Experiments
Authors of RAF database [16] provide manually anno-
4.1.BenchmarkDatasets tated face landmarks, while those of SFEW 2.0 [2] and
AFEW [1] datasets do not and instead provide landmarks
Datasets that contain samples with either real or acted and aligned images obtained using mixture-of-parts based
facial expressions in the wild were chosen. Such datasets model[28]. Imagesandvideoscapturedinthewildcontain
are better approximation to the real world scenarios than large amount of non-essential information. Face detection
poseddatasetsandarealsomorechallenging. andalignmenthelpsremovenon-essentialinformationfrom
thedatasamples. Furthermore,tobeabletocomparevari-
Image-based Facial Expression Recognition For com- ationsinlocalfacialfeaturesacrossimages,facealignment
paring our deep learning architectures for image-based fa- is important. This serves as normalization of data. While
cialexpressionrecognitionagainststandardresults,weuse tryingtocategorizefacialexpressionsfromvideos,motion
Static Facial Expressions in the Wild (SFEW) 2.0 [2] [1] ofpeople,changeofbackgroundetc. alsoleadtolargeun-
datasetandReal-worldAffectiveFaces(RAF)dataset[16]. wantedvariationacrossimageframes. Duetothis,training
SFEW 2.0 contains 1394 images, of which 958 are to be algorithmsonoriginalunaligneddataisnotfeasible. Face
used for training and 436 for validation. This dataset was alignmentadditionallyhelpstocapturethedynamicevolu-RAF SFEW2.0 IncorporationofSPDManifoldNetwork Asdiscussed
Models
Total Total above, we introduce covariance pooling and subsequently
the layers from the SPD manifold network (SPDNet) after
VGG-VD-16network[3] - 54.82
thefinalconvolutionallayer. Whileintroducingcovariance
Inception-ResnetV1
82.6 47.37 pooling, we experimented with various models for the ar-
(Trainedfromscratch)‡
chitecture. Thedetailsofthevariousmodelsconsideredare
Inception-ResnetV1
83.4 51.9 summarizedinTable2.
(Finetuned)‡
BaselineModel ‡ 84.5 54.45
Baseline Mode- Model-2 Model-3 Model-4
Table1.Comparisonofimage-baserecognitionaccuraciesofvar-
1
iousstandardmodelsonvalidationsetoftheRAFandSFEW2.0
datasets.Herethemodelslabelled‡weretrainedonourown. Conv256 Conv256 Conv256 Conv256 Conv256
Cov Cov Cov Cov
BiRe BiRe BiRe BiRe
tionoflocalfacialfeaturesacrossimagesofthesamevideos
LogEig LogEig BiRe LogEig
inaneffectivemanner.
LogEig
For face and facial landmark detection Multi-task Cas-
cade Convolutional Neural Networks (MTCNN) [27] was
FC2000 FC2000 FC2000 FC2000 FC2000
used. MTCNN was found to be more accurate and suc-
FC7 FC7 FC128 FC7 FC512
cessful for alignment compared to mixture-of-parts based
FC7 FC7
model. After successful face and facial landmark detec-
tion, we use three points constrained affine transformation Table 2. Various models considered for covariance pooling. For
brevity,initialconvolutionlayersareignored.
for face alignment. Coordinates of left eye, right eye and
midpointofcornersofthelipswereusedforalignment.
4.4.ResultsonImage-basedProblem
4.3. Baseline Model and Architectures for Image-
basedProblem
Covariance pooling was applied after final convolution
layer and before fully connected layers. Various models
Comparison of Standard Architectures In Table 1 we
described in Table 2 and their accuracies are listed below
presentthecomparisonofaccuraciesoftrainingorfinetun-
in Table 3. For the RAF database, as stated earlier, the
ing various standard network architectures. For a baseline
model, wetakethenetworkarchitecturepresentedin[16].
ThescoresreportedonRAFdatabaseforVGGnetworkand RAF SFEW2.0
Model
AlexNet in [16] is less compared to their base line model. TotalAccuracy TotalAccuracy
Sothenetworksarenottrainedagainhere.Itisworthpoint-
BaselineModel [16] 84.7 54.45
ingoutthatthere,authorsreportperclassaverageaccuracy
Model-1 86.3 55.40
butwereporttotalaccuracyonlyhere. Here,weusecenter
Model-2 87.0 56.72
loss[22] to train the network in all cases rather than local-
Model-3 85.0 57.48
ity preserving loss[16] as we do not deal with compound
Model-4 85.4 58.14
emotions. In all cases, dataset was augmented using stan-
dard techniques such as random crop, random rotate and VGG-VD-16[3] - 54.82
random flip. For SFEW 2.0, in all cases, output of sec- EmotiW-1(2015)[26] - 55.96
ond to last fully connected layer was used as image fea- EmotiW-2(2015)[15] - 52.80
tures and Support Vector Machines (SVMs) were trained Table 3. Image-based recognition accuracies for various models
separately. Notethatthemodelslabelled‡ weretrainedon withandwithoutcovariancepooling.
our own. Inception-ResnetV1 [19] was both trained from
scratch,aswellasfinetunedonamodelpre-trainedonsub- network was trained in end-to-end fashion. However, for
setofMS-Celeb-1Mdataset.Itisevidentfromthetablethat SFEW2.0dataset,weuseoutputofpenultimatefullycon-
fine-tuning the Inception-ResnetV1 trained on face recog- nected layer (which ranges from 128 to 2000 dimensional
nitiondatasetperformsbetterthantrainingfromscratch. It feature depending on the model considered). It is worth
shouldnotcomeasasurprisethatarelativelysmallnetwork pointingoutthatforSFEW2.0oursinglemodelperformed
outperformsInception-ResNetmodelastherearemorepa- better than ensemble of convolutional neural networks in
rameters to be learned in deeper models. For further ex- [26] and [15]. It could be argued that the datasets used
periments and to introduce covariance pooling, we use the for pre-training were different in our case and in [26][15].
baselinemodelfrom[16]. However,improvementofalmost3.7%overbaselineintheSFEW2.0datasetjustifiestheuseofSPDNetforfacialex-
pressionrecognition.
It is also important to point out that on the SFEW 2.0
andAFEWdatasets,facedetectionfailedinseveralimages
and videos. To report validation score, we assign random
uniform probability of success (1) for correct recognition
7
tothesamplesonwhichfacedetectiondidnotsucceed.
Figure 7. Confusion matrices for our method (4-Bire) on the
AFEWdataset.
4.6.ResultsonVideo-basedProblem
The results of our proposed methods, baseline method
and the accuracies of other C3D and CNN-RNN models
from[9]arepresentedforcontext. However,datasetsused
Figure5.ConfusionmatrixforModel-2ontheRAFdataset. forthosepretrainingothermodelsarenotuniform,andde-
tailed comparison of all existing methods is not within the
scope of this work. As seen from Table 5, our model was
4.5. Baseline Model for Video-based Recognition
able to slightly surpass the results of the base line model.
Problem
Our method also performed better than all single models
ForcomparingthebenefitsofusingSPDNetoverexist- thatweretrainedonpubliclyavailabletrainingdataset. The
ingmethods,weusekernelbasedPLSthatusedcovariance networkfrom[4]wastrainedonprivatedatasetcontaining
matrices as features [17] in baseline method. 128 dimen- anorderofmagnitudemoresamples. Asasideexperimen-
sional features were extracted from each image frame of a tation,wealsointroducedcovariancepoolingtoC3Dmodel
video and the video was modeled with a covariance ma- in[9]anddidnotobtainanyimprovement.Weobtainedac-
trix. TheneitherSPDNetorkernelbasedSVMwitheither curacyof39.78%.
RBF or Polynomial kernel were used for recognition. The
SPDNetwasabletooutperformothermethods. 5.Conclusion
Inthiswork,weexploittheuseofSPDNetonfacialex-
pression recognition problems. As shown above, SPDNet
appliedtocovarianceofconvolutionalfeaturescanclassify
facial expressions more efficiently. We study that second-
order networks are better able to capture facial landmark
distortions. Similarly, covariance matrix computed from
image feature vectors were used as input to SPDNet for
video-basedfacialexpressionrecognitionproblem.
Wewereabletoobtainstate-of-the-artresultsonimage-
basedfacialexpressionrecognitionproblemsontheSFEW
2.0 and RAF datasets. In video-based facial expression
recognition,trainingSPDNetonimage-basedfeatureswas
abletoobtainresultscomparabletostate-of-the-artresults.
In the context of video-based facial expression recog-
nition problem, architecture presented in Figure 8 can be
trained in end-to-end training. Though with brief experi-
Figure6.ConfusionmatrixforModel-4ontheSFEW2.0dataset. mentation, wewereabletoobtainaccuracyofonly32.5%OriginalClass CorrectlyPredicted IncorrectlyPredicted PredictedClasses
Neutral, Neutral, Neutral,
Angry
Neutral,Happy
Sad, Sad, Surprise, Sad,
Disgust
Neutral
Happy,Happy,Neutral,An-
Fear
gry,Happy
Sad, Neutral, Neutral, Sad,
Happy
Angry
Angry, Happy, Happy,
Neutral
Happy,Happy
Neutral,Angry,Happy,Sur-
Sad
prise,Neutral
Happy,Happy,Happy,Neu-
Surprise
tral,Happy
Table4.SamplesfromeachclassoftheSFEWdatasetthatweremostaccuratelyandleastaccuratelyclassified.Thefirstcolumnindicates
groundtruthandfinalcolumnindicatespredictedlabelsforincorrectlypredictedimages.
6.FurtherWorks
Model AFEW
VGG13[4] 57.07 Inthiswork,weleveragedcovariancematrixtocapture
SingleBestCNN-RNN[9] 45.43 second-orderstatistics. Asstudiedin[12],Gaussianmatrix
SingleBestC3D[9] 39.69 isabletofurtherimprovetheeffectivenessofsecond-order
SingleBestHoloNet[23] 44.57 statistics. Formally, the SPD form of Gaussian matrix can
Baseline(RBFKernel)[17] 45.95 becomputedby
Baseline(PolyKernel)[17] 45.43
Ourproposedmethod(2-Bire) 42.25 (cid:18) Σ+µµT µ(cid:19)
G= , (6)
Ourproposedmethod(3-Bire) 44.09 µT 1
Ourproposedmethod(4-Bire) 46.71
whereΣisthecovariancematrixdefinedinEqn.1,and
MultipleCNN-RNNandC3D(cid:63)(cid:63)[9] 51.8
VGG13+VGG16+ResNet(cid:63)(cid:63)[23] 59.16 (cid:88)n
µ= f (7)
Table 5. Video-based recognition accuracies for various single i
i=1
models and fusion of multiple models. Here the results of the
methods marked with (cid:63)(cid:63) were obtained either by score level or isthemeanofthesamplesf ,f ,...,f capturesbothfirst-
1 2 n
featurelevelfusionofmultiplemodels.
order and second-order statistics. It was also employed
in [25] to develop second-order convolutional neural net-
works. Extending current work from covariance pooling
whichisworsethanthescorereported [11].Itislikelytobe
to Gaussian pooling would be an interesting direction and
aresultofrelativelysmallsizeofAFEWdatasetcompared
shouldtheoreticallyimproveresults.
toparametersinthenetwork. Furtherworkisnecessaryto
seeiftrainingend-to-endusingjointconvolutionalnetand References
SPDnetcanimproveresults.
[1] J. J. K. S. A. Dhall, R. Goecke and T. Gedeon. Emotion
recognition in the wild challenge 2014: Baseline, data and
protocol. InACMICMI,2014. 4
[2] S.L.A.Dhall,R.GoeckeandT.Gedeon. Collectinglarge,
richlyannotatedfacialexpressiondatabasesfrommovies. In
IEEEMultiMedia19,2012. 4
Figure8.Architectureforend-to-endtrainingonvideosdirectly. [3] S.AlbanieandA.Vedaldi. Learninggrimacesbywatching
tv. InProceedingsoftheBritishMachineVisionConference
(BMVC),2016. 2,5[4] S.A.Bargal,E.Barsoum,C.C.Ferrer,andC.Zhang. Emo- [16] S.Li,W.Deng,andJ.Du. Reliablecrowdsourcinganddeep
tion recognition in the wild from videos using images. In locality-preservinglearningforexpressionrecognitioninthe
Proceedings of the 18th ACM International Conference on wild. InTheIEEEConferenceonComputerVisionandPat-
Multimodal Interaction, ICMI 2016, pages 433–436, New ternRecognition(CVPR),July2017. 4,5
York,NY,USA,2016.ACM. 2,6,7 [17] M. Liu, R. Wang, S. Li, S. Shan, Z. Huang, and X. Chen.
[5] C. F. Benitez-Quiroz, R. Srinivasan, and A. M. Martinez. Combining multiple kernel methods on riemannian mani-
Emotionet: An accurate, real-time algorithm for the auto- fold for emotion recognition in the wild. In Proceedings
maticannotationofamillionfacialexpressionsinthewild. ofthe16thInternationalConferenceonMultimodalInterac-
In2016IEEEConferenceonComputerVisionandPattern tion,ICMI’14,pages494–501,NewYork,NY,USA,2014.
Recognition(CVPR),pages5562–5570,June2016. 4 ACM. 1,2,3,6,7
[6] J. a. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. [18] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar,
Semanticsegmentationwithsecond-orderpooling. InPro- andI.Matthews. Theextendedcohn-kanadedataset(ck+):
ceedingsofthe12thEuropeanConferenceonComputerVi- A complete dataset for action unit and emotion-specified
sion - Volume Part VII, ECCV’12, pages 430–443, Berlin, expression. In 2010 IEEE Computer Society Conference
Heidelberg,2012.Springer-Verlag. 2,3 on Computer Vision and Pattern Recognition - Workshops,
[7] H. Ding, S. K. Zhou, and R. Chellappa. Facenet2expnet: pages94–101,June2010. 4
Regularizing a deep face recognition net for expression [19] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi.
recognition. In 2017 12th IEEE International Conference Inception-v4, inception-resnet and the impact of residual
on Automatic Face Gesture Recognition (FG 2017), pages connectionsonlearning. InAAAI,2017. 5
118–126,May2017. 2 [20] O.Tuzel,F.Porikli,andP.Meer. Regioncovariance: Afast
[8] S.EbrahimiKahou,V.Michalski,K.Konda,R.Memisevic, descriptorfordetectionandclassification. InProceedingsof
and C. Pal. Recurrent neural networks for emotion recog- the9thEuropeanConferenceonComputerVision-Volume
nitioninvideo. InProceedingsofthe2015ACMonInter- PartII,ECCV’06,pages589–600,Berlin,Heidelberg,2006.
national Conference on Multimodal Interaction, ICMI ’15, Springer-Verlag. 1,2,3
pages467–474,NewYork,NY,USA,2015.ACM. 2
[21] F.Wang,X.Xiang,C.Liu,T.D.Tran,A.Reiter,G.D.Hager,
[9] Y.Fan,X.Lu,D.Li,andY.Liu.Video-basedemotionrecog- H.Quon,J.Cheng,andA.L.Yuille. Regularizingfacever-
nitionusingcnn-rnnandc3dhybridnetworks. InProceed- ification nets for pain intensity regression. 2017 IEEE In-
ings of the 18th ACM International Conference on Multi- ternational Conference on Image Processing (ICIP), pages
modalInteraction, ICMI2016, pages445–450, NewYork, 1087–1091,2017. 2
NY,USA,2016.ACM. 2,6,7
[22] Y.Wen,K.Zhang,Z.Li,andY.Qiao.ADiscriminativeFea-
[10] I. J. Goodfellow, D. Erhan, P. Luc Carrier, A. Courville,
ture Learning Approach for Deep Face Recognition, pages
M. Mirza, B. Hamner, W. Cukierski, Y. Tang, D. Thaler,
499–515. SpringerInternationalPublishing,Cham,2016. 5
D.-H.Lee, Y.Zhou, C.Ramaiah, F.Feng, R.Li, X.Wang,
[23] J.Yan,W.Zheng,Z.Cui,C.Tang,T.Zhang,Y.Zong,and
D. Athanasakis, J. Shawe-Taylor, M. Milakov, J. Park,
N. Sun. Multi-clue fusion for emotion recognition in the
R. Ionescu, M. Popescu, C. Grozea, J. Bergstra, J. Xie,
wild. InProceedingsofthe18thACMInternationalConfer-
L.Romaszko,B.Xu,Z.Chuang,andY.Bengio. Challenges
enceonMultimodalInteraction,ICMI2016,pages458–463,
inrepresentationlearning. NeuralNetw.,64(C):59–63,Apr.
NewYork,NY,USA,2016.ACM. 2,7
2015. 4
[24] A.Yao, J.Shao, N.Ma, andY.Chen. Capturingau-aware
[11] Z. Huang and L. V. Gool. A riemannian network for spd
facialfeaturesandtheirlatentrelationsforemotionrecogni-
matrixlearning. InAAAI,2017. 1,2,3,7
tioninthewild. InProceedingsofthe2015ACMonInter-
[12] Z. Huang, R. Wang, S. Shan, X. Li, and X. Chen. Log-
national Conference on Multimodal Interaction, ICMI ’15,
euclidean metric learning on symmetric positive definite
pages451–458,NewYork,NY,USA,2015.ACM. 2
manifold with application to image set classification. In
[25] K.YuandM.Salzmann. Second-orderconvolutionalneural
ICML,pages720–729,2015. 7
networks. CoRR,abs/1703.06817,2017. 1,2,3,7
[13] C.Ionescu,O.Vantzos,andC.Sminchisescu. Matrixback-
[26] Z. Yu and C. Zhang. Image based static facial expression
propagationfordeepnetworkswithstructuredlayers. 2015
recognition with multiple deep network learning. In Pro-
IEEEInternationalConferenceonComputerVision(ICCV),
ceedings of the 2015 ACM on International Conference on
pages2965–2973,2015. 1
Multimodal Interaction, ICMI ’15, pages 435–442, New
[14] T.Kanade,J.F.Cohn,andY.Tian.Comprehensivedatabase
York,NY,USA,2015.ACM. 2,5
forfacialexpressionanalysis. InProceedingsFourthIEEE
[27] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao. Joint face detec-
International Conference on Automatic Face and Gesture
tion and alignment using multitask cascaded convolutional
Recognition(Cat.No.PR00580),pages46–53,2000. 4
networks. IEEE Signal Process. Lett., 23(10):1499–1503,
[15] B.-K.Kim,H.Lee,J.Roh,andS.-Y.Lee.Hierarchicalcom-
2016. 5
mitteeofdeepcnnswithexponentially-weighteddecisionfu-
[28] X. Zhu and D. Ramanan. Face detection, pose estimation
sionforstaticfacialexpressionrecognition. InProceedings
andlandmarkestimationinthewild.InIEEEConferenceon
ofthe2015ACMonInternationalConferenceonMultimodal
ComputerVisionandPatternRecognition(CVPR),2012. 4
Interaction,ICMI’15,pages427–434,NewYork,NY,USA,
2015.ACM. 2,5"
52,54,Decline or improvement?: Age-related differences in facial expression recognition,"['A Suzuki', 'T Hoshino', 'K Shigemasu', 'M Kawamura']",2007,173,Japanese Female Facial Expression,facial expression recognition,"emotion recognition. Currently, there is a growing emphasis on the inseparability between  emotional experiences and emotion recognition, particularly facial expression  the Japanese",No DOI,Biological psychology,https://www.sciencedirect.com/science/article/pii/S0301051106001669,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
53,55,"Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond","['D Kollias', 'P Tzirakis', 'MA Nicolaou']",2019,440,"Affective Faces Database, Expression in-the-Wild","CNN, classification, deep learning, machine learning",", we show that our network can be also used for other emotion  -Face network, pre-trained  for face recognition on the VGG- We note that all deep learning architectures have been",No DOI,International Journal of …,https://arxiv.org/abs/1804.10938,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"InternationalJournalofComputerVision-SpecialIssueonDeepLearningforFaceAnalysismanuscriptNo.
(willbeinsertedbytheeditor)
Deep Affect Prediction in-the-wild: Aff-Wild Database and Challenge,
Deep Architectures, and Beyond
DimitriosKollias(cid:63) · PanagiotisTzirakis† · MihalisA.Nicolaou∗ · Athanasios
Papaioannou(cid:107) · GuoyingZhao1 · Bjo¨rnSchuller2 · IreneKotsia3 · Stefanos
Zafeiriou4
Accepted:29January2019
Abstract Automatic understanding of human affect using architecturewhichperformspredictionofcontinuousemo-
visual signals is of great importance in everyday human- tion dimensions based on visual cues. The proposed deep
machine interactions. Appraising human emotional states, learningarchitecture,AffWildNet,includesconvolutionaland
behaviorsandreactionsdisplayedinreal-worldsettings,can recurrentneuralnetwork(CNN-RNN)layers,exploitingthe
be accomplished using latent continuous dimensions (e.g., invariantpropertiesofconvolutionalfeatures,whilealsomod-
the circumplex model of affect). Valence (i.e., how posi- eling temporal dynamics that arise in human behavior via
tive or negative is an emotion) and arousal (i.e., power of therecurrentlayers.TheAffWildNetproducedstate-of-the-
the activation of the emotion) constitute popular and effec- art results on the Aff-Wild Challenge. We then exploit the
tiverepresentationsforaffect.Nevertheless,themajorityof AffWild database for learning features, which can be used
collected datasets this far, although containing naturalistic as priors for achieving best performances both for dimen-
emotional states, have been captured in highly controlled sional,aswellascategoricalemotionrecognition,usingthe
recording conditions. In this paper, we introduce the Aff- RECOLA,AFEW-VAandEmotiW2017datasets,compared
Wildbenchmarkfortrainingandevaluatingaffectrecogni- toallothermethodsdesignedforthesamegoal.Thedatabase
tion algorithms. We also report on the results of the First andemotionrecognitionmodelsareavailableathttp://ibug.
Affect-in-the-wildChallenge(Aff-WildChallenge)thatwas doc.ic.ac.uk/resources/first-affect-wild-challenge.
recently organized in conjunction with CVPR 2017 on the
Keywords deep convolutional recurrent Aff-Wild
Aff-Wild database, and was the first ever challenge on the · · · ·
database challenge in-the-wild facial dimensional
estimationofvalenceandarousalin-the-wild.Furthermore, · · · · ·
categorical emotion recognition valence arousal
we design and extensively train an end-to-end deep neural · · · · ·
AffWildNet RECOLA AFEW AFEW-VA EmotiW
· · · ·
(cid:63)dimitrios.kollias15@imperial.ac.uk
†panagiotis.tzirakis12@imperial.ac.uk
1 Introduction
∗m.nicolaou@gold.ac.uk
(cid:107)a.papaioannou11@imperial.ac.uk
1guoying.zhao@oulu.fi Currentresearchinautomaticanalysisoffacialaffectaims
2bjoern.schuller@imperial.ac.uk at developing systems, such as robots and virtual humans,
3I.Kotsia@mdx.ac.uk
that will interact with humans in a naturalistic way under
4s.zafeiriou@imperial.ac.uk
real-world settings. To this end, such systems should auto-
QueensGate,LondonSW72AZ,UK maticallysenseandinterpretfacialsignalsrelevanttoemo-
tions,appraisalsandintentions.Moreover,sincereal-world
∗ Department of Computing, Goldsmiths University of London,
settingsentailuncontrolledconditions,wheresubjectsoper-
LondonSE146NW,U.K
ateinadiversityofcontextsandenvironments,systemsthat
1,4Center for Machine Vision and Signal Analysis, University performautomaticanalysisofhumanbehaviorshouldbero-
ofOulu,Oulu,Finland busttovideorecordingconditions,thediversityofcontexts
andthetimingofdisplay.1
3 Department of Computer Science, Middlesex University of
London,LondonNW44BT,U.K 1 Itiswellknownthattheinterpretationofafacialexpressionmay
dependonitsdynamics,e.g.posedvs.spontaneousexpressions[66].
9102
beF
1
]VC.sc[
5v83901.4081:viXra2 DimitriosKollias(cid:63)etal.
Fig.1:The2-DEmotionWheel
For the past twenty years research in automatic analy- To this end, the 2-D valence and arousal space is the most
sisoffacialbehaviorwasmainlylimitedtoposedbehavior usual dimensional emotion representation. Figure 1 shows
which was captured in highly controlled recording condi- the 2-D Emotion Wheel [43], with valence ranging from
tions[35,41,55,57].Somerepresentativedatasets,whichare verypositivetoverynegativeandarousalrangingfromvery
still used in many recent works [27], are the Cohn-Kanade activetoverypassive.
database[35,55],MMIdatabase[41,57],Multi-PIEdatabase Some emotion recognition databases exist in the liter-
[22]andtheBU-3DandBU-4Ddatabases[62,63]. ature that utilize dimensional emotion representation. Ex-
amplesaretheSAL[21],SEMAINE[39],MAHNOB-HCI
Nevertheless,itisnowacceptedbythecommunitythat
[53],Belfastnaturalistic2,Belfastinduced[52],DEAP[29],
the facial expressions of naturalistic behaviors can be radi-
RECOLA[46],SEWA3andAFEW-VA[31]databases.
cally different from the posed ones [10,48,66]. Hence, ef-
Currently, there are many challenges (competitions) in
forts have been made in order to collect subjects display-
thebehavioranalysisdomain.OnesuchexampleistheAu-
ingnaturalisticbehavior.Examplesincludetherecentlycol-
dio/Visual Emotion Challenges (AVEC) series [44,45,56,
lectedEmoPain[4]andUNBC-McMaster[36]databasesfor
58,59]whichstartedin2011.Thefirstchallenge[49](2011)
analysisofpain,theRU-FACSdatabaseofsubjectspartici-
usedtheSEMAINEdatabaseforclassificationpurposesby
patinginafalseopinionscenario[5]andtheSEMAINEcor-
binarizingitscontinuousvalues,whilethesecondchallenge
pus [39] which contains recordings of subjects interacting
[50] (2012) used the same database but with its original
withaSensitiveArtificialListener(SAL)incontrolledcon-
values. The last challenge (2017) [45] utilized the SEWA
ditions.Alltheabovedatabaseshavebeencapturedinwell-
database. Before this and for two consecutive years (2015
controlled recording conditions and mainly under a strictly
[44],2016[56])theRECOLAdatasetwasused.
definedscenarioelicitingpain.
Howeverthesedatabaseshavesomeofthebelowlimita-
Representinghumanemotionshasbeenabasictopicof tions,asshowninTable1:
research in psychology. The most frequently used emotion
(1) theycontaindatarecordedinlaboratoryorcontrolleden-
representationisthecategoricalone,includingthesevenba-
vironments.
sic categories, i.e., Anger, Disgust, Fear, Happiness, Sad-
(2) theirdiversityislimitedduetothesmalltotalnumberof
ness, Surprise and Neutral [14] [11]. It is, however, the di-
subjects they contain, the limited amount of head pose
mensionalemotionrepresentation[61],[47]whichismore
appropriatetorepresentsubtle,i.e.,notonlyextreme,emo- 2 https://belfast-naturalistic-db.sspnet.eu/
tions appearing in everyday human computer interactions. 3 http://sewaproject.euDeepAffectPredictionin-the-wild:Aff-WildDatabaseandChallenge,DeepArchitectures,andBeyond 3
variations and present occlusion, the static background Coefficient(CCC),whichwealsocompareitwiththeusual
oruniformillumination Mean Squared Error (MSE) criterion. Additionally, we ap-
(3) thetotaldurationoftheirincludedvideosisrathershort propriately fused, within the network structures, two types
of inputs, the 2-D facial images - presented at the input of
the end-to-end architecture - and the 2-D facial landmark
Table 1: Databases annotated for both valence and arousal positions-presentedat the1stfullyconnectedlayerof the
&theirattributes. architecture.
We have also investigated the use of the created CNN-
noof noof duration
Database condition RNNarchitectureforvalenceandarousalestimationinother
subjects videos ofeachvideo
MAHNOB- datasets,focusingontheRECOLAandtheAFEW-VAones.
27 20 34.9−117secs controlled
HCI[53] Lastbutnotleast,takingintoconsiderationthelargein-the-
DEAP[29] 32 40 1min controlled
wildnatureofthisdatabase,weshowthatournetworkcan
AFEW-VA[31] <600 600 0.5−4secs in-the-wild
SAL[21] 4 24 25mins controlled be also used for other emotion recognition tasks, such as
SEMAINE[39] 150 959 5mins controlled classificationoftheuniversalexpressions.
Belfast
naturalistic2 125 298 10−60secs controlled The only challenge, apart from last AVEC (2017) [45],
Belfast using’in-the-wild’dataistheseriesofEmotiW[16–20].It
37 37 5−30secs controlled
induced[52]
usestheAFEWdataset,whosesamplescomefrommovies,
RECOLA[46] 46 46 5mins controlled
SEWA3 <398 538 10−30secs in-the-wild TV shows and series. To the best of our knowledge, this is
the first time that a dimensional database and features ex-
tracted from it, are used as priors for categorical emotion
To tackle the aforementioned limitations, we collected
recognition in-the-wild, exploiting the EmotiW Challenge
the first, to the best of our knowledge, large scale captured
dataset.
in-the-wilddatabaseandannotateditintermsofvalenceand
To summarize, there exist several databases for dimen-
arousal. To do so, we capitalized on the abundance of data
sionalemotionrecognition.However,theyhavelimitations,
availableinvideo-sharingwebsites,suchasYouTube[64]4
mostlyduetothefactthattheyarenotcapturedin-the-wild
and selected videos that display the affective behavior of
(i.e.,notinuncontrolledconditions).Thisurgedustocreate
people,forexamplevideosthatdisplaythebehaviorofpeo-
thebenchmarkAff-WilddatabaseandorganizetheAff-Wild
ple when watching a trailer, a movie, a disturbing clip, or
Challenge. The results acquired are presented later in full
reactionstopranks.
detail.Weproceededinconductingexperimentsandbuild-
Tothisendwehavecollected298videosdisplayingre-
ing CNN and CNN plus RNN architectures, including the
actionsof200subjects,withatotalvideodurationofmore
AffWildNet,producingstate-of-the-artresults.
than 30 hours. This database has been annotated by 8 lay
Themaincontributionsofthepaperarethefollowing:
expertswithregardstotwocontinuousemotiondimensions,
i.e. valence and arousal. We then organized the Aff-Wild
Itisthefirsttimethatalargein-the-wilddatabase-with
ChallengebasedontheAff-Wilddatabase[65][30],incon- •
a big variety of: (1) emotional states, (2) rapid emotional
junctionwithInternationalConferenceonComputerVision
changes, (3) ethnicities, (4) head poses, (5) illumination
&PatternRecognition(CVPR)2017.Theparticipatingteams
conditions and (6) occlusions - has been generated and
submitted their results to the challenge, outperforming the
usedforemotionrecognition.
providedbaseline.However,asdescribedlaterinthispaper,
Anappropriatestate-of-the-artdeepneuralnetwork(DNN)
theachievedperformanceswereratherlow. •
(AffWildNet)hasbeendeveloped,whichiscapableoflearn-
Forthisreason,wecapitalizedontheAff-Wilddatabase
ingtomodelallthesephenomena.Thishasnotbeentech-
to build CNN and CNN plus RNN architectures shown to
nically straightforward, as can be verified by comparing
achieveexcellentperformanceonthisdatabase,outperform-
theAffWildNet’sperformancetotheperformancesofother
ing all previous participants’ performances. We have made
DNNs developed by other research groups which partici-
extensive experimentations, testing structures for combin-
patedintheAff-WildChallenge.
ingconvolutionalandrecurrentneuralnetworksandtraining
ItisshownthattheAffWildNethasbeencapableofgener-
themaltogetherasanend-to-endarchitecture.Wehaveused •
alizingitsknowledgeinotheremotionrecognitiondatasets
alossfunctionthatisbasedontheConcordanceCorrelation
and contexts. By learning complex and emotionally rich
4 Thecollectionhasbeenconductedunderthescrutinyandapproval features of the AffWild, the AffWildNet constitutes a ro-
oftheImperialCollegeEthicalCommittee(ICREC).Themajorityof bust prior for both dimensional and categorical emotion
thechosenvideoswereunderCreativeCommonsLicense(CCL).For
recognition. To the best of our knowledge, it is the first
thosevideosthatwerenotunderCCL,wehavecontactedtheperson
timethatstate-of-the-artperformancesareachievedinthis
whocreatedthemandaskedfortheirapprovaltobeusedinthisre-
search. way.4 DimitriosKollias(cid:63)etal.
Table2:Currentdatabasesusedforemotionrecognitioninthispaper,theirattributesandlimitationscomparedtoAff-Wild.
total noof noof
Database modelofaffect condition limitations/comments
noofframes videos annotators
-laboratoryenvironment
valence-arousal
RECOLA controlled 345,000 46 6 -moderatetotalamountofframes
(continuous)
-smallnumberofsubjects(46)
-only7basicexpressions
sevenbasic -smalltotalamountofframes
AFEW in-the-wild 113,355 1809 3
facialexpressions -smallnumberofannotators
-imbalancedexpressioncategories
-verysmalltotalamountofframes
valence-arousal
AFEW-VA in-the-wild 30,050 600 2 -discretevalenceandarousalvalues
(discrete)
-smallnumberofannotators
valence-arousal
Aff-Wild in-the-wild 1,224,100 298 8 -
(continuous)
Therestofthepaperisorganizedasfollows.Section2 test (15 subjects), in such a way that the gender, age and
presents the databases generated and used in the presented mothertonguearestratified(i.e.,balanced).
experiments.Section3describesthepre-processingandan- The main limitations of this dataset include the tightly
notation methodologies that we used. Section 4 begins by controlledlaboratoryenvironment,aswellasthesmallnum-
describing the Aff-Wild Challenge that was organized, the berofsubjects.Itshouldbealsonotedthatitcontainsamod-
baselinemethod,themethodologiesoftheparticipatingteams eratetotalnumberofframes.
andtheirresults.Itthenpresentstheend-to-endDNNswhich
wedevelopedandthebestperformingAffWildNetarchitec-
ture. Finally experimental studies and results are presented 2.2 TheAFEWDataset
anddiscussed,illustratingtheabovedevelopments.Section
5 describes how the AffWildNet can be used as a prior for The series of EmotiW challenges [16–20] make use of the
other, both dimensional and categorical, emotion recogni- datafromtheActedFacialExpressionInTheWild(AFEW)
tionproblemsyieldingstate-of-the-artresults.Finally,Sec- dataset [16]. This dataset is a dynamic temporal facial ex-
tion 6 presents the conclusions and future work following pressionsdatacorpusconsistingofclosetorealworldscenes
thereporteddevelopments. extractedfrommoviesandrealityTVshows.Intotalitcon-
tains1809videos.Thewholedatasetissplitintothreesets:
trainingset(773videoclips),validationset(383videoclips)
andtestset(653videoclips).Itshouldbeemphasizedthat
2 ExistingDatabases
both training and validation sets are mainly composed of
realmovierecords,however114outof653videoclipsinthe
We briefly present the RECOLA, AFEW, AFEW-VA data-
testsetarerealTVclips,thusincreasingthedifficultyofthe
basesusedforemotionrecognitionandmentiontheirlimi-
challenge.Thenumberofsubjectsismorethan330,aged1-
tationswhichleadtothecreationoftheAff-Wilddatabase.
77years.Theannotationisaccordingto7facialexpressions
Table 2 summarizes these limitations, also showing the su-
(Anger,Disgust,Fear,Happiness,Neutral,SadnessandSur-
periorpropertiesofAff-Wild.
prise) and is performed by three annotators. The EmotiW
challenges focus on audiovisual classification of each clip
intothesevenbasicemotioncategories.
2.1 RECOLADataset The limitations of the AFEW dataset include its small
size(intermsoftotalnumberofframes)anditsrestrictionto
TheREmoteCOLlaborativeandAffective(RECOLA)data- onlysevenemotioncategories,someofwhich(fear,disgust,
base was introduced by Ringeval et al. [46] and it contains surprise)includeasmallnumberofsamples.
naturalandspontaneousemotionsinthecontinuousdomain
(arousalandvalence).Thecorpusincludesfourmodalities:
audio,visual,electro-dermalactivityandelectro-cardiogram. 2.3 TheAFEW-VADatabase
It consists of 46 French speaking subjects being recorded
for9.5hrecordingsintotal.Therecordingswereannotated Very recently, a part of the AFEW dataset of the series of
for 5minutes each by 6 French-speaking annotators (three EmotiWchallengeshasbeenannotatedintermsofvalence
male,threefemale).Thedatasetisdividedintothreeparts, andarousal,thuscreatingthesocalledAFEW-VA[31]data-
namely, training (16 subjects), validation (15 subjects) and base.Intotal,itcontains600videoclipsthatwereextractedDeepAffectPredictionin-the-wild:Aff-WildDatabaseandChallenge,DeepArchitectures,andBeyond 5
Fig. 2: Frames from the Aff-Wild database which show subjects in different emotional states, of different ethnicities, in a
varietyofheadposes,illuminationconditionsandocclusions.
from feature films and simulate real-world conditions, i.e., MostofthevideosareinYUV4:2:0format,withsome
occlusions,differentilluminationconditionsandfreemove- ofthembeinginAVIformat.Eightsubjectshaveannotated
ments from subjects. The videos range from short (around thevideosfollowingamethodologysimilartotheonepro-
10 frames) to longer clips (more than 120 frames). This posedin[12],intermsofvalenceandarousal.Anonlinean-
databaseincludesper-frameannotationsofvalenceandarou- notationprocedurewasused,accordingtowhichannotators
sal. In total, more than 30,000 frames were annotated for were watching each video and provided their annotations
dimensionalaffectpredictionofarousalandvalence,using throughajoystick.Valenceandarousalrangecontinuously
discretevaluesintherangeof[ 10,+10]. in [ 1, +1]. All subjects present in each video have been
− −
Thedatabase’slimitationsincludeitssmallsize(interms annotated.Thetotalnumberofsubjectsis200,with130of
of total number of frames), the small number of annota- thembeingmaleand70ofthemfemale.Table3showsthe
tors (only 2) and the use of discrete values for valence and generalattributesoftheAff-Wilddatabase.Figure2shows
arousal.Itshouldbenotedthatthe2-DEmotionWheel(Fig- someframesfromtheAff-Wilddatabase,withpeoplefrom
ure1)isacontinuousspace.Therefore,usingdiscreteonly different ethnicities displaying various emotions, with dif-
values for valence and arousal provides a rather coarse ap- ferentheadposesandilluminationconditions,aswellasoc-
proximationofthebehaviorofpersonsintheireverydayin- clusionsinthefacialarea.
teractions. On the other hand, using continuous values can
provideimprovedmodelingoftheexpressivenessandrich-
nessofemotionalstatesmetineverydayhumanbehaviors.
Table3:AttributesoftheAff-WildDatabase
Attribute Description
2.4 TheAff-WildDatabase Lengthofvideos 0.10−14.47min
Videoformat AVI,MP4
AverageImageResolution(AIR) 607×359
Wecreatedadatabaseconsistingof298videos,withatotal
StandarddeviationofAIR 85×11
lengthofmorethan30hours.Theaimwastocollectsponta-
MedianImageResolution 640×360
neousfacialbehaviorsinarbitraryrecordingconditions.To
thisend,thevideoswerecollectedusingtheYoutubevideo
sharing web-site. The main keyword that was used to re-
trievethevideoswas”reaction”.Thedatabasedisplayssub-
jects reacting to a variety of stimuli, e.g. viewing an unex- Figure 3 shows an example of annotated valence and
pectedplottwistofamovieorseries,atrailerofahighlyan- arousal values over a part of a video in the Aff-Wild, to-
ticipatedmovie,ortastingsomethinghotordisgusting.The gether with corresponding frames. This illustrates the in-
subjectsdisplaybothpositiveornegativeemotions(orcom- the-wildnatureofourdatabase,namely,includingmanydif-
binationsofthem).Inothercases,subjectsdisplayemotions ferentemotionalstates,rapidemotionalchangesandocclu-
whileperforminganactivity(e.g.,ridingarollingcoaster). sions in the facial areas. Figure 3 also shows the use of
Insomevideos,subjectsreactonapracticaljoke,oronpos- continuousvaluesforvalenceandarousalannotation,which
itivesurprises(e.g.,agift).Thevideoscontainsubjectsfrom givestheabilitytoeffectivelymodelallthesedifferentphe-
differentgendersandethnicitieswithhighvariationsinhead nomena. Figure 4 provides a histogram for the annotated
poseandlightning. valuesforvalenceandarousalinthegenerateddatabase.6 DimitriosKollias(cid:63)etal.
1
0.5
0
0.5
−
1
− 0 200 400 600 800 1,000 1,200 1,400 1,600 1,800 2,000
Frames
snoitatonnA
Valence
Arousal
Fig.3:Valenceandarousalannotationsoverapartofavideo,alongwithcorrespondingframes;illustrating(i)thein-the-wild
natureofAff-Wild(differentemotionalstates,rapidemotionalchanges,occlusions)and(ii)theuseofcontinuousvaluesfor
valenceandarousal
Fig.4:HistogramofvalenceandarousalannotationsoftheAff-Wilddatabase.
3 DataPre-processingandAnnotation Finally, we present a statistical analysis of the annota-
tions created for each video, illustrating the consistency of
Inthissectionwedescribethepre-processingprocessofthe annotationsachievedbyusingtheaboveprocedure.
Aff-Wild videos so as to perform face and facial landmark
detection.Thenwepresenttheannotationprocedureinclud-
ing: 3.1 Aff-Wildvideopre-processing
(1) Creationoftheannotationtool. VirtualDub[33]wasusedfirstsoastotrimtherawYouTube
(2) Generationofguidelinesforsixexpertstofollowinor- videos,mainlyattheirbeginningandend-points,inorderto
dertoperformtheannotation. removeuselesscontent(e.g.,advertisements).Then,weex-
(3) Post-processing annotation: the six annotators watched tracted a total of 1,224,100 video frames using the Menpo
allvideosagain,checkedtheirannotationsandperformed software[2].Ineachframe,wedetectedthefacesandgen-
anycorrections;twonewannotatorswatchedallvideos eratedcorrespondingboundingboxes,usingthemethodde-
and selected 2-4 annotations that best described each scribed in [38]. Next, we extracted facial landmarks in all
video;finalannotationsarethemeanoftheselectedan- framesusingthebestperformingmethodasindicatedin[8].
notationsbythesetwonewannotators. During this process, we removed frames in which the
boundingboxorlandmarkdetectionfailed.Failuresoccurred
The detected faces and facial landmarks, as well as the wheneithertheboundingboxes,orlandmarks,werewrongly
generated annotations are publicly available with the Aff- detected, or were not detected at all. The former case was
Wilddatabase. semi-automatically discovered by: (i) detecting significantDeepAffectPredictionin-the-wild:Aff-WildDatabaseandChallenge,DeepArchitectures,andBeyond 7
shiftsintheboundingboxandlandmarkpositionsbetween Itshouldalsobeaddedthattheannotationtoolhasalso
consecutiveframesand(ii)havingtheannotatorsverifythe the ability to show the inserted valence and arousal anno-
wrongdetectionintheframes. tation while displaying a respective video. This is used for
annotationverificationinapost-processingstep.
3.2 Annotationtool
3.3 Annotationguidelines
Fordataannotation,wedevelopedourownapplicationthat
Sixexpertswerechosentoperformtheannotationtask.Each
buildsonotherexistingones,likeFeeltrace[12]andGtrace
annotator was instructed orally and through a multi-page
[13]. A time-continuous annotation is performed for each
documentontheproceduretofollowforthetask.Thisdoc-
affective dimension, with the annotation process being as
umentincludedalistofsomewellidentifiedemotionalcues
follows:
forbotharousalandvalence,providingacommonbasisfor
(a) theuserlogsintotheapplicationusinganidentifier(e.g. theannotationtask.Ontopofthattheexpertsusedtheirown
his/hername)andselectsanappropriatejoystick; appraisalofthesubject’semotionalstateforcreatingthean-
(b) ascrollinglistofallvideosappearsandtheuserselects notations.5 Beforestartingtheannotationofeachvideo,the
avideotoannotate; expertswatchedthewholevideosoastoknowwhattoex-
(c) a screen appears that shows the selected video and a pectregardingtheemotionsbeingdisplayedinthevideo.
sliderofvalenceorarousalvaluesrangingin[ 1,1];
−
(d) the user annotates the video by moving the joystick ei-
therupordown;
3.4 AnnotationPost-processing
(e) finally, a file is created including the annotation values
and the corresponding time instances that the annota-
Apost-processingannotationverificationstepwasalsoper-
tionsaregenerated.
formed.Everyexpert-annotatorwatchedallvideosforasec-
ond time in order to verify that the recorded annotations
It should be mentioned that the time instances gener-
were in accordance with the shown emotions in the videos
atedintheabovestep(e),didnotgenerallymatchthevideo
orchangetheannotationsaccordingly.Inthisway,afurther
framerate.Totacklethisproblem,wemodified/re-sampled
validationofannotationswasachieved.
the annotation time instances using nearest neighbor inter-
After the annotations have been validated by the anno-
polation.
tators, a final annotation selection step followed. Two new
Figure5showsthegraphicalinterfaceofourtoolwhen
expertswatchedallvideosand,foreveryvideo,selectedthe
annotatingvalence(theinterfaceforarousalissimilar);this
annotations(betweentwoandfour)whichbestdescribedthe
corresponds to step (c) of the above described annotation
displayedemotions.Themeanoftheseselectedannotations
process.
constitutethefinalAff-Wildlabels.
This step is significant for obtaining highly correlated
annotations, as shown by the statistical analysis presented
next.
3.5 StatisticalAnalysisofAnnotations
Inthefollowingweprovideaquantitativeandrichstatisti-
cal analysis of the achieved Aff-Wild labeling. At first, for
each video, and independently for valence and arousal, we
computed:
(i) the inter-annotator correlations, i.e., the correlations of
eachoneofthesixannotatorswithallotherannotators,
Fig. 5: The GUI of the annotation tool when whichresultedinfivecorrelationvaluesperannotator;
annotatingvalence(theGUIforarousalisex-
5 Allannotatorswerecomputerscientistswhowereworkingonface
actlythesame).
analysisproblems andallhada workingunderstandingoffacial ex-
pressions.8 DimitriosKollias(cid:63)etal.
1
0.8
0.6
0.4
0.2
0
0 200 400 600 800 1,0001,2001,4001,6001,8002,0002,2002,400
Frames
snoitatonnAecnelaV
1
Annotator1
Annotator2 0.8
Annotator3
Annotator4
0.6
0.4
0.2
0
0.2
− 0 200 400 600 800 1,0001,2001,4001,6001,8002,0002,2002,400
Frames
(a)
snoitatonnAlasuorA
Annotator1
Annotator2
Annotator3
Annotator4
(b)
Fig.6:Thefourselectedannotationsinavideosegmentfor(a)valenceand(b)arousal.Inbothcases,thevalueofMAC-S
(mean of average correlations between these four annotations) is 0.70. This value is similar to the mean MAC-S obtained
overallAff-Wild.
1
0.8
0.6
0.4
0.2
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
AverageCorrelation
soediVfoegatnecreP
1
AllAnnotators(MAC-A)
SelectedAnnotators(MAC-S)
0.8
0.6
0.4
0.2
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
AverageCorrelation
(a)
soediVfoegatnecreP
AllAnnotators(MAC-A)
SelectedAnnotators(MAC-S)
(b)
Fig.7:ThecumulativedistributionofMAC-S(meanofaverageinter-selected-annotatorcorrelations)andMAC-A(meanof
average inter-annotator correlations) values over all Aff-Wild videos for valence (Figure 7a) and arousal (Figure 7b). The
FigureshowsthepercentageofvideoswithaMAC-S/MAC-Avaluegreaterorequaltothevaluesshowninthehorizontal
axis.ThemeanMAC-Svalue,correspondingtoavalueof0.5intheverticalaxis,is0.71forvalenceand0.70forarousal.
(ii) for each annotator, his/her average inter-annotator cor- arousal,respectively,withMAC-Svalueof0.70(similarto
relations,resultinginonevalueperannotator;themean themeanMAC-SvalueobtainedoverallAff-Wild).
ofthosesixaverageinter-annotatorcorrelationsvalueis
denotednextasMAC-A;
In addition, Figure 7 shows the cumulative distribution
(iii) theaverageinter-annotatorcorrelations,acrossonlythe
ofMAC-SandMAC-AvaluesoverallAff-Wildvideosfor
selectedannotators,asdescribedintheprevioussubsec-
valence (Figure 7a) and arousal (Figure 7b). In each case,
tion, resulting in one value per selected annotator; the
two curves are shown. Every point (x,y) on these curves
mean of those 2-4 average inter-selected-annotator cor-
has a y value showing the percentage of videos with a (i)
relationsvaluesisdenotednextasMAC-S.
MAC-S(redcurve)or(ii)MAC-A(bluecurve)valuegreater
Wethencomputedoverallvideosandindependentlyfor or equal to x; the latter denotes an average correlation in
valence and arousal, the mean of MAC-A and the mean of [0,1]. It can be observed that the mean MAC-S value, cor-
MAC-Scomputedin(ii)and(iii)above.ThemeanMAC-A respondingtoavalueof0.5intheverticalaxis,is0.71for
is 0.47 for valence and 0.46 for arousal, whilst the mean valenceand0.70forarousal.Theseplotsalsoillustratethat
MAC-S for valence is 0.71 and for arousal 0.70. An ex- the MAC-S values are much higher than the correspond-
ample set of annotations is shown in Figure 6, in an effort ingMAC-Avaluesinbothvalenceandarousalannotation,
to further clarify the obtained MAC-S values. It shows the verifyingtheeffectivenessoftheannotationpost-processing
fourselectedannotationsinavideosegmentforvalenceand procedure.DeepAffectPredictionin-the-wild:Aff-WildDatabaseandChallenge,DeepArchitectures,andBeyond 9
1
0.8
0.6
0.4
0.2
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
AverageCorrelation
soediVfoegatnecreP
1
(i)AllAnnotations
(ii)SelectedAnnotations
0.8
0.6
0.4
0.2
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
AverageCorrelation
(a)
soediVfoegatnecreP
(i)AllAnnotations
(ii)SelectedAnnotations
(b)
Fig.8:Thecumulativedistributionofthecorrelationbetweenlandmarksandtheaverageof(i)allor(ii)selectedannotations
overallAff-Wildvideosforvalence(Figure8a)andarousal(Figure8b).TheFigureshowsthepercentageofvideoswitha
correlationvaluegreaterorequaltothevaluesshowninthehorizontalaxis.
Next,weconductedsimilarexperimentsforthevalence/ (withoutannotations).Theparticipantsweregiventhefree-
arousalaverageannotationsandthefaciallandmarksineach domtosplitthedataintotrainandvalidationsets,aswellas
video,inordertoevaluatethecorrelationofannotationsto touseanyotherdataset.Themaximumnumberofsubmitted
landmarks. To this end, we utilized Canonical Correlation entries for each participant was three. Table 4 summarizes
Analysis (CCA) [23]. In particular, for each video and in- the specific attributes (numbers of males, females, videos,
dependentlyforvalenceandarousal,wecomputedthecor- frames)ofthetrainingandtestsetsofthechallenge.
relationbetweenlandmarksandtheaverageof(i)allor(ii)
selectedannotations.
Figure8showsthecumulativedistributionofthesecor- Table4:AttributesofTrainingandTestsetsofAff-Wild.
relations over all Aff-Wild videos for valence (Figure 8a)
and arousal (Figure 8b), similarly to Figure 7. Results of Set noof noof noof totalnoof
males females videos frames
this analysis verify that the annotator-landmark correlation
Training 106 48 252 1,008,650
ismuchhigherinthecaseofselectedannotationsthaninthe
Test 24 22 46 215,450
caseofallannotations.
In total, ten different research groups downloaded the
4 DevelopingtheAffWildNet
Aff-Wilddatabase.Sixofthemmadeexperimentsandsub-
mittedtheirresultstotheworkshopportal.Basedontheper-
This section begins by presenting the first Aff-Wild Chal-
formancetheyobtainedonthetestdata,threeofthemwere
lenge that was organized based on the Aff-Wild database
selectedtopresenttheirresultstotheworkshop.
andheldinconjunctionwithCVPR2017.Itincludesshort
descriptionsandresultsofthealgorithmsofthesixresearch Two criteria were considered for evaluating the perfor-
groups that participated in the challenge. Although the re- mance of the networks. The first one is Concordance Cor-
sultsarepromising,thereismuchroomforimprovement. relation Coefficient (CCC) [32], which is widely used in
For this reason we developed our own CNN and CNN measuring the performance of dimensional emotion recog-
plusRNNarchitecturesbasedontheAff-Wilddatabase.We nition methods, e.g., the series of AVEC challenges. CCC
propose the AffWildNet as the best performing among the evaluates the agreement between two time series (e.g., all
developedarchitectures.Ourdevelopments,ablationstudies video annotations and predictions) by scaling their corre-
anddiscussionsarepresentednext. lation coefficient with their mean square difference. In this
way, predictions that are well correlated with the annota-
tionsbutshiftedinvaluearepenalizedinproportiontothe
4.1 TheAff-WildChallenge deviation.CCCtakesvaluesintherange[ 1,1],where+1
−
indicates perfect concordance and 1 denotes perfect dis-
−
The training data (i.e., videos and annotations) of the Aff- cordance. The highest the value of the CCC the better the
Wild challenge were made publicly available on the 30th fit between annotations and predictions, and therefore high
of January 2017, followed by the release of the test videos valuesaredesired.ThemeanvalueofCCCforvalenceand10 DimitriosKollias(cid:63)etal.
Table 5: Concordance Correlation Coefficient (CCC) and Mean Squared Error (MSE) of valence & arousal predictions
provided by the methods of the three participating teams and the baseline architecture. A higher CCC and a lower MSE
valueindicateabetterperformance.
Methods CCC Methods MSE
Valence Arousal MeanValue Valence Arousal MeanValue
MM-Net 0.196 0.214 0.205 MM-Net 0.134 0.088 0.111
FATAUVA-Net 0.396 0.282 0.339 FATAUVA-Net 0.123 0.095 0.109
DRC-Net 0.042 0.291 0.167 DRC-Net 0.161 0.094 0.128
Baseline 0.150 0.100 0.125 Baseline 0.130 0.140 0.135
arousalestimationwasadoptedasthemainevaluationcrite- 4.1.2 ParticipatingTeams’Algorithms
rion.CCCisdefinedasfollows:
The three papers accepted to this challenge are briefly re-
ported below, while Table 5 compares the acquired results
2s 2s s ρ
xy x y xy
ρ = = , (1) (in terms of CCC and MSE) by all three methods and the
c s2 +s2 +(x¯ y¯)2 s2 +s2 +(x¯ y¯)2
x y − x y − baseline network. As one can see, FATAUVA-Net [6] has
where ρ is the Pearson Correlation Coefficient (Pearson provided the best results in terms of the mean CCC and
xy
CC),s ands arethevariancesofallvideovalence/arousal meanMSEforvalenceandarousal.
x y
annotationsandpredictedvalues,respectivelyands isthe Weshouldnotethataftertheendofthechallenge,more
xy
correspondingcovariancevalue. groups enquired about the Aff-Wild database and sent re-
ThesecondcriterionistheMeanSquaredError(MSE), sults for evaluation, but here we report only on the teams
whichisdefinedasfollows: thatparticipatedinthechallenge.
IntheMM-Netmethod[34],avariationofadeepcon-
volutionalresidualneuralnetwork(ResNet)[24]isfirstpre-
N
1 (cid:88) sented for affective level estimation of facial expressions.
MSE = (x y )2, (2)
N i − i Then, multiple memory networks are used to model tem-
i=1
poralrelationsbetweenthevideoframes.Finally,ensemble
wherexandyarethe(valence/arousal)annotationsandpre- modelsareusedtocombinethepredictionsofthemultiple
dictions,respectively,andN isthetotalnumberofsamples. memorynetworks,showingthatthelatterstepsimprovethe
The MSE gives us a rough indication of how the derived initiallyobtainedperformance,asfarasMSEisconcerned,
emotionmodelisbehaving,providingasimplecomparative bymorethan10%.
metric.AsmallvalueofMSEisdesired. IntheFATAUVA-Netmethod[6],adeeplearningframe-
work is presented, in which a core layer, an attribute layer,
4.1.1 BaselineArchitecture an action unit (AU) layer and a valence-arousal layer are
trained sequentially. The core layer is a series of convo-
The baseline architecture for the challenge was based on lutional layers, followed by the attribute layer which ex-
the CNN-M [7] network, as a simple model that could be tracts facial features. These layers are applied to supervise
usedtoinitiatetheprocedure.Inparticular,ournetworkused the learning of AUs. Finally, AUs are employed as mid-
theconvolutionalandpoolingpartsofCNN-Mhavingbeen levelrepresentationstoestimatetheintensityofvalenceand
trainedontheFaceValuedataset[3].Ontopofthatweadded arousal.
one4096-fullyconnectedlayeranda2-fullyconnectedlayer IntheDRC-Netmethod[37],threeneuralnetwork-based
thatprovidesthevalenceandarousalpredictions.Theinter- methodswhicharebasedonInception-ResNet[54]modules
estedreadercanrefertoAppendixAforashortdescription redesignedspecificallyforthetaskoffacialaffectestimation
andthestructureofthisarchitecture. are presented and compared. These methods are: Shallow
Theinputtothenetworkwerethefacialimagesresized Inception-ResNet, Deep Inception-ResNet, and Inception-
to resolution of 224 224 3, or 96 96 3, with the ResNetwithLongShortTermMemory[25].Facialfeatures
× × × ×
intensityvaluesbeingnormalizedtotherange[ 1,1]. are extracted in different scales and both, the valence and
−
Inordertotrainthenetwork,weutilizedtheAdamopti- arousal, are simultaneously estimated in each frame. Best
mizeralgorithm;thebatchsizewassetto80,andtheinitial resultsareobtainedbytheDeepInception-ResNetmethod.
learningratewassetto0.001.Trainingwasperformedona All participants applied deep learning methods to the
single GeForce GTX TITAN X GPU and the training time problem of emotion analysis of the video inputs. The fol-
wasabout4-5days.Theplatformusedforthisimplementa- lowingconclusionscanbedrawnfromthereportedresults.
tionwasTensorflow[1]. First,CCCofarousalpredictionswasreallylowforallthreeDeepAffectPredictionin-the-wild:Aff-WildDatabaseandChallenge,DeepArchitectures,andBeyond 11
methods.Second,MSEofvalencepredictionswashighfor where ρ and ρ are the CCC for the arousal and va-
a v
allthreemethodsandCCCwaslow,exceptforthewinning lence,respectively.
method.Thisillustratesthedifficultyinrecognizingemotion
in-the-wild,where,forinstance,illuminationconditionsdif-
fer,occlusionsarepresentanddifferentheadposesaremet. D. In order to have a more balanced dataset for training,
weperformeddataaugmentation,mainlythroughover-
sampling by duplicating [40] some data from the Aff-
4.2 DeepNeuralArchitectures&AblationStudies Wild database. We copied small video parts showing
less-populatedvalenceandarousalvalues.Inparticular,
Here,wepresentourdevelopmentsandablationstudiesto- we duplicated consecutive video frames that had neg-
wards designing deep CNN and CNN plus RNN architec- ativevalenceandarousalvalues,aswellaspositiveva-
tures for the Aff-Wild. We present the proposed architec- lenceandnegativearousalvalues.Asaconsequence,the
ture, AffWildNet, which is a CNN plus RNN network that training set consisted of about 43% of positive valence
producedthebestresultsinthedatabase. and arousal values, 24% of negative valence and pos-
itive arousal values, 19% of positive valence and neg-
ative arousal values and 14% of negative valence and
4.2.1 TheRoadmap
arousalvalues.Ourmaintargethasbeenatrade-offbe-
tweengeneratingbalancedemotionsetsandavoidingto
A. Weconsideredtwonetworksettings:
severelychangethecontentofvideos.
(1) aCNNnetworktrainedinanend-to-endmanner,i.e.,
using raw intensity pixels, to produce 2-D predic-
4.2.2 DevelopingCNNarchitecturesfortheAff-Wild
tionsofvalenceandarousal,
(2) a RNN stacked on top of the CNN to capture tem-
For the CNN architectures, we considered the ResNet-50
poral information in the data, before predicting the
and VGG-16 networks, pre-trained on the ImageNet [15]
affectdimensions;thiswasalsotrainedinanend-to-
datasetthathasbeenbroadlyusedforstate-of-the-artobject
endmanner.
detection.WealsoconsideredtheVGG-Facenetwork,pre-
To extract features from the frames we experimented
trained for face recognition on the VGG-Face dataset [42].
withthreeCNNarchitectures,namely,ResNet-50,VGG-
The VGG-Face has proven to provide the best results, as
Face[42]andVGG-16[51].Toconsiderthecontextual
reported next in the experimental section. It is worth men-
information in the data (RNN case) we experimented
tioningthatinourexperimentswehavetrainedthosearchi-
with both the Long Short-Term Memory (LSTM) and
tecturesforpredictingbothvalenceandarousalattheirout-
theGatedRecurrentUnit(GRU)[9]architectures.
put,aswellasforpredictingvalenceandarousalseparately.
The obtained results were similar in the two cases. In all
experiments presented next, we focus on the simultaneous
B. To further boost the performance of the networks, we
predictionofvalenceandarousal.
alsoexperimentedwiththeuseoffaciallandmarks.Here
The first architecture we utilized was the deep residual
we should note that the facial landmarks are provided
network (ResNet) of 50 layers [24], on top of which we
on-the-flyfortrainingandtestingthenetworks.Thefol-
stackeda2-layerfullyconnected(FC)network.Forthefirst
lowingtwoscenariosweretested:
FC layer, best results have been obtained when using 1500
(1) The networks were applied directly on cropped fa-
units. For the second FC layer, 256 units provided the best
cialvideoframesofthegenerateddatabase.
results.Anoutputlayerwithtwolinearunitsfollowedpro-
(2) The networks were trained on both the facial video
viding the valence and arousal predictions. The interested
framesaswellasthefaciallandmarkscorresponding
reader can refer to Appendix A for a short description and
tothesameframe.
thestructureofthisarchitecture.
Theotherarchitecturethatweutilizedwasbasedonthe
convolutionalandpoolinglayersofVGG-FaceorVGG-16
C. SincethemainevaluationcriterionoftheAff-WildChal- networks,ontopofwhichwestackeda2-layerFCnetwork.
lengewasthemeanvalueofCCCforvalenceandarousal, For the first and second FC layers, best results have been
ourlossfunctionwasbasedonthatcriterionandwasde- obtainedwhenusing4096units.Anoutputlayerfollowed,
finedas: includingtwolinearunits,providingthevalenceandarousal
predictions. The interested reader can refer to Appendix A
forashortdescriptionandthestructureofthisarchitecture
ρ +ρ
a v
total =1 , (3) aswell.
L − 212 DimitriosKollias(cid:63)etal.
Fig.9:TheAffWildNet:itconsistsofconvolutionalandpoolinglayersofeitherVGG-FaceorResNet-50structures(denoted
as CNN), followed by a fully connected layer (denoted as FC1) and two RNN layers with GRU units (V and A stand for
valenceandarousalrespectively).
Inthecasewhenlandmarkswereused(scenarioB.2in the convolutional and pooling layers of the CNN architec-
subsection4.2.1),thesewereinputtothefirstFClayeralong tures described above (VGG-Face, or ResNet-50) that was
with:i)theoutputsoftheResNet-50,orii)theoutputsofthe followed by a fully connected layer. Note that in the case
last pooling layer of the VGG-Face/VGG-16. In this way, ofscenarioB.2ofsubsection4.2.1,boththeoutputsofthe
both outputs and landmarks were mapped to the same fea- last pooling layer of the CNN, as well as the 68 landmark
turespacebeforeperformingtheprediction. 2-Dpositions(68 2values)wereprovidedasinputstothis
×
WithrespecttoparameterselectioninthoseCNNarchi- fullyconnectedlayer.Table6showstherespectivenumber
tectures,wehaveusedabatchsizeintherange10 100and ofunitsfortheGRUandthefullyconnectedlayers.Wecall
−
aconstantlearningratevalueintherange0.00001 0.001. this CNN plus RNN architecture AffWildNet and illustrate
−
Thebestresultshavebeenobtainedwithbatchsizeequalto itinFigure9.
50andlearningrateequalto0.0001.Thedropoutprobabil-
ityvaluehasbeensetto0.5.
Table6:TheAffWildNetarchitecture:thefullyconnected1
layerhas4096,or1500hiddenunits,dependingonwhether
4.2.3 DevelopingCNNplusRNNarchitecturesforthe
VGG-FaceorResNet-50isused.
Aff-Wild
VGG-FaceorResNet-50
In order to consider the contextual information in the data, block1
conv&poolingparts
wedevelopedaCNN-RNNarchitecture,inwhichtheRNN block2 fullyconnected1 4096or1500
partwasfedwiththeoutputsofeitherthefirst,orthesecond dropout
fullyconnectedlayeroftherespectiveCNNnetworks. block3 GRUlayer1 128
dropout
The structure of the RNN, which we examined, con-
block4 GRUlayer2 128
sistedofoneortwohiddenlayers,with100 150units,fol-
block5 fullyconnected2 2
−
lowing either the LSTM neuron model with peephole con-
nections, or the GRU neuron model. Using one fully con-
nected layer in the CNN part and two hidden layers in the Network evaluation has been performed by testing dif-
RNN part, including GRUs, has been found to provide the ferentparametervalues.Theparametersincluded:thebatch
bestresults.Anoutputlayerfollowed,includingtwolinear sizeandsequencelengthusedfornetworkparameterupdat-
units,providingthevalenceandarousalpredictions. ing,thevalueofthelearningrateandthedropoutprobability
Table 6 shows the configuration of the CNN-RNN ar- value.Finalselectionoftheseparameterswassimilartothe
chitecture.TheCNNpartofthisarchitecturewasbasedon CNN cases, apart from the sequence length which was se-DeepAffectPredictionin-the-wild:Aff-WildDatabaseandChallenge,DeepArchitectures,andBeyond 13
Table 7: CCC and MSE based evaluation of valence & arousal predictions provided by the VGG-Face (using the mean of
annotatorsvalues,orusingonlyoneannotatorvalues;whenlandmarkswereorwerenotgivenasinputtothenetwork).
With Without With Without
CCC MSE
Landmarks Landmarks Landmarks Landmarks
Valence Arousal Valence Arousal Valence Arousal Valence Arousal
One One
0.39 0.27 0.35 0.25 0.15 0.13 0.16 0.14
Annotator Annotator
Meanof Meanof
0.51 0.33 0.44 0.32 0.10 0.08 0.12 0.11
Annotators Annotators
lectedintherange50 200andbatchsizethatwasselected Table8:ObtainedCCCvaluesforvalence&arousalestima-
−
in the range 2 10. Best results have been obtained with tion, when changing the number of hidden units & hidden
−
sequence length 80 and batch size 4. We note that all deep layers in the VGG-Face-GRU architecture. A higher CCC
learningarchitectureshavebeenimplementedintheTensor- valueindicatesabetterperformance.
flowplatform.
CCC 1HiddenLayer 2HiddenLayers
HiddenUnits Valence Arousal Valence Arousal
100 0.44 0.36 0.50 0.41
4.3 ExperimentalResults 128 0.53 0.40 0.57 0.43
150 0.46 0.39 0.51 0.41
In the following we present the affect recognition results
obtained when applying the above derived CNN-only and
obtainedvalence/arousalpredictionstothegroundtruthval-
CNNplusRNNarchitecturestotheAff-Wilddatabase.
ues,in10000consecutiveframesoftestdata.
At first, we have trained the VGG-Face network using
Moreover, in Figures 11(a) and 11(b), we illustrate, in
twodifferentannotations.One,whichisprovidedintheAff-
the2-Dvalence&arousalspace,thehistogramsoftheground
Wilddatabase,istheaverageoftheselected(asdescribedin
truthlabelsofthetestsetandthecorrespondingpredictions
subsection 3.4) annotations. The second is that of a single
ofourAffWildNet.
annotator (the one with the highest correlation to the land-
TheresultsshowninTable9andtheaboveFiguresver-
marks). It should be mentioned that the latter is generally
ifytheexcellentperformanceoftheAffWildNet.Theyalso
less smooth than the former, average, one. Hence, they are
show that it greatly outperformed all methods submitted in
moredifficulttobemodeled.Then,wetestedthetwotrained
theAff-WildChallenge.
networksintwoscenarios,asdescribedinsubsection4.2.1
caseB,using/notusingthe682-Dlandmarkinputs.
TheresultsaresummarizedinTable7.Aswasexpected, 4.4 DiscussingAffWildNet’sPerformance
better results were obtained when the mean of annotations
was used. Moreover, Table 7 shows that there is a notable The reasons why the AffWildNet outperformed the other
improvementintheperformance,whenwealsousedthe68 methodsarerelatedtoboththenetworkdesignandthenet-
2-Dlandmarkpositionsasinputdata. worktraining.
Next, we examined the use of various numbers of hid- At first, the AffWildNet is a CNN-RNN network. The
denlayersandhiddenunitsperlayerwhentrainingandtest- CNN part is based on the VGG-Face (or ResNet-50) net-
ingtheVGG-Face-GRUnetwork.Somecharacteristicselec- work’sconvolutionalandpoolinglayers.TheVGG-Facenet-
tionsandtheircorrespondingperformancesareshowninTa- workhasbeenpre-trainedwithalargedatasetforfacerecog-
ble8.Itcanbeseenthatthebestresultshavebeenobtained nition (many human faces have been, therefore, used in its
when the RNN part of the network consisted of 2 layers, construction).
eachof128hiddenunits. In our implementation, this CNN part is followed by a
Table9summarizestheCCCandMSEvaluesobtained single FC layer. The inputs of this layer are: a) the outputs
whenapplyingalldevelopedarchitecturesdescribedinsub- ofthelastpoolinglayeroftheCNNpart;b)thefacialland-
sections4.2.2and4.2.3,totheAff-Wildtestset.Itshowsthe marks,whicharedirectlypassedasinputstothisFClayer.
improvementintheCCCandMSEvaluesobtainedwhenus- Asaconsequence,thislayerhastheroletomapitstwotypes
ing the AffWildNet compared to all other developed archi- ofinputstothesamefeaturespace,beforeforwardingthem
tectures. This improvement clearly indicates the ability of totheRNNpart.Thefaciallandmarks,whichareprovided
theAffWildNettobettercapturethedynamicsinAff-Wild. asadditionalinputtothenetwork,inthisway,contributeto
In Figures 10(a) and 10(b), we qualitatively illustrate boosting the performance of our model. The output of the
someoftheobtainedresultsbycomparingasegmentofthe fullyconnectedlayeristhenpassedtotheRNNpart.14 DimitriosKollias(cid:63)etal.
Table 9: CCC and MSE based evaluation of valence & arousal predictions provided by: 1) the CNN architecture when
usingthreedifferentpre-trainednetworksforinitialization(VGG-16,ResNet-50,VGG-Face)and2)theVGG-Face-LSTM
andAffWildNetarchitectures(2RNNlayerswith128unitseach).AhigherCCCandalowerMSEvalueindicateabetter
performance.
CCC MSE
Valence Arousal MeanValue Valence Arousal MeanValue
VGG-16 0.40 0.30 0.35 VGG-16 0.13 0.11 0.12
ResNet-50 0.43 0.30 0.37 ResNet-50 0.11 0.11 0.11
VGG-Face 0.51 0.33 0.42 VGG-Face 0.10 0.08 0.09
VGG-Face-LSTM 0.52 0.38 0.45 VGG-Face-LSTM 0.10 0.09 0.10
AffWildNet 0.57 0.43 0.50 AffWildNet 0.08 0.06 0.07
1
0.8
0.6
0.4
0.2
0
0.2
−
0.4
−
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Frames 104
·
noitatonnAecnelaV
Predictions
Labels
(a) Valence
1
0.8
0.6
0.4
0.2
0
0.2
−
0.4
−
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Frames 104
·
snoitatonnAlasuorA
(a) annotations
Predictions
Labels
(b) Arousal
Fig.10:PredictionsvsLabelsfor(a)valenceand(b)arousal
overavideosegmentoftheAff-Wild.
(b) predictions
Fig. 11: Histogram in the 2-D valence & arousal space of:
TheRNNisusedinordertomodelthecontextualinfor-
(a) annotations and (b) predictions of AffWildNet, on the
mationinthedata,takingintoaccounttemporalvariations.
testsetoftheAff-WildChallenge.
TheRNNiscomposedof2-layers,withGRUunitsineach
layer;thefirstlayerprocessestheFClayeroutputs,thesec-
ondlayerisfollowedbytheoutputlayerthatgivesthefinal
estimatesforvalenceandarousal.
function used for network training was another important
PartofAffWildNet’sdesignwasthefixingofitsoptimal issue.OurlossfunctionwasbasedontheCCC,asthiswas
hyper-parameters (number of FC and RNN layers, number themainevaluationcriterionoftheAff-WildChallenge;this
ofhiddenunitsintheselayers,batchsize,sequencelength, was not the case in the competing methods that used the
dropout,learningrate).Finally,thespecificationoftheloss usualMSEcriterionintheirtrainingphases.DeepAffectPredictionin-the-wild:Aff-WildDatabaseandChallenge,DeepArchitectures,andBeyond 15
As far as network training is concerned, the AffWild- Table 10: CCC based evaluation of valence & arousal pre-
Nethasbeentrainedasanend-to-endarchitecture,byjointly dictions provided by the fine-tuned AffWildNet and the
trainingitsCNNandRNNparts,ratherthanseparatelytrain- ResNet-GRUontheRECOLAtestset.AhigherCCCvalue
ingthetwoparts. indicatesabetterperformance.
We would also like to mention that the data augmen-
CCC
tationthatwasconductedsoastoachieveamorebalanced
Valence Arousal
dataset,alsocontributedinachievingtheAffWildNetastate-
Fine-tunedAffWildNet 0.526 0.273
of-the-artperformance. ResNet-GRU 0.462 0.209
5 FeatureLearningfromAff-Wild
When it comes to dimensional emotion recognition, there
exists great variability between different databases, espe-
cially those containing emotions in-the-wild. In particular,
the annotators and the range of the annotations are differ-
ent and the labels can be either discrete or continuous. To
tackle the problems caused by this variability, we take ad-
vantageofthefactthattheAff-Wildisapowerfuldatabase
thatcanbeexploitedforlearningfeatures,whichmaythen
be used as priors for dimensional emotion recognition. In
the following, we show that it can be used as prior for the
RECOLA and AFEW-VA databases that are annotated for
valence and arousal, just like Aff-Wild. In addition to this, (a) annotations
weuseitasapriorforcategoricalemotionrecognition,on
theEmotiWdataset,whichisannotatedintermsoftheseven
basicemotions.Experimentshavebeenconductedonthese
databasesyieldingstate-of-the-artresultsandthusverifying
thestrengthofAff-Wildforaffectrecognition.
5.1 PriorforValenceandArousalPrediction
5.1.1 ExperimentalResultsfortheAff-WildandRECOLA
database
In this subsection, we demonstrate the superiority of our
database when it is used for pre-training a DNN. In partic-
ular,wefine-tunetheAffWildNetontheRECOLAandfor
(b) predictions
comparisonpurposeswealsotrainonRECOLAanarchitec-
ture comprised of a ResNet-50 and a 2-layer GRU stacked
Fig. 12: Histogram in the 2-D valence & arousal space of
ontop(letuscallitResNet-GRUnetwork).Table10shows
(a) annotations and (b) predictions for the test set of the
theresultsonlyfortheCCCscoreasourminimizationloss
RECOLAdatabase.
wasdependingonthismetric.Itisclearthattheperformance
onbotharousalandvalenceofthefine-tunedmodelonthe
Aff-Wild database is much higher than the performance of RECOLA, for the valence and arousal dimensions, respec-
theResNet-GRUmodel. tively.
To further demonstrate the benefits of our model when
predictingvalenceandarousal,wedemonstrateahistogram 5.1.2 ExperimentalResultsfortheAFEW-VAdatabase
inthe2-Dvalence&arousalspaceoftheannotations(Fig-
ure12(a))andpredictionsofthefine-tunedAffWildNet(Fig- In this subsection, we focus on recognition of emotions in
ure12(b))forthewholetestsetofRECOLA. the AFEW-VA database, which annotation’s is somewhat
Finally,wealsoillustrateinFigures13(a)and13(b)the different from the annotation of the Aff-Wild database. In
network prediction and ground truth for one test video of particular, the labels of the AFEW-VA database are in the16 DimitriosKollias(cid:63)etal.
1
0.5
0
0.5
−
1
− 0 1,000 2,000 3,000 4,000 5,000 6,000 7,000
Frames
snoitatonnAecnelaV
Predictions
Labels
(a) Valence
1
0.5
0
0.5
−
1
− 0 1,000 2,000 3,000 4,000 5,000 6,000 7,000
Frames
snoitatonnAlasuorA
Predictions
Fig. 14: Discrete values of annotations of the AFEW-VA
Labels
database.
(b) Arousal
Fig.13:Fine-tunedAffWildNet’sPredictionsvsLabelsfor
(a) valence and (b) arousal for a single test video of the
RECOLAdatabase.
range[ 10,+10],whilethelabelsoftheAff-Wilddatabase
−
areintherange[ 1,+1].Totacklethisproblem,wescaled Fig. 15: Histogram in the 2-D valence & arousal space of
−
the range of the AFEW-VA labels to [ 1, +1]. Moreover, annotationsoftheAFEW-VAdatabase.
−
differenceswereobserved,duetothefactthatthelabelsof
theAFEW-VAarediscrete,whilethelabelsoftheAff-Wild
Table 11: Pearson Correlation Coefficient (Pearson CC)
are continuous. Figure 14 shows the discrete valence and
basedevaluationofvalence&arousalpredictionsprovided
arousal values of the annotations in AFEW-VA database,
bythebestarchitecturein[31]vsourAffWildNetfine-tuned
whereasFigure15showsthecorrespondinghistograminthe
on the AFEW-VA. A higher Pearson CC value indicates a
2-Dvalence&arousalspace.
betterperformance.
Wethenperformedfine-tuningoftheAffWildNettothe
AFEW-VAdatabaseandtestedtheperformanceofthegen-
Group PearsonCC
eratednetwork.Similarlyto[31],weuseda5-foldperson- Valence Arousal
independentcross-validationstrategy.Table11showsacom- bestof[31] 0.407 0.45
parison of the performance of the fine-tuned AffWildNet Fine-tunedAffWildNet 0.514 0.575
with the best results reported in [31]. Those results are in
termsofthePearsonCC.Itcanbeeasilyseenthatthefine-
tunedAffWildNetgreatlyoutperformedthebestmethodre- and2048hiddenunits,respectively.AsshowninTable13,
portedin[31]. the performance of the fine-tuned AffWildNet, in terms of
For comparison purposes, we also trained a CNN net- CCC,greatlyoutperformedthisnetworkaswell.
work on the AFEW-VA database. This network’s architec- All these verify that our network can be used as a pre-
ture was based on the convolution and pooling layers of trainedonetoyieldexcellentresultsacrossdifferentdimen-
VGG-Facefollowedby2fullyconnectedlayerswith4096 sionaldatabases.DeepAffectPredictionin-the-wild:Aff-WildDatabaseandChallenge,DeepArchitectures,andBeyond 17
Table12:AccuraciesontheEmotiWvalidationsetobtainedbydifferentCNNandCNN-RNNarchitecturesvsthefine-tuned
AffWildNet.Ahigheraccuracyvalueindicatesbetterperformance.
Architectures Accuracy
Neutral Anger Disgust Fear Happy Sad Surprise Total
VGG-16 0.327 0.424 0.102 0.093 0.476 0.138 0.133 0.263
VGG-16+RNN 0.431 0.559 0.026 0.07 0.444 0.259 0.044 0.293
ResNet 0.31 0.153 0.077 0.023 0.534 0.207 0.067 0.211
ResNet+RNN 0.431 0.237 0.077 0.07 0.587 0.155 0.089 0.261
VGG-Face+RNN 0.552 0.593 0.026 0.047 0.794 0.259 0.111 0.384
fine-tunedAffWildNet 0.569 0.627 0.051 0.023 0.746 0.709 0.111 0.454
Table 13: CCC based evaluation of valence & arousal pre- Itshouldalsobementionedthatthefine-tunedAffWildNet’s
dictionsprovidedbytheCNNarchitecturebasedonVGG- performance,intermsoftotalaccuracy,is:
Faceandthefine-tunedAffWildNetontheAFEW-VAtrain-
(i) much higher than the baseline total accuracy of 0.3881
ingset.AhigherCCCvalueindicateabetterperformance.
reportedin[16]
CCCAFEW-VA (ii) better than all vanilla architectures’ performances that
Valence Arousal werereportedbythethreewinningmethodsintheaudio-
onlyCNN 0.44 0.474
video emotion recognition EmotiW 2017 Grand Chal-
Fine-tunedAffWildNet 0.515 0.556
lenge[26][28][60]
(iii) comparableandbetterinsomecasesthantherestofthe
resultsobtainedbythethreewinningmethods[26][28]
5.2 PriorforCategoricalEmotionRecognition
[60]
5.2.1 ExperimentalResultsfortheEmotiWdataset The above are shown in Table 14. Those results verify that
theAffWildNetcanbeappropriatelyfine-tunedandsuccess-
To further show the strength of the AffWildNet, we used fully used for dimensional, as well as for categorical emo-
theAffWildNet-whichistrainedfordimensionalemotion tionrecognition.
recognition task - in a very different problem, that of cat-
egorical in-the-wild emotion recognition, focusing on the
EmotiW2017GrandChallenge.Totacklecategoricalemo-
6 ConclusionsandFutureWork
tionrecognition,wemodifiedtheAffWildNet’soutputlayer
toinclude7neurons(oneforeachbasicemotioncategory)
Deeplearninganddeepneuralnetworkshavebeensuccess-
andperformedfine-tuningontheAFEW5.0dataset.
fully used in the past years for facial expression and emo-
Inthepresentedexperiments,wecomparethefine-tuned tionrecognitionbasedonstillimageandvideoframeanal-
AffWildNet’sperformancewiththatofotherstate-of-the-art ysis.Recentresearchfocusesonin-the-wildfacialanalysis
CNN and CNN-RNN networks; the CNN part of which is andreferseithertocategoricalemotionrecognition,target-
based on the ResNet 50, VGG-16 and VGG-Face architec- ing recognition of the seven basic emotion categories, or
tures,trainedonthesameAFEW5.0dataset.Theaccuracies todimensionalemotionrecognition,analyzingthevalence-
of all networks on the validation set of the EmotiW 2017 arousal(V-A)representationspace.
GrandChallengeareshowninTable12.Ahigheraccuracy In this paper, we introduce Aff-Wild, a new, large in-
value indicates better performance for the model. We can the-wild database that consists of 298 videos of 200 sub-
easily see that the AffWildNet outperforms all those other jects, with a total length of more than 30 hours. We also
networksintermsoftotalaccuracy. present the Aff-WildChallenge that was organized on Aff-
Weshouldnotethat: Wild.Wereporttheresultsofthechallenge,andthepitfalls
andchallengesintermsofpredictingvalenceandarousalin-
(i) theAffWildNetwastrainedtoclassifyonlyvideoframes the-wild.Furthermore,wedesignadeepconvolutionaland
(and not audio) and then video classification based on recurrent neural architecture and perform extensive exper-
frameaggregationwasperformed imentation with the Aff-Wild database. We show that the
(ii) the cropped faces provided by the challenge were only generatedAffWildNetprovidesthebestperformanceforva-
used (and not our own detection and/or normalization lence and arousal estimation on the Aff-Wild dataset, both
procedure) intermsoftheConcordanceCorrelationCoefficientandthe
(iii) no data-augmentation, post-processing of the results or MeanSquaredErrorcriteria,whencomparedwithotherdeep
ensemblemethodologyhavebeenconducted. learningnetworkstrainedonthesamedatabase.18 DimitriosKollias(cid:63)etal.
Table14:OverallaccuraciesofthebestarchitecturesofthethreewinningmethodsoftheEmotiW2017GrandChallenge
reportedonthevalidationsetvsourfine-tunedAffWildNet.Ahigheraccuracyvalueindicatesbetterperformance.
Group Architecture TotalAccuracy
After
Data
Original Fine-Tuning
augmentation
onFER2013
DenseNet-121 0.414
[26] HoloNet 0.41 - -
ResNet-50 0.418
VGG-Face 0.379 0.483 -
FR-Net-A 0.337 0.446 -
[28] FR-Net-B 0.334 0.488 -
FR-Net-C 0.376 0.452 -
LSTM+FR-NET-B - 0.465 0.504
WeightedC3D(nooverlap) 0.421
LSTMC3D(nooverlap) 0.432
[60] - -
VGG-Face 0.414
VGG-LSTM1layer 0.486
Our Fine-tunedAffWildNet 0.454 - -
Subsequently,wethendemonstratethattheAffWildNet tiveemotions,emotionsingames,insocialgroupsandother
andAff-Wilddatabaseconstitutetoolsthatcanbeusedfor humanmachine(orrobot)interactions.
facialexpressionandemotionrecognitiononotherdatasets.
Usingappropriatefine-tuningandretrainingmethodologies,
Acknowledgements The work of Stefanos Zafeiriou has been par-
we show that best results can be obtained by applying the tially funded by the FiDiPro program of Tekes with project number
AffWildNet to other dimensional databases, including the 1849/31/2015.TheworkofDimitrisKolliaswasfundedbyaTeach-
ing Fellowship of Imperial College London. The support of the EP-
RECOLAandtheAFEW-VAonesandbycomparingtheob-
SRC Centre for Doctoral Training in High Performance Embedded
tained performances with other state-of-the-art pre-trained
and Distributed Systems (HiPEDS, Grant Reference EP/L016796/1)
andfine-tunednetworks. isgratefullyacknowledged.WealsothanktheNVIDIACorporation
Furthermore,weobservethatfine-tuningontheAffWild- fordonatingaTitanXGPU.Wewouldlikealsotoacknowledgethe
contributionoftheYoutubeusersthatgaveusthepermissiontouse
Net can produce state-of-the-art performance, not only for
theirvideos(especiallyZalzarandEddiefromThe1stTake).Wewish
dimensional, but also for categorical emotion recognition. tothankDrADhallforprovidinguswiththedataoftheEmotiw2017
We use this approach to tackle the facial expression and GrandChallenge.Additionally,wewouldliketothankthereviewers
emotionrecognitionpartsoftheEmotiW2017GrandChal- fortheirvaluablecommentsthathelpedustoimprovethispaper.
lenge, referring to recognition of the seven basic emotion
categories,findingthatweproducecomparableorbetterre- References
sultstothewinnersofthiscontest.
Itshouldbestressedthatitisthefirsttime,tothebestof 1. Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J.,
Devin,M.,Ghemawat,S.,Irving,G.,Isard,M.,etal.:Tensorflow:
ourknowledge,thatthesamedeeparchitecturecanbeused
asystemforlarge-scalemachinelearning. In:OSDI,vol.16,pp.
forbothtypesofdimensionalandcategoricalemotionanal-
265–283(2016)
ysis. To achieve this, the AffWildNet has been effectively 2. Alabort-i-Medina, J., Antonakos, E., Booth, J., Snape, P.,
trained with the largest existing, in-the-wild, database for Zafeiriou, S.: Menpo: A comprehensive platform for parametric
imagealignmentandvisualdeformablemodels. In:Proceedings
continuousvalence-arousalrecognition(regressionanalysis
of the ACM International Conference on Multimedia, MM ’14,
problem)andthenusedfortacklingthediscretesevenbasic pp.679–682.ACM,NewYork,NY,USA(2014)
emotionrecognition(classification)problem. 3. Albanie,S.,Vedaldi,A.:Learninggrimacesbywatchingtv. In:
ProceedingsoftheBritishMachineVisionConference(BMVC)
Theproposedprocedureforfine-tuningtheAffWildNet
(2016)
can be applied to further extend its use in the analysis of 4. Aung, M.S., Kaltwang, S., Romera-paredes, B., Martinez, B.,
othernewvisualemotionrecognitiondatasets.Thisincludes Singh, A., Cella, M., Valstar, M.F., Meng, H., Kemp, A.,
ourcurrentworkonextendingtheAff-Wildwithnewin-the- Elkins,A.C.,Tyler,N.,Watson,P.J.,Williams,A.C.,Pantic,M.,
Berthouze, N.: The automatic detection of chronic pain-related
wildaudiovisualinformation,aswellasusingitasameans
expression: requirements, challenges and a multimodal dataset.
for unifying different approaches to facial expression and IEEETransactionsonAffectiveComputing(2016)
emotionrecognition.Theseapproachescontaindimensional 5. Bartlett,M.S.,Littlewort,G.,Frank,M.,Lainscsek,C.,Fasel,I.,
Movellan, J.: Fully automatic facial action recognition in spon-
emotionrepresentations,basicandcompoundemotioncate-
taneousbehavior. In:AutomaticFaceandGestureRecognition,
gories,facialactionunitrepresentations,aswellasspecific
2006.FGR2006.7thInternationalConferenceon,pp.223–230.
emotion categories met in different contexts, such as nega- IEEE(2006)DeepAffectPredictionin-the-wild:Aff-WildDatabaseandChallenge,DeepArchitectures,andBeyond 19
6. Chang,W.Y.,Hsu,S.H.,Chien,J.H.:Fatauva-net:Anintegrated 25. Hochreiter,S.,Schmidhuber,J.:Longshort-termmemory.Neural
deep learning framework for facial attribute recognition, action computation9(8),1735–1780(1997)
unit(au)detection,andvalence-arousalestimation. In:Proceed- 26. Hu,P.,Cai,D.,Wang,S.,Yao,A.,Chen,Y.:Learningsupervised
ings of the IEEE Conference on Computer Vision and Pattern scoringensembleforemotionrecognitioninthewild.In:Proceed-
RecognitionWorkshop(2017) ings of the 19th ACM International Conference on Multimodal
7. Chatfield,K.,Simonyan,K.,Vedaldi,A.,Zisserman,A.:Return Interaction,pp.553–560.ACM(2017)
ofthedevilinthedetails:Delvingdeepintoconvolutionalnets. 27. Jung,H.,Lee,S.,Yim,J.,Park,S.,Kim,J.:Jointfine-tuningin
arXivpreprintarXiv:1405.3531(2014) deepneuralnetworksforfacialexpressionrecognition. In:Pro-
8. Chrysos,G.G.,Antonakos,E.,Snape,P.,Asthana,A.,Zafeiriou, ceedingsoftheIEEEInternationalConferenceonComputerVi-
S.:Acomprehensiveperformanceevaluationofdeformableface sion,pp.2983–2991(2015)
tracking in-the-wild. International Journal of Computer Vision 28. Knyazev, B., Shvetsov, R., Efremova, N., Kuharenko, A.: Con-
126(2-4),198–232(2018) volutional neural networks pretrained on large face recognition
9. Chung,J.,Gulcehre,C.,Cho,K.,Bengio,Y.:Empiricalevaluation datasets for emotion classification from video. arXiv preprint
ofgatedrecurrentneuralnetworksonsequencemodeling. arXiv arXiv:1711.04598(2017)
preprintarXiv:1412.3555(2014) 29. Koelstra, S., Muhl, C., Soleymani, M., Lee, J.S., Yazdani, A.,
10. Corneanu,C.,Oliu,M.,Cohn,J.,Escalera,S.:Surveyonrgb,3d, Ebrahimi,T.,Pun,T.,Nijholt,A.,Patras,I.:Deap:Adatabasefor
thermal,andmultimodalapproachesforfacialexpressionrecogni- emotionanalysis;usingphysiologicalsignals. IEEETransactions
tion:History,trends,andaffect-relatedapplications. IEEEtrans- onAffectiveComputing3(1),18–31(2012)
actionsonpatternanalysisandmachineintelligence(2016) 30. Kollias, D., Nicolaou, M., Kotsia, I., Zhao, G., Zafeiriou, S.:
11. Cowie,R.,Cornelius,R.R.:Describingtheemotionalstatesthat Recognitionofaffectinthewildusingdeepneuralnetworks. In:
are expressed in speech. Speech communication 40(1), 5–32 ProceedingsoftheIEEEConferenceonComputerVisionandPat-
(2003) ternRecognitionWorkshop(2017)
12. Cowie, R., Douglas-Cowie, E., Savvidou*, S., McMahon, E.,
31. Kossaifi,J.,Tzimiropoulos,G.,Todorovic,S.,Pantic,M.:Afew-
Sawey, M., Schro¨der, M.: ’feeltrace’: An instrument for record-
vadatabaseforvalenceandarousalestimationin-the-wild.Image
ingperceivedemotioninrealtime.In:ISCAtutorialandresearch
andVisionComputing(2017)
workshop(ITRW)onspeechandemotion(2000)
32. Lawrence, I., Lin, K.: A concordance correlation coefficient to
13. Cowie,R.,McKeown,G.,Douglas-Cowie,E.:Tracingemotion:
evaluatereproducibility. Biometricspp.255–268(1989)
anoverview. InternationalJournalofSyntheticEmotions(IJSE)
33. Lee,A.:Welcometovirtualdub.org!-virtualdub.org(2002)
3(1),1–17(2012)
34. Li,J.,Chen,Y.,Xiao,S.,Zhao,J.,Roy,S.,Feng,J.,Yan,S.,Sim,
14. Dalgleish, T., Power, M.: Handbook of cognition and emotion.
T.:Estimationofaffectivelevelinthewildwithmultiplememory
JohnWiley&Sons(2000)
networks. In:ProceedingsoftheIEEEConferenceonComputer
15. Deng,J.,Dong,W.,Socher,R.,Li,L.J.,Li,K.,Fei-Fei,L.:Im-
VisionandPatternRecognitionWorkshop(2017)
agenet:Alarge-scalehierarchicalimagedatabase. In:Computer
35. Lucey, P., Cohn, J.F., Kanade, T., Saragih, J., Ambadar, Z.,
VisionandPatternRecognition,2009.CVPR2009.IEEEConfer-
Matthews, I.: The extended cohn-kanade dataset (ck+): A com-
enceon,pp.248–255.IEEE(2009)
pletedatasetforactionunitandemotion-specifiedexpression. In:
16. Dhall, A., Goecke, R., Ghosh, S., Joshi, J., Hoey, J., Gedeon,
ComputerVisionandPatternRecognitionWorkshops(CVPRW),
T.:Fromindividualtogroup-levelemotionrecognition:Emotiw
2010IEEEComputerSocietyConferenceon,pp.94–101.IEEE
5.0. In:Proceedingsofthe19thACMInternationalConference
(2010)
onMultimodalInteraction,pp.524–528.ACM(2017)
36. Lucey,P.,Cohn,J.F.,Prkachin,K.M.,Solomon,P.E.,Matthews,I.:
17. Dhall, A., Goecke, R., Joshi, J., Hoey, J., Gedeon, T.: Emotiw
Painfuldata:Theunbc-mcmastershoulderpainexpressionarchive
2016:Videoandgroup-levelemotionrecognitionchallenges. In:
database. In:AutomaticFace&GestureRecognitionandWork-
Proceedingsofthe18thACMInternationalConferenceonMulti-
shops(FG2011),2011IEEEInternationalConferenceon,pp.57–
modalInteraction,pp.427–432.ACM(2016)
18. Dhall,A.,Goecke,R.,Joshi,J.,Sikka,K.,Gedeon,T.:Emotion 64.IEEE(2011)
37. Mahoor,M.,Hasani,B.:Facialaffectestimationinthewildusing
recognition in the wild challenge 2014: Baseline, data and pro-
deepresidualandconvolutionalnetworks. In:Proceedingsofthe
tocol. In: Proceedings of the 16th International Conference on
IEEE Conference on Computer Vision and Pattern Recognition
MultimodalInteraction,pp.461–466.ACM(2014)
19. Dhall,A.,Goecke,R.,Joshi,J.,Wagner,M.,Gedeon,T.:Emotion Workshop(2017)
recognition in the wild challenge 2013. In: Proceedings of the 38. Mathias,M.,Benenson,R.,Pedersoli,M.,VanGool,L.:Facede-
15thACMonInternationalconferenceonmultimodalinteraction, tectionwithoutbellsandwhistles. In:EuropeanConferenceon
pp.509–516.ACM(2013) ComputerVision,pp.720–735.Springer(2014)
20. Dhall, A., Ramana Murthy, O., Goecke, R., Joshi, J., Gedeon, 39. McKeown,G.,Valstar,M.,Cowie,R.,Pantic,M.,Schro¨der,M.:
T.:Videoandimagebasedemotionrecognitionchallengesinthe Thesemainedatabase:Annotatedmultimodalrecordsofemotion-
wild:Emotiw2015.In:Proceedingsofthe2015ACMonInterna- allycoloredconversationsbetweenapersonandalimitedagent.
tionalConferenceonMultimodalInteraction,pp.423–426.ACM AffectiveComputing,IEEETransactionson3(1),5–17(2012)
(2015) 40. More, A.: Survey of resampling techniques for improving clas-
21. Douglas-Cowie,E.,Cowie,R.,Cox,C.,Amier,N.,Heylen,D.K.: sification performance in unbalanced datasets. arXiv preprint
Thesensitiveartificiallistner:aninductiontechniqueforgenerat- arXiv:1608.06048(2016)
ingemotionallycolouredconversation. In:LRECWorkshopon 41. Pantic, M., Valstar, M., Rademaker, R., Maat, L.: Web-based
CorporaforResearchonEmotionandAffect.ELRA(2008) databaseforfacialexpressionanalysis. In:MultimediaandExpo,
22. Gross,R.,Matthews,I.,Cohn,J.,Kanade,T.,Baker,S.:Multi-pie. 2005.ICME2005.IEEEInternationalConferenceon,pp.5–pp.
ImageandVisionComputing28(5),807–813(2010) IEEE(2005)
23. Hardoon,D.R.,Szedmak,S.,Shawe-Taylor,J.:Canonicalcorre- 42. Parkhi,O.M.,Vedaldi,A.,Zisserman,A.:Deepfacerecognition.
lationanalysis;anoverviewwithapplicationtolearningmethods. In:BMVC,vol.1,p.6(2015)
Technicalreport,RoyalHolloway,UniversityofLondon(2003) 43. Plutchik,R.:Emotion:Apsychoevolutionarysynthesis. Harper-
24. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for collinsCollegeDivision(1980)
image recognition. In: Proceedings of the IEEE Conference on 44. Ringeval,F.,Schuller,B.,Valstar,M.,Cowie,R.,Pantic,M.:Avec
ComputerVisionandPatternRecognition,pp.770–778(2016) 2015: The 5th international audio/visual emotion challenge and20 DimitriosKollias(cid:63)etal.
workshop.In:Proceedingsofthe23rdACMinternationalconfer- GestureRecognition,2008.FG’08.8thIEEEInternationalCon-
enceonMultimedia,pp.1335–1336.ACM(2015) ferenceOn,pp.1–6.IEEE(2008)
45. Ringeval, F., Schuller, B., Valstar, M., Gratch, J., Cowie, R., 63. Yin,L.,Wei,X.,Sun,Y.,Wang,J.,Rosato,M.J.:A3dfacialex-
Scherer, S., Mozgai, S., Cummins, N., Schmi, M., Pantic, M.: pressiondatabaseforfacialbehaviorresearch.In:Automaticface
Avec2017–real-lifedepression,andaffectrecognitionworkshop andgesturerecognition,2006.FGR2006.7thinternationalcon-
andchallenge(2017) ferenceon,pp.211–216.IEEE(2006)
46. Ringeval,F.,Sonderegger,A.,Sauer,J.,Lalanne,D.:Introducing 64. YouTube,L.:Youtube. Retrieved27,2011(2011)
therecolamultimodalcorpusofremotecollaborativeandaffective 65. Zafeiriou, S., Kollias, D., Nicolaou, M.A., Papaioannou, A.,
interactions. In:AutomaticFaceandGestureRecognition(FG), Zhao, G., Kotsia, I.: Aff-wild: Valence and arousal in-the-
201310thIEEEInternationalConferenceandWorkshopson,pp. wildchallenge. In: Computer Vision and Pattern Recognition
1–8.IEEE(2013) Workshops(CVPRW),2017IEEEConferenceon,pp.1980–1987.
47. Russell,J.A.:Evidenceofconvergentvalidityonthedimensions IEEE(2017)
of affect. Journal of personality and social psychology 36(10), 66. Zeng,Z.,Pantic,M.,Roisman,G.I.,Huang,T.S.:Asurveyofaf-
1152(1978) fectrecognitionmethods:Audio,visual,andspontaneousexpres-
48. Sariyanidi, E., Gunes, H., Cavallaro, A.: Automatic analysis of sions. PatternAnalysisandMachineIntelligence,IEEETransac-
facialaffect:Asurveyofregistration,representation,andrecog- tionson31(1),39–58(2009)
nition.PatternAnalysisandMachineIntelligence,IEEETransac-
tionson37(6),1113–1133(2015)
49. Schuller, B., Valstar, M., Eyben, F., McKeown, G., Cowie, R., A Appendix
Pantic,M.:Avec2011–thefirstinternationalaudio/visualemotion
challenge.In:AffectiveComputingandIntelligentInteraction,pp.
A.1 Baseline:CNN-M
415–424.Springer(2011)
50. Schuller,B.,Valster,M.,Eyben,F.,Cowie,R.,Pantic,M.:Avec
2012:thecontinuousaudio/visualemotionchallenge.In:Proceed- TheexactstructureofthenetworkisshowninTable15.Intotal,itcon-
ingsofthe14thACMinternationalconferenceonMultimodalin- sistsof5convolutional,batchnormalizationandpoolinglayersand2
teraction,pp.449–456.ACM(2012) fullyconnected(FC)ones.Foreachconvolutionallayertheparameters
51. Simonyan,K.,Zisserman,A.:Verydeepconvolutionalnetworks arethefilterandthestride,intheformof(filterheight,filterwidth,
forlarge-scaleimagerecognition. arXiv:1409.1556(2014) input channels , output channels/feature maps) and (1, stride height,
52. Sneddon,I.,McRorie,M.,McKeown,G.,Hanratty,J.:Thebelfast stridewidth,1),respectively,andforthemaxpoolinglayerthepa-
inducednaturalemotiondatabase.IEEETransactionsonAffective rametersaretheksizeandstride,intheformof(poolingheight,pool-
Computing3(1),32–41(2012) ingwidth,inputchannels,outputchannels)and(1,strideheight,stride
53. Soleymani,M.,Lichtenauer,J.,Pun,T.,Pantic,M.:Amultimodal width,1),respectively.WefollowtheTensorFlow’splatformnotation
databaseforaffectrecognitionandimplicittagging. IEEETrans- forthevaluesofallthoseparameters.Notethattheactivationfunction
actionsonAffectiveComputing3(1),42–55(2012) intheconvolutionalandbatchnormalizationlayersistheReLUone;
54. Szegedy,C.,Ioffe,S.,Vanhoucke,V.,Alemi,A.A.:Inception-v4, thisisalsothecaseinthefirstFClayer.Theactivationfunctionofthe
inception-resnetandtheimpactofresidualconnectionsonlearn- secondFClayer,whichistheoutputlayer,isalinearone.
ing. In:AAAI,vol.4,p.12(2017)
55. Tian, Y.l., Kanade, T., Cohn, J.F.: Recognizing action units for
Table 15: Baseline architecture based on CNN-M, showing the val-
facialexpressionanalysis. PatternAnalysisandMachineIntelli-
uesoftheparametersoftheconvolutionalandpoolinglayersandthe
gence,IEEETransactionson23(2),97–115(2001)
56. Valstar,M.,Gratch,J.,Schuller,B.,Ringeval,F.,Lalanne,D.,Tor- numberofhiddenunitsinthefullyconnectedlayers.Wefollowthe
res Torres, M., Scherer, S., Stratou, G., Cowie, R., Pantic, M.: TensorFlow’splatformnotationforthevaluesofallthoseparameters.
Avec2016:Depression,mood,andemotionrecognitionworkshop
andchallenge. In:Proceedingsofthe6thInternationalWorkshop Layer filter ksize stride padding noofunits
conv1 [7,7,3,96] [1,2,2,1] ’VALID’
onAudio/VisualEmotionChallenge,pp.3–10.ACM(2016) batchnorm
57. Valstar,M.,Pantic,M.:Induceddisgust,happinessandsurprise: maxpooling [1,3,3,1] [1,2,2,1] ’VALID’
anadditiontothemmifacialexpressiondatabase. In:Proc.3rd conv2 [5,5,96,256] [1,2,2,1] ’SAME’
batchnorm
Intern.WorkshoponEMOTION(satelliteofLREC):Corporafor maxpooling [1,3,3,1] [1,2,2,1] ’SAME’
ResearchonEmotionandAffect,p.65(2010) conv3 [3,3,256,512] [1,1,1,1] ’SAME’
58. Valstar,M.,Schuller,B.,Smith,K.,Almaev,T.,Eyben,F.,Kra- batchnorm
conv4 [3,3,512,512] [1,1,1,1] ’SAME’
jewski,J.,Cowie,R.,Pantic,M.:Avec2014:3ddimensionalaf- batchnorm
fectanddepressionrecognitionchallenge. In:Proceedingsofthe conv5 [3,3,512,512] [1,1,1,1] ’SAME’
batchnorm
4thInternationalWorkshoponAudio/VisualEmotionChallenge,
maxpooling [1,2,2,1] [1,2,2,1] ’SAME’
pp.3–10.ACM(2014) fullyconnected1 4096
59. Valstar,M.,Schuller,B.,Smith,K.,Eyben,F.,Jiang,B.,Bilakhia, fullyconnected2 2
S.,Schnieder,S.,Cowie,R.,Pantic,M.:Avec2013:thecontin-
uousaudio/visualemotionanddepressionrecognitionchallenge.
In:Proceedingsofthe3rdACMinternationalworkshoponAu-
A.2 ResNet-50
dio/visualemotionchallenge,pp.3–10.ACM(2013)
60. Vielzeuf, V., Pateux, S., Jurie, F.: Temporal multimodal fusion
Residuallearningisadoptedinthesemodelsbystackingmultipleblocks
for video emotion classification in the wild. arXiv preprint
oftheform:
arXiv:1709.07200(2017)
61. Whissel,C.:Thedictionaryofaffectinlanguage,emotion:The-
ory,researchandexperience:vol.4,themeasurementofemotions, o =B(x ,{W })+h(x ), (4)
r. PlutchikandH.Kellerman,Eds.,NewYork:Academic(1989) k k k k
62. Yin,L.,Chen,X.,Sun,Y.,Worm,T.,Reale,M.:Ahigh-resolution wherex ,W ando indicatetheinput,theweights,andtheout-
k k k
3d dynamic facial expression database. In: Automatic Face & put of layer k, respectively, B indicates the residual function that isDeepAffectPredictionin-the-wild:Aff-WildDatabaseandChallenge,DeepArchitectures,andBeyond 21
ResNet-50 Fully Connected
7x7
conv,
3,64
max
pool
1x1
conv,64,64
3x3
conv,
64,64
1x1
conv,
64,256
1x1
conv,
256,64
3x3
conv,
64,64
1x1
conv,
64,256
1x1
conv,
256,128
3x3
conv,
128,128
1x1
conv,
128,512
1x1
conv,
512,128
3x3
conv,
128,128
1x1
conv,
128,512
1x1
conv,
512,256
3x3
conv,
256,256
1x1
conv,
256,1024
1x1
conv,
1024,256
3x3
conv,
256,256
1x1
conv,
256,1024
1x1
conv,
1024,512
3x3
conv,
512,512
1x1
conv,
512,2048
1x1
conv,
2048,512
3x3
conv,
2048,512
1x1
conv,
512,2048
avg
pool
VA
1500 256
x2 x3 x5 x2
Fig. 16: The CNN-only architecture for valence and arousal estimation, based on ResNet-50 structure and including two
fullyconnectedlayers(VandAstandforvalenceandarousalrespectively).Eachconvolutionallayerisintheformat:filter
height filterwidth,numberofinputfeaturemaps,numberofoutputfeaturemaps.
×
Fig. 17: The CNN-only architecture for valence and arousal estimation, based on VGG-Face structure (V and A stand for
valenceandarousalrespectively).
learntandhistheidentitymappingbetweentheresidualfunctionand Table16:CNNarchitecturebasedonVGG-Face/VGG-16,showingthe
theinput.Thehidentitymappingisaprojectionofx tomatchthe valuesoftheparametersoftheconvolutionalandpoolinglayersand
k
dimensionsofB(x ,{W })(doneby1×1convolutions),asin[24]. thenumberofhiddenunitsinthefullyconnectedlayers.Wefollowthe
Thefirstlayerk ofthek ResNet-50modeliscomprisedofa7×7 TensorFlow’splatformnotationforthevaluesofallthoseparameters.
convolutionallayerwith64featuremaps,followedbyamaxpooling
layerofsize3×3.Next,thereare4-bottleneckblocks,whereashortcut Layer filter ksize stride padding noofunits
conv1 [3,3,3,64] [1,1,1,1] ’SAME’
connectionisaddedaftereachblock.Eachoftheseblocksiscomprised conv2 [3,3,64,64] [1,1,1,1] ’SAME’
of3convolutionallayersofsizes1×1,3×3,and1×1withdifferent maxpooling [1,2,2,1] [1,2,2,1] ’SAME’
numberoffeaturemaps. conv3 [3,3,64,128] [1,1,1,1] ’SAME’
conv4 [3,3,128,128] [1,1,1,1] ’SAME’
ThearchitectureofthenetworkisdepictedinFigure16.Eachcon-
maxpooling [1,2,2,1] [1,2,2,1] ’SAME’
volutionallayerisintheformat:filterheight×filterwidth,numberof conv5 [3,3,128,256] [1,1,1,1] ’SAME’
inputfeaturemaps,numberofoutputfeaturemaps. conv6 [3,3,256,256] [1,1,1,1] ’SAME’
conv7 [3,3,256,256] [1,1,1,1] ’SAME’
maxpooling [1,2,2,1] [1,2,2,1] ’SAME’
conv8 [3,3,256,512] [1,1,1,1] ’SAME’
conv9 [3,3,512,512] [1,1,1,1] ’SAME’
A.3 VGG-Face/VGG-16
conv10 [3,3,512,512] [1,1,1,1] ’SAME’
maxpooling [1,2,2,1] [1,2,2,1] ’SAME’
conv11 [3,3,512,512] [1,1,1,1] ’SAME’
Table 16 shows the configuration of the CNN architecture based on
conv12 [3,3,512,512] [1,1,1,1] ’SAME’
VGG-Face or VGG-16. In total, it is composed of thirteen convolu- conv13 [3,3,512,512] [1,1,1,1] ’SAME’
tionalandpoolinglayersandthreefullyconnectedones.Forallthose maxpooling [1,2,2,1] [1,2,2,1] ’SAME’
layerstheformoftheparametersisthesameasdescribedaboveinthe fullyconnected1 4096
dropout
baseline architecture. We follow the TensorFlow’s platform notation fullyconnected2 4096
forthevaluesofallthoseparameters.Theoutputnumberofunitsis fullyconnected3 2
alsoshownintheTable.
A linear activation function was used in the last FC layer, pro-
vidingthefinalestimates.AllunitsintheremainingFClayerswere
equipped with the ReLU. Dropout has been added after the first FC
layerinordertoavoidover-fitting.Thearchitectureofthenetworkis
depictedinFigure17."
54,56,Deep convolution network based emotion analysis towards mental health care,"['Z Fei', 'E Yang', 'DDU Li', 'S Butler', 'W Ijomah', 'X Li', 'H Zhou']",2020,150,Karolinska Directed Emotional Faces,CNN,expression databases including the Karolinska Directed Emotional Faces (KDEF) Database  [25] CNN works as the deep feature extractor. The advantages of using a pre-trained CNN to,No DOI,Neurocomputing,https://www.sciencedirect.com/science/article/pii/S0925231220300783,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
55,57,Deep facial expression recognition: A survey,"['S Li', 'W Deng']",2020,1828,"Acted Facial Expressions In The Wild, Affective Faces Database, Binghamton University 3D Facial Expression, Japanese Female Facial Expression, MMI Facial Expression, Radboud Faces Database, Static Facial Expression in the Wild, Toronto Face Database","FER, deep learning, facial expression recognition, neural network","definition of facial expressions. In this survey, we limit our discussion on FER based on the  cat Pantic, “Induced disgust, happiness and surprise: an addition to the mmi facial expression",No DOI,IEEE transactions on affective computing,https://arxiv.org/abs/1804.08348,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"1
Deep Facial Expression Recognition: A Survey
Shan Li and Weihong Deng∗, Member, IEEE
Abstract—Withthetransitionoffacialexpressionrecognition(FER)fromlaboratory-controlledtochallengingin-the-wildconditions
andtherecentsuccessofdeeplearningtechniquesinvariousfields,deepneuralnetworkshaveincreasinglybeenleveragedtolearn
discriminativerepresentationsforautomaticFER.RecentdeepFERsystemsgenerallyfocusontwoimportantissues:overfitting
causedbyalackofsufficienttrainingdataandexpression-unrelatedvariations,suchasillumination,headposeandidentitybias.Inthis
paper,weprovideacomprehensivesurveyondeepFER,includingdatasetsandalgorithmsthatprovideinsightsintotheseintrinsic
problems.First,weintroducetheavailabledatasetsthatarewidelyusedintheliteratureandprovideaccepteddataselectionand
evaluationprinciplesforthesedatasets.WethendescribethestandardpipelineofadeepFERsystemwiththerelatedbackground
knowledgeandsuggestionsofapplicableimplementationsforeachstage.ForthestateoftheartindeepFER,wereviewexisting
noveldeepneuralnetworksandrelatedtrainingstrategiesthataredesignedforFERbasedonbothstaticimagesanddynamicimage
sequences,anddiscusstheiradvantagesandlimitations.Competitiveperformancesonwidelyusedbenchmarksarealsosummarized
inthissection.Wethenextendoursurveytoadditionalrelatedissuesandapplicationscenarios.Finally,wereviewtheremaining
challengesandcorrespondingopportunitiesinthisfieldaswellasfuturedirectionsforthedesignofrobustdeepFERsystems.
IndexTerms—FacialExpressionsRecognition,Facialexpressiondatasets,Affect,DeepLearning,Survey.
(cid:70)
1 INTRODUCTION
F ACIAL expression is one of the most powerful, natural and representation is encoded with only spatial information from the
universalsignalsforhumanbeingstoconveytheiremotional currentsingleimage,whereasdynamic-basedmethods[15],[16],
states and intentions [1], [2]. Numerous studies have been con- [17] consider the temporal relation among contiguous frames in
ducted on automatic facial expression analysis because of its the input facial expression sequence. Based on these two vision-
practicalimportanceinsociablerobotics,medicaltreatment,driver basedmethods,othermodalities,suchasaudioandphysiological
fatiguesurveillance,andmanyotherhuman-computerinteraction channels,havealsobeenusedinmultimodalsystems[18]toassist
systems. In the field of computer vision and machine learning, therecognitionofexpression.
various facial expression recognition (FER) systems have been Themajorityofthetraditionalmethodshaveusedhandcrafted
explored to encode expression information from facial represen- featuresorshallowlearning(e.g.,localbinarypatterns(LBP)[12],
tations.Asearlyas thetwentiethcentury,EkmanandFriesen[3] LBP on three orthogonal planes (LBP-TOP) [15], non-negative
definedsixbasicemotionsbasedoncross-culturestudy[4],which matrixfactorization(NMF)[19]andsparselearning[20])forFER.
indicatedthathumansperceivecertainbasicemotionsinthesame However, since 2013, emotion recognition competitions such as
way regardless of culture. These prototypical facial expressions FER2013 [21] and Emotion Recognition in the Wild (EmotiW)
areanger,disgust,fear,happiness,sadness,andsurprise.Contempt [22], [23], [24] have collected relatively sufficient training data
wassubsequentlyaddedasoneofthebasicemotions[5].Recently, from challenging real-world scenarios, which implicitly promote
advancedresearchonneuroscienceandpsychologyarguedthatthe thetransitionofFERfromlab-controlledtoin-the-wildsettings.In
modelofsixbasicemotionsareculture-specificandnotuniversal themeanwhile,duetothedramaticallyincreasedchipprocessing
[6]. abilities(e.g.,GPUunits)andwell-designednetworkarchitecture,
Althoughtheaffectmodelbasedonbasicemotionsislimited studies in various fields have begun to transfer to deep learning
in the ability to represent the complexity and subtlety of our methods,whichhaveachievedthestate-of-the-artrecognitionac-
dailyaffectivedisplays[7],[8],[9],andotheremotiondescription curacyandexceededpreviousresultsbyalargemargin(e.g.,[25],
models,suchastheFacialActionCodingSystem(FACS)[10]and [26],[27],[28]).Likewise,givenwithmoreeffectivetrainingdata
thecontinuousmodelusingaffectdimensions[11],areconsidered of facial expression, deep learning techniques have increasingly
to represent a wider range of emotions, the categorical model been implemented to handle the challenging factors for emotion
that describes emotions in terms of discrete basic emotions is recognitioninthewild.Figure1illustratesthisevolutiononFER
still the most popular perspective for FER, due to its pioneering intheaspectofalgorithmsanddatasets.
investigationsalongwiththedirectandintuitivedefinitionoffacial Exhaustive surveys on automatic expression analysis have
expressions. And in this survey, we will limit our discussion on been published in recent years [7], [8], [29], [30]. These surveys
FERbasedonthecategoricalmodel. have established a set of standard algorithmic pipelines for FER.
FERsystemscanbedividedintotwomaincategoriesaccord- However, they focus on traditional methods, and deep learning
ing to the feature representations: static image FER and dynamic has rarely been reviewed. Very recently, FER based on deep
sequenceFER.Instatic-basedmethods[12],[13],[14],thefeature learninghasbeensurveyedin[31],whichisabriefreviewwithout
introductionsonFERdatasetsandtechnicaldetailsondeepFER.
Therefore, in this paper, we make a systematic research on deep
• TheauthorsarewiththePatternRecognitionandIntelligentSystemLab-
oratory,SchoolofInformationandCommunicationEngineering,Beijing learning for FER tasks based on both static images and videos
UniversityofPostsandTelecommunications,Beijing,100876,China. (image sequences). We aim to give a newcomer to this filed an
E-mail:{ls1995,whdeng}@bupt.edu.cn.
overview of the systematic framework and prime skills for deep
8102
tcO
22
]VC.sc[
2v84380.4081:viXra2
of image or video samples, collection environment, expression
Algorithm 2007 Dataset
Zhao et al. [15] (LBP-TOP, SVM) distributionandadditionalinformation.
CK+ [33]:TheExtendedCohnKanade(CK+)databaseisthe
Shan et al. [12] (LBP, AdaBoost) 2009
CK+ most extensively used laboratory-controlled database for evaluat-
----
>
MMI ing FER systems. CK+ contains 593 video sequences from 123
Zhiet al. [19] (NMF) 2011 subjects. The sequences vary in duration from 10 to 60 frames
Zhonget al. [20] (Sparse learning) and show a shift from a neutral facial expression to the peak
expression.Amongthesevideos,327sequencesfrom118subjects
Tang (CNN) [130] (winner of FER2013)2013
Kahouet al. [57] (CNN, DBN, DAE) FER2013 are labeled with seven basic expression labels (anger, contempt,
----
>
(winner of EmotiW2013) EmotiW
disgust,fear,happiness,sadness,andsurprise)basedontheFacial
Fan et al. [108] (CNN-LSTM, C3D) 2015 Action Coding System (FACS). Because CK+ does not provide
(winner of EmotiW2016)
EmotioNet specifiedtraining,validationandtestsets,thealgorithmsevaluated
HoloNet
P IAP CD NN N tupletclustL eP r ll oo ss ss 2017 RAF-DB on this database are not uniform. For static-based methods, the
F …a c …eNet2ExpNet Island …lo s …s AffectNet most common data selection method is to extract the last one to
threeframeswithpeakformationandthefirstframe(neutralface)
Fig.1.Theevolutionoffacialexpressionrecognitionintermsofdatasets ofeachsequence.Then,thesubjectsaredividedintongroupsfor
andmethods.
person-independent n-fold cross-validation experiments, where
commonlyselectedvaluesofnare5,8and10.
FER. MMI [34], [35]: The MMI database is laboratory-controlled
and includes 326 sequences from 32 subjects. A total of 213
Despitethepowerfulfeaturelearningabilityofdeeplearning,
sequences are labeled with six basic expressions (without “con-
problemsremainwhenappliedtoFER.First,deepneuralnetworks
tempt”), and 205 sequences are captured in frontal view. In
require a large amount of training data to avoid overfitting.
contrasttoCK+,sequencesinMMIareonset-apex-offsetlabeled,
However,theexistingfacialexpressiondatabasesarenotsufficient
i.e., the sequence begins with a neutral expression and reaches
totrainthewell-knownneuralnetworkwithdeeparchitecturethat
peak near the middle before returning to the neutral expression.
achieved the most promising results in object recognition tasks.
Furthermore,MMIhasmorechallengingconditions,i.e.,thereare
Additionally, high inter-subject variations exist due to different
largeinter-personal variationsbecausesubjects performthesame
personal attributes, such as age, gender, ethnic backgrounds and
expression non-uniformly and many of them wear accessories
level of expressiveness [32]. In addition to subject identity bias,
(e.g., glasses, mustache). For experiments, the most common
variations in pose, illumination and occlusions are common in
methodistochoosethefirstframe(neutralface)andthethreepeak
unconstrained facial expression scenarios. These factors are non-
frames in each frontal sequence to conduct person-independent
linearly coupled with facial expressions and therefore strengthen
10-foldcross-validation.
the requirement of deep networks to address the large intra-class
JAFFE[36]:TheJapaneseFemaleFacialExpression(JAFFE)
variability and to learn effective expression-specific representa-
database is a laboratory-controlled image database that contains
tions.
213samplesofposedexpressionsfrom10Japanesefemales.Each
In this paper, we introduce recent advances in research on
person has 3˜4 images with each of six basic facial expressions
solvingtheaboveproblemsfordeepFER.Weexaminethestate-
(anger, disgust, fear, happiness, sadness, and surprise) and one
of-the-art results that have not been reviewed in previous survey
image with a neutral expression. The database is challenging be-
papers. The rest of this paper is organized as follows. Frequently
cause it contains few examples per subject/expression. Typically,
used expression databases are introduced in Section 2. Section 3
alltheimagesareusedfortheleave-one-subject-outexperiment.
identifies three main steps required in a deep FER system and
TFD[37]:TheTorontoFaceDatabase(TFD)isanamalgama-
describes the related background. Section 4 provides a detailed
tion of several facial expression datasets. TFD contains 112,234
reviewofnovelneuralnetworkarchitecturesandspecialnetwork
images, 4,178 of which are annotated with one of seven expres-
training tricks designed for FER based on static images and
sion labels: anger, disgust, fear, happiness, sadness, surprise and
dynamicimagesequences.Wethencoveradditionalrelatedissues
neutral.Thefaceshavealreadybeendetectedandnormalizedtoa
and other practical scenarios in Section 5. Section 6 discusses
sizeof48*48suchthatallthesubjectseyesarethesamedistance
someofthechallengesandopportunitiesinthisfieldandidentifies
apartandhavethesameverticalcoordinates.Fiveofficialfoldsare
potentialfuturedirections.
providedinTFD;eachfoldcontainsatraining,validation,andtest
setconsistingof70%,10%,and20%oftheimages,respectively.
FER2013[21]:TheFER2013databasewasintroducedduring
2 FACIAL EXPRESSION DATABASES
theICML2013ChallengesinRepresentationLearning.FER2013
Having sufficient labeled training data that include as many is a large-scale and unconstrained database collected automati-
variations of the populations and environments as possible is cally by the Google image search API. All images have been
importantforthedesignofadeepexpressionrecognitionsystem. registered and resized to 48*48 pixels after rejecting wrongfully
In this section, we discuss the publicly available databases that labeled frames and adjusting the cropped region. FER2013 con-
containbasicexpressionsandthatarewidelyusedinourreviewed tains 28,709 training images, 3,589 validation images and 3,589
papersfordeeplearningalgorithmsevaluation.Wealsointroduce test images with seven expression labels (anger, disgust, fear,
newly released databases that contain a large amount of affective happiness,sadness,surpriseandneutral).
images collected from the real world to benefit the training of AFEW [48]: The Acted Facial Expressions in the Wild
deep neural networks. Table 1 provides an overview of these (AFEW) database was first established and introduced in [49]
datasets,includingthemainreference,numberofsubjects,number and has served as an evaluation platform for the annual Emo-3
TABLE1
Anoverviewofthefacialexpressiondatasets.P=posed;S=spontaneous;Condit.=Collectioncondition;Elicit.=Elicitationmethod.
Database Samples Subject Condit. Elicit. Expressiondistribution Access
593image 6basicexpressionspluscontempt
CK+[33] 123 Lab P&S http://www.consortium.ri.cmu.edu/ckagree/
sequences andneutral
740images
MMI[34],[35] and2,900 25 Lab P 6basicexpressionsplusneutral https://mmifacedb.eu/
videos
JAFFE[36] 213images 10 Lab P 6basicexpressionsplusneutral http://www.kasrl.org/jaffe.html
112,234
TFD[37] N/A Lab P 6basicexpressionsplusneutral josh@mplab.ucsd.edu
images
https://www.kaggle.com/c/challenges-in-representatio
FER-2013[21] 35,887images N/A Web P&S 6basicexpressionsplusneutral
n-learning-facial-expression-recognition-challenge
AFEW7.0[24] 1,809videos N/A Movie P&S 6basicexpressionsplusneutral https://sites.google.com/site/emotiwchallenge/
SFEW2.0[22] 1,766images N/A Movie P&S 6basicexpressionsplusneutral https://cs.anu.edu.au/few/emotiw2015.html
755,370 Smile,surprised,squint,disgust,
Multi-PIE[38] 337 Lab P http://www.flintbox.com/public/project/4742/
images screamandneutral
http://www.cs.binghamton.edu/∼lijun/Research/3DFE
BU-3DFE[39] 2,500images 100 Lab P 6basicexpressionsplusneutral
/3DFE Analysis.html
2,880image
Oulu-CASIA[40] 80 Lab P 6basicexpressions http://www.cse.oulu.fi/CMV/Downloads/Oulu-CASIA
sequences
6basicexpressionspluscontempt
RaFD[41] 1,608images 67 Lab P http://www.socsci.ru.nl:8180/RaFD2/RaFD
andneutral
KDEF[42] 4,900images 70 Lab P 6basicexpressionsplusneutral http://www.emotionlab.se/kdef/
1,000,000 23basicexpressionsorcompound
EmotioNet[43] N/A Web P&S http://cbcsl.ece.ohio-state.edu/dbform emotionet.html
images expressions
6basicexpressionsplusneutraland
RAF-DB[44],[45] 29672images N/A Web P&S http://www.whdeng.cn/RAF/model1.html
12compoundexpressions
450,000
AffectNet[46] images N/A Web P&S 6basicexpressionsplusneutral http://mohammadmahoor.com/databases-codes/
(labeled)
http://mmlab.ie.cuhk.edu.hk/projects/socialrelation/ind
ExpW[47] 91,793images N/A Web P&S 6basicexpressionsplusneutral
ex.html
tion Recognition In The Wild Challenge (EmotiW) since 2013. 755,370 images from 337 subjects under 15 viewpoints and 19
AFEW contains video clips collected from different movies with illuminationconditionsinuptofourrecordingsession.Eachfacial
spontaneous expressions, various head poses, occlusions and il- image is labeled with one of six expressions: disgust, neutral,
luminations. AFEW is a temporal and multimodal database that scream, smile, squint and surprise. This dataset is typically used
provides with vastly different environmental conditions in both formultiviewfacialexpressionanalysis.
audio and video. Samples are labeled with seven expressions:
BU-3DFE [39]: The Binghamton University 3D Facial Ex-
anger, disgust, fear, happiness, sadness, surprise and neutral. The
pression (BU-3DFE) database contains 606 facial expression se-
annotation of expressions have been continuously updated, and
quencescapturedfrom100people.Foreachsubject,sixuniversal
realityTVshowdatahavebeencontinuouslyadded.TheAFEW
facial expressions (anger, disgust, fear, happiness, sadness and
7.0 in EmotiW 2017 [24] is divided into three data partitions in
surprise)areelicitedbyvariousmannerswithmultipleintensities.
anindependentmannerintermsofsubjectandmovie/TVsource:
Similar to Multi-PIE, this dataset is typically used for multiview
Train (773 samples), Val (383 samples) and Test (653 samples),
3Dfacialexpressionanalysis.
whichensuresdatainthethreesetsbelongtomutuallyexclusive
Oulu-CASIA[40]:TheOulu-CASIAdatabaseincludes2,880
moviesandactors.
image sequences collected from 80 subjects labeled with six
SFEW [50]: The Static Facial Expressions in the Wild basicemotionlabels:anger,disgust,fear,happiness,sadness,and
(SFEW) was created by selecting static frames from the AFEW surprise. Each of the videos is captured with one of two imaging
databasebycomputingkeyframesbasedonfacialpointclustering. systems, i.e., near-infrared (NIR) or visible light (VIS), under
The most commonly used version, SFEW 2.0, was the bench- three different illumination conditions. Similar to CK+, the first
marking data for the SReco sub-challenge in EmotiW 2015 [22]. frame is neutral and the last frame has the peak expression.
SFEW 2.0 has been divided into three sets: Train (958 samples), Typically, only the last three peak frames and the first frame
Val (436 samples) and Test (372 samples). Each of the images is (neutral face) from the 480 videos collected by the VIS System
assignedtooneofsevenexpressioncategories,i.e.,anger,disgust, undernormalindoorilluminationareemployedfor10-foldcross-
fear, neutral, happiness, sadness, and surprise. The expression validationexperiments.
labels of the training and validation sets are publicly available,
RaFD [41]: The Radboud Faces Database (RaFD) is
whereas those of the testing set are held back by the challenge
laboratory-controlled and has a total of 1,608 images from 67
organizer.
subjects with three different gaze directions, i.e., front, left and
Multi-PIE [38]: The CMU Multi-PIE database contains right.Eachsampleislabeledwithoneofeightexpressions:anger,4
contempt,disgust,fear,happiness,sadness,surpriseandneutral. TABLE2
KDEF [42]: The laboratory-controlled Karolinska Directed Summaryofdifferenttypesoffacealignmentdetectorsthatarewidely
usedindeepFERmodels.
Emotional Faces (KDEF) database was originally developed for
use in psychological and medical research. KDEF consists of
type #pointsreal-time speed performance usedin
images from 70 actors with five different angles labeled with six
poor
basicfacialexpressionsplusneutral. Holistic AAM[53] 68 (cid:55) fair [54],[55]
generalization
In addition to these commonly used datasets for basic emo- MoT[56] 39/68 (cid:55) slow/ [57],[58]
Part-based good
tionrecognition,severalwell-establishedandlarge-scalepublicly DRMF[59] 66 (cid:55) fast [60],[61]
available facial expression databases collected from the Internet SDM[62] 49 (cid:51) [16],[63]
Cascaded fast/ good/
that are suitable for training deep neural networks have emerged regression3000fps[64] 68 (cid:51) veryfast verygood [55]
Incremental[65] 49 (cid:51) [66]
inthelasttwoyears.
Deep cascadedCNN[67] 5 (cid:51) good/ [68]
EmotioNet[43]:EmotioNetisalarge-scaledatabasewithone learning MTCNN[69] 5 (cid:51) fast verygood [70],[71]
million facial expression images collected from the Internet. A
total of 950,000 images were annotated by the automatic action
unit (AU) detection model in [43], and the remaining 25,000 and publicly available implementations that are widely used in
imagesweremanuallyannotatedwith11AUs.Thesecondtrackof deepFER.
theEmotioNetChallenge[51]providessixbasicexpressionsand Given a series of training data, the first step is to detect the
tencompoundexpressions[52],and2,478imageswithexpression face and then to remove background and non-face areas. The
labelsareavailable. Viola-Jones (V&J) face detector [72] is a classic and widely
RAF-DB[44],[45]:TheReal-worldAffectiveFaceDatabase employed implementation for face detection, which is robust and
(RAF-DB) is a real-world database that contains 29,672 highly computationallysimplefordetectingnear-frontalfaces.
diverse facial images downloaded from the Internet. With man- Althoughfacedetectionistheonlyindispensableprocedureto
ually crowd-sourced annotation and reliable estimation, seven enable feature learning, further face alignment using the coordi-
basic and eleven compound emotion labels are provided for the nates of localized landmarks can substantially enhance the FER
samples. Specifically, 15,339 images from the basic emotion set performance [14]. This step is crucial because it can reduce the
are divided into two groups (12,271 training samples and 3,068 variation in face scale and in-plane rotation. Table 2 investigates
testingsamples)forevaluation. faciallandmarkdetectionalgorithmswidely-usedindeepFERand
AffectNet [46]: AffectNet contains more than one million comparesthemintermsofefficiencyandperformance.TheActive
imagesfromtheInternetthatwereobtainedbyqueryingdifferent AppearanceModel(AAM)[53]isaclassicgenerativemodelthat
search engines using emotion-related tags. It is by far the largest optimizestherequiredparametersfromholisticfacialappearance
databasethatprovidesfacialexpressionsintwodifferentemotion andglobalshapepatterns.Indiscriminativemodels,themixtures
models (categorical model and dimensional model), of which of trees (MoT) structured models [56] and the discriminative
450,000 images have manually annotated labels for eight basic responsemapfitting(DRMF)[59]usepart-basedapproachesthat
expressions. represent the face via the local appearance information around
ExpW [47]: The Expression in-the-Wild Database (ExpW) each landmark. Furthermore, a number of discriminative models
contains 91,793 faces downloaded using Google image search. directly use a cascade of regression functions to map the image
Each of the face images was manually annotated as one of the appearance to landmark locations and have shown better results,
sevenbasicexpressioncategories.Non-faceimageswereremoved e.g., the supervised descent method (SDM) [62] implemented
intheannotationprocess. in IntraFace [73], the face alignment 3000 fps [64], and the
incremental face alignment [65]. Recently, deep networks have
been widely exploited for face alignment. Cascaded CNN [67]
3 DEEP FACIAL EXPRESSION RECOGNITION
is the early work which predicts landmarks in a cascaded way.
Inthissection,wedescribethethreemainstepsthatarecommon Based on this, Tasks-Constrained Deep Convolutional Network
inautomaticdeepFER,i.e.,pre-processing,deepfeaturelearning (TCDCN)[74]andMulti-taskCNN(MTCNN)[69]furtherlever-
and deep feature classification. We briefly summarize the widely age multi-task learning to improve the performance. In general,
usedalgorithmsforeachstepandrecommendtheexistingstate-of- cascaded regression has become the most popular and state-of-
the-art best practice implementations according to the referenced the-artmethodsforfacealignmentasitshighspeedandaccuracy.
papers. In contrast to using only one detector for face alignment,
some methods proposed to combine multiple detectors for better
landmarkestimationwhenprocessingfacesinchallenginguncon-
3.1 Pre-processing
strained environments. Yu et al. [75] concatenated three different
Variationsthatareirrelevanttofacialexpressions,suchasdifferent
facial landmark detectors to complement each other. Kim et al.
backgrounds,illuminationsandheadposes,arefairlycommonin
[76] considered different inputs (original image and histogram
unconstrainedscenarios.Therefore,beforetrainingthedeepneural
equalized image) and different face detection models (V&J [72]
network to learn meaningful features, pre-processing is required
andMoT[56]),andthelandmarksetwiththehighestconfidence
to align and normalize the visual semantic information conveyed
providedbytheIntraface[73]wasselected.
bytheface.
3.1.2 Dataaugmentation
3.1.1 Facealignment Deep neural networks require sufficient training data to ensure
Face alignment is a traditional pre-processing step in many face- generalizability to a given recognition task. However, most pub-
related recognition tasks. We list some well-known approaches liclyavailabledatabasesforFERdonothaveasufficientquantity5
Emotion Training
Labels
Input Images C1 Layer P1 Layer C2 Layer P2 Layer CNN
scaling, rotation, colors, noises…
Full
Convolutions Subsampling Convolutions Subsampling Connected Trained
Face Model
Data Augmentation
𝑾𝒉hidden variables
BStipruacrttuitree
ℎℎ 23
RBM
label 𝑉
𝑠𝑜
𝑊
Unfold
𝒔𝒕𝑉
−𝟏𝑶𝒕−𝟏
𝒔𝑉
𝒕𝑶𝒕
𝒔𝒕+𝑉
𝟏𝑶𝒕+𝟏
ℎ1 𝑊 𝑊 𝑊 𝑊 Anger
Alignment Normalization Image𝑽 visible variables DBN 𝑣 𝑈 𝑥 RNN 𝑈 𝒙𝒕−𝟏 𝑈 𝒙𝒕 𝑈 𝒙𝒕+𝟏 Co Dn ist ge um stpt
Fear
As close as possible Noise Happiness
SI em qa ug ee ns
c
&
es
Illumination Pose
reyaL
tupnI
𝑊1 reyaL 𝑊2 reyaL … reyaL elttob reyaL … reyaL 𝑊2𝑇 reyaL 𝑊1𝑇 reyaL
tuptuO
saD mat pa l e Discrimi Dna at to
a
r
saG me
s
pan
lm
ee r ?pa lt eed
Generator
SSN uae
d
ru pnt rer isa
s
esl
𝑥 Encoder Code Decoder 𝑥
DAE Yes / No GAN
Trained
Input Pre-processing Model Testing DeepNetworks Output
Fig.2.Thegeneralpipelineofdeepfacialexpressionrecognitionsystems.
of images for training. Therefore, data augmentation is a vital 3.1.3 Facenormalization
step for deep FER. Data augmentation techniques can be divided
into two groups: on-the-fly data augmentation and offline data Variations in illumination and head poses can introduce large
augmentation. changes in images and hence impair the FER performance.
Therefore, we introduce two typical face normalization methods
Usually,theon-the-flydataaugmentationisembeddedindeep
to ameliorate these variations: illumination normalization and
learning toolkits to alleviate overfitting. During the training step,
posenormalization(frontalization).
theinputsamplesarerandomlycroppedfromthefourcornersand
centeroftheimageandthenflippedhorizontally,whichcanresult
Illumination normalization: Illumination and contrast can
inadatasetthatistentimeslargerthantheoriginaltrainingdata.
varyindifferentimagesevenfromthesamepersonwiththesame
Two common prediction modes are adopted during testing: only
expression, especially in unconstrained environments, which can
thecenterpatchofthefaceisusedforprediction(e.g.,[61],[77])
result in large intra-class variances. In [60], several frequently
or the prediction value is averaged over all ten crops (e.g., [76],
used illumination normalization algorithms, namely, isotropic
[78]).
diffusion (IS)-based normalization, discrete cosine transform
Besides the elementary on-the-fly data augmentation, various (DCT)-based normalization [85] and difference of Gaussian
offlinedataaugmentationoperationshavebeendesignedtofurther (DoG), were evaluated for illumination normalization. And [86]
expanddataonbothsizeanddiversity.Themostfrequentlyused employed homomorphic filtering based normalization, which has
operationsincluderandomperturbationsandtransforms,e.g.,rota- beenreportedtoyieldthemostconsistentresultsamongallother
tion,shifting,skew,scaling,noise,contrastandcolorjittering.For techniques, to remove illumination normalization. Furthermore,
example,commonnoisemodels,salt&pepperandspecklenoise related studies have shown that histogram equalization combined
[79]andGaussiannoise[80],[81]areemployedtoenlargethedata with illumination normalization results in better face recognition
size.Andforcontrasttransformation,saturationandvalue(Sand performance than that achieved using illumination normalization
VcomponentsoftheHSVcolorspace)ofeachpixelarechanged on it own. And many studies in the literature of deep FER (e.g.,
[70] for data augmentation. Combinations of multiple operations [75], [79], [87], [88]) have employed histogram equalization to
cangeneratemoreunseentrainingsamplesandmakethenetwork increase the global contrast of images for pre-processing. This
more robust to deviated and rotated faces. In [82], the authors method is effective when the brightness of the background and
applied five image appearance filters (disk, average, Gaussian, foreground are similar. However, directly applying histogram
unsharpandmotionfilters)andsixaffinetransformmatricesthat equalization may overemphasize local contrast. To solve this
wereformalizedbyaddingslightgeometrictransformationstothe problem, [89] proposed a weighted summation approach to
identity matrix. In [75], a more comprehensive affine transform combine histogram equalization and linear mapping. And in
matrix was proposed to randomly generate images that varied in [79], the authors compared three different methods: global
terms of rotation, skew and scale. Furthermore, deep learning contrastnormalization(GCN),localnormalization,andhistogram
based technology can be applied for data augmentation. For equalization. GCN and histogram equalization were reported
example,asyntheticdatagenerationsystemwith3Dconvolutional to achieve the best accuracy for the training and testing steps,
neuralnetwork(CNN)wascreatedin[83]toconfidentiallycreate respectively.
faces with different levels of saturation in expression. And the
generativeadversarialnetwork(GAN)[84]canalsobeappliedto Pose normalization: Considerable pose variation is another
augmentdatabygeneratingdiverseappearancesvaryinginposes commonandintractableprobleminunconstrainedsettings.Some
andexpressions.(seeSection4.1.7). studies have employed pose normalization techniques to yield6
frontal facial views for FER (e.g., [90], [91]), among which the TABLE3
most popular was proposed by Hassner et al. [92]. Specifically, ComparisonofCNNmodelsandtheirachievements.DA=Data
augmentation;BN=Batchnormalization.
after localizing facial landmarks, a 3D texture reference model
generic to all faces is generated to efficiently estimate visible
facialcomponents.Then,theinitialfrontalizedfaceissynthesized AlexNet VGGNet GoogleNet ResNet
by back-projecting each input face image to the reference [25] [26] [27] [28]
coordinate system. Alternatively, Sagonas et al. [93] proposed an
Year 2012 2014 2014 2015
effective statistical model to simultaneously localize landmarks #oflayers† 5+3 13/16+3 21+1 151+1
andconvertfacialposesusingonlyfrontalfaces.Veryrecently,a Kernelsize(cid:63) 11,5,3 3 7,1,3,5 7,1,3,5
seriesofGAN-baseddeepmodelswereproposedforfrontalview DA (cid:51) (cid:51) (cid:51) (cid:51)
synthesis(e.g.,FF-GAN[94],TP-GAN[95])andDR-GAN[96]) Dropout (cid:51) (cid:51) (cid:51) (cid:51)
andreportpromisingperformances. Inception (cid:55) (cid:55) (cid:51) (cid:55)
BN (cid:55) (cid:55) (cid:55) (cid:51)
3.2 Deepnetworksforfeaturelearning Usedin [110] [78],[111] [17],[78] [91],[112]
Deep learning has recently become a hot research topic and has †numberofconvolutionallayers+fullyconnectedlayers
achievedstate-of-the-artperformanceforavarietyofapplications (cid:63)sizeoftheconvolutionkernel
[97]. Deep learning attempts to capture high-level abstractions
through hierarchical architectures of multiple nonlinear transfor-
mations and representations. In this section, we briefly introduce proposedthewell-designedC3D,whichexploits3Dconvolutions
some deep learning techniques that have been applied for FER. onlarge-scalesupervisedtrainingdatasetstolearnspatio-temporal
The traditional architectures of these deep neural networks are features. Many related studies (e.g., [108], [109]) have employed
showninFig.2. thisnetworkforFERinvolvingimagesequences.
3.2.1 Convolutionalneuralnetwork(CNN) 3.2.2 Deepbeliefnetwork(DBN)
CNN has been extensively used in diverse computer vision ap- DBN proposed by Hinton et al. [113] is a graphical model that
plications, including FER. At the beginning of the 21st century, learnstoextractadeephierarchicalrepresentationofthetraining
several studies in the FER literature [98], [99] found that the data.ThetraditionalDBNisbuiltwithastackofrestrictedBoltz-
CNN is robust to face location changes and scale variations and mann machines (RBMs) [114], which are two-layer generative
behaves better than the multilayer perceptron (MLP) in the case stochastic models composed of a visible-unit layer and a hidden-
of previously unseen face pose variations. [100] employed the unit layer. These two layers in an RBM must form a bipartite
CNN to address the problems of subject independence as well graph without lateral connections. In a DBN, the units in higher
as translation, rotation, and scale invariance in the recognition of layersaretrainedtolearntheconditionaldependenciesamongthe
facialexpressions. unitsintheadjacentlowerlayers,exceptthetoptwolayers,which
A CNN consists of three types of heterogeneous layers: haveundirectedconnections.ThetrainingofaDBNcontainstwo
convolutional layers, pooling layers, and fully connected layers. phases:pre-trainingandfine-tuning[115].First,anefficientlayer-
The convolutional layer has a set of learnable filters to convolve by-layer greedy learning strategy [116] is used to initialize the
throughthewholeinputimageandproducevariousspecifictypes deepnetworkinanunsupervisedmanner,whichcanpreventpoor
of activation feature maps. The convolution operation is associ- local optimal results to some extent without the requirement of
ated with three main benefits: local connectivity, which learns alargeamountoflabeleddata.Duringthisprocedure,contrastive
correlationsamongneighboringpixels;weightsharinginthesame divergence[117]isusedtotrainRBMsintheDBNtoestimatethe
featuremap,whichgreatlyreducesthenumberoftheparameters approximationgradientofthelog-likelihood.Then,theparameters
tobelearned;andshift-invariancetothelocationoftheobject.The ofthenetworkandthedesiredoutputarefine-tunedwithasimple
poolinglayerfollowstheconvolutionallayerandisusedtoreduce gradientdescentundersupervision.
thespatialsizeofthefeaturemapsandthecomputationalcostof
the network. Average pooling and max pooling are the two most 3.2.3 Deepautoencoder(DAE)
commonlyusednonlineardown-samplingstrategiesfortranslation DAE was first introduced in [118] to learn efficient codings for
invariance. The fully connected layer is usually included at the dimensionalityreduction.Incontrasttothepreviouslymentioned
endofthenetworktoensurethatallneuronsinthelayerarefully networks, which are trained to predict target values, the DAE
connected to activations in the previous layer and to enable the is optimized to reconstruct its inputs by minimizing the recon-
2Dfeaturemapstobeconvertedinto1Dfeaturemapsforfurther structionerror.VariationsoftheDAEexist,suchasthedenoising
featurerepresentationandclassification. autoencoder [119], which recovers the original undistorted input
We list the configurations and characteristics of some well- from partially corrupted data; the sparse autoencoder network
known CNN models that have been applied for FER in Table 3. (DSAE) [120], which enforces sparsity on the learned feature
Besides these networks, several well-known derived frameworks representation; the contractive autoencoder (CAE1) [121], which
also exist. In [101], [102], region-based CNN (R-CNN) [103] addsanactivitydependentregularizationtoinducelocallyinvari-
was used to learn features for FER. In [104], Faster R-CNN ant features; the convolutional autoencoder (CAE2) [122], which
[105] was used to identify facial expressions by generating high- uses convolutional (and optionally pooling) layers for the hidden
quality region proposals. Moreover, Ji et al. proposed 3D CNN layers in the network; and the variational auto-encoder (VAE)
[106]tocapturemotioninformationencodedinmultipleadjacent [123], which is a directed graphical model with certain types of
framesforactionrecognitionvia3Dconvolutions.Tranetal.[107] latentvariablestodesigncomplexgenerativemodelsofdata.7
3.2.4 Recurrentneuralnetwork(RNN) suchassupportvectormachineorrandomforest,totheextracted
RNNisaconnectionistmodelthatcapturestemporalinformation representations [133], [134]. Furthermore, [135], [136] showed
and is more suitable for sequential data prediction with arbitrary that the covariance descriptors computed on DCNN features
lengths.Inadditiontotrainingthedeepneuralnetworkinasingle and classification with Gaussian kernels on Symmetric Positive
feed-forward manner, RNNs include recurrent edges that span Definite (SPD) manifold are more efficient than the standard
adjacenttimestepsandsharethesameparametersacrossallsteps. classificationwiththesoftmaxlayer.
The classic back propagation through time (BPTT) [124] is used
to train the RNN. Long-short term memory (LSTM), introduced
by Hochreiter & Schmidhuber [125], is a special form of the 4 THE STATE OF THE ART
traditional RNN that is used to address the gradient vanishing Inthissection,wereviewtheexistingnoveldeepneuralnetworks
and exploding problems that are common in training RNNs. The designed for FER and the related training strategies proposed
cell state in LSTM is regulated and controlled by three gates: to address expression-specific problems. We divide the works
an input gate that allows or blocks alteration of the cell state by presented in the literature into two main groups depending on
the input signal, an output gate that enables or prevents the cell the type of data: deep FER networks for static images and deep
statetoaffectotherneurons,andaforgetgatethatmodulatesthe FER networks for dynamic image sequences. We then provide
cell’sself-recurrentconnectiontoaccumulateorforgetitsprevious an overview of the current deep FER systems with respect to
state.Bycombiningthesethreegates,LSTMcanmodellong-term the network architecture and performance. Because some of the
dependencies in a sequence and has been widely employed for evaluateddatasetsdonotprovideexplicitdatagroupsfortraining,
video-basedexpressionrecognitiontasks. validation and testing, and the relevant studies may conduct
experimentsunderdifferentexperimentalconditionswithdifferent
3.2.5 GenerativeAdversarialNetwork(GAN) data,wesummarizetheexpressionrecognitionperformancealong
GANwasfirstintroducedbyGoodfellowetal[84]in2014,which withinformationaboutthedataselectionandgroupingmethods.
trains models through a minimax two-player game between a
generatorG(z)thatgeneratessynthesizedinputdatabymapping
4.1 DeepFERnetworksforstaticimages
latents z to data space with z ∼ p(z) and a discriminator D(x)
that assigns probability y = Dis(x) ∈ [0,1] that x is an actual Alargevolumeoftheexistingstudiesconductedexpressionrecog-
training sample to tell apart real from fake input data. The gen- nition tasks based on static images without considering temporal
eratorandthediscriminatoraretrainedalternativelyandcanboth information due to the convenience of data processing and the
improve themselves by minimizing/maximizing the binary cross availability of the relevant training and test material. We first
entropyL GAN =log(D(x))+log(1−D(G(z)))withrespectto introducespecificpre-trainingandfine-tuningskillsforFER,then
D / G with x being a training sample and z ∼ p(z). Extensions review the novel deep neural networks in this field. For each of
of GAN exist, such as the cGAN [126] that adds a conditional themostfrequentlyevaluateddatasets,Table4showsthecurrent
information to control the output of the generator, the DCGAN state-of-the-artmethodsinthefieldthatareexplicitlyconductedin
[127] that adopts deconvolutional and convolutional neural net- aperson-independentprotocol(subjectsinthetrainingandtesting
works to implement G and D respectively, the VAE/GAN [128] setsareseparated).
thatuseslearnedfeaturerepresentationsintheGANdiscriminator
as basis for the VAE reconstruction objective, and the InfoGAN 4.1.1 Pre-trainingandfine-tuning
[129] that can learn disentangled representations in a completely
As mentioned before, direct training of deep networks on rela-
unsupervisedmanner.
tively small facial expression datasets is prone to overfitting. To
mitigatethisproblem,manystudiesusedadditionaltask-oriented
3.3 Facialexpressionclassification datatopre-traintheirself-builtnetworksfromscratchorfine-tuned
Afterlearningthedeepfeatures,thefinalstepofFERistoclassify onwell-knownpre-trainedmodels(e.g.,AlexNet[25],VGG[26],
thegivenfaceintooneofthebasicemotioncategories. VGG-face [148] and GoogleNet [27]). Kahou et al. [57], [149]
Unlike the traditional methods, where the feature extraction indicatedthattheuseofadditionaldatacanhelptoobtainmodels
step and the feature classification step are independent, deep withhighcapacitywithoutoverfitting,therebyenhancingtheFER
networks can perform FER in an end-to-end way. Specifically, performance.
a loss layer is added to the end of the network to regulate the To select appropriate auxiliary data, large-scale face recogni-
back-propagation error; then, the prediction probability of each tion (FR) datasets (e.g., CASIA WebFace [150], Celebrity Face
sample can be directly output by the network. In CNN, softmax in the Wild (CFW) [151], FaceScrub dataset [152]) or relatively
loss is the most common used function that minimizes the cross- large FER datasets (FER2013 [21] and TFD [37]) are suitable.
entropybetweentheestimatedclassprobabilitiesandtheground- Kaya et al. [153] suggested that VGG-Face which was trained
truthdistribution.Alternatively,[130]demonstratedthebenefitof forFRoverwhelmedImageNetwhichwasdevelopedforobjected
using a linear support vector machine (SVM) for the end-to-end recognition. Another interesting result observed by Knyazev et
trainingwhichminimizesamargin-basedlossinsteadofthecross- al. [154] is that pre-training on larger FR data positively affects
entropy.Likewise,[131]investigatedtheadaptationofdeepneural the emotion recognition accuracy, and further fine-tuning with
forests (NFs) [132] which replaced the softmax loss layer with additionalFERdatasetscanhelpimprovetheperformance.
NFsandachievedcompetitiveresultsforFER. Instead of directly using the pre-trained or fine-tuned models
Besides the end-to-end learning way, another alternative is to to extract features on the target dataset, a multistage fine-tuning
employthedeepneuralnetwork(particularlyaCNN)asafeature strategy [63] (see “Submission 3” in Fig. 3) can achieve better
extraction tool and then apply additional independent classifiers, performance: after the first-stage fine-tuning using FER2013 on8
TABLE4
Performancesummaryofrepresentativemethodsforstatic-baseddeepfacialexpressionrecognitiononthemostwidelyevaluateddatasets.
Networksize=depth&numberofparameters;Pre-processing=FaceDetection&DataAugmentation&FaceNormalization;IN=Illumination
Normalization;NE=NetworkEnsemble;CN =CascadedNetwork;MN =MultitaskNetwork;LOSO=leave-one-subject-out.
Datasets Method Network Network Pre-processing Dataselection Data Additional Performance1(%)
type size group classifier
Ouellet14[110] CNN(AlexNet) - - V&J - - SVM 7classes†:(94.4)
thelastframe LOSO
Lietal.15[86] RBM 4 - V&J - IN (cid:55) 6classes:96.8
Liuetal.14[13] DBN CN 6 2m (cid:51) - - 8folds AdaBoost 6classes:96.7
Liuetal.13[137] CNN,RBM CN 5 - V&J - - 10folds SVM 8classes:92.05(87.67)
Liuetal.15[138] CNN,RBM CN 5 - V&J - - thelastthreeframes 10folds SVM 7classes‡:93.70
CK+ K eh to ar l.ra 1m 5i [139] zero-biasCNN 4 7m (cid:51) (cid:51) - andthefirstframe 10folds (cid:55) 6classes:95.7;8classes:95.1
Dingetal.17 CNN fine-tune 8 11m IntraFace (cid:51) - 10folds (cid:55) 6classes:(98.6);8classes:(96.8)
[111]
Zengetal.18 DAE(DSAE) 3 - AAM - - thelastfourframes LOSO (cid:55) 7classes†:95.79(93.78)
[54] andthefirstframe 8classes:89.84(86.82)
Caietal.17[140] CNN losslayer 6 - DRMF (cid:51)IN 10folds (cid:55) 7classes†:94.39(90.66)
Mengetal.17 CNN MN 6 - DRMF (cid:51) - 8folds (cid:55) 7classes†:95.37(95.51)
[61]
Liuetal.17[77] CNN losslayer 11 - IntraFace (cid:51)IN thelastthreeframes 8folds (cid:55) 7classes†:97.1(96.1)
Yangetal.18 GAN(cGAN) - - MoT (cid:51) - 10folds (cid:55) 7classes†:97.30(96.57)
[141]
Zhangetal.18 CNN MN - - (cid:51) (cid:51) - 10folds (cid:55) 6classes:98.9
[47]
Liuetal.14[13] DBN CN 6 2m (cid:51) - - AdaBoost 7classes‡:91.8
LOSO
JAFFE
Hamester [142] CNN,CAE NE 3 - - - IN 213images (cid:55) 7classes‡:(95.8)
etal.15
Liuetal.13[137] CNN,RBM CN 5 - V&J - - themiddlethreeframes 10folds SVM 7classes‡:74.76(71.73)
andthefirstframe
Liuetal.15[138] CNN,RBM CN 5 - V&J - - 10folds SVM 7classes‡:75.85
Mollahosseini
MMI etal.16 CNN(Inception) 11 7.3m IntraFace (cid:51) - imagesfromeach 5folds (cid:55) 6classes:77.9
sequence
[14]
Liuetal.17[77] CNN losslayer 11 - IntraFace (cid:51)IN 10folds (cid:55) 6classes:78.53(73.50)
Lietal.17[44] CNN losslayer 8 5.8m IntraFace (cid:51) - 5folds SVM 6classes:78.46
themiddlethreeframes
Yangetal.18 GAN(cGAN) - - MoT (cid:51) - 10folds (cid:55) 6classes:73.23(72.67)
[141]
Reedetal.14 4,178emotionlabeled
RBM MN - - - - - SVM Test:85.43
[143] 3,874identitylabeled
TFD Devriesetal.14 CNN MN 4 12.0m MoT (cid:51)IN 5official (cid:55) Validation:87.80
[58] Test:85.13(48.29)
folds
Khorrami [139] zero-biasCNN 4 7m (cid:51) (cid:51) - (cid:55) Test:88.6
etal.15 4,178labeledimages
Dingetal.17 CNN fine-tune 8 11m IntraFace (cid:51) - (cid:55) Test:88.9(87.7)
[111]
Tang13[130] CNN losslayer 4 12.0m - (cid:51)IN (cid:55) Test:71.2
FER Devriesetal.14 CNN MN 4 12.0m MoT (cid:51)IN (cid:55) Validation+Test:67.21
[58]
2013
Zhangetal.15 CNN MN 6 21.3m SDM - - TrainingSet:28,709 (cid:55) Test:75.10
[144] ValidationSet:3,589
Guoetal.16 CNN losslayer 10 2.6m SDM (cid:51) - TestSet:3,589 k-NN Test:71.33
[145]
Kimetal.16 CNN NE 5 2.4m IntraFace (cid:51)IN (cid:55) Test:73.73
[146]
pramerdorfer
etal.16 CNN NE 10/16/331.8/1.2/5.3 - (cid:51)IN (cid:55) Test:75.2
(m)
[147]
levietal.15[78] CNN NE VGG-S/VGG-M/ MoT (cid:51) - 891training,431validation, (cid:55) Validation:51.75
GoogleNet and372test Test:54.56
Ngetal.15[63] CNN fine-tune AlexNet IntraFace (cid:51) - 921training,?validation, (cid:55) Validation:48.5(39.63)
and372test Test:55.6(42.69)
Lietal.17[44] CNN losslayer 8 5.8m IntraFace (cid:51) - 921training,427validation SVM Validation:51.05
Dingetal.17 CNN fine-tune 8 11m IntraFace (cid:51) - 891training,425validation (cid:55) Validation:55.15(46.6)
[111]
SFEW Liuetal.17[77] CNN losslayer 11 - IntraFace (cid:51)IN (cid:55) Validation:54.19(47.97)
2.0
Caietal.17[140] CNN losslayer 6 - DRMF (cid:51)IN (cid:55) Validation:52.52(43.41)
Test:59.41(48.29)
Mengetal.17 CNN MN 6 - DRMF (cid:51) - 958training, (cid:55) Validation:50.98(42.57)
[61] 436validation, Test:54.30(44.77)
Kimetal.15[76] CNN NE 5 - multiple (cid:51)IN and372test (cid:55) Validation:53.9
Test:61.6
Yuetal.15[75] CNN NE 8 6.2m multiple (cid:51)IN (cid:55) Validation:55.96(47.31)
Test:61.29(51.27)
1Thevalueinparenthesesisthemeanaccuracy,whichiscalculatedwiththeconfusionmatrixgivenbytheauthors.
†7Classes:Anger,Contempt,Disgust,Fear,Happiness,Sadness,andSurprise.
‡7Classes:Anger,Disgust,Fear,Happiness,Neutral,Sadness,andSurprise.9
Fig. 5. Image intensities (left) and LBP codes (middle). [78] proposed
mappingthesevaluestoa3Dmetricspace(right)astheinputofCNNs.
4.1.2 Diversenetworkinput
Fig.3.Flowchartofthedifferentfine-tuningcombinationsusedin[63].
Here, “FER28” and “FER32” indicate different parts of the FER2013
Traditional practices commonly use the whole aligned face of
datasets.“EmotiW”isthetargetdataset.Theproposedtwo-stagefine-
RGB images as the input of the network to learn features for
tuningstrategy(Submission3)exhibitedthebestperformance.
FER.However,theserawdatalackimportantinformation,suchas
homogeneousorregulartexturesandinvarianceintermsofimage
scaling,rotation,occlusionandillumination,whichmayrepresent
confounding factors for FER. Some methods have employed
diverse handcrafted features and their extensions as the network
inputtoalleviatethisproblem.
Low-levelrepresentationsencodefeaturesfromsmallregions
in the given RGB image, then cluster and pool these features
withlocalhistograms,whicharerobusttoilluminationvariations
and small registration errors. A novel mapped LBP feature [78]
(see Fig. 5) was proposed for illumination-invariant FER. Scale-
invariant feature transform (SIFT) [155]) features that are robust
against image scaling and rotation are employed [156] for multi-
view FER tasks. Combining different descriptors in outline, tex-
ture, angle, and color as the input data can also help enhance the
deepnetworkperformance[54],[157].
Part-based representations extract features according to the
targettask,whichremovenoncriticalpartsfromthewholeimage
andexploitkeypartsthataresensitivetothetask.[158]indicated
thatthreeregionsofinterest(ROI),i.e.,eyebrows,eyesandmouth,
are strongly related to facial expression changes, and cropped
these regions as the input of DSAE. Other researches proposed
Fig.4.Two-stagetrainingflowchartin[111].Instage(a),thedeeperface
to automatically learn the key parts for facial expression. For ex-
net is frozen and provides the feature-level regularization that pushes
theconvolutionalfeaturesoftheexpressionnettobeclosetotheface ample,[159]employedadeepmulti-layernetwork[160]todetect
net by using the proposed distribution function. Then, in stage (b), to thesaliencymapwhichputintensitiesonpartsdemandingvisual
furtherimprovethediscriminativenessofthelearnedfeatures,randomly attention.And[161]appliedtheneighbor-centerdifferencevector
initializedfullyconnectedlayersareaddedandjointlytrainedwiththe
(NCDV)[162]toobtainfeatureswithmoreintrinsicinformation.
wholeexpressionnetusingtheexpressionlabelinformation.
4.1.3 Auxiliaryblocks&layers
Based on the foundation architecture of CNN, several studies
pre-trainedmodels,asecond-stagefine-tuningbasedonthetrain-
have proposed the addition of well-designed auxiliary blocks or
ing part of the target dataset (EmotiW) is employed to refine the
layers to enhance the expression-related representation capability
modelstoadapttoamorespecificdataset(i.e.,thetargetdataset).
oflearnedfeatures.
Althoughpre-trainingandfine-tuningonexternalFRdatacan Based on the foundation architecture of CNN, several studies
indirectly avoid the problem of small training data, the networks have proposed the addition of well-designed auxiliary blocks or
are trained separately from the FER and the face-dominated layers to enhance the expression-related representation capability
information remains in the learned features which may weaken oflearnedfeatures.
the networks ability to represent expressions. To eliminate this A novel CNN architecture, HoloNet [90], was designed for
effect,atwo-stagetrainingalgorithmFaceNet2ExpNet[111]was FER, where CReLU [163] was combined with the powerful
proposed (see Fig. 4). The fine-tuned face net serves as a good residual structure [28] to increase the network depth without
initialization for the expression net and is used to guide the efficiency reduction and an inception-residual block [164], [165]
learningoftheconvolutionallayersonly.Andthefullyconnected was uniquely designed for FER to learn multi-scale features to
layers are trained from scratch with expression information to capture variations in expressions. Another CNN model, Super-
regularizethetrainingofthetargetFERnet. vised Scoring Ensemble (SSE) [91], was introduced to enhance10
TABLE5
Threeprimaryensemblemethodsonthedecisionlevel.
usedin
definition
(example)
determinetheclasswiththemost
Majority [76],[146],
votesusingthepredictedlabel
Voting [173]
yieldedfromeachindividual
(a) Threedifferentsupervisedblocksin[91].SS Blockforshallow-layer
determinetheclasswiththe
supervision,IS Blockforintermediate-layersupervision,andDS Blockfor
deep-layersupervision. highestmeanscoreusingthe
Simple [76],[146],
posteriorclassprobabilities
Average [173]
yieldedfromeachindividual
withthesameweight
determinetheclasswiththe
highestweightedmeanscore
Weighted [57],[78],
usingtheposteriorclass
Average [147],[153]
(b) Island loss layer in [140]. The island loss calculated at the feature probabilitiesyieldedfromeach
extraction layer and the softmax loss calculated at the decision layer are individualwithdifferentweights
combinedtosupervisetheCNNtraining.
formalized to pull the locally neighboring features of the same
classtogethersothattheintra-classlocalclustersofeachclassare
compact. Besides, based on the triplet loss [169], which requires
onepositiveexampletobeclosertotheanchorthanonenegative
examplewithafixedgap,twovariationswereproposedtoreplace
(c) (N+M)-tuple clusters loss layer in [77]. During training, the identity-
or assist the supervision of the softmax loss: (1) exponential
awarehard-negativeminingandonlinepositiveminingschemesareusedto
decreasetheinter-identityvariationinthesameexpressionclass. triplet-based loss [145] was formalized to give difficult samples
more weight when updating the network, and (2) (N+M)-tuples
Fig. 6. Representative functional layers or blocks that are specifically clusterloss[77]wasformalizedtoalleviatethedifficultyofanchor
designedfordeepfacialexpressionrecognition. selection and threshold validation in the triplet loss for identity-
invariant FER (see Fig. 6(c) for details). Besides, a feature loss
[170]wasproposedtoprovidecomplementaryinformationforthe
the supervision degree for FER, where three types of super- deepfeatureduringearlytrainingstage.
vised blocks were embedded in the early hidden layers of the
mainstreamCNNforshallow,intermediateanddeepsupervision, 4.1.4 Networkensemble
respectively(seeFig.6(a)).
Previousresearchsuggestedthatassembliesofmultiplenetworks
And a feature selection network (FSN) [166] was designed can outperform an individual network [171]. Two key factors
byembeddingafeatureselectionmechanisminsidetheAlexNet, shouldbeconsideredwhenimplementingnetworkensembles:(1)
which automatically filters irrelevant features and emphasizes sufficientdiversityofthenetworkstoensurecomplementarity,and
correlatedfeaturesaccordingtolearnedfeaturemapsoffacialex- (2)anappropriateensemblemethodthatcaneffectivelyaggregate
pression.Interestingly,Zengetal.[167]pointedoutthattheincon- thecommitteenetworks.
sistent annotations among different FER databases are inevitable Intermsofthefirstfactor,differentkindsoftrainingdataand
which would damage the performance when the training set is variousnetworkparametersorarchitecturesareconsideredtogen-
enlarged by merging multiple datasets. To address this problem, erate diverse committees. Several pre-processing methods [146],
theauthorsproposedanInconsistentPseudoAnnotationstoLatent suchasdeformationandnormalization,andmethodsdescribedin
Truth (IPA2LT) framework. In IPA2LT, an end-to-end trainable Section4.1.2cangeneratedifferentdatatotraindiversenetworks.
LTNet is designed to discover the latent truths from the human By changing the size of filters, the number of neurons and the
annotations and the machine annotations trained from different number of layers of the networks, and applying multiple random
datasets by maximizing the log-likelihood of these inconsistent seeds for weight initialization, the diversity of the networks can
annotations. also be enhanced [76], [172]. Besides, different architectures of
The traditional softmax loss layer in CNNs simply forces networks can be used to enhance the diversity. For example, a
features of different classes to remain apart, but FER in real- CNNtrainedinasupervisedwayandaconvolutionalautoencoder
world scenarios suffers from not only high inter-class similarity (CAE)trainedinanunsupervisedwaywerecombinedfornetwork
but also high intra-class variation. Therefore, several works have ensemble[142].
proposed novel loss layers for FER. Inspired by the center loss Forthesecondfactor,eachmemberofthecommitteenetworks
[168], which penalizes the distance between deep features and canbeassembledattwodifferentlevels:thefeaturelevelandthe
theircorrespondingclasscenters,twovariationswereproposedto decision level. For feature-level ensembles, the most commonly
assistthesupervisionofthesoftmaxlossformorediscriminative adopted strategy is to concatenate features learned from different
features for FER: (1) island loss [140] was formalized to further networks [88], [174]. For example, [88] concatenated features
increasethepairwisedistancesbetweendifferentclasscenters(see learned from different networks to obtain a single feature vector
Fig. 6(b)), and (2) locality-preserving loss (LP loss) [44] was to describe the input image (see Fig. 7(a)). For decision-level11
(a) Feature-levelensemblein[88].Threedifferentfeatures(fc5ofVGG13
+fc7ofVGG16+poolofResnet)afternormalizationareconcatenatedto Fig. 8. Representative multitask network for FER. In the proposed
createasinglefeaturevector(FV)thatdescribestheinputframe. MSCNN[68],apairofimagesissentintotheMSCNNduringtraining.
The expression recognition task with cross-entropy loss, which learns
features with large between-expression variation, and the face verifi-
cationtaskwithcontrastiveloss,whichreducesthevariationinwithin-
expressionfeatures,arecombinedtotraintheMSCNN.
(b)Decision-levelensemblein[76].A3-levelhierarchicalcommitteearchi-
tecturewithhybriddecision-levelfusionswasproposedtoobtainsufficient
decisiondiversity.
Fig. 7. Representative network ensemble systems at the feature level
anddecisionlevel.
Fig. 9. Representative cascaded network for FER. The proposed AU-
aware deep network (AUDN) [137] is composed of three sequential
ensembles, three widely-used rules are applied: majority voting, modules: in the first module, a 2-layer CNN is trained to generate an
over-completerepresentationencodingallexpression-specificappear-
simple average and weighted average. A summary of these three
ance variations over all possible locations; in the second module, an
methodsisprovidedinTable5.Becausetheweightedaveragerule AU-aware receptive field layer is designed to search subsets of the
considerstheimportanceandconfidenceofeachindividual,many over-complete representation; in the last module, a multilayer RBM is
weightedaveragemethodshavebeenproposedtofindanoptimal exploitedtolearnhierarchicalfeatures.
set of weights for network ensemble. [57] proposed a random
search method to weight the model predictions for each emotion
sensitive contrastive loss to learn identity-related features for
type.[75]usedthelog-likelihoodlossandhingelosstoadaptively
identity-invariant FER. In [68], a multisignal CNN (MSCNN),
assign different weights to each network. [76] proposed an ex-
which was trained under the supervision of both FER and face
ponentiallyweightedaveragebasedonthevalidationaccuracyto
verificationtasks,wasproposedtoforcethemodeltofocusonex-
emphasizequalifiedindividuals(seeFig.7(b)).[172]usedaCNN
pressioninformation(seeFig.8).Furthermore,anall-in-oneCNN
tolearnweightsforeachindividualmodel.
model [177] was proposed to simultaneously solve a diverse set
offaceanalysistasksincludingsmiledetection.Thenetworkwas
4.1.5 Multitasknetworks
first initialized using the weight pre-trained on face recognition,
Many existing networks for FER focus on a single task and
thentask-specificsub-networkswerebranchedoutfromdifferent
learnfeaturesthataresensitivetoexpressionswithoutconsidering
layers with domain-based regularization by training on multiple
interactionsamongotherlatentfactors.However,intherealworld,
datasets. Specifically, as smile detection is a subject-independent
FER is intertwined with various factors, such as head pose,
taskthatreliesmoreonlocalinformationavailablefromthelower
illumination, and subject identity (facial morphology). To solve
layers,theauthorsproposedtofusethelowerconvolutionallayers
thisproblem,multitaskleaningisintroducedtotransferknowledge
toformagenericrepresentationforsmiledetection.Conventional
fromotherrelevanttasksandtodisentanglenuisancefactors.
supervisedmultitasklearningrequirestrainingsampleslabeledfor
Reed et al. [143] constructed a higher-order Boltzmann ma- alltasks.Torelaxthis,[47]proposedanovelattributepropagation
chine (disBM) to learn manifold coordinates for the relevant methodwhichcanleveragetheinherentcorrespondencesbetween
factors of expressions and proposed training strategies for dis- facial expression and other heterogeneous attributes despite the
entanglingsothattheexpression-relatedhiddenunitsareinvariant disparatedistributionsofdifferentdatasets.
to face morphology. Other works [58], [175] suggested that
simultaneously conducted FER with other tasks, such as facial
4.1.6 Cascadednetworks
landmark localization and facial AUs [176] detection, can jointly
improveFERperformance. In a cascaded network, various modules for different tasks are
Besides, several works [61], [68] employed multitask learn- combined sequentially to construct a deeper network, where the
ing for identity-invariant FER. In [61], an identity-aware CNN outputs of the former modules are utilized by the latter modules.
(IACNN)withtwoidenticalsub-CNNswasproposed.Onestream Relatedstudieshaveproposedcombinationsofdifferentstructures
used expression-sensitive contrastive loss to learn expression- tolearnahierarchyoffeaturesthroughwhichfactorsofvariation
discriminative features, and the other stream used identity- thatareunrelatedwithexpressionscanbegraduallyfilteredout.12
Most commonly, different networks or learning methods are focuses (computation efficiency, performance and difficulty of
combined sequentially and individually, and each of them con- networktraining).
tributesdifferentlyandhierarchically.In[178],DBNsweretrained Pre-training and fine-tuning have become mainstream in
to first detect faces and to detect expression-related areas. Then, deep FER to solve the problem of insufficient training data and
theseparsedfacecomponentswereclassifiedbyastackedautoen- overfitting. A practical technique that proved to be particularly
coder. In [179], a multiscale contractive convolutional network useful is pre-training and fine-tuning the network in multiple
(CCNET)wasproposedtoobtainlocal-translation-invariant(LTI) stages using auxiliary data from large-scale objection or face
representations.Then,contractiveautoencoderwasdesignedtohi- recognitiondatasetstosmall-scaleFERdatasets,i.e.,fromlargeto
erarchically separate outthe emotion-related factors fromsubject smallandfromgeneraltospecific.However,whencomparedwith
identity and pose. In [137], [138], over-complete representations the end-to-end training framework, the representational structure
werefirstlearnedusingCNNarchitecture,thenamultilayerRBM that are unrelated to expressions are still remained in the off-the-
was exploited to learn higher-level features for FER (see Fig. shelf pre-trained model, such as the large domain gap with the
9). Instead of simply concatenating different networks, Liu et al. objectionnet[153]andthesubjectidentificationdistractioninthe
[13]presentedaboostedDBN(BDBN)thatiterativelyperformed face net [111]. Thus the extracted features are usually vulnerable
featurerepresentation,featureselectionandclassifierconstruction to identity variations and the performance would be degraded.
inaunifiedloopystate.Comparedwiththeconcatenationwithout Noticeably,withtheadventoflarge-scalein-the-wildFERdatasets
feedback, this loopy framework propagates backward the classi- (e.g., AffectNet and RAF-DB), the end-to-end training using
fication error to initiate the feature selection process alternately deep networks with moderate size can also achieve competitive
untilconvergence.Thus,thediscriminativeabilityforFERcanbe performances[45],[167].
substantiallyimprovedduringthisiteration. In addition to directly using the raw image data to train the
deep network,diverse pre-designed featuresare recommendedto
4.1.7 Generativeadversarialnetworks(GANs) strengthenthenetwork’srobustnesstocommondistractions(e.g.,
illumination, head pose and occlusion) and to force the network
Recently, GAN-based methods have been successfully used in
to focus more on facial areas with expressive information. More-
imagesynthesistogenerateimpressivelyrealisticfaces,numbers,
over, the use of multiple heterogeneous input data can indirectly
and a variety of other image types, which are beneficial to train-
enlarge the data size. However, the problem of identity bias is
ing data augmentation and the corresponding recognition tasks.
commonlyignoredinthismethods.Moreover,generatingdiverse
SeveralworkshaveproposednovelGAN-basedmodelsforpose-
dataaccountsforadditionaltimeconsumingandcombiningthese
invariantFERandidentity-invariantFER.
multipledatacanleadtohighdimensionwhichmayinfluencethe
For pose-invariant FER, Lai et al. [180] proposed a GAN-
computationalefficiencyofthenetwork.
basedfacefrontalizationframework,wherethegeneratorfrontal-
Training a deep and wide network with a large number of
izesinputfaceimageswhilepreservingtheidentityandexpression
hidden layers and flexible filters is an effective way to learn
characteristicsandthediscriminatordistinguishestherealimages
deephigh-levelfeaturesthatarediscriminativeforthetargettask.
from the generated frontal face images. And Zhang et al. [181]
However,thisprocessisvulnerabletothesizeoftrainingdataand
proposed a GAN-based model that can generate images with
canunderperformifinsufficienttrainingdataisavailabletolearn
different expressions under arbitrary poses for multi-view FER.
thenewparameters.Integratingmultiplerelativelysmallnetworks
Foridentity-invariantFER,Yangetal.[182]proposedanIdentity-
inparallelorinseriesisanaturalresearchdirectiontoovercome
Adaptive Generation (IA-gen) model with two parts. The upper
this problem. Network ensemble integrates diverse networks at
part generates images of the same subject with different expres-
the feature or decision level to combine their advantages, which
sions using cGANs, respectively. Then, the lower part conducts
is usually applied in emotion competitions to help boost the
FER for each single identity sub-space without involving other
performance. However, designing different kinds of networks
individuals,thusidentityvariationscanbewellalleviated.Chenet
to compensate each other obviously enlarge the computational
al. [183] proposed a Privacy-Preserving Representation-Learning
cost and the storage requirement. Moreover, the weight of each
Variational GAN (PPRL-VGAN) that combines VAE and GAN
sub-network is usually learned according to the performance on
to learn an identity-invariant representation that is explicitly
original training data, leading to overfitting on newly unseen
disentangled from the identity information and generative for
testing data. Multitask networks jointly train multiple networks
expression-preserving face image synthesis. Yang et al. [141]
withconsiderationofinteractionsbetweenthetargetFERtaskand
proposedaDe-expressionResidueLearning(DeRL)procedureto
othersecondarytasks,suchasfaciallandmarklocalization,facial
exploretheexpressiveinformation,whichisfilteredoutduringthe
AUrecognitionandfaceverification,thustheexpression-unrelated
de-expression process but still embedded in the generator. Then
factors including identity bias can be well disentangled. The
themodelextractedthisinformationfromthegeneratordirectlyto
downside of this method is that it requires labeled data from all
mitigate the influence of subject variations and improve the FER
tasksandthetrainingbecomesincreasinglycumbersomeasmore
performance.
tasks are involved. Alternatively, cascaded networks sequentially
train multiple networks in a hierarchical approach, in which case
4.1.8 Discussion
thediscriminativeabilityofthelearnedfeaturesarecontinuously
Theexistingwell-constructeddeepFERsystemsfocusontwokey strengthened. In general, this method can alleviate the overfitting
issues: the lack of plentiful diverse training data and expression- problem, and in the meanwhile, progressively disentangling fac-
unrelatedvariations,suchasillumination,headposeandidentity. tors that are irrelevant to facial expression. A deficiency worths
Table 6 shows relative advantages and disadvantages of these considering is that the sub-networks in most existing cascaded
different types of methods with respect to two open issues (data systems are training individually without feedback, and the end-
size requirement and expression-unrelated variations) and other to-end training strategy is preferable to enhance the training13
TABLE6
Comparisonofdifferenttypesofmethodsforstaticimagesintermsofdatasizerequirement,variations*(headpose,illumination,occlusionand
otherenvironmentfactors),identitybias,computationalefficiency,accuracy,anddifficultyonnetworktraining.
Networktype data variations* identitybias efficiency accuracy difficulty
Pre-train&Fine-tune low fair vulnerable high fair easy
Diverseinput low good vulnerable low fair easy
Auxiliarylayers varies good varies varies good varies
Networkensemble low good fair low good medium
Multitasknetwork high varies good fair varies hard
Cascadednetwork fair good fair fair fair medium
GAN fair good good fair good hard
effectivenessandtheperformance[13].
Ideally, deep networks, especially CNNs, have good capa-
bilities for dealing with head-pose variations, yet most current
FER networks do not address head-pose variations explicitly and
are not tested in naturalistic scenarios. Generative adversarial
networks(GANs)canbeexploitedtosolvethisissuebyfrontaliz-
ingfaceimageswhilepreservingexpressioncharacteristics[180]
or synthesizing arbitrary poses to help train the pose-invariant (a) Frameaveraging (b) Frameexpansion
network [181]. Another advantage of GANs is that the identity
variations can be explicitly disentangled through generating the Fig.10.Frameaggregationin[57].Theflowchartistop-down.(a)For
sequences with more than 10 frames, we averaged the probability
corresponding neutral face image [141] or synthesizing different
vectorsof10independentgroupsofframestakenuniformlyalongtime.
expressionswhilepreservingtheidentityinformationforidentity- (b)Forsequenceswithlessthan10frames,weexpandedbyrepeating
invariant FER [182]. Moreover, GANs can help augment the framesuniformlytoobtain10totalframes.
training data on both size and diversity. The main drawback of
GAN is the training instability and the trade-off between visual
Two aggregation approaches have been considered to generate a
qualityandimagediversity.
fixed-length feature vector for each sequence [57], [191]: frame
averaging and frame expansion (see Fig. 10 for details). An
4.2 DeepFERnetworksfordynamicimagesequences alternative approach which dose not require a fixed number of
Although most of the previous models focus on static images, frames is applying statistical coding. The average, max, average
facial expression recognition can benefit from the temporal cor- ofsquare,averageofmaximumsuppressionvectorsandsooncan
relations of consecutive frames in a sequence. We first intro- beusedtosummarizetheper-frameprobabilitiesineachsequence.
duce the existing frame aggregation techniques that strategically For feature-level frame aggregation, the learned features of
combine deep features learned from static-based FER networks. frames in the sequence are aggregate. Many statistical-based
Then, considering that in a videostream people usually display encoding modules can be applied in this scheme. A simple and
the same expression with different intensities, we further review effective way is to concatenate the mean, variance, minimum,
methodsthatuseimagesindifferentexpressionintensitystatesfor and maximum of the features over all frames [88]. Alternatively,
intensity-invariantFER.Finally,weintroducedeepFERnetworks matrix-based models such as eigenvector, covariance matrix and
thatconsiderspatio-temporalmotionpatternsinvideoframesand multi-dimensional Gaussian distribution can also be employed
learned features derived from the temporal structure. For each of foraggregation[186],[192].Besides,multi-instancelearninghas
themostfrequentlyevaluateddatasets,Table7showsthecurrent been explored for video-level representation [193], where the
state-of-the-artmethodsconductedintheperson-independentpro- cluster centers are computed from auxiliary image data and then
tocol. bag-of-words representation is obtained for each bag of video
frames.
4.2.1 Frameaggregation
Because the frames in a given video clip may vary in expres- 4.2.2 ExpressionIntensitynetwork
sion intensity, directly measuring per-frame error does not yield Most methods (introduced in Section 4.1) focus on recognizing
satisfactory performance. Various methods have been proposed the peak high-intensity expression and ignore the subtle lower-
to aggregate the network output for frames in each sequence intensity expressions. In this section, we introduced expression
to improve the performance. We divide these methods into two intensity-invariantnetworksthattaketrainingsampleswithdiffer-
groups: decision-level frame aggregation and feature-level frame ent intensities as input to exploit the intrinsic correlations among
aggregation. expressionsfromasequencethatvaryinintensity.
Fordecision-levelframeaggregation,n-classprobabilityvec- In expression intensity-invariant network, image frames with
tors of each frame in a sequence are integrated. The most con- intensity labels are used for training. During test, data that vary
venientwayistodirectlyconcatenatetheoutputoftheseframes. in expression intensity are used to verify the intensity-invariant
However,thenumberofframesineachsequencemaybedifferent. ability of the network. Zhao et al. [17] proposed a peak-piloted14
TABLE7
Performancesofrepresentativemethodsfordynamic-baseddeepfacialexpressionrecognitiononthemostwidelyevaluateddatasets.Network
size=depth&numberofparameters;Pre-processing=FaceDetection&DataAugmentation&FaceNormalization;IN=Illumination
Normalization;FA=FrameAggregation;EIN =ExpressionIntensity-invariantNetwork;FLT =FacialLandmarkTrajectory;CN =Cascaded
Network;NE=NetworkEnsemble;S=SpatialNetwork;T=TemporalNetwork;LOSO=leave-one-subject-out.
Datasets Methods Network Network Pre-processing TrainingdataSelectionTestingdataselection Data Performance1(%)
type size ineachsequence ineachsequence group
Zhaoetal.16[17] EIN 22 6.8m (cid:51) - - fromthe7thtothelast2 thelastframe 10folds 6classes:99.3
Yuetal.17[70] EIN 42 - MTCNN (cid:51) - fromthe7thtothelast2 thepeakexpression 10folds 6classes:99.6
kimetal.17[184] EIN 14 - (cid:51) (cid:51) - allframes 10folds 7classes:97.93
Sunetal.17[185] NE 3*GoogLeNetv2 (cid:51) - - S:emotional 10folds 6classes:97.28
T:neutral+emotional
CK+
Jungetal.15[16] FLT 2 177.6k IntraFace (cid:51) - fixednumberofframes 10folds 7classes:92.35
Jungetal.15[16] C3D 4 - IntraFace (cid:51) - fixednumberofframes thesameas 10folds 7classes:91.44
Jungetal.15[16] NE FLT/C3D IntraFace (cid:51) - fixednumberofframes thetrainingdata 10folds 7classes:97.25(95.22)
kuoetal.18[89] FA 6 2.7m IntraFace (cid:51)IN fixedlength9 10folds 7classes:98.47
Zhangetal.17[68] NE 7/5 2k/1.6m SDM/ (cid:51) - S:thelastframe 10folds 7classes:98.50(97.78)
CascadedCNN T:allframes
Kimetal.17[66] EIN,CN 7 1.5m Incremental (cid:51) - 5intensitiesframes LOSO 6classes:78.61(78.00)
kimetal.17[184] EIN 14 - (cid:51) (cid:51) - allframes 10folds 6classes:81.53
Hasanietal.17[112] FLT,CN 22 - 3000fps - - tenframes 5folds 6classes:77.50(74.50)
thesameas
Hasanietal.17[55] CN 29 - AAM - - staticframes 5folds 6classes:78.68
MMI thetrainingdata
Zhangetal.17[68] NE 7/5 2k/1.6m SDM/ (cid:51) - S:themiddleframe 10folds 6classes:81.18(79.30)
CascadedCNN T:allframes
Sunetal.17[185] NE 3*GoogLeNetv2 (cid:51) - - S:emotional 10folds 6classes:91.46
T:neutral+emotional
Zhaoetal.16[17] EIN 22 6.8m (cid:51) - - fromthe7thtothelast2 thelastframe 10folds 6classes:84.59
Yuetal.17[70] EIN 42 - MTCNN (cid:51) - fromthe7thtothelast2 thepeakexpression 10folds 6classes:86.23
Oulu- Jungetal.15[16] FLT 2 177.6k IntraFace (cid:51) - fixednumberofframes 10folds 6classes:74.17
CASIA Jungetal.15[16] C3D 4 - IntraFace (cid:51) - fixednumberofframes 10folds 6classes:74.38
Jungetal.15[16] NE FLT/C3D IntraFace (cid:51) - fixednumberofframes thesameas 10folds 6classes:81.46(81.49)
Zhangetal.17[68] NE 7/5 2k/1.6m SDM/ (cid:51) - S:thelastframe thetrainingdata 10folds 6classes:86.25(86.25)
CascadedCNN T:allframes
kuoetal.18[89] NE 6 2.7m IntraFace (cid:51)IN fixedlength9 10folds 6classes:91.67
Dingetal.16[186] FA AlexNet (cid:51) - - Training:773;Validation:373;Test:593 Validation:44.47
AFEW* Yanetal.16[187] CN VGG16-LSTM (cid:51) (cid:51) - 40frames 3folds 7classes:44.46
Yanetal.16[187] FLT 4 - [188] - - 30frames 3folds 7classes:37.37
6.0 Fanetal.16[108] CN VGG16-LSTM (cid:51) - - 16featuresforLSTM Validation:45.43(38.96)
Fanetal.[108] C3D 10 - (cid:51) - - severalwindowsof16consecutiveframes Validation:39.69(38.55)
Yanetal.16[187] fusion / Training:773;Validation:383;Test:593 Test:56.66(40.81)
Fanetal.16[108] fusion / Training:774;Validation:383;Test:593 Test:59.02(44.94)
Ouyangetal.17[189] CN VGG-LSTM MTCNN (cid:51) - 16frames Validation:47.4
AFEW* O Vu iy ea ln zeg ue ft ea tl. al1 .7 [1[ 91 08 ]9] C C3 ND 10 C3D-LST-
M
MT (cid:51)CNN (cid:51)
(cid:51)
-
-
detect1 e6 df fr aa cm ee fs
rames
V Va al li id da at ti io on n: :3 45 3. .2
2
7.0 Vielzeufetal.[190] CN VGG16-LSTM (cid:51) (cid:51) - severalwindowsof16consecutiveframes Validation:48.6
Vielzeufetal.[190] fusion / Training:773;Validation:383;Test:653 Test:58.81(43.23)
1Thevalueinparenthesesisthemeanaccuracycalculatedfromtheconfusionmatrixgivenbyauthors.
2Apairofimages(peakandnon-peakexpression)ischosenfortrainingeachtime.
*Wehaveincludedtheresultofasinglespatio-temporalnetworkandalsothebestresultafterfusionwithbothvideoandaudiomodalities.
†7ClassesinCK+:Anger,Contempt,Disgust,Fear,Happiness,Sadness,andSurprise.
‡7ClassesinAFEW:Anger,Disgust,Fear,Happiness,Neutral,Sadness,andSurprise.
deep network (PPDN) that takes a pair of peak and non-peak andencodingintermediateintensity,respectively.
images of the same expression and from the same subject as
input and utilizes the L2-norm loss to minimize the distance
between both images. During back propagation, a peak gradient Considering that images with different expression intensities
suppression (PGS) was proposed to drive the learned feature of for an individual identity is not always available in the wild,
the non-peak expression towards that of peak expression while severalworksproposedtoautomaticallyacquiretheintensitylabel
avoiding the inverse. Thus, the network discriminant ability on ortogeneratenewimageswithtargetedintensity.Forexample,in
lower-intensity expressions can be improved. Based on PPDN, [194] the peak and neutral frames was automatically picked out
Yu et al. [70] proposed a deeper cascaded peak-piloted network fromthesequencewithtwostages:aclusteringstagetodivideall
(DCPN)thatusedadeeperandlargerarchitecturetoenhancethe frames into the peak-like group and the neutral-like group using
discriminativeabilityofthelearnedfeaturesandemployedanin- K-means algorithm, and a classification stage to detect peak and
tegrationtrainingmethodcalledcascadefine-tuningtoavoidover- neutral frames using a semi-supervised SVM. And in [184], a
fitting.In[66],moreintensitystateswereutilized(onset,onsetto deep generative-contrastive model was presented with two steps:
apextransition,apex,apextooffsettransitionandoffset)andfive a generator to generate the reference (less-expressive) face for
loss functions were adopted to regulate the network training by each sample via convolutional encoder-decoder and a contrastive
minimizing expression classification error, intra-class expression network to jointly filter out information that is irrelevant with
variation,intensityclassificationerrorandintra-intensityvariation, expressions through a contrastive metric loss and a supervised
reconstructionloss.15
Fig.12.Theproposed3DCNN-DAP[199].Theinputn-framesequence
isconvolvedwith3Dfilters;then,13∗c∗kpartfilterscorrespondingto
13manuallydefinedfacialpartsareusedtoconvolvekfeaturemapsfor
thefacialactionpartdetectionmapsofcexpressionclasses.
Fig.11.TheproposedPPDNin[17].Duringtraining,PPDNistrainedby sequence and weighted based on their prediction scores. Instead
jointlyoptimizingtheL2-normlossandthecross-entropylossesoftwo ofdirectlyusingC3Dforclassification,[109]employedC3Dfor
expressionimages.Duringtesting,thePPDNtakesonestillimageas
spatio-temporal feature extraction and then cascaded with DBN
inputforprobabilityprediction.
forprediction.In[201],C3Dwasalsousedasafeatureextractor,
followed by a NetVLAD layer [202] to aggregate the temporal
informationofthemotionfeaturesbylearningclustercenters.
4.2.3 Deepspatio-temporalFERnetwork
Although the frame aggregation can integrate frames in the Facial landmark trajectory: Related psychological studies
videosequence,thecrucialtemporaldependencyisnotexplicitly have shown that expressions are invoked by dynamic motions
exploited. By contrast, the spatio-temporal FER network takes of certain facial parts (e.g., eyes, nose and mouth) that contain
a range of frames in a temporal window as a single input the most descriptive information for representing expressions.
without prior knowledge of the expression intensity and utilizes To obtain more accurate facial actions for FER, facial landmark
both textural and temporal information to encode more subtle trajectory models have been proposed to capture the dynamic
expressions. variationsoffacialcomponentsfromconsecutiveframes.
To extract landmark trajectory representation, the most direct
RNN and C3D: RNN can robustly derive information from way is to concatenate coordinates of facial landmark points
sequencesbyexploitingthefactthatfeaturevectorsforsuccessive from frames over time with normalization to generate a one-
dataareconnectedsemanticallyandarethereforeinterdependent. dimensional trajectory signal for each sequence [16] or to form
Theimprovedversion,LSTM,isflexibletohandlevarying-length an image-like map as the input of CNN [187]. Besides, relative
sequentialdatawithlowercomputationcost.DerivedfromRNN, distance variation of each landmark in consecutive frames can
an RNN that is composed of ReLUs and initialized with the also be used to capture the temporal information [203]. Further,
identity matrix (IRNN) [195] was used to provide a simpler part-based model that divides facial landmarks into several parts
mechanism for addressing the vanishing and exploding gradient according to facial physical structure and then separately feeds
problems [87]. And bidirectional RNNs (BRNNs) [196] were themintothenetworkshierarchicallyisprovedtobeefficientfor
employed to learn the temporal relations in both the original and both local low-level and global high-level feature encoding [68]
reversed directions [68], [187]. Recently, a Nested LSTM was (see “PHRNN” in Fig. 13) . Instead of separately extracting the
proposedin[71]withtwosub-LSTMs.Namely,T-LSTMmodels trajectory features and then input them into the networks, Hasani
the temporal dynamics of the learned features, and C-LSTM et al. [112] incorporated the trajectory features by replacing the
integrates the outputs of all T-LSTMs together so as to encode shortcut in the residual unit of the original 3D Inception-ResNet
the multi-level features encoded in the intermediate layers of the withelement-wisemultiplicationoffaciallandmarksandtheinput
network. tensoroftheresidualunit.Thus,thelandmarkbasednetworkcan
Compared with RNN, CNN is more suitable for computer betrainedend-to-end.
vision applications; hence, its derivative C3D [107], which uses
3D convolutional kernels with shared weights along the time Cascaded networks: By combining the powerful perceptual
axis instead of the traditional 2D kernels, has been widely used vision representations learned from CNNs with the strength of
for dynamic-based FER (e.g., [83], [108], [189], [197], [198]) LSTM for variable-length inputs and outputs, Donahue et al.
to capture the spatio-temporal features. Based on C3D, many [204]proposedabothspatiallyandtemporallydeepmodelwhich
derivedstructureshavebeendesignedforFER.In[199],3DCNN cascades the outputs of CNNs with LSTMs for various vision
was incorporated with the DPM-inspired [200] deformable facial tasks involving time-varying inputs and outputs. Similar to this
action constraints to simultaneously encode dynamic motion hybridnetwork,manycascadednetworkshavebeenproposedfor
and discriminative part-based representations (see Fig. 12 for FER(e.g.,[66],[108],[190],[205]).
details). In [16], a deep temporal appearance network (DTAN) Instead of CNN, [206] employed a convolutional sparse
was proposed that employed 3D filters without weight sharing autoencoder for sparse and shift-invariant features; then, an
along the time axis; hence, each filter can vary in importance LSTM classifier was trained for temporal evolution. [189]
overtime.Likewise,aweightedC3Dwasproposed[190],where employed a more flexible network called ResNet-LSTM, which
several windows of consecutive frames were extracted from each allows nodes in lower CNN layers to directly contact with16
trajectory”) and the integrated network (see Fig. 14 for details),
whichoutperformedtheweighedsumstrategy.
4.2.4 Discussion
In the real world, people display facial expressions in a dynamic
process,e.g.,fromsubtletoobvious,andithasbecomeatrendto
conductFERonsequence/videodata.Table8summarizesrelative
merits of different types of methods on dynamic data in regards
tothecapabilityofrepresentingspatialandtemporalinformation,
therequirementontrainingdatasizeandframelength(variableor
Fig. 13. The spatio-temporal network proposed in [68]. The tempo- fixed),thecomputationalefficiencyandtheperformance.
ral network PHRNN for landmark trajectory and the spatial network Frameaggregationisemployedtocombinethelearnedfeature
MSCNNforidentity-invariantfeaturesaretrainedseparately.Then,the
or prediction probability of each frame for a sequence-level
predicted probabilities from the two networks are fused together for
spatio-temporalFER. result. The output of each frame can be simply concatenated
(fixed-length frames is required in each sequence) or statistically
aggregated to obtain video-level representation (variable-length
frames processible). This method is computationally simple and
can achieve moderate performance if the temporal variations of
thetargetdatasetisnotcomplicated.
According to the fact that the expression intensity in a video
sequence varies over time, the expression intensity-invariant net-
work considers images with non-peak expressions and further
exploits the dynamic correlations between peak and non-peak
expressions to improve performance. Commonly, image frames
with specific intensity states are needed for intensity-invariant
Fig. 14. The joint fine-tuning method for DTAGN proposed in [16]. To FER.
integrate DTGA and DTAN, we freeze the weight values in the gray Despite the advantages of these methods, frame aggregation
boxes and retrain the top layer in the green boxes. The logit values
handles frames without consideration of temporal information
of the green boxes are used by Softmax3 to supervise the integrated
network.Duringtraining,wecombinethreesoftmaxlossfunctions,and andsubtleappearancechanges,andexpressionintensity-invariant
forprediction,weuseonlySoftmax3. networks require prior knowledge of expression intensity which
is unavailable in real-world scenarios. By contrast, Deep spatio-
temporalnetworksaredesignedtoencodetemporaldependencies
LSTMs to capture spatio-temporal information. In addition in consecutive frames and have been shown to benefit from
to concatenating LSTM with the fully connected layer of learning spatial features in conjunction with temporal features.
CNN, a hypercolumn-based system [207] extracted the last RNN and its variations (e.g., LSTM, IRNN and BRNN) and C3D
convolutional layer features as the input of the LSTM for longer are foundational networks for learning spatio-temporal features.
range dependencies without losing global coherence. Instead However, the performance of these networks is barely satisfac-
of LSTM, the conditional random fields (CRFs) model [208] tory. RNN is incapable of capturing the powerful convolutional
that are effective in recognizing human activities was employed features. And 3D filers in C3D are applied over very short
in[55]todistinguishthetemporalrelationsoftheinputsequences. video clips ignoring long-range dynamics. Also, training such
a huge network is computationally a problem, especially for
Networkensemble: Atwo-streamCNNforactionrecognitionin dynamic FER where video data is insufficient. Alternatively,
videos, which trained one stream of the CNN on the multi-frame facial landmark trajectory methods extract shape features based
dense optical flow for temporal information and the other stream on the physical structures of facial morphological variations to
oftheCNNonstillimagesforappearancefeaturesandthenfused capturedynamicfacialcomponentactivities,andthenapplydeep
the outputs of two streams, was introduced by Simonyan et al. networksforclassification.Thismethodiscomputationallysimple
[209]. Inspired by this architecture, several network ensemble and can get rid of the issue on illumination variations. However,
modelshavebeenproposedforFER. it is sensitive to registration errors and requires accurate facial
Sun et al. [185] proposed a multi-channel network that ex- landmark detection, which is difficult to access in unconstrained
tractedthespatialinformationfromemotion-expressingfacesand conditions. Consequently, this method performs less well and is
temporal information (optical flow) from the changes between moresuitabletocomplementappearancerepresentations.Network
emotioanlandneutralfaces,andinvestigatedthreefeaturefusion ensembleisutilizedtotrainmultiplenetworksforbothspatialand
strategies: score average fusion, SVM-based fusion and neural- temporal information and then to fuse the network outputs in the
network-based fusion. Zhang et al. [68] fused the temporal net- finalstage.Opticflowandfaciallandmarktrajectorycanbeused
work PHRNN (discussed in “Landmark trajectory”) and the as temporal representations to collaborate spatial representations.
spatial network MSCNN (discussed in section 4.1.5) to extract One of the drawbacks of this framework is the pre-computing
the partial-whole, geometry-appearance, and static-dynamic in- and storage consumption on optical flow or landmark trajectory
formation for FER (see Fig. 13). Instead of fusing the network vectors. And most related researches randomly selected fixed-
outputs with different weights, Jung et al. [16] proposed a joint lengthvideoframesasinput,leadingtothelossofusefultemporal
fine-tuning method that jointly trained the DTAN (discussed in information. Cascaded networks were proposed to first extract
the“RNNandC3D”),theDTGN(discussedinthe“Landmark discriminative representations for facial expression images and17
TABLE8
Comparisonofdifferenttypesofmethodsfordynamicimagesequencesintermsofdatasizerequirement,representabilityofspatialandtemporal
information,requirementonframelength,performance,andcomputationalefficiency.FLT =FacialLandmarkTrajectory;CN =Cascaded
Network;NE=NetworkEnsemble.
Networktype data spatial temporal framelength accuracy efficiency
Frameaggregation low good no depends fair high
Expressionintensity fair good low fixed fair varies
RNN low low good variable low fair
Spatio- C3D high good fair fixed low fair
temporal FLT fair fair fair fixed low high
network CN high good good variable good fair
NE low good good fixed good low
then input these features to sequential networks to reinforce the [215] proposed a multi-channel pose-aware CNN (MPCNN) that
temporal information encoding. However, this model introduces contains three cascaded parts (multi-channel feature extraction,
additional parameters to capture sequence information, and the jointlymulti-scalefeaturefusionandthepose-awarerecognition)
featurelearningnetwork(e.g.,CNN)andthetemporalinformation to predict expression labels by minimizing the conditional joint
encoding network (e.g., LSTM) in current works are not trained loss of pose and expression recognition. Besides, the technology
jointly, which may lead to suboptimal parameter settings. And of generative adversarial network (GAN) has been employed in
traininginanend-to-endfashionisstillalongroad. [180], [181] to generate facial images with different expressions
Comparedwithdeepnetworksonstaticdata,Table4andTable underarbitraryposesformulti-viewFER.
7demonstratethepowerfulcapabilityandpopularitytrendofdeep
spatio-temporal networks. For instance, comparison results on 5.2 FERoninfrareddata
widelyevaluatedbenchmarks(e.g.,CK+andMMI)illustratethat
AlthoughRBGorgraydataarethecurrentstandardindeepFER,
trainingnetworksbasedonsequencedataandanalyzingtemporal
these data are vulnerable to ambient lighting conditions. While,
dependencybetweenframescanfurtherimprovetheperformance.
infraredimagesthatrecordtheskintemporaldistributionproduced
Also, in the EmotiW challenge 2015, only one system employed
byemotionsarenotsensitivetoilluminationvariations,whichmay
deep spatio-networks for FER, whereas 5 of 7 reviewed systems
be a promising alternative for investigation of facial expression.
intheEmotiWchallenge2017reliedonsuchnetworks.
Forexample,Heetal.[216]employedaDBMmodelthatconsists
ofaGaussian-binaryRBMandabinaryRBMforFER.Themodel
5 ADDITIONAL RELATED ISSUES
was trained by layerwise pre-training and joint training and was
In addition to the most popular basic expression classification then fine-tuned on long-wavelength thermal infrared images to
task reviewed above, we further introduce a few related issues learn thermal features. Wu et al. [217] proposed a three-stream
thatdependondeepneuralnetworksandprototypicalexpression- 3D CNN to fuse local and global spatio-temporal features on
relatedknowledge. illumination-invariantnear-infraredimagesforFER.
5.1 Occlusionandnon-frontalheadpose 5.3 FERon3Dstaticanddynamicdata
Occlusion and non-frontal head pose, which may change the Despite significant advances have achieved in 2D FER, it fails
visualappearanceoftheoriginalfacialexpression,aretwomajor to solve the two main problems: illumination changes and pose
obstaclesforautomaticFER,especiallyinreal-worldscenarios. variations [29]. 3D FER that uses 3D face shape models with
For facial occlusion, Ranzato et al. [210], [211] proposed a depth information can capture subtle facial deformations, which
deep generative model that used mPoT [212] as the first layer arenaturallyrobusttoposeandlightingvariations.
of DBNs to model pixel-level representations and then trained Depth images and videos record the intensity of facial pixels
DBNs to fit an appropriate distribution to its inputs. Thus, the based on distance from a depth camera, which contain critical
occluded pixels in images could be filled in by reconstructing informationoffacialgeometricrelations.Forexample,[218]used
the top layer representation using the sequence of conditional kinect depth sensor to obtain gradient direction information and
distributions. Cheng et al. [213] employed multilayer RBMs thenemployedCNNonunregisteredfacialdepthimagesforFER.
with a pre-training and fine-tuning process on Gabor features [219], [220] extracted a series of salient features from depth
to compress features from the occluded facial parts. Xu et al. videos and combined them with deep networks (i.e., CNN and
[214] concatenated high-level learned features transferred from DBN) for FER. To emphasize the dynamic deformation patterns
two CNNs with the same structure but pre-trained on different of facial expression motions, Li et al. [221] explore the 4D FER
data: the original MSRA-CFW database and the MSRA-CFW (3DFERusingdynamicdata)usingadynamicgeometricalimage
databasewithadditiveoccludedsamples. network.Furthermore,Changetal.[222]proposedtoestimate3D
Formulti-viewFER,Zhangetal.[156]introducedaprojection expressioncoefficientsfromimageintensitiesusingCNNwithout
layer into the CNN that learned discriminative facial features requiring facial landmark detection. Thus, the model is highly
by weighting different facial landmark points within 2D SIFT robust to extreme appearance variations, including out-of-plane
featurematriceswithoutrequiringfacialposeestimation.Liuetal. headrotations,scalechanges,andocclusions.18
Recently,moreandmoreworkstrendtocombine2Dand3D 6 CHALLENGES AND OPPORTUNITIES
data to further improve the performance. Oyedotun et al. [223] 6.1 Facialexpressiondatasets
employed CNN to jointly learn facial expression features from
As the FER literature shifts its main focus to the challeng-
both RGB and depth map latent modalities. And Li et al. [224]
ing in-the-wild environmental conditions, many researchers have
proposed a deep fusion CNN (DF-CNN) to explore multi-modal
committed to employing deep learning technologies to handle
2D+3D FER. Specifically, six types of 2D facial attribute maps
difficulties,suchasilluminationvariation,occlusions,non-frontal
(i.e.,geometry,texture,curvature,normalcomponentsx,y,andz)
head poses, identity bias and the recognition of low-intensity
were first extracted from the textured 3D face scans and were
expressions.GiventhatFERisadata-driventaskandthattraining
then jointly fed into the feature extraction and feature fusion
a sufficiently deep network to capture subtle expression-related
subnets to learn the optimal combination weights of 2D and 3D
deformations requires a large amount of training data, the major
facial representations. To improve this work, [225] proposed to
challenge that deep FER systems face is the lack of training data
extractdeepfeaturesfromdifferentfacialpartsextractedfromthe
intermsofbothquantityandquality.
textureanddepthimages,andthenfusedthesefeaturestogetherto
Because people of different age ranges, cultures and genders
interconnectthemwithfeedback.Weietal.[226]furtherexplored
display and interpret facial expression in different ways, an ideal
thedatabiasproblemin2D+3DFERusingunsuperviseddomain
facial expression dataset is expected to include abundant sample
adaptiontechnique.
images with precise face attribute labels, not just expression but
other attributes such as age, gender and ethnicity, which would
5.4 Facialexpressionsynthesis
facilitate related research on cross-age range, cross-gender and
Realistic facial expression synthesis, which can generate various
cross-cultural FER using deep learning techniques, such as mul-
facialexpressionsforinteractiveinterfaces,isahottopic.Susskind
titask deep networks and transfer learning. In addition, although
et al. [227] demonstrated that DBN has the capacity to capture
occlusion and multipose problems have received relatively wide
the large range of variation in expressive appearance and can be
interest in the field of deep face recognition, the occlusion-
trainedonlargebutsparselylabeleddatasets.Inlightofthiswork,
robustandpose-invariantissueshavereceivelessattentionindeep
[210], [211], [228] employed DBN with unsupervised learning
FER. One of the main reasons is the lack of a large-scale facial
to construct facial expression synthesis systems. Kaneko et al.
expressiondatasetwithocclusiontypeandhead-poseannotations.
[149] proposed a multitask deep network with state recognition
On the other hand, accurately annotating a large volume of
and key-point localization to adaptively generate visual feedback
image data with the large variation and complexity of natural
to improve facial expression recognition. With the recent success
scenariosisanobviousimpedimenttotheconstructionofexpres-
of the deep generative models, such as variational autoencoder
siondatasets.Areasonableapproachistoemploycrowd-sourcing
(VAE),adversarialautoencoder(AAE),andgenerativeadversarial
models[44],[46],[249]undertheguidanceofexpertannotators.
network (GAN), a series of facial expression synthesis systems
Additionally,afullyautomaticlabelingtool[43]refinedbyexperts
have been developed based on these models (e.g., [229], [230],
is alternative to provide approximate but efficient annotations. In
[231], [232] and [233]). Facial expression synthesis can also be
both cases, a subsequent reliable estimation or labeling learning
applied to data augmentation without manually collecting and
process is necessary to filter out noisy annotations. In particular,
labeling huge datasets. Masi et al. [234] employed CNN to
few comparatively large-scale datasets that consider real-world
synthesizenewfaceimagesbyincreasingface-specificappearance
scenarios and contain a wide range of facial expressions have
variation,suchasexpressionswithinthe3Dtexturedfacemodel.
recently become publicly available, i.e., EmotioNet [43], RAF-
DB [44], [45] and AffectNet [46], and we anticipate that with
5.5 Visualizationtechniques
advancesintechnologyandthewidespreadoftheInternet,more
In addition to utilizing CNN for FER, several works (e.g., [139],
complementary facial expression datasets will be constructed to
[235], [236]) employed visualization techniques [237] on the
promotethedevelopmentofdeepFER.
learned CNN features to qualitatively analyze how the CNN
contributes to the appearance-based learning process of FER and
to qualitatively decipher which portions of the face yield the 6.2 Incorporatingotheraffectivemodels
most discriminative information. The deconvolutional results all AnothermajorissuethatrequiresconsiderationisthatwhileFER
indicated that the activations of some particular filters on the within the categorical model is widely acknowledged and re-
learnedfeatureshavestrongcorrelationswiththefaceregionsthat searched,thedefinitionoftheprototypicalexpressionscoversonly
correspondtofacialAUs. a small portion of specific categories and cannot capture the full
repertoire of expressive behaviors for realistic interactions. Two
5.6 Otherspecialissues additional models were developed to describe a larger range of
Several novel issues have been approached on the basis of the emotionallandscape:theFACSmodel[10],[176],wherevarious
prototypical expression categories: dominant and complementary facialmuscleAUsarecombinedtodescribethevisibleappearance
emotion recognition challenge [238] and the Real versus Fake changes of facial expressions, and the dimensional model [11],
expressed emotions challenge [239]. Furthermore, deep learning [250], where two continuous-valued variables, namely, valence
techniques have been thoroughly applied by the participants of andarousal,areproposedtocontinuouslyencodesmallchangesin
thesetwochallenges(e.g.,[240],[241],[242]).Additionalrelated theintensityofemotions.Anothernoveldefinition,i.e.,compound
real-world applications, such as the Real-time FER App for expression,wasproposedbyDuetal.[52],whoarguedthatsome
smartphones [243], [244], Eyemotion (FER using eye-tracking facial expressions are actually combinations of more than one
cameras)[245],privacy-preservingmobileanalytics[246],Unfelt basicemotion.Theseworksimprovethecharacterizationoffacial
emotions[247]andDepressionrecognition[248],havealsobeen expressions and, to some extent, can complement the categorical
developed. model. For instance, as discussed above, the visualization results19
of CNNs have demonstrated a certain congruity between the [4] P.Ekman,“Strongevidenceforuniversalsinfacialexpressions:areply
learnedrepresentationsandthefacialareasdefinedbyAUs.Thus, torussell’smistakencritique,”Psychologicalbulletin,vol.115,no.2,
pp.268–287,1994.
we can design filters of the deep neural networks to distribute
[5] D. Matsumoto, “More evidence for the universality of a contempt
different weights according to the importance degree of different
expression,”MotivationandEmotion,vol.16,no.4,pp.363–368,1992.
facialmuscleactionparts. [6] R.E.Jack,O.G.Garrod,H.Yu,R.Caldara,andP.G.Schyns,“Facial
expressionsofemotionarenotculturallyuniversal,”Proceedingsofthe
NationalAcademyofSciences,vol.109,no.19,pp.7241–7244,2012.
6.3 Datasetbiasandimbalanceddistribution [7] Z.Zeng,M.Pantic,G.I.Roisman,andT.S.Huang,“Asurveyofaffect
recognition methods: Audio, visual, and spontaneous expressions,”
Data bias and inconsistent annotations are very common among
IEEEtransactionsonpatternanalysisandmachineintelligence,vol.31,
different facial expression datasets due to different collecting
no.1,pp.39–58,2009.
conditionsandthesubjectivenessofannotating.Researcherscom- [8] E.Sariyanidi,H.Gunes,andA.Cavallaro,“Automaticanalysisoffacial
monly evaluate their algorithms within a specific dataset and can affect:Asurveyofregistration,representation,andrecognition,”IEEE
transactionsonpatternanalysisandmachineintelligence,vol.37,no.6,
achieve satisfactory performance. However, early cross-database
pp.1113–1133,2015.
experiments have indicated that discrepancies between databases
[9] B.MartinezandM.F.Valstar,“Advances,challenges,andopportuni-
existduetothedifferentcollectionenvironmentsandconstruction ties in automatic facial expression recognition,” in Advances in Face
indicators [12]; hence, algorithms evaluated via intra-database DetectionandFacialImageAnalysis. Springer,2016,pp.63–100.
protocols lack generalizability on unseen test data, and the per- [10] P.Ekman,“Facialactioncodingsystem(facs),”Ahumanface,2002.
formance in cross-dataset settings is greatly deteriorated. Deep [11] H.GunesandB.Schuller,“Categoricalanddimensionalaffectanalysis
incontinuousinput:Currenttrendsandfuturedirections,”Imageand
domain adaption and knowledge distillation are alternatives to
VisionComputing,vol.31,no.2,pp.120–136,2013.
address this bias [226], [251]. Furthermore, because of the in- [12] C.Shan,S.Gong,andP.W.McOwan,“Facialexpressionrecognition
consistent expression annotations, FER performance cannot keep based on local binary patterns: A comprehensive study,” Image and
improving when enlarging the training data by directly merging VisionComputing,vol.27,no.6,pp.803–816,2009.
[13] P.Liu,S.Han,Z.Meng,andY.Tong,“Facialexpressionrecognitionvia
multipledatasets[167].
aboosteddeepbeliefnetwork,”inProceedingsoftheIEEEConference
Anothercommonprobleminfacialexpressionisclassimbal- onComputerVisionandPatternRecognition,2014,pp.1805–1812.
ance, which is a result of the practicalities of data acquisition: [14] A.Mollahosseini,D.Chan,andM.H.Mahoor,“Goingdeeperinfacial
eliciting and annotating a smile is easy, however, capturing in- expressionrecognitionusingdeepneuralnetworks,”inApplicationsof
ComputerVision(WACV),2016IEEEWinterConferenceon. IEEE,
formation for disgust, anger and other less common expressions
2016,pp.1–10.
can be very challenging. As shown in Table 4 and Table 7, the
[15] G.ZhaoandM.Pietikainen,“Dynamictexturerecognitionusinglocal
performance assessed in terms of mean accuracy, which assigns binarypatternswithanapplicationtofacialexpressions,”IEEEtrans-
equal weights to all classes, decreases when compared with the actionsonpatternanalysisandmachineintelligence,vol.29,no.6,pp.
915–928,2007.
accuracy criterion, and this decline is especially evident in real-
[16] H.Jung,S.Lee,J.Yim,S.Park,andJ.Kim,“Jointfine-tuningindeep
world datasets (e.g., SFEW 2.0 and AFEW). One solution is to
neuralnetworksforfacialexpressionrecognition,”inComputerVision
balancetheclassdistributionduringthepre-processingstageusing (ICCV), 2015 IEEE International Conference on. IEEE, 2015, pp.
dataaugmentationandsynthesis.Anotheralternativeistodevelop 2983–2991.
acost-sensitivelosslayerfordeepnetworksduringtraining. [17] X. Zhao, X. Liang, L. Liu, T. Li, Y. Han, N. Vasconcelos, and
S.Yan,“Peak-piloteddeepnetworkforfacialexpressionrecognition,”
inEuropeanconferenceoncomputervision. Springer,2016,pp.425–
442.
6.4 Multimodalaffectrecognition
[18] C. A. Corneanu, M. O. Simo´n, J. F. Cohn, and S. E. Guerrero,
Last but not the least, human expressive behaviors in realistic “Survey on rgb, 3d, thermal, and multimodal approaches for facial
applicationsinvolveencodingfromdifferentperspectives,andthe expressionrecognition:History,trends,andaffect-relatedapplications,”
IEEEtransactionsonpatternanalysisandmachineintelligence,vol.38,
facial expression is only one modality. Although pure expression
no.8,pp.1548–1568,2016.
recognition based on visible face images can achieve promis-
[19] R.Zhi,M.Flierl,Q.Ruan,andW.B.Kleijn,“Graph-preservingsparse
ing results, incorporating with other models into a high-level nonnegative matrix factorization with application to facial expression
framework can provide complementary information and further recognition,” IEEE Transactions on Systems, Man, and Cybernetics,
PartB(Cybernetics),vol.41,no.1,pp.38–52,2011.
enhance therobustness. Forexample, participantsin theEmotiW
[20] L. Zhong, Q. Liu, P. Yang, B. Liu, J. Huang, and D. N. Metaxas,
challenges and Audio Video Emotion Challenges (AVEC) [252],
“Learning active facial patches for expression analysis,” in Computer
[253]consideredtheaudiomodeltobethesecondmostimportant Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on.
element and employed various fusion techniques for multimodal IEEE,2012,pp.2562–2569.
affect recognition. Additionally, the fusion of other modalities, [21] I. J. Goodfellow, D. Erhan, P. L. Carrier, A. Courville, M. Mirza,
B. Hamner, W. Cukierski, Y. Tang, D. Thaler, D.-H. Lee et al.,
such as infrared images, depth information from 3D face models
“Challenges in representation learning: A report on three machine
andphysiologicaldata,isbecomingapromisingresearchdirection learningcontests,”inInternationalConferenceonNeuralInformation
duetothelargecomplementarityforfacialexpressions. Processing. Springer,2013,pp.117–124.
[22] A. Dhall, O. Ramana Murthy, R. Goecke, J. Joshi, and T. Gedeon,
“Video and image based emotion recognition challenges in the wild:
REFERENCES Emotiw 2015,” in Proceedings of the 2015 ACM on International
ConferenceonMultimodalInteraction. ACM,2015,pp.423–426.
[1] C.DarwinandP.Prodger,Theexpressionoftheemotionsinmanand [23] A.Dhall,R.Goecke,J.Joshi,J.Hoey,andT.Gedeon,“Emotiw2016:
animals. OxfordUniversityPress,USA,1998. Videoandgroup-levelemotionrecognitionchallenges,”inProceedings
[2] Y.-I. Tian, T. Kanade, and J. F. Cohn, “Recognizing action units for ofthe18thACMInternationalConferenceonMultimodalInteraction.
facialexpressionanalysis,”IEEETransactionsonpatternanalysisand ACM,2016,pp.427–432.
machineintelligence,vol.23,no.2,pp.97–115,2001. [24] A. Dhall, R. Goecke, S. Ghosh, J. Joshi, J. Hoey, and T. Gedeon,
[3] P.EkmanandW.V.Friesen,“Constantsacrossculturesinthefaceand “Fromindividualtogroup-levelemotionrecognition:Emotiw5.0,”in
emotion.”Journalofpersonalityandsocialpsychology,vol.17,no.2, Proceedingsofthe19thACMInternationalConferenceonMultimodal
pp.124–129,1971. Interaction. ACM,2017,pp.524–528.20
[25] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classifica- [48] A.Dhall,R.Goecke,S.Lucey,T.Gedeonetal.,“Collectinglarge,richly
tionwithdeepconvolutionalneuralnetworks,”inAdvancesinneural annotatedfacial-expressiondatabasesfrommovies,”IEEEmultimedia,
informationprocessingsystems,2012,pp.1097–1105. vol.19,no.3,pp.34–41,2012.
[26] K.SimonyanandA.Zisserman,“Verydeepconvolutionalnetworksfor [49] A.Dhall,R. Goecke,S.Lucey, andT.Gedeon, “Acted facialexpres-
large-scaleimagerecognition,”arXivpreprintarXiv:1409.1556,2014. sionsinthewilddatabase,”AustralianNationalUniversity,Canberra,
[27] C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed,D.Anguelov,D.Erhan, Australia,TechnicalReportTR-CS-11,vol.2,p.1,2011.
V.Vanhoucke,andA.Rabinovich,“Goingdeeperwithconvolutions,” [50] ——, “Static facial expression analysis in tough conditions: Data,
inProceedingsoftheIEEEconferenceoncomputervisionandpattern evaluation protocol and benchmark,” in Computer Vision Workshops
recognition,2015,pp.1–9. (ICCV Workshops), 2011 IEEE International Conference on. IEEE,
[28] K.He,X.Zhang,S.Ren,andJ.Sun,“Deepresiduallearningforimage 2011,pp.2106–2112.
recognition,”inProceedingsoftheIEEEconferenceoncomputervision [51] C. F. Benitez-Quiroz, R. Srinivasan, Q. Feng, Y. Wang, and A. M.
andpatternrecognition,2016,pp.770–778. Martinez, “Emotionet challenge: Recognition of facial expressions of
[29] M. Pantic and L. J. M. Rothkrantz, “Automatic analysis of facial emotioninthewild,”arXivpreprintarXiv:1703.01210,2017.
expressions:Thestateoftheart,”IEEETransactionsonpatternanalysis [52] S.Du,Y.Tao,andA.M.Martinez,“Compoundfacialexpressionsof
andmachineintelligence,vol.22,no.12,pp.1424–1445,2000. emotion,”ProceedingsoftheNationalAcademyofSciences,vol.111,
[30] B.FaselandJ.Luettin,“Automaticfacialexpressionanalysis:asurvey,” no.15,pp.E1454–E1462,2014.
Patternrecognition,vol.36,no.1,pp.259–275,2003. [53] T.F.Cootes,G.J.Edwards,andC.J.Taylor,“Activeappearancemod-
[31] T. Zhang, “Facial expression recognition based on deep learning: A els,” IEEE Transactions on Pattern Analysis & Machine Intelligence,
survey,” in International Conference on Intelligent and Interactive no.6,pp.681–685,2001.
SystemsandApplications. Springer,2017,pp.345–352. [54] N. Zeng, H. Zhang, B. Song, W. Liu, Y. Li, and A. M. Dobaie,
[32] M.F.Valstar,M. Mehu,B.Jiang,M.Pantic, and K.Scherer,“Meta- “Facialexpressionrecognitionvialearningdeepsparseautoencoders,”
analysis of the first facial expression recognition challenge,” IEEE Neurocomputing,vol.273,pp.643–649,2018.
TransactionsonSystems,Man,andCybernetics,PartB(Cybernetics), [55] B.HasaniandM.H.Mahoor,“Spatio-temporalfacialexpressionrecog-
vol.42,no.4,pp.966–979,2012. nition using convolutional neural networks and conditional random
[33] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and fields,” in Automatic Face & Gesture Recognition (FG 2017), 2017
I. Matthews, “The extended cohn-kanade dataset (ck+): A complete 12thIEEEInternationalConferenceon. IEEE,2017,pp.790–795.
datasetforactionunitandemotion-specifiedexpression,”inComputer [56] X.ZhuandD.Ramanan,“Facedetection,poseestimation,andland-
VisionandPatternRecognitionWorkshops(CVPRW),2010IEEECom- marklocalizationinthewild,”inComputerVisionandPatternRecogni-
puterSocietyConferenceon. IEEE,2010,pp.94–101. tion(CVPR),2012IEEEConferenceon. IEEE,2012,pp.2879–2886.
[34] M.Pantic,M.Valstar,R.Rademaker,andL.Maat,“Web-baseddatabase [57] S. E. Kahou, C. Pal, X. Bouthillier, P. Froumenty, C¸. Gu¨lc¸ehre,
forfacialexpressionanalysis,” inMultimediaandExpo,2005.ICME R. Memisevic, P. Vincent, A. Courville, Y. Bengio, R. C. Ferrari
2005.IEEEInternationalConferenceon. IEEE,2005,pp.5–pp. etal.,“Combiningmodalityspecificdeepneuralnetworksforemotion
[35] M.ValstarandM.Pantic,“Induceddisgust,happinessandsurprise:an recognitioninvideo,”inProceedingsofthe15thACMonInternational
addition to the mmi facial expression database,” in Proc. 3rd Intern. conferenceonmultimodalinteraction. ACM,2013,pp.543–550.
WorkshoponEMOTION(satelliteofLREC):CorporaforResearchon [58] T.Devries,K.Biswaranjan,andG.W.Taylor,“Multi-tasklearningof
EmotionandAffect,2010,p.65. faciallandmarksandexpression,”inComputerandRobotVision(CRV),
[36] M. Lyons, S. Akamatsu, M. Kamachi, and J. Gyoba, “Coding facial 2014CanadianConferenceon. IEEE,2014,pp.98–103.
expressions with gabor wavelets,” in Automatic Face and Gesture [59] A.Asthana,S.Zafeiriou,S.Cheng,andM.Pantic,“Robustdiscrimina-
Recognition,1998.Proceedings.ThirdIEEEInternationalConference tiveresponsemapfittingwithconstrainedlocalmodels,”inComputer
on. IEEE,1998,pp.200–205. Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on.
[37] J.M.Susskind,A.K.Anderson,andG.E.Hinton,“Thetorontoface IEEE,2013,pp.3444–3451.
database,” Department of Computer Science, University of Toronto, [60] M. Shin, M. Kim, and D.-S. Kwon, “Baseline cnn structure analysis
Toronto,ON,Canada,Tech.Rep,vol.3,2010. for facial expression recognition,” in Robot and Human Interactive
[38] R.Gross,I.Matthews,J.Cohn,T.Kanade,andS.Baker,“Multi-pie,” Communication(RO-MAN),201625thIEEEInternationalSymposium
ImageandVisionComputing,vol.28,no.5,pp.807–813,2010. on. IEEE,2016,pp.724–729.
[39] L. Yin, X. Wei, Y. Sun, J. Wang, and M. J. Rosato, “A 3d facial [61] Z.Meng,P.Liu,J.Cai,S.Han,andY.Tong,“Identity-awareconvo-
expression database for facial behavior research,” in Automatic face lutionalneuralnetworkforfacialexpressionrecognition,”inAutomatic
andgesturerecognition,2006.FGR2006.7thinternationalconference Face&GestureRecognition(FG2017),201712thIEEEInternational
on. IEEE,2006,pp.211–216. Conferenceon. IEEE,2017,pp.558–565.
[40] G. Zhao, X. Huang, M. Taini, S. Z. Li, and M. Pietika¨Inen, “Facial [62] X.XiongandF.DelaTorre,“Superviseddescentmethodanditsappli-
expression recognition from near-infrared videos,” Image and Vision cationstofacealignment,”inComputerVisionandPatternRecognition
Computing,vol.29,no.9,pp.607–619,2011. (CVPR),2013IEEEConferenceon. IEEE,2013,pp.532–539.
[41] O.Langner,R.Dotsch,G.Bijlstra,D.H.Wigboldus,S.T.Hawk,and [63] H.-W.Ng,V.D.Nguyen,V.Vonikakis,andS.Winkler,“Deeplearning
A.vanKnippenberg,“Presentationandvalidationoftheradboudfaces for emotion recognition on small datasets using transfer learning,” in
database,”CognitionandEmotion,vol.24,no.8,pp.1377–1388,2010. Proceedings of the 2015 ACM on international conference on multi-
[42] D. Lundqvist, A. Flykt, and A. O¨hman, “The karolinska directed modalinteraction. ACM,2015,pp.443–449.
emotional faces (kdef),” CD ROM from Department of Clinical Neu- [64] S. Ren, X. Cao, Y. Wei, and J. Sun, “Face alignment at 3000 fps
roscience,Psychologysection,KarolinskaInstitutet,no.1998,1998. via regressing local binary features,” in Proceedings of the IEEE
[43] C.F.Benitez-Quiroz,R.Srinivasan,andA.M.Martinez,“Emotionet: Conference on Computer Vision and Pattern Recognition, 2014, pp.
An accurate, real-time algorithm for the automatic annotation of a 1685–1692.
millionfacialexpressionsinthewild,”inProceedingsofIEEEInterna- [65] A.Asthana,S.Zafeiriou,S.Cheng,andM.Pantic,“Incrementalface
tionalConferenceonComputerVision&PatternRecognition(CVPR), alignment in the wild,” in Proceedings of the IEEE conference on
LasVegas,NV,USA,2016. computervisionandpatternrecognition,2014,pp.1859–1866.
[44] S.Li,W.Deng,andJ.Du,“Reliablecrowdsourcinganddeeplocality- [66] D.H.Kim,W.Baddar,J.Jang,andY.M.Ro,“Multi-objectivebased
preserving learning for expression recognition in the wild,” in 2017 spatio-temporalfeaturerepresentationlearningrobusttoexpressionin-
IEEEConferenceonComputerVisionandPatternRecognition(CVPR). tensityvariationsforfacialexpressionrecognition,”IEEETransactions
IEEE,2017,pp.2584–2593. onAffectiveComputing,2017.
[45] S. Li and W. Deng, “Reliable crowdsourcing and deep locality- [67] Y.Sun,X.Wang,andX.Tang,“Deepconvolutionalnetworkcascade
preserving learning for unconstrained facial expression recognition,” forfacialpointdetection,”inComputerVisionandPatternRecognition
IEEETransactionsonImageProcessing,2018. (CVPR),2013IEEEConferenceon. IEEE,2013,pp.3476–3483.
[46] A.Mollahosseini,B.Hasani,andM.H.Mahoor,“Affectnet:Adatabase [68] K.Zhang,Y.Huang,Y.Du,andL.Wang,“Facialexpressionrecognition
forfacialexpression,valence,andarousalcomputinginthewild,”IEEE based on deep evolutional spatial-temporal networks,” IEEE Transac-
TransactionsonAffectiveComputing,vol.PP,no.99,pp.1–1,2017. tionsonImageProcessing,vol.26,no.9,pp.4193–4203,2017.
[47] Z. Zhang, P. Luo, C. L. Chen, and X. Tang, “From facial expression [69] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao, “Joint face detection and
recognitiontointerpersonalrelationprediction,”InternationalJournal alignment using multitask cascaded convolutional networks,” IEEE
ofComputerVision,vol.126,no.5,pp.1–20,2018. SignalProcessingLetters,vol.23,no.10,pp.1499–1503,2016.21
[70] Z.Yu,Q.Liu,andG.Liu,“Deepercascadedpeak-pilotednetworkfor InternationalConferenceonMultimodalInteraction. ACM,2016,pp.
weakexpressionrecognition,”TheVisualComputer,pp.1–9,2017. 472–478.
[71] Z. Yu, G. Liu, Q. Liu, and J. Deng, “Spatio-temporal convolutional [91] P. Hu, D. Cai, S. Wang, A. Yao, and Y. Chen, “Learning supervised
featureswithnestedlstmforfacialexpressionrecognition,”Neurocom- scoringensembleforemotionrecognitioninthewild,”inProceedings
puting,vol.317,pp.50–57,2018. ofthe19thACMInternationalConferenceonMultimodalInteraction.
[72] P.ViolaandM.Jones,“Rapidobjectdetectionusingaboostedcascade ACM,2017,pp.553–560.
of simple features,” in Computer Vision and Pattern Recognition, [92] T.Hassner,S.Harel,E.Paz,andR.Enbar,“Effectivefacefrontalization
2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society in unconstrained images,” in Proceedings of the IEEE Conference on
Conferenceon,vol.1. IEEE,2001,pp.I–I. ComputerVisionandPatternRecognition,2015,pp.4295–4304.
[73] F.DelaTorre,W.-S.Chu,X.Xiong,F.Vicente,X.Ding,andJ.F.Cohn, [93] C. Sagonas, Y. Panagakis, S. Zafeiriou, and M. Pantic, “Robust sta-
“Intraface,”inIEEEInternationalConferenceonAutomaticFaceand tistical face frontalization,” in Proceedings of the IEEE International
GestureRecognition(FG),2015. ConferenceonComputerVision,2015,pp.3871–3879.
[74] Z.Zhang,P.Luo,C.C.Loy,andX.Tang,“Faciallandmarkdetectionby [94] X.Yin,X.Yu,K.Sohn,X.Liu,andM.Chandraker,“Towardslarge-
deepmulti-tasklearning,”inEuropeanConferenceonComputerVision. pose face frontalization in the wild,” in Proceedings of the IEEE
Springer,2014,pp.94–108. Conference on Computer Vision and Pattern Recognition, 2017, pp.
[75] Z.YuandC.Zhang,“Imagebasedstaticfacialexpressionrecognition 3990–3999.
withmultipledeepnetworklearning,”inProceedingsofthe2015ACM [95] R.Huang,S.Zhang,T.Li,andR.He,“Beyondfacerotation:Globaland
onInternationalConferenceonMultimodalInteraction. ACM,2015, local perception gan for photorealistic and identity preserving frontal
pp.435–442. viewsynthesis,”inProceedingsoftheIEEEConferenceonComputer
[76] B.-K. Kim, H. Lee, J. Roh, and S.-Y. Lee, “Hierarchical committee VisionandPatternRecognition,2017,pp.2439–2448.
of deep cnns with exponentially-weighted decision fusion for static [96] L. Tran, X. Yin, and X. Liu, “Disentangled representation learning
facial expression recognition,” in Proceedings of the 2015 ACM on gan for pose-invariant face recognition,” in Proceedings of the IEEE
International Conference on Multimodal Interaction. ACM, 2015, Conference on Computer Vision and Pattern Recognition, 2017, pp.
pp.427–434. 1415–1424.
[77] X. Liu, B. Kumar, J. You, and P. Jia, “Adaptive deep metric learning [97] L. Deng, D. Yu et al., “Deep learning: methods and applications,”
foridentity-awarefacialexpressionrecognition,”inProc.IEEEConf. Foundations and Trends(cid:13)R in Signal Processing, vol. 7, no. 3–4, pp.
Comput. Vis. Pattern Recognit. Workshops (CVPRW), 2017, pp. 522– 197–387,2014.
531. [98] B.Fasel,“Robustfaceanalysisusingconvolutionalneuralnetworks,”in
[78] G.LeviandT.Hassner,“Emotionrecognitioninthewildviaconvolu- PatternRecognition,2002.Proceedings.16thInternationalConference
tionalneuralnetworksandmappedbinarypatterns,”inProceedingsof on,vol.2. IEEE,2002,pp.40–43.
the2015ACMoninternationalconferenceonmultimodalinteraction. [99] ——,“Head-poseinvariantfacialexpressionrecognitionusingconvo-
ACM,2015,pp.503–510. lutionalneuralnetworks,”inProceedingsofthe4thIEEEInternational
[79] D. A. Pitaloka, A. Wulandari, T. Basaruddin, and D. Y. Liliana, ConferenceonMultimodalInterfaces. IEEEComputerSociety,2002,
“Enhancingcnnwithpreprocessingstageinautomaticemotionrecog- p.529.
nition,”ProcediaComputerScience,vol.116,pp.523–529,2017. [100] M.Matsugu,K.Mori,Y.Mitari,andY.Kaneda,“Subjectindependent
[80] A. T. Lopes, E. de Aguiar, A. F. De Souza, and T. Oliveira-Santos, facialexpressionrecognitionwithrobustfacedetectionusingaconvolu-
“Facialexpressionrecognitionwithconvolutionalneuralnetworks:cop- tionalneuralnetwork,”NeuralNetworks,vol.16,no.5-6,pp.555–559,
ingwithfewdataandthetrainingsampleorder,”PatternRecognition, 2003.
vol.61,pp.610–628,2017. [101] B. Sun, L. Li, G. Zhou, X. Wu, J. He, L. Yu, D. Li, and Q. Wei,
[81] M. V. Zavarez, R. F. Berriel, and T. Oliveira-Santos, “Cross-database “Combiningmultimodalfeatureswithinafusionnetworkforemotion
facial expression recognition based on fine-tuned deep convolutional recognitioninthewild,”inProceedingsofthe2015ACMonInterna-
network,” in Graphics, Patterns and Images (SIBGRAPI), 2017 30th tionalConferenceonMultimodalInteraction. ACM,2015,pp.497–
SIBGRAPIConferenceon. IEEE,2017,pp.405–412. 502.
[82] W. Li, M. Li, Z. Su, and Z. Zhu, “A deep-learning approach to [102] B. Sun, L. Li, G. Zhou, and J. He, “Facial expression recognition in
facial expression recognition with candid images,” in Machine Vision thewildbasedonmultimodaltexturefeatures,”JournalofElectronic
Applications (MVA), 2015 14th IAPR International Conference on. Imaging,vol.25,no.6,p.061407,2016.
IEEE,2015,pp.279–282. [103] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature
[83] I. Abbasnejad, S. Sridharan, D. Nguyen, S. Denman, C. Fookes, and hierarchies for accurate object detection and semantic segmentation,”
S. Lucey,“Usingsynthetic data to improvefacial expressionanalysis inProceedingsoftheIEEEconferenceoncomputervisionandpattern
with3dconvolutionalnetworks,”inProceedingsoftheIEEEConfer- recognition,2014,pp.580–587.
ence on Computer Vision and Pattern Recognition, 2017, pp. 1609– [104] J.Li,D.Zhang,J.Zhang,J.Zhang,T.Li,Y.Xia,Q.Yan,andL.Xun,
1618. “Facial expression recognition with faster r-cnn,” Procedia Computer
[84] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, Science,vol.107,pp.135–140,2017.
S.Ozair,A.Courville,andY.Bengio,“Generativeadversarialnets,”in [105] S.Ren,K.He,R.Girshick,andJ.Sun,“Fasterr-cnn:Towardsreal-time
Advances in neural information processing systems, 2014, pp. 2672– objectdetectionwithregionproposalnetworks,”inAdvancesinneural
2680. informationprocessingsystems,2015,pp.91–99.
[85] W. Chen, M. J. Er, and S. Wu, “Illumination compensation and nor- [106] S.Ji,W.Xu,M.Yang,andK.Yu,“3dconvolutionalneuralnetworks
malizationforrobustfacerecognitionusingdiscretecosinetransform forhumanactionrecognition,”IEEEtransactionsonpatternanalysis
inlogarithmdomain,”IEEETransactionsonSystems,Man,andCyber- andmachineintelligence,vol.35,no.1,pp.221–231,2013.
netics,PartB(Cybernetics),vol.36,no.2,pp.458–466,2006. [107] D.Tran,L.Bourdev,R.Fergus,L.Torresani,andM.Paluri,“Learning
[86] J.LiandE.Y.Lam,“Facialexpressionrecognitionusingdeepneural spatiotemporalfeatureswith3dconvolutionalnetworks,”inComputer
networks,” in Imaging Systems and Techniques (IST), 2015 IEEE Vision(ICCV),2015IEEEInternationalConferenceon. IEEE,2015,
InternationalConferenceon. IEEE,2015,pp.1–6. pp.4489–4497.
[87] S.EbrahimiKahou,V.Michalski,K.Konda,R.Memisevic,andC.Pal, [108] Y. Fan, X. Lu, D. Li, and Y. Liu, “Video-based emotion recognition
“Recurrentneuralnetworksforemotionrecognitioninvideo,”inPro- using cnn-rnn and c3d hybrid networks,” in Proceedings of the 18th
ceedingsofthe2015ACMonInternationalConferenceonMultimodal ACM International Conference on Multimodal Interaction. ACM,
Interaction. ACM,2015,pp.467–474. 2016,pp.445–450.
[88] S. A. Bargal, E. Barsoum, C. C. Ferrer, and C. Zhang, “Emotion [109] D. Nguyen, K. Nguyen, S. Sridharan, A. Ghasemi, D. Dean, and
recognitioninthewildfromvideosusingimages,”inProceedingsofthe C. Fookes, “Deep spatio-temporal features for multimodal emotion
18thACMInternationalConferenceonMultimodalInteraction. ACM, recognition,”inApplicationsofComputerVision(WACV),2017IEEE
2016,pp.433–436. WinterConferenceon. IEEE,2017,pp.1215–1223.
[89] C.-M.Kuo,S.-H.Lai,andM.Sarkis,“Acompactdeeplearningmodel [110] S. Ouellet, “Real-time emotion recognition for gaming using deep
for robust facial expression recognition,” in Proceedings of the IEEE convolutionalnetworkfeatures,”arXivpreprintarXiv:1408.3750,2014.
Conference on Computer Vision and Pattern Recognition Workshops, [111] H.Ding,S.K.Zhou,andR.Chellappa,“Facenet2expnet:Regularizing
2018,pp.2121–2129. a deep face recognition net for expression recognition,” in Automatic
[90] A.Yao,D.Cai,P.Hu,S.Wang,L.Sha,andY.Chen,“Holonet:towards Face&GestureRecognition(FG2017),201712thIEEEInternational
robustemotionrecognitioninthewild,”inProceedingsofthe18thACM Conferenceon. IEEE,2017,pp.118–126.22
[112] B. Hasani and M. H. Mahoor, “Facial expression recognition using [137] M.Liu,S.Li,S.Shan,andX.Chen,“Au-awaredeepnetworksforfacial
enhanceddeep3dconvolutionalneuralnetworks,”inComputerVision expression recognition,” in Automatic Face and Gesture Recognition
andPatternRecognitionWorkshops(CVPRW),2017IEEEConference (FG), 2013 10th IEEE International Conference and Workshops on.
on. IEEE,2017,pp.2278–2288. IEEE,2013,pp.1–6.
[113] G.E.Hinton,S.Osindero,andY.-W.Teh,“Afastlearningalgorithmfor [138] ——, “Au-inspired deep networks for facial expression feature learn-
deepbeliefnets,”Neuralcomputation,vol.18,no.7,pp.1527–1554, ing,”Neurocomputing,vol.159,pp.126–136,2015.
2006. [139] P.Khorrami,T.Paine,andT.Huang,“Dodeepneuralnetworkslearn
[114] G. E. Hinton and T. J. Sejnowski, “Learning and releaming in boltz- facialactionunitswhendoingexpressionrecognition?”arXivpreprint
mann machines,” Parallel distributed processing: Explorations in the arXiv:1510.02969v3,2015.
microstructureofcognition,vol.1,no.282-317,p.2,1986. [140] J.Cai,Z.Meng,A.S.Khan,Z.Li,J.OReilly,andY.Tong,“Islandloss
[115] G. E. Hinton, “A practical guide to training restricted boltzmann forlearningdiscriminativefeaturesinfacialexpressionrecognition,”in
machines,”inNeuralnetworks:Tricksofthetrade. Springer,2012, Automatic Face & Gesture Recognition (FG 2018), 2018 13th IEEE
pp.599–619. InternationalConferenceon. IEEE,2018,pp.302–309.
[116] Y.Bengio,P.Lamblin,D.Popovici,andH.Larochelle,“Greedylayer- [141] H.Yang,U.Ciftci,andL.Yin,“Facialexpressionrecognitionbyde-
wise training of deep networks,” in Advances in neural information expressionresiduelearning,”inProceedingsoftheIEEEConferenceon
processingsystems,2007,pp.153–160. ComputerVisionandPatternRecognition,2018,pp.2168–2177.
[117] G.E.Hinton,“Trainingproductsofexpertsbyminimizingcontrastive [142] D.Hamester,P.Barros,andS.Wermter,“Faceexpressionrecognition
divergence,”Neuralcomputation,vol.14,no.8,pp.1771–1800,2002. with a 2-channel convolutional neural network,” in Neural Networks
[118] G.E.HintonandR.R.Salakhutdinov,“Reducingthedimensionalityof (IJCNN), 2015 International Joint Conference on. IEEE, 2015, pp.
datawithneuralnetworks,”science,vol.313,no.5786,pp.504–507, 1–8.
2006. [143] S.Reed,K.Sohn,Y.Zhang,andH.Lee,“Learningtodisentanglefac-
[119] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol, torsofvariationwithmanifoldinteraction,”inInternationalConference
“Stackeddenoisingautoencoders:Learningusefulrepresentationsina onMachineLearning,2014,pp.1431–1439.
deep network with a local denoising criterion,” Journal of Machine
[144] Z. Zhang, P. Luo, C.-C. Loy, and X. Tang, “Learning social relation
LearningResearch,vol.11,no.Dec,pp.3371–3408,2010.
traits from face images,” in Proceedings of the IEEE International
[120] Q.V.Le,“Buildinghigh-levelfeaturesusinglargescaleunsupervised ConferenceonComputerVision,2015,pp.3631–3639.
learning,”inAcoustics,SpeechandSignalProcessing(ICASSP),2013
[145] Y. Guo, D. Tao, J. Yu, H. Xiong, Y. Li, and D. Tao, “Deep neural
IEEEInternationalConferenceon. IEEE,2013,pp.8595–8598.
networkswithrelativitylearningforfacialexpressionrecognition,”in
[121] S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio, “Con- Multimedia & Expo Workshops (ICMEW), 2016 IEEE International
tractive auto-encoders: Explicit invariance during feature extraction,” Conferenceon. IEEE,2016,pp.1–6.
in Proceedings of the 28th International Conference on International
[146] B.-K.Kim,S.-Y.Dong,J.Roh,G.Kim,andS.-Y.Lee,“Fusingaligned
ConferenceonMachineLearning. Omnipress,2011,pp.833–840.
and non-aligned face information for automatic affect recognition in
[122] J.Masci,U.Meier,D.Cires¸an,andJ.Schmidhuber,“Stackedconvolu-
the wild: A deep learning approach,” in Proceedings of the IEEE
tionalauto-encodersforhierarchicalfeatureextraction,”inInternational
Conference on Computer Vision and Pattern Recognition Workshops,
ConferenceonArtificialNeuralNetworks. Springer,2011,pp.52–59.
2016,pp.48–57.
[123] D.P.KingmaandM.Welling,“Auto-encodingvariationalbayes,”arXiv
[147] C. Pramerdorfer and M. Kampel, “Facial expression recognition us-
preprintarXiv:1312.6114,2013.
ing convolutional neural networks: State of the art,” arXiv preprint
[124] P.J.Werbos,“Backpropagationthroughtime:whatitdoesandhowto
arXiv:1612.02903,2016.
doit,”ProceedingsoftheIEEE,vol.78,no.10,pp.1550–1560,1990.
[148] O.M.Parkhi,A.Vedaldi,A.Zissermanetal.,“Deepfacerecognition.”
[125] S.HochreiterandJ.Schmidhuber,“Longshort-termmemory,”Neural
inBMVC,vol.1,no.3,2015,p.6.
computation,vol.9,no.8,pp.1735–1780,1997.
[149] T.Kaneko,K.Hiramatsu,andK.Kashino,“Adaptivevisualfeedback
[126] M. Mirza and S. Osindero, “Conditional generative adversarial nets,”
generation for facial expression improvement with multi-task deep
arXivpreprintarXiv:1411.1784,2014.
neural networks,” in Proceedings of the 2016 ACM on Multimedia
[127] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation
Conference. ACM,2016,pp.327–331.
learning with deep convolutional generative adversarial networks,”
[150] D.Yi,Z.Lei,S.Liao,andS.Z.Li,“Learningfacerepresentationfrom
arXivpreprintarXiv:1511.06434,2015.
scratch,”arXivpreprintarXiv:1411.7923,2014.
[128] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and O. Winther,
[151] X.Zhang,L.Zhang,X.-J.Wang,andH.-Y.Shum,“Findingcelebrities
“Autoencodingbeyondpixelsusingalearnedsimilaritymetric,”arXiv
inbillionsofwebimages,”IEEETransactionsonMultimedia,vol.14,
preprintarXiv:1512.09300,2015.
no.4,pp.995–1007,2012.
[129] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and
[152] H.-W. Ng and S. Winkler, “A data-driven approach to cleaning large
P.Abbeel,“Infogan:Interpretablerepresentationlearningbyinforma-
tion maximizing generative adversarial nets,” in Advances in neural face datasets,” in Image Processing (ICIP), 2014 IEEE International
informationprocessingsystems,2016,pp.2172–2180. Conferenceon. IEEE,2014,pp.343–347.
[130] Y.Tang,“Deeplearningusinglinearsupportvectormachines,”arXiv [153] H.Kaya,F.Gu¨rpınar,andA.A.Salah,“Video-basedemotionrecogni-
preprintarXiv:1306.0239,2013. tioninthewildusingdeeptransferlearningandscorefusion,”Image
[131] A.DapognyandK.Bailly,“Investigatingdeepneuralforestsforfacial
andVisionComputing,vol.65,pp.66–75,2017.
expressionrecognition,”inAutomaticFace&GestureRecognition(FG [154] B.Knyazev,R.Shvetsov,N.Efremova,andA.Kuharenko,“Convolu-
2018),201813thIEEEInternationalConferenceon. IEEE,2018,pp. tionalneuralnetworkspretrainedonlargefacerecognitiondatasetsfor
629–633. emotion classification from video,” arXiv preprint arXiv:1711.04598,
[132] P. Kontschieder, M. Fiterau, A. Criminisi, and S. Rota Bulo, “Deep 2017.
neural decision forests,” in Proceedings of the IEEE international [155] D.G.Lowe,“Objectrecognitionfromlocalscale-invariantfeatures,”in
conferenceoncomputervision,2015,pp.1467–1475. Computervision,1999.TheproceedingsoftheseventhIEEEinterna-
[133] J.Donahue,Y.Jia,O.Vinyals,J.Hoffman,N.Zhang,E.Tzeng,and tionalconferenceon,vol.2. Ieee,1999,pp.1150–1157.
T.Darrell,“Decaf:Adeepconvolutionalactivationfeatureforgeneric [156] T.Zhang,W.Zheng,Z.Cui,Y.Zong,J.Yan,andK.Yan,“Adeepneural
visual recognition,” in International conference on machine learning, network-drivenfeaturelearningmethodformulti-viewfacialexpression
2014,pp.647–655. recognition,” IEEE Transactions on Multimedia, vol. 18, no. 12, pp.
[134] A.S.Razavian,H.Azizpour,J.Sullivan,andS.Carlsson,“Cnnfeatures 2528–2536,2016.
off-the-shelf: an astounding baseline for recognition,” in Computer [157] Z.Luo,J.Chen,T.Takiguchi,andY.Ariki,“Facialexpressionrecogni-
VisionandPatternRecognitionWorkshops(CVPRW),2014IEEECon- tionwithdeepage,”inMultimedia&ExpoWorkshops(ICMEW),2017
ferenceon. IEEE,2014,pp.512–519. IEEEInternationalConferenceon. IEEE,2017,pp.657–662.
[135] N.Otberdout,A.Kacem,M.Daoudi,L.Ballihi,andS.Berretti,“Deep [158] L. Chen, M. Zhou, W. Su, M. Wu, J. She, and K. Hirota, “Softmax
covariance descriptors for facial expression recognition,” in BMVC, regression based deep sparse autoencoder network for facial emotion
2018. recognitioninhuman-robotinteraction,”InformationSciences,vol.428,
[136] D.Acharya,Z.Huang,D.PaniPaudel,andL.VanGool,“Covariance pp.49–61,2018.
poolingforfacialexpressionrecognition,”inProceedingsoftheIEEE [159] V. Mavani, S. Raman, and K. P. Miyapuram, “Facial expression
Conference on Computer Vision and Pattern Recognition Workshops, recognition using visual saliency and deep learning,” arXiv preprint
2018,pp.367–374. arXiv:1708.08016,2017.23
[160] M.Cornia,L.Baraldi,G.Serra,andR.Cucchiara,“Adeepmulti-level tive adversarial networks,” in Automatic Face & Gesture Recognition
networkforsaliencyprediction,”inPatternRecognition(ICPR),2016 (FG2018),201813thIEEEInternationalConferenceon. IEEE,2018,
23rdInternationalConferenceon. IEEE,2016,pp.3488–3493. pp.294–301.
[161] B.-F. Wu and C.-H. Lin, “Adaptive feature mapping for customizing [183] J. Chen, J. Konrad, and P. Ishwar, “Vgan-based image representation
deeplearningbasedfacialexpressionrecognitionmodel,”IEEEAccess, learningforprivacy-preservingfacialexpressionrecognition,”inPro-
2018. ceedings of the IEEE Conference on Computer Vision and Pattern
[162] J. Lu, V. E. Liong, and J. Zhou, “Cost-sensitive local binary feature RecognitionWorkshops,2018,pp.1570–1579.
learningforfacialageestimation,”IEEETransactionsonImagePro- [184] Y. Kim, B. Yoo, Y. Kwak, C. Choi, and J. Kim, “Deep generative-
cessing,vol.24,no.12,pp.5356–5368,2015. contrastivenetworksforfacialexpressionrecognition,”arXivpreprint
[163] W. Shang, K. Sohn, D. Almeida, and H. Lee, “Understanding and arXiv:1703.07140,2017.
improving convolutional neural networks via concatenated rectified [185] N. Sun, Q. Li, R. Huan, J. Liu, and G. Han, “Deep spatial-temporal
linearunits,”inInternationalConferenceonMachineLearning,2016, featurefusionforfacialexpressionrecognitioninstaticimages,”Pattern
pp.2217–2225. RecognitionLetters,2017.
[164] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Re- [186] W. Ding, M. Xu, D. Huang, W. Lin, M. Dong, X. Yu, and H. Li,
thinkingtheinceptionarchitectureforcomputervision,”inProceedings “Audio and face video emotion recognition in the wild using deep
oftheIEEEConferenceonComputerVisionandPatternRecognition, neuralnetworksandsmalldatasets,”inProceedingsofthe18thACM
2016,pp.2818–2826. InternationalConferenceonMultimodalInteraction. ACM,2016,pp.
[165] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi, “Inception-v4, 506–513.
inception-resnetandtheimpactofresidualconnectionsonlearning.”in [187] J. Yan, W. Zheng, Z. Cui, C. Tang, T. Zhang, Y. Zong, and N. Sun,
AAAI,vol.4,2017,p.12. “Multi-cluefusionforemotionrecognitioninthewild,”inProceedings
[166] S. Zhao, H. Cai, H. Liu, J. Zhang, and S. Chen, “Feature selection ofthe18thACMInternationalConferenceonMultimodalInteraction.
mechanismincnnsforfacialexpressionrecognition,”inBMVC,2018. ACM,2016,pp.458–463.
[167] J. Zeng, S. Shan, and X. Chen, “Facial expression recognition with [188] Z.Cui,S.Xiao,Z.Niu,S.Yan,andW.Zheng,“Recurrentshaperegres-
inconsistently annotated datasets,” in Proceedings of the European sion,”IEEETransactionsonPatternAnalysisandMachineIntelligence,
ConferenceonComputerVision(ECCV),2018,pp.222–237. 2018.
[168] Y. Wen, K. Zhang, Z. Li, and Y. Qiao, “A discriminative feature
[189] X.Ouyang,S.Kawaai,E.G.H.Goh,S.Shen,W.Ding,H.Ming,and
learningapproachfordeepfacerecognition,”inEuropeanConference
D.-Y. Huang, “Audio-visual emotion recognition using deep transfer
onComputerVision. Springer,2016,pp.499–515. learning and multiple temporal models,” in Proceedings of the 19th
[169] F.Schroff,D.Kalenichenko,andJ.Philbin,“Facenet:Aunifiedembed- ACM International Conference on Multimodal Interaction. ACM,
dingforfacerecognitionandclustering,”inProceedingsoftheIEEE 2017,pp.577–582.
conferenceoncomputervisionandpatternrecognition,2015,pp.815–
[190] V.Vielzeuf,S.Pateux,andF.Jurie,“Temporalmultimodalfusionfor
823.
video emotion classification in the wild,” in Proceedings of the 19th
[170] G.Zeng,J.Zhou,X.Jia,W.Xie,andL.Shen,“Hand-craftedfeature ACM International Conference on Multimodal Interaction. ACM,
guided deep learning for facial expression recognition,” in Automatic
2017,pp.569–576.
Face&GestureRecognition(FG2018),201813thIEEEInternational
[191] S. E. Kahou, X. Bouthillier, P. Lamblin, C. Gulcehre, V. Michal-
Conferenceon. IEEE,2018,pp.423–430.
ski, K. Konda, S. Jean, P. Froumenty, Y. Dauphin, N. Boulanger-
[171] D.Ciregan,U.Meier,andJ.Schmidhuber,“Multi-columndeepneural
Lewandowskietal.,“Emonets:Multimodaldeeplearningapproaches
networks for image classification,” in Computer vision and pattern
foremotionrecognitioninvideo,”JournalonMultimodalUserInter-
recognition(CVPR),2012IEEEconferenceon. IEEE,2012,pp.3642–
faces,vol.10,no.2,pp.99–111,2016.
3649.
[192] M.Liu,R.Wang,S.Li,S.Shan,Z.Huang,andX.Chen,“Combining
[172] G.PonsandD.Masip,“Supervisedcommitteeofconvolutionalneural
multiplekernelmethodsonriemannianmanifoldforemotionrecogni-
networksinautomatedfacialexpressionanalysis,”IEEETransactions
tioninthewild,”inProceedingsofthe16thInternationalConference
onAffectiveComputing,2017.
onMultimodalInteraction. ACM,2014,pp.494–501.
[173] B.-K.Kim,J.Roh,S.-Y.Dong,andS.-Y.Lee,“Hierarchicalcommittee
[193] B. Xu, Y. Fu, Y.-G. Jiang, B. Li, and L. Sigal, “Video emotion
of deep convolutional neural networks for robust facial expression
recognition with transferred deep feature encodings,” in Proceedings
recognition,” Journal on Multimodal User Interfaces, vol. 10, no. 2,
ofthe2016ACMonInternationalConferenceonMultimediaRetrieval.
pp.173–189,2016.
ACM,2016,pp.15–22.
[174] K.Liu,M.Zhang,andZ.Pan,“Facialexpressionrecognitionwithcnn
[194] J.Chen,R.Xu,andL.Liu,“Deeppeak-neutraldifferencefeaturefor
ensemble,” in Cyberworlds (CW), 2016 International Conference on.
facialexpressionrecognition,”MultimediaToolsandApplications,pp.
IEEE,2016,pp.163–166.
1–17,2018.
[175] G. Pons and D. Masip, “Multi-task, multi-label and multi-domain
[195] Q. V. Le, N. Jaitly, and G. E. Hinton, “A simple way to ini-
learningwithresidualconvolutionalnetworksforemotionrecognition,”
tialize recurrent networks of rectified linear units,” arXiv preprint
arXivpreprintarXiv:1802.06664,2018.
arXiv:1504.00941,2015.
[176] P.EkmanandE.L.Rosenberg,Whatthefacereveals:Basicandapplied
studies of spontaneous expression using the Facial Action Coding [196] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural net-
System(FACS). OxfordUniversityPress,USA,1997. works,”IEEETransactionsonSignalProcessing,vol.45,no.11,pp.
2673–2681,1997.
[177] R.Ranjan,S.Sankaranarayanan,C.D.Castillo,andR.Chellappa,“An
all-in-oneconvolutionalneuralnetworkforfaceanalysis,”inAutomatic [197] P.BarrosandS.Wermter,“Developingcrossmodalexpressionrecogni-
Face&GestureRecognition(FG2017),201712thIEEEInternational tionbasedonadeepneuralmodel,”Adaptivebehavior,vol.24,no.5,
Conferenceon. IEEE,2017,pp.17–24. pp.373–396,2016.
[178] Y. Lv, Z. Feng, and C. Xu, “Facial expression recognition via deep [198] J. Zhao, X. Mao, and J. Zhang, “Learning deep facial expression
learning,” in Smart Computing (SMARTCOMP), 2014 International features from image and optical flow sequences using 3d cnn,” The
Conferenceon. IEEE,2014,pp.303–308. VisualComputer,pp.1–15,2018.
[179] S.Rifai,Y.Bengio,A.Courville,P.Vincent,andM.Mirza,“Disentan- [199] M. Liu, S. Li, S. Shan, R. Wang, and X. Chen, “Deeply learning
glingfactorsofvariationforfacialexpressionrecognition,”inEuropean deformablefacialactionpartsmodelfordynamicexpressionanalysis,”
ConferenceonComputerVision. Springer,2012,pp.808–822. inAsianconferenceoncomputervision. Springer,2014,pp.143–157.
[180] Y.-H. Lai and S.-H. Lai, “Emotion-preserving representation learning [200] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan,
via generative adversarial network for multi-view facial expression “Object detection with discriminatively trained part-based models,”
recognition,” in Automatic Face & Gesture Recognition (FG 2018), IEEEtransactionsonpatternanalysisandmachineintelligence,vol.32,
201813thIEEEInternationalConferenceon. IEEE,2018,pp.263– no.9,pp.1627–1645,2010.
270. [201] S.Pini,O.B.Ahmed,M.Cornia,L.Baraldi,R.Cucchiara,andB.Huet,
[181] F. Zhang, T. Zhang, Q. Mao, and C. Xu, “Joint pose and expression “Modeling multimodal cues in a deep learning-based framework for
modelingforfacialexpressionrecognition,”inProceedingsoftheIEEE emotion recognition in the wild,” in Proceedings of the 19th ACM
Conference on Computer Vision and Pattern Recognition, 2018, pp. International Conference on Multimodal Interaction. ACM, 2017,
3359–3368. pp.536–543.
[182] H. Yang, Z. Zhang, and L. Yin, “Identity-adaptive facial expression [202] R.Arandjelovic,P.Gronat,A.Torii,T.Pajdla,andJ.Sivic,“Netvlad:
recognitionthroughexpressionregenerationusingconditionalgenera- Cnn architecture for weakly supervised place recognition,” in Pro-24
ceedings of the IEEE Conference on Computer Vision and Pattern of rgb-depth map latent representations,” in 2017 IEEE International
Recognition,2016,pp.5297–5307. ConferenceonComputerVisionWorkshop(ICCVW),2017.
[203] D. H. Kim, M. K. Lee, D. Y. Choi, and B. C. Song, “Multi-modal [224] H.Li,J.Sun,Z.Xu,andL.Chen,“Multimodal2d+3dfacialexpression
emotionrecognitionusingsemi-supervisedlearningandmultipleneural recognition with deep fusion convolutional neural network,” IEEE
networks in the wild,” in Proceedings of the 19th ACM International TransactionsonMultimedia,vol.19,no.12,pp.2816–2831,2017.
ConferenceonMultimodalInteraction. ACM,2017,pp.529–535. [225] A.Jan,H.Ding,H.Meng,L.Chen,andH.Li,“Accuratefacialparts
[204] J.Donahue,L.AnneHendricks,S.Guadarrama,M.Rohrbach,S.Venu- localizationanddeeplearningfor3dfacialexpressionrecognition,”in
gopalan,K.Saenko,andT.Darrell,“Long-termrecurrentconvolutional Automatic Face & Gesture Recognition (FG 2018), 2018 13th IEEE
networksforvisualrecognitionanddescription,”inProceedingsofthe InternationalConferenceon. IEEE,2018,pp.466–472.
IEEEconferenceoncomputervisionandpatternrecognition,2015,pp. [226] X.Wei,H.Li,J.Sun,andL.Chen,“Unsuperviseddomainadaptation
2625–2634. withregularizedoptimaltransportformultimodal2d+3dfacialexpres-
[205] D.K.Jain,Z.Zhang,andK.Huang,“Multiangleoptimalpattern-based sionrecognition,”inAutomaticFace&GestureRecognition(FG2018),
deep learning for automatic facial expression recognition,” Pattern 201813thIEEEInternationalConferenceon. IEEE,2018,pp.31–37.
RecognitionLetters,2017. [227] J. M. Susskind, G. E. Hinton, J. R. Movellan, and A. K. Anderson,
[206] M.Baccouche,F.Mamalet,C.Wolf,C.Garcia,andA.Baskurt,“Spatio- “Generating facial expressions with deep belief nets,” in Affective
temporalconvolutionalsparseauto-encoderforsequenceclassification.” Computing. InTech,2008.
inBMVC,2012,pp.1–12. [228] M. Sabzevari, S. Toosizadeh, S. R. Quchani, and V. Abrishami, “A
[207] S. Kankanamge, C. Fookes, and S. Sridharan, “Facial analysis in the fast and accurate facial expression synthesis system for color face
wild with lstm networks,” in Image Processing (ICIP), 2017 IEEE imagesusingfacegraphanddeepbeliefnetwork,”inElectronicsand
InternationalConferenceon. IEEE,2017,pp.1052–1056. InformationEngineering(ICEIE),2010InternationalConferenceOn,
[208] J.D.Lafferty,A.Mccallum,andF.C.N.Pereira,“Conditionalrandom vol.2. IEEE,2010,pp.V2–354.
fields:Probabilisticmodelsforsegmentingandlabelingsequencedata,” [229] R. Yeh, Z. Liu, D. B. Goldman, and A. Agarwala, “Semantic
ProceedingsofIcml,vol.3,no.2,pp.282–289,2001. facial expression editing using autoencoded flow,” arXiv preprint
arXiv:1611.09961,2016.
[209] K.SimonyanandA.Zisserman,“Two-streamconvolutionalnetworks
for action recognition in videos,” in Advances in neural information [230] Y. Zhou and B. E. Shi, “Photorealistic facial expression synthesis by
processingsystems,2014,pp.568–576. the conditional difference adversarial autoencoder,” in Affective Com-
puting and Intelligent Interaction (ACII), 2017 Seventh International
[210] J. Susskind, V. Mnih, G. Hinton et al., “On deep generative models
Conferenceon. IEEE,2017,pp.370–376.
with applications to recognition,” in Computer Vision and Pattern
[231] L.Song,Z.Lu,R.He,Z.Sun,andT.Tan,“Geometryguidedadversarial
Recognition (CVPR), 2011 IEEE Conference on. IEEE, 2011, pp.
facialexpressionsynthesis,”arXivpreprintarXiv:1712.03474,2017.
2857–2864.
[232] H. Ding, K. Sricharan, and R. Chellappa, “Exprgan: Facial expres-
[211] V.Mnih,J.M.Susskind,G.E.Hintonetal.,“Modelingnaturalimages
sioneditingwithcontrollableexpressionintensity,”inAAAI,2018,p.
usinggatedmrfs,”IEEEtransactionsonpatternanalysisandmachine
67816788.
intelligence,vol.35,no.9,pp.2206–2222,2013.
[233] F. Qiao, N. Yao, Z. Jiao, Z. Li, H. Chen, and H. Wang, “Geometry-
[212] V.Mnih,G.E.Hintonetal.,“Generatingmorerealisticimagesusing
contrastivegenerativeadversarialnetworkforfacialexpressionsynthe-
gated mrf’s,” in Advances in Neural Information Processing Systems,
sis,”arXivpreprintarXiv:1802.01822,2018.
2010,pp.2002–2010.
[234] I.Masi,A.T.Tran,T.Hassner,J.T.Leksut,andG.Medioni,“Dowe
[213] Y.Cheng,B.Jiang,andK.Jia,“Adeepstructureforfacialexpression
reallyneedtocollectmillionsoffacesforeffectivefacerecognition?”
recognitionunderpartialocclusion,”inIntelligentInformationHiding
in European Conference on Computer Vision. Springer, 2016, pp.
andMultimediaSignalProcessing(IIH-MSP),2014TenthInternational
579–596.
Conferenceon. IEEE,2014,pp.211–214.
[235] N. Mousavi, H. Siqueira, P. Barros, B. Fernandes, and S. Wermter,
[214] M.Xu,W.Cheng,Q.Zhao,L.Ma,andF.Xu,“Facialexpressionrecog-
“Understandinghowdeepneuralnetworkslearnfaceexpressions,”in
nitionbasedontransferlearningfromdeepconvolutionalnetworks,”in
Neural Networks (IJCNN), 2016 International Joint Conference on.
NaturalComputation(ICNC),201511thInternationalConferenceon.
IEEE,2016,pp.227–234.
IEEE,2015,pp.702–708.
[236] R.BreuerandR.Kimmel,“Adeeplearningperspectiveontheorigin
[215] Y.Liu,J.Zeng,S.Shan,andZ.Zheng,“Multi-channelpose-awarecon-
offacialexpressions,”arXivpreprintarXiv:1705.01842,2017.
volutionneuralnetworksformulti-viewfacialexpressionrecognition,”
[237] M.D.ZeilerandR.Fergus,“Visualizingandunderstandingconvolu-
inAutomaticFace&GestureRecognition(FG2018),201813thIEEE
tionalnetworks,”inEuropeanconferenceoncomputervision. Springer,
InternationalConferenceon. IEEE,2018,pp.458–465.
2014,pp.818–833.
[216] S.He,S.Wang,W.Lan,H.Fu,andQ.Ji,“Facialexpressionrecognition
[238] I.Lu¨si,J.C.J.Junior,J.Gorbova,X.Baro´,S.Escalera,H.Demirel,
using deep boltzmann machine from thermal infrared images,” in
J.Allik,C.Ozcinar,andG.Anbarjafari,“Jointchallengeondominant
AffectiveComputingandIntelligentInteraction(ACII),2013Humaine
andcomplementaryemotionrecognitionusingmicroemotionfeatures
AssociationConferenceon. IEEE,2013,pp.239–244.
and head-pose estimation: Databases,” in Automatic Face & Gesture
[217] Z. Wu, T. Chen, Y. Chen, Z. Zhang, and G. Liu, “Nirexpnet: Three- Recognition(FG2017),201712thIEEEInternationalConferenceon.
stream3dconvolutionalneuralnetworkfornearinfraredfacialexpres- IEEE,2017,pp.809–813.
sionrecognition,”AppliedSciences,vol.7,no.11,p.1184,2017.
[239] J. Wan, S. Escalera, X. Baro, H. J. Escalante, I. Guyon, M. Madadi,
[218] E.P.IjjinaandC.K.Mohan,“Facialexpressionrecognitionusingkinect J. Allik, J. Gorbova, and G. Anbarjafari, “Results and analysis of
depthsensorandconvolutionalneuralnetworks,”inMachineLearning chalearnlapmulti-modalisolatedandcontinuousgesturerecognition,
and Applications (ICMLA), 2014 13th International Conference on. andrealversusfakeexpressedemotionschallenges,”inChaLearnLaP,
IEEE,2014,pp.392–396. Action,Gesture,andEmotionRecognitionWorkshopandCompetitions:
[219] M.Z.Uddin,M.M.Hassan,A.Almogren,M.Zuair,G.Fortino,and Large Scale Multimodal Gesture Recognition and Real versus Fake
J.Torresen,“Afacialexpressionrecognitionsystemusingrobustface expressedemotions,ICCV,vol.4,no.6,2017.
featuresfromdepthvideosanddeeplearning,”Computers&Electrical [240] Y.-G.KimandX.-P.Huynh,“Discriminationbetweengenuineversus
Engineering,vol.63,pp.114–125,2017. fakeemotionusinglong-shorttermmemorywithparametricbiasand
[220] M. Z. Uddin, W. Khaksar, and J. Torresen, “Facial expression recog- faciallandmarks,”inComputerVisionWorkshop(ICCVW),2017IEEE
nition using salient features and convolutional neural network,” IEEE InternationalConferenceon. IEEE,2017,pp.3065–3072.
Access,vol.5,pp.26146–26161,2017. [241] L.Li,T.Baltrusaitis,B.Sun,andL.-P.Morency,“Combiningsequential
[221] W.Li,D.Huang,H.Li,andY.Wang,“Automatic4dfacialexpression geometryandtexturefeaturesfordistinguishinggenuineanddeceptive
recognition using dynamic geometrical image network,” in Automatic emotions,”inProceedingsoftheIEEEConferenceonComputerVision
Face&GestureRecognition(FG2018),201813thIEEEInternational andPatternRecognition,2017,pp.3147–3153.
Conferenceon. IEEE,2018,pp.24–30. [242] J.Guo,S.Zhou,J.Wu,J.Wan,X.Zhu,Z.Lei,andS.Z.Li,“Multi-
[222] F.-J.Chang,A.T.Tran,T.Hassner,I.Masi,R.Nevatia,andG.Medioni, modality network with visual and geometrical information for micro
“Expnet: Landmark-free, deep, 3d facial expressions,” in Automatic emotion recognition,” in Automatic Face & Gesture Recognition (FG
Face&GestureRecognition(FG2018),201813thIEEEInternational 2017),201712thIEEEInternationalConferenceon. IEEE,2017,pp.
Conferenceon. IEEE,2018,pp.122–129. 814–819.
[223] O. K. Oyedotun, G. Demisse, A. E. R. Shabayek, D. Aouada, and [243] I. Song, H.-J. Kim, and P. B. Jeon, “Deep learning for real-time
B. Ottersten, “Facial expression recognition via joint deep learning robust facial expression recognition on a smartphone,” in Consumer25
Electronics(ICCE),2014IEEEInternationalConferenceon. IEEE,
2014,pp.564–567.
[244] S.Bazrafkan,T.Nedelcu,P.Filipczuk,andP.Corcoran,“Deeplearning
for facial expression recognition: A step closer to a smartphone that
knows your moods,” in Consumer Electronics (ICCE), 2017 IEEE
InternationalConferenceon. IEEE,2017,pp.217–220.
[245] S. Hickson, N. Dufour, A. Sud, V. Kwatra, and I. Essa, “Eyemotion:
Classifyingfacialexpressionsinvrusingeye-trackingcameras,”arXiv
preprintarXiv:1707.07204,2017.
[246] S.A.Ossia,A.S.Shamsabadi,A.Taheri,H.R.Rabiee,N.Lane,and
H.Haddadi,“Ahybriddeeplearningarchitectureforprivacy-preserving
mobileanalytics,”arXivpreprintarXiv:1703.02952,2017.
[247] K. Kulkarni, C. A. Corneanu, I. Ofodile, S. Escalera, X. Baro,
S.Hyniewska,J.Allik,andG.Anbarjafari,“Automaticrecognitionof
facial displays of unfelt emotions,” arXiv preprint arXiv:1707.04061,
2017.
[248] X.Zhou,K.Jin,Y.Shang,andG.Guo,“Visuallyinterpretablerepre-
sentationlearningfordepressionrecognitionfromfacialimages,”IEEE
TransactionsonAffectiveComputing,pp.1–1,2018.
[249] E. Barsoum, C. Zhang, C. C. Ferrer, and Z. Zhang, “Training deep
networks for facial expression recognition with crowd-sourced label
distribution,”inProceedingsofthe18thACMInternationalConference
onMultimodalInteraction. ACM,2016,pp.279–283.
[250] J. A. Russell, “A circumplex model of affect.” Journal of personality
andsocialpsychology,vol.39,no.6,p.1161,1980.
[251] S.LiandW.Deng,“Deepemotiontransfernetworkforcross-database
facial expression recognition,” in Pattern Recognition (ICPR), 2018
26thInternationalConference. IEEE,2018,pp.3092–3099.
[252] M. Valstar, J. Gratch, B. Schuller, F. Ringeval, D. Lalanne, M. Tor-
resTorres,S.Scherer,G.Stratou,R.Cowie,andM.Pantic,“Avec2016:
Depression,mood,andemotionrecognitionworkshopandchallenge,”
in Proceedings of the 6th International Workshop on Audio/Visual
EmotionChallenge. ACM,2016,pp.3–10.
[253] F. Ringeval, B. Schuller, M. Valstar, J. Gratch, R. Cowie, S. Scherer,
S. Mozgai, N. Cummins, M. Schmitt, and M. Pantic, “Avec 2017:
Real-life depression,and affect recognition workshop andchallenge,”
inProceedingsofthe7thAnnualWorkshoponAudio/VisualEmotion
Challenge. ACM,2017,pp.3–9."
56,58,Deep imbalanced learning for face recognition and attribute prediction,"['C Huang', 'Y Li', 'CC Loy', 'X Tang']",2019,390,Toronto Face Database,"deep learning, machine learning","To mitigate this issue, contemporary deep learning methods typically follow classic strategies  such as class re- Given an imagery dataset with imbalanced class distribution, our goal is to",No DOI,… pattern analysis and machine …,https://arxiv.org/abs/1806.00194,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"1
Deep Imbalanced Learning for Face Recognition
and Attribute Prediction
Chen Huang, Yining Li, Chen Change Loy, Senior Member, IEEE and Xiaoou Tang, Fellow, IEEE
Abstract—Dataforfaceanalysisoftenexhibithighly-skewedclassdistribution,i.e.,mostdatabelongtoafewmajorityclasses,while
theminorityclassesonlycontainascarceamountofinstances.Tomitigatethisissue,contemporarydeeplearningmethodstypically
followclassicstrategiessuchasclassre-samplingorcost-sensitivetraining.Inthispaper,weconductextensiveandsystematic
experimentstovalidatetheeffectivenessoftheseclassicschemesforrepresentationlearningonclass-imbalanceddata.Wefurther
demonstratethatmorediscriminativedeeprepresentationcanbelearnedbyenforcingadeepnetworktomaintaininter-cluster
marginsbothwithinandbetweenclasses.Thistightconstrainteffectivelyreducestheclassimbalanceinherentinthelocaldata
neighborhood,thuscarvingmuchmorebalancedclassboundarieslocally.Weshowthatitiseasytodeployangularmarginsbetween
theclusterdistributionsonahyperspheremanifold.SuchlearnedCluster-basedLargeMarginLocalEmbedding(CLMLE),when
combinedwithasimplek-nearestclusteralgorithm,showssignificantimprovementsinaccuracyoverexistingmethodsonbothface
recognitionandfaceattributepredictiontasksthatexhibitimbalancedclassdistribution.
IndexTerms—ImbalancedLearning,DeepConvolutionalNeuralNetworks,FaceRecognition,AttributePrediction
(cid:70)
1 INTRODUCTION
MANY data in face analysis domain naturally exhibit option is cost-sensitive learning, which assigns higher mis-
imbalanceintheirclassdistribution.Forinstance,the classificationcoststotheminorityclassthantothemajority.
numbers of positive and negative face pairs in face verifi- Caesar et al. [18] proposed to calibrate an ensemble of
cation [1], [2] are highly skewed since it is easier to obtain SVMs with inverse class frequencies as costs to combat
face images with different identities (negative) than faces classimbalanceinsemanticsegmentation.Similarlyindeep
with matched identity (positive) during data collection. CNNs,thelossfunctionisrescaledwiththeinverse[19],rel-
For face attribute prediction [3], it is comparatively easy ative[20]andmedian[21]classfrequencies,respectivelyfor
to find persons with “normal-sized nose” attribute from semanticsegmentation,faceattributepredictionandmulti-
web images than that of “big-nose”. Such face recognition tasksceneunderstanding.Forimageedgedetection[22],the
and attribute prediction problems provide perfect testbeds softmax loss of CNN is regularized with equal weights for
forstudyinggenericimbalancedlearningalgorithms,either the positive and negative edge classes. An alternative [23]
under closed- or open-set protocol [4]. Indeed, without goes beyond conventional cost-sensitive strategies by re-
handlingtheimbalanceissueconventionalmethodstendto weighting the contributions of spatial image pixels based
be biased toward the majority class with poor accuracy for ontheiractualobservedlossesforsemanticsegmentation.
theminorityclass[5],[6]. Can these methods help the deep face recognition and
Deeprepresentationlearninghasrecentlyachievedgreat attribute prediction tasks where data imbalance is barely
success due to its high learning capacity, but still cannot handled?Arethesemethodsthemosteffectivewaytodeal
escape from the negative impact of imbalanced data. To with data imbalance in the context of deep representation
counter such negative effect, one often chooses from a few learning? The aforementioned options are well studied for
available options, which have been extensively studied in the ‘shallow’ model [24] but their implications have not
the past [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], yet been systematically studied for deep representation
[16]. The first option is re-sampling, which aims to balance learning. Importantly, such schemes are well-known for
the class priors by under-sampling the majority class or some inherent limitations. For instance, over-sampling can
over-sampling the minority class (or both). For instance, easily introduce undesirable noise with increased compu-
Oquab et al. [17] resampled the number of foreground tational cost and overfitting risk. Under-sampling is of-
andbackgroundimagepatchesforlearningaconvolutional ten preferred [9] but it may remove valuable information.
neural network (CNN) for object classification. The second Cost-sensitivelearningapproachesoftendesigncostsusing
heuristicsorstaticclasslabelstatistics.
Such nuisance factors can be equally applicable to the
• C.HuangwaswiththeRoboticsInstitute,CarnegieMellonUniversity,
Pittsburgh,PA,15213. recent deep imbalanced learning methods based on such
E-mail:chenh2@andrew.cmu.edu common schemes. Methods in this line [25], [26], [27], [28]
• C. C. Loy is with the School of Computer Science and Engineering, allfailtoprovidenoticeableimprovementsinperformance.
NationalTechnologicalUniversity,Singapore.
Two advances [29], [30] excel by providing new insights.
E-mail:ccloy@ntu.edu.sg
• Y.LiandX.TangarewiththeDepartmentofInformationEngineering, Wangetal.[29]proposedameta-networktotransferknowl-
TheChineseUniversityofHongKong. edge(modelparameters)fromthemajorityclasstominority
E-mail:{ly015,xtang}@ie.cuhk.edu.hk
class. Thus the model dynamics between many-shot and
9102
rpA
03
]VC.sc[
2v49100.6081:viXra2
imbalance,leadingtomuchmorebalancedclassboundaries
locally (Fig. 1). We demonstrate the margins can be deter-
ministicallyderivedonahyperspherefeaturespace.Wealso
studytheeffectivenessofclassicschemesofre-samplingand
cost-sensitivelearninginourcontext.
Using the learned feature representation, we show the
evaluation can be simply done by a soft k-nearest-cluster
metric which is consistent with our learning objective. The
proposed approach, called Cluster-based Large Margin Local
Embedding (CLMLE), achieves the new state-of-the-art per-
formance on several face recognition datasets using only
smalltrainingdata.CLMLEalsodrasticallyoutperformsthe
standardsoftmaxandtripletlossesandsurpassestherecent
imbalancedlearningmethods.Forfaceattributeprediction,
Fig. 1. Example of class imbalance for the binary face attribute “wear CLMLE achieves superior performance measured by the
hat”.Ourmethodaimstoseparatetheclusterdistributionsbothwithin
balancedaccuracyacrossmultipleattributes.
and between classes. This effectively reduces the class imbalance in
localneighborhoodsandformsbalancedlocalclassboundariesthatare A preliminary version of this work has been published
insensitivetotheimbalancedsizeofremainingclasssamples. in [32]. This work extends the initial method LMLE [32] in
significant ways. (1) Sampling of quintuplets (composed of
5 data points) in LMLE is generalized to the sampling of
few-shot models is learned, achieving superior classification
entire clusters. This alleviates the training inefficiencies of
results on existing imbalanced datasets like ImageNet [31].
LMLEduetotheexponentialgrowthofquintupletnumber.
Dong et al. [30] proposed a Class Rectification Loss (CRL)
Also,penalizingtheoverlapbetweenclusterdistributionsis
that can further handle imbalanced multi-label attributes
muchmorecoherentthanpenalizingindividualquintuplets
andismorerelatedtoourgoal.CRLperformshardmining
or triplets, leading to faster and better convergence than
fortheminorityattributeclassesineachbatch,andenforces
LMLEandtripletloss[33].(2)AnewCLMLElossfunction
their feature constraints to rectify the learning bias of the
isproposed,withcustomizedclusterre-samplingandcost-
conventionalcrossentropyloss.However,regularizationfor
sensitive learning techniques. (3) We design angular mar-
only minority class cannot guarantee equal learning for all
gins to be enforced between the involved cluster distribu-
classes,andthelearningqualityandspeedcanbehindered
tions.ThisismorenaturalthanenforcingEuclideandistance
by the hard mining online that sees one batch at a time,
onahyperspheremanifoldasinLMLE.(4)Theeffectiveness
where a global characterization of feature space is lacking
of CLMLE is validated in the imbalanced tasks of not only
forcorrectregularization.
face attribute prediction but also face recognition, where
Inthispaper,wewishtoinvestigateamoreeffectiveap- the additional open-set scenario demonstrates the superior
proachfordeepimbalancedlearning.Weshowitsimportant generalizationabilityofCLMLE.
applications to face recognition1 and attribute prediction
fromubiquitouslyimbalanceddatasets.Notesuchtaskscan
beevaluatedundereitherclosed-oropen-setprotocol.The
2 RELATED WORK
open-set protocol is harder since the testing classes may be Previous efforts to tackle the class imbalance problem can
unseenfromthetrainingclasses.Itusuallyrequiresdiscrim- be mainly divided into two groups: data re-sampling [5],
inative feature representations with built-in large margins, [6], [7], [9], [10], [11], [15] and cost-sensitive learning [8],
whichareembodiedinourapproach. [12], [13], [14], [16]. The former group aims to alter the
Our method is motivated by the observation that the training data distribution to learn equally good classifiers
minority class often contains very few instances with high forallclasses,usuallybyrandomunder-samplingandover-
degreeofvisualvariability.Thescarcityandhighvariability sampling techniques. The latter group, instead of manipu-
make the genuine neighborhood of these instances easy to latingsamplesatthedatalevel,operatesatthealgorithmic
be invaded by other imposter nearest neighbors2. Such in- level by adjusting misclassification costs. A comprehensive
vasionwillconfusetheunderlyingclassboundariesformed literaturesurveycanbefoundin[5],[6].
byeitheralocalorglobalclassifier.Tothisend,wepropose A well-known issue with replication-based random
tolearnanembeddingf(x)∈RdwithaCNNtoameliorate
over-sampling is its tendency to overfit. More radically, it
theinvasion.TheCNNistrainedwithamaintainedindexof does not increase any information and fails in solving the
clusters for each class, which we will update continuously fundamental “lack of data” problem for the minority class.
throughout training. Our objective, then, would enforce Tothisend,SMOTE[7]createsnewnon-replicatedexamples
margins between hard-mined clusters in the local neigh- by interpolating neighboring minority class instances. Sev-
borhood from both the same and different classes. Such eralvariantsofSMOTE[10],[11],[15]followedforimprove-
marginsintroduceatightconstraintforreducinglocaldata ments. However, their broadened decision regions are still
error-pronebysynthesizingborderlineexamples.Therefore
1.Facerecognitioncanbecategorizedasfaceidentification(i.e.,clas- under-sampling is often preferred to over-sampling [9],
sifyonefacetoaspecificidentity)andfaceverification(i.e.,determine
althoughpotentiallyvaluableinformationmayberemoved.
whetherapairoffacesbelongtothesameidentity).
2.Animposterneighborofadatapointxi isanotherdatapointxj Cost-sensitive methods avoid these issues by directly im-
withadifferentclasslabel,yi(cid:54)=yj. posing heavier cost on misclassifying the minority class.3
Currently, how to determine the cost representation is still learning, and combine softmax with contrastive [41], [42],
anopenproblem.Commonly-agreedpracticesincludeusing [43],centerloss[44]ormarginalloss[45]forimprovements.
the inverse class frequency or pre-defined misclassification Another popular choice is the triplet loss [33], [46], which
costs, and they are typically applied to SVMs [12] and leads to state-of-the-art performance. The recent methods
decisiontrees[16].Boosting[13]offersanothernaturalway that handle class imbalance augment the minority classes
to embed the costs in example weights. Many other meth- inthe imagespace [47]andfeature space[48],respectively.
odsfollowthisphilosophyofdesigningclassifierensemble Others align the feature centers [49] or weight norms [50]
(e.g., [8], [14]) to combat imbalance. In [8], [14], the authors of the minority classes to the majority. Zhang et al. [51]
combined cost sensitivity with bagging which is less vul- proposed a range loss to minimize the intra-class variance
nerabletonoisethanboosting,andgeneratedcost-sensitive basedonthelargestintra-classdistances(ranges)computed
versionofrandomforests. regardlessoftheimbalancedclasssize.Ourmethodcomple-
Deep imbalanced learning. To our knowledge, only few ments these methods by providing a data structure-aware
works [25], [26], [27], [28], [29], [30], [34], [35] approach loss function that enforces margins between local data to
imbalanced learning via deep models. Jeatrakul et al. [25] reducetheimbalanceinanylocalneighborhood.
treated the Complementary Neural Network as an under- Deep face attribute prediction. Face attributes are useful
sampling technique, and combined it with SMOTE-based as mid-level features for many applications like face ver-
over-samplingtore-balancedata.ZhouandLiu[28]studied ification [3], [52]. It is challenging to predict them from
dataresamplingfortrainingcost-sensitiveneuralnetworks. unconstrained face images due to the large facial varia-
In [26], [27], the cost-sensitive deep features and the cost tions. Most existing methods utilize part-based models to
parameter are jointly optimized. All these works can be extract features from the localized part regions, and then
seen as direct “deep” extensions of traditional imbalanced train SVM classifiers to predict the presence of an array
learning techniques. Alternatives [34], [35] simply tune the of face attributes, e.g., “male” and “smile”. For example,
networks to maximize a class-balanced accuracy measure. Kumar et al. [3] extracted HOG-like features from various
More recently, Ren et al. [36] proposed to reweight batch localfaceregionsforattributeprediction.Recentdeeplearn-
samplesbasedontheirgradientdirectionsonline,whichcan ing methods [53], [54], [55] excel by learning powerful fea-
combatimbalance.However,acleanunbiasedvalidationset tures. Kalayeh et al. [55] further combined a deep semantic
is needed to represent the target distribution. Liu et al. [37] segmentation network to guide attribute prediction to the
proposedtomaximizethehypersphericalmarginregardless correspondinglocalregion.Thesestudies,however,sharea
of class imbalance, while Wang et al. [29] proposed a meta- common drawback: they neglect the class imbalance issue
learning approach that transfers model parameters from inthoserelativelyrareattributeslike“bignose”and“bald”.
the majority to minority class. Both methods achieve good To our knowledge, only two works handle class imbalance
classificationresultsonimbalanceddatasets. in attribute prediction. The Mixed Objective Optimization
Unfortunately, none of the above works takes into ac- Network (MOON) [20] re-weights attributes in a cost-
count the data structure of imbalanced classes which helps sensitivemanner,andtheClassRectificationLoss(CRL)[30]
learning.OneexceptionistheClassRectificationLoss(CRL) performsonlineregularizationforminorityattributeclasses
in [30], where “hard” minority classes are searched in each inbatch.Ourmethodshowsastrongerimbalancedlearning
batch and are regularized in feature space to rectify the abilitywithanewlossfunction.
learning bias of conventional loss, e.g., cross entropy. How-
ever, feature regularization for only minority class cannot
3 LEARNING DEEP REPRESENTATION FROM
guarantee equal learning for all classes. We propose here a
“structure-aware” approach by enforcing large margins be-
CLASS-IMBALANCED DATA
tweenintra-classandinter-classclusters.Thisway,balanced Given an imagery dataset with imbalanced class distri-
class boundaries can be equally drawn for every class from bution, our goal is to learn an embedding function f(x)
its involved local clusters. We show the clusters provide from an image x into a feature space Rd, such that the
a global characterization of class distributions, leading to embedded features are discriminative with local class im-
faster and better convergence than purely online methods balance ameliorated. We constrain this embedding to live
like CRL [30] where the global information is missing. We on a d-dimensional hypersphere, i.e., ||f(x)|| 2 = 1. Such
also show our cluster separation rule can be easily applied normalizationiscommonplaceinexistingembeddingmeth-
to both training and testing, where the clustering process ods (e.g., triplet embedding [33]), in order to achieve scale
onlyincursnegligiblecomputationalcostduringtraining. invariance under different image conditions, e.g., lighting,
Deep face recognition. Softmax loss has been pioneering contrastandsoon.
effective CNN models for deep face recognition [1], [2], To achieve the above learning goal, we start by giving
includingrecognitionundertheopen-setprotocol[4].How- a brief review of the challenges with existing embedding
ever,open-setrecognition,unliketheclosed-setone,cannot methodsthathindertheirperformanceonclass-imbalanced
be addressed as a classification problem of known face data.Theywillmotivateourworktofollow.
identities as in softmax. Open-set scenario usually requires
morediscriminativelylearnedfeatureswithbuilt-inmargin.
3.1 ChallengeswithExistingEmbeddingMethods
Recent L-Softmax loss [38], A-Softmax loss [39], and Large
Margin Cosine Loss (LMCL) [40] generalize softmax by Triplet loss. The triplet loss [33] and contrastive loss [41]
enforcing large angular margin between classes to enhance aretwopopularapproachestolearnaEuclideanembedding
featurediscrimination.Otherworksadoptideasfrommetric functionf(x).Sometripletvariants[56],[57]wererecently4
Fig. 2. The 2-D feature space of triplet loss [33], A-Softmax loss [39], Large Margin Local Embedding (LMLE) [32] and the proposed Cluster-
basedLargeMarginLocalEmbedding(CLMLE).Classimbalanceisexemplifiedinabinary-classcase.ThetripletandA-Softmaxlossesenforce
Euclideanandangularmarginsrespectivelyatclass-level,assumingeachclasscanbecapturedbyasinglemode.Suchunimodaldiscrimination
imposestoostrongofarequirement,andmayfailtocollapsethemajorityclasswithlargervariationandleadtoclassoverlap.TheLMLEenforces
Euclideanmarginsamongquintupleexamplessampledfromtheintra-andinter-classlocalclusters.Suchconstraintpreservesdiscriminationin
localneighborhoodandhelpsformlocalclassboundariesthatareinsensitivetotheimbalancedclasssize.TheproposedCLMLEsamplesthe
entireclusterdistributionsinsteadtoaddresstheinefficiencyandinconsistencyissueswithquintupletsamplinginLMLE.CLMLEalsonaturally
facilitatesthederivationofangularmarginsbetweenclusterdistributionsontheunithypersphere.
proposed for improvements. For simplicity here, we only neighbors or even domination of the majority class in local
usethevanillatripletlosstoexemplifytheinefficacyofthis neighborhood.
lineofmethodswhenhandlingclassimbalance. A-Softmax loss. The A-Softmax loss in SphereFace [39]
Triplet embedding is trained on a set of triplets P = enhancesthediscriminationpowerofSoftmaxbyimposing
{(x i,xp i,xn i)}wherex i istheanchorpoint,associatedwith theangularmargininahyperspherespace.However,itstill
thepositiveandnegativeexamplesxp i andxn i.Thesampling suffersfromtheunimodalassumptionofclassdistributions,
of triplets is usually semantic, informed by class labels: which is insufficient on imbalanced data (see Fig. 2). In the
(x i,xp i) come from the same class, and (x i,xn i) come from traditionalSoftmaxloss,theclassscores j forsamplex i can
different classes. Then the goal is to enforce semantic sim- bewrittenass
j
=WWWT jf(x i)=(cid:107)WWW j(cid:107)(cid:107)f(x i)(cid:107)cos(θ j)where
ilarity between the feature embeddings f(·;Θ) extracted WWW j is the j-th column of fully connected layer WWW, and θ j
by one CNN with parameters Θ (we will omit Θ later for istheanglebetweenWWW
j
andf(x i).InA-Softmaxloss,each
brevity). More precisely, the objective is to push away the WWW j isnormalized(cid:107)WWW j(cid:107)=1,∀j andthelossbecomes:
negative example xn i from the anchor x i in the embedding
space by a Euclidean distance margin g > 0 compared to (cid:32) (cid:33)
thepositiveexamplexp
: J =
−1 (cid:88)
log
e(cid:107)f(xi)(cid:107)ψ(θyi)
,
D(f(x
i),f(xi
p i))+g <D(f(x i),f(xn i)), (1)
ang |P|
i∈P
e(cid:107)f(xi)(cid:107)ψ(θyi)+(cid:80) j(cid:54)=yie(cid:107)f(xi)(cid:107)cos(θj)
(3)
where x i has the class label y i ∈ [1,C], and j (cid:54)= y i are
whereD(f(x i),f(x j))=(cid:107)f(x i)−f(x j)(cid:107)2 2 istheEuclidean the other labels in the training set P. The function ψ(·)
distance. incorporates an angular margin for the class y i on unit
Thecostfunctionisdefinedas: hypersphere.Obviously,duetotheenforcementofangular
marginsatclasslevel,theintra-classstructuresarelostagain
J = 1 (cid:88) [D(f(x ),f(xp))−D(f(x ),f(xn))+g] , andtheissueswithtripletlossonimbalanceddataapplyto
tri |P| i i i i +
i∈P theA-Softmaxlossaswell.
(2)
Large Margin Local Embedding (LMLE). Our previously
where[·]
+
=max(0,·)denotesthehingefunction.
proposedLMLE[32]addressestheclass-imbalanceissueby
It is widely observed that the convergence rate and
assuming that each class’s distribution can be represented
performance of triplet embeddings are hindered by two
as a variant number of clusters. Then we are able to draw
factors: 1) the cubic growth of the number of triplets, and
balanced class boundaries only among the involved local
2) the penalty on individual triplets not being necessarily
clusters, not at the whole class level anymore. To this end,
consistent. Hard negative mining [33] (semi-hard negative
quintupletinstances(seeFig.2),sampledfromclustersboth
mininginthepaper)isoneofthewaystoimprovethetriplet
within and between classes, are used as hard examples to
quality and hence the learning efficiency. However, hard
formimplicitlocalboundaries:
miningstillcannotconquerthelimitationofthetripletloss
in capturing the structure of imbalanced data. Specifically, • x i :ananchor,
the similarity information is only extracted at the class-level • xp i+ :theanchor’smostdistantwithin-clusterneighbor,
by triplets. This tends to equally collapse the intra-class • xp i− : the nearest within-class neighbor of the anchor,
variation and encourage unimodal discrimination. But it is butfromadifferentcluster,
easy to tell that the difficulty of collapsing different classes • xp i−−: the most distant within-class neighbor of the
varies. Fig. 2 shows an imbalanced binary case. Obviously anchor,
itismoredifficulttocollapsethemajorityclasswithlarger • xn i :thenearestbetween-classneighboroftheanchor.
variation than minority class. In case of collapsing failure,
it would immediately lead to the invasion of imposter We wish to ensure that the following relationship holds5
intheembeddingspace: them. The number of clusters K = (cid:98)L c/l(cid:99) is adaptively
D(f(x ),f(xp+))<D(f(x ),f(xp−))
determinedforeachclassthathassizeL c.Notehereweuse
i i i i the inner product as the similarity metric, which is more
< D(f(x i),f(xp i−−))<D(f(x i),f(xn i)). (4) suitablethanEuclideandistanceontheunithypersphere.It
alsofacilitatesthederivationofourangularmargins,aswill
Such a fine-grained relationship embraces the multi-
beelaboratednext.
modality of class distribution: it preserves not only local-
Duringtraining,weareinterestedinaglobalcharacteri-
ity across the same-class clusters but also discrimination
zation of data neighborhoods and penalizing any overlap
between classes. As a result, it is capable of preserving
of cluster distributions in a coherent way. Specifically, at
discrimination in any local neighborhood, and forming lo-
c Oa tl hc el ras irs reb lo eu vn and tar si ae ms pw leit sh inth ae cm lao ss st ad reis ec fr fi em ctin iva et li yve “is ga nm op rele ds ”. nea ec ah ret sr ta cin luin stg erit se {ra It mio }n M m, =w − 11e fr re ot mrie bv oe thfo tr ha eq su amer ey ac nlu dst de ir ffI e1 rei nts
t
forclassseparation,makingthelocalboundariesinsensitive classes in the neighborhood. We define µ m as the centroid
to imbalanced class sizes. To enforce the relationship in
of cluster I m, and let c(µ m) denote the class label of µ m.
Then local discrimination can be achieved by making all
Eq. (4), a triple-header hinge loss is formulated to constrain
the clusters inter-distinct and intra-compact, so that each
threemarginsbetweenthefourEuclideandistances:
clusterisabletorepresentauniquemodeforfuturediscrim-
1 (cid:88) ination.Concretely,wemaximizetheintra-clustersimilarity
J = (ε +τ +σ ), s.t.:
lmle |P| i i i betweenf(x i)anditscontainingclustercentroidµ m,while
i∈P
(cid:104)
g 1+D(f(x i),f(xp i+))−D(f(x i),f(xp
i−))(cid:105)
+
≤ε i,
om thin ei rm ci lz ui sn tg erth cee ni tn rote idr- scl {u µs kte :kr (cid:54)=s mim }.il War eit uy sb ee tt hw ee ie nn nef r( px ri) oda un cd
t
(cid:104) (cid:105) againtocharacterizesimilarityanddefineourlossfunction
g +D(f(x ),f(xp−))−D(f(x ),f(xp−−)) ≤τ ,
2 i i i i i asfollows:
+
∀(cid:104) g i,3+ ε iD ≥( 0f ,(x τi i) ≥,f 0(x ,p i σ− i− ≥)) 0−D(f(x i),f(xn i))(cid:105) + ≤σ i, (5) J clmle = M1 l m(cid:88)M =1i(cid:88) ∈Im(cid:34) −log (cid:80) ke :f k(cid:54)=(x mi)T eµ fm (x− i)a Tµk(cid:35) +, (8)
where ε i,τ i,σ i are the slack variables, g 1,g 2,g 3 are the whereaisanangularmarginontheunitcircle,thedesired
enforced Euclidean distance margins that can be explicitly angulargapbetweentheclustercentroidµ m andothers.
determinedonthehyperspheremanifold. The above loss function actually penalizes the probabil-
Despite being effective on imbalanced data, quintuplets ity assigned to the example x i of a particular cluster µ m
have similar sampling issues as triplets — the exponential under the distribution of another by a large margin a. This
growth of quintuplet number and the potential inconsis- willgraduallycollapseeachclusterdistributionintoasmall
tency between sampled quintuplets, both of which can region and form safe margins between adjacent regions
hindertheconvergencerateandquality.Thisworkgeneral- on the hypersphere. More importantly, those “marginal”
izes to sample the entire cluster distributions for learning. clusters near the class bounds will implicitly draw a bal-
We will demonstrate the significantly improved learning anced classification boundary between them. As a result,
efficiency and consistency, and hence better discrimination the class imbalance is effectively reduced locally, ignoring
intheimbalancedcontext. theimpactofotherfar-awayclusters.Toguaranteesufficient
class separation between those marginal clusters, we inject
theinformationofclasslabelintoEq.(8),enforcingalarger
3.2 Cluster-basedLargeMarginLocalEmbedding
angular margin for those between-class clusters than for
We propose to learn Cluster-based Large Margin Local
same-classclusters:
Embedding (CLMLE) from all the examples in contextual
clusters, rather than only hard examples (quintuplets) in 1 (cid:88)M (cid:88) (cid:34) ef(xi)Tµm−a1 (cid:35)
J = −log
c
t fh
elu
ae
ts ult oe rr
c
es
a
v.
l
edC
ci
tl
s
ou
t
rs
r
sit be {r
u
fi tn (iog
xn
i)st }e oc ah
f
rn eci
l
aq
u
lu
s
wte
e
as
r
ysa sfr noe
r
ore
e
mm
ve
ap
r
lil
y
zo ey
c
dle ad
oss
n.t to
oSi
tnc ha
c
ep
e
ut ou nur ie
r
t
clmle M +l m
(cid:34)
−=1 loi∈ gIm ef(cid:80) (xk i):c T( µµ mk) −(cid:54)= ac 2(µm)ef(xi (cid:35))Tµ ,k +
(9)
hypersphere (see Fig. 2), we choose the spherical k-means (cid:80) k:k(cid:54)=m,c(µk)=c(µm)ef(xi)Tµk
+
algorithm for clustering. Suppose we have the training set
P ={(x i,y i)}L
i=1
withsamplex
i
andclasslabely
i
∈[1,C]. where a 1 and a 2 are the inter-cluster angular margins en-
Thenforeachclassc,wehaveK clusterassignments: forced between different classes and within the same class,
respectively. We will introduce how to explicitly derive a 1
I 1c,...,I Kc = argmax(cid:88)K (cid:88) f(x i)Tµc k, (6) a Cn od ma p2 al ra iste or n.
s with related embedding methods. The new
I 1c,...,I Kc k=1i∈I kc loss in Eq. (9) is similar in spirit to the probabilistic model
µc
k
= |I1
c|
(cid:88) f(x i), µc
k
= (cid:107)µ µc k c(cid:107), (7) o isf tN hae tar wes et aC dl oa pss
t
M
a
e na on n-( uN nC ifoM rm) [5 a8 s] s. umTh pe tiom nai fn ord ti hff eer ce ln ac se
s
k i∈Ic k
k distribution, each modeled as a mixture of homogeneous
where {µc}K are the cluster centroids in class c. We gen- clusters.Thisiswellsuitedtoclass-imbalanceddataandal-
k k=1
erateclusterswiththesamesize|Ic|=ltoensureanequal lowsforlocaldiscriminationusingonlyabalancedsubsetof
k
complexity to collapse these clusters during embedding clusters.WhileinNCM,thehomogeneousclassassumption
learning,andtodrawimplicitbalancedboundariesbetween isadopted,whichisinvalidinimbalancedsettings.Another6
Fig.3.Speedandperformanceanalysesinfacerecognition(onLFW[1]) Fig.4.Extreme2Dcasesforderivingtheupperboundsofinter-cluster
and face attribute (on CelebA [53]) tasks. We compare convergence angularmarginsbetweenclass(am 1ax)andwithinclass(am 2ax).
speedbythenumberofseentrainingdata,whichisfairandindependent
ofdifferencesinGPU,batchsizeusedandotherfactors.Wealsoreport
training times for the face attribute task. The compared methods are wouldpeakwhenallclustersoreventhewholeclassesare
triplet loss [33], Center loss [44], A-Softmax [39], LMCL [40], Large collapsed into single points on the hypersphere and they
Margin Local Embedding (LMLE) [32] and our Cluster-based Large
are all widely separated in between. In such extreme case,
MarginLocalEmbedding(CLMLE).CLMLEstrikesagoodperformance- (cid:80)
speedtrade-off. eachunnormalizedclustercentroidµ m = i∈Imf(x i)/l is
identicaltotheclustermembersthushasunitnormtoo.The
difference is that we learn deep feature representations inner product f(x i)Tµ k = (cid:107)f(x i)(cid:107)(cid:107)µ k(cid:107)cos(θ k) = cos(θ k)
ratherthanusefixedonesasinNCM. inEq.(9)nowdependsontheangleθ k only,whiletheintra-
Whencomparedwiththeembeddingmethodsbasedon
clustersimilarityisfixedasf(x i)Tµ
m
= cos(θ m) = cos(0).
individual triplets [33] or quintuplets [32], our method has Hence we can define the maximum angular margin in
twomainmerits:1)Oursamplingofentireclustersislinear the form of cos(0) − cos(θ k). Easily, we have am 1 ax =
to the size of training data O(N), thus significantly avoids cos(0) − cos(2π/C) when each class becomes a single
thesamplingcomplexityoftripletsO(N3)andquintuplets point with the maximum inter-class angle θ k = 2π/C.
O(N5)andimprovesthetrainingefficiency.2)Ourlossin- am
2
ax =cos(0)−cos(2πL c/L)whentheconsideredclusters
formedofentireclusterdistributionshasasufficientinsight are most far apart in their belonging class that occupies a
of contextual neighborhoods. By separating all clusters at
proportionL c/Lofthehypersphericalspace.
once, the training is more coherent and globally consistent Cluster updates. It is worth noting that the cluster assign-
than training with individual triple or quintuple exam- ment I 1c,...,I Kc for each class c in Eq. (6) is initialized
ples. Fig. 3 demonstrates our CLMLE indeed converges usingthefeaturesfromapre-trainedCNN.Sincethefeature
much faster than the triplet loss [33] and quintuplet-based representations {f(x i)} are updated continuously during
LMLE[32],whileachievingbetterperformanceatthesame training, we should gradually update the clusters as well
time. In comparison to other state-of-the-art embedding on the newly learned features to reflect their true distri-
methods (e.g., Center loss [44]), CLMLE strikes a better bution. To this end, for each class, we maintain a running
performance-speedtrade-off. index of clusters and refresh it after a fixed number of
The recent Class Rectification Loss (CRL) [30] performs iterations using the latest features. The computational cost
regularizationforthe“hard”minorityclassesineverybatch. oftheclusteringalgorithmofsphericalk-meansisnegligible
By contrast, our CLMLE method performs equal learning comparedtothecostoflearningCNNfeatures.
for both minority and majority classes over batches. More Embedding visualization. Fig. 5 visualizes the 2-D space
importantly, the clustering step for all classes informs our of the initial embedding and final converged CLMLE in an
learning process of the global feature distribution. It en- imbalanced face attribute example. Triplet embedding [33]
courages faster and better learning convergence than the as a representative unimodal learning method, is included
purelyonlinemethodCRL,atnegligiblecost.OntheCelebA for comparison. Since triplets operate at the class-level and
dataset, for instance, CLMLE converges about 15 times homogeneously collapse each class, we see their inabil-
fasterthanCRL. ity to capture the fine-grained variations (represented by
Angular margin derivation.Onegoodcharacteristicofour clusters) within the imbalanced classes, which leads to a
loss function in Eq. (9) is that the margins a 1 and a 2 can large overlap between the minority (positive) and majority
beexplicitlyderivedfollowingageometricintuition.These (negative) classes. Such issue equally applies to the other
margins are designed angular, which translates well to the unimodal methods of the contrastive loss [41], advanced
innerproductbasedsimilaritymetriconaunithypersphere. triplet loss [56], and angular losses like A-Softmax [39]
Angularmarginsalsosharethefavorablepropertiesofscale and L-Softmax [38]. By contrast, our CLMLE explores the
androtationinvariance. multimodalpropertyofclassbyenforcingangularmargins
Fig. 4 illustrates how to set a 1 and a 2 properly. Obvi- both within and between classes. As demonstrated in the
ously, their lower bounds are zero. Then we derive their figure, this helps to learn unique clusters of balanced size
theoretical upper bounds amax and amax in 2D space, to in each class, which are able to draw balanced local class
1 2
provideareasonableparameterrangeforgridsearchduring boundaries for discrimination. We will show this can also
training. Recall that a 1 denotes the inter-cluster angular boost the feature generalization in open-set scenarios (for
marginfromdifferentclasses,anda 2denotestheonewithin face verification) that often require discriminative features
the same class. It is easy to first find the angular gap withbuilt-inmargins.ItisworthnotingthatWangetal.[60]7
Fig.5.The2-Dfeaturespaceusingt-SNE[59]andpairwisefeaturesimilarityforonebinaryfaceattributefromtheCelebAdataset[53].Weonly
show2PositiveClusters(PC)and5NegativeClusters(NC)torepresenttheclassimbalance.Theembeddingofapre-trainedmodel,ourCLMLE,
andtripletembeddingarecompared.Wecanseethatbetween-classclusters(withdifferentcolors)arewellseparatedinCLMLE,buttheyare
overlappedintripletembedding,leadingtooverlappingbinaryscoredistributions.
aimtolearnfine-grainedsimilaritywithinclassaswell,but gives those less frequent classes more importance. For the
theydonotexplicitlyrelyonclusteringtechniquestomodel problem of predicting multiple attribute labels from a face,
thewithin-classvariations. cost-sensitivityhelpsevenmorebybeingabletore-balance
alllabelssimultaneously.Notere-samplingmulti-labeldata
3.3 Overall Training Procedure with Re-Sampling and is structurally infeasible because sampling to balance one
Cost-Sensitivity labelwillaffectthesamplingofothers.
During training, we construct one mini-batch with a query OveralltrainingprocedureforCLMLE:
clusterI 1anditsretrievedM−1nearestclusters{I m}M m=− 11 1) Cluster for each class by spherical k-means using
by computing µT 1µ m. We greedily make sure the retrieved the latest features. Initially, we use the features
clusters would come from both the same and different extractedbythepre-trainedCNN.
classes.Sincetheclustersizel = 200isnotsmall,thebatch 2) For CNN training, repeatedly construct mini-
size Ml will be very large even if we only retrieve a small batches with one query cluster (with highest loss)
number of clusters M. This is not viable for training due from a random class, and the M −1 = 11 nearest
to memory constraints. On the other hand, we observed clusters retrieved from both the same and different
empirically that using only a few clusters (i.e., low cluster classes.
diversity) hurts performance. Thus we choose to sample a 3) Randomly sample 20 examples for each of the
small portion of data in each considered cluster to increase M =12clustersinbatch,andcomputetheircluster
theclusternumberM whilemaintainingareasonablebatch centroids.
size. In practice, we randomly sample 20 examples out of
4) Compute the loss in Eq. (9) with cost-sensitivities
l = 200 from each of M = 12 clusters. The batch size is
(by inverse class frequency). Back-propagate the
240. The cluster centroid in Eq. (9) is then approximated as
gradients to update the CNN parameters and fea-
(cid:80)
µˆ
m
= i=1,...,20,i∈Imf(x i)/20.Suchclusterdatasampling
tureembeddings.
is simply repeated during CNN training. It avoids large
5) Alternate between step 1 and 2-4 periodically until
information loss as in traditional random under-sampling
convergence(oftenwithin5alternationrounds).
techniques. When compared with over-sampling, it intro-
ducesnoartificialnoise.
So far, one important problem is still left unaddressed: 4 FAST EVALUATION WITH NEAREST CLUSTERS
howtosamplethequeryclusterI 1inmini-batch.Wefound Our learned CLMLE offers crucial feature representations
a random sampling strategy is not ideal for performance. for accurate evaluation on imbalanced data. We choose a
Herewesimplyfollowthecommonpracticesofre-sampling simple k-nearest cluster algorithm which is consistent with
and cost-sensitive techniques as detailed below. Section 5 our training objective. The nearest neighbor rule is appeal-
willquantifytheirefficacysystematically. ingduetoitsnon-parametricnature,anditiseasytoextend
Re-samplingofqueryclusterI 1.Toensureadequatelearn- tonewclasseswithoutretraining.
ingforallclasses,wesampleI 1 evenlyfrombothmajority Specifically, for a query q, we retrieve its N nearest
and minority classes. To determine the exact query cluster clusters{I m}N m=1fromallthetrainingclassesbycomputing
tousefromthechosenclass,weborrowtheideaof [23]to f(q)Tµ m,anddecideitsclasslabelby:
pick I 1 as the one with the highest observed loss in class. min ef(q)Tµm
This allows us to adapt to the current feature distribution
a wn id thf no ec iu gs hbo on rinth ge oh na er sd .e Inst pc rl au cs tt ie cr e,t th ha et lh oa ss
s
ola fr age clo uv ste er rla ip
s
y q =a c=rg 1,m ...a ,Cx (cid:80)m: kc :( cµ (m µk) )= (cid:54)=c cef(q)Tµk, (10)
computed by averaging the losses of cluster members that where the label y q is predicted as the class whose least
arecachedonline. similar cluster is more similar than the clusters from other
Cost-sensitive learning in batch. Note the query and re- classesbythelargestmargin.
trieved clusters are most likely to be class-imbalanced in Such testing procedure makes the nearest neighbor
onemini-batch.Wefollowthecommonlyusedcost-sensitive search a function of the cluster number (cid:98)L/l(cid:99) rather than
approach [18], [19] to scale the loss of each sample by oftheexamplenumberL.Wefurtherspeedupthecluster-
the inverse class frequency in mini-batch. For multi-way wise search using the KD-tree [61] whose runtime is log-
classification (e.g., in face identification), this effectively arithmic in the cluster number ((cid:98)L/l(cid:99)) with a complexity8
TABLE1
TheCNNarchitectureandpriorfeaturesforinitialclusteringinour
consideredimbalancedtasks.
Task Network Priorfeatures
Facerecognition Samew.r.t.[39] Pre-trainedbysoftmax
Faceattributes Samew.r.t.[41] DeepID2featuresin[41]
from 20 : 10 : 200 on validation set. The task-specific
networkarchitectureandpriorfeaturesforinitialclustering
are summarized in Table 1. Note the prior features are not
criticaltothefinalresultsbecausewewillgraduallyupdate
thedeepfeaturesinalternationwiththeclusteringprocess.
Thealternationhappensevery2k-5kiterations,depending
onthetask-specificconvergencerate.Usuallydifferentprior
featuresconvergetosimilarresults.Forourfaceverification
(on LFW [1]) and attribute prediction experiments, differ-
ent prior features lead to accuracy difference of no more
than 0.02% (pre-trained by softmax vs. triplet) and 0.3%
Fig. 6. Imbalanced data distribution for face recognition and face at- (pre-trained by face recognition vs. multi-attribute softmax
tributeprediction.(a)Long-taileddistributionofimagenumberperclass
classification)respectively.
onCASIA-WebFacedataset[43].(b)40binaryfaceattributesonCelebA
dataset[53],eachwithimbalancedpositiveandnegativesamples. Computational cost.Forfacerecognitionandfaceattribute
prediction, it takes about 1.5 and 3 days respectively to
trainCLMLEonGPU(NVIDIATeslaK40),bothconverged
of O(L/llog(L/l)). This leads to up to three orders of within 5 alternation rounds. The clustering process in each
magnitude speedup over standard example-wise search in round incurs negligible cost compared to feature learning.
practice, making it easy to scale to large datasets. It is CLMLE has about 1.3 - 2 times faster convergence than its
worth mentioning that we use such a simple testing algo- earlier version LMLE [32] (see Fig. 3) thanks to the avoid-
rithm to show the efficacy of our learned features. Better anceofquintuplet-basedsamplingandlearning.Fortesting,
performance is expected with the use of more elaborated it takes 10ms to extract features, and the cluster-wise kNN
algorithms. searchistypically1000timesfasterthanstandardkNN.This
enables real-time application to large-scale problems with
hundredsofthousandstomillionsofsamples.
5 EXPERIMENTS
We study the face recognition and face attribute prediction
5.1 ExperimentalSettings
tasks,bothwithlarge-scaleimbalanceddatasets(seeFig.6).
The face recognition task is cast as either an identification Face recognition. All faces and landmarks are detected
(multi-way classification of a face) or verification (binary by MTCNN [62] for the training and testing images. The
classificationofafacepair)problem,evaluatedintheopen- detectedlandmarks(twoeyes,noseandmouthcorners)are
set scenario. As shown in the figure, the available training usedforsimilaritytransformation,andthefacesarecropped
datasetofCASIA-WebFace[43]ishighlyimbalanced.Some to112×96pixels.Eachpixel(in[0,255])incroppedimages
classes of the 10k subjects have hundreds of images, while isnormalizedbysubtracting127.5andthendividingby128.
about 39% of them have no more than 20 images. The Fortraining, we use the same CNN architecture as
average number of images per class is 42.8. Such data in [39], [40] with 64 convolutional layers based on residual
imbalancemakesitdifficultforlearningequallygoodclass units. This enables fair comparison with the recent strong
representations which are essential for comparing paired methods and their variants. For the same reason, we use
images from different classes. The face attribute prediction a small training set, the publicly available CASIA-WebFace
taskiscastasa(closed-set)multi-taskclassificationproblem. dataset [43] which contains 0.45M face images from 10,575
Weaimtopredict40binaryattributessimultaneously,each identities. The training images with identities appearing in
with imbalanced positive and negative samples (e.g., for our test sets are already removed. Note the scale of our
“Bald” attribute: 2% vs. 98%). In this case, the variant training data is much smaller than that of other private
imbalancelevelinmulti-labeldatamakestheproblemeven datasetsusedinDeepFace[4](4M),VGGFace[63](2M)and
harder. FaceNet [33] (200M). In practice, we apply data augmenta-
Parameters. Our CNN is trained using Caffe with fixed tionbyflippingthetrainingimageshorizontally.
momentum 0.9, weight decay λ = 0.0005. The learning Fortesting, we extract the deep features f(x) from the
rate starts with 0.1 and 0.001 for face recognition and at- output of the FC1 layer. The features of the original image
tribute prediction respectively, and is divided by 10 when and flipped image are concatenated to obtain the final face
the performance plateaus. We have cluster size l = 200 representation. The inner product between normalized fea-
and M = 12 clusters in one batch. The down-sampled M turesisthencalculatedasthesimilarityscore.Intheclosed-
clusters leads to the batch size 240. For testing, the optimal setscenariowithfixedclassset,facerecognitioncanbedone
number of retrieved clusters N in Eq. (10) is searched byusingthek-nearestclusterruleinEq.(10);whileinopen-9
set scenarios with new classes, identification and verifica- TABLE2
tionarerespectivelyconductedbyrankingandthresholding Faceverificationaccuracy(%)onLFW[1]andYTF[64]datasets.Most
methodsinthefirstcelluselarge-scaleoutsidedatathatarenot
thesimilarityscoresfollowingtheconvention,whichwillbe
publiclyavailable(+denotesdataexpansion).Thesecondcellincludes
showntostillbenefitfromourlearnedCLMLE. recentimbalancedlearningmethods.Thestate-of-the-artloss
We test on three popular large-scale datasets: LFW [1], functionsinthesecond-to-lastcellandoursinthelastcellusethe
smalltrainingdata(CASIA-WebFace[43]:0.45M)andthesame
YTF [64] and MegaFace [2]. LFW dataset contains 13,233
64-layerCNNmodelforfaircomparison.
web images from 5,749 face identities captured in uncon-
strainedconditions.YTFdatasetcontains3,425videosfrom
Method #Nets Traindata LFW YTF
1,595 identities. Each video varies from 48 to 6,070 frames,
DeepFace[4] 3 4M 97.35 91.4
withanaveragelengthas181.3frames.Bothdatasetsexhibit FaceNet[33] 1 200M 99.63 95.1
large facial variations in pose, expression and lighting. We Web-scale[65] 4 4.5M 98.37 -
VGGFace[63] 1 2.6M 98.95 97.3
follow the standard protocol of unrestricted with labeled
DeepID2+[42] 25 0.3M 99.47 93.2
outside data [1] for both datasets, and test on 6k face pairs Baidu[46] 1 1.3M 99.13 -
from LFW and 5k video pairs from YTF. MegaFace dataset CenterFace[44] 1 0.7M 99.28 94.9
Marginalloss[45] 1 4M 99.48 95.98
is a very challenging benchmark for face recognition at the
NoisySoftmax[66] 1 WebFace+ 99.18 94.88
million scale of distractors. The gallery set in MegaFace CoColoss[67] 1 2M 99.86 -
contains more than 1M images from 690k identities, while Rangeloss[51] 1 1.5M 99.52 93.7
the probe set consists of two existing datasets — Facescrub Augmentation[47] 1 WebFace 98.06 -
Centerinvariantloss[49] 1 WebFace 99.12 93.88
and FGNET. We choose the larger Facescrub dataset as
Featuretransfer[48] 1 4.8M 99.37 -
our probe set which contains 106,863 face images from 530
Softmaxloss 1 WebFace 97.88 93.1
celebrities.Wereportthefaceidentificationandverification Softmax+Contrastive[41] 1 WebFace 98.78 93.5
resultsundertwoprotocols(smallorlargetrainingset). Tripletloss[33] 1 WebFace 98.70 93.4
L-Softmaxloss[38] 1 WebFace 99.10 94.0
Forevaluation, we not only use the simple metrics of
Softmax+Centerloss[44] 1 WebFace 99.05 94.4
the identification or verification accuracy, but also the True SphereFace(A-Softmax)[39] 1 WebFace 99.42 95.0
AcceptRate(TAR)atfixedFalseAcceptRate(FAR)aswell CosFace(LMCL)[40] 1 WebFace 99.33 96.1
as ROC curves that can take into account any imbalance in LMLE[32] 1 WebFace 99.51 95.8
CLMLE 1 WebFace 99.62 96.5
testingpairs.
Faceattributes.WeusetheCelebAdataset[53]thatcontains
202,599 images from 10,177 identities, each with about 20
images (the training set is regarded as small only if it
images. Every face image is annotated with 40 attributes
and 5 key points to align the image to 55×47 pixels. We contains no more than 0.5M images). While the proposed
method only uses the small, publicly available training
partitionthedatasetfollowing[53]:thefirst162,770images
data(CASIA-WebFace[43]with0.45Mimages)andasingle
(i.e., 8k identities) for training (10k images for validation),
model. Our CLMLE achieves the best performance in this
thefollowing19,867imagesfortrainingSVMclassifiersfor
setting - 99.62% on LFW and 96.5% on YTF. By taking
thePANDA[54]andANet[53]methods,andtheremaining
intoaccounttheclassimbalanceduringtraining,oursingle
19,962imagesfortesting.Theidentitiesarenon-overlapping
CLMLEmodelevenoutperformsorperformscloselytothe
in these splits. During training, horizontal flipping is ap-
FaceNet[33]whichusesaround200Moutsidetrainingdata,
plied for data augmentation. We use the CNN architecture
andDeepID2+[42]whichensembles25models.
from [41] as in ANet [53], LMLE [32] and CRL [30]. One
extra 64-d FC layer is learned for each binary attribute via For a fair comparison with recent loss functions, we
Eq.(9)inamulti-taskmanner. use the same WebFace training data and the same 64-layer
For testing, we extract the deep features f(x) for each CNNarchitecture.The comparedlossfunctionsaretrained
attribute from its FC layer, and classify attributes via with their default hyper-parameters. One can observe that
Eq. (10) under the closed-set classification scenario. To ac- ourCLMLEconsiderablyoutperformstheSoftmaxvariants
count for the imbalanced positive and negative samples (including L-Softmax [38], A-Softmax [39], and LMCL [40])
for each attribute, we adopt a balanced accuracy metric and metric learning methods (including contrastive [41],
accuracy = 0.5(t p/N p + t n/N n), where N p and N n are triplet [33] and center loss [44]). They all suffer from the
the numbers of positive and negative samples, while t p unimodal assumption of class distribution which is not
and t n are the numbers of true positives and true neg- suitableinclass-imbalancedscenarios.
atives. Note this metric differs from the one employed When compared with the recent imbalanced learning
in [53], i.e., accuracy = ((t p +t n)/(N p +N n)) which can methods trained on WebFace or larger data (second cell in
bebiasedtothemajorityclass. Table 2), CLMLE is shown to achieve consistent gains. The
methodsofminorityclassaugmentationinimagespace[47]
andfeaturespace[48]obtainsub-optimalresults,whilethe
5.2 FaceRecognition
range loss [51] and center invariant loss [49] are not as
Experiments on LFW and YTF. Table 2 lists the face verifi- discriminative as our CLMLE. Our approach demonstrates
cation accuracy on the two datasets. Existing state-of-the- superior feature discrimination by enforcing large margins
art face verification systems (in the first cell) either use between local clusters from imbalanced classes. Moreover,
large training data or model ensemble. For example, both CLMLE improves over its previous version LMLE [32] by
the top performing CoCo loss [67] on LFW (99.86%) and discriminating entire cluster distributions rather than indi-
VGG Face [63] on YTF (97.3%) use more than 2M training vidual quintuplets sampled from them. Our large margin10
TABLE3
FacerecognitiononMegaFaceChallenge1[2]undertheprotocolsof
smallandlargetrainingset.“Rank1”referstorank-1faceidentification
accuracy(%)with1Mdistractors,and“Veri.”referstofaceverification
TAR(%)under10−6FAR.Thestate-of-the-artlossfunctionsinthe
second-to-lastcellandimbalancedlearninglossfunctionsinthelast
cellusethesmalltrainingdata(CASIA-WebFace[43]:0.45M)andthe
same64-layerCNNmodelforfaircomparison.
Method Protocol Rank1 Veri.
BeijingFaceAll Norm 1600 Large 64.80 67.11
Google-FaceNetv8 Large 70.49 86.47
Fig. 7. CMC and ROC curves of top performing methods with 1M
NTechLAB-facenx large Large 73.30 85.08
distractorsonMegaFaceChallenge1[2].NotetheFaceNetv8follows
SIATMMLABTencentVision Large 74.20 87.27
thelargetrainingsetprotocol,whileothermethodsfollowthesmallone.
DeepSenseV2 Large 81.29 95.99
YouTuLab Large 83.29 91.34
Vocord-deepVoV3 Large 91.76 94.96
SIAT MMLAB Small 65.23 76.72
DeepSense-Small Small 70.98 82.85
SphereFace-Small Small 75.76 90.04
BeijingFaceAllV2 Small 76.66 77.60
GRCCV Small 77.67 74.88
FUDAN-CS SDS Small 77.98 79.19
CoColoss[67] Small 76.57 -
Softmaxloss Small 54.85 65.92
Softmax+Contrastive[41] Small 65.21 78.86
Tripletloss[33] Small 64.79 78.32
L-Softmaxloss[38] Small 67.12 80.42 Fig. 8. Challenging pairs (green: positive pair; red: negative pair) that
Softmax+Centerloss[44] Small 65.49 80.14 arecorrectlyrecognizedbyourmethod.
SphereFace(A-Softmax)[39] Small 72.72 85.56
CosFace(LMCL)[40] Small 77.11 89.88 TABLE4
Rangeloss[51] Small 72.94 83.62 FacerecognitiononMegaFaceChallenge2[68]underthelarge
LMLE[32] Small 78.53 89.45 trainingsetprotocol.“Rank1”referstorank-1faceidentification
CLMLE Small 79.68 91.85 accuracy(%)with1Mdistractors,and“Veri.”referstofaceverification
TAR(%)under10−6FAR.TheSphereFace[39],CosFace[40]and
imbalancedlearningmethodsinthelastcellusethesame64-layer
CNNmodelforfaircomparison.
feature learning method is particularly suitable for gener-
alization test in the open-set face recognition problem, as
Method Protocol Rank1 Veri.
verifiedbytheaboveexperiments.
3DiVi Large 57.04 66.45
Experiments on MegaFace Challenge 1. The feature gen- NEC Large 62.12 66.84
eralizationcanbefurthertestedintheMegaFaceChallenge GRCCV Large 75.77 74.84
SphereFace(A-Softmax)[39] Large 71.17 84.22
1 [2] where the gallery set contains more than 1M distrac-
CosFace(LMCL)[40] Large 74.11 86.77
tors. For the face identification task which aims to match
Rangeloss[51] Large 69.54 82.67
oneprobeimagetotheimageswiththesamepersoninthe LMLE[32] Large 74.76 87.78
gallery,thelargeMegaFacegallerysetposesabigchallenge CLMLE Large 76.26 89.41
and naturally causes data imbalance. For face verification,
weshoulddecideifanimagepaircontainsthesameperson
ornot.Thereare4billionnegativepairsgeneratedbetween and softmax loss, with weights suggested by the authors.
the probe and gallery sets, which are imbalanced with ItisevidentfromthetablethatCLMLEoutperformsinthe
respecttotheavailablepositivepairs. imbalancehandlingability.
Table 3 first reports the face identification and verifica- Fig. 7 shows the Cumulative Match Characteristics
tionresultsintermsofthesimplerank-1identificationaccu- (CMC)curves(forfaceidentification)aswellastheReceiver
racy and verification True Accept Rate (TAR) at fixed False Operating Characteristic (ROC) curves (for verification)
Accept Rate (FAR). Our training set CASIA-WebFace [43] that can take into account any imbalance in testing pairs.
has 0.45M images, thus we follow the small training set Note the MegaFace gallery set contains different scales of
protocol.Underthisprotocol,ourCLMLEperformsbestfor distractors, from 10 to 1M with increasing difficulty. The
boththeidentificationandverificationtasks.Forsomemod- figure shows the curves at 1M scale for the top perform-
elstrainedunderthelargetrainingsetprotocol,e.g.,Google- ing methods under small and large protocols. We can see
FaceNetv8with500MimagesandNTechLAB-facenx large that our CLMLE method achieves the new state-of-the-art
with18Mimages,theyarealsobeatenbyourCLMLEwith performance(seeFig.8forsomevisualresults).
small training data. In comparison to the recent models Experiments on MegaFace Challenge 2. For MegaFace
trained with the same small data and network architecture Challenge 2 [68], all the algorithms use the same training
(in second-to-last cell), our CLMLE shows better results data provided by MegaFace and follow the large training
andsuperiorgeneralizationabilityagain.Wefurtherimple- set protocol. The training set includes 4.7M images from
mentedtherangeloss[51](areportedlycompetitiveimbal- 672kidentities.Thegallerysetcontains1Mimagesthatare
ancedlearningmethod)andLMLE[32]inthesamesettings. different from those in Challenge 1. Table 4 illustrates our
Fortherangeloss,weformedaweightedcombinationofit CLMLEattainsthetopperformanceforbothfaceidentifica-11
TABLE5
Meanper-classaccuracy(%)andclassimbalancelevel(=|positiveclassrate-50|%)ofeachofthe40binaryattributesonCelebAdataset[53].
Attributesaresortedbytheimbalancelevelinanascendingorder.Thesecondandthirdcellsliststate-of-the-artmethodsandimbalanced
learningmethods,respectively.NotetheresultsofbalancedaccuracyaredifferentfromtheclassificationaccuracyreportedbyANet[53].Also,
MOON(-D)andAFFACT(-D)usenetworksofVGG-16andResNet-50,muchlargerthanoursasdescribedin[41].Thesuffix’D’denotesdomain
adaptationtothebalancedattributedistribution,theunderlyingdistributionsuggestedbyourbalancedevaluationmetric.
evitcarttA
nepOhtuoM
gnilimS
kcitspiLraeW
senobkeehChgiH
elaM
puekaMyvaeH
riaHyvaW ecaFlavO
esoNytnioP
sworbeyEdehcrA
riaHkcalB spiLgiB esoNgiB
gnuoY
riaHthgiartS riaHnworB
seyErednUsgaB
sgnirraEraeW
draeBoN
sgnaB
Imbalancelevel 1 2 2 3 5 8 11 18 22 22 23 26 26 27 28 29 30 30 31 33 35
Triplet-kNN[33] 83 92 92 91 86 91 88 77 61 61 73 82 55 68 75 63 76 63 69 82 81
PANDA[54] 85 93 98 97 89 99 95 78 66 67 77 84 56 72 78 66 85 67 77 87 92
ANet[53] 87 96 97 95 89 99 96 81 67 69 76 90 57 78 84 69 83 70 83 93 90
Down-sampling[9] 78 87 90 91 80 90 89 70 58 63 70 80 61 76 80 61 76 71 70 88 88
MOON[20] 82 94 93 94 87 98 91 79 65 66 78 86 59 72 79 68 81 70 80 88 92
Over-sampling[9] 77 89 90 92 84 95 87 70 63 67 79 84 61 73 75 66 82 73 76 88 90
Cost-sensitive[5] 78 89 90 91 85 93 89 75 64 65 78 85 61 74 75 67 84 74 76 88 90
AFFACT[69] 83 94 93 94 88 98 91 84 66 66 80 87 62 76 82 75 83 76 86 94 92
CRL-I[30] 83 95 93 94 89 96 84 79 66 73 80 90 68 80 84 73 86 80 83 94 95
MOON-D[20] 82 94 93 94 87 98 91 81 69 71 82 88 66 77 84 75 85 81 86 94 94
AFFACT-D[69] 83 94 93 94 88 98 92 86 72 72 84 89 69 79 86 82 86 83 89 95 95
LMLE[32] 88 96 99 99 92 99 98 83 68 72 79 92 60 80 87 73 87 73 83 96 98
CLMLE 90 97 99 98 94 99 98 87 72 78 86 95 66 85 90 80 89 82 86 98 99
riaHdnolB
sworbeyEyhsuB
ecalkceNraeW
seyEworraN
wodahSkcolc’o5 enilriaHgnideceR
eitkceNraeW
sessalgeyE
skeehCysoR
eetaoG
ybbuhC snrubediS
yrrulB
taHraeW
nihCelbuoD
nikSelaP riaHyarG ehcatsuM
dlaB
egarevA
Imbalancelevel 35 36 38 38 39 42 43 44 44 44 44 44 45 45 45 46 46 46 48
Triplet-kNN[33] 81 68 50 47 66 60 73 82 64 73 64 71 43 84 60 63 72 57 75 71.55
PANDA[54] 91 74 51 51 76 67 85 88 68 84 65 81 50 90 64 69 79 63 74 76.95
ANet[53] 90 82 59 57 81 70 79 95 76 86 70 79 56 90 68 77 85 61 73 79.58
Down-sampling[9] 85 75 66 61 82 79 80 85 82 85 78 80 68 90 80 78 88 60 79 77.45
MOON[20] 91 79 57 58 78 69 82 97 70 84 66 82 71 91 66 69 81 68 83 78.59
Over-sampling[9] 90 80 71 65 85 82 79 91 90 89 83 90 76 89 84 82 90 90 92 81.48
Cost-sensitive[5] 89 79 71 65 84 81 82 91 92 86 82 90 76 90 84 80 90 88 93 81.60
AFFACT[69] 91 80 70 64 86 76 90 99 79 90 76 91 74 95 74 75 87 74 89 82.69
CRL-I[30] 95 84 74 72 90 87 88 96 88 96 87 92 85 98 89 92 95 94 97 86.60
MOON-D[20] 94 86 76 75 90 88 93 99 89 94 87 95 86 96 88 91 94 93 96 87.02
AFFACT-D[69] 94 87 81 79 92 88 94 99 91 96 89 95 91 98 90 92 95 95 97 88.84
LMLE[32] 99 82 59 59 82 76 90 98 78 95 79 88 59 99 74 80 91 73 90 83.83
CLMLE 99 88 69 71 91 82 96 99 86 98 85 94 72 99 87 94 96 82 95 88.78
tion (76.26% rank-1 accuracy) and face verification (89.41%
TAR under 10−6 FAR). This validates the importance of
tackling imbalance for feature representation learning, and
ourCLMLEoutperformsitspreviousversionLMLEandthe
rangeloss[51]forimbalancedlearning.Moreover,ourlarge
margin nature is shown to significantly improve the open-
set recognition performance. By contrast, the range loss as
a regularization method over softmax, is more suitable to
closed-setclassificationproblems.
5.3 FaceAttributePrediction
Table 5 compares our CLMLE method for face attribute
prediction with the state-of-the-art methods of Triplet- Fig. 9. Relative gains over the state-of-the-art methods without im-
kNN [33], PANDA [54] and ANet [53] which are trained balance handling mechanism across the 40 imbalanced attributes on
CelebA[53].
on the same data and tuned to their best performance. The
attributes and their mean per-class accuracies are given in
the ascending order of class imbalance level (= |positive
class rate-50|%). This is to highlight the impact of class variant imbalance levels of different attributes need to be
imbalance on performance. Note the CelebA dataset [53] handledsimultaneously.
alsoposeschallengestojointattributeprediction,wherethe It is shown that CLMLE outperforms these methods12
TABLE6
Averagebalancedaccuracyandclassificationaccuracy(%)forthe40
binaryattributesonCelebAdataset[53].
Method AttCNN[70] SSP[55] CLMLE
Averagebalancedacc. - 88.24 88.78
Averageclassificationacc. 90.97 91.67 91.13 Fig.10.Mostimbalancedfaceattributes(fromTable5)thatarecorrectly
predictedbyourmethod.
across all attributes, with an average gap of about 9% over
TABLE7
ANet. Considering most face attributes exhibit high class
Ablationstudyintermsofthefaceverificationaccuracy(%)onLFW
imbalance with an average positive class rate of only 23%, andaveragebalancedaccuracy(%)forattributepredictiononCelebA.
suchimprovementsarenontrivialandprovetheefficacyof
ourlearnedfeaturesonimbalanceddata.AlthoughPANDA Method LFW CelebA
and ANet are capable of learning robust feature represen- CLMLE 99.62 88.78
tations by ensemble and multi-task learning respectively, uniformclustersampling 99.52 88.23
nocost-sensitivelearning 99.57 87.46
theyignoretheimbalanceissueandthusstruggleforhighly
imbalancedattributes,e.g.,“Bald”.Bycontrast,ourmethod
performswellonsuchattributes.Ouradvantageoverthese TABLE8
Meanclass-balancedaccuracy(%)onCIFAR-100[71]and
non-imbalanced learning methods is more evident when
MNIST-rot-back-image[72]datasetswithsimulatedclassimbalance
observingtherelativeaccuracygainsfordifferentattributes. (withparameterγ-thesmaller,themoreimbalanced).Triplet+denotes
Fig.9 showsthe gainstendto belarger forthoseattributes tripletloss[33]withover-samplingandcostsensitivity.Balanced
baselinedenotestheCE+CRLresultsunderclass-balancedsetting.
withahigherclassimbalancelevel.
ThebottomcellsofTable5illustratetheresultsofimbal-
Method CIFAR-100 CIFAR-100 MNIST MNIST
anced learning methods. We can see that our CLMLE out-
γ=1 γ=0.5 γ=1 γ=0.5
performstraditionalre-samplingandcost-sensitivelearning
Triplet+ 42.2 32.2 65.2 59.8
techniques, as well as the previous LMLE method [32] and CE+CRL[30] 45.2 37.4 71.2 64.5
recentCRL-I[30],allofwhichusethesamenetworkarchi- LMLE[32] 44.3 38.1 71.9 65.7
tecture[41]forfaircomparison.CRL-Idenotesregularizing CLMLE 47.4 39.7 73.5 68.8
Balancedbaseline 48.2 78.3
theminorityclasswithInstancelevelhardmining.However
it still cannot guarantee equal learning for all classes and
has inferior results. Thanks to the authors of MOON [20]
tobenon-uniforminclass-wepicktheonewiththehighest
(with adaptive re-weighting) and AFFACT [69], we further
observedloss.Resultsshowthatithurtsperformanceifwe
compareagainsttheirprovidedresultsofbalancedaccuracy
choose uniform cluster sampling instead. It is more so for
from using VGG-16 and ResNet-50 networks respectively.
face recognition, because compared to the binary attribute
Our CLMLE still outperforms the two methods by large
prediction problem, face recognition has far more classes
margin (about 10% and 6% on average), even using a
whose discrimination is more sensitive to the hard modes
much smaller network [41]. On top of larger networks, the
(clusters). The cost-sensitive scheme is used by us to re-
MOON-DandAFFACT-Dvariantsarefurtherfine-tunedto
balance the classes in batch at score level. Note when this
completely balanced attribute distribution, the same as the
scheme is applied to perfectly class-balanced batches, it
underlying distribution suggested by our balanced evalu-
would have no effects. However in the case of multi-label
ation metric. While CLMLE, without assuming the target
attributes, class-balanced batch samples for one attribute
attribute distribution or using large networks, is able to
willalmostcertainlybeimbalancedfortheotherattributes.
achievecomparableorevenbetterperformance.
This is when the class costs can help. Our ablation study
Table 6 compares CLMLE with state-of-the-art
confirms that the cost-sensitivity has a larger impact on
AttCNN [70] and SSP [55] in terms of two evaluation
CelebA attributes. To highlight the effects of our k-nearest
metrics. AttCNN addresses the multi-label imbalance
cluster classifier in Eq. (10), we replace it with the regular
problem by selective data learning. However, only average
instance-wisekNNclassifier.Asexpected,weobservemuch
classification accuracy is reported, and our superiority still
lowerspeedandabitworseresults(88.59%)forattributes.
holds for this metric. The Semantic Segmentation-based
Pooling (SSP) method does not handle imbalance, but
adds a segmentation network to improve the attribute 5.5 GenericImbalancedImageClassification
network. Our CLMLE is quite comparable in both metrics In addition to the face recognition and attribute predic-
without increasing network capacity. Fig. 10 shows some tion tasks with real-world imbalanced data, we have also
challenging images with highly imbalanced attributes that tested CLMLE on generic image classification problems
arecorrectlypredictedbyCLMLEbutnotbySSP. (single-label & multi-class). We experiment on standard
benchmarksCIFAR-100[71]andMNIST-rot-back-image[72]
5.4 AblationStudy
but with simulated class imbalance. As in [30], the simu-
Table 7 quantifies the benefits of the re-sampling and cost- latedclass-imbalanceddatafollowapower-lawdistribution
sensitivecomponentsadoptedinourCLMLEmethod.Both f(c) = Lmax/(cγ + Lmin) where Lmax and Lmin are the
c c c c
the classic schemes prove themselves as useful for our im- largest and smallest sizes of classes c ∈ [1,C]. The param-
balancedtasks.ThesamplingofqueryclusterI 1isdesigned eter γ controls the degree of class imbalance — the smaller13
γ is, the more imbalanced the distribution is. We follow [11] T.MaciejewskiandJ.Stefanowski,“Localneighbourhoodexten-
the same experimental settings of [30] and [32] for CIFAR- sionofSMOTEforminingimbalanceddata,”inICDM,2011.
[12] Y.Tang,Y.-Q.Zhang,N.Chawla,andS.Krasser,“SVMsmodeling
100andMNISTrespectively,includingdatausage,network
for highly imbalanced classification,” TSMC, vol. 39, no. 1, pp.
architecture and hyper-parameters. Table 8 reports classifi-
281–288,2009.
cation results under two imbalnaced ratios γ = {1,0.5}, [13] K.M.Ting,“Acomparativestudyofcost-sensitiveboostingalgo-
andcomparesCLMLEwithsomeimbalancedlearningbase- rithms,”inICML,2000.
linesTriplet+,CE(Cross-Entropy)+CRL[30]andLMLE[32]. [14] C.Huang,C.C.Loy,andX.Tang,“Discriminativesparseneighbor
approximationforimbalancedlearning,”TNNLS,vol.PP,no.99,
Classification results under the class-balanced scenario are
pp.1–11,2017.
reportedusingCE+CRLasabalancedbaseline.Wecansee [15] H. He, Y. Bai, E. A. Garcia, and S. Li, “ADASYN: Adaptive
that CLMLE’s superiority still holds for imbalanced image syntheticsamplingapproachforimbalancedlearning,”inIJCNN,
classification. 2008.
[16] Z.H.ZhouandX.Y.Liu,“Onmulti-classcost-sensitivelearning,”
inAAAI,2006.
6 CONCLUSION [17] M.Oquab,L.Bottou,I.Laptev,andJ.Sivic,“Learningandtransfer-
ring mid-level image representations using convolutional neural
Classimbalanceiscommoninmanyvisiontasks,including networks,”inCVPR,2014.
[18] H.Caesar,J.Uijlings,andV.Ferrari,“Jointcalibrationforsemantic
face recognition and attribute prediction. Contemporary
segmentation,”inBMVC,2015.
deep representation learning methods typically adopt class
[19] M. Mostajabi, P. Yadollahpour, and G. Shakhnarovich, “Feedfor-
re-sampling or cost-sensitive learning schemes. Through ward semantic segmentation with zoom-out features,” in CVPR,
extensive experiments, we have validated their usefulness 2015.
[20] E.M.Rudd,M.Gu¨nther,andT.E.Boult,“MOON:Amixedobjec-
and further demonstrated that the proposed Cluster-based
tiveoptimizationnetworkfortherecognitionoffacialattributes,”
Large Margin Local Embedding (CLMLE) works remark-
inECCV,2016.
ably well when just combined with a simple k-nearest [21] D. Eigen and R. Fergus, “Predicting depth, surface normals and
cluster classifier. CLMLE maintains inter-cluster angular semantic labels with a common multi-scale convolutional archi-
tecture,”inICCV,2015.
margins both within and between classes, thus carving
[22] W.Shen,X.Wang,Y.Wang,X.Bai,andZ.Zhang,“DeepContour:
much more balanced class boundaries locally. Our feature
Adeepconvolutionalfeaturelearnedbypositive-sharinglossfor
learning converges fast and achieves the new state-of-the- contourdetection,”inCVPR,2015.
art performance on existing face recognition and attribute [23] S. R. Bulo`, G. Neuhold, and P. Kontschieder, “Loss max-pooling
benchmarks. forsemanticimagesegmentation,”inCVPR,2017.
[24] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent,
and S. Bengio, “Why does unsupervised pre-training help deep
learning?”JMLR,vol.11,pp.625–660,2010.
ACKNOWLEDGMENTS
[25] P. Jeatrakul, K. Wong, and C. Fung, “Classification of imbal-
This work is supported by SenseTime Group Limited and anceddatabycombiningthecomplementaryneuralnetworkand
SMOTEalgorithm,”inICONIP,2010.
the General Research Fund sponsored by the Research
[26] S.H.Khan,M.Hayat,M.Bennamoun,F.A.Sohel,andR.Togneri,
Grants Council of the Hong Kong SAR (CUHK 14241716, “Cost-sensitivelearningofdeepfeaturerepresentationsfromim-
14224316,14209217). balanceddata,”TNNLS,vol.PP,no.99,pp.1–15,2018.
[27] C. L. Castro and A. P. Braga, “Novel cost-sensitive approach to
improve the multilayer perceptron performance on imbalanced
REFERENCES data,”TNNLS,vol.24,no.6,pp.888–899,2013.
[28] Z.-H.ZhouandX.-Y.Liu,“Trainingcost-sensitiveneuralnetworks
[1] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller, “La- with methods addressing the class imbalance problem,” TKDE,
beledfacesinthewild:Adatabaseforstudyingfacerecognition vol.18,no.1,pp.63–77,2006.
in unconstrained environments,” University of Massachusetts, [29] Y.-X.Wang,D.Ramanan,andM.Hebert,“Learningtomodelthe
Amherst,Tech.Rep.07–49,October2007. tail,”inNIPS,2017.
[2] I.Kemelmacher-Shlizerman,S.M.Seitz,D.Miller,andE.Brossard, [30] Q.Dong,S.Gong,andX.Zhu,“Classrectificationhardminingfor
“The MegaFace benchmark: 1 million faces for recognition at imbalanceddeeplearning,”inICCV,2017.
scale,”inCVPR,2016. [31] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
[3] N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar, “De- Z.Huang,A.Karpathy,A.Khosla,M.Bernstein,A.C.Berg,and
scribablevisualattributesforfaceverificationandimagesearch,” L. Fei-Fei, “Imagenet large scale visual recognition challenge,”
TPAMI,vol.33,no.10,pp.1962–1977,2011. IJCV,vol.115,no.3,pp.211–252,2015.
[4] Y.Taigman,M.Yang,M.Ranzato,andL.Wolf,“DeepFace:Closing [32] C.Huang,Y.Li,C.C.Loy,andX.Tang,“Learningdeeprepresen-
thegaptohuman-levelperformanceinfaceverification,”inCVPR, tationforimbalancedclassification,”inCVPR,2016.
2014.
[33] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A unified
[5] H.HeandE.A.Garcia,“Learningfromimbalanceddata,”TKDE,
embeddingforfacerecognitionandclustering,”inCVPR,2015.
vol.21,no.9,pp.1263–1284,2009.
[34] S. Wang, W. Liu, J. Wu, L. Cao, Q. Meng, and P. J. Kennedy,
[6] H.HeandY.Ma,ImbalancedLearning:Foundations,Algorithms,and
“Training deep neural networks on imbalanced data sets,” in
Applications. Wiley-IEEEPress,2013.
IJCNN,2016.
[7] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer,
[35] W.W.Ng,G.Zeng,J.Zhang,D.S.Yeung,andW.Pedrycz,“Dual
“SMOTE: synthetic minority over-sampling technique,” JAIR,
autoencoders features for imbalance classification problem,” PR,
vol.16,no.1,pp.321–357,2002.
vol.60,pp.875–889,2016.
[8] B.Krawczyk,M.Woz´niak,andG.Schaefer,“Cost-sensitivedeci-
sion tree ensembles for effective imbalanced classification,” ASC [36] M.Ren,W.Zeng,B.Yang,andR.Urtasun,“Learningtoreweight
Journal,vol.14,pp.554–562,2014. examplesforrobustdeeplearning,”inICML,2018.
[9] C. Drummond and R. C. Holte, “C4.5, class imbalance, and [37] W.Liu,R.Lin,Z.Liu,L.Liu,Z.Yu,B.Dai,andL.Song,“Learning
cost sensitivity: Why under-sampling beats over-sampling,” in towardsminimumhypersphericalenergy,”inNeurIPS,2018.
ICMLW,2003. [38] W.Liu,Y.Wen,Z.Yu,andM.Yang,“Large-marginsoftmaxloss
[10] H.Han,W.-Y.Wang,andB.-H.Mao,“Borderline-SMOTE:Anew forconvolutionalneuralnetworks,”inICML,2016.
over-samplingmethodinimbalanceddatasetslearning,”inICIC, [39] W.Liu,Y.Wen,Z.Yu,M.Li,B.Raj,andL.Song,“Sphereface:Deep
2005. hypersphereembeddingforfacerecognition,”inCVPR,2017.14
[40] H. Wang, Y. Wang, Z. Zhou, X. Ji, Z. Li, D. Gong, J. Zhou, [72] H.Larochelle,D.Erhan,A.Courville,J.Bergstra,andY.Bengio,
and W. Liu, “CosFace: Large margin Cosine loss for deep face “Anempiricalevaluationofdeeparchitecturesonproblemswith
recognition,”inCVPR,2018. manyfactorsofvariation,”inICML,2007.
[41] Y. Sun, Y. Chen, X. Wang, and X. Tang, “Deep learning face
representationbyjointidentification-verification,”inNIPS,2014.
[42] Y.Sun,X.Wang,andX.Tang,“Deeplylearnedfacerepresentations
aresparse,selective,androbust,”inCVPR,2015.
[43] D.Yi,Z.Lei,S.Liao,andS.Z.Li,“Learningfacerepresentation ChenHuangreceivedthePh.D.degreeinElec-
fromscratch,”arXivpreprint,arXiv:1411.7923,2014. tronic Engineering from Tsinghua University,
[44] Y. Wen, K. Zhang, Z. Li, and Y. Qiao, “A discriminative feature Beijing, China, in 2014. He was a postdoctoral
learningapproachfordeepfacerecognition,”inECCV,2016. fellowintheRoboticsInstituteofCarnegieMel-
[45] J. Deng, Y. Zhou, and S. Zafeiriou, “Marginal loss for deep face lon University, and also in the Department of
recognition,”inCVPRW,2017. Information Engineering, Chinese University of
[46] J. Liu, Y. Deng, T. Bai, and C. Huang, “Targeting ultimate ac- Hong Kong. His research interests include ma-
curacy: Face recognition via deep embedding,” arXiv preprint, chine learning and computer vision, with focus
arXiv:1506.07310,2015. ondeeplearningandfaceanalysis.
[47] I. Masi, A. T. an Tra˜n, T. Hassner, J. T. Leksut, and G. Medioni,
“Do we really need to collect millions of faces for effective face
recognition?”inECCV,2016.
[48] X. Yin, X. Yu, K. Sohn, X. Liu, and M. Chandraker, “Feature
transfer learning for deep face recognition with long-tail data,”
arXivpreprint,arXiv:1803.09014,2018. YiningLireceivedtheB.EdegreeinAutomation
[49] Y.Wu,H.Liu,J.Li,andY.Fu,“Deepfacerecognitionwithcenter fromTsinghuaUniversityin2014.Heiscurrently
invariantloss,”inThematicWorkshopofACM-MM,2017. working toward the Ph.D. degree in the De-
[50] Y. Guo and L. Zhang, “One-shot face recognition by promoting partment of Information Engeneering, Chinese
underrepresentedclasses,”arXivpreprint,arXiv:1707.05574,2018. UniversityofHongKong.Hisresearchinterests
[51] X.Zhang,Z.Fang,Y.Wen,Z.Li,andY.Qiao,“Rangelossfordeep include computer vision and machine learning,
facerecognitionwithlong-tailedtrainingdata,”inICCV,2017. withfocusonsemanticattributerecognitionand
[52] T. Berg and P. N. Belhumeur, “POOF: Part-Based One-vs-One analysis.
Features for fine-grained categorization, face verification, and
attributeestimation,”inCVPR,2013.
[53] Z.Liu,P.Luo,X.Wang,andX.Tang,“Deeplearningfaceattributes
inthewild,”inICCV,2015.
[54] N. Zhang, M. Paluri, M. Ranzato, T. Darrell, and L. Bourdev,
“PANDA: Pose aligned networks for deep attribute modeling,”
inCVPR,2014. ChenChangeLoy(S’06-M’10-SM’17)received
[55] M.M.Kalayeh,B.Gong,andM.Shah,“Improvingfacialattribute thePh.D.degreeincomputersciencefromthe
predictionusingsemanticsegmentation,”inCVPR,2017. Queen Mary University of London in 2010. He
[56] K.Sohn,“Improveddeepmetriclearningwithmulti-classN-pair is an Associate Professor with the School of
lossobjective,”inNIPS,2016. Computer Science and Engineering, Nanyang
[57] C. Huang, C. C. Loy, and X. Tang, “Local similarity-aware deep TechnologicalUniversity.PriortojoiningNTU,he
featureembedding,”inNIPS,2016. served as a Research Assistant Professor with
[58] T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka, “Distance- theDepartmentofInformationEngineering,The
based image classification: Generalizing to new classes at near- ChineseUniversityofHongKong,from2013to
zerocost,”TPAMI,vol.35,no.11,pp.2624–2637,2013. 2018. His research interests include computer
[59] L.vanderMaatenandG.Hinton,“Visualizinghigh-dimensional vision and deep learning, with focus on face
datausingt-SNE,”JMLR,vol.9,pp.2579–2605,2008. analysis, image processing, and visual surveillance. He serves as an
[60] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin, Associate Editor of the International Journal of Computer Vision. He
B.Chen,andY.Wu,“Learningfine-grainedimagesimilaritywith alsoserves/servedasanAreaChairofCVPR2019,BMVC2019,ECCV
deepranking,”inCVPR,2014. 2018andBMVC2018.HeisaseniormemberofIEEE.
[61] C.Silpa-AnanandR.Hartley,“OptimisedKD-treesforfastimage
descriptormatching,”inCVPR,2008.
[62] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao, “Joint face detection
andalignmentusingmultitaskcascadedconvolutionalnetworks,”
SPL,vol.23,no.10,pp.1499–1503,2016. Xiaoou Tang (S’93-M’96-SM’02-F’09) received
[63] O.M.Parkhi,A.Vedaldi,andA.Zisserman,“Deepfacerecogni- the BS degree from the University of Science
tion,”inBMVC,2015. andTechnologyofChina,Hefei,in1990,theMS
[64] L. Wolf, T. Hassner, and I. Maoz, “Face recognition in uncon- degree from the University of Rochester, New
strained videos with matched background similarity,” in CVPR, York,in1991,andthePhDdegreefromtheMas-
2011. sachusetts Institute of Technology, Cambridge,
[65] Y.Taigman,M.Yang,M.Ranzato,andL.Wolf,“Web-scaletraining in1996.HeisaProfessoroftheDepartmentof
forfaceidentification,”inCVPR,2015. InformationEngineering,TheChineseUniversity
[66] B. Chen, W. Deng, and J. Du, “Noisy softmax: Improving the ofHongKong.Heworkedasthegroupmanager
generalizationabilityofDCNNviapostponingtheearlysoftmax oftheVisualComputingGroupattheMicrosoft
saturation,”inCVPR,2017. ResearchAsia,from2005to2008.Hisresearch
[67] Y.Liu,H.Li,andX.Wang,“Rethinkingfeaturediscriminationand interests include computer vision, pattern recognition, and video pro-
polymerizationforlarge-scalerecognition,”inNIPSW,2017. cessing. He received the Best Paper Award at the IEEE Conference
[68] A.NechandI.Kemelmacher-Shlizerman,“Levelplayingfieldfor on Computer Vision and Pattern Recognition (CVPR) 2009 and Out-
millionscalefacerecognition,”inCVPR,2017. standing Student Paper Award at the AAAI 2015. He was a program
[69] M.Gu¨nther,,A.Rozsa,andT.E.Boult,“Affact:Alignmentfree chairoftheIEEEInternationalConferenceonComputerVision(ICCV)
facialattributeclassificationtechnique,”inIJCB,2017. 2009 and served as an associate editor of the IEEE Transactions on
Pattern Analysis and Machine Intelligence. He is an Editor-in-Chief of
[70] E. Hand, C. Castillo, and R. Chellappa, “Doing the best we can
theInternationalJournalofComputerVision.HeisafellowoftheIEEE.
withwhatwehave:Multi-labelbalancingwithselectivelearning
forattributeprediction,”inAAAI,2018.
[71] A. Krizhevsky and G. Hinton, “Learning multiple layers of fea-
tures from tiny images,” Master’s thesis, Department of Computer
Science,UniversityofToronto,2009."
57,59,Deep joint spatiotemporal network (DJSTN) for efficient facial expression recognition,"['D Jeong', 'BG Kim', 'SY Dong']",2020,105,MMI Facial Expression,"deep learning, facial expression recognition, neural network",facial emotion recognition systems with various signals. Most of them employ facial expression  recognition  : An addition to the mmi facial expression database. In Proceedings of the 3rd,No DOI,Sensors,https://www.mdpi.com/1424-8220/20/7/1936,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,mdpi.com,
58,60,Deep learning approaches for facial emotion recognition: A case study on FER-2013,"['P Giannopoulos', 'I Perikos', 'I Hatzilygeroudis']",2018,206,Affective Faces Database,FER,"the emotional behavior and the affective state of the human.  , for images of the Jaffe face  database with little noise and with  /non smiling expressions in the ORL database. In [32], a",No DOI,Advances in hybridization of …,https://link.springer.com/chapter/10.1007/978-3-319-66790-4_1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,springer.com,
59,61,Deep learning based facs action unit occurrence and intensity estimation,"['A Gudi', 'HE Tasli', 'TM Den Uyl']",2015,221,Toronto Face Database,deep learning,method on the SEMAINE dataset was much lower than on the BP4D dataset. One of the main  contributing factors to this observation is the low number of individual faces included in the,No DOI,… on automatic face and …,https://ieeexplore.ieee.org/document/7284873,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
60,62,Deep learning for biometrics: A survey,"['K Sundararajan', 'DL Woodard']",2018,314,Toronto Face Database,deep learning,"with the SVS2004 dataset, it can be observed that both deep learning and other approaches  seem to perform comparably. This could be attributed to smaller dataset size. However, for",No DOI,ACM Computing Surveys (CSUR),https://dl.acm.org/doi/10.1145/3190618,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,acm.org,
61,63,Deep learning for emotion recognition on small datasets using transfer learning,"['HW Ng', 'VD Nguyen', 'V Vonikakis']",2015,802,Affective Faces Database,deep learning,"dataset, when a sufficiently large face dataset such as FER-2013 is available, whether by  adding it to the auxiliary dataset  exploit deep neural networks such as CNN for face expression",No DOI,Proceedings of the 2015 …,https://dl.acm.org/doi/10.1145/2818346.2830593,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,acm.org,
62,64,Deep learning for human affect recognition: Insights and new developments,"['PV Rouast', 'MTP Adam', 'R Chiong']",2019,613,"Affective Faces Database, Toronto Face Database",deep learning,"deep neural networks, we comprehensively quantify their applications in this field. We find  that deep learning is used for learning  number of examples per dataset does not see a clear",No DOI,IEEE Transactions on …,https://arxiv.org/abs/1901.02884,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 1
Deep Learning for Human Affect Recognition:
Insights and New Developments
Philipp V. Rouast, Student Member, IEEE, Marc T. P. Adam, and Raymond Chiong, Senior Member, IEEE
Abstract—Automatichumanaffectrecognitionisakeysteptowardsmorenaturalhuman-computerinteraction.Recenttrendsinclude
recognitioninthewildusingafusionofaudiovisualandphysiologicalsensors,achallengingsettingforconventionalmachinelearning
algorithms.Since2010,noveldeeplearningalgorithmshavebeenappliedincreasinglyinthisfield.Inthispaper,wereviewthe
literatureonhumanaffectrecognitionbetween2010and2017,withaspecialfocusonapproachesusingdeepneuralnetworks.By
classifyingatotalof950studiesaccordingtotheirusageofshallowordeeparchitectures,weareabletoshowatrendtowardsdeep
learning.Reviewingasubsetof233studiesthatemploydeepneuralnetworks,wecomprehensivelyquantifytheirapplicationsinthis
field.Wefindthatdeeplearningisusedforlearningof(i)spatialfeaturerepresentations,(ii)temporalfeaturerepresentations,and(iii)
jointfeaturerepresentationsformultimodalsensordata.Exemplarystate-of-the-artarchitecturesillustratetheprogress.Ourfindings
showtheroledeeparchitectureswillplayinhumanaffectrecognition,andcanserveasareferencepointforresearchersworkingon
relatedapplications.
IndexTerms—Affectrecognition,Deeplearning,Emotionrecognition,Human-computerinteraction.
(cid:70)
1 INTRODUCTION
HUMAN-COMPUTER INTERACTION (HCI) is seeing a the focus started to shift towards more realistic, sponta-
gradual shift from the computer-centered approach neous displays of affective behavior [11]. These trends are
of the past to a more user-centered approach. Commercial exemplified in the annual competitions Emotion Recognition
success has made user-friendly input methods, portable in the Wild (EmotiW) [12] and Audio Video Emotion Chal-
devicesandmulti-sensoravailabilityanewstandardinper- lenge (AVEC) [13]. Since 2010, deep learning methods have
sonal computing. Despite the progress, it has been argued beenappliedtoaffectrecognitionproblemsacrossmultiple
thatHCIisstilllackingacentralelementofhuman-human modalities and led to improvements in accuracy, including
interaction: The communication of information through af- winningperformancesatEmotiW[14],[15],[16]andAVEC
fective display [1]. The term affective computing was coined [17],[18],[19].
by Rosalind Picard in 1995 [2], inspired by findings from In this paper, we review the state of research on af-
neuroscience, psychology, and cognitive science highlight- fect recognition using deep learning. We start by giving
ingtheimportantroleaffectplaysinintelligentbehavior.It an introduction to deep learning, the reasoning behind its
encompasses efforts to (i) automatically recognize human application in artificial intelligence (AI), and to the most
affect, and (ii) generate corresponding responses by the relevant architectures. We then break down the challenges
computer, providing a richer context for HCI outcomes. facedinaffectrecognitionresearch,andoutlinewhymodels
Applications range from education [3] and health care [4] incorporating deep learning are useful in meeting these.
toentertainment[5]andembodiedagents[6]. Finally, we go on to discuss the directions research has
In human interactions, a significant amount of informa- taken since 2010 and in what way deep learning methods
tion is not communicated explicitly, but through the way areutilized.Ourcontributionstothefieldare:
wespeak,ourfacialexpressions,gestures,andothermeans.
Initial research on affect recognition1 focused mainly on • By conducting a comprehensive literature search,
unimodal approaches, with facial expression recognition andclassificationofatotalof950studies,weareable
(FER) and speech emotion recognition (SER) gaining most toidentifyandmeasureatrendtowardsuseofdeep
attention and highest accuracies [7]. Psychophysiological neuralnetworksforaffectrecognition;
measureswerealsoshowntocontaininformationaboutaf- • reviewing 233 studies that employ deep learning,
fective states [8]. Public database availability has improved we identify the main application areas as being
sincethe2000s[9]andmultimodalsensorcombinationwas the learning of (i) spatial feature representations, (ii)
foundtoimproverecognitionaccuracyandrobustness[10]. temporal feature representations, and (iii) joint feature
As good performance was achieved on posed databases, representationsformultimodalsensordata;
• we discuss and give exemplary illustrations of how
• P.V.Rouast,M.T.P.AdamandR.ChiongarewiththeSchoolofElectrical deepneuralnetworksareappliedonvisual,auditory,
EngineeringandComputing,UniversityofNewcastle,Callaghan,NSW
andphysiologicalsensordata;
2308,Australia.
E-mail:philipp.rouast@uon.edu.au • we provide an overview of the most relevant
databasesbyquantifyingtheiruseacross233studies,
ManuscriptsubmittedFeb.26,2018;revisedAug.31,2018,Nov.16,2018.
includinglargedatabasesestablishedsince2016;and
1.Unless stated otherwise, in this article we use the term affect
recognitiontorefertohumanaffectrecognition. • wediscussopenissuesandresearchdirections.
Thisarticlehasbeenacceptedforpublicationinafutureissueofthisjournal,buthasnotbeenfullyedited.Contentmaychangepriortofinalpublication.Citationinformation:DOI10.1109/TAFFC.2018.2890471,IEEE
1949-3045(cid:13)c 2018IEEE.Personaluseispermitted,butrepublication/redistributionrequiresIEEEpermission.Seehttp://www.ieee.org/publicationsstandards/publications/rights/index.htmlformoreinformation.
9102
naJ
9
]GL.sc[
1v48820.1091:viXraJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 2
2 DEEP LEARNING FOR AFFECT RECOGNITION Itconsistsofmultiplelayersofprocessingunits(“neurons”):
Aninputlayer,multiplehiddenlayers,andanoutputlayer.
Automatic affect recognition relies on visual and auditory
Units in adjacent layers can have weighted connections.
perception [20], which are examples of abilities commonly
We speak of fully-connected DNNs if there are connections
expected of AI. A class of methods known as machine
between all pairs of units in adjacent layers. Information
learning has turned out to be effective in handling the
in the network flows forward through these connections,
desired perception tasks. By enabling computers to learn
each unit computing its activation as a function of its in-
directlyfromexamples,thesealgorithmsovercometheneed
puts.Unitsinhiddenlayersintroduceanonlinearityinthe
toprovideanexplicitmodel.
process. By adjusting the connection weights, a DNN can
Although perception tasks such as visual and auditory
effectively learn a feature representation of its input data in
affect recognition seem intuitive to humans, some charac-
each layer’s unit activations. A well-trained DNN learns a
teristics rooted in their real-world origin make them hard
deephierarchyofdistributedrepresentations3.Thisenablesthe
problemstosolve:Theyrequireunderstandingofproblems
network to learn very expressive representations capturing
characterized by highly varying functions in terms of the
alargenumberofpossibleinputconfigurations[26].
input;anothercommoncharacteristicisthehighdimension-
Someofthekeyadvantagesofdeeparchitecturesarede-
alityofexamplesintheformofimagesandaudiofiles.
rivedfromtheirdepth.Increasingdepthpromotesre-useof
In this context, a common challenge for traditional ma-
learned features [27]. On a related note, the deep hierarchy
chine learning approaches is the curse of dimensionality, a
offeaturerepresentationsallowslearningatdifferentlevels
phenomenon where the higher the number of dimensions
of abstraction building on top of each other. Here, higher
used to represent the data, the less effective conventional
levelsofabstractionaregenerallyassociatedwithinvariance
computational and statistical methods become [21]. With a
tolocalchangesoftheinput[27].
very high number of dimensions, it becomes increasingly
While the theoretical advantages of such deep architec-
difficult to comprehensively sample all possible combina-
tureswereknownforsometime,progresswasheldbackby
tions, resulting in vast unexplored regions in the feature
the difficulty of training them [25]. An initial breakthrough
space. To circumvent this problem, a straightforward and
in 2006 [23] showed that the so-called vanishing gradient
widelyusedsolutionistoprojectthehigh-dimensionaldata
problemintrainingDNNscanbeovercomebyunsupervised
into a lower-dimensional space through approaches such
pre-training. Multiple strategies of alleviating the problem
as feature selection. Machine learning algorithms with so-
areknowntoday,including(i)architecturesunaffectedbyit
called shallow architectures, such as kernel methods and
[28],[29],(ii)improvedoptimizers[30],(iii)certaintraining
single-layerneuralnetworks,canthenbeefficientlyapplied
and design choices [31], [32], [33], and (iv) use of powerful
for modeling purposes. However, when considering com-
computing systems, especially GPU-based. Three interre-
putational and statistical efficiency as well as human in-
latedfactorsdrivethecontinuedsuccessofDNNs:
volvement,ithasbeensuggestedthatshallowarchitectures
may not be the most efficient way to approach challenging • Increased learning capacity4. A central theme in the
learningproblemssuchasaffectrecognition[20].Hence,in evolution of deep models has been a link between
2010 researchers have started to explore the application of bettergeneralizationabilityandincreasednumberof
deeparchitecturesforaffectrecognition. parameters, especially when growing models deeper
ratherthan wider[22, ch.6.4].This appliesas longas
2.1 Deeplearning trainingisfeasibleandthedatasetislargeenoughto
takeadvantageofthearchitecture[29].
To distinguish between shallow and deep machine learning
• Growing computing power. Training of state-of-the-art
models, one can think of their architectures as subsequent
deep learning models is an intense task involving
layers of hierarchical computation. We will use the notion
millionsofparameterstooptimize.GPUsarepartic-
ofarchitecturedepthtodenotethenumberofcomputational
ularly well suited for the operations involved, and
layers in an architecture2. Traditional learning algorithms
specializedsoftwareisavailable.Seethesupplemen-
can generally be represented as two layers of computation,
talmaterial(SectionS4)forfurtherinformation.
wherethefirstlayerconsistsoftemplatematchersorsimple
• Largedatasets.Partofthepromiseofdeeplearningis
trainablebasisfunctions,andthesecondlayerisaweighted
to take advantage of large datasets with millions of
sum [20]. This is why we talk of them as shallow architec-
examples. An investigation into the effect of dataset
tures, when comparing them with deep neural networks,
sizesuggestsalogarithmicrelationshipbetweenper-
which consist of three or more layers. Although there is
formanceonvisiontasksandtrainingdatasize[34].
no universally agreed upon rule to determine depth or
distinguish between shallow and deep architectures [22, Whendealingwithdatathatareknowntohaveacertain
ch.1], a cutoff of three or more layers is commonly used structure(e.g.,spatialstructure,temporalstructure),DNNs
[23],[24],[25]. canbemodifiedtocreatemorespecializedarchitecturesthat
takeadvantageofsaidstructures.Inthefollowing,wepro-
2.1.1 Deepneuralnetworks videabriefoverviewofsuchspecializedDNNarchitectures.
The design of deep neural networks (DNNs) is loosely
3.This implies a many-to-many relationship between learned con-
inspiredbybiologicalneuralnetworks.Atypicalexampleis
ceptsandunitsrepresentingthem.
the deep feedforward network (or multi-layer perceptron).
4.Due to the complexity of deep learning algorithms, it is difficult
toexplicitlydeterminetheirlearningcapacity[22,ch.5.2],butitcanbe
2.Thisexcludestheinputlayer,whichlackslearnableparameters. thoughtofasbeingrelatedtothenumberofparametersandlayers[34].JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 3
2.1.2 Learningspatially:Convolutionalneuralnetworks with a single hidden layer can be considered as very deep
networks, which becomes clear when we imagine “un-
Sensorrecordingsofnaturalscenesofteninherentlycontain
rolling” them along the time dimension. It turns out that
a spatial structure along some dimensions. Convolutional
thisdepthmakestrainingRNNsconsiderablymoredifficult,
neural networks (CNNs) introduce layers with specialized
as gradients tend to vanish or explode during training—
operations into DNNs, which take into account the spatial
especiallywhenprocessinglongsequences.
structureofthedatatomakethenetworkmoreefficient.
Todealwiththisproblem,multiplespecializedRNNar-
The convolutional layer consists of several learnable
chitectureswithgatedunitshavebeenproposed.Longshort-
kernels, which are convolved with the layer input to pro-
term memory (LSTM) RNNs [28] are successful at learning
duce activations. In terms of the layer’s units, this can be
long-term dependencies by providing gate mechanisms to
interpreted as parameter sharing between units, since the
addandforgetinformationselectively.Gatedrecurrentunits
same kernel is applied over different spatial locations. The
(GRUs) are a gating mechanism proposed more recently in
kernel’s receptive field is typically much smaller than the
thecontextofsequence-to-sequenceprocessing[41].RNNs,
input, which leads to sparse connectivity between units of
especially LSTMs, have had a profound impact on how
adjacentlayers[22,ch.9.2].Incontrasttothefully-connected
sequencesofdataareprocessed[24].Theyareincorporated
equivalent, this means that convolutional layers only es-
in state-of-the-art AI systems, for example in automatic
tablish connections between units that are spatially close,
speechrecognition(ASR)[42].
whichdramaticallyreducesthenumberofparameters.Since
CNNs are inspired by the mammalian visual cortex [35],
2.1.4 Unsupervisedlearningmodels
we primarily see 2D convolutions applied to image data.
To learn useful feature representations from data, the most
However, it is possible to apply convolutions along any
commonapproachtodayissupervisedlearning:Researchers
dimensionoftheinputdata,including1D(e.g.,audiodata)
provide the learning algorithm with clues on how to im-
and3Dconvolutions(e.g.,videodata).
prove parameters, typically in the form of corresponding
After the convolution operation, a nonlinearity is ap-
data labels and a loss function measuring how “bad” a
plied, typically the rectified linear unit [31]. Pooling layers
representation is. Taking a classification task for example,
betweenconvolutionallayersfacilitatenonlineardownsam-
ausefulrepresentationwouldbeonethatmakestheclasses
pling of layer activations, and make the network invariant
of interest linearly separable. However, labeling data is
totranslationsintheinput[22,ch.9.3].Invariancestoother
expensive and large unlabeled datasets are easier to come
transformations such as rotation or scaling are not directly
by.Unsupervisedlearningalgorithmsattempttolearnuseful
implied by the architecture, but can be learned in convolu-
representations without being given explicit clues such as
tionallayers.Finally,itiscommontoaddoneortwofully-
datalabels—instead,aformofregularizationisintroduced.
connectedlayersafterseveralalternatingconvolutionaland
Anautoencoder(AE)[43]isaneuralnetworkthatlearns
poolinglayers(e.g.,[36]).
twofunctions,f : x (cid:55)→ z andg : z (cid:55)→ x,torestoreitsinput
Compared to fully-connected DNNs, training CNNs is
xfromanintermediaterepresentationz.TheideaoftheAE
less difficult due to the reduced number of parameters.
is to avoid identity mapping between f and g, either by
They were trained successfully in the 1990s [37], while
forcingz tobeoflowerdimensionalitythanxinbasicAEs,
fully-connected DNNs were still believed to be too diffi-
orbyotherformsofregularization,suchasrestoringxfrom
cult to train [25]. Since a record-breaking performance [36]
acorruptedversionx˜ofitselfindenoisingAEs.Whenmul-
at the ImageNet Large Scale Visual Recognition Challenge
tiple layers are between input or output and intermediate
(ILSVRC)[38]in2012,CNNshaveattractedmuchattention
representation in an AE, we speak of stacked autoencoders
fromresearchersandthemainstreammedia.
(SAEs). Restricted Boltzmann machines (RBMs) [44] are
undirected probabilistic models that learn a representation
2.1.3 Learningfromsequences:Recurrentneuralnetworks
of their input in a layer of latent units. Deep Boltzmann
Whenlearningfromsequentialdata(e.g.,audio,video),the machines(DBMs)[45]consistofseverallayersoflatentunits
goalistocapturetemporaldynamicsinanefficientwaythat with undirected connections. A deep belief network (DBN)
allows generalization tosequences of arbitrary length.This [23]ontheotherhandconsistsofseverallayersofdirected
can be accomplished by sharing parameters across time, connections,withanRBMasitsfinallayer.
instead of re-learning them for every step. As mentioned In practical applications, the lines between supervised
previously, CNNs can accomplish parameter sharing in a andunsupervisedlearningareoftenblurred[22,p.105].Un-
shallowway,byapplyingthesamekernelatdifferentpoints supervisedpre-trainingisatechniquewherebysingle-layer
in time. Recurrent neural networks (RNNs), on the other unsupervisedmodels,suchasRBMs,areiterativelytrained
hand, introduce recurrent connections in time, allowing and stacked into a deep model [23]. Both supervised and
parameterstobesharedinadeeperway[22,ch.10]. unsupervisedmodelscanbenefitfromthelearnedhierarchy
A basic RNN extends the feedforward architecture by of feature representations: By adding a classification layer
allowing recurrent connections to exist within layers. Sim- and supervised fine-tuning, it was found that difficulties
ply put, the previous model state can be regarded as an in training fully-connected DNNs could be overcome [46].
additional input at each temporal step, which allows the This technique was responsible for the resurgence of deep
RNNtoformamemoryinitshiddenstateoverinformation learning since 2006, but has later gone out of fashion as it
fromallpreviousinputs[39].RNNshavearepresentational is no longer required for training fully-connected DNNs.
advantage over hidden Markov models (HMMs), whose Another use is found in initializing deep unsupervised
discrete hidden states limit their memory [40]. Even RNNs models,suchasDBNsandDBMs.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 4
2.2 Thenotionofaffectinaffectrecognition advantage of available sensor data. The annual competi-
Thenotionsofaffectandemotion5 aresubjectiveinnature. tions AVEC and EmotiW have been established to focus
on multimodal affect recognition, encouraging researchers
As a result, the question of how affective states should be
to benchmark model accuracy in a fair manner. Multiple
represented is still an unsolved issue with no consensus
factorsmakebuildingsuchmodelschallenging:
reachedintheliterature.Twodifferingviewsaredominant:
The categorical view of affect as discrete states, and the • Natural settings. Data collected in uncontrolled real-
dimensional view of affect where states are represented in a life settings can introduce many additional sources
continuous-valuedspace.Bothcategoricalanddimensional of variation that need to be accounted for. Visually,
models are also used to decode the nature of human emo- these include occlusions, complex illumination con-
tioninspecificbrainregions(e.g.,amygdala,insula),which ditions in different settings, spontaneous behavior
issubjecttoanongoingdebateinaffectiveneuroscience(see and poses, as well as rigid movements. Audio may
[48] for a review). Affective computing has largely stayed contain background noise, unclear speech, interrup-
agnostic on the debate regarding the appropriateness of tions,andotherartifacts.
these views [9]. In practice, the type of affect labels avail- • Temporal dynamics. The temporal dimension is an
able in databases is often chosen to most naturally fit the integralelementofaffectivedisplaythathasnotyet
availablesensordata—categoricalifimagesorsequencesare been fully embraced by research on affect recogni-
to be matched with a single affective state, and dimensional tion,especiallyFER.Temporaldynamicscanberich
forcontinuousaffectprediction(seeSection3.4). with contextual information that could be captured
The notion of categorical affective states originated in bymodels,e.g.,todistinguishbetweendisplaysthat
Charles Darwin’s research on the evolution of affect, and aresimilarintheshortterm,andtoassesstherelative
remains much discussed in the literature [49]. Most cate- importanceofspecificsegments[7].
goricalmodelsassumetheexistenceofbasicaffectivestates • Multimodal sensor fusion. Humans rely on multiple
as building blocks of more complex ones [49]. In practice, modalities when expressing and sensing affective
Ekman’s basic affective states derived from facial expres- states in social interactions. It seems natural that
sions [50] are most commonly used in the context of affect computers could benefit from the same variety of
recognition;theycompriseanger,happiness,surprise,disgust, sensors[47].Infact,therehasbeenanincreasedinter-
sadness, and fear. Others include Plutchik’s wheel of emo- esttodesignsuchmultimodalsystems[59],anditis
tions [51], and the basic model [52]. In application-driven generallyacceptedthataudiovisualsensorfusioncan
efforts, non-basic affective states are more commonly used, increase model robustness and accuracy. However,
where specific user states and the intensity thereof are of without knowledge of how exactly humans handle
interest—suchasboredomorfrustrationinHCI[9],[11].This thisproblem,itisunclearhowandatwhichlevelof
approach sidesteps issues of theoretical validity to instead abstractionthemodalitiesneedtobefused.
focusdirectlyontherelevantnon-basicaffectivestates. • Limited availability of labeled data. The increase of
Dimensional models aim to avoid the restrictiveness of factors that make learning difficult—such as model
discretestates,andallowmoreflexibledefinitionofaffective size and the nature of ambitious affect recognition
states as points in a multi-dimensional space spanned by tasks—has outpaced the increase in availability of
concepts such as affect intensity and positivity. This ad- labeled examples. It is a challenging task to suc-
dresses the notion that discrete categories may not fully cessfully train large DNNs with relatively few la-
reflect the complexity of affective states and the associated beled data. Advanced regularization methods are
problemsinlabelingandevaluatingaffectivedisplays(e.g., necessarytoavoidproblemslikeoverfitting.Transfer
labeler agreement) [11], [9], [53]. For affect recognition, the learning [60] attempts to transfer knowledge be-
dimensionalspaceiscommonlyoperationalizedasaregres- tween emotion corpora and other domains such as
siontask(e.g.,forarousal[54],[18])orasaclassificationtask object recognition. Furthermore, unsupervised and
wherethecontinuousspaceisdiscretized(e.g.,fordifferent semi-supervised learning are being explored to ac-
levels of arousal, [55], [56]). Mappings can be established cessknowledgeencodedinunlabeleddatasets.
between dimensional and categorical models of affect (e.g.,
[57]). The most commonly used example is Russell’s cir-
2.4 Towardslearningdeepmodelsofaffect
cumplex model [58], which consists of the two dimensions
While traditional human affect recognition systems follow
valence and arousal, sometimes extended by dominance and
standard machine learning approaches, it seems intuitive
likability.
thatthesealgorithmsmaybenefitfrombeingmorereminis-
centofhowthehumanbrainworks.Leadingresearchersin
2.3 Frontiersinaffectrecognition affectivecomputinghavesupportedthisnotioninthepast,
voicing the expectation that biologically inspired systems
Research on affect recognition has seen considerable
could be more suitable for affect recognition than human-
progressasthefocushasshiftedfromthestudyoflab-based,
engineeredsystems[1],[47].
acteddatabasestoreal-lifescenariossincethe2000s[11].In
InlightofthechallengesdiscussedinSection2.3,current
the 2010s, the focus has remained on such conditions, as
affect recognition tasks combine several characteristics of
research is exploring more complex models to better take
difficult perception problems in AI. The variation in the
sensor data is dominated by factors mostly unrelated to
5.Thetermsaffectandemotionarewidelyusedsynonymouslyinthe
contextofaffectivecomputing[47]. the task, (i) spatially (e.g., in image data and short-termJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 5
audio segments), (ii) temporally (e.g., in image sequences 200
andlong-termaudio),and(iii)distributedacrossmodalities
on different time scales. In order to perform the desired
150
classification and regression tasks in affect recognition, it is
necessary to obtain useful feature representations from the
100
available multimodal high-dimensional sensor data. These
feature representations should disentangle the underlying
50
factorsofvariationinordertoisolatethefactorsthatdistin-
guishaffectivedisplays[61].
Previous approaches rely on cleverly handcrafted fea- 0
2010 2012 2014 2016
tures and shallow learning models to derive such feature Year
representations. Unfortunately, the steps involved in de-
signing handcrafted features can be work-intensive and
error-prone. Motivated by the success of deep learning in
other AI-relevant tasks based on images and speech [24],
researchershavestartedtousedeepmodelsforrepresenta-
tion learning in affect recognition. As discussed in Section
2.1, DNNs are more efficient at learning and representing
thegivenfunctions,bothstatisticallyandregardinghuman
involvement [20]. Labeled data and computation time are
limitedandcostly,incentivizingtheadoptionofsuchmod-
elsinaffectrecognition.Basedonthepreviousdiscussionof
challengesandmodeltypes,weidentifythreedistinctways
in which deep learning is leveraged to learn useful feature
representationsforaffectrecognitiontasks:
• Learningspatialfeaturerepresentations.Forimages[61],
short-term6 image sequences [62] and audio seg-
ments [54], DNNs and especially CNNs are used to
learnspatialfeaturerepresentations.
• Learning temporal feature representations. To learn rep-
resentations of the temporal dynamics found in au-
dio[63],sequencesofimages[64],andphysiological
measurements [55], DNNs and especially RNNs are
successfullyapplied.
• Learning joint feature representations for multimodal
data. In multimodal approaches, DNNs are lever-
agedtolearnjointfeaturerepresentationsfrommul-
tipleunimodalfeaturerepresentationstoaccomplish
feature-levelfusion[65].
After learning such feature representations, the derived
features (or deep features) can be used as input for simple
classification and regression methods, such as logistic re-
gression and support vector regression (SVR). In Section
3, we give an in-depth breakdown of how deep learning
modelsforhumanaffectrecognitionareappliedinpractice.
3 THE STATE OF THE ART
Since around 2010, deep learning methods have started to
fulfill crucial roles in human affect recognition systems.
Indeed, it is safe to state that they are currently driving
most state-of-the-art results in this field. Our goals are to
(i) measure the adoption of deep learning in the field, and
(ii)breakdownwhatspecificfunctionsarebeingfulfilledby
DNNsinhumanaffectrecognitionsystems.
Because of these goals, we conducted a two-stage liter-
ature search7 of studies on human affect recognition since
6.Forthisstudy,weconsidersegmentsofupto1sasshort-term.
7.Databases searched: The ACM Digital Library, IEEE Xplore,
SpringerLink,andWebofScience.
dehsilbupseidutsforebmuN
Model
Shallow
Deep
(≥3layers)
Fig. 1. Stage 1: Number of studies on affect recognition published by
year,distinguishedbytheiruseofdeeporshallowlearningmodels.
2010. This search is restricted to studies that use sensor
data of cues directly given by the human body8, i.e., facial
expressions, movements, speech, and physiology. Further
affect recognition fields that focus on affect communicated
throughmeansotherthanthehumanbody—suchasgeneric
images and videos [66], music [67], [68], and text [69]—are
out of the scope of this review. See [70], [7], and [11] for
earlierreviewsonhumanaffectrecognition.
Stage 1 of our search yielded a total of 950 studies.
Following the terminology introduced in Section 2.1, these
studieswerethenmanuallyclassifiedtoindicatetheuseof
shallowordeepmodels.Consideringthetemporaldistribu-
tionofthestudies,Fig.1illustratestwodevelopments:
• Between 2010 and 20169, there was an overall in-
crease in the number of studies on human affect
recognition(25%year-on-yearincreaseonaverage).
• Deep learning has gained considerable attention in
thisfieldsince2010:Upfromonetotwostudiesper
year,itisbeingemployedin52%ofstudiesin2017—
a119%averageyear-on-yearincreaseinthenumber
ofpublishedstudies.
InStage2,wefocusexclusivelyonthe233studiesfound
in our review that use deep learning for affect recognition.
They form the basis for the review in this section. These
studies were further classified by (i) the usage of deep
learning according to the three ways introduced in Section
2.4, and (ii) the modality used as a basis for recognizing
affect. Table 1 lays out the result of these classifications by
listing the numbers of studies falling into each category.
Note that individual studies often use multiple modalities
orapplydeeplearninginmorethanoneway.
FromTable1,itisapparentthattheapplicationofDNNs
for FER has attracted the most attention in the literature,
featured in almost twice as many studies compared to
SER,whichisfeaturedsecondmostfrequently.Considering
physiologicalsignals,DNNsaremostfrequentlyappliedin
studiesbasedonelectroencephalography(EEG).
In the central part of Table 1, we can see that learning
feature representations of spatial information is the most
8.Sinceweonlylookatrecognitionofaffectivestates,facialaction
unitdetectionandfacialfeaturepointdetectionareexcluded.ForSER,
werestrictthescopetoparalinguisticcontent,whichisthemainfocus
ofresearchduringthetimeframecoveredbyourreview.
9.The search only contains partial data for 2017, hence 2017 is
excludedhere.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 6
TABLE1
Numberofidentifiedstudiesonhumanaffectrecognitionbymodality,andapplicationsofdeeplearning.
Modality Learningdeepfeaturerepresentations Totala
Spatial(Section3.1) Temporal(Section3.2) Joint(Section3.3)
Facialexpression 141 36 19 158
Visual
Bodymovement - 4 2 4
Auditory Speech 35 64 17 82
EEG 4 17 3 18
Physiological Peripheral - 6 4 7
Other - 2 3 3
Totala 173 105 21 233
aTotalsmaynotequalrow/columnsumsduetooverlapsbetweenmodalitiesandapproaches.
common application of DNNs in human affect recognition
Features Static
to date, especially in FER. For SER in particular, learning (S1a) (Gabor, spatial
oftemporalfeaturerepresentationswithDNNsisanactive LBP,…) FCDNN features
research area. A less active, but developing application
Low-level Short-term
area of DNNs in human affect recognition is learning joint (S1b) descriptors spatial
featurerepresentationsforthepurposeofearlyfeaturefusion (MFCCs,...) FCDNN features
acrossdifferentmodalities.
Static
(S2a) spatial
2DCNN features
3.1 Learningspatialfeaturerepresentations
Thegoalinspatialfeaturelearningistolearnexpressivefea- Short-term
(S2b) spatial
ture representations of data with spatial structure. In prac-
features
3DCNN
tice, we find that deep architectures are frequently applied
toexploitthischaracteristicinsensorrecordingscontaining
Short-term
staticandshort-termcuesofaffectivebehavior—bothvisual (S2c) spatial
imageryandshortsegmentsofaudioandphysiologicaldata 2DCNN features
can be interpreted in this way. Some approaches combine
Short-term
handcrafted features with deep architectures such as fully- (S2d) spatial
connected DNNs (see Fig. 2, S1a–S1b). As discussed in 1DCNN features
Section2.1.2,thedesignofCNNsisbasedaroundtheprior
Short-term
of spatial coherence. Hence, CNNs are the most popular (S2e) spatial
approachforlearningspatialfeatures(seeFig.2,S2a–S2e). 2DCNN features
3.1.1 LearningspatialfeaturesforFER Fig.2.Applicationsofdeeplearningforspatialfeaturelearningwithfully-
connectedDNNs(S1a–S1b)andCNNs(S2a–S2e).
Mehrabian [71] famously posited that 55% of the emotion
conveyed in a message is perceived visually. Indeed, FER
is most prominently featured in our search results on spa- distributions.Whiletheyproviderobustnessagainstillumi-
tial feature learning, as Table 1 reveals. Detailed surveys nation variations, they are less suitable for discrimination
are available [72], [73], giving an overview of the field in between high-level concepts such as facial features. On the
general. In this section, we discuss the application of deep contrary, CNNs natively learn a hierarchy of features that
learning across 141 studies to learn spatial features from builds from low-level to high-level representations. While
images. We distinguish between approaches using fully- thefirstlayerlearnsgeneralconceptssimilartoGaborfilters
connectedDNNs(Fig.2,S1a),andCNNs(Fig.2,S2a–S2b). [22,ch.9.10],thelastlayerslearnmorespecificconceptsthat
Conventional approaches rely on handcrafted features tend to be semantically interpretable. As a result, 93% of
to represent faces by their shape or appearance. Shape repre- studies reporting direct comparisons find that deep spatial
sentations use explicit knowledge about facial geometry to featuresoutperformhandcraftedspatialfeaturesforFER.
encodeagivenexpression,suchasthelocationofcertainfa- Learning spatial features from intermediate handcrafted fea-
cial feature points. The most common appearance features, tures (S1a in Fig. 2). As evident from Table 2, especially
suchaslocalbinarypatterns(LBP),localphasequantization during the initial adoption of DNNs in FER, handcrafted
(LPQ), and histogram of oriented gradients (HoG), encode features and DNNs were combined in a sequential way. In
low-level texture information in local histograms. Other the first step, this approach extracts low-level handcrafted
methods of feature extraction include convolving the input featuresfrompixelvalues.AppearancefeaturessuchasLBP
with handcrafted Gabor filters and scale-invariant feature (e.g., [74], [75]) and Gabor features (e.g., [76], [77]) are pre-
transform(SIFT).AsSariyanidietal.[72]pointedout,such ferredforthisapproach.Duetothereduceddimensionality,
handcraftedfeaturesfocusonlow-leveldescriptionofedge itisthenfeasibletoapplyfully-connectedDBNs(e.g.,[76])JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 7
TABLE2
4 Learningspatialfeaturerepresentations:Numberofstudiesadopting
differentapproachesovertime.
3
Year FCDNN 1D/2D/3DCNN 2
S1a S1b S2a S2b S2c S2d S2e
1
2012 - - 1 - - - -
2013 - 1 1 - - - -
2014 3 1 4 - 1 - - 0
2015 3 - 24 2 2 - - 1 2 3 4 567810 20 30 40 60 80100
Numberofconvolutionallayers
2016 2 2 42 - 5 3 3
2017 3 2 57 9 11 9 1
Total 11 6 129 11 19 12 4
andSAEs(e.g.,[77],[78])forunsupervisedlearningofhigh-
levelfeatures.
Learningspatialfeaturesdirectlyfrom2DimagewithCNNs
(S2ainFig.2).CNNsarewellsuitedtolearnspatialfeatures
directly from image pixels. As becomes clear from Tables
1 and 2, this approach dominates our search results. Due
to a lack of understanding why deep learning works well
inpractice[79],andlimitedavailabilityoflabeleddata,the
challenge faced by researchers is choosing an appropriate
architecture. Fig. 3 provides an overview of typical choices
regardingthenumberoffully-connectedandconvolutional
layers, and indicates whether transfer learning was used.
We can distinguish between two approaches: Region I in
the left of Fig. 3 refers to architectures totalling six or
less convolutional and fully-connected layers. These CNN
architectures,whicharespecificallydesignedforFER,make
up56%ofthestudies.Mostofthesestudiesrelyonsmaller
modelsizesinsteadoftransferlearningtoavoidoverfitting
therelativelysmallnumberofexamples.
Region II consists of architectures with more convolu-
tional layers, and hence potential to learn more expressive
features. Many of these are existing architectures that have
provensuccessfulforothertaskssuchasobjectrecognition.
Most frequently chosen are VGG Net [80] (23 studies) and
AlexNet [36] (18 studies). AlexNet is the architecture that
won the 2012 ILSVRC [38]. It consists of five convolutional
and three fully-connected layers, with a total of 60 million
parameters. VGG Net, an entry at the 2014 ILSVRC, is
a deeper CNN coming in variants of 16 and 19 layers.
Other choices include GoogLeNet [81] (winner of ILSVRC
2014), ResNet [29] (winner of ILSVRC 2015), and Tang’s
winning entry at ICML 2013’s FER challenge [82]. Most of
these studies use additional datasets and transfer learning
toavoidoverfitting.
Learning short-term spatial features from image sequences
with 3D CNNs (S2b in Fig. 2). Building on the concept of
spatial representation for a single 2D image, this approach
interprets an image sequence as a spatio-temporal volume.
Standard CNNs can be extended to accept 3D volumes as
input by increasing filter dimensionality to support spatio-
temporalconvolutions.Such3DCNN architectures[83]are
theoretically capable of learning spatio-temporal features
such as motion of facial action units. Since the number of
model parameters and therefore the number of required
examplesincreasewithtemporaldepthofinputsequences,
sreyaldetcennocyllufforebmuN RegionI RegionII Transferlearning
AlexNet VGG-19
NO
YES
VGG-16
Numberofstudies
ResNet-50 5
10
GoogLeNet
15
Fig.3.NumberofhiddenlayersinCNNsforspatialfeaturelearningin
FER.
thechosennumberofframesistypicallyquitelow10,and3D
CNNs are limited to short-term sequences (up to 1 s). The
nature of this approach also requires that input sequences
consistofastandardizednumberofframes.Thismeansthat
researchersneedtodownsampleorinterpolatevideos[62].
Group-level FER. Group-level affect recognition is a sub-
discipline of FER, where the goal is to assess the overall
expression of all persons in an image. It has been featured
in the EmotiW competition in 2016 [12] and 2017 [86]. For
this purpose, spatial features are typically extracted for
each person, and fused in some way—e.g., by considering
multiplefacesasasequenceandapplyinganLSTM[15].
Complementarity of deep and handcrafted spatial features. In
benchmarking experiments across various datasets, many
studies found that features extracted with CNNs lead to
higher recognition accuracies than handcrafted features
(e.g.,[87],[88],[89],[90],[91],[64],[92],[93],[94]).However,
several challenge-winning studies [95], [15], [17], choose
to use both handcrafted and deep features, e.g., by score-
level[95]ormodel-level[17]fusionofseparatehandcrafted
anddeepmodels.Thissuggeststhatdeepandhandcrafted
features are complementary. As of this writing, the find-
ings of most studies doing related comparisons support
this assumption [96], [97], [98], [99], though in many cases
a comparison is difficult as reported accuracies for well-
performing fusion approaches include features from other
modalities (see Section 3.3). Only one study reported that
deepandhandcraftedfeaturesarenotcomplementary[100].
Pre-processing to simplify the FER learning task. Instead
of learning from unprocessed images, the majority of the
reviewed studies apply some form of pre-processing to
imagedata.Thesestepsreducetheamountofvariationthe
model has to account for, and thus simplify the learning
task. Face cropping is standard practice, and reported to
increase model accuracy (e.g., from 54% to 72% in [94]; see
also[15]).Thisinvolvesthedetectionofthefaceandfeature
points,althoughsomedatabasescomewithpre-detectedor
cropped faces (e.g., [101], [102], [103]). Spatial normalization
techniques include face alignment and face frontalization:
Simple adjustment of face rotation and facial feature point
alignmentisreportedtoimprovemodelaccuracy(e.g.,from
54% to 62% in [94]; see also [15], [99]). More advanced face
frontalization involving the approximation of 3D shape is
useful when dealing with 3D head pose variation [104],
[105]. Intensity normalization, on the other hand, aims to
10.Thereviewedstudiesusedbetween3[84]and16[85]frames.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 8
normalize illumination related factors such as brightness We distinguish between approaches using fully-connected
and contrast, which can be an issue with images taken in DNNs(seeFig.2,S1b)andCNNs(seeFig.2,S2c–S2d).
the wild or across multiple databases. Researchers report Research has shown that short-term spectral, prosodic,
improvementsinaccuracybyapplyingintensitynormaliza- and energy features of speech carry affective information
tion(e.g.,from54%to57%in[94];seealso[98]). [121]. In conventional SER approaches, it is common prac-
Limited availability of labeled data. Since the number of tice to capture such properties using handcrafted features
labeledexamplesforFERremainsrelativelylimited,large11 known as low-level descriptors (LLDs). LLDs are sampled
models are likely to overfit the data [106], [107], [89], from small overlapping audio segments or frames; a com-
[108].Severaltechniquesareavailabletoaddressthisprob- monchoiceisawindowsizeof25msandastepsizeof10
lem: (i) transfer learning, (ii) data augmentation, and (iii) ms [122]. Most recent models use pre-defined sets of LLDs
architecture-andtrainingchoicespromotingregularization, forspatialmodeling.StandardsetssuchaseGeMAPS[121]
such as dropout. In transfer learning, the goal is to use ad- and ComParE [123] typically include cepstral descriptors
ditional corpora to learn generic visual descriptors that are such as Mel-frequency cepstral coefficients (MFCCs), energy-
foundtobeeffectiveinimprovinginitialmodelparameters relateddescriptorssuchasshimmerandloudness,frequency-
[109],andthusreduceoverfitting.Especiallywhenadopting related descriptors such as pitch and jitter, and spectral
large models originally intended for object recognition, it parameters. They can be extracted with software tools
is common to pre-train on a large-scale database such as such as openSMILE [124] and openEAR [122]. This review
ImageNet [110] (14M annotated images). Another database highlights how DNNs are used in the state of the art to
frequently chosen for pre-training is VGG-Face [111] (2.6M complementandreplacehandcraftedLLDs.Overall,90%of
face images). Smaller, more relevant databases such as studies reporting direct comparisons find that deep spatial
FER2013 [103] and CK+ [112] are also frequently used for featuresoutperformhandcraftedspatialfeaturesforSER.
pre-training.Tomakeuseofgenericvisualfeatures,authors Learning spatial features from handcrafted feature represen-
acquire large models pre-trained for object classification, tations of speech (S1b in Fig. 2). A limited number of early
”freeze” the lower-level layers, and fine-tune a selection of applicationsinSERwerecombinationsofDNNswithhand-
higher-level layers for affect recognition. Improvements in craftedLLDs.Theideawastousefully-connectedDNNsto
accuracy are reported when following this approach (e.g., replaceGaussianMixtureModels(GMMs),whichoccupied
from39%to42%in[89];seealso[99],[92]). the role of short-term modeling in the commonly used
Data augmentation is a technique whereby existing im- GMM-HMM architecture from ASR. For example, Li et al.
ages or sequences are manipulated to reduce overfitting. [125] used a six-layer DNN to learn frame-level features
Researchers either use static rules to generate new ex- from concatenated MFCCs of a sliding context window. In
amples for the same original label, or manipulate images theirexperiments,thisyieldedanaccuracyimprovementof
randomly before training. Such manipulations include hor- morethan10%overGMMs.
izontalflipping,cropping,rotation,translations,changesto Learningspatialfeaturesfromrawspectralrepresentationsof
color, brightness, and saturation, as well as scaling. This speechwithCNNs(S2cinFig.2).ManyrecentstudiesinSER
way,researchersartificiallyincreasethenumberofavailable leverage DNNs to avoid the step of handcrafted feature
examples or training epochs by a factor typically between engineering (see Table 2). Mirsamadi et al. [63] pointed
10 and 30 [88], [113], [114], and up to 300 [115]. Studies out that most commonly used frame-level LLDs in SER
running experiments on this technique report accuracy im- can be derived from spectral representations of the raw
provements (e.g., from 79% to 89% in [87]; see also [74], speech signal. Without any feature engineering, they were
[94]). Dropout [116] is a technique that reduces overfitting abletolearnfeaturessimilartoLLDsfromtherawspectral
by randomly dropping out neurons during training, thus representationofindividualaudioframesat25ms,leading
forcing the network to learn redundantly. It is widely used toanaccuracyincreaseof4%.Suchlearnedfeaturescanbe
inthereviewedstudies—57%reportusingdropoutforfully- showntohavesimilaritiestohandcraftedLLDs[54].
connectedlayers,and12%reportusingitforconvolutional Spectrogram representations are computed using mul-
layers.Khorramietal.[87]reportedanincreaseinaccuracy tiple frames and allow speech segments to be interpreted
of2.5%afterapplyingdropouttofully-connectedlayers. as 2D images. They can be based on Fourier transform of
the raw waveform (e.g., [126], [127]) or minimally hand-
3.1.2 LearningspatialfeaturesforSER engineered on the log Mel-frequency cepstrum representa-
tion (e.g., [85], [128]), which closer matches the character-
Beyond spoken words, the acoustic properties of human
istics of human auditory perception. CNNs can be applied
speecharerichwithinformationaboutthespeaker,suchas
to directly learn features from such representations, which
gender, age, and affect (see [117], [118] for comprehensive
are typically between 250 ms and 1 s in length. Since the
reviews).Inthissection,wefocusonthe35studiesthatem-
suggested minimum time required to identify affect from
ploy deep learning for spatial feature learning in SER. The
speech is quoted in the literature as 250 ms [129], [85], the
potential of replacing or complementing traditional short-
resulting features can directly be used for classification of
termdescriptorswithDNNsinspeechrelatedclassification
short utterances (e.g., [126], [130]), or be regarded as short-
tasks was pointed out as early as 2009 [119]. Especially
termfeatures(e.g.,[85],[131])forfurthertemporalmodeling
CNNs are found useful in modeling speech features [120].
asdiscussedinSection3.2.3.
Most studies chose custom architectures of one to three
11.WhenusingCNNsforFER,researcherstendtoresorttomethods
convolutionallayersandonetothreefully-connectedlayers
like transfer learning in Region II, see Fig. 3. Note however that the
numberoftrainableparameterscanbesubjecttomanyotherfactors. for this task. Some considered using known architecturesJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 9
from object recognition [132], [85], [127]. Here, the authors
100
regardeditasnecessarytopre-traintheselargermodelson 80
60 the ImageNet database to avoid overfitting the relatively
40
fewlabeledexamples. 30
LearningspatialfeaturesfromtherawwaveformwithCNNs 20 l
(S2dinFig.2).Featurelearningdirectlyfromtherawwave-
10
form was proposed in 2011 [133]. Since 2016 (see Table 8
6
2), a number of studies have started applying this idea in
4
SER. CNNs can be applied to raw 1D audio (see Section 3
2.1.2);indeed,allexceptonestudyinourreviewuseCNNs 2
for this task. Trigeorgis et al. [54] were the first to do so: 2012 22001133 222222222000000000111111111444444444 22222222222222222222222222222000000000000000000000000000001111111111111111111111111111155555555555555555555555555555 2222222222222222222222222222222222222222222222222222000000000000000000000000000000000000000000000000000011111111111111111111111111111111111111111111111111116666666666666666666666666666666666666666666666666666 22222222222222222222222222222222222222222222222222222222222222222222222222222222222222000000000000000000000000000000000000000000000000000000000000000000000000000000000000001111111111111111111111111111111111111111111111111111111111111111111111111111111111111177777777777777777777777777777777777777777777777777777777777777777777777777777777777777
Year
They used two convolutional layers for spatial modeling,
almost doubling ground truth correlation over LLDs for
arousal, and slightly improving for valence. Bertero et al.
[134]proposedoneconvolutionallayerwith25mskernels,
whichachieveda3%accuracyimprovementoverLLDs.
Limited availability of labeled data. To avoid overfitting,
both dropout [116] and batch normalization [33] can help
toachievebetterregularization.Dropoutisappliedinfully-
connected(reportedin43%ofstudies,e.g.,[135])andconvo-
lutionallayers(14%ofstudies,e.g.,[108]).Multiplestudies
haveshownthattransferlearningcanimprovemodelaccu-
racybyleveragingadditionalsourcesofrelatedknowledge
(e.g.,fromotherparalinguistictasks[136],variousstandard
databases [137], and different affect representations [135]).
SoundNet [138], a 1D CNN trained with unlabeled video,
has been shown to perform well in SER even without
fine-tuning [139], and was featured in a challenge-winning
submission [17]. Semi-supervised learning can give access
toknowledgecontainedinunlabeleddatasets[140].Knowl-
edge transfer from domains like music [141] and visual
objectrecognition[132],[85]isalsopossible.Dataaugmen-
tation to artificially increase the dataset size is used less
oftenthanforFER;notableexamplesincludetheadditionof
Gaussian noise [129], different sampling frequencies [130],
andmodifiedplaybackspeed[142].
3.1.3 Learningspatialfeaturesfromphysiology
As early as 2001, physiological responses were shown to
conveyinformationaboutaffectivestatesinmachinelearn-
ing[8].Candidatesincludemeasuresoftheperipheralphys-
iologyviaelectrocardiography(ECG),electrodermalactivity
(EDA),andbrainactivityviaEEG.However,affectivecom-
puting research initially focused mostly on FER and SER,
partially due to a lack of interest and inconvenient sensors
[7].Morerecently,interestinphysiologicalaffectrecognition
isseeingaresurgence[143],owedinparttothecapabilities
ofmodern,portablemonitoringdevices[144].
As part of our review, we identified only four studies
using2DCNNstolearnspatialfeaturesfromEEGdata(see
S2e in Fig. 2). All achieved accuracy improvements over
handcrafted approaches, but a lack of training data was
also mentioned [145], [146]. For example, Yanagimoto and
Sugimoto[145]dividedtheraw16-channelEEGdatainto1s
segments and used a seven-layer CNN with 10 ms kernels
onthefirstlayer,leadingtoaccuracyimprovementsofover
20%.SimilartosomeworkinSER,Lietal.[55]considereda
spectrogramrepresentationoftheEEGsignalataframesize
of 1s. Another way to learn spatial features from the EEG
signal is to reflect it as a 2D map representing the location
oftheelectrodesonthehead[147].
sreyal
neddih
fo
rebmuN
Fig.4.NumberofhiddenlayersusedforspatialfeaturelearninginFER.
3.1.4 Takeawaysforspatialfeaturelearning
• Deep spatial features lead to higher accuracies than
handcrafted spatial features. Out of 103 studies that
reportedcomparisons,93%supportthisfinding.
• However, in contrast to fields such as object- and
speech recognition, both are often found to be com-
plementary in affect recognition as of this writing.
Thissuggeststhatthefullpotentialofdeeplearning
inaffectrecognitionmaynothavebeenseenyet.
• CNNs are the most widely used architecture for
spatial feature learning (91% of the studies in our
review; see Table 2). Instead of using spectrogram
representations,recentresearchstartstoapplyCNNs
directlytorawspeechandphysiologicaldata.
• To achieve higher accuracy, research strives towards
“deeper”models(Fig.4illustratesthisforFER),but
runsintotheproblemofoverfitting.Thisisthemain
challengeforcurrentresearch.
3.2 Learningtemporalfeaturerepresentations
When learning from sequences, the goal is to learn feature
representations that capture temporal dynamics [40]. This
allows models to consider the temporal variation of spatial
characteristics in sensor data (e.g., in SER [118] and video-
based FER [106]). As discussed in Section 2.1, both CNNs
and RNNs provide architectures that can learn represen-
tations of sequences of data. We found that the existing
architectures—spanning all studies and modalities—can be
classified into one of three approaches illustrated in Fig.
5: (T1) fully-connected DNNs for learning spatio-temporal
features from aggregated frame-level spatial features, (T2)
RNNs for global temporal modeling based on frame-level
spatialfeatures,and(T3)CNNsforlocaltemporalmodeling.
3.2.1 LearningtemporalfeaturesforFER
A straightforward approach to derive sequence-level fea-
tures from video data is to first extract high-level spatial
features (such as facial characteristics, see Fig. 7) from
individual frames, and then aggregate these in some way.
This can be achieved by simple feature pooling strategies
such as mean pooling, max pooling, or feature concate-
nation. However, such strategies typically ignore most of
the temporal variation in the sequence, which may contain
valuablecontextualinformation.Well-designedmodelsseekJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 10
Spatial
features
RNN
Spatio-
Spatial
temporal features
RNN features
Spatial
features
RNN
Spatio-
(T3) temporal
features
… …
Spatial
features
Spatial
features
Spatial
features
(T2)
…
(T1)
t be improved: 94% of studies reporting comparisons with
handcrafted approaches find that deep temporal features
performbetterforFER.
Learningspatio-temporalfeaturesfromaggregatedframe-level
Spatio-
spatial features (T1 in Fig. 5). In some cases, fully-connected
temporal
FCDNN features DNNs are applied to achieve dimensionality reduction on
high-dimensional spaces of aggregated handcrafted fea-
tures. For example, Zhang et al. [148] and Ranganathan et
*
al. [149] used fully-connected DBN models to learn from
aggregated facial feature point trajectories, both improving
recognitionaccuraciesovershallowaggregationstrategies.
Global temporal modeling with RNN based on frame-level
spatial features (T2 in Fig. 5). The properties of RNNs, as
discussed in Section 2.1, make them well-suited to model
the temporal variation of frame-level spatial features. This
approachfirstextractshigh-levelspatialfeaturesfromeach
face image, which are then considered as sequential input
to the RNN. Advantages of this approach include the abil-
ity to process long sequences, and the possibility of both
* sequence-level and continuous frame-level affect recogni-
tiononimagesequencesofarbitrarylength.Earlyon,RNNs
wereusedto learntemporalcontextfromhandcraftedspa-
tialfeaturessuchascoordinatesoffacialfeaturepoints[150],
opticalflow[129],andLBP[151].
More recent studies combine RNNs with deep methods
for spatial feature learning discussed in Section 3.1.1, by
adoptingdeepfeaturesfromthelastlayerofaCNNtrained
for affect recognition (e.g., [106], [18], [64]). We see both
Spatio-temporalCNN CNN-RNN (e.g., [18], [90]) and CNN-LSTM (e.g., [108],
[105], [64]) architectures, with CNN-LSTM being the more
frequent choice among the reviewed studies. Global tem-
*
poral modeling is found to lead to improved accuracies
when compared with simpler methods such as pooling of
*Note:Theseareunimodalapproachesapplicabletodifferent
modalities.Audioandvideoarestandinginasexampleshere. spatial features (e.g., [152], [90], [108]). A disadvantage of
most CNN-LSTM implementations is that training occurs
Fig.5.Applicationsofdeeplearningfortemporalfeaturelearningwith
in a disconnected way: The CNN is trained on frame-level,
fully-connectedDNNs(T1),RNNs(T2),andCNNs(T3).
specifically for static spatial affect recognition. Hence, the
extracted features are not necessarily optimal for further
TABLE3 temporalcontextlearningbytheRNN.End-to-endtraining
Learningtemporalfeaturerepresentations:Numberofstudiesadopting of the entire CNN-LSTM system addresses this problem,
differentapproachesovertime.
andcanleadtoaccuracyimprovements[108].
Local temporal modeling with 3D CNN (T3 in Fig. 5).
Year
FCDNN(T1)a RNN(T2)a CNN(T3)a
When using 3D CNN for spatio-temporal modeling of
V A P V A P V A P image sequences as discussed in Section 3.1.1, the line
2010 - - - - 1 - - - - between spatial and temporal representation learning can
2011 - 1 - 1 1 - - - - be blurred. While this approach is typically limited to very
2012 - - - 1 1 - - - - short sequences, with further pooling steps necessary to
2013 - - - - - - - - 1
derive sequence-level labels (e.g., [84], [85]), in some cases
2014 - 2 2 2 3 1 - - -
2015 2 3 2 1 - - 1 1 - spatio-temporal features can be derived for entire (short)
2016 3 10 4 10 5 2 - 5 2 sequences. For example, Gupta et al. [62] used a variant
2017 1 11 6 15 13 1 3 12 1
called slow fusion [153], which treats the time domain like
Total 6 27 14 30 24 4 4 18 4 a spatial domain, progressively learning low-level to high-
aModalities: V = Visual (FER, Body), A = Audio (SER), P = Physio level temporal features. As the amount of parameters re-
(EEG,Peripheral,Other). quiredduetothetemporaldepthoftheinputiseffectively
reduced,thisallowsformoreinputframes.
to further exploit such information. To some extent, this 3.2.2 Learningtemporalfeaturesfrombodymovement
ispossiblewithcommonhandcraftedfeatures:Appearance Besidesfacialexpression,bodymovementandgesturesare
featurescanbeextendedforspatio-temporalrepresentation othermeansofexpressingaffectvisually[1].Inthereviewed
by considering a third orthogonal plane [96], [99]. With studies, spatial features representing such movements are
DNNs, temporal modeling capabilities in FER can further extractedusingskeletalandshouldertracking.Forexample,JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 11
Ranganathan et al. [149] and Kaza et al. [154] used the found that the CNN-LSTM architecture yields the best re-
approach illustrated in Fig. 5 (T1), to learn spatio-temporal sult on Emo-DB [164]. Applications of similar architectures
featuresfromstatisticsofskeletaltrackingpointtrajectories. based on the raw waveform have also been attempted
ShouldercueswereusedbyNicolaouetal.[150]intheRNN [54], [108], showing that end-to-end learning can outper-
approachillustratedinFig.5(T2).Incomparisonwithfacial form shallow models. Overfitting is still a problem for this
expressionandspeech,theywerefoundtobelessexpressive approach due to the large number of model parameters
forpredictionofbotharousalandvalence. and limited dataset sizes. Mirsamadi et al. [63] found that
model performance is slightly lower with joint learning of
3.2.3 LearningtemporalfeaturesforSER both short-term spatial features and temporal context on
To derive fixed-length features at the utterance level, SER the IEMOCAP dataset [165], while both improve model
models traditionally aggregate LLDs by high-level statistical performancewhenappliedindependently.
functionals (HSFs) such as mean and standard deviation. Local temporal modeling with CNNs (T3 in Fig. 5). When
Standard sets of HSFs and LLDs are given in eGeMAPS CNNsareusedformodelingspeech,theytypicallycombine
[121] and ComParE [123]. HMMs have long served as a spatialmodelingintheshorttermwithtemporalmodeling
standard choice for further modeling of temporal variation of longer segments or entire utterances. The kernels of
in speech signals, especially in ASR [155], but also in SER higher-level (i.e., second or third) convolutional layers can
[125].Morerecently,RNNshaveemergedasapreferredway oftenbeinterpretedaslearningtemporalstructurebasedon
ofmodelingthesequentialaspectofspeech[156].Inparticu- spatialfeatureslearnedbythekernelsinthefirstlayer(see
lar,91%ofstudiesreportingcomparisonswithhandcrafted Section 3.1.2). For example, Trigeorgis et al. [54] performed
approachesfindthatdeeptemporalfeaturesperformbetter pooling across time after learning spatial characteristics
forSER.ThisreviewhighlightshowDNNscancomplement from the raw signal in the first layer, and added a second
or replace both HSFs and HMMs for learning temporal layer with 500 ms kernels to learn temporal characteristics.
representationsinSER. Similarly,Zhangetal.[85]usedanAlexNettoaddincreas-
Learning from aggregated frame-level features (T1 in Fig. ingly more temporal context to learned feature representa-
5). Since there is no consensus in the literature over a tions. It is worth noting that while some studies directly
”universal” handcrafted feature set with superior perfor- used CNN features for affect prediction [142], [128], others
mance [121], many recent studies have applied a ”brute- combined local temporal modeling with global temporal
force” approach, resulting in a large number of features modelingviaRNN[108],[163],orpoolingapproaches[85].
per utterance. This number varies from several hundred
[157] to several thousand [158], [123], depending on the
3.2.4 Learningtemporalfeaturesfromphysiologicaldata
employedLLDsandHSFs.DNNscanbeintegratedtolearn
morehigh-levelrepresentationsofthesehandcraftedspatio- Learning from handcrafted features with fully-connected DNNs
temporal feature spaces. Studies aiming to reduce feature (T1 in Fig. 5). As highlighted in Table 3, approach T1 is
spacedimensionalitywithdeeplearningalmostexclusively the primary application of DNNs for feature learning from
usefully-connectedDNNs,consistingoftwotofourhidden physiologicaldata.Forthispurpose,fully-connectedDNNs
layers. It is common to initialize model parameters layer- are initialized by iterative training and stacking of unsu-
wise via unsupervised pre-training as RBMs (e.g., [159], pervised models such as RBMs or AEs [166], and applied
[160]),orAEs(e.g.,[78]);subsequently,aSoftmaxclassifica- to functionals of frame-level spatial features. Typical for
tionlayerisaddedforsupervisedfine-tuning.Alternatively, EEG are handcrafted features derived from the frequency
DBNsor SAEscanserveas featureextractorsforclassifica- domain, such as power spectral density (PSD) coefficients
tionviasupportvectormachine(e.g.,[137]).Dimensionality of different frequency bands. Zheng et al. [167] found that
reduction of handcrafted features with DNNs can lead to DBNscanimproverecognitionaccuracyofmodelsbasedon
improvementsinaccuracyovervariousdatabases[161]. differential entropy features. Similarly, Xu and Plataniotis
Global temporal modeling based on frame-level features with [168] showed that DBNs can build on PSD features to
RNNs (T2 in Fig. 5). In this approach, an utterance-level outperform state-of-the-art methods on the DEAP dataset.
RNNmodelsthetemporalvariationofframe-levelfeatures. Deep learning has also been used as part of ensemble
Most straightforwardly, LLDs can directly be fed into the methods[169],andintheformofEchoStateNetworks[170]
RNN at the frame level (e.g., [141], [162], [63]). Depending for dimensionality reduction of handcrafted EEG features.
on the nature of the source audio, it can be beneficial to Yin et al. [171] used stacked autoencoders (SAEs) to learn
apply HSFs to frame-level LLDs according to a sliding high-level representations from various peripheral sensors
window before applying the RNN [151], [65]. One study including skin temperature and blood volume pressure,
suggestedthatasmallerwindowsize(2s)couldbethebest improvingthestate-of-the-artby5%.
choice [65]. In general, the addition of RNN for temporal Learning temporal context from spatial features with RNNs
modeling is associated with an increase in model accuracy (T2 in Fig. 5). RNNs have been used to learn temporal
(e.g., [129], [97], [162]). When dealing with dimensional contextfromEEGfeaturestoimproverecognitionaccuracies
labels,thisallowslearningfeaturesattheframelevel.Here, [146], [55]. Brady et al. [18] found that learning temporal
LSTMisfoundtooutperformstate-of-the-arttechniqueslike context with LSTM and handcrafted features leads to im-
SVR(e.g.,[150],[151]). provements over shallow baseline models. Ringeval et al.
Since2016,eightstudieshaveexploredcombiningdeep [65] used a similar approach. They found that while the
spatial features and RNN-based temporal feature learning. givenphysiologicalsignalhaslowerpredictivepowerthan
For spectrogram-based spatial features, Lim et al. [163] audiovisualsignals,botharecomplementary.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 12
TABLE4
Learningjointfeaturerepresentations:Numberofstudiesadopting
differentapproachesovertime.
FCDNN(J1)a RNN(J2)a
Year
VA VAP VP PP VA VAP VP PP
2011 - - - - 1 - - -
2012 - - - - 1 - - -
2013 1 - - - - - - -
2014 - - - - - 1 - -
2015 2 1 1 - - - - -
2016 2 1 - 1 1 - - -
2017 3 - - 1 4 - - -
Total 8 2 1 2 7 1 0 0
aModalities: V = Visual (FER, Body), A = Audio (SER), P = Physio
(EEG,Peripheral,Other).
Learning spatio-temporal representations from raw data with
CNNs (T3 in Fig. 5). A limited number of four studies have
attempted to learn spatio-temporal features directly from
rawphysiologicaldatafordiscriminationbetweenaffective
states.YanagimotoandSugimoto[145]usedaCNNonraw
16-channel EEG data to differentiate between positive and
negativeaffectivestates,whichisshowntooutperformshal-
low models based on common features. Similar results are
reported when learning from intermediate representations
basedondifferentialentropy[55].Martinezetal.[172]were
the first to learn deep features directly from the peripheral
physiology. For this purpose, they used CNNs trained in
an unsupervised way via AEs to learn features from raw
blood volume pulse and skin conductance signals, which
outperformedmodelsbasedonhandcraftedfeatures.
3.2.5 Takeawaysfortemporalfeaturelearning
• Deep temporal features lead to higher accuracies
thanhandcraftedtemporalfeatures.Outof73studies
thatreportedcomparisons,92%supportthisfinding.
• WhileCNNsarewellsuitedforlocaltemporalmod-
eling,RNNsarefoundtobeusefulforglobaltempo-
ralmodelingofaffect.
• Since2015,therearestudiesusingdeeplearningfor
bothspatialandtemporalfeaturelearning.
• We are starting to see studies implementing end-to-
end training for such models [54], [108], however
in this setting the problem of limited labeled data
becomesespeciallynoticeable[63],[139].
3.3 Learningjointfeaturerepresentations
It is generally accepted in the literature that multimodal
(e.g., audiovisual) sensor combinations have complemen-
taryeffectsandthusmayincreasemodelaccuracy[11].The
challenge in joint multimodal feature learning is how and
at what stage to fuse data from multiple modalities. This
challengeiscomplicatedbythehighdimensionalityofraw
data,differingtemporalresolutions,anddifferingtemporal
dynamicsacrossmodalities.Surveysonthegeneralproblem
of sensor fusion [173] and specifically on fusion for affect
recognition[59],[174]areavailable.
Fusion can be achieved at early model stages close to
the raw sensor data, or at a later stage by combining inde-
pendentmodels.Inearlyorfeature-levelfusion,featuresare
… …
Audio
features
(J1)
Visual
features
(J2)
t
* Audio-
visual
FCDNN features
*
Audio
features
RNN
Visual
features
* Audio-
visual
features
Audio
features
Visual RNN
features
*Note:Audiovisualisusedasanexample.
* Othersensorcombinationsarepossible.
Fig.6.Applicationsofdeeplearningforjointmultimodalfeaturelearning
withfully-connectedfusionDNNs(J1)andfusionRNNs(J2).
extracted independently and then concatenated for further
learning of a joint feature representation; this allows the
model to capture correlations between the modalities. Late
ordecision-levelfusionaggregatestheresultsofindependent
recognitionmodels.Todate,theliteraturegenerallyreports
thatdecision-levelfusionworksbetterforaffectrecognition
given the datasets and models currently used [65]. While
decision-level fusion typically only involves simple score
weighing, feature-level fusion is a representation learning
task that may benefit from deep learning. Here, we report
on the approaches of 21 studies that use deep learning for
jointfeaturelearningfrommultimodaldata.
3.3.1 Learningjointfeatureswithaudiovisualdata
The most common sensor combination found in 18 studies
involvesfacialexpressionsandspeech.
Feature-levelfusionwithfully-connectedDNNs(J1inFig.6).
In this approach, joint feature representations are learned
without considering the temporal context for fusion. For
both modalities, video-level features are extracted using
FER and SER methods that may involve both handcrafted
and deep features (see Sections 3.1 and 3.2). A fully-
connectedDNN,typicallyinitializedviaunsupervisedpre-
training,thenlearnsahigh-leveljointfeaturerepresentation
of both modalities as an improvement over “shallow” fea-
ture fusion. Kim et al. [159] and others (e.g., [149], [148])
demonstrated how this can be achieved with DBNs. This
approach is feasible especially in cases where the goal is to
labeleachvideowithoneaffectivestate.Alternatively,joint
feature representations can be learned at the frame level,
and then aggregated to the video level: Zhang et al. [85]JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 13
usedaDBNtofuseframe-levelaudiovisualfeatureslearned is essential to further understanding of the field. The main
independently via CNNs; the learned features are average- differences lie in the available modalities, the number of
pooled for classification at the video level and lead to an subjects and examples, details on how data was acquired,
improvementoverstate-of-the-artmethods. how affect was elicited and annotated, as well as the type
Feature-level fusion with RNNs (J2 in Fig. 6). Especially of affective states used for labels. Some databases are more
when predictions are required at the frame level for di- frequently mentioned due to them being featured in com-
mensionalaffectivestates,feature-levelfusioncouldbenefit petitions. In Table 5, we give a summary of the 15 most
by taking into account the temporal context. Modeling via commonlyuseddatabasesinthereviewedstudies.
RNNs makes this possible, potentially improving model Asexpectedfrompreviousfindings,thevisualmodality
robustnessandhelpingtodealwithtemporallagsbetween is featured most frequently. Some databases focus exclu-
modalities [162]. Initial studies reported that dynamic fea- sively on static FER with discrete labels of categorical af-
ture fusion can lead to performance improvements com- fectivestates.Here,morerecentdatabasessuchasFER2013
pared to simpler fusion strategies [162]. However, several and SFEW2 tend to contain more examples than older
other studies based on handcrafted features found that databases such as JAFFE and CK+—this is made possible
decision-level fusion on top of individual LSTM models by resorting to sources like the web and semi-automatic
leads to better performance [65], [129]. Learning from raw labeling procedures as opposed to manual annotation of
audiovisual data with two CNNs, Tzirakis et al. [108] used data collected in a laboratory. Audiovisual databases are a
a two-layer LSTM network for feature fusion, which was secondtypeevidentfromtheliterature.Atypicalsetupfor
foundtooutperformthestateoftheart. earlierinstances(e.g.,IEMOCAP,SEMAINE)isalab-based
videorecordingofsubjects,withinducedratherthanposed
3.3.2 Learningjointfeatureswithphysiologicaldata affective states. More recently, physiological sensors have
A small number of three studies combined the audiovisual alsobeenincluded,withvariousperipheralsignalsandEEG
andphysiological(AVP)modalities.Ranganathanetal.[149] (e.g.,DEAP,RECOLA,andMAHNOB-HCI[184]).Afurther
demonstrated the feasibility of learning joint feature repre- approach is to use excerpts from movies and television
sentationsofAVPsensordatawithapproachJ1andaDBN, shows, which can be labeled semi-automatically based on
butdonotcomparetheperformancesofdifferentmodality subtitles(e.g.,AFEWandCASIA).
combinations.Ringevaletal.[65]usedapproachJ2withthe Another important aspect of databases is the employed
AVPmodalitiesandLSTM.Theyconcludedthatinfeature- model of affect. Of the 77 public databases used in the
level fusion, ECG data helps for prediction of valence, but studies covered in our review, 58 use categorical models
notarousal. (75%), 14 use dimensional models (18%), and only 4 use
Feature-level fusion can also be based solely on physi- both (5.2%). Further, only one database provides unlabeled
ological measurements. Yin et al. [171] successfully used a affectivedisplays(AUTOENCODER).Thisheavyrelianceof
fusion SAE to aggregate handcrafted features from several databasesoncategoricalmodelsisalsoreflectedinthemod-
differentsensors.Similarly,Liuetal.[175]usedhandcrafted els employed in the reviewed studies. Overall, 190 studies
features derived from EEG and eye tracking as input into use categorical models (82%), 37 studies use dimensional
a SAE. Both studies found that the representation learned models (16%), and only 6 use both (2.6%). For categorical
through feature-level fusion leads to improved accuracy affective states, every sequence is typically labeled in a
overindividualmodalities. discrete fashion with one affective state from a set of pre-
defined labels; whereas, for dimensional affective states,
3.3.3 Takeawaysforjointfeaturelearning frames are labeled continuously or in discrete steps. The
ambiguity of human affect inherently makes both affect
• Joint feature learning is most commonly applied to
recognition and the labeling process difficult—there is an
audiovisualfusion(seeTable4).
accuracylimitinthedegreeofagreementbetweenmultiple
• To date, there is no consensus whether feature-level
labelers. Overall, the trend apparent here goes towards
fusionwithdeeplearningleadstosuperioraccuracy
capturingmorenaturalisticaffectivedisplays,asweventure
over simple decision fusion. Out of 16 studies that
from posed to spontaneous displays. Also, because of the
reportedcomparisons,only69%findthatitdoes.
challenges associated with categorical models, researchers
• Whiledimensionalmodelsofaffectareonlyusedin
have advocated for further investigating the application of
10% of spatial and 30% of temporal feature learning
dimensional models in affect recognition and comparing
studies,theyareemployedin52%ofstudiesonjoint
themwithcategoricalmodels[9],[53],[56].Unfortunately,at
featurelearning.
thisstage,thenumberofexamplesperdatasetdoesnotsee
aclearupwardtrendyetandonlyfewdeeplearningstudies
3.4 Databasesandcompetitions
coveredinourreviewinvestigatebothtypesofmodels.
Most researchers rely on publicly available databases of Unlabeled databases are used exclusively for trans-
affective display as source material for their studies. Of fer learning. When considering large general-purpose
the 233 studies in our review, only 11% involved private databases like ImageNet, the idea is to learn general low-
databasesnotavailabletothepublic.Atotalof77different level descriptors that help to improve initial model pa-
public databases were used across the reviewed literature. rameters. For FER, more relevant databases of unlabeled
The specifics of these databases have considerable impact face images (e.g., VGG-Face) can be used. Smaller, labeled
on algorithm design for affect recognition, which is why a databases such as FER2013 are also used frequently for
comprehensive overview of databases and their properties supervised pre-training. Note that these data sources forJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 14
TABLE5
Thetop15mostuseddatabasesinthereviewedstudies;alsoincludedarethreelargedatabasespublishedin2016and2017.
Name Year Modalitya Examplesb Detailsonelicitationandannotation Usesc
V A P M Subjects Examples Source Annotation Affect(Label) Target Transf.
CK+[112] 2010 • 123 593 Posed(Lab) Manual Categorical(Discrete) 44 6
FER2013[103] 2013 • 35887 Websearch Semi-aut. Categorical(Discrete) 17 23
ImageNet[110] 2009 • 14.2M Websearch (Genericimagecategories) 0 27
JAFFE[176] 1998 • 10 219 Posed(Lab) Manual Categorical(Discrete) 22 0
Emo-DB[164] 2005 • 10 800 Posed(Lab) Manual Categorical(Discrete) 17 3
VGG-Face[111] 2015 • 2622 2.6M Websearch (Genericfaceimages) 0 18
IEMOCAP[165] 2008 • • 10 1039 Induced(Lab) Manual Cat./Dim.(Discrete) 14 3
SFEW2[102] 2015 • 1635 Movies Semi-aut. Categorical(Discrete) 9 3
DEAP[177] 2012 • • 32 40 Induced(Lab) Semi-aut. Dimensional(Discrete) 11 0
AFEW5[178] 2015 • • 1645 Movies Semi-aut. Categorical(Discrete) 9 0
eNTERFACE[179] 2005 • • 42 1166 Induced(Lab) Manual Categorical(Discrete) 8 1
AFEW6[12] 2016 • • 1749 Movies Semi-aut. Categorical(Discrete) 8 0
RECOLA[180] 2013 • • • 23 46 Spont.(Lab) Manual Dimensional(Cont.) 8 0
CASIA[181] 2014 • • 219 2hr TVshows Manual Categorical(Discrete) 7 1
SEMAINE[182] 2010 • • 20 150 Induced(Lab) Manual Dimensional(Cont.) 7 0
AffectNet[57] 2017 • 450K 1M Websearch Semi-aut. Cat./Dim.(Discrete) 1 0
EmotioNet[183] 2016 • 1M Websearch Automatic Categorical(Discrete) 1 0
AUTOENCODER[62] 2017 • • 6.5M Websearch (Non-labeledaffectivedisplays) 0 1
aModalities:V=Visual(FER,Body),A=Auditory(SER),P=Physiological(EEG,Peripheral,Other).
bM=Mode; =Static, =Sequence;Numberofsubjectsgivenwhereknown.
cTargetcountsthenumberofstudiesthatpredictedgivendata;Transfercountsthenumberofstudiesthatusedgivendatafortransferlearning.
transfer learning are primarily focused on static examples accuracies is measurable: On the AFEW dataset, which
ofthevisualmodality. is extended with additional data every year, recognition
Databases published in 2016 and 2017 aim to provide accuracieshaveincreasedfrom41%in2013to60.3%in2017.
sufficienttrainingdatafordeeplearningmodels.Compared
to older databases, they contain many more examples,
which is made possible by (semi-)automating the label-
4 DISCUSSION
ing process, or providing unlabeled examples. Three such
In this review, we have seen that DNNs are part of most
databasesareincludedinTable5.BothAffectNetandEmo-
state-of-the-artaffectrecognitionsystems.Theyareapplied
tioNetarelargeweb-baseddatabases,eachataround1Mla-
for learning of (i) spatial feature representations, (ii) tem-
beledimages.Notably,AffectNetincludesbothdimensional
poral feature representations, and (iii) joint feature rep-
and categorical labels, encouraging studies to bridge the
resentations for multimodal data. Considering the recent
gap between affect representations. The AUTOENCODER
trend towards continuous and multimodal prediction of
datasetisthelargestfacevideodatasetwith6.5Mexamples;
spontaneousaffectivedisplaysinthewild,deeplearningis
thedatasetcontainsonly2777labeledexamplesandisthus
generallywellsuitedtoaddressthechallengesfacedbysuch
largelyunlabeled.Itcanservethepurposeofunsupervised
systems. Particularly, our results show that out of the 150
pre-trainingorsemi-supervisedlearning[62].
studies reporting comparisons between shallow and deep
In this review, we have thus far avoided directly com- architectures, 95% reported that deep learning can lead to
paring studies based on their recognition accuracies. Our improvementsoverconventionalapproaches.
reasoning is that even if both studies focus on the same However, in comparison to related fields like object
dataset,differingtestandtrainingsetsaswellasevaluation detectionandASR,theimpactofdeeplearninghasnotyet
statistics lead to difficulties in fairly comparing results. been fully felt. This can in part be attributed to the higher
Thissituationisdifferentfororganizedcompetitions,where difficulty and inherent ambiguity of affective displays—
the criteria are clearly defined and results independently although many databases now provide annotations from
verified.Toillustratestate-of-the-artrecognitionaccuracies, multiple labelers, the ground truth tends to be unreliable
Table 6 reports the winning entries at affect recognition and datasets are often imbalanced. A major obstacle is the
competitions from 2013 to 2017. These can generally be relativelysmallsizeoflabeleddatasets.Ithinderstheability
thought of as the latest and best results for the respective of deep models to generalize well, and makes it difficult to
datasets at publication time. We employ the classification train large models. In this vein, regularization techniques
scheme introduced in Table 1 and used throughout this from general deep learning research [192] are one future
reviewtoindicatetheuseofdeeplearninginthesestudies. research avenue. However, as of this writing, employed
It is evident that since 2015, all winning entries at these techniques such as knowledge transfer from related disci-
competitions have made use of deep learning. Evaluation plinesanddataaugmentationultimatelycannotfullymake
statisticsaregivenbyrecognitionaccuracyfordiscreteaffect upforthelackofdata.
recognition (EmotiW), and correlation coefficients for con- Although we notice a trend towards larger labeled
tinuous affect recognition (AVEC). Progress in recognition datasets,newapproachesareneededtodealwiththisissue.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 15
TABLE6
Winningentriesataffectrecognitioncompetitions(2013-2017).
Competition Deepfeaturelearning
Database Winner Evaluationstatisticb
Name Sub-Challengea Spatial Temp. Joint
AVEC2013 Fullycont.A/V AVID Mengetal.[185] CC:0.141
EmotiW2013 A/V AFEW3 Kahouetal.[186] • Accuracy:41.0%
ICML2013 StaticFER FER2013 Tangetal.[82] • Accuracy:71.2%
INTERSPEECH’13 SER GEMEP Gosztolyaetal.[187] Accuracy:73.5%(A),63.3%(V)
AVEC2014 Fullycont.A/V AVID Kacheleetal.[188] CC:0.63(A),0.58(V),0.57(D)
EmotiW2014 A/V AFEW4 Liuetal.[189] • Accuracy:50.4%
AVEC2015 Fullycont.A/V/P RECOLA Heetal.[19] • CCC:0.747(A),0.609(V)
EmotiW2015 A/V AFEW5 Yaoetal.[190] • Accuracy:53.8%
EmotiW2015 StaticFER SFEW2 Kimetal.[16] • Accuracy:61.6%
AVEC2016 Fullycont.A/V/P RECOLA Bradyetal.[18] • • CCC:0.77(A),0.687(V)
EmotiW2016 A/V AFEW6 Fanetal.[14] • • Accuracy:59.0%
EmotiW2016 Group-levelFER HAPPEI Lietal.[15] • RMSE:0.82
AVEC2017 Fullycont.A/V SEWA Chenetal.[17] • • • CCC:0.68(A),0.76(V),0.51(L)
EmotiW2017 A/V AFEW7 Huetal.[95] • Accuracy:60.3%
EmotiW2017 Group-levelFER GAF Tanetal.[191] • Accuracy:80.9%
aModalities:V=Visual(FER,Body),A=Auditory(SER),P=Physiological(EEG,Peripheral,Other).
bReportingstatisticsusedincompetitions:CC=Correlationcoefficient,CCC=Concordancecorrelationcoefficient,A=Arousal,V=Valence,D
=Dominance,L=Likability,RMSE=Rootmeansquareerror.
The labeling process for video-based affective datasets, es- features.Researchsuggeststhatthesizeofadatasetcanbe
peciallywithcontinuousannotations,isexpensiveandcan- a bottleneck for performance of deep learning models [34].
noteasilybeautomated.Unsupervisedandsemi-supervised As thesecircumstances change, larger and moreexpressive
learning are promising trends in this regard. While un- deep models specialized for affect recognition will have a
supervised learning allows models to learn better initial chance of being established. Pre-trained deep models for
parameters from unlabeled datasets, fine-tuning with la- FER and SER could become readily available as feature
beled data is still required to ”guide” the model towards extractors,similartohandcraftedfeatures.
itslearninggoal.Theideaofsemi-supervisedlearningisto As we have seen, affect recognition models are com-
labelonlyafractionoftheexamples,avoidingtheexpensive prisedofmultiplecomponents,includingthestepsofspatial
labeling process for the most part. These labeled examples and temporal feature learning as well as an additional
can then be leveraged to learn from the remaining, much fusion mechanism in the case of multimodal systems. We
larger unlabeled part of the database that was acquired find that as of now, DNNs are generally applied in an
at a lower cost. Gupta et al. [62] demonstrated the feasi- isolated way to manage individual components—hence, a
bility of semi-supervised learning for affective computing, combination of multiple DNN-based components comes to
achieving promising results using 2777 labeled face videos mind. In fact, one ideal of classic deep learning research is
to learn from the AUTOENCODER dataset of 6.5M unla- the idea of integrating multiple components into globally
beledfacevideoclips.In2018,supervisedpre-trainingwas trainable systems [194]. Some recent contributions have
successfully used at Facebook [193] to transfer knowledge shown that affect recognition systems can combine multi-
of 3.5B public Instagram images and hashtags for generic ple DNN-based components. As discussed throughout this
imageclassificationandobjectdetection.Thisdevelopment review, the combination of CNN for feature learning and
suggests that affect recognition too may benefit from much LSTMtolearnthetemporalcontextismostwidelyused(21
largerdatasets.Unsupervisedlearninghasalsoprovedsuc- studies). However, most of these approaches train individ-
cessful in SER–for example, Deng et al. [140] were able to ualcomponentsseparately(e.g.,[106],[90]),whichmaylead
improve the training process on labeled data by exploiting to suboptimal performance considering the whole system.
knowledgefromunlabeleddatausingAEs. Future multimodal affect recognition systems are likely to
Inmoststate-of-the-artaffectrecognitionmodels,hand- betrainedinanend-to-endfashion,whichalignstrainingof
crafted features still play an important role. While they are theentirenetworkcloserwiththetrueperformancemeasure
typically outperformed by DNN-learned features in direct [195]butrequiresalargeramountoftrainingexamples.The
comparison,somechallenge-winningmodelsrelyonhybrid feasibility and effectiveness of global end-to-end training
architectures that can take advantage of complementari- has been demonstrated in two recent studies: Trigeorgis et
ties between the two [152], [18]. This can be attributed to al. [54] combined CNN and LSTM to learn features and
multiplefactors:Handcraftedfeaturesarereadilyavailable, temporal context directly from speech signals. Similarly,
widely recognized, and well-designed for specific applica- Tzirakis et al. [108] used the CNN-LSTM architecture to
tionssuchasFERandSER.Ontheotherhand,deepmodels learnfeaturesandtemporalcontextdirectlyfromrawaudio-
have not yet been established as methods of affect recog- visualsignals.
nition. There are few specialized architectures researchers Another recent trend from deep learning research that
can draw from, which is why many fall back to ones from lendsitselftoaffectrecognitionofsequentialdataistheuse
objectrecognition.Furthermore,thenumberofexamplesto of attention mechanisms [196]. They allow models to learn
drawfromisnotlargeenoughtolearntrulyexpressivedeep topaymoreattentiontopromisingsegmentsof,say,avideoJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 16
REFERENCES
[1] R.W.Picard,AffectiveComputing. MITPress,1997.
[2] ——,“Affectivecomputing,”MITMediaLab.,Tech.Rep.,1995.
[3] S. D’Mello, R. W. Picard, and A. Graesser, “Toward an affect-
sensitive autotutor,” IEEE Intell. Syst., vol. 22, no. 4, pp. 53–61,
2007.
[4] C. Lisetti, F. Nasoz, C. LeRouge, O. Ozyer, and K. Alvarez,
“Developingmultimodalintelligentaffectiveinterfacesfortele-
homehealthcare,”Int.J.Hum.-Comput.Stud.,vol.59,no.1,pp.
245–255,2003.
[5] G.N.YannakakisandJ.Togelius,“Experience-drivenprocedural
contentgeneration,”IEEETrans.Affect.Comput.,vol.2,no.3,pp.
147–161,2011.
Fig. 7. CNNs learn high-level features similar to Action Units in FER
[6] F.DeRosis,C.Pelachaud,I.Poggi,V.Carofiglio,andB.DeCar-
(takenfrom[91]withpermission).
olis,“Fromgreta’smindtoherface:Modellingthedynamicsof
affectivestatesinaconversationalembodiedagent,”Int.J.Hum.-
Comput.Stud.,vol.59,no.1,pp.81–118,2003.
of affective display. Mirsamadi et al. [63] found that even [7] M.PanticandL.J.Rothkrantz,“Towardanaffect-sensitivemul-
a simple attention mechanism contributes to SER by, for timodalhuman-computerinteraction,”Proc.IEEE,vol.91,no.9,
pp.1370–1390,2003.
example,learningtoignoresilentframes—animprovement
[8] R.W.Picard,E.Vyzas,andJ.Healey,“Towardmachineemotional
thatiseasilyinterpretableandadds1-2%tomodelaccuracy. intelligence:Analysisofaffectivephysiologicalstate,”IEEETrans.
In general, DNNs are often described as ”black boxes”, PatternAnal.Mach.Intell.,vol.23,no.10,pp.1175–1191,2001.
as their inner workings are complex and difficult to inter- [9] R. A. Calvo and S. D’Mello, “Affect detection: An interdisci-
plinaryreviewofmodels,methods,andtheirapplications,”IEEE
pret. Recent research has attempted to improve this situa-
Trans.Affect.Comput.,vol.1,no.1,pp.18–37,2010.
tion, for example by offering ways to visualize the activa- [10] A. Jaimes and N. Sebe, “Multimodal human–computer interac-
tions of individual filters in CNNs [197]. Multiple studies tion:Asurvey,”Comput.VisionandImageUnderstanding,vol.108,
have followed this approach to understand how CNNs no.1,pp.116–134,2007.
[11] Z.Zeng,M.Pantic,G.I.Roisman,andT.S.Huang,“Asurvey
recognizefacialexpressions,findingthathigherlayerslearn
of affect recognition methods: Audio, visual, and spontaneous
concepts similar to Action Units (e.g., [87], [91]), validating expressions,”IEEETrans.PatternAnal.Mach.Intell.,vol.31,no.1,
previousresearchdoneinFER—Fig.7featuressomeexam- pp.39–58,2009.
[12] A. Dhall, R. Goecke, J. Joshi, J. Hoey, and T. Gedeon, “Emotiw
ples.Similarly,forSER,Tzirakisetal.[108]foundthatLSTM
2016:Videoandgroup-levelemotionrecognitionchallenges,”in
cells learn representations similar to well-known prosodic Proc.Int.Conf.MultimodalInteract.,2016,pp.427–432.
features. Such results indicate that the study of DNNs in [13] M.Valstar,J.Gratch,B.Schuller,F.Ringeval,D.Lalanne,M.Tor-
affective computing could contribute to interdisciplinary resTorres,S.Scherer,G.Stratou,R.Cowie,andM.Pantic,“Avec
2016:Depression,mood,andemotionrecognitionworkshopand
emotionresearchbygivingafurtherperspectiveonaffective
challenge,”inProc.AVEC,2016,pp.3–10.
displaysandtheirrepresentationingeneral. [14] Y.Fan,X.Lu,D.Li,andY.Liu,“Video-basedemotionrecognition
Thequestionofcategoricalversusdimensionalrepresen- using cnn-rnn and c3d hybrid networks,” in Proc. Int. Conf.
tations of affective states remains unsolved, as no deciding MultimodalInteract.,2016,pp.445–450.
[15] J. Li, S. Roy, J. Feng, and T. Sim, “Happiness level prediction
trendemergesfromtheliterature.Inpractice,specificdetails
withsequentialinputsviamultipleregressions,”inProc.Int.Conf.
of available modalities (e.g., static images or video data) MultimodalInteract.,2016,pp.487–493.
and requirements regarding predictions (e.g., continuous [16] B.-K.Kim,H.Lee,J.Roh,andS.-Y.Lee,“Hierarchicalcommittee
of deep cnns with exponentially-weighted decision fusion for
or video-level) are relevant factors, and available labels on
staticfacialexpressionrecognition,”inProc.Int.Conf.Multimodal
common databases widely dictate which representation is
Interact.,2015,pp.427–434.
used. As can be seen in our review, the majority of studies [17] S. Chen, Q. Jin, J. Zhao, and S. Wang, “Multimodal multi-task
stillrelyoncategoricalmodelsofaffect(82%),particularlyin learning fordimensional andcontinuousemotionrecognition,”
inProc.AVEC,2017,pp.19–26.
spatialfeaturelearning(92%).However,giventhenatureof
[18] K.Brady,Y.Gwon,P.Khorrami,E.Godoy,W.Campbell,C.Dagli,
sensordataintypicalHCIscenarios,itcanbeexpectedthat and T. S. Huang, “Multi-modal audio, video and physiological
dimensional representations, and combinations of categori- sensor learning for continuous emotion prediction,” in Proc.
calanddimensionalmodelsofaffectwillbecomeevenmore AVEC,2016,pp.97–104.
[19] L.He,D.Jiang,L.Yang,E.Pei,P.Wu,andH.Sahli,“Multimodal
relevant as a more natural way of dealing with continuous
affective dimension prediction using deep bidirectional long
data [53]. This already becomes apparent in joint feature short-term memory recurrent neural networks,” in Proc. AVEC,
learning and to some extent in temporal feature learning, 2015,pp.73–80.
where dimensional models are already employed in 52% [20] Y. Bengio and Y. LeCun, “Scaling learning algorithms towards
ai,”inLarge-ScaleKernelMachines,L.Bottou,O.Chapelle,D.De-
and 30% of the reviewed studies, respectively. Databases
Coste,andJ.Weston,Eds. MITPress,2007,pp.1–41.
such as AffectNet [57], which provide categorical as well [21] Y. Bengio, O. Delalleau, and N. Le Roux, “The curse of di-
as dimensional annotations of affect in the wild,arean im- mensionalityforlocalkernelmachines,”Universite´deMontre´al,
Tech.Rep.,March2005.
portantstepforfurtheringtheinvestigationofdimensional
[22] I.Goodfellow,Y.Bengio,andA.Courville,DeepLearning. MIT
models and for better integrating the context of affective
Press,2016.
displayintohumanaffectrecognition. [23] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning
algorithm for deep belief nets,” Neural Comput., vol. 18, no. 7,
pp.1527–1554,2006.
ACKNOWLEDGMENTS [24] Y.LeCun,Y.Bengio,andG.Hinton,“Deeplearning,”Nature,vol.
521,pp.436–444,2015.
ThisresearchwassupportedbyanAustralianGovernment
[25] J. Schmidhuber, “Deep learning in neural networks: An
ResearchTrainingProgram(RTP)Scholarship. overview,”NeuralNetworks,vol.61,pp.85–117,2015.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 17
[26] Y. Bengio, “Learning deep architectures for ai,” Found. Trends [53] H. Gunes and B. Schuller, “Categorical and dimensional affect
Mach.Learn.,vol.2,no.1,pp.1–127,2009. analysis in continuous input: Current trends and future direc-
[27] Y.Bengio,A.Courville,andP.Vincent,“Representationlearning: tions,”ImageVisionComput.,vol.31,pp.120–136,2013.
Areviewandnewperspectives,”IEEETrans.PatternAnal.Mach. [54] G.Trigeorgis,F.Ringeval,R.Brueckner,E.Marchi,M.A.Nico-
Intell.,vol.35,no.8,pp.1798–1828,2013. laou,B.Schuller,andS.Zafeiriou,“Adieufeatures?end-to-end
[28] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” speechemotionrecognitionusingadeepconvolutionalrecurrent
NeuralComput.,vol.9,no.8,pp.1735–1780,1997. network,”inProc.ICASSP,2016,pp.5200–5204.
[29] K.He,X.Zhang,S.Ren,andJ.Sun,“Deepresiduallearningfor [55] X. Li, D. Song, P. Zhang, G. Yu, Y. Hou, and B. Hu, “Emotion
imagerecognition,”inProc.CVPR,2016,pp.770–778. recognition from multi-channel eeg data through convolutional
[30] J.MartensandI.Sutskever,“Learningrecurrentneuralnetworks recurrentneuralnetwork,”inProc.Int.Conf.Bioinf.andBiomed.,
withhessian-freeoptimization,”inProc.Int.Conf.Mach.Learn., 2016,pp.352–359.
2011,pp.1033–1040. [56] L.A.Bugnon,R.A.Calvo,andD.H.Milone,“Dimensionalaffect
[31] X.Glorot,A.Bordes,andY.Bengio,“Deepsparserectifierneural recognitionfromhrv:Anapproachbasedonsupervisedsomand
networks,”inProc.Int.Conf.Artif.Intell.Stat.,2011,pp.315–323. elm,”IEEETrans.Affect.Comput.,2017.
[32] I.Sutskever,J.Martens,G.Dahl,andG.Hinton,“Ontheimpor- [57] A. Mollahosseini, B. Hasani, and M. H. Mahoor, “Affectnet: A
tanceofinitializationandmomentumindeeplearning,”inProc.
databaseforfacialexpression,valence,andarousalcomputingin
Int.Conf.Mach.Learn.,2013,pp.1139–1147. thewild,”IEEETrans.Affect.Comput.,2017.
[33] S.IoffeandC.Szegedy,“Batchnormalization:Acceleratingdeep
[58] J.A.Russell,“Acircumplexmodelofaffect,”J.Pers.Soc.Psychol.,
network training by reducing internal covariate shift,” in Proc.
vol.39,no.6,pp.1161–1178,1980.
Mach.Learn.Res.,vol.37,2015,pp.448–456.
[59] S.K.D’MelloandJ.Kory,“Areviewandmeta-analysisofmul-
[34] C. Sun, A. Shrivastava, S. Singh, and A. Gupta, “Revisiting
timodalaffectdetectionsystems,”ACMComput.Surveys,vol.47,
unreasonableeffectivenessofdataindeeplearningera,”inProc.
no.3,p.43,2015.
Int.Conf.Comput.Vision,2017,pp.843–852.
[60] S. J. Pan, Q. Yang et al., “A survey on transfer learning,” IEEE
[35] D.H.HubelandT.N.Wiesel,“Receptivefields,binocularinter-
Trans.Knowl.DataEng.,vol.22,no.10,pp.1345–1359,2010.
action and functional architecture in the cat’s visual cortex,” J.
Physiol.,vol.160,no.1,pp.106–154,1962. [61] S.Rifai,Y.Bengio,A.Courville,P.Vincent,andM.Mirza,“Dis-
[36] A.Krizhevsky,I.Sutskever,andG.E.Hinton,“Imagenetclassifi- entanglingfactorsofvariationforfacialexpressionrecognition,”
cationwithdeepconvolutionalneuralnetworks,”inProc.NIPS, inProc.Europ.Conf.Comput.Vision,2012,pp.808–822.
vol.25,2012,pp.1097–1105. [62] O. Gupta, D. Raviv, and R. Raskar, “Multi-velocity neural net-
[37] Y. LeCun, L. Jackel, B. Boser, J. Denker, H. Graf, I. Guyon, works for facial expression recognition in videos,” IEEE Trans.
D.Henderson,R.Howard,andW.Hubbard,“Handwrittendigit Affect.Comput.,2017.
recognition:Applicationsofneuralnetworkchipsandautomatic [63] S. Mirsamadi, E. Barsoum, and C. Zhang, “Automatic speech
learning,”IEEECommun.Mag.,vol.27,no.11,pp.41–46,1989. emotionrecognitionusingrecurrentneuralnetworkswithlocal
[38] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, attention,”inProc.ICASSP,2017,pp.2227–2231.
Z.Huang,A.Karpathy,A.Khosla,M.Bernstein,A.C.Berg,and [64] D. H. Kim, W. Baddar, J. Jang, and Y. M. Ro, “Multi-objective
F.-F.Li,“Imagenetlargescalevisualrecognitionchallenge,”Int. based spatio-temporal feature representation learning robust to
J.Comput.Vision,vol.115,no.3,pp.211–252,2015. expressionintensityvariationsforfacialexpressionrecognition,”
[39] Y. Ming, S. Cao, R. Zhang, Z. Li, Y. Chen, Y. Song, and H. Qu, IEEETrans.Affect.Comput.,2017.
“Understandinghiddenmemoriesofrecurrentneuralnetworks,” [65] F.Ringeval,F.Eyben,E.Kroupi,A.Yuce,J.-P.Thiran,T.Ebrahimi,
inProc.Conf.VisualAnal.Sci.Technol.,2017. D.Lalanne,andB.Schuller,“Predictionofasynchronousdimen-
[40] Z. C. Lipton, J. Berkowitz, and C. Elkan, “A critical review of sionalemotionratingsfromaudiovisualandphysiologicaldata,”
recurrentneuralnetworksforsequencelearning,”arXivpreprint PatternRecognit.Lett.,vol.66,pp.22–30,2015.
arXiv:1506.00019,2015. [66] M.Chen,L.Zhang,andJ.P.Allebach,“Learningdeepfeatures
[41] K. Cho, B. Van Merrie¨nboer, C. Gulcehre, D. Bahdanau, forimageemotionclassification,”inProc.Int.Conf.ImageProcess.,
F.Bougares,H.Schwenk,andY.Bengio,“Learningphraserep- 2015,pp.4491–4495.
resentations using rnn encoder-decoder for statistical machine [67] Y.E.Kim,E.M.Schmidt,R.Migneco,B.G.Morton,P.Richard-
translation,” in Proc. Conf. Empirical Methods Natural Language son, J. Scott, J. A. Speck, and D. Turnbull, “Music emotion
Process.,2017,pp.1724–1734. recognition: A state of the art review,” in Proc. Int. Soc. Music
[42] K. J. Han, A. Chandrashekaran, J. Kim, and I. Lane, “Densely Inf.Retrieval,2010,pp.255–266.
connected networks for conversational speech recognition,” in
[68] E.M.SchmidtandY.E.Kim,“Learningemotion-basedacoustic
Proc.INTERSPEECH,2018,pp.796–800.
featureswithdeepbeliefnetworks,”inProc.Worksh.Appl.Signal
[43] P.BaldiandK.Hornik,“Neuralnetworksandprincipalcompo-
Process.AudioAcoust.,2011,pp.65–68.
nent analysis: Learning from examples without local minima,”
[69] R.A.CalvoandS.MacKim,“Emotionsintext:Dimensionaland
NeuralNetworks,vol.2,pp.53–58,1989.
categorical models,” Comput. Intell., vol. 29, no. 3, pp. 527–543,
[44] P. Smolensky, “Information processing in dynamical systems:
2013.
Foundationsofharmonytheory,”inParallelDistributedProcessing:
[70] R.Cowie,E.Douglas-Cowie,N.Tsapatsoulis,G.Votsis,S.Kollias,
Volume1:Foundations,D.E.RumelhartandJ.L.McClelland,Eds.
W. Fellenz, and J. G. Taylor, “Emotion recognition in human-
MITPress,1986,pp.194–281.
computer interaction,” IEEE Signal Process. Mag., vol. 18, no. 1,
[45] R.SalakhutdinovandH.Larochelle,“Efficientlearningofdeep
pp.32–80,2001.
boltzmannmachines,”inProc.Int.Conf.Artif.Intell.Stat.,2010,
pp.693–700. [71] A.Mehrabian,“Communicationwithoutwords,”PsychologyTo-
[46] Y.Bengio, P. Lamblin, D.Popovici, and H.Larochelle, “Greedy
day,vol.2,no.4,pp.53–56,1968.
layer-wise training of deep networks,” in Proc. NIPS, vol. 19, [72] E.Sariyanidi,H.Gunes,andA.Cavallaro,“Automaticanalysis
2007,pp.153–160. of facial affect: A survey of registration, representation, and
[47] M. Pantic, N. Sebe, J. F. Cohn, and T. Huang, “Affective multi- recognition,”IEEETrans.PatternAnal.Mach.Intell.,vol.37,no.6,
modalhuman-computerinteraction,”inProc.Int.Conf.Multime- pp.1113–1133,2015.
dia,2005,pp.669–676. [73] C. A. Corneanu, M. O. Simon, J. F. Cohn, and S. E. Guerrero,
[48] P.KragelandK.LaBar,“Decodingthenatureofemotioninthe “Survey on rgb, 3d, thermal, and multimodal approaches for
brain,”TrendsinCogn.Sciences,vol.20,no.6,pp.444–455,2016. facial expression recognition: History, trends, and affect-related
[49] A.OrtonyandT.J.Turner,“What’sbasicaboutbasicemotions?” applications,”IEEETrans.PatternAnal.Mach.Intell.,vol.38,no.8,
PsychologicalReview,vol.97,no.3,pp.315–331,1990. pp.1548–1568,2016.
[50] P. Ekman and W. V. Friesen, “The repertoire of nonverbal be- [74] G. Levi and T. Hassner, “Emotion recognition in the wild via
havior:Categories,origins,usage,andcoding,”Semiotica,vol.1, convolutionalneuralnetworksandmappedbinarypatterns,”in
no.1,pp.49–98,1969. Proc.Int.Conf.MultimodalInteract.,2015,pp.503–510.
[51] R.Plutchik,“Ageneralpsychoevolutionarytheoryofemotion,” [75] A. Yao, D. Cai, P. Hu, S. Wang, L. Sha, and Y. Chen, “Holonet:
inTheoriesofEmotion,1980,pp.3–33. Towards robust emotion recognition in the wild,” in Proc. Int.
[52] W.James,ThePrinciplesofPsychology,VolumeI. Holt,1890. Conf.MultimodalInteract.,2016,pp.472–478.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 18
[76] Y. Cheng, B. Jiang, and K. Jia, “A deep structure for facial [101] J. Susskind, A. Anderson, and G. E. Hinton, “The toronto face
expressionrecognitionunderpartialocclusion,”inProc.Int.Conf. dataset,”UniversityofToronto,Tech.Rep.TR2010-001,2010.
Intell.Inf.HidingMultim.Sign.Proc.,2014,pp.211–214. [102] A.Dhall,R.Goecke,S.Lucey,andT.Gedeon,“Staticfacialexpres-
[77] Y.Lv,Z.Feng,andC.Xu,“Facialexpressionrecognitionviadeep sionanalysisintoughconditions:Data,evaluationprotocoland
learning,”inProc.SMARTCOMP,2014,pp.303–308. benchmark,” in Proc. Int. Conf. Comput. Vision Workshops, 2011,
[78] C. Fadil, R. Alvarez, C. Mart´ınez, J. Goddard, and H. Rufiner, pp.2106–2112.
“Multimodalemotionrecognitionusingdeepnetworks,”inLatin [103] I.J.Goodfellow,D.Erhan,P.L.Carrier,A.Courville,M.Mirza,
Am.Congr.Biomed.Eng.,2014,pp.813–816. B. Hamner, W. Cukierski, Y. Tang, D. Thaler, D.-H. Lee et al.,
[79] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals, “Un- “Challenges in representation learning: A report on three ma-
derstanding deep learning requires rethinking generalization,” chinelearningcontests,”NeuralNetworks,vol.64,pp.59–63,2015.
inProc.Int.Conf.Learn.Represent.,2017. [104] T.Hassner,S.Harel,E.Paz,andR.Enbar,“Effectivefacefrontal-
[80] K. Simonyan and A. Zisserman, “Very deep convolutional net- izationinunconstrainedimages,”inProc.CVPR,2015,pp.4295–
worksforlarge-scaleimagerecognition,”inProc.Int.Conf.Learn. 4304.
Represent.,2014. [105] P.Rodriguez,G.Cucurull,J.Gonza`lez,J.M.Gonfaus,K.Nasrol-
[81] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, lahi,T.B.Moeslund,andF.X.Roca,“Deeppain:Exploitinglong
D.Erhan,V.Vanhoucke,andA.Rabinovich,“Goingdeeperwith short-termmemorynetworksforfacialexpressionclassification,”
convolutions,”inProc.CVPR,2015,pp.1–9. IEEETrans.Cybern.,vol.PP,no.99,pp.1–11,2017.
[82] Y.Tang,“Deeplearningusinglinearsupportvectormachines,” [106] S.E.Kahou,V.Michalski,K.Konda,R.Memisevic,andC.Pal,
inProc.Int.Conf.Mach.Learn.,2013. “Recurrentneuralnetworksforemotionrecognitioninvideo,”in
[83] S. Ji, W. Xu, M. Yang, and K. Yu, “3d convolutional neural Proc.Int.Conf.MultimodalInteract.,2015,pp.467–474.
networksforhumanactionrecognition,”IEEETrans.PatternAnal. [107] H.-W. Ng, V. D. Nguyen, V. Vonikakis, and S. Winkler, “Deep
Mach.Intell.,vol.35,no.1,pp.221–231,2013. learningforemotionrecognitiononsmalldatasetsusingtransfer
[84] P. Barros, D. Jirak, C. Weber, and S. Wermter, “Multimodal learning,” in Proc. Int. Conf. Multimodal Interact., 2015, pp. 443–
emotional state recognition using sequence-dependent deep hi- 449.
erarchicalfeatures,”NeuralNetworks,vol.72,pp.140–151,2015.
[108] P. Tzirakis, G. Trigeorgis, M. A. Nicolaou, B. Schuller, and
[85] S. Zhang, S. Zhang, T. Huang, W. Gao, and Q. Tian, “Learning S.Zafeiriou,“End-to-endmultimodalemotionrecognitionusing
affective features with a hybrid deep model for audio-visual deep neural networks,” IEEE J. Sel. Top. Signal Process., vol. 11,
emotion recognition,” IEEE Trans. Circuits Syst. Video Technol., no.8,pp.1301–1309,2017.
2017.
[109] A.S.Razavian,H.Azizpour,J.Sullivan,andS.Carlsson,“Cnn
[86] A.Dhall,R.Goecke,S.Ghosh,J.Joshi,J.Hoey,andT.Gedeon,
featuresoff-the-shelf:Anastoundingbaselineforrecognition,”in
“From individual to group-level emotion recognition: Emotiw Proc.CVPRWorkshops,2014,pp.806–813.
5.0,”inProc.Int.Conf.MultimodalInteract.,2017,pp.524–528.
[110] J.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andF.-F.Li,“Imagenet:
[87] P.Khorrami,T.Paine,andT.Huang,“Dodeepneuralnetworks
Alarge-scalehierarchicalimagedatabase,”inProc.CVPR,2009,
learnfacialactionunitswhendoingexpressionrecognition?”in
pp.248–255.
Proc.Int.Conf.Comput.Vision,2015,pp.19–27.
[111] O.M.Parkhi,A.Vedaldi,A.Zissermanetal.,“Deepfacerecogni-
[88] H.Jung,S.Lee,J.Yim,S.Park,andJ.Kim,“Jointfine-tuningin
tion,”inProc.Brit.Mach.VisionConf.,2015,pp.1–12.
deepneuralnetworksforfacialexpressionrecognition,”inProc.
[112] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and
Int.Conf.Comput.Vision,2015,pp.2983–2991.
I.Matthews,“Theextendedcohn-kanadedataset(ck+):Acom-
[89] S. Chen, X. Li, Q. Jin, S. Zhang, and Y. Qin, “Video emotion
pletedatasetforactionunitandemotion-specifiedexpression,”
recognitioninthewildbasedonfusionofmultimodalfeatures,”
inProc.CVPRWorkshops,2010,pp.94–101.
inProc.Int.Conf.MultimodalInteract.,2016,pp.494–500.
[113] Y. Guo, D. Tao, J. Yu, H. Xiong, Y. Li, and D. Tao, “Deep
[90] P.Khorrami,T.LePaine,K.Brady,C.Dagli,andT.Huang,“How
neural networks with relativity learning for facial expression
deepneuralnetworkscanimproveemotionrecognitiononvideo
recognition,”inProc.Int.Conf.MultimediaExpoWorkshops,2016,
data,”inProc.Int.Conf.ImageProcess.,2016,pp.619–623.
pp.1–6.
[91] R. Breuer and R. Kimmel, “A deep learning perspective on
[114] Y.Cai,W.Zheng,T.Zhang,Q.Li,Z.Cui,andJ.Ye,“Videobased
theoriginoffacialexpressions,”arXivpreprintarXiv:1705.01842,
emotion recognition using cnn and brnn,” in Proc. Chin. Conf.
2017.
PatternRecognition,2016,pp.679–691.
[92] H. Li, J. Sun, Z. Xu, and L. Chen, “Multimodal 2d+3d facial
[115] H.Ding,S.K.Zhou,andR.Chellappa,“Facenet2expnet:Regular-
expression recognition with deep fusion convolutional neural
network,”IEEETrans.Multimedia,vol.19,no.12,pp.2816–2831, izingadeepfacerecognitionnetforexpressionrecognition,”in
Proc.Int.Conf.Automat.FaceGestureRecognit.,2017,pp.118–126.
2017.
[93] L.Gui,T.Baltrusˇaitis,andL.-P.Morency,“Curriculumlearning [116] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and
forfacialexpressionrecognition,”inProc.Int.Conf.Automat.Face R. Salakhutdinov, “Dropout: A simple way to prevent neural
GestureRecognit.,2017,pp.505–511. networks from overfitting,” J. Mach. Learn. Res., vol. 15, no. 1,
[94] A. T. Lopes, E. de Aguiar, A. F. De Souza, and T. Oliveira- pp.1929–1958,2014.
Santos,“Facialexpressionrecognitionwithconvolutionalneural [117] M.ElAyadi,M.S.Kamel,andF.Karray,“Surveyonspeechemo-
networks:Copingwithfewdataandthetrainingsampleorder,” tionrecognition:Features,classificationschemes,anddatabases,”
PatternRecognit.,vol.61,pp.610–628,2017. PatternRecognit.,vol.44,no.3,pp.572–587,2011.
[95] P.Hu,D.Cai,S.Wang,A.Yao,andY.Chen,“Learningsupervised [118] C.-N. Anagnostopoulos, T. Iliou, and I. Giannoukos, “Features
scoring ensemble for emotion recognition in the wild,” in Proc. and classifiers for emotion recognition from speech: A survey
Int.Conf.MultimodalInteract.,2017,pp.553–560. from2000to2011,”Artif.Intell.Rev.,vol.43,no.2,pp.155–177,
[96] M.M.GhaziandH.K.Ekenel,“Automaticemotionrecognition 2015.
inthewildusinganensembleofstaticanddynamicrepresenta- [119] H. Lee, P. Pham, Y. Largman, and A. Y. Ng, “Unsupervised
tions,”inProc.Int.Conf.MultimodalInteract.,2016,pp.514–521. featurelearningforaudioclassificationusingconvolutionaldeep
[97] W.Ding,M.Xu,D.Huang,W.Lin,M.Dong,X.Yu,andH.Li, beliefnetworks,”inProc.NIPS,2009,pp.1096–1104.
“Audio and face video emotion recognition in the wild using [120] O.Abdel-Hamid,A.-r.Mohamed,H.Jiang,L.Deng,G.Penn,and
deep neural networks and small datasets,” in Proc. Int. Conf. D.Yu,“Convolutionalneuralnetworksforspeechrecognition,”
MultimodalInteract.,2016,pp.506–513. IEEE/ACMTrans.Audio,Speech,LanguageProcess.,vol.22,no.10,
[98] B.Sun,L.Li,G.Zhou,andJ.He,“Facialexpressionrecognition pp.1533–1545,2014.
in the wild based on multimodal texture features,” J. Electron. [121] F. Eyben, K. R. Scherer, B. W. Schuller, J. Sundberg, E. Andre´,
Imaging,vol.25,no.6,pp.1–8,2016. C. Busso, L. Y. Devillers, J. Epps, P. Laukka, S. S. Narayanan
[99] H. Kaya, F. Gu¨rpınar, and A. A. Salah, “Video-based emotion etal.,“Thegenevaminimalisticacousticparameterset(gemaps)
recognition in the wild using deep transfer learning and score for voice research and affective computing,” IEEE Trans. Affect.
fusion,”ImageVisionComput.,vol.65,pp.66–75,2017. Comput.,vol.7,no.2,pp.190–202,2016.
[100] B. Xu, Y. Fu, Y.-G. Jiang, B. Li, and L. Sigal, “Video emotion [122] F. Eyben, M. Wo¨llmer, and B. Schuller, “Openear—introducing
recognitionwithtransferreddeepfeatureencodings,”inProc.Int. themunichopen-sourceemotionandaffectrecognitiontoolkit,”
Conf.MultimediaRetrieval,2016,pp.15–22. inProc.ACII,2009,pp.1–6.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 19
[123] B. Schuller, S. Steidl, A. Batliner, A. Vinciarelli, K. Scherer, [146] T.Zhang,W.Zheng,Z.Cui,Y.Zong,andY.Li,“Spatial-temporal
F.Ringeval,M.Chetouani,F.Weninger,F.Eyben,E.Marchietal., recurrent neural network for emotion recognition,” IEEE Trans.
“Theinterspeech2013computationalparalinguisticschallenge,” Cybern.,2017.
inProc.INTERSPEECH,2013. [147] J. Li, Z. Zhang, and H. He, “Implementation of eeg emotion
[124] F.Eyben,M.Wo¨llmer,andB.Schuller,“Opensmile:Themunich recognition system based on hierarchical convolutional neural
versatile and fast open-source audio feature extractor,” in Proc. networks,”inProc.Int.Conf.BrainInspiredCogn.Syst.,2016,pp.
Int.Conf.Multimedia,2010,pp.1459–1462. 22–33.
[125] L. Li, Y. Zhao, D. Jiang, Y. Zhang, F. Wang, I. Gonzalez, [148] B.Zhang,G.Essl,andE.MowerProvost,“Automaticrecognition
E.Valentin,andH.Sahli,“Hybriddeepneuralnetwork–hidden of self-reported and perceived emotion: Does joint modeling
markov model (dnn-hmm) based speech emotion recognition,” help?”inProc.Int.Conf.MultimodalInteract.,2016,pp.217–224.
inProc.ACII,2013,pp.312–317. [149] H. Ranganathan, S. Chakraborty, and S. Panchanathan, “Multi-
[126] Z. Huang, M. Dong, Q. Mao, and Y. Zhan, “Speech emotion modal emotion recognition using deep learning architectures,”
recognition using cnn,” in Proc. Int. Conf. Multimedia, 2014, pp. inProc.WinterConf.Appl.Comput.Vision,2016,pp.1–9.
801–804. [150] M.A.Nicolaou,H.Gunes,andM.Pantic,“Continuouspredic-
[127] A. M. Badshah, J. Ahmad, N. Rahim, and S. W. Baik, “Speech tionofspontaneousaffectfrommultiplecuesandmodalitiesin
emotionrecognitionfromspectrogramswithdeepconvolutional valence-arousalspace,”IEEETrans.Affect.Comput.,vol.2,no.2,
neural network,” in Proc. Int. Conf. Platform Technol. Serv., 2017, pp.92–105,2011.
pp.1–5. [151] J.Wei,E.Pei,D.Jiang,H.Sahli,L.Xie,andZ.Fu,“Multimodal
[128] H. M. Fayek, M. Lech, and L. Cavedon, “Evaluating deep continuousaffectrecognitionbasedonlstmandmultiplekernel
learning architectures for speech emotion recognition,” Neural learning,”inProc.Annu.SummitConf.Asia-Pac.SignalInf.Proc.
Networks,vol.92,pp.60–68,2017. Assoc.,2014,pp.1–4.
[129] M. Wo¨llmer, M. Kaiser, F. Eyben, B. Schuller, and G. Rigoll, [152] J.Yan,W.Zheng,Z.Cui,C.Tang,T.Zhang,Y.Zong,andN.Sun,
“Lstm-modelingofcontinuousemotionsinanaudiovisualaffect “Multi-cluefusionforemotionrecognitioninthewild,”inProc.
recognitionframework,”ImageVisionComput.,vol.31,no.2,pp. Int.Conf.MultimodalInteract.,2016,pp.458–463.
153–163,2013. [153] A.Karpathy,G.Toderici,S.Shetty,T.Leung,R.Sukthankar,and
[130] H. M. Fayek, M. Lech, and L. Cavedon, “Towards real-time L. Fei-Fei, “Large-scale video classification with convolutional
speechemotionrecognitionusingdeepneuralnetworks,”inProc. neuralnetworks,”inProc.CVPR,2014,pp.1725–1732.
Int.Conf.SignalProcess.Commun.Syst.,2015,pp.1–5. [154] K.Kaza,A.Psaltis,K.Stefanidis,K.C.Apostolakis,S.Thermos,
[131] W.Zheng,J.Yu,andY.Zou,“Anexperimentalstudyofspeech K.Dimitropoulos,andP.Daras,“Bodymotionanalysisforemo-
emotion recognition based on deep convolutional neural net- tion recognition in serious games,” in Proc. Int. Conf. Universal
works,”inProc.ACII,2015,pp.827–831. AccessinHum.-Comput.Interact.,2016,pp.33–42.
[132] C.-T.Ho,Y.-H.Lin,andJ.-L.Wu,“Emotionpredictionfromuser- [155] M. Gales, S. Young et al., “The application of hidden markov
generated videos by emotion wheel guided deep learning,” in models in speech recognition,” Found. Trends Signal Process.,
Proc.Int.Conf.NeuralInf.Process.,2016,pp.3–12. vol.1,no.3,pp.195–304,2008.
[133] N. Jaitly and G. Hinton, “Learning a better representation of [156] A.Graves,A.-r.Mohamed,andG.Hinton,“Speechrecognition
speech soundwaves using restricted boltzmann machines,” in withdeeprecurrentneuralnetworks,”inProc.ICASSP,2013,pp.
Proc.ICASSP,2011,pp.5884–5887. 6645–6649.
[134] D.BerteroandP.Fung,“Afirstlookintoaconvolutionalneural [157] B. Schuller, S. Steidl, and A. Batliner, “The interspeech 2009
network for speech emotion detection,” in Proc. ICASSP, 2017, emotionchallenge,”inProc.INTERSPEECH,2009.
pp.5115–5119. [158] B. W. Schuller, S. Steidl, A. Batliner, E. No¨th, A. Vinciarelli,
[135] Y.Zhang,Y.Liu,F.Weninger,andB.Schuller,“Multi-taskdeep F. Burkhardt, R. Van Son, F. Weninger, F. Eyben, T. Bocklet
neural network with shared hidden layers: Breaking down the et al., “The interspeech 2012 speaker trait challenge,” in Proc.
wall between emotion representations,” in Proc. ICASSP, 2017, INTERSPEECH,2012,pp.254–257.
pp.4990–4994. [159] Y. Kim, H. Lee, and E. M. Provost, “Deep learning for robust
[136] J. Gideon, S. Khorram, Z. Aldeneh, D. Dimitriadis, and featuregenerationinaudiovisualemotionrecognition,”inProc.
E. Provost, “Progressive neural networks for transfer learning ICASSP,2013,pp.3687–3691.
inemotionrecognition,”inProc.INTERSPEECH,2017,pp.1098– [160] S.E.Kahou,X.Bouthillier,P.Lamblin,C.Gulcehre,V.Michalski,
1102. K. Konda, S. Jean, P. Froumenty, Y. Dauphin, N. Boulanger-
[137] J. Deng, Z. Zhang, E. Marchi, and B. Schuller, “Sparse Lewandowski et al., “Emonets: Multimodal deep learning ap-
autoencoder-basedfeaturetransferlearningforspeechemotion proaches for emotion recognition in video,” J. Multimodal User
recognition,”inProc.ACII,2013,pp.511–516. Interfaces,vol.10,no.2,pp.99–111,2016.
[138] Y. Aytar, C. Vondrick, and A. Torralba, “Soundnet: Learning [161] A. Stuhlsatz, C. Meyer, F. Eyben, T. Zielke, G. Meier, and
sound representations from unlabeled video,” in Proc. NIPS, B.Schuller,“Deepneuralnetworksforacousticemotionrecogni-
2016. tion:Raisingthebenchmarks,”inProc.ICASSP,2011,pp.5688–
[139] S. Pini, O. B. Ahmed, M. Cornia, L. Baraldi, R. Cucchiara, and 5691.
B. Huet, “Modeling multimodal cues in a deep learning-based [162] F.Lingenfelser,J.Wagner,J.Deng,R.Bruckner,B.Schuller,and
frameworkforemotionrecognitioninthewild,”inProc.Int.Conf. E. Andre, “Asynchronous and event-based fusion systems for
MultimodalInteract. ACM,2017,pp.536–543. affectrecognitiononnaturalisticdataincomparisontoconven-
[140] J.Deng,X.Xu,Z.Zhang,S.Fru¨hholz,andB.Schuller,“Univer- tionalapproaches,”IEEETrans.Affect.Comput.,2016.
sum autoencoder-based domain adaptation for speech emotion [163] W.Lim,D.Jang,andT.Lee,“Speechemotionrecognitionusing
recognition,”IEEESignalProcess.Lett.,vol.24,no.4,pp.500–504, convolutional and recurrent neural networks,” in Proc. Annu.
2017. SummitConf.Asia-Pac.SignalInf.Proc.Assoc.,2016,pp.1–4.
[141] E.Coutinho,J.Deng,andB.Schuller,“Transferlearningemotion [164] F. Burkhardt, A. Paeschke, M. Rolfes, W. F. Sendlmeier, and
manifestationacrossmusicandspeech,”inProc.Int.JointConf. B. Weiss, “A database of german emotional speech,” in Proc.
NeuralNetworks,2014,pp.3592–3598. INTERSPEECH,vol.5,2005,pp.1517–1520.
[142] Z. Aldeneh and E. M. Provost, “Using regional saliency for [165] C.Busso,M.Bulut,C.-C.Lee,A.Kazemzadeh,E.Mower,S.Kim,
speechemotionrecognition,”inProc.ICASSP,2017. J.N.Chang,S.Lee,andS.S.Narayanan,“Iemocap:Interactive
[143] S. H. Fairclough, “Fundamentals of physiological computing,” emotionaldyadicmotioncapturedatabase,”LanguageResources
Interact.Comput.,vol.21,no.1-2,pp.133–145,2008. Eval.,vol.42,no.4,p.335,2008.
[144] F. H. Wilhelm and P. Grossman, “Emotions beyond the labora- [166] S. Jirayucharoensak, S. Pan-Ngum, and P. Israsena, “Eeg-based
tory:Theoreticalfundaments,studydesign,andanalyticstrate- emotionrecognitionusingdeeplearningnetworkwithprincipal
giesforadvancedambulatoryassessment,”Biol.Psychol.,vol.84, componentbasedcovariateshiftadaptation,”TheScientificWorld
no.3,pp.552–569,2010. Journal,vol.2014,2014.
[145] M.YanagimotoandC.Sugimoto,“Recognitionofpersistingemo- [167] W.-L.Zheng,J.-Y.Zhu,Y.Peng,andB.-L.Lu,“Eeg-basedemotion
tionalvalencefromeegusingconvolutionalneuralnetworks,”in classification using deep belief networks,” in Proc. Int. Conf.
Proc.Int.WorkshopComputat.Intell.Appl.,2016,pp.27–32. MultimediaExpo,2014,pp.1–6.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2015 20
[168] H.XuandK.N.Plataniotis,“Affectivestatesclassificationusing [190] A.Yao,J.Shao,N.Ma,andY.Chen,“Capturingau-awarefacial
eegandsemi-superviseddeeplearningapproaches,”inProc.Int. featuresandtheirlatentrelationsforemotionrecognitioninthe
WorkshopMultimediaSignalProcess.,2016,pp.1–6. wild,”inProc.Int.Conf.MultimodalInteract.,2015,pp.451–458.
[169] R.M.Mehmood,R.Du,andH.J.Lee,“Optimalfeatureselection [191] L.Tan,K.Zhang,K.Wang,X.Zeng,X.Peng,andY.Qiao,“Group
and deep learning ensembles method for emotion recognition emotion recognition with individual facial emotion cnns and
fromhumanbraineegsensors,”IEEEAccess,vol.5,pp.14797– globalimagebasedcnns,”inProc.Int.Conf.MultimodalInteract.,
14806,2017. 2017,pp.549–552.
[170] L.Bozhkov,P.Koprinkova-Hristova,andP.Georgieva,“Learning [192] J.Kukacˇka,V.Golkov,andD.Cremers,“Regularizationfordeep
to decode human emotions with echo state networks,” Neural learning:Ataxonomy,”arXivpreprintarXiv:1710.10686,2017.
Networks,vol.78,pp.112–119,2016. [193] D.Mahajan,R.Girshick,V.Ramanathan,K.He,M.Paluri,Y.Li,
[171] Z.Yin,M.Zhao,Y.Wang,J.Yang,andJ.Zhang,“Recognitionof A. Bharambe, and L. van der Maaten, “Exploring the limits of
emotionsusingmultimodalphysiologicalsignalsandanensem- weakly supervised pretraining,” in Proc. Europ. Conf. Comput.
bledeeplearningmodel,”Comput.Meth.ProgramsBiomed.,vol. Vision,2018,pp.185–201.
140,pp.93–110,2017. [194] Y.LeCun,L.Bottou,Y.Bengio,andP.Haffner,“Gradient-based
[172] H.P.Martinez,Y.Bengio,andG.N.Yannakakis,“Learningdeep learning applied to document recognition,” Proc. IEEE, vol. 86,
physiologicalmodelsofaffect,”IEEEComput.Intell.Mag.,vol.8, no.11,pp.2278–2324,1998.
no.2,pp.20–33,2013. [195] A.GravesandN.Jaitly,“Towardsend-to-endspeechrecognition
[173] F.Lingenfelser,J.Wagner,andE.Andre´,“Asystematicdiscussion withrecurrentneuralnetworks,”inProc.Int.Conf.Mach.Learn.,
offusiontechniquesformulti-modalaffectrecognitiontasks,”in 2014,pp.1764–1772.
Proc.Int.Conf.MultimodalInteract.,2011,pp.19–26. [196] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.
[174] S. Poria, E. Cambria, R. Bajpai, and A. Hussain, “A review Gomez,L.Kaiser,andI.Polosukhin,“Attentionisallyouneed,”
of affective computing: From unimodal analysis to multimodal inProc.NIPS,2017.
fusion,”Inf.Fusion,vol.37,pp.98–125,2017. [197] M. D. Zeiler and R. Fergus, “Visualizing and understanding
[175] W. Liu, W.-L. Zheng, and B.-L. Lu, “Emotion recognition using convolutional networks,” in Proc. Europ. Conf. Comput. Vision,
multimodaldeeplearning,”inProc.Int.Conf.NeuralInf.Process., 2014,pp.818–833.
2016,pp.521–529.
[176] M.Lyons,S.Akamatsu,M.Kamachi,andJ.Gyoba,“Codingfa-
cialexpressionswithgaborwavelets,”inProc.Int.Conf.Automat.
FaceGestureRecognit.,1998,pp.200–205.
[177] S. Koelstra, C. Muhl, M. Soleymani, J.-S. Lee, A. Yazdani,
T.Ebrahimi,T.Pun,A.Nijholt,andI.Patras,“Deap:Adatabase
PhilippV.RouastreceivedtheB.Sc.andM.Sc.
for emotion analysis using physiological signals,” IEEE Trans.
degreesinIndustrialEngineeringfromKarlsruhe
Affect.Comput.,vol.3,no.1,pp.18–31,2012.
Institute of Technology, Germany, in 2013 and
[178] A.Dhall,O.RamanaMurthy,R.Goecke,J.Joshi,andT.Gedeon,
2016 respectively. He is currently working to-
“Video and image based emotion recognition challenges in the
wardsthePh.D.degreeinInformationSystems
wild:Emotiw2015,”inProc.Int.Conf.MultimodalInteract.,2015,
andisagraduateresearchassistantatTheUni-
pp.423–426.
versityofNewcastle,Australia.Hisresearchin-
[179] O.Martin,I.Kotsia,B.Macq,andI.Pitas,“TheeNTERFACE’05 terestsincludedeeplearning,affectivecomput-
audio-visual emotion database,” in Proc. Int. Conf. Data Eng. ing, HCI, and related applications of computer
Workshops,2006,pp.8–8. vision.Findhimathttps://prouast.github.io.
[180] F.Ringeval,A.Sonderegger,J.Sauer,andD.Lalanne,“Introduc-
ing the recola multimodal corpus of remote collaborative and
affective interactions,” in Proc. Int. Conf. Automat. Face Gesture
Recognit.,2013,pp.1–8.
[181] W. Bao, Y. Li, M. Gu, M. Yang, H. Li, L. Chao, and J. Tao,
“Building a chinese natural emotional audio-visual database,”
inProc.Int.Conf.SignalProcess.,2014,pp.583–587.
[182] G.McKeown,M.Valstar,R.Cowie,M.Pantic,andM.Schroder, Marc T. P. Adam is a Senior Lecturer in Com-
“Thesemainedatabase:Annotatedmultimodalrecordsofemo- puting and Information Technology at the Uni-
tionally colored conversations between a person and a limited versity of Newcastle, Australia. In his research,
agent,”IEEETrans.Affect.Comput.,vol.3,no.1,pp.5–17,2012. he investigates the interplay of human users’
[183] C.F.Benitez-Quiroz,R.Srinivasan,A.M.Martinezetal.,“Emo- cognitionandaffectinhuman-computerinterac-
tionet:Anaccurate,real-timealgorithmfortheautomaticanno- tion. He received an undergraduate degree in
tationofamillionfacialexpressionsinthewild,”inProc.CVPR, ComputerSciencefromtheUniversityofApplied
2016,pp.5562–5570. Sciences Wu¨rzburg, Germany, and a PhD in
[184] M. Soleymani, J. Lichtenauer, T. Pun, and M. Pantic, “A multi- Economics of Information Systems from Karl-
modaldatabaseforaffectrecognitionandimplicittagging,”IEEE sruheInstituteofTechnology,Germany.
Trans.Affect.Comput.,vol.3,no.1,pp.42–55,2012.
[185] H. Meng, D. Huang, H. Wang, H. Yang, M. AI-Shuraifi, and
Y. Wang, “Depression recognition based on dynamic facial and
vocalexpressionfeaturesusingpartialleastsquareregression,”
inProc.AVEC,2013,pp.21–30.
[186] S. E. Kahou, C. Pal, X. Bouthillier, P. Froumenty, C¸. Gu¨lc¸ehre,
R. Memisevic, P. Vincent, A. Courville, Y. Bengio, R. C. Ferrari Raymond Chiong (M’05-SM’14) is a Senior
et al., “Combining modality specific deep neural networks for Lecturer at the University of Newcastle, Aus-
emotion recognition in video,” in Proc. Int. Conf. Multimodal tralia. He received his MSc from the University
Interact.,2013,pp.543–550. ofBirmingham,UK,andPhDfromtheUniversity
[187] G. Gosztolya, R. Busa-Fekete, and L. To´th, “Detecting autism, of Melbourne, Australia. His research interests
emotions and social signals using adaboost,” in Proc. INTER- include evolutionary game theory, optimization,
SPEECH,2013,pp.220–224. data analytics, and modeling of complex adap-
[188] M.Ka¨chele,M.Schels,andF.Schwenker,“Inferringdepression tive systems. He is the Editor-in-Chief of the
andaffectfromapplicationdependentmetaknowledge,”inProc. JournalofSystemsandInformationTechnology,
AVEC,2014,pp.41–48. anEditorofEngineeringApplicationsofArtificial
[189] M.Liu,R.Wang,S.Li,S.Shan,Z.Huang,andX.Chen,“Combin- Intelligence,andanAssociateEditoroftheCom-
ingmultiplekernelmethodsonriemannianmanifoldforemotion putationalIntelligenceMagazine.
recognition in the wild,” in Proc. Int. Conf. Multimodal Interact.,
2014,pp.494–501."
63,65,Deep neural network augmentation: Generating faces for affect analysis,"['D Kollias', 'S Cheng', 'E Ververas', 'I Kotsia']",2020,135,"Affective Faces Database, Radboud Faces Database","CNN, classification, deep learning, neural network",train Deep Neural Networks over eight databases face database is annotated in terms of  valence and arousal and is then used for affect synthesis. The fact that this a temporal database,No DOI,International Journal of …,https://arxiv.org/abs/1811.05027,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"Noname manuscript No.
(will be inserted by the editor)
Deep Neural Network Augmentation: Generating Faces for
Affect Analysis
Dimitrios Kollias (cid:63) · Shiyang Cheng † · Evangelos Ververas ∗ · Irene
Kotsia1 · Stefanos Zafeiriou2
Received:Sept30th2018/Accepted: date
Abstract This paper presents a novel approach for recognitionoveralldatabases;greatlyimprovedperfor-
synthesizing facial affect; either in terms of the six ba- mances are achieved when compared with state-of-the-
sic expressions (i.e., anger, disgust, fear, joy, sadness art methods, as well as with GAN-based data augmen-
and surprise), or in terms of valence (i.e., how posi- tation, in all cases.
tive or negative is an emotion) and arousal (i.e., power
Keywords dimensional, categorical affect, valence,
of the emotion activation). The proposed approach ac-
arousal,basicemotions,facialaffectsynthesis,4DFAB,
cepts the following inputs: i) a neutral 2D image of a
blendshape models, 3DMM fitting, DNNs, StarGAN,
person;ii)abasicfacialexpressionorapairofvalence-
Ganimation, data augmentation, affect recognition,
arousal (VA) emotional state descriptors to be gener-
facial expression transfer
ated,orapathofaffectinthe2DVASpacetobegener-
atedasanimagesequence.Inordertosynthesizeaffect
intermsofVA,forthisperson,600,000framesfromthe
1 Introduction
4DFAB database were annotated. The affect synthesis
isimplementedbyfittinga3DMorphableModelonthe Rendering photorealistic facial expressions from single
neutral image, then deforming the reconstructed face static faces while preserving the identity information
and adding the inputted affect, and blending the new is an open research topic which has significant impact
facewiththegivenaffectintotheoriginalimage.Qual- on the area of affective computing. Generating faces of
itative experiments illustrate the generation of realistic a specific person with different facial expressions can
images, when the neutral image is sampled from thir- be used in various applications, including face recogni-
teenwellknownlab-controlledorin-the-wilddatabases, tion[12][44],faceverification[58][60],emotionpredic-
including Aff-Wild, AffectNet, RAF-DB; comparisons tion, expression database generation, facial expression
with Generative Adversarial Networks (GANs) show augmentation and entertainment.
the higher quality achieved by the proposed approach. This paper describes a novel approach that uses an
Then,quantitativeexperimentsareconducted,inwhich arbitraryfaceimagewithaneutralexpressionandsyn-
the synthesized images are used for data augmenta- thesizes a new face image of the same person, but with
tionintrainingDeepNeuralNetworkstoperformaffect a different expression, generated according to a cate-
gorical or dimensional emotion representation model.
(cid:63)dimitrios.kollias15@imperial.ac.uk This problem cannot be tackled using small databases
†shiyang.cheng11@imperial.ac.uk
with labeled facial expressions, as it would be really
∗e.ververas16@imperial.ac.uk
1I.Kotsia@mdx.ac.uk difficult to disentangle facial expressions and identity
2s.zafeiriou@imperial.ac.uk information through them. Our approach is based on
the analysis of a large 4D facial database, the 4DFAB
(cid:63),†,∗,2Department of Computing, Imperial College London,
[14], which we appropriately annotated and used for
QueensGate,LondonSW7 2AZ,UK
1 Department of Computer Science, Middlesex University of facial expression synthesis on a given subject’s face.
London,LondonNW44BT,U.K At first, a dimensional emotion model, in terms of
the continuous variables, valence (i.e., how positive or
9102
luJ
61
]VC.sc[
2v72050.1181:viXra2 D.Kollias
negative is an emotion) and arousal (i.e., power of the high quality and realistic. All other methods produce
emotionactivation)[67][53],hasbeenusedtoannotate synthesized faces according to the six basic, or a few
a large amount of 600,000 facial images. This model more, expressions. We further show that the proposed
can represent, not only primary, extreme expressions, approachcanaccuratelysynthesizethesixbasicexpres-
but also subtle expressions which are met in everyday sions.
human to human, or human to machine interactions. Moreover,itisthefirsttimethata4Dfacedatabase
Additionally, a categorical emotion model, in terms of isannotatedintermsofvalenceandarousalandisthen
the six basic facial expressions, has been used, accord- used for affect synthesis. The fact that this a temporal
ing to which 12,000 expressions from the 4DFAB were database ensures that successive video frames’ annota-
selected, including 2,000 cases for each of the six basic tions are adjacent in the VA Space. Consequently, we
expressions. generate temporal affect sequences on a given neutral
Theproposedapproachaccepts:i)apairofvalence- face by using annotations that are adjacent in the VA
arousal values and synthesize the respective facial af- Space. Results are presented in the qualitative experi-
fect, ii) a path of affect in the 2D VA Space and syn- mental study that illustrate this novel capability.
thesize a temporal sequence showing it, iii) a value in- It should be also mentioned that the proposed ap-
dicatingthebasicfacialexpressiontobesynthesized;a proach works well, when presented with a neutral face
given neutral 2D image of a person is used in all cases image, obtained either in a controlled environment, or
to appropriately transfer the synthesized affect. in-the-wild, e.g., irrespective of the head pose of the
Section 2 refers to related work regarding facial ex- person appearing in the image.
pressionsynthesis,aswellasdataaugmentationrelated An extensive experimental study is provided, over
methodologies. Section 3 presents materials and meth- most significant databases with affect, showing that
ods that are used in the current work. We describe the the developed DNNs based on the proposed facial af-
annotationanduseofthe4DFABdatabaseandthe3D fect synthesis approach outperform the existing state-
Morphable Model that we utilize in our developments. of-the-art,aswellthesameDNNsbasedonfacialaffect
Section 4 presents our approach, explaining in detail synthesis produced by GAN architectures.
allstepsusedtosynthesizeaffectonanimageorimage
sequence. Section 5 mentions the categorical and di-
2 Related Work
mensional databases, which are used by our approach.
AnextensiveexperimentalstudyispresentedinSec- Facial expression transfer is a research field for map-
tion6.Atfirst,aqualitativeevaluationoftheproposed ping and generating desired images of specified subject
approach is provided, also showing the achieved higher and facial expression. Many methods achieved signifi-
qualitywhencomparedtoGAN-generatedfacialaffect. cant results for high-resolution images and are applied
Then,weusethesynthesizedfacialimagesfordataaug- to a wide range of applications, such as facial anima-
mentation and train Deep Neural Networks over eight tion, facial editing, and facial expression recognition.
databases, annotated with either dimensional or cate- There are mainly two categories of methods for fa-
gorical affect labels. We show that the achieved per- cialexpressiontransferfrom asingle image:traditional
formance is much higher than i) that obtained by the graphic-based methods and emerging generative meth-
respectivestate-of-the-artmethods,ii)theperformance ods. In the first case, some methods directly warp the
ofthesameDNNswithdataaugmentationprovidedby input face to create the targeted expression, by either
theStarGANandGanimationnetworks.Afurthercom- 2D warps [22,23], or 3D warps [7,11,36]. Other meth-
parison with GANs is performed, with the synthesized odsconstructparametricglobalmodels.In[41],aprob-
facialimagesbeingused,togetherwiththeoriginalim- abilistic model is learned, in which existing and gen-
ages, as DNN training and/or test data respectively; erated images obey structural constraints. [5] added
this also verifies the improved performance of our ap- finescale dynamic details, such as wrinkles and inner
proach.Anablationstudyisalsopresented,illustrating mouth, that are associated with facial expressions. Al-
the effect of data granularity and subjects’ age on the though these methods have achieved some positive re-
performance of the proposed method. Finally, conclu- sultsinhigh-resolutionandone-to-manyimagesynthe-
sions and future work are presented in Section 6. sis, they are still limited due to their sophisticated de-
Theproposedapproachincludesmanynovelcontri- sign and expensive computation.
butions. To the best of our knowledge, it is the first In [61], the authors developed a real-time face-to-
time that the dimensional model of affect is taken into faceexpressiontransfersystem,withan extrablending
account when synthesizing face images. As verified in stepformouth.This2D-to-3Dapproachshowspromis-
the experimental study, the generated images are of ing results, but due to the nature of its formulation, itGeneratingfacesforaffect analysis 3
is unable to retrieve fine-details, and its applicability is smoothtransitionfromanemotiontoanother.[20]pro-
limited to expressions lying in a linear shape subspace posed an Expression Generative Adversarial Network
with known rank. The authors extended this system to (ExprGAN) in which the expression intensity could be
humanportraitvideotransfer[62].Theycapturedfacial controlledinacontinuousmannerfromweaktostrong.
expressions, eye gaze, rigid head pose, and motions of Theidentityandexpressionrepresentationlearningwere
the upper body of a source actor and transferred them disentangledandtherewasnorigidrequirementofpaired
to a target actor in real time. samples for training. The authors developed a three-
The second category of methods is based on data- stageincrementallearningalgorithmtotrainthemodel
driven generative models. At the beginning, some gen- on small datasets.
erative models, such as deep belief nets (DBN)[59] and
In [47], the authors proposed a weakly supervised
higher-order Boltzmann machines [50], had been ap-
adversarial learning framework for automatic facial ex-
pliedtofacialexpressionsynthesis.However,thesemod-
pression synthesis based on continuous action unit co-
els faced problems such as blurry generated images, in-
efficients. In [48], the Ganimation was proposed that
capability of fine control of facial expression and low-
additionallycontrolledthegeneratedexpressionbyAU
resolution outputs.
labels,andallowedacontinuousexpressiontransforma-
With the recent development of Generative Adver- tion. In addition, the authors introduced an attention-
sarialNetworks(GANs)[25],thesenetworkshavebeen basedgeneratortopromotetherobustnessoftheirmodel
appliedtofacialexpressiontransfer;duetothefactthat for distracting backgrounds and illuminations.
thegeneratedimagesareofhigh-quality,theseprovided
Recently, [57] utilized landmarks and proposed the
positiveresults.Agenerativemodelistrainedaccording
geometry-guided GAN (G2GAN) to generate smooth
to a dataset, including all information about identity,
image sequences of facial expressions. G2GAN uses ge-
expression, viewing angle, etc, while performing facial
ometryinformationbasedondualadversarialnetworks
expression transfer. Generative modeling methods re-
to express face changes and synthesizes facial images.
duce the complicated design of the connection between
Through manipulating landmarks, smoothly changed
facial textures and emotional states and encode intu-
images can also be generated. However, this method
itionistic facial features into parameters of data distri-
demands a neutral face of the targeted person as the
bution. However, the main drawback of GANs is the
intermediateoffacialexpressiontransfer.Althoughthe
traininginstabilityandthetrade-offbetweenvisualqual-
expression removal network could generate a neutral
ity and image diversity.
expression of a specific person, this procedure brings
Since the original GAN could not generate facial
additional artifacts and degrades the performance of
images with a specific facial expression referring to a
expression transition.
specific person, some methods conditioned on expres-
[49]usedgeometry(faciallandmarks)tocontrolthe
sion categories have been proposed. Conditional GANs
expression synthesis with a facial geometry embedding
(cGANs)[40](andconditionalvariationalautoencoders
networkandproposedaGeometry-ContrastiveGenera-
(cVAEs) [56]) can generate samples conditioned on at-
tiveAdversarialNetwork(GC-GAN)totransfercontin-
tribute information, when this is available. Those net-
uousemotionsacrossdifferentsubjects,evenifthereex-
works require large training databases so that identity
istedbigdifferenceinshapes.[69]proposedaboundary
informationcanbeproperlydisambiguated.Otherwise,
latent space and boundary transformer. They mapped
whenpresentedwithanunseenface,thenetworkstend
the source face into the boundary latent space, and
to generate faces which look like the closest subject in
transformed the source faces boundary to the targets
the training datasets. During training, those networks
boundary, which was the medium to capture facial ge-
require the knowledge of the attribute labels; it is not
ometric variances during expression transfer.
clear how to adapt them to new attributes without
retraining from scratch. Finally, these networks suffer In [37], an unpaired learning framework was devel-
from mode-collapse (e.g., the generator only outputs oped to learn the mapping between any two facial ex-
samples from a single mode, or with extremely low va- pressions in the facial blendshape space. This frame-
riety) and blurriness. work automatically transforms the source expression
The conditional difference adversarial autoencoder in an input video clip to a specified target expression.
(CDAAE)[74]aimsatsynthesizingspecificexpressions This work lacks the capability to generate personalized
for unseen persons with a targeted emotion or facial expressions; individual-specific expression characteris-
action unit label. However, such GAN-based methods tics,suchaswrinklesandcreases,areignored.Also,the
are still limited to discrete facial expression synthesis, transitions between different expressions are not taken
i.e., they cannot generate a face sequence showing a into consideration. Finally, this work is limited in the4 D.Kollias
sensethatitcannotproducehighlyexaggeratedexpres- sessions, different video clips were shown that stimu-
sions. lateddifferentspontaneousbehaviors.Inthispaper,we
Both the graphic-based methods and the generera- use all 1,580 spontaneous expression sequences (video
tive methods of facial expression transfer have been clips) for dimensional emotion analysis and synthesis.
used to create synthetic data that are used as auxil- The frame rate of 4DFAB database is 60 FPS and the
iary data in network training, augmenting the train- averagecliplengthforspontaneousexpressionsequences
ing dataset. A synthetic data generation system with is 380 frames. Consequently the 1,580 expression se-
a 3D convolutional neural network (CNN) was created quences correspond to 600,000 frames, which we anno-
in [1] to confidentially create faces with different levels tated in terms of valence and arousal (details follow
ofsaturationinexpression.[4]proposedtheDataAug- in the next subsection). These sequences cover a wide
mentation Generative Adversarial Network (DAGAN) range of expressions as shown in Figs. 2 and 3.
which is based on cGAN and tested its effectiveness Moreover,tobeabletodevelopthecategoricalemo-
on vanilla classifiers and one shot learning. DAGAN tion synthesis model, we used the 2,000 expressive 3D
is a basic framework for data augmentation based on meshes per basic expression (12,000 meshes in total)
cGAN. thatwereprovidedalongwith4DFAB.Those3Dmeshes
[76]presentedanotherbasicframeworkforfacedata corresponded to (annotated) apex frames of posed ex-
augmentationbasedonCycleGAN[75].SimilartocGAN, pressionsequencesin4DFAB.Suchexamplesareshown
CycleGANisalsoangeneral-purposesolutionforimage- in Fig. 1.
to-image translation, but it learns a dual mapping be-
tween two domains simultaneously with no need for
paired training examples, because it combines a cycle
consistencylosswithadversarialloss.Theauthorsused AN
this framework to generate auxiliary data for imbal-
anceddatasets,wherethedataclasswithfewersamples
DI
was selected as transfer target and the data class with
more samples was the reference.
FE
J
3 Materials & Methods
Inthefollowing,wefirstdescribethe4DFABdatabase, SA
itsannotationintermsofvalence-arousalandtheselec-
tionofexpressivecategoricalsequencesfromit.Thean- SU
notated 4DFAB database has been used for construct-
Fig.1 Examplesfromthe4DFABofapexframeswithposed
ing the 3D facial expression gallery that is the basis expressionsforthesixbasicexpressions:Anger(AN),Disgust
of our affect synthesis pipeline described in the next (DI),Fear(FE),Joy (J),Sadness(SA),Surprise(SU)
Section. Then we describe the methods we have used:
a) for registering and correlating all components of the
3Dgalleryintoauniversalcoordinateframe;b)forcon-
structing the 3D Morphable Model used in this work.
3.2 4DFAB Dimensional Annotation
Targeting to develop the novel dimensional expression
3.1 The 4DFAB Database synthesismethod,all1,580dynamic3Dsequences(i.e.,
over 600,000 frames) of 4DFAB have been annotated
The4DFABdatabase[14]isthefirstlargescale4Dface in terms of valence and arousal emotion dimensions. In
database designed for biometric applications and facial total,threeexpertswerechosentoperformtheannota-
expression analysis. It consists of 180 subjects (60 fe- tiontask.Eachexpertperformedatime-continuousan-
males,120males)agingfrom5to75years.4DFABwas notationforbothaffectivedimensions.Theapplication-
collected over a period of 5 years under four different tool described in [71], was used in the annotation pro-
sessions, with over 1,800,000 3D faces. The database cess.
was designed to capture articulated facial actions and Each expert logged into the application-annotation
spontaneous facial behaviors, the latter being elicited toolusinganidentifier(e.g.his/hername)andselected
by watching emotional video clips. In each of the four an appropriate joystick; then the application showedGeneratingfacesforaffect analysis 5
a scrolling list of all videos and the expert selected a
video to annotate; then a screen appeared that showed
theselectedvideoandasliderofvalenceorarousalval-
ues ranging in [−1,1] ; the expert annotated the video
by moving the joystick either up or down; finally, a
file was created with the annotations. The mean inter-
annotation correlation per annotator was 0.66, 0.70,
0.68forvalenceand0.59,0.62,0.59forarousal.Theav-
erage of those mean inter-annotation correlations was
0.68 for valence and 0.60 for arousal. Those values are
high, indicating a very good agreement between anno-
tators. As a consequence, the final label values were
chosen to be the mean of those three annotations.
Examples of frames from the 4DFAB along with
their annotations, are shown in Fig. 2. Fig. 3 shows
the2Dhistogramofannotationsof4DFAB.Intherest Fig. 3 The2Dhistogramofannotations of 4DFAB
of the paper, we refer to the 4DFAB database either
as: i) the 600,000 frames with their corresponding 3D
meshes, which have been annotated with 2D valence the above properties are said to be in dense correspon-
and arousal (VA) emotion values or ii) the 12,000 apex dencewithoneanother.So,correlatingallthesemeshes
frames of posed expressions with their corresponding with a universal coordinate frame (viz. a 3D face tem-
3D meshes, which have categorical annotation. plate) is a step to follow so as to establish dense corre-
spondence.
In order to do so, we need to define a 2D UV space
for each mesh, which in fact is a contiguous flattened
atlas that embeds the 3D facial surface. Such a UV
space is associated with its corresponding 3D surface
through a bijective mapping; thus, establishing dense
correspondence between two UV images implicitly es-
tablishes a 3D-to-3D correspondence for the mapped
mesh. UV mapping is the 3D modelling process of pro-
jecting a 2D image to a 3D model’s surface for texture
mapping. The letters U and V denote the axes of the
2D texture, since X, Y and Z are already taken to de-
note the axes of the 3D object in model space.
Weemployanoptimalcylindricalprojectionmethod
[10]tosyntheticallycreateaUVspaceforeachmesh.A
UVmap(whichisanimageI),witheachpixelencoding
Fig. 2 The2DValence-ArousalSpaceandsomerepresenta-
tiveframesof4DFAB both spatial information (X, Y, Z) and texture infor-
mation (R, G, B), is produced, on which we perform
non-rigid alignment. Non-rigid alignment is performed
through the UV-TPS method that utilises key land-
marksfittingandThinPlateSpline(TPS)warping[17].
3.3 MeshPre-Processing:EstablishingDenseCorrespon- Following[14],weperformseveralmodificationsto[17],
dence to suit our data. Firstly, we build session-and-person-
specific Active Appearance Models (AAMs)[39] to au-
Each3Dmeshisfirstre-parameterizedintoaconsistent tomatically track feature points in the UV sequences.
form where the number of vertices, the triangulation This means that 4 different AAMs are built and used
and the anatomical meaning of each vertex are made separately for one subject. Main reasons behind this
consistent across all meshes. For example, if the vertex are:i)texturesofdifferentsessionsdifferduetoseveral
withindexiinonemeshcorrespondstothenosetip,it facts (i.e. aging, beards, make-ups, experiment lighting
isrequiredthatthevertexwiththesameindexinevery condition), ii) person-specific model is proven more ac-
meshcorrespondstothenosetiptoo.Meshessatisfying curate and robust in specific domains [15].6 D.Kollias
In total, 435 neutral meshes and 1047 expression denoting the number of vertices in the shape model;
meshes (1 neutral and 2-3 expressive meshes per per- U
s
∈ R3N×ns is the shape eigenbase (in our case it is
son and session) in 4DFAB were selected; these con- the identity subspace of LSFM) with n << 3N being
s
tained annotations with 79 3D landmarks. They were the number of principal components (n is chosen to
s
unwrapped and rasterised to UV space, then grouped explain a percentage of the training set variance; gen-
for building the corresponding AAMs. Each UV map erally,thispercentageis99.5%);andp∈Rns isavector
was flipped to increase fitting robustness. Once all the of parameters which allows for the generation of novel
UV sequences were tracked with 79 landmarks, they shape instances.
werethenwarpedtothecorrespondingreferenceframe Thepurposeofcameramodelistoprojecttheobject-
usingTPS,thusachievingthe3Ddensecorrespondence. centered Cartesian coordinates of a 3D mesh instance
For each subject and session, one specific reference co- into 2D Cartesian coordinates in an image plane. At
ordinateframefromhis/herneutralUVmapwasbuilt. first, given that the camera is static, the 3D mesh is
From each warped frame, we could uniformly sample rotated and translated using a linear view transforma-
the texture and 3D coordinates. Eventually, a set of tion, which results in 3D rotation and translation com-
non-rigidly corresponded 3D meshes under the same ponents. Then, a nonlinear perspective transformation
topology and density were obtained. is applied. Note that quaternions [32,66] are used to
Given that meshes have been aligned to their des- parametrise the 3D rotation, which ensures computa-
ignated reference frame, the last step was to establish tionalefficiency,robustnessandsimplerdifferentiation.
dense 3D-to-3D correspondences between those refer- In this manner we construct the camera parameters
ence frames and a 3D template face. This is a 3D mesh (i.e., 3D translation components, quaternions and pa-
registration problem, solved by Non-rigid ICP[3]. We rameteroflinearperspectivetransformation).Thecam-
employed it to register the neutral reference meshes era model of the 3DMM applies the above transforma-
to a common template, the Large Scale Facial Model tionsonthe3Dshapeinstancesgeneratedbytheshape
(LSFM) [9]. We brought all 600,000 3D meshes into model. Finally, the camera model can be written as:
full correspondence with the mean face of LSFM. As a
W(p,c)=P(S(p),c), (2)
result, we created a new set of 600,000 3D faces that
share identical mesh topology, while maintaining their
where S(p) is a 3D face instance; c ∈ Rnc are the
camera parameters (for rotation, translation and focal
originalfacialexpressions.Inthefollowing,thissetcon-
length; n is 7); and P :R3N →R2N is the perspective
stitutes the 3D facial expression gallery which we use c
camera projection.
for facial affect synthesis.
Forthetexturemodel,largefacialin-the-wilddata-
basesannotatedforsparselandmarksareneeded.Letus
assumethatthemesheshavecorrespondingcameraand
3.4 Constructing a 3D Morphable Model
shape parameters. These images are passed through a
3.4.1 General Pipeline dense feature extraction function that returns feature-
based representations for each image. These are then
Acommon3DMMconsistsofthreeparametricmodels: sampled from the camera model at each vertex loca-
the shape, the camera and the texture models. tionsoastobuildatexturesample,whichwillbenon-
To build the shape model, the training 3D meshes sensical for some regions mainly due to self occlusions
shouldbeputindensecorrespondence(similarlytothe present in the mesh projected in the image space. To
previous Mesh Pre-Processing subsection). Next, Gen- complete the missing information of the texture sam-
eralized Procrustes Analysis is performed to remove ples, Robust PCA (RPCA) with missing values [55] is
any similarity effects, leaving only shape information. applied.Thisproducescompletefeature-basedtextures
Finally, Principal Component Analysis (PCA) is ap- thatcanbeprocessedwithPCAtocreatethestatistical
pliedtothesemeshes,whichgeneratesa3Ddeformable model of texture, which can be written as:
modelasalinearbasisofshapes.Thismodelallowsfor T(λ)=¯t+U λ, (3)
t
thegenerationofnovelshapeinstances.Themodelcan
where t ∈ R3N is the mean texture component (in
be expressed as:
our case it is the mean of texture model from LSFM);
S(p)=¯s+U sp (1) U t ∈ R3N×nt and λ ∈ Rnt are the texture subspace
(eigenbase) and texture parameters, respectively, with
where ¯s ∈ R3N is the mean component of 3D shape n << 3N being the number of principal components.
t
(in our case it is the mean of shape models from the This model can be used to generate novel 3D feature-
LSFM model described in the next subsection) with N based texture instances.Generatingfacesforaffect analysis 7
Fig. 4 Thefacialaffectsynthesisframework:theuserinputsanarbitrary2Dneutralfaceandtheaffecttobesynthesized(a
pairofvalence-arousalvaluesin thiscase)
3.4.2 The Large Scale Facial Model (LSFM) face and adds the input affect, and 4) blends the new
face with the given affect into the original image. Here
We have adopted the LSFM model constructed using let us note that the total time needed for the first two
the MeIn3D dataset [9]. The construction pipeline of steps is about 400ms; this has to be performed only
LSFM starts with a robust approach to 3D landmark once,ifgeneratingmultipleimagesfromthesameinput
localization resulting in generating 3D landmarks for image. Specific details regarding the described steps of
the meshes. The 3D landmarks are then employed as ourapproachfollow.ThisprocedureisshowninFig.4.
soft constraints in Non-rigid ICP to place all meshes
in correspondence with a template facial surface; the
meanfaceoftheBaselFaceModel[45]hasbeenchosen.
4.1 Face Detection & Landmark Localization
However, the large cohort of data could result in con-
vergence failures. These are an unavoidable byproduct
The first step to edit an image is to locate landmark
of the fact that both landmark localization and NICP
points that will be used for fitting the 3DMM. We first
are non-convex optimization problems sensitive to ini-
perform face detection with the face detection model
tialization.
from [73] and then utilize [18] to localize 68 2D facial
A refinement post-processing step weeds out prob-
landmark points which are aware of the 3D structure
lematic subjects automatically, guaranteeing that the
of the face, in the sense that points on occluded parts
LSFM models are only constructed from training data
of the face (most commonly part of the jawline) are
forwhichthereexistahighconfidenceofsuccessfulpro-
correctly localized.
cessing. Finally, the LSFM models are derived by ap-
plying PCA on the corresponding training sets, after
excluding the shape vectors that have been classified
as outliers. In total, 9,663 subjects are used to build 4.2 3DMM-Fitting: Cost Function & Optimization
LSFM, which covers a wide variety of age (from 5 to
The goal of this step is to retrieve a reconstructed 3D
over80s),gender(48%male,52%female),andethnicity
face with the texture sampled from the original image.
(82% White, 9% Asian, 5% Mixed Heritage, 3% Black
In order to do so, we first need a 3DMM; we select the
and 1% other).
LSFM.
Fittinga3DMMonfaceimagesisaninversegraph-
4 The Proposed Approach ics approach to 3D reconstruction and consists of op-
timizing three parametric models of the 3DMM, the
In this Section, we present the fully automatic facial shape, texture and camera models. The optimization
affect synthesis framework. The user needs to provide aims at rendering a 2D image which is as close as as
a neutral image and an affect, which can be a VA pair possible to the input one. In our pipeline we follow the
of values, a path in the 2D VA space, or one of the 3DMM fitting approach of [8]. As is already noted, we
basic expression categories. Our approach: 1) performs employtheLSFM[9]S(p)tomodeltheidentitydefor-
face detection and landmark localization on the input mationoffaces.Moreover,weadopttherobust,feature-
neutral image, 2) fits a 3D Morphable Model (3DMM) based texture model T(λ) of [8], built from in-the-wild
ontheresultingimage[8],3)deformsthereconstructed images. The employed camera model is a perspective8 D.Kollias
transformation W(p,c), which projects shape S(p) on 4.4 Synthesizing 2D Face
the image plane.
Consequently, the objective function that we opti- The final step in our pipeline is to render the new 3D
mize can be formulated as: face s new back to the original 2D image. To do that
we employ the mesh that we have deformed accord-
ing to the given affect, the extracted UV texture and
argmin (cid:107)F(W(p,c))−T(λ)(cid:107)2+c l(cid:107)W l(p,c)−s l(cid:107)2 the optimal camera transformation of the 3DMM. For
p,λ,c rendering, we pass the three model instances to a ren-
+ c s(cid:107)p(cid:107)2 Σs−1 +c t(cid:107)λ(cid:107)2 Σ t−1, (4) derer and we use as background the background of the
input image. Lastly, the rendered image is fused with
where the first term denotes the pixel loss between the
the original image via poisson blending [46] to smooth
featurebasedimageFsampledattheprojectedshape’s
theboundarybetweenforegroundfaceandimageback-
locations and the model generated texture; the second
groundsoastoproduceanaturalandrealisticresult.In
termdenotesasparselandmarklossbetweentheimage
our experiments, we used both a CPU-based renderer
2D landmarks and the corresponding 2D projected 3D
[2]andaGPU-basedrenderer[24].TheGPU-basedren-
points, where the 2D shape, s , is provided by [18]; the
l derer greatly decreases the rendering time, as it needs
rest two terms are regularization terms which serve as
20ms to render a single image, while the CPU-based
counter over-fitting mechanism, where Σ and Σ are
s t renderer needs 400ms.
diagonal matrices with the main diagonal being eigen-
valuesoftheshapeandtexturemodelsrespectively;c ,
l
c s andc t areweightsusedtoregularizetheimportance 4.5 Synthesizing Expressive Faces with Given Affect
of each term during optimization and were empirically
setto105,3×106and1,respectively,following[8].Note
also, that the 2D landmarks term is useful as it drives
theoptimizationtoconvergefaster.ProblemofEq.4is
solved by the Project-Out variation of Gauss-Newton
optimization as formulated in [8].
From the optimized models, the optimal shape in-
stance constitutes the neutral 3D representation of the
inputface.Moreover,byutilizingtheoptimalshapeand
camera models, we are able to sample the input image
at the projected locations of the recovered mesh and
extract a UV texture, that we later use for rendering.
4.3 Deforming Face & Adding Affect
Given an affect and an arbitrary 2D image I, we first
fit the LSFM to this image using the aforementioned
3DMM fitting method. After that, we can retrieve a
reconstructed 3D face s with the texture sampled
orig
fromtheoriginalimage(texturesamplingissimplyex-
tractingimagepixelvalueforeachprojected3Dvertex
Fig. 5 Somemeanfaces of the550classesintheVA Space
in image plane). Let us assume that we have created
anaffectsynthesismodelM thattakestheaffectas
Aff
input and generates a new expressive face (denoted as
s gen),i.e.,s=M Aff(affect)(specificdetailsregarding 4.5.1 VA & Basic Expression cases: Building Blend-
the generation of the expressive face, can be found in shape Models & Computing Mean Faces
subsection 4.5). Next, we calculate the facial deforma-
tion ∆s by subtracting the synthesized face s from Let us first describe the VA case. We have 600,000
gen
the LSFM template ¯s, i.e., ∆s = s − ¯s, and im- 3Dmeshes(establishedintodensecorrespondence)and
gen
pose this deformation on the reconstructed mesh, i.e., their VA annotations. We want to appropriately dis-
s = s + ∆s. Therefore, we obtain a 3D face cretize the VA Space into classes, so that each class
new orig
(dubbed s ) with facial affect. contains a sufficient number of data. This is due to the
newGeneratingfacesforaffect analysis 9
Fig. 6 Generationofnewfacial affectfromthe4Dfacegallery; theuserprovidesa targetVApair
factthatifclassescontainonlyfewexamples,itismore Static synthesis If the user selects ’static synthesis’,
likelytoincludeidentityinformation.However,thesyn- then the user should input a specific VA pair of values.
thesizedfacialaffectshouldonlydescribetheexpression Then, we retrieve the mean face of the class to which
associated with the VA pair of values, rather than in- this VA value belongs. We use this mean face as the
formation for the person’s identity, gender, or age. We affect to be added on the 3D face reconstructed from
havechosentoperformagglomerativeclustering[38]on the provided neutral image. Fig. 4 shows the proposed
the VA values, using the euclidean distance as metric approachforthisspecificcase.Fig.6illustratesthepro-
and the ward as linkage criterion (keeping the corre- cedure described in 4.5.1 given that the 550 VA classes
spondence between VA values and 3D meshes). In this are already created.
manner, we created 550 clusters, i.e., classes. Then we Temporal synthesisIftheuserselects’temporalsyn-
built blendshape models and computed the mean face thesis’, then, (s)he should provide a path in the VA
per class. Fig. 5 illustrates the mean faces of various space(forinstancebydrawing)thatthesynthesizedse-
classes. It should be mentioned that the majority of quence should follow. Then, we retrieve the mean faces
classes correspond to the first two quadrants of the VA of the classes to which the VA values of the path be-
Space, namely the regions of positive valence (as can long. We use each of these mean faces as the affect to
be seen in the 2D histogram of Fig. 3). be added on the 3D faces reconstructed from the pro-
As far as the basic expression case is concerned, vided neutral image. As a consequence, an expressive
based on the derived 12,000 3D meshes, 2,000 for each sequenceisgeneratedthatshowstheevolutionofaffect
of the six basic expressions, we built six blendshape on the VA path specified by the user.
models and six corresponding mean faces. Here let us mention that the fact that the 4DFAB
used in our approach is a temporal database, ensures
that successive video frames’ annotations are adjacent
4.5.2 UserSelection:VA/BasicExpr&Static/Temporal intheVASpace,sincetheygenerallyshowthesameor
Synthesis slightly different states of affect. Thus, the 3D meshes
of successive video frames will lie in the same and in
The user first chooses the type of affect that our ap- adjacent classes in the 2-D VA space. Thus mean faces
proachwillgenerate.Theaffectcouldbeeitherapoint, from adjacent classes can be used to show temporal
or a path in the VA space, or one the six basic expres- evolution of affect as was above described.
sion categories. If the user chooses the latter, then we
retrieve the mean face of this category and add it on 4.5.3 Expression Blendshape Models
the3Dfacereconstructedfromtheuser’sinputneutral
image. In this case, the only difference in Fig. 4 would Expressionblendshapemodelsprovideaneffectiveway
be for the user to input a basic expression, the happy to parameterize facial behaviors. The localized blend-
one, instead of a VA pair of values. If the user chooses shape model [43] has been used to describe the se-
the former, then (s)he needs to additionally clarify if lected VA samples. To build this model, we first bring
our approach should generate an image (’static synthe- all meshes into full correspondence following the dense
sis’)orasequenceofimages(’temporalsynthesis’)with registration approach described in Section 3.3. As a re-
this affect. sult, we have a set of training meshes with the same10 D.Kollias
number of vertices and identical topology. Note that they contain and the range of ages of the subjects, and
we have also selected one neutral mesh for each sub- ii) the total number of images that we synthesized us-
ject, which should have full correspondence with the ing our approach (both in the valence-arousal and the
restdata.Next,wesubtracteach3Dmeshfromthere- six basic expressions cases).
spective neutral mesh, and create a set of m difference
vectors d ∈ R3N. We then stack them into a matrix
i
D = [d ,...,d ] ∈ R3N×m, where N is number of ver- 6 Experimental Study
1 m
tices in the mesh. Finally, a variant of sparse Principal
This section describes the experiments performed so as
Component Analysis (PCA) is applied to the data ma-
toevaluatetheproposedapproach.Atfirst,weprovide
trixD,soastoidentifysparsedeformationcomponents
C∈Rh×1: a qualitative evaluation of our approach by showing
many synthesized images or image sequences from all
thirteendatabasesdescribedinthepreviousSection;as
argmin(cid:107)D−BC(cid:107)2 +Ω(C) s.t.V(B), (5) well as by comparing images generated by state-of-the-
F
art GANs (StarGAN, Ganimation) and our approach.
where, the constraint V can be either max(|B k|) = Next, a quantitative evaluation is performed by using
1, ∀k or max(B k) = 1, B ≥ 1, ∀k, with B k ∈ R3N×1 thesynthesizedimagesasadditionaldatatotrainDeep
denoting the kth components of sparse weight matrix Neural Networks (DNNs); it is shown that the trained
B=[B 1,··· ,B h].Selectionofthesetwoconstraintsde- DNNsoutperformcurrentstate-of-the-artnetworksand
pends on the actual usage; the major difference is that GAN-basedmethodsoneachdatabase.Finallyanabla-
the latter one allows for negative weights and there- tionstudyisperformedinwhich:i)thesynthesizeddata
foreenablesdeformationtowardsbothdirections,which are considered and used as a training (test) dataset,
is useful for describing shapes like muscle bulges. In while the original data are respectively used as test
this paper, we have selected the latter constraint, as (training) dataset, ii) the effect of the amount of syn-
we wish to enable bidirectional muscle movement and thesizeddataonnetworkperformanceisstudied,iii)an
synthesise a rich variety of expressions. The regulariza- analysis is performed based on subjects’ age.
tionofsparsecomponentsCwasperformedwith(cid:96)1/(cid:96)2
norm [68,6]. To permit more local deformations, addi-
tionalregularizationparameterswereaddedintoΩ(C). 6.1 Qualitative evaluation of achieved facial affect syn-
To compute optimal C and B, an iterative alternating thesis
optimizationwasemployed(pleasereferto[43]formore
details). We used all databases mentioned in Section 5 to sup-
ply the proposed approach with ’input’ neutral faces.
Wethensynthesizedtheemotionalstatecorresponding
5 Databases to specific affects (both in VA case and in the six ba-
sic expressions one) for these images. At first we show
To evaluate our facial affect synthesis method in differ- many generated images (static synthesis) according to
ent scenarios (e.g. controlled laboratory environment, differentVAvalues,thenweillustrateexamplesofgen-
uncontrolledin-the-wildsetting),weutilizedneutralfa- erated image sequences (temporal synthesis) and next
cial images from as many as 13 databases (both small we present some synthesized (static) images according
and large in terms of size). Table 1 briefly presents to the six basic expressions. Finally, we visually com-
the Multi-PIE [27], Aff-Wild [30,71], AFEW 5.0 [19], pare images generated by our approach with synthe-
AFEW-VA [31], BU-3DFE [70], RECOLA [51], Affect- sized images by StarGAN and Ganimation.
Net[42],RAF-DB[35],KF-ITW[8],Faceplace,FEI[63],
2DFaceSetsandBosphorus[54]databasesthatweused 6.1.1 Results on Static & Temporal Affect Synthesis
in our experimental study. Let us note that for Affect-
Net no test set is released and thus we use the released Fig. 7 shows representative results of facial affect syn-
validationsettotestonandrandomlydividethetrain- thesis, when user inputs a VA pair and selects to gen-
ing set into a training and a validation subset (with a erate a static image. These results are organized in
85/15 split). three age groups: Fig. 7(b) kids, Fig. 7(c) elderly peo-
Table 1 presents these databases by showing: i) the ple and Fig. 7(a) in-between ages. In each part, the
model of affect they use, their condition, their type first row illustrates neutral images sampled from each
(static images or audiovisual image sequences), the to- of the aforementioned databases, the second one shows
tal number of frames and (male/female) subjects that the respective synthesized images and the third showsGeneratingfacesforaffect analysis 11
Table 1 Databases used in our approach, along with their properties and the number of synthesized images in the valence-
arousalcaseandthe sixbasicexpressionsone;’static’ meansimages,’A/V’means audiovisualsequences,i.e.,videos
Total#of
Databases(DBs) DBType ModelofAffect Condition DBSize #ofSubjects AgeRange
SynthesizedImages
VA BasicExpr
337
Neutral,Surprise,Disgust,
MULTI-PIE[27] static controlled 755,370 Male:235 - 52,254 5,520
Smile+Squint,Scream
Female:102
KinectFusionITW[8] static Neutral,Happiness,Surprise in-the-wild 3,264 17 - 116,235 12,236
200
FEI[63] static Neutral,Smile controlled 2,800 Male:100 19-40 11,400 1,200
Female:100
235
Faceplace1 static 6BasicExpr,Neutral,Confusion controlled 6,574 Male:143 - 59,736 6,288
Female:92
AFEW5.0[19] A/V 6BasicExpr,Neutral in-the-wild 41,406 >330 1-77 705,649 56,514
46
RECOLA[51] A/V VA controlled 345,000 Male:19 - 46,455 4,890
Female:27
100
BU-3DFE[70] static 6BasicExpr,Neutral controlled 2,500 Male:56 18-70 5,700 600
Female:44
105
Bosphorus[54] static 6BasicExpr controlled 4,666 Male:60 25-35 17,018 1,792
Female:45
450,000
VA+6BasicExpr,
AffectNet[42] static in-the-wild manually - 0to>50 2,476,235 176,425
Neutral+Contempt
annotated
200
Aff-Wild[30][71] A/V VA in-the-wild 1,224,094 Male:130 - 60,135 6,330
Female:70
AFEW-VA[31] A/V VA in-the-wild 30,050 <600 - 108,864 11,460
6Basic,Neutral 15,339
RAF-DB[35] static in-the-wild - 0-70 121,866 12,828
+11CompoundExpr +3,954
23
2DFaceSets2: 6Basic,Neutral
static controlled 599 Male:13 - 2,736 288
Pain +10PainExpr
Female:10
34
2DFaceSets:
static Neutral,Smile controlled 369 Male:0 - 2,679 282
Iranian
Female:34
100
2DFaceSets:
static Neutral controlled 100 Male:50 - 5,700 600
NottinghamScans
Female:50
(a)
(b) (c)
Fig. 7 (a)-(c). VA Case of static (facial) synthesis across all databases; first rows show the neutral, second ones show the
corresponding synthesized images and third rows show the corresponding VA values. Images of: (b) kids, (c) elderly people
and(a)in-betweenages,areshown.12 D.Kollias
(a) (b)
Fig. 9 Basic Expression Case of facial synthesis: on the left
hand side of (a) and (b) are the neutral 2D images and on
therightthesynthesizedimageswithsomebasicexpressions
fectively synthesize facial affect irregardless of image
conditions(e.g.,occlusions,illuminationandheadposes).
6.1.2 Comparison with GANs
In order to characterize the value that the proposed
approach imparts, we provide qualitative comparisons
with two state-of-the-art GANs, namely StarGAN [16]
andGanimation.LikeCycleGAN(referencedinSection
2),Star-GANperformsimage-to-imagetranslation,but
adoptsaunifiedapproachsuchthatasinglegeneratoris
trainedtomapaninputimagetooneofmultipletarget
domains,selectedbytheuser.Bysharingthegenerator
weightsamongdifferentdomains,adramaticreduction
of the number of parameters is achieved. Ganimation
was described in Section 2.
We used these networks to fit the VA case, namely
to generate expressions according to VA values. We
trained them with the same 600,000 frames of 4DFAB
that we used in our approach. Preprocessing also in-
Fig. 8 VA case of facial synthesis: on the left hand side are
cluded face detection and alignment. For a fair com-
theneutral2Dimagesandontherightthesynthesizedimages
parison, in all presented comparisons (both qualitative
withdifferentlevels ofaffect
and quantitative), the GANs generated samples, while
provided with the same neutral images and the same
the respective VA values that were synthesized. More- VA values.
over, Fig. 8 shows neutral images on the left hand side Fig.11presentsavisualcomparisonbetweenimages
(firstcolumn)andsynthesizedimages,withvariousva- generatedbyourapproach,StarGANandGanimation.
lenceandarousalvalues,ontherighthandside(follow- It shows the neutral images, the synthesized VA values
ing columns). It can be observed that the synthesized andtheresultingimages.Itisevidentthatourapproach
images are identity preserving, realistic and vivid. Fig. synthesizes samples that: i) look much more natural
9 refers to the basic expression case; it shows neutral andrealistic,ii)maintainthedegreeofsharpnessofthe
images on the left hand side of (a) and (b) and synthe- originalneutralimage,andiii)combinevisualaccuracy
sized images with basic expressions on the right hand with spatial resolution.
side. Fig. 10 illustrates the VA case for temporal syn- Some further deductions can be made from Fig. 11.
thesis,aswasdescribedinSection4.5.2.Neutralimages StarGAN does not perform well when tested on differ-
are shown on the left hand side, while synthesized face ent in-the-wild and controlled databases that include
sequences with time-varying levels of affect are shown variations in illumination conditions and head poses.
on the right hand side. StarGAN is shown to not reflect detailed illumination;
All these Figs. show that the proposed framework unnaturallightingchangeswereobservedontheresults.
works well, when using images from either in-the-wild, These can be explained because in the original Start-
or controlled databases. This indicates that we can ef- GANpaper[16],itscapabilitytogenerateaffecthasnotGeneratingfacesforaffect analysis 13
Fig.10 VACaseoftemporal(facial)synthesis:onthelefthandsidearetheneutral2Dimagesandontherightthesynthesized
imagesequences
beentestedonin-the-wildfacialanalysis(wereferonly cases, it shows artifacts and in some cases certain lev-
to the case of emotion recognition). In general, Star- els of blurriness. When compared to StarGAN, Gan-
GAN yields more realistic results when it is trained si- imation seems more robust to changing backgrounds
multaneously with multiple datasets annotated for dif- andlightingconditions;thisisduetotheattentionand
ferent tasks. color masks that it contains. Nevertheless, in general,
Additionally,in[16],whenreferringtoemotionrecog- errorsintheattentionmechanismoccurwhentheinput
nition, StarGAN was trained and evaluated on Rad- containsextremeexpressions.Theattentionmechanism
boudFacesDatabase(RaFD)[33]which:i)isverysmall does not seem to sufficiently weight the color trans-
in terms of size (around 4,800 images) and ii) is a lab- formation, causing transparencies. It is interesting to
controlledandposedexpressiondatabase.Lastbutnot notethatontheLeonardoDiCaprioimage,thesynthe-
least, StarGAN has been tested to change only a par- sized image by Ganimation shows open eyes, whereas
ticular aspect of a face among a discrete number of at- on the neutral image (and the one synthesized by our
tributes/emotions defined by the annotation granular- approach) eyes are closed; this illustrates errors of the
ity of the dataset. As can be seen in Fig. 11, StarGAN mask.Forexample,inFig.11,imagesproducedbyGan-
cannot accurately provide realistic results when tested imation in columns 1, 3, 4, 5, 6, 9 show the discussed
in the much broader and more difficult task of valence problems.
and arousal generation (and estimation).
As far as Ganimation is concerned, its results are
also worse than the results of our approach. In most14 D.Kollias
Fig. 11 Generatedresultsbyourapproach, StarGANandGanimation
6.2 Quantitativeevaluationofthefacialaffectsynthesis from StarGAN and Ganimation. It is shown that the
through data augmentation DNNs trained with the proposed data augmentation
methodologyoutperformboththestate-of-the-arttech-
niques and the DNNs trained with StarGAN and Gan-
It is generally accepted that using more training data
imation, in all experiments, validating the effectiveness
- of good quality - leads to better results in supervised
of the proposed facial synthesis approach. Let us first
training.Dataaugmentationincreasestheeffectivesize
explain some notations. In the followings, by reporting
of the training dataset. In this Section we present a
’network nametrainedusingStarGAN’,’network name
dataaugmentationstrategywhichusesthesynthesized
trained using Ganimation’ and ’network name trained
data produced by our approach, as additional data to
usingtheproposedapproach’,werefertonetworkstrai-
trainDNNs,forbothvalence-arousalprediction,aswell
nedwiththespecificdatabase’strainingsetaugmented
as classification into the basic expression categories. In
with data synthesized by StarGAN, Ganimation and
particular,wedescribeexperimentsperformedoneight
the proposed approach, respectively.
databases, presenting the adopted evaluation criteria,
thenetworksweusedandtheobtainedresults.Wealso
report the performances of the networks trained -in a
data augmentation manner- with synthesized imagesGeneratingfacesforaffect analysis 15
6.2.1 LeveragingsynthesizeddatafortrainingDeepNeu- TheSAGRtakesvaluesintherange[0,1],withhigh
ral Networks: Valence-Arousal case values being desirable. It is defined as follows:
N
In this set of experiments we consider four facial affect 1 (cid:88)
SAGR= δ(sign(x ),sign(y )), (9)
databases annotated in terms of valence and arousal, N i i
n=1
theAff-Wild,RECOLA,AffectNetandAFEW-VAdata-
bases. At first, we selected neutral frames from these where N is the total number of samples, x and y are
databases,i.e.,frameswithzerovalenceandarousalval- thegroundtruthandpredictedvaluesrespectively,δ is
ues(humaninspectionwasalsoconductedtomakesure the Kronecker delta function and δ(sign(x),sign(y)) is
that they represented neutral faces). For every frame, defined as:
we synthesized facial affect according to the methodol-
ogy described in Section 4. We start by first describing
 1, x(cid:62)0 and y (cid:62)0
the evaluation criteria used in our experiments. 
δ(sign(x),sign(y))= 1, x(cid:54)0 and y (cid:54)0 (10)
6.2.1.1 The adopted evaluation criteria
0,
otherwise
The main evaluation criterion that we use is the Con-
6.2.1.2 Experiments on Dimensional Affect
cordanceCorrelationCoefficient(CCC)[34],whichhas
been widely used in related Challenges (e.g., [64]); we
Aff-Wild We synthesized 60,135 images from the Aff-
also report the Mean Squared Error (MSE), since this
Wild database and added those images to the training
has been also frequently used in related research.
set of the first Affect-in-the-wild Challenge. The em-
CCC evaluates the agreement between two time se-
ployednetworkarchitecturewastheAffWildNet(VGG-
ries by scaling their correlation coefficient with their
FACE-GRU) described in [29,30].
mean square difference. CCC takes values in the range
Table 2 shows a comparison of the performance of:
[−1,1],where+1indicatesperfectconcordanceand−1
the VGG-FACE-GRU trained using: i) our approach,
denotes perfect discordance. Therefore high values are
ii) StarGAN, and iii) Ganimation; the best performing
desired. CCC is defined as follows:
network,AffWildNet,reportedin[29,30];thewinnerof
ρ = 2s xy , (6) the Aff-Wild Challenge [13] (FATAUVA-Net).
c s2 +s2 +(x¯−y¯)2
x y
where s x and s y are the variances of the ground truth Table 2 Aff-Wild: CCC and MSE evaluation of valence &
and predicted values respectively, x¯ and y¯ are the cor- arousalpredictionsprovidedbytheVGG-FACE-GRUtrained
usingourapproachvsstate-of-the-artnetworksandmethods.
responding mean values and s is the respective co-
xy Valenceandarousalvaluesarein[−1,1].
variance value.
The Mean Squared Error (MSE) provides a simple Networks CCC MSE
Valence Arousal Valence Arousal
comparativemetric,withasmallvaluebeingdesirable. FATAUVA-Net[13] 0.396 0.282 0.123 0.095
MSE is defined as follows: VGG-FACE-GRU 0.556 0.424 0.085 0.060
trainedusingStarGAN
MSE =
1 (cid:88)N
(x −y )2, (7)
trainV eG dG u- sF inA gC GE a-G niR mU
ation
0.576 0.433 0.077 0.057
N i i AffWildNet[29,30] 0.570 0.430 0.080 0.060
i=1 VGG-FACE-GRU
trainedusingthe 0.595 0.445 0.074 0.051
wherexandyarethegroundtruthandpredictedvalues proposedapproach
respectively and N is the total number of samples.
In some cases we also report the Pearson-CC (P-
From Table 2, it can be verified that the network
CC) and the Sign Agreement Metric (SAGR), since
trained on the augmented dataset, with synthesized
they have been reported by respective state-of-the-art
by our approach images, outperformed all other net-
methods.
works. It should be noted that the number of synthe-
The P-CC takes values in the range [-1,1] and high
sized images (around 60K) was small compared to the
values are desired. It is defined as follows:
size of Aff-Wild’s training set (around 1M), the lat-
s
ρ = xy , (8) ter being already sufficient for training the best per-
xy s s
x y forming DNN; consequently, the improvement was not
where s and s are the variances of the ground truth large, about 2%. An interesting observation is that the
x y
andpredictedvaluesrespectivelyands istherespec- network trained using StarGAN displayed worse per-
xy
tive covariance value. formance than AffWildNet. This means that the 6816 D.Kollias
Table 3 RECOLA: CCC evaluation of valence & arousal
predictions provided by the ResNet-GRU trained using the
proposed approach vs other state-of-the-art networks and
methods.
Networks CCC
Valence Arousal
ResNet-GRU[30] 0.462 0.209
ResNet-GRU
0.503 0.245
trainedusingStarGAN
ResNet-GRU
0.486 0.222
trainedusingGanimation
fine-tunedAffWildNet[30] 0.526 0.273
ResNet-GRUtrained
0.554 0.312
usingtheproposedapproach
(a)
(b)
Fig.12 The2DhistogramofvalenceandarousalAff-Wild’s
(a)
test set annotations, along with the MSE per grid area, in
thecaseof(a)AffWildNetand(b)VGG-FACE-GRUtrained
usingtheproposedapproach
landmark points that were passed as additional input
to the AffWildNet helped the network in reaching a
better performance than just adding a small amount
(compared to the training set size) of auxiliary synthe-
sizeddata.TheMSEerrorimprovementonValenceand
Arousalestimationprovidedbytheaugmentedtraining
vs the AffWildNet one, over the different areas of the
VA space, is shown through the 2D histograms pre-
sented in Fig. 12. It can be seen that the improvement (b)
onMSEwasbetterinareasinwhichalargernumberof
Fig.13 The2DhistogramofvalenceandarousalRECOLA’s
newsampleswasgenerated,i.e.,inthepositivevalence testsetannotations,alongwiththeMSEpergridarea,inthe
regions. case of (a) ResNet-GRU and (b) ResNet-GRU trained using
theproposedapproach
RECOLAWegenerated46,455imagesfromRECOLA;
this number corresponds to around 40% of its training
data set size. The employed network architecture was othernetworks.Theabovegainsinperformancecanbe
the ResNet-GRU described in [30]. justifiedbythefactthatthenumberofsynthesizedim-
Table 3 shows a comparison of the performance of: ages (around 46,500) was significant compared to the
theResNet-GRUnetworktrainedusing:i)ourapproach, size of RECOLA’s training set (around 120,000) and
ii)StarGAN,andiii)Ganimation;theAffWildNetfine- that the original training set size was not very suffi-
tuned on the RECOLA, as reported in [30]; a ResNet- cient to train the DNNs. It is worth mentioning that
GRUdirectlytrainedonRECOLA,asreportedin[30]. the GAN based methods have not managed to provide
From Table 3, it can be verified that the network asufficientlyenricheddatasetsothatasimilarboostin
trained using the proposed approach outperformed all theachievedperformancescouldbeobtained.TheMSEGeneratingfacesforaffect analysis 17
Table 4 AffectNet: CCC, P-CC, SAGR and MSE evaluation of valence & arousal predictions provided by the VGG-FACE
trainedusingthe proposedapproach vsstate-of-the-artnetworksandmethods. Valenceandarousal values arein[−1,1].
Networks CCC P-CC SAGR MSE
Valence Arousal Valence Arousal Valence Arousal Valence Arousal
AlexNet[42] 0.60 0.34 0.66 0.54 0.74 0.65 0.14 0.17
theVGG-FACEbaseline 0.50 0.37 0.54 0.48 0.65 0.60 0.19 0.18
VGG-FACE
0.55 0.42 0.58 0.49 0.74 0.73 0.17 0.16
trainedusingStarGAN
VGG-FACEtrained
0.56 0.45 0.59 0.51 0.74 0.74 0.15 0.16
usingGanimation
VGG-FACEtrained
0.62 0.54 0.66 0.55 0.78 0.75 0.14 0.15
usingtheproposedapproach
error improvement on Valence and Arousal estimation
providedbytheaugmentedtrainingvstheoriginalone
(which was 0.045-0.100 vs 0.055-0.160), over the differ-
ent areas of the VA space, is shown through the 2D
histograms presented in Fig. 13. Big reduction of MSE
value was achieved in all covered VA areas.
AffectNet The AffectNet database contains around
450,000manuallyannotatedimagesandaround550,000
automaticallyannotatedimagesforvalence-arousal.We
only used the manually annotated images so as to be
consistent with the state-of-the-art networks that were
also trained using this set. Additionally, the manually (a)
annotated set ensures that the images used by our ap-
proach to synthesize new, are indeed neutral. We cre-
ated 2,476,235 synthesized images from the AffectNet
database, a number that is more than 5 times bigger
than the training data size. The employed network ar-
chitecture was VGG-FACE. For comparison purposes,
we trained the network using the original training data
set(letuscallthisnetwork’theVGG-FACEbaseline’).
Table 4 shows a comparison of the performance of:
the VGG-FACE baseline; the VGG-FACE trained us-
ing:i)ourapproach,ii)StarGAN,andiii)Ganimation;
AlexNet,whichisthebaselinenetworkoftheAffectNet (b)
database [42].
Fig.14 The2DhistogramofvalenceandarousalAffectNet’s
test set annotations, along with the MSE per grid area, in
From Table 4, it can be verified that the network
thecaseof(a)theVGG-FACEbaseline,(b)theVGG-FACE
trained by the proposed methodology outperformed all
trainedusingthe proposedapproach
other networks. This boost in performance has been
large, in all evaluation criteria, compared to the VGG-
FACE baseline network, with spread of this improve- valence estimation the performance gain was also sig-
mentovertheVAspaceshowninFig.14.Theexplana- nificant.
tionarisesfromthelargenumberofsynthesizedimages AFEW-VA. We synthesized 108,864 images from the
that helped the network train and generalize better, AFEW-VA database, a number that is more than 3.5
sinceinthetrainingsetthereexistedalotofrangesthat timesbiggerthanitsoriginalsize.Fortraining,weused
werepoorlyrepresented.Thisisshowninthehistogram the VGG-FACE-GRU architecture described in [30].
of the -manually annotated- training set, for valence Similarly to [31], we used a 5-fold person-independent
andarousal,inFig.15.Ournetworkalsooutperformed cross-validationstrategyandateachfoldweaugmented
the AffectNet’s database baseline. For the arousal esti- the training set with the synthesized images of people
mation, the performance gain was remarkable, mainly appearing only in that set (preserving the person inde-
in CCC and SAGR evaluation criteria, whereas for the pendence).18 D.Kollias
Fig. 16 The 2D histogram of valence and arousal AFEW-
VA’stestsetannotations,alongwiththeMSEpergridarea,
in the case of the VGG-FACE trained using the proposed
Fig.15 The2DhistogramofvalenceandarousalAffectNet’s approach
annotationsforthemanuallyannotatedtrainingset
AffectNet, AFEW and BU-3DFE. Our first step has
Table 5 shows a comparison of the performance of: beentoselectneutralframesfromthesefourdatabases.
theVGG-FACE-GRUnetworktrainedusing:i)ourap- Then, for each frame, we synthesized facial affect ac-
proach,ii)StarGAN,andiii)Ganimation;thebestper- cording to the methodology described in Section 4. We
forming network as reported in [31]. start by first describing the evaluation criteria used in
our experiments.
Table 5 AFEW-VA: P-CC and MSE evaluation of valence
& arousal predictions provided by the VGG-FACE trained 6.2.2.1 The adopted evaluation criteria
usingtheproposedapproachvsstate-of-the-artnetworkand
Oneevaluationcriterionusedintheexperimentsistotal
methods.Valenceand arousal valuesarein[−1,1].
accuracy,definedasthetotalnumberofcorrectpredic-
Networks PearsonCC MSE
tions divided by the total number of samples. Another
Valence Arousal Valence Arousal
bestof[31] 0.407 0.450 0.484 0.247 criterion is the F score, which is a weighted average
1
VGG-FACE
0.512 0.489 0.262 0.097 of the recall (= the ability of the classifier to find all
trainedusingStarGAN
VGG-FACE thepositivesamples)andprecision(=theabilityofthe
0.491 0.453 0.308 0.151
trainedusingGanimation
classifier not to label as positive a sample that is neg-
VGG-FACE-GRU
trainedusing 0.562 0.614 0.226 0.075 ative). The F score reaches its best value at 1 and its
1
theproposedapproach
worstscoreat0.Inourmulti-classproblem,F scoreis
1
the unweighted mean of the F scores of the expression
1
From Table 5, it can be verified that the network classes. F 1 score of each class is defined as:
trained using the proposed approach outperformed all
othernetworks.Greatboostinperformancewasachieved. 2×precision×recall
F = (11)
Thegeneralgaininperformancecanbejustifiedbythe 1 precision+recall
fact that the number of synthesized images (around
Another criterion that is used is the average of the
109,000) is much greater than the number of images
diagonal values of the confusion matrix for the seven
in the dataset (around 30,000), with the latter being
basic expressions.
rather small for effectively training the DNNs. The 2D
One, or more of the above criteria are used in our
histograminFig.16showstheachievedMSEwhenus-
experiments, so as to illustrate the comparison with
ing the proposed approach over the different areas of
other state-of-the-art methods.
the VA space.
6.2.2.2 Experiments on Categorical Affect
6.2.2 LeveragingsynthesizeddatafortrainingDeepNeu-
ral Networks: Basic Expressions case RAF-DB. In this database we only considered the six
basic expression categories, since our approach synthe-
In the following experiments we used the synthesized sizesimagesbasedonthesecategories;weignoredcom-
facestotrainDNNs,forclassificationintothesixbasic pound expressions that were included in the original
expressions,overfourfacialaffectdatabases,RAF-DB, dataset. We created 12,828 synthesized images, whichGeneratingfacesforaffect analysis 19
Table 6 RAF-DB: The diagonal values of the confusion matrix for the seven basic expressions and their average, using the
VGG-FACEtrainedusingthe proposedapproach,aswell asusingotherstate-of-the-artnetworks.
Networks Anger Disgust Fear Happy Sad Surprise Neutral Average
LDA-VGG-FACE [35] 0.661 0.250 0.378 0.731 0.515 0.535 0.472 0.506
mSVM-VGG-FACE[35] 0.685 0.275 0.351 0.853 0.649 0.663 0.599 0.582
theVGG-FACEbaseline 0.691 0.287 0.363 0.853 0.661 0.666 0.600 0.589
mSVM-DLP-CNN[35] 0.716 0.522 0.622 0.928 0.801 0.812 0.803 0.742
VGG-FACEtrained
0.784 0.644 0.622 0.911 0.812 0.845 0.806 0.775
usingtheproposedapproach
(a) (b)
Fig. 17 The confusion matrix of (a) the VGG-FACE baseline and (b) the VGG-FACE trained using the proposed approach
fortheRAF-DBdatabase; 0:Neutral,1:Anger,2: Disgust,3:Fear, 4:Joy,5:Sadness,6:Surprise
are slightly more than the training images (12,271). FACEnetworks,theboostinperformancehasbeensig-
We employed the VGG-FACE network. For compari- nificant.Thiscanbeexplainedbythefactthatthedis-
son purposes, we trained the network using the origi- gustandfearclasses,originally,didnotcontainalotof
nal training dataset (let us call this network the VGG- training images, but after adding the synthesized data,
FACE baseline). they did. This resulted in obtaining a better perfor-
mance in the other classes, as well. Interestingly, there
For further comparison purposes, we used the net-
was also a considerable performance gain in the neu-
works defined in [35]: i) mSVM-VGG-FACE: first the
tral class, that did not contain any synthesized images.
VGG-FACEwastrainedontheRAF-DBdatabaseand
This can be explained by considering the fact that the
thenfeaturesfromthepenultimatefullyconnectedlayer
network trained with the augmented data could distin-
were extracted and fed into a Support Vector Machine
guishbettertheclasses,sinceithadmoresamplesinthe
(SVM)thatperformedtheclassification,ii)LDA-VGG-
two above described categories. Fig. 17 illustrates the
FACE:sameasbefore:LDAwasappliedonthefeatures
wholeconfusionmatrixoftheVGG-FACEbaselineand
which were extracted from the penultimate fully con-
the VGG-FACE trained using the proposed approach,
nected layer and performed the final classification and
giving a better insight on the improved performance
iii)mSVM-DLP-CNN:thedesignedDeepLocalityPre-
and verifying the above explanations.
servingCNNnetwork(werefertheinterestedreaderfor
more details to [35]) was first trained on the RAF-DB
AffectNet. We synthesized 176,425 images from the
database and then a SVM performed the classification
AffectNetdatabase,anumberthatisalmost40%ofits
using the features extracted from the penultimate fully
size.ItshouldbementionedthattheAffectNetdatabase
connected layer of this architecture.
contained the six basic expressions and another one,
Table 6 shows a comparison of the performance of contempt. Our approach synthesized images only for
the above described networks. From Table 6, it can be thebasicexpressions,soforthecontemptclassweonly
verified that the network trained using the proposed kept the original training data. The network architec-
approach outperformed all state-of-the-art nets. When ture that we employed here was VGG-FACE. For com-
compared to the mSVM-VGG-FACE and LDA-VGG- parison purproses, we trained a VGG-FACE network20 D.Kollias
(a) (b)
Fig. 18 The confusion matrix of (a) the VGG-FACE baseline and (b) the VGG-FACE trained using the proposed approach
fortheAffectNet database;0:Neutral,1:Anger, 2:Disgust,3:Fear,4:Joy,5:Sadness,6: Surprise,7:Contempt
using the training set of the AffectNet database (let us 2017 Grand Challenge: i) VGG-FACE-FER: the VGG-
call this network ’the VGG-FACE baseline’). FACE was first fine-tuned on the FER2013 database
Table 7 shows a comparison of the performance of: [26]andthentrainedontheAFEWasdescribedin[28],
i)theVGG-FACEbaseline,ii)theVGG-FACEnetwork ii) VGG-FACE-external: the VGG-FACE was trained
trained using the proposed approach and iii) AlexNet, on the union of the AFEW database and some ex-
the baseline network of the AffectNet database [42]. ternal data as described in [65] and iii) VGG-FACE-
LSTM-external-augmentation: the VGG-FACE-LSTM
was trained on the union of the AFEW database and
Table7 AffectNet:TotalaccuracyandF scoreoftheVGG-
1 some external data; then data augmentation was per-
FACEtrainedusingtheproposedapproachvsstate-of-the-art
formed, as described in [65].
networks
Networks TotalAccuracy F1 score
AlexNet[42] 0.58 0.58
theVGG-FACEbaseline 0.52 0.51 Table 8 AFEW: Total accuracy of the VGG-FACE trained
VGG-FACEtrained usingtheproposedapproachvs state-of-the-art networks
0.60 0.59
usingtheproposedapproach
Networks TotalAccuracy
theVGG-FACEbaseline 0.379
VGG-FACE-external[65] 0.414
From Table 7, it can be verified that the network VGG-FACE-FER[28] 0.483
trained using the proposed approach outperformed all VGG-FACE-LSTM-external-augmentation[65] 0.486
VGG-FACEtrained
the other networks. In more detail, when compared to 0.484
usingtheproposedapproach
the VGG-FACE baseline network, the boost in per-
formance was significant, as also shown in Fig. 18 in
terms of the confusion matrices obtained by the two Table 8 shows a comparison of the performance of
networks. This can be explained by the big size of the the above described networks. From Table 8, one can
added synthesized images. When compared to the Af- see that the VGG-FACE trained using the proposed
fectNet’sbaseline,aslightlyimprovedperformancewas approach performed much better than the same net-
also obtained; this could be higher, if we had synthe- work trained on, either only the AFEW database, or
sized images for the contempt category as well. the union of the AFEW database with some external
AFEW.Wesynthesized56,514imagesfromtheAFEW datawhosesizeintermsofvideoswasthesameasthat
database;thisnumberwasalmost1.4timesbiggerthan of AFEW. The boost in performance can be explained
itstrainingsetsize(41,406).Theemployednetworkar- taking into account the fact that the fear, disgust and
chitecture was VGG-FACE. For comparison purposes, surprise classes contained few data in AFEW and that
wefirsttrainedabaselinenetworkonAFEW’straining our approach augmented the data size of those classes;
set, which we call the VGG-FACE baseline. For fur- intotalthelargenumberofsynthesizedimagesassisted
ther comparisons, we used the following networks de- toimprovetheperformanceofthenetwork.Thisisevi-
veloped by the three winning methods of the EmotiW dentwhencomparingtheconfusionmatrixoftheVGG-Generatingfacesforaffect analysis 21
(a) (b)
Fig. 19 The confusion matrix of (a) the VGG-FACE baseline and (b) the VGG-FACE trained using the proposed approach
fortheAFEWdatabase; 0: Neutral,1:Anger,2: Disgust,3:Fear, 4: Joy,5:Sadness,6:Surprise
FACE baseline to the one of VGG-FACE trained using strategy; in each fold, we augmented the training set
the proposed approach, as can be seen in Fig.19. The with the synthesized images of people appearing only
diagonal of the two confusion matrices indicates that in that set (preserving person independence). The re-
there is an increase in the performance in almost all portedtotalaccuracyofthemodelhasbeentheaverage
basic categories. of the total accuracies over the 10-folds.
Additionally,performanceofournetworkisslightly Atfirst,wetrainedtheabovedescribedVGG-FACE
better than the performance of the same VGG-FACE network(letuscallthisnetwork’theVGG-FACEbase-
network first fine-tuned on the FER2013 database and line’).Next,wetrainedtheabovedescribedVGG-FACE
then trained on the AFEW. FER2013 is a database network,butalsoappliedon-the-flydataaugmentation
of around 35,000 still images and different identities, techniques, such as: small rotations, left and right flip-
annotated with the six basic expressions. In this case, ping, first resize and then random crop to original di-
the network that was first fine-tuned on the FER2013 mensions,randombrightnessandsaturation(letuscall
databasehasseenmorefaces,sincethetasksweresim- this network ’VGG-FACE-augmentation’). Finally, we
ilar. However, still our network provided a slightly bet- trainedtheabovedescribedVGG-FACEnetworkusing
terperformance.Ontheotherhand,ournetworkhada the proposed approach.
slightly worse performance than a VGG-FACE-LSTM
network that was trained with the same external data
mentioned before and was also trained with data aug- Table 9 BU-3DFE: Total accuracy of the VGG-FACE
mentation. Here, it was the LSTM network, which due trainedusingtheproposedapproachvstheVGG-FACEbase-
line and the VGG-FACE trained with on-the-fly data aug-
to the time recurrent nature could better exploit the
mentation.
fact that AFEW consists of video sequences.
Networks Total Accuracy
BU-3DFE. We synthesized 600 images from the BU-
theVGG-FACEbaseline 0.528
3DFE database. This number was almost one fourth
VGG-FACE-augmentation 0.588
of its size (2,500). BU-3DFE is a small database and is VGG-FACEtrained
0.768
notreallysuitedfortrainingDNNs.Thenetworkarchi- usingtheproposedapproach
tecturethatweemployedherewasVGG-FACE,witha
modification in the number of hidden units in the two
first fully connected layers. Since we did not have a lot Table 9 shows a comparison of the performance of
ofdatafortrainingthenetwork,wei)used256and128 thosenetworks.FromTable9,itcanbeverifiedthatthe
units in the two fully connected layers and ii) kept the network trained using the proposed approach greatly
convolutionalweightsfixed,trainingonlythefullycon- outperformed the networks trained without it. This in-
nectedones.Fortrainingthenetworkonthisdatabase, dicatesthattheproposedapproachforsynthesizingim-
we used a 10-fold person-independent cross-validation agescanbeusedfordataaugmentationincasesofsmall22 D.Kollias
amount of DNN training data, being able to signifi- databasetoaugmentitstrainingset.Table12showsthe
cantly improve the obtained performances. databases and its corresponding N values.
Fig. 20 shows the improvement in network perfor-
mance when training using additionally auxiliary data;
6.3 Quantitativeevaluationofthefacialaffectsynthesis the improvement shown per database is the difference
used in testing or training tasks in the performances when training networks with only
thedatabase’strainingsetandwhentrainingthemwith
Resultsintheprevioussectionshowthatthedatagen-
theunionofthetrainingsetandauxiliarydata.Fig.20
eratedusingourapproachprovideimprovementsinnet-
illustrates for each database the difference in network
workperformanceinbothvalence-arousalandbasicex-
performance, when N synthesized data generated by
pressions settings, when used for data augmentation.
our approach (N defined in Table 12) are used as aux-
In the following, we perform further analysis (two dif-
iliary data.
ferent settings) to assess the quality of our generated
The performance measure for Aff-Wild, RECOLA,
data, compared to the data synthesized by StarGAN
AffectNetandAFEW-VAistheaverageofvalenceCCC
andGanimation,focusingonlyonthesynthesizeddata.
andarousalCCC.Theperformancemeasurefortherest
In the first setting, the synthesized data are eval-
databasesdependsonthedatabase.Moredetailsfollow.
uated as a test set, for each database, against mod-
els trained on real data/images. The AffWildNet that Dimensional affect generation
has been trained solely on Aff-Wild’s training set, the
For the Aff-Wild database, we use the VGG-FACE-
ResNet-GRU trained on the RECOLA’s training set
GRUnetwork.Whenaugmentingthedatasetwith30K
and the VGG-FACE baseline trained on AffectNet’s
or less synthesized images, no performance improve-
trainingset(alldescribedinSection6.2.1.2),havebeen
ment is seen, whereas when augmenting it with more
used as emotion regressors and are being evaluated on
than 30K, the performance is increasing, following the
eachofthethreeafore-mentionedsynthesizeddatasets.
increase in the granularity of synthesized data. Adding
FromTable10itisevidentthatthenetworkstrainedon
synthesized data to the training set seems to be bene-
theaforementioneddatabasesdisplayedamuchbetter
ficial for improving the performance and thus the im-
performance (in all databases) when tested on the syn-
provement would be much greater if we added more
thesized data from the proposed approach in compari-
than 60K (if we had more neutral expressions), al-
son to the synthesized data from StarGAN and Gani-
though probably at a given point, a plateau would be
mation.
reached(consideringthelargetrainingsetthatconsists
We further conducted a second setting, using the
of around 1M images).
synthesizeddatatotrainrespectiveDNNmodels.These
For the RECOLA database, we use the ResNet-
models are then evaluated on the real test set of Aff-
GRU network. When augmenting the dataset with up
Wild, RECOLA and AffectNet. Table 11 shows the re-
to 30K synthesized images, there exists small perfor-
sults of this setting. The performance in terms of both
manceimprovement,whereaswhenaugmentingitwith
CCC and MSE is much higher in all databases when
more than 30K, the performance is continuously in-
the networks are trained with the data synthesized by
creasingfollowingtheincreaseinthegranularityofsyn-
theproposedapproach.Thisdifferenceinthecompared
thesized data; this increase is large. This is expected,
performances, alongwith the former results, reflect the
since 120K frames are not sufficient for training a net-
direct value of our generated data in enhancing regres-
work for regression and additionally, 170K frames are
sion performance.
not either.
FortheAffectNetdatabase,weusetheVGG-FACE
6.4 Effect of synthesized data granularity on perfor- network.Afteradding10K synthesizedimages,theper-
mance improvement formance starts to increase. This increase continues to
happen as more data are added until the training set
In this subsection we performed experiments using a has been augmented with 1.5M data. If more data are
subsetofoursynthesizeddataforaugmentingthedata- added,theperformancedoesnotchange,implyingthat
bases. Our aim is to see if all synthesized data are a plateau has been reached. The final performance im-
neededforaugmentingnetworktrainingandmoregen- provement is large.
erally to see how the improvement in classification and FortheAFEW-VAdatabase,weusetheVGG-FACE-
regressionscalewiththegranularityofsynthesizeddata. GRUnetwork.Theimprovementissystematicallyvery
In more detail, for each database used in our experi- significant. When adding more than 30K data, the in-
ments,weusedasubsetofN synthesiseddatafromthis crease in performance is more rapid. The performanceGeneratingfacesforaffect analysis 23
Table 10 CCC and MSE evaluation of valence & arousal predictions provided by the: i) AffWildNet (trained on Aff-Wild),
ii)ResNet-GRU(trainedonRECOLA)andiii)theVGG-FACEbaseline(trainedonAffectNet);thesenetworksaretestedon
thesynthesizedimagesbyStarGAN,Ganimationandourapproach.Eachscoreisshownintheformat:Valencevalue-Arousal
value
Databases Methods EvaluationMetrics Networks
AffWildNet[30] ResNet-GRU[30] theVGG-FACEbaseline
CCC 0.33-0.26
StarGAN - -
MSE 0.21-0.19
Aff-Wild
CCC 0.35-0.28
Ganimation - -
MSE 0.19-0.16
CCC 0.43-0.33
Ours - -
MSE 0.15-0.13
StarGAN CCC - 0.29-0.23 -
RECOLA Ganimation CCC - 0.28-0.22 -
Ours CCC - 0.34-0.33 -
CCC 0.23-0.23
StarGAN - -
MSE 0.34-0.37
AffectNet
CCC 0.26-0.21
Ganimation - -
MSE 0.31-0.38
CCC 0.39-0.31
Ours - -
MSE 0.27-0.28
Table 11 CCCandMSEevaluationofvalence&arousalpredictionsprovidedbythe:i)AffWildNet,ii)ResNet-GRUandiii)
the VGG-FACE baseline; these networks are trained on the synthesized images by StarGAN, Ganimation and our approach;
these networks are evaluated on the Aff-Wild, RECOLA and AffectNet test sets. Each score is shown in the format: Valence
value-Arousalvalue
Databases Methods EvaluationMetrics Networks
AffWildNet ResNet-GRU VGG-FACEbaseline
CCC 0.16-0.13
StarGAN - -
MSE 0.18-0.17
Aff-Wild
CCC 0.17-0.14
Ganimation - -
MSE 0.17-0.15
CCC 0.21-0.20
Ours - -
MSE 0.15-0.12
StarGAN CCC - 0.19-0.10 -
RECOLA Ganimation CCC - 0.17-0.10 -
Ours CCC - 0.23-0.14 -
CCC 0.37-0.29
StarGAN - -
MSE 0.23-0.21
AffectNet
CCC 0.40-0.31
Ganimation - -
MSE 0.20-0.19
CCC 0.45-0.35
Ours - -
MSE 0.18-0.17
Table 12 Databases used in our approach and the different values of N for each one; N denotes a subset of the synthesized
data(perdatabase)bythe proposedapproach
Databases N synthesizeddata
Aff-Wild N ∈{10K,20K,30K,40K,50K,60K}
RECOLA N ∈{10K,20K,30K,40K,50K}
N ∈{10K,20K,30K,40K,50K,60K,70K,80K,90K,100K,110K,300K,600K,
AffectNet(VA)
1M,1.5M,2M,2.5M}
AFEW-VA N ∈{10K,20K,30K,40K,50K,60K,70K,80K,90K,100K,110K}
RAF-DB N ∈{200,400,600,3.5K,6.5K,9.5K,12.5K}
AffectNet(Expressions) N ∈{6.5K,12.5K,25K,38K,56.5K,75K,100K,150K,180K}
AFEW N ∈{3.5K,6.5K,12.5K,25K,38K,56.5K}
BU-3DFE N ∈{200,400,600}24 D.Kollias
(a) (b)
Fig. 20 Improvement in network performance vs amount of synthesized data; criteria: (a) mean/average CCC of VA in
Aff-Wild, RECOLA, AffectNet, AFEW-VA and (b) mean diagonal value of the confusion matrix for RAF-DB, F1 score for
AffectNet,TotalAccuracyforAFEWand BU-3DFE
is expected to continue increasing while more data are General deductions that can be made from Fig. 20:
added, as both the initial training set of around 23K – the smaller the size of the database, the bigger and
frames and the augmented set of around 135K frames faster the increase in performance would be, when
are not large enough to train a DNN for regression. augmenting it with synthesized data from our ap-
proach
Categorical affect generation – the improvement in performance is small if we aug-
ment the training set with few data in proportion to
FortheRAF-DBdatabase,weusetheVGG-FACEnet- its size
work and the performance is measured in terms of the – in dimensionally annotated databases, a plateau is
mean diagonal value of the confusion matrix. The in- reached and no further improvement is seen when a
creaseinperformanceisalmostlinearasmoredataare lot of data (about ≥1.5M in our case) are added
used. The final performance gain is great. RAF-DB is – the performance due to data augmentation does not
a very small database (of size about 12K images) and increase commensurately; in the AffectNet database
therefore if we had more data to add, the performance (mainly in the valence-arousal case) the gain yielded
would further improve. by data augmentation saturates as N increases
In the AffectNet database, we use the VGG-FACE – generally, the performance increase is larger in cat-
network and performance is measured in terms of the egorically annotated databases in comparison to di-
F1score.Increasingtheamountofaddeddataprovides mensionally annotated ones. This is an interesting
a respective increase in the performance. After adding result, since it indicates that synthesizing more data
60K images the performance is increasing at a lower is needed in the latter case, to make the data distri-
rate. It should be mentioned that the results include bution more dense.
erroneousclassificationofthecontemptclass.Ifwesyn-
thesized samples of the contempt class as well, the net-
work would provide a higher performance; but this is
beyond the scope of the current paper. 6.5 Effect of subjects’ age in classification & regression
IntheAFEWdatabase,weusetheVGG-FACEnet- results
work; the performance measure is total accuracy. The
performance is increasing with the addition of more Itisinterestingtoquantitativelyassesstheeffectofage
data.Theperformanceincreaseissignificant.TheAFEW ontheperformanceoftheproposedapproach.However,
databaseisasmalldatabase(ofsizeabout40K images) not all databases contain age information about their
and therefore adding data is expected to increment the subjects. To achieve this, we trained an age estimator
performance. on them. In more detail, we trained a Wide Residual
In the BU-3DFE database, we use the VGG-FACE Network (WideResNet) [72] on the union of IMDB [52]
network; the performance measure is total accuracy. and Adience datasets [21] (so that the training dataset
There is a huge and rapid increase in network perfor- contained an adequate number of images of people un-
mance with the addition of data. This is explained by der the age of 25) and tested it on WIKI [52]. Then we
the very small size of BU-3DFE (around 2K) which applied this estimator on the test sets of the examined
makes it impossible to train a neural network on it. databases.Generatingfacesforaffect analysis 25
Table 13 AgeAnalysisin termsofCCCandMSEforthe dimensionallyannotateddatabases
Databases Ages #TestSamples #SynthesizedSamples Network-Augmented Network
CCC MSE CCC MSE
20-29 29,013 5,301 0.61-0.38 0.101-0.063 0.59-0.37 0.102-0.066
30-39 99,962 23,427 0.66-0.47 0.077-0.054 0.61-0.44 0.088-0.066
Aff-Wild
40-49 44,727 21,831 0.50-0.48 0.048-0.033 0.46-0.44 0.054-0.044
50-59 41,748 9,120 0.58-0.40 0.074-0.054 0.57-0.38 0.075-0.057
total 215,450 59,679 0.60-0.45 0.074-0.051 0.57-0.43 0.080-0.060
30-39 90,000 11,001 0.61-0.38 - 0.60-0.34 -
RECOLA 40-49 15,000 16,188 0.43-0.24 - 0.36-0.19 -
50-59 7,500 11,742 0.49-0.20 - 0.44-0.10 -
total 112,500 38,931 0.55-0.31 - 0.53-0.27 -
0-19 172 118,902 0.67-0.55 0.105-0.156 0.61-0.41 0.127-0.181
20-29 1,179 714,232 0.60-0.53 0.128-0.159 0.51-0.36 0.170-0.193
30-39 1,218 814,588 0.64-0.54 0.139-0.145 0.50-0.39 0.193-0.169
AffectNet
40-49 762 452,504 0.64-0.61 0.149-0.134 0.49-0.44 0.202-0.166
50-59 569 229,938 0.58-0.53 0.161-0.149 0.47-0.34 0.216-0.181
60-89 600 146,091 0.62-0.44 0.145-0.167 0.51-0.29 0.200-0.195
total 4,500 2,476,235 0.62-0.54 0.141-0.150 0.50-0.37 0.190-0.180
20-29 766 17,466 0.46-0.60 0.192-0.084 - -
30-39 1,990 36,388 0.51-0.62 0.254-0.080 - -
AFEW-VA 40-49 1,558 34,906 0.59-0.47 0.211-0.076 - -
50-59 946 15,102 0.74-0.85 0.215-0.045 - -
60-79 396 4,102 0.63-0.45 0.236-0.100 - -
total 5,646 108,864 0.57-0.59 0.226-0.075 - -
Table 13 shows, for each dimensionally annotated (<20 years old) in RAF-DB and AffectNet, each con-
database (Aff-Wild, RECOLA, AffectNet and AFEW- taining more than 150 subjects, or elderly (e.g., 70-79
VA), the estimated age groups (we split the age values years old) in AffectNet, also containing more than 150
intoappropriategroupssothateachgroupcontaineda subjects. In the former case, the F1 value improved
significantamountofsamples),thenumberoftestsam- from about 0.45 to 0.6; the F1 values over all cate-
plesthatarewithintheagegroups,thenumberofsyn- gories improved from about 0.51 to 0.66. Although the
thesized by our approach samples for each age group, F1 values in the very young category were lower than
different evaluation metrics (CCC and MSE) for each the mean F1 values over all ages, the improvement in
age group in two cases: when a network trained only both cases was similar. A similar observation can be
with the training set of each database was used (de- made in the latter case, of elderly persons, with the F1
notedas’Network’inTable13)andwhenthesamenet- valueinthecategorybeingimprovedfromabout0.4to
workwastrainedwiththetrainingsetaugmentedwith 0.47. Although these values were lower than the total
our approach’s synthesized data (denoted as ’Network- F1 values over all ages, which were 0.51 and 0.59 re-
Augmented’inTable13).ForAff-WildandAFEW-VA, spectively, the improvement in these cases was similar
the VGG-FACE-GRU network was used, for RECOLA as well. This verifies the above-mentioned observation
the ResNet-GRU and for AffectNet the VGG-FACE. that the proposed approach for data augmentation can
Table 14 is similar to Table 13 with the difference bealsobeneficialincaseswherethenumberofavailable
beingthatitreferstocategoricallyannotateddatabases samples is rather small.
(RAF-DB, AffectNet, AFEW and BU-3DFE). In this
case, the evaluation metrics are the F1 score for RAF-
DB and AffectNet, and the total accuracy for AFEW 7 Conclusions and Future Work
andBU-3DFE.The’VGG-FACE-Augmented’refersto
the case in which the VGG-FACE network is trained A novel approach to generate facial affect in faces has
on the union of training set of each database and data beenpresentedinthispaper.Itleveragesadimensional
synthesized by our approach. emotionmodelintermsofvalenceandarousalorthesix
By observing the two Tables (13 and 14), it is seen basic expressions, and a large scale 4D face database,
that augmenting the training dataset with the images the 4DFAB. We performed dimensional annotation of
generatedbyourapproachisbeneficialinallagegroups, the 4DFAB and used the facial images with their re-
bothforregressionandclassification.Itwouldbeinter- spective annotations to generate mean faces on a dis-
esting to focus on specific groups, such as very young cretized 2-D affect space.26 D.Kollias
Table 14 AgeAnalysisforthecategoricallyannotateddatabases;criterionforRAF-DB&AffectNetisF1score,forAFEW
&BU-3DFEis total accuracy;AFEWtestsamplesrefer to:numberofvideos (frames)
Databases Ages #TestSamples #SynthesizedSamples VGG-FACE-Augmented VGG-FACE
Performance Metric Performance Metric
10-19 168 210 0.631 0.446
20-29 911 2,250 0.813 0.556
30-39 998 4,320 0.739 0.498
RAF-DB 40-49 516 3,606 0.744 0.511
50-59 258 1,776 0.709 0.440
60-69 149 552 0.657 0.550
70-79 68 128 0.904 0.635
total 3,068 12,828 0.738 0.505
0-19 152 12,516 0.593 0.453
20-29 882 45,182 0.584 0.477
30-39 962 55,513 0.593 0.518
40-49 594 27,632 0.586 0.532
AffectNet
50-59 431 20,204 0.648 0.606
60-69 289 11,178 0.564 0.498
70-79 161 3,582 0.466 0.398
80-89 29 618 0.448 0.410
total 3,500 176,425 0.590 0.510
20-29 29(1,536) 6,474 0.379 0.241
30-39 156(8,568) 22,518 0.455 0.333
AFEW 40-49 132(7,803) 17,934 0.553 0.439
50-59 57(3,202) 7,482 0.474 0.456
60-79 16(764) 2,106 0.438 0.313
total 390(21,873) 56,514 0.484 0.379
20-29 115 192 0.800 0.600
30-39 100 240 0.820 0.570
BU-3DFE 40-49 100 120 0.800 0.550
50-59 100 30 0.790 0.490
60-70 85 18 0.600 0.400
total 500 600 0.768 0.528
Amethodologyhasbeenproposedusingthesemean ofthesameDNNswithdataaugmentationprovidedby
facestosynthesizefaceswithaffect,bothcategoricalor the StarGAN and Ganimation networks.
dimensional, static or dynamic. Using a given neutral In our future work we will extend this approach to
image and the desired affect, which can be a Valence synthesize,notonlydimensional,aswellascategorical,
Arousal pair of values, a path in the 2D VA space, or affectinfaces,butalsoFacialActionUnits.Inthisway
oneofthebasicexpressioncategories,theproposedap- a Global Local synthesis of facial affect will be possi-
proach performs face detection and landmark localiza- ble, through a unified modeling of global dimensional
tion on the input neutral image, fits a 3D Morphable emotion and local action unit based facial expression
Modelontheresultingimage,deformsthereconstructed synthesis. Another future direction will be to generate
face,addstheinputaffectandblendsthenewfacewith faces of different genders and human races.
the given affect into the original image.
Acknowledgements The work of Stefanos Zafeiriou was
Anextensiveexperimentalstudyhasbeenconducted,
partiallyfundedbytheFiDiProprogramofTekeswithproject
providingbothqualitativeandquantitativeevaluations number1849/31/2015.TheworksofDimitriosKollias,aswell
of the proposed approach. The qualitative results show as Evangelos Ververas were funded by Teaching Fellowships
ofImperialCollegeLondon.WealsothanktheNVIDIACor-
theachievedhigherqualityofthesynthesizeddatacom-
porationfordonatingaTitanXGPU.Additionally,wewould
paredtoGAN-generatedfacialaffect.Thequantitative
like to thank the reviewers for their valuable comments that
results are based on using the synthesized facial im- helpedustoimprovethispaper.
ages for data augmentation and training of Deep Neu-
ral Networks over eight databases, annotated with ei-
References
therdimensionalorcategoricalaffectlabels.Ithasbeen
shown that, over all databases, the achieved perfor-
1. Abbasnejad, I., Sridharan, S., Nguyen, D., Denman, S.,
mance is much higher than i) the performance of the
Fookes, C., Lucey, S.: Using synthetic data to improve
respectivestate-of-the-artmethods,ii)theperformance facialexpressionanalysiswith3dconvolutionalnetworks.Generatingfacesforaffect analysis 27
In:ProceedingsoftheIEEEInternationalConferenceon 16. Choi, Y., Choi, M., Kim, M., Ha, J.W., Kim, S., Choo,
ComputerVision,pp. 1609–1618(2017) J.: Stargan: Unified generative adversarial networks for
2. Alabort-i-Medina, J., Antonakos, E., Booth, J., Snape, multi-domain image-to-image translation. In: Proceed-
P., Zafeiriou, S.: Menpo: A comprehensive platform for ings of the IEEE Conference on Computer Vision and
parametricimagealignmentandvisualdeformablemod- Pattern Recognition,pp.8789–8797(2018)
els.In:ProceedingsoftheACMInternationalConference 17. Cosker, D., Krumhuber, E., Hilton, A.: A facs valid 3d
on Multimedia, MM ’14, pp. 679–682. ACM, New York, dynamicactionunitdatabasewithapplicationsto3ddy-
NY, USA (2014). DOI 10.1145/2647868.2654890. URL namicmorphablefacialmodeling. In:2011International
http://doi.acm.org/10.1145/2647868.2654890 Conference on Computer Vision, pp. 2296–2303. IEEE
3. Amberg, B., Romdhani, S., Vetter, T.: Optimal step (2011)
nonrigid icp algorithms for surface registration. In: 18. Deng, J., Zhou, Y., Cheng, S., Zaferiou, S.: Cascade
2007IEEEConferenceonComputerVisionandPattern multi-viewhourglassmodelforrobust3dfacealignment.
Recognition,pp.1–8. IEEE(2007) pp.399–403(2018). DOI10.1109/FG.2018.00064
4. Antoniou, A., Storkey, A., Edwards, H.: Data augmen- 19. Dhall, A., Goecke, R., Ghosh, S., Joshi, J., Hoey, J.,
tation generative adversarial networks. arXiv preprint Gedeon, T.: From individual to group-level emotion
arXiv:1711.04340(2017) recognition: Emotiw 5.0. In: Proceedings of the 19th
5. Averbuch-Elor,H.,Cohen-Or,D.,Kopf,J.,Cohen,M.F.: ACM International Conference on Multimodal Interac-
Bringingportraitstolife.ACMTransactionsonGraphics tion,pp.524–528.ACM(2017)
(TOG) 36(6),196(2017) 20. Ding, H., Sricharan, K., Chellappa, R.: Exprgan: Facial
expression editing with controllable expression intensity.
6. Bach, F., Jenatton, R., Mairal, J., Obozinski, G.: Opti-
In:Thirty-SecondAAAIConferenceonArtificialIntelli-
mization with sparsity-inducing penalties. Foundations
gence(2018)
and Trends in Machine Learning 4(1), 1–106 (2012).
21. Eidinger,E.,Enbar,R.,Hassner,T.:Ageandgenderes-
DOI10.1561/2200000015. URLhttp://dx.doi.org/10.
timationofunfilteredfaces. IEEETransactionsonInfor-
1561/2200000015
mationForensicsandSecurity 9(12),2170–2179(2014)
7. Blanz,V.,Basso,C.,Poggio,T.,Vetter,T.:Reanimating
22. Fried, O., Shechtman, E., Goldman, D.B., Finkelstein,
facesinimagesandvideo. In:Computergraphicsforum,
A.: Perspective-aware manipulation of portrait photos.
vol.22,pp.641–650.WileyOnline Library (2003)
ACMTransactionsonGraphics(TOG)35(4),128(2016)
8. Booth, J., Antonakos, E., Ploumpis, S., Trigeorgis, G.,
23. Garrido,P.,Valgaerts,L.,Rehmsen,O.,Thormahlen,T.,
Panagakis, Y., Zafeiriou, S.: 3d face morphable models
Perez,P.,Theobalt,C.:Automaticfacereenactment. In:
”in-the-wild”. In:IEEEConferenceonComputerVision
ProceedingsoftheIEEEConferenceonComputerVision
and Pattern Recognition (CVPR) (2017). URL https:
andPattern Recognition,pp.4217–4224(2014)
//arxiv.org/abs/1701.05360
24. Genova, K., Cole, F., Maschinot, A., Sarna, A., Vla-
9. Booth, J., Roussos, A., Ponniah, A., Dunaway, D.,
sic, D., Freeman, W.T.: Unsupervised training for 3d
Zafeiriou, S.: Large scale 3d morphable models. Inter-
morphable model regression. In: The IEEE Conference
national Journal of Computer Vision 126(2-4), 233–254
on Computer Vision and Pattern Recognition (CVPR)
(2018)
(2018)
10. Booth,J.,Zafeiriou,S.:Optimaluvspacesforfacialmor-
25. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
phable model construction. In: 2014 IEEE International
Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.:
Conference on Image Processing (ICIP), pp. 4672–4676.
Generative adversarial nets. In: Advances in neural in-
IEEE(2014)
formationprocessingsystems,pp.2672–2680(2014)
11. Cao, C., Hou, Q., Zhou, K.: Displaced dynamic expres-
26. Goodfellow, I.J., Erhan, D., Carrier, P.L., Courville, A.,
sion regression for real-time facial tracking and anima-
Mirza,M.,Hamner,B.,Cukierski,W.,Tang,Y.,Thaler,
tion. ACM Transactions on graphics (TOG) 33(4), 43
D., Lee, D.H., et al.: Challenges in representation learn-
(2014)
ing: A report on three machine learning contests. In:
12. Cao, Q., Shen, L., Xie, W., Parkhi, O.M., Zisserman, InternationalConferenceonNeuralInformationProcess-
A.:Vggface2:Adatasetforrecognisingfacesacrosspose ing,pp.117–124.Springer(2013)
andage. In:AutomaticFace&GestureRecognition(FG 27. Gross,R.,Matthews,I.,Cohn,J.,Kanade,T.,Baker,S.:
2018), 2018 13th IEEE International Conference on, pp. Multi-pie. Image and Vision Computing 28(5), 807–813
67–74.IEEE(2018) (2010)
13. Chang, W.Y., Hsu, S.H., Chien, J.H.: Fatauva-net : An 28. Knyazev,B.,Shvetsov,R.,Efremova,N.,Kuharenko,A.:
integrated deep learning framework for facial attribute Convolutional neural networks pretrained on large face
recognition, action unit (au) detection, and valence- recognitiondatasetsforemotionclassificationfromvideo.
arousalestimation. In:ProceedingsoftheIEEEConfer- arXivpreprintarXiv:1711.04598(2017)
enceonComputerVisionandPatternRecognitionWork- 29. Kollias, D., Nicolaou, M.A., Kotsia, I., Zhao, G.,
shop(2017) Zafeiriou, S.: Recognition of affect in the wild using
14. Cheng, S., Kotsia, I., Pantic, M., Zafeiriou, S.: 4dfab: deep neural networks. In: Computer Vision and Pattern
A large scale 4d database for facial expression analysis Recognition Workshops (CVPRW), 2017 IEEE Confer-
andbiometricapplications. In:2018IEEEConferenceon enceon,pp. 1972–1979.IEEE (2017)
ComputerVisionandPatternRecognition(CVPR2018). 30. Kollias, D., Tzirakis, P., Nicolaou, M.A., Papaioannou,
SaltLakeCity,Utah,US(2018) A., Zhao, G., Schuller, B., Kotsia, I., Zafeiriou, S.: Deep
15. Chew,S.W.,Lucey,P.,Lucey,S.,Saragih,J.,Cohn,J.F., affectpredictionin-the-wild:Aff-wilddatabaseandchal-
Matthews, I., Sridharan, S.: In the pursuit of effective lenge, deep architectures, and beyond. International
affective computing: The relationship between features JournalofComputerVision 127(6-7),907–929(2019)
and registration. IEEE Transactions on Systems, Man, 31. Kossaifi,J.,Tzimiropoulos,G.,Todorovic,S.,Pantic,M.:
andCybernetics,PartB(Cybernetics)42(4),1006–1016 Afew-va database for valence and arousal estimation in-
(2012) the-wild. ImageandVision Computing(2017)28 D.Kollias
32. Kuipers,J.B.,etal.:Quaternionsandrotationsequences, 51. Ringeval,F.,Sonderegger,A.,Sauer,J.,Lalanne,D.:In-
vol.66. PrincetonuniversitypressPrinceton(1999) troducingtherecolamultimodalcorpusofremotecollab-
33. Langner, O., Dotsch, R., Bijlstra, G., Wigboldus, D.H., orativeandaffectiveinteractions.In:AutomaticFaceand
Hawk,S.T.,VanKnippenberg,A.:Presentationandval- GestureRecognition(FG),201310thIEEEInternational
idation of the radboud faces database. Cognition and ConferenceandWorkshops on,pp.1–8.IEEE(2013)
emotion24(8),1377–1388(2010) 52. Rothe,R.,Timofte,R.,VanGool,L.:Dex:Deepexpecta-
34. Lawrence, I., Lin, K.: A concordance correlation coeffi- tionofapparentagefromasingleimage. In:Proceedings
cienttoevaluatereproducibility. Biometricspp.255–268 of the IEEE International Conference on Computer Vi-
(1989) sionWorkshops, pp.10–15(2015)
35. Li,S.,Deng,W.,Du,J.:Reliablecrowdsourcinganddeep
53. Russell, J.A.: Evidence of convergent validity on the di-
locality-preserving learning for expression recognition in
mensionsofaffect. Journalofpersonalityandsocialpsy-
the wild. In: Computer Vision and Pattern Recognition
chology 36(10),1152(1978)
(CVPR),2017IEEEConferenceon,pp.2584–2593.IEEE
54. Savran, A., Alyu¨z, N., Dibeklio˘glu, H., C¸eliktutan, O.,
(2017)
G¨okberk,B.,Sankur,B.,Akarun,L.:Bosphorusdatabase
36. Liu, X., Mao, T., Xia, S., Yu, Y., Wang, Z.: Facial an-
for3dfaceanalysis. In:EuropeanWorkshoponBiomet-
imation by optimized blendshapes from motion capture
ricsandIdentityManagement,pp.47–56.Springer(2008)
data. ComputerAnimationandVirtualWorlds19(3-4),
235–245(2008) 55. Shang, F., Liu, Y., Cheng, J., Cheng, H.: Robust prin-
37. Ma, L., Deng, Z.: Real-time facial expression transfor- cipal component analysis with missing data. In: Pro-
mationformonocularrgbvideo. In:ComputerGraphics ceedings of the 23rd ACM International Conference on
Forum,vol.38,pp.470–481.WileyOnlineLibrary(2019) ConferenceonInformationandKnowledgeManagement,
38. Maimon,O.,Rokach,L.:Dataminingandknowledgedis- CIKM ’14, pp. 1149–1158. ACM, New York, NY, USA
coveryhandbook(2005) (2014). DOI 10.1145/2661829.2662083. URL http:
39. Alabort-i Medina, J., Zafeiriou, S.: A unified framework //doi.acm.org/10.1145/2661829.2662083
forcompositionalfittingofactiveappearancemodels.In- 56. Sohn, K., Lee, H., Yan, X.: Learning structured output
ternational Journal of Computer Vision 121(1), 26–64 representationusingdeepconditionalgenerativemodels.
(2017) In:AdvancesinNeuralInformationProcessingSystems,
40. Mirza,M.,Osindero,S.:Conditionalgenerativeadversar- pp.3483–3491(2015)
ialnets. arXiv preprintarXiv:1411.1784(2014)
57. Song, L., Lu, Z., He, R., Sun, Z., Tan, T.: Geometry
41. Mohammed, U., Prince, S.J., Kautz, J.: Visio-lization:
guided adversarial facial expression synthesis. In: 2018
generating novel facial images. ACM Transactions on
ACMMultimediaConferenceonMultimediaConference,
Graphics(TOG) 28(3),57(2009)
pp.627–635.ACM(2018)
42. Mollahosseini, A., Hasani, B., Mahoor, M.H.: Affectnet:
58. Sun, Y., Chen, Y., Wang, X., Tang, X.: Deep learning
A database for facial expression, valence, and arousal
facerepresentationbyjointidentification-verification.In:
computing in the wild. arXiv preprint arXiv:1708.03985
Advances in neural information processing systems, pp.
(2017)
1988–1996(2014)
43. Neumann, T., Varanasi, K., Wenger, S., Wacker, M.,
Magnor, M., Theobalt, C.: Sparse localized deformation 59. Susskind,J.M.,Hinton,G.E.,Movellan,J.R.,Anderson,
components. ACM Transactions on Graphics (TOG) A.K.:Generatingfacialexpressionswithdeepbeliefnets.
32(6),179(2013) In:AffectiveComputing.IntechOpen(2008)
44. Parkhi, O.M., Vedaldi, A., Zisserman, A.: Deep face 60. Taigman, Y., Yang, M., Ranzato, M., Wolf, L.: Deep-
recognition. In:BMVC,vol.1,p.6(2015) face:Closingthegaptohuman-levelperformanceinface
45. Paysan,P.,Knothe,R.,Amberg,B.,Romdhani,S.,Vet- verification. In: Proceedings of the IEEE Conference
ter,T.:A3dfacemodelforposeandilluminationinvari- onComputerVisionandPatternRecognition,pp.1701–
ant face recognition. In: 2009 Sixth IEEE International 1708(2014)
ConferenceonAdvancedVideoandSignalBasedSurveil- 61. Thies, J., Zollhofer, M., Stamminger, M., Theobalt, C.,
lance,pp.296–301.Ieee(2009) Nießner,M.:Face2face:Real-timefacecaptureandreen-
46. P´erez, P., Gangnet, M., Blake, A.: Poisson image edit- actmentofrgbvideos. In:ProceedingsoftheIEEECon-
ing.In:ACMSIGGRAPH2003Papers,SIGGRAPH’03, ferenceonComputerVisionandPatternRecognition,pp.
pp. 313–318. ACM, New York, NY, USA (2003). DOI 2387–2395(2016)
10.1145/1201775.882269. URLhttp://doi.acm.org/10.
62. Thies, J., Zollho¨fer, M., Theobalt, C., Stamminger, M.,
1145/1201775.882269
Nießner, M.: Headon: real-time reenactment of human
47. Pham, H.X., Wang, Y., Pavlovic, V.: Generative ad-
portrait videos. ACM Transactions on Graphics (TOG)
versarial talking head: Bringing portraits to life with
37(4),164(2018)
a weakly supervised neural network. arXiv preprint
63. Thomaz,C.E.,Giraldi,G.A.:Anewrankingmethodfor
arXiv:1803.07716(2018)
principalcomponentsanalysisanditsapplicationtoface
48. Pumarola, A., Agudo, A., Martinez, A.M., Sanfeliu, A.,
imageanalysis.ImageandVisionComputing28(6),902–
Moreno-Noguer,F.:Ganimation:Anatomically-awarefa-
913(2010)
cialanimationfromasingleimage.In:Proceedingsofthe
European Conference on Computer Vision (ECCV), pp. 64. Valstar, M., Gratch, J., Schuller, B., Ringeval, F.,
818–833(2018) Lalanne, D., Torres Torres, M., Scherer, S., Stratou, G.,
49. Qiao, F., Yao, N., Jiao, Z., Li, Z., Chen, H., Wang, H.: Cowie, R., Pantic, M.: Avec 2016: Depression, mood,
Geometry-contrastive gan for facial expression transfer. and emotion recognition workshop and challenge. In:
arXivpreprintarXiv:1802.01822(2018) Proceedings of the 6th International Workshop on Au-
50. Reed, S., Sohn, K., Zhang, Y., Lee, H.: Learning to dis- dio/VisualEmotionChallenge, pp.3–10.ACM(2016)
entangle factors of variation with manifold interaction. 65. Vielzeuf,V.,Pateux,S.,Jurie,F.:Temporalmultimodal
In: International Conference on Machine Learning, pp. fusionforvideoemotionclassificationinthewild. arXiv
1431–1439(2014) preprint arXiv:1709.07200(2017)Generatingfacesforaffect analysis 29
66. Wheeler, M.D., Ikeuchi, K.: Iterative estimation of ro-
tation and translation using the quaternion. Carnegie-
Mellon University. Department of Computer Science
(1995)
67. Whissell, C.M.: The dictionary of affect in language.
In: The measurement of emotions, pp. 113–131. Elsevier
(1989)
68. Wright, S.J., Nowak, R.D., Figueiredo, M.A.T.: Sparse
reconstructionbyseparableapproximation. IEEETrans-
actions on Signal Processing 57(7), 2479–2493 (2009).
DOI10.1109/TSP.2009.2016892
69. Wu, W., Zhang, Y., Li, C., Qian, C., Change Loy, C.:
Reenactgan: Learning to reenact faces via boundary
transfer. In: Proceedings of the European Conference
onComputerVision (ECCV),pp.603–619(2018)
70. Yin, L., Wei, X., Sun, Y., Wang, J., Rosato, M.J.: A 3d
facial expression database for facial behavior research.
In: Automatic face and gesture recognition, 2006. FGR
2006.7thinternationalconferenceon,pp.211–216.IEEE
(2006)
71. Zafeiriou, S., Kollias, D., Nicolaou, M.A., Papaioan-
nou, A., Zhao, G., Kotsia, I.: Aff-wild: Valence and
arousal in-the-wildchallenge. In: 2017 IEEE Conference
onComputerVisionandPatternRecognitionWorkshops
(CVPRW),pp.1980–1987.IEEE(2017)
72. Zagoruyko, S., Komodakis, N.: Wide residual networks.
arXivpreprintarXiv:1605.07146(2016)
73. Zhang, K., Zhang, Z., Li, Z., Qiao, Y.: Joint face de-
tectionandalignmentusingmultitaskcascadedconvolu-
tionalnetworks. IEEESignalProcessingLetters23(10),
1499–1503(2016). DOI10.1109/LSP.2016.2603342
74. Zhou,Y.,Shi,B.E.:Photorealisticfacialexpressionsyn-
thesis by the conditional difference adversarial autoen-
coder. In: 2017 Seventh International Conference on Af-
fectiveComputingandIntelligentInteraction(ACII),pp.
370–376.IEEE(2017)
75. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired
image-to-image translation using cycle-consistent adver-
sarial networks. In: Proceedings of the IEEE inter-
national conference on computer vision, pp. 2223–2232
(2017)
76. Zhu,X.,Liu,Y.,Li,J.,Wan,T.,Qin,Z.:Emotionclassi-
fication with data augmentation using generative adver-
sarial networks. In: Pacific-Asia Conference on Knowl-
edge Discovery and Data Mining, pp. 349–360. Springer
(2018)"
64,66,Deep neural networks with relativity learning for facial expression recognition,"['Y Guo', 'D Tao', 'J Yu', 'H Xiong', 'Y Li']",2016,127,Acted Facial Expressions In The Wild,"deep learning, neural network",Here we present a deep learning method termed Deep Neural Networks with Relativity   The SFEW 2.0 dataset was created from Acted Facial Expressions in the Wild (AFEW) [3] using,No DOI,2016 IEEE International …,https://ieeexplore.ieee.org/document/7574736,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
65,67,Deep spatial-temporal feature fusion for facial expression recognition in static images,"['N Sun', 'Q Li', 'R Huan', 'J Liu', 'G Han']",2019,116,MMI Facial Expression,"deep learning, facial expression recognition, neural network",MMI database holds 2885 videos and over 500 images of 88 subjects displaying various  facial expressions  output of the channel to recognize the facial expression. For MDSTFN with,No DOI,Pattern Recognition Letters,https://www.sciencedirect.com/science/article/pii/S0167865517303902,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
66,68,Deep-emotion: Facial expression recognition using attentional convolutional network,"['S Minaee', 'M Minaei', 'A Abdolrashidi']",2021,664,"Affective Faces Database, Extended Cohn-Kanade, Japanese Female Facial Expression, Toronto Face Database","FER, deep learning, facial expression recognition","FER dataset, and the images in the second and fourth rows belong to the extended Cohn-Kanade   images in the second and fourth rows belong to the extended Cohn-Kanade dataset.",No DOI,Sensors,https://www.mdpi.com/1424-8220/21/9/3046,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,mdpi.com,
67,69,Deeply learning deformable facial action parts model for dynamic expression analysis,"['M Liu', 'S Li', 'S Shan', 'R Wang', 'X Chen']",2015,389,MMI Facial Expression,CNN,"evaluated on two posed expression datasets, CK+, MMI, and a  is how to represent different  facial expressions. In the past  is applying CNN or 3D CNN directly to expression analysis,",No DOI,Computer Vision--ACCV 2014: 12th …,https://link.springer.com/chapter/10.1007/978-3-319-16817-3_10,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,springer.com,
68,70,Development of a real-time emotion recognition system using facial expressions and EEG based on machine learning and deep neural network methods,"['A Hassouneh', 'AM Mutawa', 'M Murugappan']",2020,239,Affective Faces Database,machine learning,A convolutional neural network was used in our system to obtain improved facial emotion  detection as it is applied to other computer fields such as face recognition [25] and object,No DOI,Informatics in Medicine …,https://www.sciencedirect.com/science/article/pii/S235291482030201X,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
69,71,Dexpression: Deep convolutional neural network for expression recognition,"['P Burkert', 'F Trier', 'MZ Afzal', 'A Dengel']",2015,192,"Extended Cohn-Kanade, MMI Facial Expression","CNN, classification, deep learning, facial expression recognition, neural network",of different image classification approaches submitted by  evaluated on the Extended  Cohn-Kanade Dataset (Section 4.2 [12] have created the Extended CohnKanade dataset. This,No DOI,arXiv preprint arXiv …,https://arxiv.org/abs/1509.05371,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"1
DeXpression: Deep Convolutional Neural
Network for Expression Recognition
Peter Burkert∗‡, Felix Trier∗‡, Muhammad Zeshan Afzal†‡,
Andreas Dengel†‡ and Marcus Liwicki‡
†German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany
‡University of Kaiserslautern, Gottlieb-Daimler-Str., Kaiserslautern 67663, Germany
p burkert11@cs.uni-kl.de, f trier10@cs.uni-kl.de, afzal@iupr.com, andreas.dengel@dfki.de,
liwicki@dfki.uni-kl.de
(cid:70)
Abstract—Weproposeaconvolutionalneuralnetwork(CNN)architec-
tureforfacialexpressionrecognition.Theproposedarchitectureisinde-
pendentofanyhand-craftedfeatureextractionandperformsbetterthan
the earlier proposed convolutional neural network based approaches.
We visualize the automatically extracted features which have been
learnedbythenetworkinordertoprovideabetterunderstanding.The
standarddatasets,i.e.ExtendedCohn-Kanade(CKP)andMMIFacial
Expression Databse are used for the quantitative evaluation. On the
CKP set the current state of the art approach, using CNNs, achieves
anaccuracyof99.2%.FortheMMIdataset,currentlythebestaccuracy
Fig. 1: Example images from the MMI (top) and CKP
foremotionrecognitionis93.33%.Theproposedarchitectureachieves
99.6%forCKPand98.63%forMMI,thereforeperformingbetterthanthe (bottom). The emotions from left to right are: Anger,
stateoftheartusingCNNs.Automaticfacialexpressionrecognitionhas Sadness, Disgust, Happiness, Fear, Surprise. The emotion
abroadspectrumofapplicationssuchashuman-computerinteraction Contempt of the CKP set is not displayed.
and safety systems. This is due to the fact that non-verbal cues are
important forms of communication and play a pivotal role in interper-
sonal communication. The performance of the proposed architecture
endorsestheefficacyandreliableusageoftheproposedworkforreal
Many methods rely on extraction of the facial region.
worldapplications.
This can be realized through manual inference [2] or
an automatic detection approach [1]. Methods often
involve the Facial Action Coding System (FACS) which
1 INTRODUCTION
describes the facial expression using Action Units (AU).
An Action Unit is a facial action like ”raising the Inner
Humans use different forms of communications such
Brow”. Multiple activations of AUs describe the facial
as speech, hand gestures and emotions. Being able to
expression [3]. Being able to correctly detect AUs is a
understand one’s emotions and the encoded feelings
helpful step, since it allows making a statement about
is an important factor for an appropriate and correct
the activation level of the corresponding emotion.
understanding.
Handcrafted facial landmarks can be used such as done
With the ongoing research in the field of robotics, es-
by Kotsia et al. [2]. Detecting such landmarks can be
peciallyinthefieldofhumanoidrobots,itbecomesinter-
hard, as the distance between them differs depending
estingtointegratethesecapabilitiesintomachinesallow-
on the person [4]. Not only AUs can be used to detect
ing for a more diverse and natural way of communica-
emotions, but also texture. When a face shows an
tion. One example is the Software called EmotiChat [1].
emotion the structure changes and different filters can
This is a chat application with emotion recognition. The
be applied to detect this [4].
user is monitored and whenever an emotion is detected
(smile, etc.), an emoticon is inserted into the chat win-
The presented approach uses Artificial Neural
dow. Besides Human Computer Interaction other fields
Networks (ANN). ANNs differ, as they are trained
like surveillance or driver safety could also profit from
on the data with less need for manual interference.
it.Beingabletodetectthemoodofthedrivercouldhelp
Convolutional Neural Networks are a special kind of
todetectthelevelofattention,sothatautomaticsystems
can adapt. *F.TrierandP.Burkertcontributedequallytothiswork.
6102
guA
71
]VC.sc[
2v17350.9051:viXra2
ANN and have been shown to work well as feature e.g. the eyes or lips are detected and points of interest
extractor when using images as input [5] and are are marked. From the patches which have the most
real-time capable. This allows for the usage of the raw variance between two images, features are extracted.
input images without any pre- or postprocessing. The dimensionality of the features is reduced and then
GoogleNet[6]isadeepneuralnetworkarchitecturethat given to a Support Vector Machine (SVM). To evaluate
reliesonCNNs.IthasbeenintroducedduringtheImage the method, a 10-fold cross-validation is applied. The
Net Large Scale Visual Recognition Challenge(ILSVRC) average accuracy is 94.09%.
2014. This challenge analyses the quality of different Video based emotion recognition has been proposed
image classification approaches submitted by different by Byeon and Kwak [10]. They have developed a three
groups. The images are separated into 1000 different dimensional CNN which uses groups of 5 consecutive
classes organized by the WordNet hierarchy. In the frames as input. A database containing 10 persons has
challenge ”object detection with additional training been used to achieve an accuracy of 95%.
data” GoogleNet has achieved about 44% precision [7]. Song et al. [11] have used a deep convolutional neural
These results have demonstrated the potential which network for learning facial expressions. The created
lies in this kind of architecture. Therefore it has been network consists of five layers with a total of 65k
used as inspiration for the proposed architecture. neurons. Convolutional, pooling, local filter layers
The proposed network has been evaluated on the and one fully connected layer are used to achieve an
Extended Cohn-Kanade Dataset (Section 4.2) and on accuracy of 99.2% on the CKP set. To avoid overfitting
the MMI Dataset (Section 4.1). Typical pictures of the dropout method was used.
persons showing emotions can be seen in Fig. 1. The Luecy et al. [12] have created the Extended Cohn-
emotion Contempt of the CKP set is not shown as no Kanade dataset. This dataset contains emotion
subject with consent for publication and an annotated annotations as well as Action Unit annotations. In
emotion is part of the dataset. Results of experiments regards to classification, they also have evaluated the
on these datasets demonstrate the success of using a datasets using Active Appearance Models (AAMs) in
deep layered neural network structure. With a 10-fold combination with SVMs. To find the position and track
cross-validation a recognition accuracy of 99.6% has the face over different images, they have employed
been achieved. AAM which generates a Mesh out of the face. From this
mesh they have extracted two feature vectors. First, the
The paper is arranged as follows: After this intro- normalized vertices with respect to rotation, translation,
duction, Related Work (Section 2) is presented which and scale. Second a gray-scale image from the mesh
focusesonEmotion/Expressionrecognitionandthevar- data, and the input images has been extracted. They
ious approaches scientists have taken. Next is Section 3, have chosen a cross-validation strategy, where one
Background, which focuses on the main components subject is left out in the training process, achieving an
of the architecture proposed in this article. Section 4 accuracy of over 80%.
contains a summary of the used Datasets. In Section 5 Anderson et al. [1] have developed a face expression
the architecture is presented. This is followed by the system, which is capable of recognizing the six basic
experimentsanditsresults(Section6).Finally,Section8 emotions. Their system is built upon three components.
summarizes the article and concludes the article. The first one is a face tracker (derivative of ratio
template) to detect the location of the face. The second
component is an optical flow algorithm to track the
2 RELATED WORK
motion within the face. The last component is the
A detailed overview for expression recognition was recognition engine itself. It is based upon Support
givenbyCa˘leanu[8]andBettadapura[9].InthisSection Vector Machines and Multilayer Perceptrons. This
mainly work which similar to the proposed method is approach has been implemented in EmotiChat. They
presented as well as few selected articles which give a achieve a recognition accuracy of 81.82%.
broad overview over the different methodologies. Kotsia and Pitas [2] detect emotions by mapping a
Candide grid, a face mask with a low number of
Recently Szegedy et al.[6] have proposed an polygons, onto a person’s face. The grid is initially
architecture called GoogLeNet. This is a 27 layer placed randomly on the image, then it has to be
deep network, mostly composed of CNNs. The network manually placed on the persons face. Throughout the
is trained using stochastic gradient descent. In the emotion,thegridistrackedusingaKanadeLucasTomasi
ILSVRC 2014 Classification Challenge this network tracker. The geometric displacement information
achieved a top-5 error rate of 6.67% winning the first provided by the grid is used as feature vector for
place. multiclass SVMs. The emotions are anger, disgust, fear,
Using the the Extended Cohn-Kanade Dataset happiness, sadness, and surprise. They evaluate the
(Section 4.2), Happy and Routray [4] classify between model on the Cohn-Kanade dataset and an accuracy of
six basic emotions. Given an input image, their solution 99.7% has been achieved.
localizes the face region. From this region, facial patches Shan et al. [13] have created an emotion recognition3
system based on Local Binary Patterns (LBP). The Using these cells is more efficient than sigmoid and
LBPs are calculated over the facial region. From the still forwards more information compared to binary
extracted LBPs a feature vector is derived. The features units. When initializing the weights uniformly, half of
depend on the position and size of the sub-regions the weights are negative. This helps creating a sparse
over witch the LBP is calculated. AdaBoost is used to feature representation. Another positive aspect is the
find the sub-regions of the images which contain the relatively cheap computation. No exponential function
most discriminative information. Different classification has to be calculated. This function also prevents the
algorithms have been evaluated of which an SVM vanishing gradient error, since the gradients are linear
with Boosted-LBP features performs the best with a functionsorzerobutinnocasenon-linearfunctions[15].
recognition accuracy of 95.1% on the CKP set.
In2013Zafaretal.[14]proposedanemotionrecognition FullyConnectedLayer: Thefullyconnectedlayeralso
system using Robust Normalized Cross Correlation known as Multilayer Perceptron connects all neurons of
(NCC). The used NCC is the ”Correlation as a Rescaled the prior layer to every neuron of its own layer. Let the
Variance of the Difference between Standardized inputbexwithsizek andlbethenumberofneuronsin
Scores”. Outlier pixels which influence the template the fully connected layer. This results in a Matrix W l×k.
matching too strong or too weak are excluded and
not considered. This approach has been evaluated F(x)=σ(W ∗x) (4)
on different databases including AR FaceDB (85%
σ is the so called activation function. In our network
Recognition Accuracy) and the Extended Cohn Kanade
σ is the identity function.
Database (100% Recognition Accuracy).
Output Layer: The output layer is a one hot vector
representing the class of the given input image. It there-
forehasthedimensionalityofthenumberofclasses.The
3 CONVOLUTIONAL NEURAL NETWORKS
resulting class for the output vector x is:
Convolutional Layer: Convolutional Layers perform C(x)={i | ∃i∀j (cid:54)=i:x ≤x } (5)
j i
a convolution over the input. Let f be the filter with a
k
kernel size n×m applied to the input x. n×m is the
Softmax Layer: The error is propagated back over
numberofinputconnectionseachCNNneuronhas.The
a Softmax layer. Let N be the dimension of the input
resulting output of the layer calculates as follows:
vector, then Softmax calculates a mapping such that:
S(x):RN →[0,1]N
n m
(cid:88)2 (cid:88)2 For each component 1 ≤ j ≤ N, the output is
C(x )= f (i,j)x (1)
u,v k u−i,v−j calculated as follows:
i=−n 2 j=−m 2 exj
S(x) = (6)
To calculate a more rich and diverse representation of j (cid:80)N exi
the input, multiple filters f with k ∈ N can be applied i=1
k
on the input. The filters f are realized by sharing
k 4 DATASETS
weights of neighboring neurons. This has the positive
effectthatlesserweightshavetobetrainedincontrastto 4.1 MMIDataset
standard Multilayer Perceptrons, since multiple weights The MMI dataset has been introduced by Pantic et
are bound together. al. [16] contains over 2900 videos and images of 75 per-
sons.Theannotationscontainactionunitsandemotions.
Max Pooling: Max Pooling reduces the input by
Thedatabasecontainsaweb-interfacewithanintegrated
applying the maximum function over the input x . Let
i search to scan the database. The videos/images are
m be the size of the filter, then the output calculates as
colored. The people are of mixed age, different gender
follows:
and have different ethnical background. The emotions
m m investigated are the six basic emotions: Anger, Disgust,
M(x i)=max{x i+k,i+l ||k|≤ 2 ,|l|≤ 2 k,l∈N} (2) Fear, Happiness, Sadness, Surprise.
This layer features translational invariance with re-
spect to the filter size. 4.2 CKPDataset
This dataset has been introduced by Lucey et al. [12].
RectifiedLinearUnit: ARectifiedLinearUnit(ReLU)
210persons,aged18to50,havebeenrecordeddepicting
is a cell of a neural network which uses the following
emotions. This dataset presented by contains recordings
activation function to calculate its output given x:
of emotions of 210 persons at the ages of 18 to 50 years.
Both female and male persons are present and from
R(x)=max(0,x) (3) different background. 81% are Euro-Americans and 13%4
Anger Disgust Fear Happy Sadness Surprise
Fig. 3: This Figure shows the differences within the
Cohn-KanadePlus(CKP)dataset.TheemotionContempt
is not shown since there is no annotated image with
the emotion being depicted, which is allowed to be
Anger Disgust Fear Happy Sadness Surprise
displayed.
Fig.2:ThisFigureshowsthedifferenceswithintheMMI
dataset. The six used emotions are listed.
are Afro-Americans. The images are of size 640×490 px
aswell640×480px.Theyarebothgrayscaleandcolored.
In total this set has 593 emotion-labeled sequences. The
emotions consist of Anger, Disgust, Fear, Happiness, Sad-
ness, Surprise, and Contempt.
4.3 Comparison
In the MMI Dataset (Fig. 2) the emotion Anger is
displayed in different ways, as can be seen by the
eyebrows, forehead and mouth. The mouth in the lower
image is tightly closed while in the upper image the
mouth is open. For Disgust the differences are also
visible, as the woman in the upper picture has a much
strongerreaction.ThemandepictingFearhascontracted
eyebrows which slightly cover the eyes. On the other
hand the eyes of the woman are wide open. As for
Happy both persons are smiling strongly. In the lower
image the woman depicting Sadness has a stronger lip
and chin reaction. The last emotion Surprise also has
differences like the openness of the mouth.
Such differences also appear in the CKP set (Fig. 3).
For Anger the eyebrows and cheeks differ. For Disgust
larger differences can be seen. In the upper picture not
onlythecurvatureofthemouthisstronger,butthenose
is also more involved. While both women displaying
Fear show the same reaction around the eyes the mouth
differs. In the lower image the mouth is nearly closed
while teeth are visible in the upper one. Happiness is
displayed similar. For the emotion Sadness the curvature
of the mouth is visible in both images, but it is stronger
in the upper one. The regions around the eyes differ as
theeyebrowsofthewomanarestraight.Thelastemotion
Surprise has strong similarities like the open mouth an
wideopeneyes.Teethareonlydisplayedbythewoman
in the upper image.
Thus for a better evaluation it is helpful to investigate
multipledatasets.Thisaimsatinvestigatingwhetherthe
Fig. 4: This is the proposed architecture. The main
proposed approach works on different ways emotions
component of this architecture is the FeatEx block. In
are shown and whether it works on different emotions.
the Convolutional layer S depicts the Stride and P the
ForexampleContemptwhichisonlyincludedintheCKP
Padding.
set.5
5 PROPOSED ARCHITECTURE FeatEx block creates two parallel paths of features with
different scales, which are combined in Concat 2. The
The proposed deep Convolutional Neural Network ar-
second FeatEx block refines the representation of the
chitecture (depicted in Figure 4) consists of four parts.
features. It also decreases the dimensionality.
The first part automatically preprocesses the data. This
begins with Convolution 1, which applies 64 different ThisvisualizationshowsthattheconcatenationofFeatEx
blocks is a valid approach to create an abstract feature
filters. The next layer is Pooling 1, which down-samples
representation. The output dimensionality of each layer
theimagesandthentheyarenormalizedbyLRN1.The
can be seen in Table 1.
nextstepsarethetwoFeatEx(ParallelFeatureExtraction
Block) blocks, highlighted in Figure 4. They are the
core of the proposed architecture and described later in
this section. The features extracted by theses blocks are
forwarded to a fully connected layer, which uses them
to classify the input into the different emotions.
The described architecture is compact, which makes it
not only fast to train, but also suitable for real-time
applications. This is also important as the network was
built with resource usage in mind.
TABLE 1: This Table lists the different output sizes
produced by each layer.
Layer OutputSize
Data 224×224
Convolution1 64×112×112
Pooling1 64×56×56
LRN1 64×56×56
Convolution2a 96×56×56
Convolution2b 208×56×56
Pooling2a 64×56×56
Convolution2c 64×56×56
Concat2 272×56×56
Pooling2b 272×28×28
Convolution3a 96×28×28
Convolution3b 208×28×28
Pooling3a 272×28×28
Convolution3c 64×28×28
Concat3 272×28×28
Pooling3b 282×14×14
Classifier 11×1×1
FeatEx: The key structure in our architecture is the
Parallel Feature Extraction Block (FeatEx). It is inspired
by the success of GoogleNet. The block consists of
Convolutional, Pooling, and ReLU Layers. The first
Convolutional layer in FeatEx reduces the dimension Fig. 5: This Figure shows example visualizations of the
since it convolves with a filter of size 1 × 1. It is different layers. The data is taken from the MMI set.
enhanced by a ReLU layer, which creates the desired
sparseness. The output is then convolved with a filter
of size 3×3. In the parallel path a Max Pooling layer
6 EXPERIMENTS AND RESULTS
is used to reduce information before applying a CNN
of size 1×1. This application of differently sized filters As implementation Caffe [17] was used. This is a deep
reflects the various scales at which faces can appear. learning framework, maintained by the Berkeley Vision
The paths are concatenated for a more diverse and Learning Center (BVLC).
representation of the input. Using this block twice
yields good results. CKP: The CKP database has been analyzed often
and many different approaches have been evaluated
in order to ”solve” this set. To determine whether the
Visualization: Thedifferentlayers ofthearchitecture architecture is competitive, it has been evaluated on the
produce feature vectors as can be seen in Fig 5. The CKP dataset. For the experiments all 5870 annotated
first part until LRN 1 preprocesses the data and creates images have been used to do a 10-fold cross-validation.
multiple modified instances of the input. These show Theproposedarchitecturehasproventobeveryeffective
mostly edges with a low level of abstraction. The first on this dataset with an average accuracy of 99.6%. In6
TABLE 3: This Table summarizes the current state of
Table 2 different results from state of the art approaches
the art in emotion recognition on the MMI database
arelistedascomparison.The100%accuracyreportedby
(Section 4.1).
Zafar [14] is based on hand picked images. The results
are not validated using cross-validation. The confusion Author Method Accuracy
matrixinFig.6adepictstheresultsandshowsthatsome WangandYin[19] LDA 93.33%
WangandYin[19] QDC 92.78%
emotions are perfectly recognized.
WangandYin[19] NBC 85.56%
DeXpression(Proposed) 98.63%
MMI: The MMI Database contains videos of peo-
ple showing emotions. From each video the 20 frames,
which represent the content of the video the most, have
been extracted fully automatically. The first two of these
frames have been discarded since they provide neutral detected. The emotion Surprise is often confused with
expressions. Disgust with a rate of 0.045% which is the highest. Of
To determine the frames, the difference between those images, where an emotion is present, only few
grayscale consecutive frames was calculated. To com- are wrongly classified.
pensate noise the images have been smoothed using a
Gaussian filter before calculation. To find the 20 most
As there is no consent for the misclassified images,
representative images, changes which occur in a small
they cannot be depicted here. However some unique
timeframe,shouldonlyberepresentedbyasingleimage.
names are provided.
Thiswasachievedbyiteratingoverthedifferencesusing
Image S119 001 00000010 is classified as Fear while
a maximum filter with decreasing filter size until 20
the annotated emotion corresponds to Surprise. The
frames have been found. In total 3740 images have been
image depicts a person with a wide open mouth and
extracted.
open eyes. Pictures representing Surprise are often very
The original images were then used for training and
similar, since the persons also have wide open mouths
testing. A 10-fold cross-validation has been applied.
and eyes. In image S032 004 00000014 the targeted
The average accuracy is 98.63%. This is better than the
label Fear is confused with Anger. While the mouth
accuracies achieved by Wang and Yin [19] (Table 3). To
region in pictures with Anger differ, the eye regions are
ourknowledgetheyhavebeentheonlyonestoevaluate
alike, since in both situations the eyes and eyebrows are
the MMI database on Emotions instead of Action Units.
contracted.
Theresultsoftheproposedapproacharedepictedinthe
Similar effects are experienced when dealing with the
ConfusionMatrixinFig.6b.Inthefigureitisshownthat
MMI Dataset. Since the first two frames are discarded
the accuracy for Fear is the lowest with 93.75% while
most pictures with neutral positions are excluded. In
Happiness is almost perfectly recognized with 98.21%.
few images a neutral position can still be found which
Fear and Surprise are the emotions confused the most.
gives rise to errors. For the same reason as the CKP set
images will not be displayed. Due to the approach to
7 DISCUSSION
extract images of the videos, a unique identifier for the
misclassified image cannot be provided.
The accuracy on the CKP set shows that the chosen
The top confusions are observed for Fear and Surprise
approach is robust, misclassification usually occurs on
with a rate of 0.0159% where Fear is wrongly
pictures which are the first few instances of an emotion
misclassified as Surprise. Session 1937 shows a woman
sequence. Often a neutral facial expression is depicted
displaying Fear but it is classified as Surprise. Both
in those frames. Thus those misclassifications are not
share common features like similar eye and mouth
necessarily an error in the approach, but in the data
movement. In both emotions, participants move the
selection. Other than that no major problem could be
head slightly backwards. This can be identified by
wrinkled skin. The second most confusion rate, Surprise
TABLE 2: The CKP database has been very well an- being mistaken as Sadness, is mostly based on neutral
alyzed and the best possible recognition accuracy has position images. Although the first two images are
been achieved by Aliya Zafar. It is noteworthy that the not used, some selected frames still do not contain an
samples he used for training are not randomly selected emotion. In Session 1985 Surprise is being mistaken as
andnocross-validationhasbeenapplied.Evaluatingthis Sadness. The image depicts a man with his mouth being
database provides information whether the proposed slightly curved, making him look sad.
approach can compete with those results.
Author Method Accuracy DeXpression extracts features and uses them to clas-
AliyaZafar[14] NCC 100%
sify images, but in very few cases the emotions are
Happyetal.[4] FacialPatches+SVM 94.09%
Luceyetal.[12] AAM+SVM ≥80% confused.Thishappens,asdiscussed,usuallyinpictures
Songetal.[18] ANN(CNN) 99.2% depicting no emotion. DeXpression performs very well
DeXpression(Proposed) 99.6% on both tested sets, if an emotion is present.7
(b) The confusion matrix of the averaged 10-fold cross-
(a) The confusion matrix of the averaged 10-fold cross- validation on the MMI Dataset. The lowest accuracy is
validation on the CKP Dataset. The lowest accuracy is achieved by the emotion Fear with 93.75%. Happiness is
achieved by the emotion Surprise with 98.79% while Con- recognized with 98.21%.
tempt/Sadness are both recognized with 100%.
8 CONCLUSION AND FUTURE WORK to new products. In this example it could predict the
amount of orders that will be made, therefore enabling
In this article DeXpression is presented which works
producing the right amount of products.
fully automatically. It is a neural network which has
little computational effort compared to current state of
the art CNN architectures. In order to create it the
ACKNOWLEDGMENTS
new composed structure FeatEx has been introduced. It We would like to thank the Affect Analysis Group of
consistsofseveralConvolutionallayersofdifferentsizes, the University of Pittsburgh for providing the Extended
as well as Max Pooling and ReLU layers. FeatEx creates CohnKanade database, and Prof. Pantic and Dr. Valstar
a rich feature representation of the input. for the MMI data-base.
The results of the 10-fold cross-validation yield, in av-
erage, a recognition accuracy of 99.6% on the CKP
REFERENCES
datasetand98.36%ontheMMIdataset.Thisshowsthat
the proposed architecture is capable of competing with [1] K.AndersonandP.W.Mcowan,“Areal-timeautomatedsystem
currentstateoftheartapproachesinthefieldofemotion for recognition of human facial expressions,” IEEE Trans. Syst.,
Man,Cybern.B,Cybern,pp.96–105,2006.
recognition.
[2] I. Kotsia and I. Pitas, “Facial expression recognition in image
In Section 7 the analysis has shown, that DeXpression sequences using geometric deformation features and support
works without major mistakes. Most misclassifications vector machines,” Image Processing, IEEE Transactions on, vol. 16,
no.1,pp.172–187,Jan2007.
haveoccurredduringthefirstfewimagesofanemotion
[3] B.V.Kumar,“Faceexpressionrecognitionandanalysis:thestate
sequence. Often in these images emotions are not yet oftheart,”CoursePaper,VisualInterfacestoComputer,2009.
displayed. [4] S.HappyandA.Routray,“Automaticfacialexpressionrecogni-
tionusingfeaturesofsalientfacialpatches,”AffectiveComputing,
IEEETransactionson,vol.6,no.1,pp.1–12,Jan2015.
Future Work: An application built on DeXpression
[5] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng,
which is used in a real environment could benefit from andT.Darrell,“Decaf:Adeepconvolutionalactivationfeaturefor
distinguishing between more emotions such as Nervous- genericvisualrecognition,”arXivpreprintarXiv:1310.1531,2013.
[6] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
ness and Panic. Such a scenario could be large events
D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper
where an early detection of Panic could help to pre- with convolutions,” CoRR, vol. abs/1409.4842, 2014. [Online].
vent mass panics. Other approaches to enhance emotion Available:http://arxiv.org/abs/1409.4842
[7] LSVRC. (2014, Jun.) Results of the lsvrc challenge. [Online].
recognition could be to allow for composed emotions.
Available:http://www.image-net.org/challenges/LSVRC/2014/
For example frustration can be accompanied by anger, results
therefore not only showing one emotion, but also the [8] C.-D.Caleanu,“Faceexpressionrecognition:Abriefoverviewof
thelastdecade,”inAppliedComputationalIntelligenceandInformat-
reason. Thus complex emotions could be more valuable
ics(SACI),2013IEEE8thInternationalSymposiumon. IEEE,2013,
thanbasicones.Besidesdistinguishingbetweendifferent pp.157–161.
emotions, also the strength of an emotion could be con- [9] V. Bettadapura, “Face expression recognition and analysis: the
stateoftheart,”arXivpreprintarXiv:1203.6722,2012.
sidered. Being able to distinguish between different lev-
[10] Y.-H.ByeonandK.-C.Kwak,“Facialexpressionrecognitionusing
els could improve applications, like evaluating reactions 3dconvolutionalneuralnetwork.”8
[11] I. Song, H.-J. Kim, and P. B. Jeon, “Deep learning for real-
time robust facial expression recognition on a smartphone,” in
ConsumerElectronics(ICCE),2014IEEEInternationalConferenceon.
IEEE,2014,pp.564–567.
[12] P. Lucey, J. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and
I. Matthews, “The extended cohn-kanade dataset (ck+): A com-
pletedatasetforactionunitandemotion-specifiedexpression,”in
ComputerVisionandPatternRecognitionWorkshops(CVPRW),2010
IEEEComputerSocietyConferenceon,June2010,pp.94–101.
[13] C. Shan, S. Gong, and P. W. McOwan, “Facial expression recog-
nition based on local binary patterns: A comprehensive study,”
ImageandVisionComputing,vol.27,no.6,pp.803–816,2009.
[14] A. Zafer, R. Nawaz, and J. Iqbal, “Face recognition with expres-
sion variation via robust ncc,” in Emerging Technologies (ICET),
2013IEEE9thInternationalConferenceon,Dec2013,pp.1–5.
[15] X.Glorot,A.Bordes,andY.Bengio,“Deepsparserectifierneural
networks,”inProceedingsoftheFourteenthInternationalConference
on Artificial Intelligence and Statistics (AISTATS-11), G. J. Gordon
and D. B. Dunson, Eds., vol. 15. Journal of Machine Learning
Research-WorkshopandConferenceProceedings,2011,pp.315–
323.
[16] M. Pantic, M. F. Valstar, R. Rademaker, and L. Maat, “Web-
based database for facial expression analysis,” in Proceedings of
IEEEInt’lConf.MultimediaandExpo(ICME’05),Amsterdam,The
Netherlands,July2005,pp.317–321.
[17] Y.Jia,E.Shelhamer,J.Donahue,S.Karayev,J.Long,R.Girshick,
S.Guadarrama,andT.Darrell,“Caffe:Convolutionalarchitecture
forfastfeatureembedding,”arXivpreprintarXiv:1408.5093,2014.
[18] I.Song,H.-J.Kim,andP.Jeon,“Deeplearningforreal-timerobust
facial expression recognition on a smartphone,” in Consumer
Electronics(ICCE),2014IEEEInternationalConferenceon,Jan2014,
pp.564–567.
[19] J. Wang and L. Yin, “Static topographic modeling for facial
expressionrecognitionandanalysis,”Comput.Vis.ImageUnderst.,
vol.108,no.1-2,pp.19–34,Oct.2007."
70,72,Dfew: A large-scale database for recognizing dynamic facial expressions in the wild,"['X Jiang', 'Y Zong', 'W Zheng', 'C Tang', 'W Xia']",2020,126,"Acted Facial Expressions In The Wild, Affective Faces Database, Expression in-the-Wild, Static Facial Expression in the Wild","CNN, FER, classification, classifier, deep learning, facial expression recognition, machine learning, neural network","Recently, facial expression recognition (FER) in the wild has gained a lot of researchers’   a dynamic facial expression in the wild database, ie, acted facial expressions in the wild (AFEW)",No DOI,Proceedings of the 28th …,https://arxiv.org/abs/2008.05924,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"0202
guA
31
]VC.sc[
1v42950.8002:viXra
DFEW: A Large-Scale Database for Recognizing Dynamic Facial
Expressions in the Wild
XingxunJiang∗ WenmingZheng† ChuangaoTang
Yuan Zong∗ KeyLaboratoryofChild SchoolofBiologicalScienceand
jiangxingxun@seu.edu.cn DevelopmentandLearningScience, MedicalEngineering,Southeast
xhzongyuan@seu.edu.cn SoutheastUniversity University
SchoolofBiologicalScienceand Nanjing,China Nanjing,China
MedicalEngineering,Southeast wenming_zheng@seu.edu.cn tcg2016@seu.edu.cn
University
Nanjing,China
WanchuangXia ChengLu Jiateng Liu
SchoolofCyberScienceand SchoolofInformationScienceand SchoolofBiologicalScienceand
Engineering,SoutheastUniversity Engineering,SoutheastUniversity MedicalEngineering,Southeast
Nanjing,China Nanjing,China University
xiawanchuag@seu.edu.cn cheng.lu@seu.edu.cn Nanjing,China
Jiateng_Liu@seu.edu.cn
ABSTRACT CCSCONCEPTS
Recently,facialexpressionrecognition(FER)inthewildhasgained • Human-centered computing → Visualization design and
alotofresearchers’attentionbecauseitisavaluabletopictoen- evaluation methods; • Computing methodologies → Com-
abletheFERtechniquestomovefromthelaboratorytothereal putervision.
applications.Inthispaper,wefocusonthischallengingbutinter-
estingtopicandmakecontributionsfromthreeaspects.First,we KEYWORDS
presentanewlarge-scale’in-the-wild’dynamicfacialexpression Dynamicfacialexpression;Facialexpressiondatabase;in-the-wild
database,DFEW(DynamicFacialExpressionintheWild),consist- facialexpressionrecognition;deeplearning
ing of over 16,000 video clips from thousands of movies. These
ACMReferenceFormat:
videoclipscontainvariouschallenging interferences inpractical
XingxunJiang,YuanZong,WenmingZheng,ChuangaoTang,Wanchuang
scenariossuchasextremeillumination,occlusions,andcapricious
Xia,ChengLu,andJiatengLiu.2020.DFEW:ALarge-ScaleDatabasefor
posechanges.Second,weproposeanovelmethodcalledExpression- RecognizingDynamicFacialExpressionsintheWild.InProceedingsofthe
ClusteredSpatiotemporalFeatureLearning(EC-STFL)framework 28thACMInternationalConferenceonMultimedia(MM’20),October12–16,
todealwithdynamicFERinthewild.Third,weconductextensive 2020,Seattle,WA,USA.ACM,NewYork,NY,USA,9pages.https://doi.org/10.1145/3394171.3413620
benchmark experiments onDFEW using a lotofspatiotemporal
deepfeaturelearningmethodsaswellasourproposedEC-STFL. 1 INTRODUCTION
ExperimentalresultsshowthatDFEWisawell-designedandchal- Facial expression isoneofthemostnaturallypre-eminent ways
lengingdatabase,andtheproposedEC-STFLcanpromisinglyim- forhumanbeingstocommunicatetheiremotionsindailylife[3].
provetheperformanceofexistingspatiotemporaldeepneuralnet- Imaginethatifcomputerswereabletounderstandemotionsfrom
worksincopingwiththeproblemofdynamicFERinthewild.Our facialexpressionsashumanbeings,ourhuman-computerinterac-
DFEWdatabaseispubliclyavailableandcanbefreelydownloaded tion (HCI) systems would be more friendly and natural. Due to
fromhttps://dfew-dataset.github.io/. thisreason,facialexpressionrecognition(FER)hasbecomeahot
researchtopicamongHCIandmultimediaanalysiscommunities.
∗Bothauthorscontributedequallytothisresearch. Over the past decades, researchers have proposed a lot of well-
†Correspondingauthor
performingmethodsforrecognizingfacialexpressions,andthese
methodsachievedpromisingperformanceinthelab-controlleden-
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor
vironments [8, 24, 30, 41–43]. However, FER techniques are still
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcita- farfromthepracticalapplications.Oneofthemainreasonsisthat
tiononthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthan thefacialexpressionsinthelab-controlledscenariosaredifferent
theauthor(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyother-
fromthereal-worldones.Theunconstrainedreal-worldfacialex-
wise,orrepublish,topostonserversortoredistributetolists,requirespriorspecific
permissionand/orafee.Requestpermissionsfrompermissions@acm.org. pressionoftensuffersfromocclusions,illuminationvariation,pose
MM’20,October12–16,2020,Seattle,WA,USA changes,andmanyotherunpredictableandchallenginginterfer-
©2020Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
ences, making theperformanceofmostexisting FERtechniques
ACMISBN978-1-4503-7988-5/20/10...$15.00
https://doi.org/10.1145/3394171.3413620 dropsharply.Forthisreason,manyresearchershaverecentlyshift-Table1:Summaryofexistingdatabasesofdynamicfacialexpressioninthewild.
Database #Sample Source ExpressionDistribution #AnnotationTimes Available?
Aff-Wild[17] 298 Web Valence-arousal 8 Yes
AFEW7.0[4] 1,809 54Movies 7basicexpressions 2 Yes
AFEW-VA[18] 600 AFEWdatabase Valence-arousal 2 Yes
CAER[19] 13,201 79TVshows 7basicexpressions 3 Yes
DFEW 16,372 1500movies 7basicexpressions 10 Yes
edtheirfocustoachallengingbutmeaningfulFERtopic,i.e.,FER integratingmultiplepredictionscoreslearnedbydifferentspatiotem-
inthewild,where’inthewild’referstothechallenging condi- poralfeaturelearningnetworks,andwonthechampion.Neverthe-
tionsinunconstrainedreal-worldenvironments. less,theaccuracyofthetestsettheyachievedisonly62.78%(7ex-
SimilartoconventionalFER,FERinthewildcanbedividedinto pressionclassificationtask),whichisstillatalowlevelanddoes
twotypesoftaskaccordingtotheformofsamples.Oneisstatic notmeettherequirementofpracticalapplications.
FERinthewild,whoseaimistopredicttheexpressioncategory Inordertoremovethebarrierofdatavolumetotheresearchof
fromunconstrainedfacialimages.TheotherisdynamicFERinthe dynamicFERinthewild,inthispaper,wefirstpresentanewlarge-
wild,inwhichthedatadescribingtheexpressioninformation,is scaleandwell-annotatedunconstraineddynamicfacialexpression
thevideocliporimagesequence.Inspiredbythesuccessofdeep database,DFEW(DynamicFacialExpressionintheWild).DFEW
learning in many vision tasks, some researchers have begun to canbeservedasabenchmarkforresearcherstodevelopandevalu-
construct large-scale facial expressions in the wild databases by atetheirmethodsfordealingwithdynamicFERinthewild.Tosee
resortingtotheInternetthatcontainsabundantfacialexpression thecharacteristicsofDFEW,wesummarizeexistingdatabasesof
resources.Forexample,Benitezquirozetal.[1]collectedfacialim- dynamicfacialexpressionsinthewildinTable1.FromTable1,it
agesfromtheInternetandthencreatedalarge-scalestaticfacial canclearlybeseenthatourDFEWhasthreemajoradvantagesover
expression in thewild databasecalledEmotioNet.EmotioNetin- existingdatabasesincludingAff-Wild[17],AFEW7.0[4],AFEW-
cludes1,000,000facialexpressionimages,inwhich25,000images VA[18],andCAER[19].First,DFEWdatabasehascurrentlylargest
were manually labeled with 11 facial Action Units (AUs). Subse- numberofdynamicfacialexpressionsamplesreachingover16,000
quently,Mollahosseinietal.[28]constructedamuchlarger-volume videoclips.Second,theformsofsceneandsampleinDFEWare
database, i.e., AffectNet, consisting of 450,000well-labeled facial many and varied because its video clips are collectedfrom over
imagesamplesqueriedfromtheInternet.Recently,Lietal.[20,21] 1,500 movies all over theworld covering various challenging in-
presentedanovelstaticfacialexpressiondatabase,RAF-DB,con- terferences,e.g.,extremeilluminations,self-occlusions,andcapri-
taining nearly 30,000web-queried facialimages. Comparedwith ciousposechanges.Lastbutnotleast,eachsampleinDFEWhas
EmotioNetandAffectNet,themajoradvantageofRAF-DBisthe beenindividuallylabeledtentimesbytheannotatorsunderpro-
annotation.RAF-DBcollectorshired315individualsastheanno- fessionalguidance.
tators, and each sample in RAF-DB is labeled about 40 times to In addition to DFEW, we also propose a novel method called
ensureitslabelingreliability. Expression-ClusteredSpatiotemporalFeatureLearning(EC-STFL)
Unfortunately,incontrasttothestaticfacialexpressionsinthe frameworktodealwithdynamicFERinthewild.EC-STFLframe-
wild,onlyafewunconstraineddynamicfacialexpressiondatabases workcan enforce the spatiotemporaldeep neural networks, e.g.,
havebeenreleaseduntilnow.Intheworkof[5],Dhalletal.built C3D[37]andP3D[32],tobetterlearndiscriminativefeaturesde-
adynamicfacialexpressioninthewilddatabase,i.e.,actedfacial scribingdynamicfacialexpressionsinthewild.Finally,weestab-
expressions in the wild (AFEW), which has been updatedtothe lishabenchmarkevaluationprotocolforDFEWandconductexten-
7thversion (AFEW 7.0) [4] and consists of 1,809 video clipscol- siveexperimentsusingmanyspatiotemporaldeeplearningmeth-
lectedfrom54movies.Recently,Leeetal.[19]builtalarge-scale odsaswellasourproposedEC-STFL.Experimentalresultsshow
benchmarkfordynamicFERinthewild,calledCAER,bycollect- thattheproposedEC-STFLframeworkcanpromisinglyimprove
ing 13,201 video clipsfrom 79 TV shows. Eachclip was individ- theperformanceofexistingspatiotemporalneuralnetworksincop-
uallylabeledbythree annotators.Tothe bestofourknowledge, ingwithFERWproblem.
CAERisthefirstlarge-scaledatabaseofdynamicfacialexpression
inthewild.However,duetothelackoflarge-scaledatabases,the
progressofdeeplearningmethodsfordynamicFERinthewild
is seriously hindered. For example, in EmotiW2019, the annual
emotion recognition challenge held at ACM ICMI based on the
AFEWdatabase,Lietal.[22]proposedaweightedfusionmethod
Figure1:Overviewoftheconstructionandtheannotation
ofDFEW.Figure2:Examplesofsevenbasicemotionsfromsingle-labeledDFEW.
2 DFEWDATABASE e.g., disgust and fear. Through the above method,we ultimately
collected16,372unconstrainedfacialexpressionvideoclips.
2.1 DataCollection
It isbelieved thatmoviesoriginate fromand mimic ourreal life,
2.2 DataAnnotation
henceactressesandactorsinmoviesmayhaveallkindsofuncon-
High-qualitydataannotationisanotherchallengeforthedatabase.
strainedfacialexpressionsoriginallyexistinginthepracticalsce-
Firstofall,annotatingsuchalargedatabaseistime-consumingand
narios.Thusitoffersusabundantsamplesofdynamicfacialexpres-
needsefficientpersonnelmanagement.Second,thoughpsycholo-
sions.Byextractingthevideoclipscontainingdifferentfacialex-
gistsP.Ekmanbelievesthatthesevenbasicemotionsareuniversal
pressionsfrommovies,weareabletobuildalarge-scaledatabase
andindependentoftheculturalmismatch[6],culturemismatchin-
ofdyanmicfacialexpressionsinthewild.Followingthismethod,
deedexistsandworthconsideringbecausethelabelingbiascanbe
several dynamic facial expression databases, e.g., Aff-Wild [17],
removedasfaraspossible.Toefficientlymanageannotatorsand
AFEW[4,5],andCAER[19]havebeensuccessivelybuiltandre-
understand the protagonist’s emotion in clips better, we entrust
leasedoverthepastfewyears,whichindeedadvancestheresearch
thelabelingworktotheprofessionalcrowdsourcingcompany,JD
ofdynamicFERinthewild.Inthispaper,wealsotakefulladvan-
crowdsourcing1,wherewehiredtwelveexpertannotators.They
tageofmoviestocollectunconstraineddynamicfacialexpression
areaskedtoidentifyeachclip’sclosestemotioninseventypicaldis-
samplestobuildourDFEWdatabase.
creteemotions,i.e.,anger,disgust,fear,happy,sad,surprise,and
ThepipelineofbuildingtheDFEWdatabaseisshowninFig.1.
neutral.Beforeformalannotation,thesetwelveannotatorsarepro-
AsFig.1shows,wefirstmakeuseofcrawlertocollectover1,500
fessionallytrainedwiththeemotionalknowledges.Theneachclip
high-definitionmoviesclosetoourreallifeandcoveringvarious
isannotatedbytenindependentannotators.Afterannotation,we
themes,e.g.,comedy,tragedy,war,andlove,fromtheInternetto
obtainedtheseven-dimensionalemotionvectorsoremotiondistri-
serveasthesamplesourceoffacialexpressionsinthewild.Then,
butionannotatinginformationof16,372clips.
wehireddozensofstudentstousevideoeditingsoftwaretoman-
Wesupposetheseven-dimensional emotiongroundtruthofj-
uallyextractvideoclipscontainingoneofsevenbasicexpressions
from their assigned movies. Note that we made several rules to
th video clip denoted by Lj = {l1,...,lk,fi,l7}, where lk repre-
sentstheannotationtimesofk-themotionlabeledbyannotators,
helpthesestudentextractorsensurethediversityoftheirextracted
k ∈ {1,2,3,4,5,6,7}refertohappy,sad,neutral,angry,surprise,
facial expression samples. For example, thestudents are only al-
disgustandfear,respectively.
lowed to extract at most 20 video clips from each movie. Mean-
while, an additional reward wouldbe given toone student if he
orshesubmittedthesamplesofrelativelyrarefacialexpressions, 1http://weigong.jd.com/Table2:Thebasicinformationofsingle-labeledDFEW.
N
1
Clips
P¯= Pi (3)
Emotions
0-2s 2-5s 5s+ Total
Percent N Õi=1
Happy 852 1252 384 2488 20.63 k
Sad 440 915 653 2008 16.65 P¯ e = p2 j (4)
Neutral 832 1335 542 2709 22.46 j=1
Õ
Angry 762 1091 376 2229 18.48
Thenwecancalculateκby
Surprise 691 648 159 1498 12.42
Disgust 71 58 17 146 1.22
Fear 408 435 138 981 8.14 κ =
P¯−P¯
e (5)
1−P¯
e
Total 4056 5734 2269 12059 100.00
WeperformFleiss’sKapaatestbothinthewholeDFEWdata-
baseandthesingle-labeledpart,and weobtainκ = 0.70forthe
whole DFEW database andκ = 0.63 for the single-labeled part.
However,notallclipscanbefurtherclearlyassignedtoaspe-
BasedonTable3,webelievethatallannotatorsachieveasubstan-
cificsingle-labeledemotioncategoryfrommulti-dimensionalemo-
tialagreement.Thatistosay,ourannotationisofhighquality.
tiondistribution.Therefore,foraccuratelabeling,wepickoutthe
emotionk as thesingle label withrespect tol > r,wherer is
k
thethresholdvalueofannotationtimes.Inthiswork,wesetthe Table3:InterpretationofκforFleiss’KapaaTest.
threshold valuer = 6, hence select 12059 clips of DFEW to be
thesingle-labeled.Weprovidebasicinformationofsingle-labeled κ Interpretation
DFEW in Table 2, and demo samples ofsingle-labeled DFEW in
<0 Pooragreement
Fig.2.Notethat,topromoteemotionresearch,wewillreleaseboth
0.01-0.20 Slightagreement
single-labeledannotationandseven-dimensionalemotiondistribu-
0.21-0.40 Fairagreement
tionannotation.
0.41-0.60 Moderateagreement
0.61-0.80 Substantialagreement
2.3 AgreementTest
0.81-1.00 Almostperfectagreement
Inthissection,wediscussthequalityofemotionannotationbased
on Fleiss’s Kappa test [10]. Fleiss’s Kapaa test calculates the de-
gree ofagreement inclassificationover thatwhichwouldbeex-
pectedbychance.Webelievethatitsresultisanexcellentindex 3 EXPRESSION-CLUSTERED
to give annotation’s reliability or quality.In the taskof annotat- SPATIOTEMPORALFEATURELEARNING
ingclips,tenindependentindividualsannotateeachclipwithk ∈
Thechallenge ofdynamic FERWis how tolearnrobustand dis-
{1,2,3,4,5,6,7}, i.e., one of the seven typical discrete emotions.
criminativefeaturestodescribefacialexpressionvideoclips,the
Here,weletnij representthenumberofannotatorswhoassigned
facialexpressionrepresentationofvideoclips,whicharecontam-
thei-thcliptothej-themotion.Sowecancalculatepj,thepropor-
inated by the abnormal conditions, such as variations of illumi-
tionofallassignmentswhichweretothej-themotion,
nation, posture,occlusionand scale. Spatiotemporalfeatures ob-
tained by the various spatiotemporal neural networks are adept
N
pj = N1
×n
i=1nij
(1)
i tn hech spa ara tic at ler si tz ri en ag mth ane ddy ten mam poic raf lac ste rem ao mt .io Bn ecin auv si ede oo fts ha em sp tl re os nf gro fim
t-
 jK =1pj =1 Í t ti un rg esa pbi el rit fy oro mfn be eu ttr ea rln the atw no thrk es t, rt ah de ith ioie nr aa lr mch ei tc ha ol dsp sa intio thte em ap no tir -a nl of ie sa e-
wheren=10istheannotaÍ tiontimeofeachclips,K =7isthenum- p dir so tb ril bem ut. eU dn info tr ht eun fea ate tuly r, et sh pe acm ea ir sg si tn ilo lf bld ui rff re inre gn dt ue em to ot ti ho on sf ee aa btu nr oe rs
-
berofemotioncategory,andN isthenumberofclips.Andwecan
malorchallengingconditions.TosimultaneouslycopewithFERW
calculatePi,theextenttowhichannotatorsagreeforthei-thclip,
and the make feature margins clear, we propose an Expression-
i.e., compute how many annotator-annotator pairs are in agree-
ClusteredSpatiotemporalFeatureLearning(EC-STFL)framework,
ment,relativetothenumberofallpossibleannotator–annotator
which can be embedded in the popular spatiotemporal network
pairs:
flexibly.DrawingontheideaofLDA,theEC-STFLenhancesintra-
classcorrelationandreducesinter-classcorrelationbydesigning
K
1 specialsimilaritymatrices,andisformulatedasfollows,
Pi =
n×(n−1)
( n i2 j)−n (2)
 j=1 
muA lan fd orco cm oep ffiut ce ieP n¯, tt κh :emeanofPi    ,Õ andP¯ e whi    chgointothefor- m Win
Õi,j
QPi ij jϕ ϕ( (x xi i, ,x xj j) ) (6)whereW isthenetwork’sweight,matrixPandmatrixQareboth 4.1 ExperimentalSetup
similaritymatrices,ϕ(xi,xj)= xi −xj isthespatiotemporalfea- Data&Protocol.Tobetterevaluatethesingle-labeledDFEWdata-
turedistanceofsamplexiandsa (cid:13)mplexj(cid:13),wherex ∈Rdisextracted basewith12,059videoclips,weadopta5-foldcross-validationpro-
from the final hidden fullycon(cid:13)nected(cid:13)layers, i.e., just before the tocolfor the benchmarks, which means we split all the samples
softmaxlayerthatproducestheclassprediction.AndthematrixP intofivesame-sizepartswithoutoverlaptoconductexperiments.
andmatrixQaredefinedasfollows: Ineachfold(fd1∼fd5),onepartofsamplesareusedfortesting
andtheremainingfortraining.Finally,allthepredictedlabelsare
usedtocomputetheevaluationmetricsbycomparingtheground
Pij =
0, ifxi andxj hasthesamelabel
(7) truth.
(1, otherwise
Preprocessing.First,weuseOpenCVtoextractimageframes
from12,059clips,face++API [33]to acquirefaceregion images
Qij =
0, ifxi andxj hasthedifferentlabel
(8)
a an nd df sa tc ai ta isl tl ia cn sd tm hear uk ss e. fW ule frr ae mm eov re att ehe ofn co ln ip-f sa tc oe( eu lin md ie nt ae tc ete td h) of sr eam lee ss
s
(1, otherwise
than50%.Totally362clipswerenottakenintoconsideration.Then,
Obviously,theEC-STFLminimizesthefeaturedistancebetween we use SeetaFace [25] for face affine transformation, which nor-
thesameemotionsandmaximizethefeaturedistancebetweendif- malizesfacesbasedonacquiredfaciallandmarks.Finally,wealign
ferent emotionstoclarifytheemotionmargin inspatiotemporal temporallengthoftheremainingclipsamplesinto16framesusing
featurespace.Toimplementitmoreeffectivelyandefficiently,we thetimeinterpolationmethodin[44,45].
calculateEC-STFLlossinthemini-batchbecauseoflimitedmem- EvaluationMetric.Wechoosetwometrics[34]widelyused
ory.Besides,wenotethatsampleunbalancewidelyexistsinthe inexistingresearchesforevaluatingtheunbalancedproblems,i.e.,
FER task [9, 15, 22, 23], which leading the classifiers prefer the theunweightedaveragerecall(UAR,i.e.,theaccuracyperclassdi-
emotionswithmoresamplesandignoringtheemotionswithfewer videdbythenumberofclasseswithoutconsiderationsofinstances
samples.TheFERtaskinourDFEWdatabasealsofacesthistrou- perclass)andweightedaveragerecall(WAR,i.e.,accuracy).They
ble.Consideringthat,wedeveloptheEC-STFLlossbyaddingdy- areappropriatefortheFERWtask.TheUARmetricindicatesthe
namicweightstobalancedifferentemotionsâĂŹlossintheupdate average accuracy of different facial expressions, and we can ad-
progressofbatchloss,andextendEC-STFLlossasfollows, equately evaluate the performance of predicting emotions with
fewsamplesusingtheUARresults.TheWARmetricindicatesthe
recognitionaccuracyofoverallexpressions.Wehopetoimprove
kxi−xjk
LEC−STFL=
1≤i,j≤n, Íxj∈N{xi} kxN i−x xi
jk
(9)
m frao
I
md me el wps’
l
oep rme kr [ef 3o
n
1r
t
]m
a
tta oin
o
ic mne pDb lo
ee
mt th
a
eii nln
s
t.U
aIn
lA
lt
mR
hi
oa
s
dn
p
ed
a
lspW
.e
AA
r,
lR
w
lmm
e
oee
m
dtr epi lc sls
o
a.
y reth tre aP iny eT dor oc nh
1≤i,j≤n,xj<N{xi}
Nxj
12Gmemory’sTitanXpwithanexcellentinitiallearningratepro-
Í videdbythegridsearchstrategy.Andthelearningratereducedata
whereN{xi}isthesetofthesamesingle-labeledemotionanno-
rateof10×whenlosssaturated.First,wetrainmodelsfromscratch
tationwithxi inmini-batch,Nxi isthesetsizeofN{xi},andnis
topresentthebenchmarks.Batchsizeissetto24,whichisthemax
themini-batchsize.CreatingthedynamicweightsbyNxi andNxj,
operationalbatchsizeofC3D[37]onTitanXp.Wesettrade-offco-
EC-STFLadjustsandbalancesthelossesofdifferentemotionsin
efficientλofmodelswithEC-STFLto10,andtrade-offcoefficient
eachmini-batch,hence alleviatetheimbalanceissueofFERtask
of center loss to 1×10−4 according to [40]. Second, we further
tosomedegree.
discuss EC-STFL about the batch size and trade-off coefficient λ
We adopt joint supervision for training softmax loss and our
basedonC3D[37]and3DResnet18[12].Theseexperimentsare
EC-STFLlosstoobtainthediscriminativespatiotemporalfeatures.
The total objective function expressed as L = Ls +λLEC−STFL, conductedontwoTitanXp.Third,wemakecross-databasetrans-
ferexperiments.Wefinetunesomeoff-the-shelfmodelsinitilized
whereLs denotessoftmaxlossandhyper-parameterλisacoeffi-
byweights providedbyotherresearchers withthebestlearning
cientusedtotrade-offLs andLEC−STFL.Notethat,wedropthe
rate.
backwardstepwhenLEC−STFL hasnomeaning, i.e., mini-batch
onlycontainssampleswithonekindofemotion.
4 EXPERIMENTS 4.2 ExperimentalResults
Inthissection,wegiveanexperimentalsetupforbenchmarkfirst, Baseline System.The existing spatiotemporal neural networks
including datapreprocessing, experimental protocol,and evalua- basedonRGBframescanbemainlycategorizedintotwogroups:
tionmetric.Thenweconductextensivespatiotemporalneuralnet- the 3D convolutional neural networks and CNN-RNN networks.
workmethodsfortheinvestigations ofourDFEWdatabase,and Inthispaper,weconductfive3DCNNmodels,i.e.,C3D[37],I3D-
thesenetworkswithEC-STFLlossfortheverification.Finally,we RGB[2],R3D18[38],3DResnet18[12],P3D[32],andtwoCNN-
maketransferexperimentsfromsomewidelyusedactiondatabases RNN models,i.e., VGG11+LSTM and Resnet18+LSTM for bench-
andourDFEWdatabasetoAFEWdatabase,toverifyDFEWcan marks.VGG11[35]andResnet18[13]areslightlymodifiedtofit
extract adequate and efficient transfer knowledge for the FERW theinputsizeof112×112.Theclassificationresultsareshownin
task. Table4.Table 4: Comparsion of the seven basic emotion classification performance of C3D, P3D, R3D18, 3D Resnet18, I3D-RGB,
VGG11+LSTM,Resnet18+LSTMonDFEWdatabase.ThemetricsincludeUAR(unweightedaveragerecall)andWAR(weighted
averagerecall).
Emotions Metric
Model
Happy Sad Neutral Angey Surprise Disgust Fear UAR WAR
C3D[37] 75.17 39.49 55.11 62.49 45.00 1.38 20.51 42.74 53.54
P3D[32] 74.85 43.40 54.18 60.42 50.99 0.69 23.28 43.97 54.47
R3D18[38] 79.67 39.07 57.66 50.39 48.26 3.45 21.06 42.79 53.22
3DResnet18[12] 73.13 48.26 50.51 64.75 50.10 0.00 26.39 44.73 54.98
I3D-RGB[2] 78.61 44.19 56.69 55.87 45.88 2.07 20.51 43.40 54.27
VGG11+LSTM[11,14,35] 76.89 37.65 58.04 60.70 43.70 0.00 19.73 42.39 53.70
Resnet18+LSTM[11,13,14] 78.00 40.65 53.77 56.83 45.00 4.14 21.62 42.86 53.08
Table 5: Expression recognition performance of different ofourknowledge, therecognition ofdisgust emotionis really a
methodswithandwithoutEC-STFLonDFEWdatabase. hardproblemintheFERWtask.
EC-STFL.Toacquiremorediscriminativefeatures,wedesign
the EC-STFL and incorporateit with some off-the-shelf 3D con-
Metric
Model volutionalneural networksand CNN-RNN networks.Theexper-
UAR WAR
iment results withand withoutEC-STFL are detailed in Table 5.
C3D 42.74 53.54 Wecanfind thatallEC-STFLbasedmodelsshow betterrecogni-
C3D,EC-STFL 45.10 55.50 tionperformancethan thosewithoutthis module.Our EC-STFL
P3D 43.97 54.47 canpromotetheUARandWARbyanaverageof1.61percentage
P3D,EC-STFL 45.22 56.48 pointsand2.08percentagepoints,respectively.Whatismore,com-
paringwiththeother modelsfromTable5,we canfind that 3D
R3D18 42.79 53.22
Resnet18withEC-STFLachievesthebestUARandWARresults.
R3D18,EC-STFL 45.05 56.19
3DResnet18 44.73 54.98
3DResnet18,EC-STFL 45.35 56.51
I3D-RGB 43.40 54.27
I3D-RGB,EC-STFL 45.05 56.19
VGG11+LSTM 42.39 53.70
VGG11+LSTM,EC-STFL 44.78 56.25
Resnet18+LSTM 42.86 53.08
Resnet18+LSTM,EC-STFL 43.60 54.72
It is seen from Table 4 that P3D [32] achieves the best WAR
at54.47%,and3DResnet18[12]achievesthebestUARat44.73%
amongallnetworks.ItisaninterestingfindingthatbothUARand
WAR attained by 3D CNN models instead of CNN-RNN models.
Amongseven typesof emotions, 3DCNN better predictshappy,
sad,angry,surprise,andfear emotions,whileCNN-RNNmodels
betteratneuralanddisgustemotions.Onepossiblereasonisthat
modelslearnfeatureexistingpreference.FromTable4,wecanalso
findthatitiseasiertoclassifythehappyemotionwhileharderto
thedisgust.Wecanalsofindthathappyemotionismorecomfort-
abletobeclassifiedwhilethedisgustismuchhardertobewellpre-
dicted.Itmayresultfromtherelativelylowvarianceofintra-class
facialfeaturesforthehappyemotionwhilesignificantvariancefor
Figure 3: The confusion matrices of selected methods
thedisgustemotion,orfewersamplesofthedisgust.Infact,fewer
with and without EC-STFL. (a)C3D, (b)C3D with EC-STFL,
disgustsamplesmeanmoreseriousimbalanceproblem,whichisa
(c)3DResnet18,(d)3DResnet18withEC-STFL.
widelyexistedproblemleadingthelousyperformance.TothebestWe provide the recognition performance of different emotion 66 05
55
detailed by confusion matrices in Fig. 3, to further discuss clas- 50
45
sificationdifferencesbetweenmodelswithandwithoutEC-STFL. 34 50
30
DisplayedinFig.3,EC-STFLimprovestherecallratesoftheC3D 25
20
modelforhappy,sad,surprise,disgust,andfearemotionby0.7%, 11 05
5
9.77%,0.95%,2.07%,and4.32%,respectively.EC-STFLimprovesthe 0
0.1 0.3 0.5 1 1.52 3 5 810
recallrateofthe3DResnet18 modelforhappy,sad,neutral,dis-
gustby6.05%,0.79%,7.34%,2.76%,respectively.Resultsaregiven
in Fig. 3 show thatourEC-STFL bothimprove therecall rateof
happy,sad,disgustfortheC3Dand3DResnet18.
Figure4:Thedistributionofdeeplyfeaturesin(a)C3Dand
(b)C3DwithEC-STFL,whosefeaturedimensionisreduced
bytSNE.Ascanbeseen,EC-STFLhelpsthelearnedfeatures
morediscriminative.
ForabetterunderstandingofthelearnedfeaturesbyEC-STFL,
weutilizeanon-linearmappingmethod,i.e.,t-SNE[27,39],tovi-
sualizethelearnedfeaturesona2Dplane,asshowninFig.4.Com-
paredwiththemodelshavenoEC-STFLmodule,weobservethat
thefeatureslearnedbyEC-STFLshowthemoresignificantinter-
classdistancebetweendifferent classes;hencethesamplesshow
abetteraggregationeffect.ItsuggeststhatourproposedEC-STFL
hastheabilitytopromotebetterfeaturerepresentation.
ThecompetitorofEC-STFLismainlythelossinspired bythe
ideaofclustering,e.g.,thewell-knownâĂĲcenterlossâĂİ[40].In
thispaper,weconductthecomparsionexperimentsbasedontwo
spatiotemporal models, i.e., C3D and 3D Resnet18. Table 6 con-
tains the comparsion of center loss and EC-STFL. As is evident
fromtheTable6thatEC-STFLandcenter lossarebothimprove
theclassificationperformanceofmodelspurelyusecrossentropy
loss.Furthermore,theEC-STFLperformsbetterthancenterloss,
andachievesthebestUARandWAR.
Hyper-parameters Discussion. The trade-off hyperparame-
terλandbatchsizemaffecttheperformanceofEC-STFL,which
arebothessentialtoEC-STFL.Soweconductexperimentstoeval-
uatemodels’sensitiveness basedonC3Dand3DResnet18inthe
fd1datasplit.Inthefirstexperiment,wefixbatchsizem=24and
varyλ ∈{1,3,5,10,15,20,30,50,80,100}.Itisapparentthatprop-
erlychoosingthevalueofλcanimprovetheverificationaccuracy
ofthelearnedfeatures.Inthesecondexperiment,wefixλ=10and
varybatchsizem ∈ {18,24,30,36,42,48}.TheWARoraccuracy
resultsarevisibleinFig.5and Fig. 6,respectively. Likewise, the
verificationperformanceofEC-STFLbasedmodelsremainlargely
stableacrossawiderangeofbatchsizes.
)%(ycaruccA
65 60
55
50
45
40 35
30
25
20
15 10
5
101 0
0.1 0.3 0.5 1 1.52 3 5 810
(a) C3D,EC-STFL
)%(ycaruccA
101
(b) 3DResnet18,EC-STFL
Figure5:Thesensitiveexperimentsresultsoftrade-offpa-
rameterfortheproposedEC-STFLframework.(a)C3Dwith
EC-STFL,(b)3DResnet18withEC-STFL.Thescaleoftrade-
offparameterisλ∈{1,3,5,10,15,20,30,50,80,100}.
65
60
55
50
45
40
35
30
25
20
15
10
5
0
18 24 30 36 42 48
batch
)%(ycaruccA
65
60
55
50
45
40
35
30
25
20
15
10
5
0
18 24 30 36 42 48
batch
(a) C3D,EC-STFL
)%(ycaruccA
(b) 3DResnet18,EC-STFL
Figure6:Thesensitiveexperimentsresultsofbatchsizefor
theproposedEC-STFLframework.(a)C3DwithEC-STFL,(b)
3D Resnet18 with EC-STFL.The scale of batch size ism ∈
{18,24,30,36,42,48}.
4.3 TransferLearning
WehypothesizethattheDFEWdatabasewouldcontributetoclip-
basedemotionclassificationmodels’transferlearningperformance
onreal-lifeapplications.Toverifythishypothesis,weconductex-
tensivetransferlearningexperimentsfromwidelyusedactiondata-
basesandourDFEWdatabasetotheAFEW[5]database.Theac-
tiondatabasesincludeUCF101[36],Sports1M[16],Kinect700[2],
andMomentsInTime[29].Weselecttwospatiotemporalneural
networksandtheirEC-STFLversion,i.e.,C3D,3DResnet18,and
C3DwithEC-STFL,3DResnet18withEC-STFL.
Weinitializemodelswiththecorrespondingpre-trainedweights
trainedfromactiondatabasesprovidedbyotherresearchersand
ourDFEWdatabaserespectively,forexample,C3DandC3Dwith
EC-STFLusepre-trainedweightsofC3Dmodel.Thenfinetuneall
the layers of network on the AFEW database at a best learning
ratesearchedbygridstrategy.Notethat,wechoosemodels’pre-
trainedweightsonourDFEWdatabasebasedontheseconddata
splitandthefifthdatasplit,denotedbyfd2andfd5forshort,re-
spectively. We use WAR metric as the evaluation and show the
transferresultsinTable7.Wefoundthatinitialweightsprovided
bytheDFEWdatabaseshowabettertransferlearningperformance
thantheactiondatabases.Wefurthercompareourtransferresults
withthosestate-of-the-artsmethods.AsresultsillustratedinTa-
ble8,transferred3DResnet18improvethestate-of-the-artmethod
onWAR about2 percent. Inthis way, wecan concludethat our
DFEWdatabaseisusefulfordevelopingexcellentemotionpredic-
tionmodelsinreal-lifeapplications.Table6:ComparisonofEC-STFLandcenterlossonDFEWdatabase.
Emotions Metric
Model
Happy Sad Neutral Angry Surprise Disgust Fear UAR WAR
C3D 75.17 39.49 55.11 62.49 45.00 1.38 20.51 42.74 53.54
C3D,centerloss 75.62 44.67 54.18 63.14 42.21 2.07 22.17 43.44 54.17
C3D,EC-STFL 75.87 49.26 54.81 61.53 45.95 3.45 24.83 45.10 55.50
3DRenset18 73.13 48.26 50.51 64.75 50.10 0.00 26.39 44.73 54.98
3DResnet18,centerloss 78.49 44.30 54.89 58.40 52.35 0.69 25.28 44.91 55.48
3DResnet18,EC-STFL 79.18 49.05 57.85 60.98 46.15 2.76 21.51 45.35 56.51
Table7:ThetransferlearningperformanceonAFEW7.0.
Finetunedmodels
Pretrained
C3D C3D,EC-STFL 3DResnet18 3DResnet18,EC-STFL
Sports1M 41.78 44.91 - -
UCF101 41.25 42.34 - -
Kinect700 - - 49.35 49.61
Kinect700+MomentsInTime - - 49.35 49.35
DFEW,fd2 44.91 45.56 53.00 53.26
DFEW,fd5 49.87 49.87 49.61 49.66
Table8:Comparisonof3DResnet18modelâĂŹstransferre- resultsshowed thatourDFEWisapromisingunconstrained dy-
sultswithotherstate-of-the-artmethodsonAFEW7.0. namicfacialexpressiondatabaseandtheproposedEC-STFLframe-
workcanimprovetheperformanceofspatiotemporaldeepneural
networksincopingwithdynamic FERinthewild.Inthefuture,
Model WAR
we willcontinue tomaintain DFEWbycollectingmoresamples
Luetal.[26] 45.31 and providing more types of label information such that DFEW
Fanetal.[9] 45.43 canbetterpromotetheprogressofFERresearch.
Huetal.[15] 46.48
Fanetal.[7] 48.04 ACKNOWLEDGMENTS
Liuetal.[23] 51.44
ThisworkwassupportedinpartbytheNationalKeyResearchand
3DResnet18,DFEWfd2 53.00
DevelopmentProgramofChinaunderGrant2018YFB1305200,in
3DResnet18,EC-STFL,DFEWfd2 53.26
partbytheNationalNaturalScienceFoundationofChinaunder
Grant61921004,Grant61902064,andGrant81971282,andinpart
bytheFundamental Research FundsfortheCentral Universities
underGrant2242018K3DN01.
5 CONCLUSIONSANDFUTUREWORK REFERENCES
Inthispaper,wehavepresentedanewlarge-scaleunconstrained [1] CFabianBenitezquiroz,RamprakashSrinivasan,andAleixMMartinez.2016.
dynamicfacialexpressiondatabase,DFEW,andproposedanovel EmotioNet:AnAccurate,Real-TimeAlgorithmfortheAutomaticAnnotation
ofaMillionFacialExpressionsintheWild.(2016),5562–5570.
spatiotemporaldeepfeaturelearningframework,EC-STFL,todeal [2] JoaoCarreira,EricNoland,ChloeHillier,andAndrewZisserman.2019.Ashort
withdynamicFERinthewild.Tothebestofourknowledge,our noteonthekinetics-700humanactiondataset. arXivpreprintarXiv:1907.06987
(2019).
DFEWhasthelargestnumberofsamplescomparedwithexisting
[3] CharlesDarwinandPhillipProdger.1998.Theexpressionoftheemotionsinman
databasesofdynamicfacialexpressioninthewild,whichcontain- andanimals.OxfordUniversityPress,USA.
ing 16,372video clipsextracted fromover 1500different movies. [4] AbhinavDhall.2019.EmotiW2019:AutomaticEmotion,EngagementandCohe-
sionPredictionTasks.In2019InternationalConferenceonMultimodalInteraction.
Moreimportantly,DFEWhasprovidedthereliabledistributionin-
546–550.
formation of 7 basic expressions for all the video clips because [5] AbhinavDhall,RolandGoecke,SimonLucey,andTomGedeon.2012.Collecting
10 well-trained annotators independently annotate each sample large,richlyannotatedfacial-expressiondatabasesfrommovies.IEEEmultime-
dia3(2012),34–41.
of DFEW. We also conducted extensive baseline experiments on [6] PaulEkmanandWallaceVFriesen.1971.Constantsacrossculturesintheface
DFEWunderthewell-designedprotocolbyusingwell-performing andemotion.Journalofpersonalityandsocialpsychology17,2(1971),124.
[7] YingruoFan,JacquelineCKLam,andVictorOKLi.2018.Video-basedemotion
spatiotemporaldeeplearningmethodsaswellastheproposedEC-
recognitionusingdeeply-supervisedneuralnetworks.InProceedingsofthe20th
STFL framework and deeply discussed the results. Experimental ACMInternationalConferenceonMultimodalInteraction.584–588.[8] YingruoFan,VictorLi,andJacquelineCKLam.2020.FacialExpressionRecogni- [32] ZhaofanQiu,TingYao,andTaoMei.2017. Learningspatio-temporalrepresen-
tionwithDeeply-SupervisedAttentionNetwork.IEEETransactionsonAffective tationwithpseudo-3dresidualnetworks.InproceedingsoftheIEEEInternational
Computing(2020). ConferenceonComputerVision.5533–5541.
[9] YinFan,XiangjuLu,DianLi,andYuanliuLiu.2016.Video-basedemotionrecog- [33] M.Inc.Face++research.[n.d.].toolkit.www.faceplusplus.com.
nitionusingCNN-RNNandC3Dhybridnetworks.InProceedingsofthe18th [34] Bjorn Schuller, Bogdan Vlasenko, Florian Eyben, Martin Wöllmer, Andre
ACMInternationalConferenceonMultimodalInteraction.445–450. Stuhlsatz,AndreasWendemuth,andGerhardRigoll.2010.Cross-corpusacous-
[10] JosephLFleiss.1971. Measuringnominalscaleagreementamongmanyraters. ticemotionrecognition:Variancesandstrategies.IEEETransactionsonAffective
Psychologicalbulletin76,5(1971),378. Computing1,2(2010),119–131.
[11] FelixAGers,JürgenSchmidhuber,andFredCummins.1999.Learningtoforget: [35] KarenSimonyanandAndrewZisserman.2014. Verydeepconvolutionalnet-
ContinualpredictionwithLSTM.(1999). worksforlarge-scaleimagerecognition.arXivpreprintarXiv:1409.1556(2014).
[12] KenshoHara,HirokatsuKataoka,andYutakaSatoh.2017. Learningspatio- [36] KhurramSoomro,AmirRoshanZamir,andMubarakShah.2012. UCF101:A
temporalfeatureswith3Dresidualnetworksforactionrecognition.InProceed- datasetof101humanactionsclassesfromvideosinthewild. arXivpreprint
ingsoftheIEEEInternationalConferenceonComputerVisionWorkshops.3154– arXiv:1212.0402(2012).
3160. [37] DuTran,LubomirBourdev,RobFergus,LorenzoTorresani,andManoharPaluri.
[13] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016. Deepresidual 2015.Learningspatiotemporalfeatureswith3dconvolutionalnetworks.InPro-
learningforimagerecognition.InProceedingsoftheIEEEconferenceoncomputer ceedingsoftheIEEEinternationalconferenceoncomputervision.4489–4497.
visionandpatternrecognition.770–778. [38] DuTran,HengWang,LorenzoTorresani,JamieRay,YannLeCun,andManohar
[14] SeppHochreiterandJürgenSchmidhuber.1997.Longshort-termmemory.Neu- Paluri.2018.Acloserlookatspatiotemporalconvolutionsforactionrecognition.
ralcomputation9,8(1997),1735–1780. InProceedingsoftheIEEEconferenceonComputerVisionandPatternRecognition.
[15] PingHu,DongqiCai,ShandongWang,AnbangYao,andYurongChen.2017. 6450–6459.
Learningsupervisedscoringensembleforemotionrecognitioninthewild.In [39] LaurensVanDerMaaten.2014.Acceleratingt-SNEusingtree-basedalgorithms.
Proceedingsofthe19thACMinternationalconferenceonmultimodalinteraction. TheJournalofMachineLearningResearch15,1(2014),3221–3245.
553–560. [40] YandongWen,KaipengZhang,ZhifengLi,andYuQiao.2016.Adiscriminative
[16] AndrejKarpathy,GeorgeToderici,SankethShetty,ThomasLeung,RahulSuk- featurelearningapproachfordeepfacerecognition.InEuropeanconferenceon
thankar,andLiFei-Fei.2014.Large-scalevideoclassificationwithconvolutional computervision.Springer,499–515.
neuralnetworks.InProceedingsoftheIEEEconferenceonComputerVisionand [41] GuoyingZhaoandMattiPietikainen.2007.Dynamictexturerecognitionusing
PatternRecognition.1725–1732. localbinarypatternswithanapplicationtofacialexpressions.IEEEtransactions
[17] DimitriosKollias,PanagiotisTzirakis,MihalisANicolaou,AthanasiosPapaioan- onpatternanalysisandmachineintelligence29,6(2007),915–928.
nou,GuoyingZhao,BjörnSchuller,IreneKotsia,andStefanosZafeiriou.2019. [42] WenmingZheng,HaoTang,ZhouchenLin,andThomasSHuang.2010.Emotion
Deepaffectpredictionin-the-wild:Aff-wilddatabaseandchallenge,deeparchi- recognitionfromarbitraryviewfacialimages.(2010),490–503.
tectures,andbeyond. InternationalJournalofComputerVision127,6-7(2019), [43] WenmingZheng,XiaoyanZhou,CairongZou,andLiZhao.2006. Facialex-
907–929. pressionrecognitionusingkernelcanonicalcorrelationanalysis(KCCA). IEEE
[18] JeanKossaifi,GeorgiosTzimiropoulos,SinisaTodorovic,andMajaPantic.2017. TransactionsonNeuralNetworks17,1(2006),233–238.
AFEW-VAdatabaseforvalenceandarousalestimationin-the-wild. Imageand [44] ZihengZhou,XiaopengHong,GuoyingZhao,andMattiPietikäinen.2013. A
VisionComputing65(2017),23–36. compactrepresentationofvisualspeechdatausinglatentvariables.IEEEtrans-
[19] JiyoungLee,SeungryongKim,SunokKim,JunginPark,andKwanghoonSohn. actionsonpatternanalysisandmachineintelligence36,1(2013),1–1.
2019.Context-AwareEmotionRecognitionNetworks.(2019). [45] ZihengZhou,GuoyingZhao,andMattiPietikäinen.2011. Towardsapractical
[20] ShanLiandWeihongDeng.2018. Reliablecrowdsourcinganddeeplocality- lipreadingsystem.InCVPR2011.IEEE,137–144.
preservinglearningforunconstrainedfacialexpressionrecognition.IEEETrans-
actionsonImageProcessing28,1(2018),356–370.
[21] ShanLi,WeihongDeng,andJunPingDu.2017.Reliablecrowdsourcinganddeep
locality-preservinglearningforexpressionrecognitioninthewild.InProceed-
ingsoftheIEEEconferenceoncomputervisionandpatternrecognition.2852–2861.
[22] SunanLi,WenmingZheng,YuanZong,ChengLu,ChuangaoTang,Xingxun
Jiang,JiatengLiu,andWanchuangXia.2019. Bi-modalityFusionforEmotion
RecognitionintheWild.In2019InternationalConferenceonMultimodalInterac-
tion.589–594.
[23] ChuanheLiu,TianhaoTang,KuiLv,andMinghaoWang.2018. Multi-feature
basedemotionrecognitionforvideoclips.InProceedingsofthe20thACMInter-
nationalConferenceonMultimodalInteraction.630–634.
[24] MengyiLiu,ShiguangShan,RuipingWang,andXilinChen.2014.Learningex-
pressionletsonspatio-temporalmanifoldfordynamicfacialexpressionrecogni-
tion.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecog-
nition.1749–1756.
[25] Xin Liu, Meina Kan, Wanglong Wu, Shiguang Shan, and Xilin Chen. 2016.
VIPLFaceNet:AnOpenSourceDeepFaceRecognitionSDK. FrontiersofCom-
puterScience(FCS)(2016).
[26] ChengLu,WenmingZheng,ChaolongLi,ChuangaoTang,SuyuanLiu,Simeng
Yan,andYuanZong.2018.Multiplespatio-temporalfeaturelearningforvideo-
basedemotionrecognitioninthewild.InProceedingsofthe20thACMInterna-
tionalConferenceonMultimodalInteraction.646–652.
[27] LaurensvanderMaatenandGeoffreyHinton.2008. Visualizingdatausingt-
SNE.Journalofmachinelearningresearch9,Nov(2008),2579–2605.
[28] AliMollahosseini,BehzadHasani,andMohammadHMahoor.2019.AffectNet:
ADatabaseforFacialExpression,Valence,andArousalComputingintheWild.
IEEETransactionsonAffectiveComputing10,1(2019),18–31.
[29] Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ramakrishnan,
SarahAdelBargal,TomYan,LisaBrown,QuanfuFan,DanGutfreund,CarlVon-
drick,etal.2019.Momentsintimedataset:onemillionvideosforeventunder-
standing. IEEEtransactionsonpatternanalysisandmachineintelligence42,2
(2019),502–508.
[30] BowenPan,ShangfeiWang,andBinXia.2019. OccludedFacialExpression
RecognitionEnhancedthroughPrivilegedInformation.InProceedingsofthe27th
ACMInternationalConferenceonMultimedia.566–573.
[31] AdamPaszke,SamGross,SoumithChintala,GregoryChanan,EdwardYang,
ZacharyDeVito,ZemingLin,AlbanDesmaison,LucaAntiga,andAdamLerer.
2017.Automaticdifferentiationinpytorch.(2017)."
71,73,Diagnostic features of emotional expressions are processed preferentially,"['E Scheller', 'C Büchel', 'M Gamer']",2012,135,Karolinska Directed Emotional Faces,classifier,control group in an emotion classification task revealed a  faces that were shown during  the experiment were selected from several picture sets (The Karolinska directed emotional faces,No DOI,PloS one,https://pubmed.ncbi.nlm.nih.gov/22848607/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,nih.gov,
72,74,Discovering hidden factors of variation in deep networks,"['B Cheung', 'JA Livezey', 'AK Bansal']",2014,225,Toronto Face Database,"classification, deep learning, machine learning, neural network","Deep learning has enjoyed a great deal of success because of its ability to  handwritten  digit database, the Toronto Faces Database (TFD) and the Multi-PIE dataset by generating",No DOI,arXiv preprint arXiv …,https://arxiv.org/abs/1412.6583,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"Discovering Hidden Factors of Variation in Deep
Networks
BrianCheung∗ JesseA.Livezey∗
RedwoodCenterforTheoreticalNeuroscience RedwoodCenterforTheoreticalNeuroscience
UniversityofCalifornia,Berkeley UniversityofCalifornia,Berkeley
Berkeley,CA94720,USA Berkeley,CA94720,USA
bcheung@berkeley.edu jesse.livezey@berkeley.edu
ArjunK.Bansal BrunoA.Olshausen
NervanaSystems,Inc. RedwoodCenterforTheoreticalNeuroscience
SanDiego,CA92121,USA UniversityofCalifornia,Berkeley
arjun@nervanasys.com Berkeley,CA94720,USA
baolshausen@berkeley.edu
Abstract
Deep learning has enjoyed a great deal of success because of its ability to learn
useful features for tasks such as classification. But there has been less explo-
rationinlearningthefactorsofvariationapartfromtheclassificationsignal. By
augmenting autoencoders with simple regularization terms during training, we
demonstrate that standard deep architectures can discover and explicitly repre-
sent factors of variation beyond those relevant for categorization. We introduce
a cross-covariance penalty (XCov) as a method to disentangle factors like hand-
writing style for digits and subject identity in faces. We demonstrate this on the
MNIST handwritten digit database, the Toronto Faces Database (TFD) and the
Multi-PIEdatasetbygeneratingmanipulatedinstancesofthedata. Furthermore,
wedemonstratethesedeepnetworkscanextrapolate‘hidden’variationinthesu-
pervisedsignal.
1 Introduction
One of the goals of representation learning is to find an efficient representation of input data that
simplifies tasks such as object classification [1] or image restoration [2]. Supervised algorithms
approach this problem by learning features which transform the data into a space where different
classes are linearly separable. However this often comes at the cost of discarding other variations
such as style or pose that may be important for more general tasks. On the other hand, unsuper-
vised learning algorithms such as autoencoders seek efficient representations of the data such that
theinputcanbefullyreconstructed,implyingthatthelatentrepresentationpreservesallfactorsof
variationinthedata. However,withoutsomeexplicitmeansforfactoringapartthedifferentsources
of variation the factors relevant for a specific task such as categorization will be entangled with
otherfactorsacrossthelatentvariables. Ourgoalinthisworkistocombinethesetwoapproachesto
disentangleclass-relevantsignalsfromotherfactorsofvariationinthelatentvariablesinastandard
deepautoencoder.
Previousapproachestoseparatingfactorsofvariationindata,suchascontentvs. style[3]orform
vs. motion [4, 5, 6, 7, 8, 9], have relied upon a bilinear model architecture in which the units
representingdifferentfactorsarecombinedmultiplicatively. Suchanapproachwasrecentlyutilized
∗Authorscontributedequally.
1
5102
nuJ
71
]GL.sc[
4v3856.2141:viXrato separate facial expression vs. identity using higher-order restricted Boltzmann machines [10].
Onedownsideofbilinearapproachesingeneralisthattheyrequirelearninganapproximateweight
tensorcorrespondingtoallthree-waymultiplicativecombinationsofunits. Despitetheimpressive
resultsachievedwiththisapproach,thequestionneverthelessremainsastowhetherthereisamore
straightforward way to separate factors of variation using standard nonlinearities in feedforward
neuralnetworks. Earlierworkby[11]demonstratedclass-irrelevantaspectsinMNIST(style)can
belearnedbyincludingadditionalunsupervisedunitsalongsidesupervisedonesinanautoencoder.
However, their model does not disentangle class-irrelevant factors from class-relevant ones. More
recently, [12] utilized a variational autoencoder in a semi-supervised learning paradigm which is
capableofseparatingcontentandstyleindata. Itisthisworkwhichistheinspirationforthesimple
trainingschemepresentedhere.
Autoencodermodelshavebeenshowntobeusefulforavarietyofmachinelearningtasks[13,14,
15]. The basic autoencoder architecture can be separated into an encoding stage and a decoding
stage. During training, the two stages are jointly optimized to reconstruct the input data from the
output of the decoder. In this work, we propose using both the encoding and decoding stages of
theautoencodertolearnhigh-levelrepresentationsofthefactorsofvariationcontainedinthedata.
The high-level representation (or encoder output) is divided into two sets of variables. The first
set (observed variables) is used in a discriminative task and during reconstruction. The second
set (latent variables) is used only for reconstruction. To promote disentangling of representations
in an autoencoder, we add two additional costs to the network. The first is a discriminative cost
on the observed variables. The second is a novel cross-covariance penalty (XCov) between the
observed and latent variables across a batch of data. This penalty prevents latent variables from
encoding input variations due to class label. [17] proposed a similar penalty over terms in the
product between the Jacobians of observed and latent variables with respect to the input. In our
penalty,thevariableswhichrepresentclassassignmentareseparatedfromthosewhichareencoding
otherfactorsofvariationsinthedata.
Weanalyzecharacteristicsofthislearnedrepresentationonthreeimagedatasets. Intheabsenceof
standardbenchmarktaskforevaluatingdisentanglingperformance,ourevaluationhereisbasedon
examiningqualitativelywhatfactorsofvariationarediscoveredfordifferentdatasets. Inthecaseof
MNIST,thelearnedfactorscorrespondtostylesuchasslantandsize. InthecaseTFDthefactors
correspondtoidentity,andinthecaseofMulti-PIEidentityspecificattributessuchasclothing,skin
tone,andhairstyle.
2 Semi-supervisedAutoencoder
Givenaninputx ∈ RD anditscorrespondingclasslabely ∈ RL foradatasetD,weconsiderthe
classlabeltobeahigh-levelrepresentationofitscorrespondinginput. However,thisrepresentation
isusuallynotinvertiblebecauseitdiscardsmuchofthevariationcontainedintheinputdistribution.
Inordertoproperlyreconstructx,autoencodersmustlearnalatentrepresentationwhichpreserves
allinputvariationsinthedataset.
encoder decoder
x ŷ xˆ
h1 h2 h-3 h-2 h-1
z
Figure 1: The encoder and decoder are combined and jointly trained to reconstruct the inputs and
predicttheobservedvariablesyˆ.
Using class labels, we incorporate supervised learning to a subset of these latent variables trans-
formingthemintoobservedvariables,yˆasshowninFigure1. Inthisframework,theremainingthe
latentvariablezmustaccountfortheremainingvariationofdatasetD. Wehypothesizethislatent
variation is a high-level representation of the input complementary to the observed variation. For
instance,theclasslabel‘5’providedbyywouldnotbesufficientforthedecodertoproperlyrecon-
2structtheimageofaparticular‘5’. Inthisscenario,zwouldencodepropertiesofthedigitsuchas
style,slant,width,etc. toprovidethedecodersufficientinformationtoreconstructtheoriginalinput
image. Mathematically,theencoderF anddecoderGaredefinedrespectivelyas:
{yˆ,z}=F(x;θ) (1)
xˆ=G(y,z;φ) (2)
whereθandφaretheparametersoftheencoderanddecoderrespectively.
2.1 Learning
Theobjectivefunctiontotrainthenetworkisdefinedasthesumeofthreeseperatecostterms.
θˆ,φˆ=argmin (cid:88) ||x−xˆ||2+β(cid:88) y log(yˆ)+γC. (3)
i i
θ,φ
{x,y}∈D i
Thefirsttermisatypicalreconstructioncost(squarederror)foranautoencoder.Thesecondtermisa
standardsupervisedcost(cross-entropy). Whiletherearemanypotentialchoicesforthereconstruc-
tion cost dependingon the distribution of datavector x, for ourexperiments we use squared-error
foralldatasets.Fortheobservedvariables,theformofthecostfunctiondependsonthetypeofvari-
ables(categorical,binary,continuous). Forourexperiments,wehadcategoricalobservedvariables
soweparametrizedthemasone-hotvectorsandcomputeyˆ =softmax(W h2+b ).
yˆ yˆ
ThethirdtermCistheunsupervisedcross-covariance(XCov)costwhichdisentanglestheobserved
andlatentvariablesoftheencoder.
C(yˆ1...N,z1...N)= 1(cid:88) [ 1 (cid:88) (yˆn−y¯ˆ)(zn−z¯ˆ )]2. (4)
2 N i i j j
ij n
TheXCovpenaltytodisentangleyˆandzissimplyasum-squaredcross-covariancepenaltybetween
theactivationsacrosssamplesinabatchofsizeN wherey¯ˆ andz¯ˆ denotemeansoverexamples. n
i j
isanindexoverexamplesandi,j indexfeaturedimensions. Unlikethereconstructionandsuper-
vised terms in the objective, XCov is a cost computed over a batch of datapoints. It is possible to
approximate this quantity with a moving average during training but we have found that this cost
hasbeenrobusttosmallbatchsizesandhavenotfoundanyissueswhentrainingwithmini-batches
assmallasN =50. Itsderivativeisprovidedinthesupplementarymaterial.
Thisobjectivefunctionnaturallyfitsasemi-supervisedlearningframework. Forunlabeleddata,the
multiplier β for the supervised cost is simply set to zero. In general, the choice of β and γ will
dependontheintendedtask. Largerβ willleadtobettertoclassificationperformancewhilelarger
γ tobetterseparationbetweenlatentandobservedfactors.
3 ExperimentalResults
Weevaluateautoencoderstrainedtominimize3onthreedatasetsofincreasingcomplexity.Thenet-
workistrainedusingADADELTA[18]withgradientsfromstandardbackpropagation.Modelswere
implementedinamodifiedversionofPylearn2[19]usingdeconvolutionandlikelihoodestimation
codefrom[20].
3.1 Datasets
MNISTHandwrittenDigitsDatabase
TheMNISThandwrittendigitsdatabase[21]consistsof60,000trainingand10,000testimagesof
handwritten digits 0-9 of size 28x28. Following previous work [22], we split the training set into
50,000samplesfortrainingand10,000samplesasavalidationsetformodelselection.
3TorontoFacesDatabase
TheTorontoFacesDatabase[23]consistsof102,236grayscalefaceimagesofsize48x48. Ofthese,
4,178 are labeled with 1 of 7 different expressions (anger, disgust, fear, happy, sad, surprise, and
neutral). Examples are shown in Figure 2. The dataset also contains 3,784 identity labels which
werenotusedinthispaper. Thedatasethas5foldsoftraining, validationandtestexamples. The
threepartitionsaredisjointandcontainnooverlapinidentities.
anger disgust fear happy sad surprise neutral
Figure2: Left: ExampleTFDimagesfromthetestsetshowing7expressionswithrandomidentity.
Right: ExampleMulti-PIEimagesfromthetestsetshowing3ofthe19cameraposeswithvariable
lightingandidentity.
Multi-PIEDataset
TheMulti-PIEdatasets[24]consistsof754,200high-resolutioncolorimagesof337subjects. Each
subjectwasrecordedunder15cameraposes: 13spacedat15degreeintervalsatheadheight,and2
positionedabovethesubject.Foreachofthesecameras,subjectswereimagedunder19illumination
conditionsandavarietyoffacialexpressions. Wediscardedimagesfromthetwooverheadcameras
due to inconsistencies found in their image. Camera pose and illumination data was retained as
supervisedlabels.
Only a small subset of the images possess facial keypoint information for each camera pose. To
perform a weak registration to appoximately localize the face region, we compute the maximum
bounding box created by all available facial keypoint coordinates for a given camera pose. This
boundingboxisappliedtoallimagesforthatcamerapose. Wethenresizedthecroppedimagesto
48x48pixelsandconverttograyscale.Wedividethedatasetinto528,060training,65,000validation
and60,580testexamples. Splitsweredeterminedbysubjectid. Therefore,thetestsetcontainsno
overlapinidentitieswiththetrainingorvalidationsets. Exampleimagesfromourtestsetareshown
inFigure2.
TheMulti-PIEdatasetcontainssignificantlymorecomplexfactorsofvariationthanMNISTorTFD.
UnlikeTFD,imagesinMulti-PIEincludesmuchmoreofthesubject’sbody. Theweakregistration
alsocausessignificantvariationinthesubject’sheadpositionandscale.
Table1: NetworkArchitectures(Softmax(SM),RectifiedLinear(ReLU))
MNIST TFD ConvDeconvMultiPIE
500ReLU 2000ReLU 20x20x32ConvReLU
500ReLU 2000ReLU 2000ReLU
10SM,2Linear 7SM,793Linear 2000ReLU
500ReLU 2000ReLU 13SM,19SM,793Linear
500ReLU 2000ReLU 2000ReLU
784Linear 2304Linear 2000ReLU
2000ReLU
2000ReLU
48x48x1Deconv
4Table2: MNISTClassificationPerformance
Model Accuracy ModelSelectionCriterion
MNIST 98.35 Reconstuction: β =10,γ =10
ConvMNIST 98.71 Reconstuction: β =10,γ =10
MaxoutMNIST+dropout 99.01 Accuracy: β =100,γ =10
Maxout+dropout[22] 99.06 Accuracy
Table3: TFDClassificationPerformance
Model Accuracy ModelSelectionCriterion
TFD 69.4 Reconstuction: β =10,γ =1e3(Fold0)
ConvTFD 84.0 Accuracy: β =10,γ =1e3(Fold0)
disBM[10] 85.4 Accuracy
CCNET+CDA+SVM[17] 85.0 Accuracy(Fold0)
3.2 ModelPerformace
3.2.1 Classification
As a sanity check, we first show that the our additional regularization term in the cost negligibly
impactsperformancefordifferentarchitecturesincludingconvolution,maxoutanddropout. Tables
2 and 3 show classification results for MNIST and TFD are comparable to previously published
results.Detailsonnetworkarchitectureforthesemodelscanbefoundinthesupplementarymaterial.
3.2.2 LearnedFactorsofVariation
WebeginouranalysisusingtheMNISTdataset.Weintentionallychoosez∈R2forthearchitecture
describedinTable1foreaseinvisualizationofthelatentvariables. AsshowninFigure3a,ztakes
onasuprisinglysimpleisotropicNormaldistributionwithmean0andstandarddeviation.35.
VisualizingLatentVariables
To visualize the transformations that the latent variables are learning, the decoder can be used to
createimagesfordifferentvaluesofz. Wevaryasingleelementz linearlyoverasetintervalwith
i
z fixedto0andyfixedtoone-hotvectorscorrespondingtoeachclasslabelasshowninFigure3b
\i
andc. Movingacrosseachcolumnforagivenrow,thedigitstyleismaintainedastheclasslabels
varies. Thissuggeststhenetworkhaslearnedaclassinvariantlatentrepresentation. Atthecenter
ofz-space,(0,0),wefindthecanonicalMNISTdigitstyle. Movingawayfromthecenter,thedigits
becomemorestylizedbutalsolessprobable. Wefindthisresultisreliablyreproducedwithoutthe
XCov regularization when the dimensionality of z is relatively small suggesting that the network
naturallypreferssuchalatentrepresentationforfactorsofvariationabsentinthesupervisedsignal.
With this knowledge, we describe a method to generate samples from such an autoencoder with
competativegenerativeperformanceinthesupplementarymaterial.
MovingFromLatentSpacetoImageSpace
Followingthelayerofobservedandlatentvariables{y,z},therearetwoadditionallayersofactiva-
tionsh−3,h−2 beforetheoutputofthemodelintoimagespace. Tovisualizethefunctionofthese
layers, we compute the Jacobian of the output image, xˆ, with respect to the activation of hidden
units, hk, in a particular layer. This analysis provides insight into the transformation each unit is
applyingtotheinputxtogeneratexˆ. Morespecifically,itisameasureofhowasmallperturbation
ofaparticularunitinthenetworkaffectstheoutputxˆ:
∂xˆ
∆xˆk = i∆h . (5)
i ∂hk j
j
5a b c
-2σ -2σ
z1 z2
2σ 2σ
Figure3: a: Histogramoftestsetz variables. b: GeneratedMNISTdigitsformedbysettingz to
2
zeroandvaryingz . c: GeneratedMNISTdigitsformedbysettingz tozeroandvaryingz . σwas
1 1 2
calculatedfromthevariationonthetestset.
Here,iistheindexofapixelintheoutputofthenetwork,jistheindexofahiddenunit,andkisthe
layernumber. WeremovehiddenunitswithzeroactivationfromtheJacobiansincetheirderivatives
arenotmeaningful. AsummaryoftheresultsareplottedinFigure4.
TheJacobianwithrespecttothezunitsshowninFigure4blocallymirrorthetransformationsseen
in Figure 3b and c further confirming the hypothesis that this latent space smoothly controls digit
style. Theslantedstylegeneratedasz approaches2σ inFigure3ciscreatedbyapplyingagabor-
2
likefiltertoverticallyorientedpartsofthedigitasshowninthesecondcolumnofFigure4b.
Ratherthanviewingeachunitinthenextlayerindividually,weanalyzethesingularvaluespectrum
of the Jacobian. For h−3, the spectrum is peaked and thus there are a small number of directions
with large effect on the image output, so we plot singular vectors with largest singular value. For
alldigitsbesides‘1’,thefirstcomponentseemstocreateatemplatedigitandtheothercomponets
makesmallstyleadjustments. Forh−2, thespectrumismoredegenerate, sowechoosearandom
setofcolumnsfromtheJacobiantoplotwhichwillbetterrepresentthelayer’sfunction. Wenotice
thatforeachlayermovingfromtheencodertotheoutput,theircontributionsbecomemorespatially
localizedandlesssemanticallymeaningful.
a b c d e
Figure4:a:Jacobiansweretakenatactivationvaluesthatleadtotheseimages. zwassettozerofor
eachdigitclass. b: Gradientsofthedecoderoutputwithrespecttoz. c: Singularvectorsfromthe
Jacobian from the activations of the first layer after {y, z}. d: Column vectors from the Jacobian
from the the activations of the second layer after {y, z}. Note that units in the columns for d are
notneccesarilythesameunit. e: Plotsofthenormalizedsingularvaluesforred: theJacobianswith
respect to {y, z}, blue: the Jacobians with respect to the activations of the first layer after {y, z}
(h−3 inFigure1),andgreen: theJacobianswithrespecttotheactivationsofthesecondlayerafter
{y,z}(h−2inFigure1).
63.3 GeneratingExpressionTransformations
WedemonstratesimilarmanipulationsontheTFDdatasetwhichcontainssubstantiallymorecom-
plex images than MNIST and has far fewer labeled examples. After training, we find the latent
representationzencodesthesubject’sidentity,amajorfactorofvariationinthedatasetwhichisnot
representedbytheexpressionlabels. Theautoencoderisabletochangetheexpressionwhilepre-
servingidentityoffacesneverbeforeseenbythemodel. Wefirstinitialize{yˆ,z}withanexample
fromthetestset. Wethenreplaceyˆwithanewexpressionlabelyˆ(cid:48) feeding{yˆ(cid:48),z}tothedecoder.
Figure 5 shows the results of this process. Expressions can be changed while leaving other facial
features largely intact. Similar to the MNIST dataset, we find the XCov penalty is not necessary
whenthedimensionalityofzislow(<10). Butconvergenceduringtrainingbecomesfarmorediffi-
cultwithsuchabottleneck. WeachievemuchbetterreconstructionerrorwiththeXCovpenaltyand
ahigh-dimensionalz. TheXCovpenaltysimplypreventsexpressionlabelvariationfrom‘leaking’
intothelatentrepresentation. Figure5showsthedecoderisunaffectedbychangesinywithoutthe
XCovpenaltybecausetheexpressionvariationisdistributedacrossthehundredsofdimensionsof
z.
3.4 ExtrapolatingObservedVariables
Previously,weshowedtheautoencoderlearnsasmoothcontinuouslatentrepresentation. Wefinda
similarresultfortheobservedexpressionvariablesdespiteonlybeingprovidedtheirdiscreteclass
labels. In Figure 6, we go a step further. We try values for yˆ well beyond those that the encoder
could ever output with a softmax activation (0 to 1). We vary the expression variable given to the
decoder from -5 to 5. This results in greatly exagerated expressions when set to extreme positive
valuesasseeninFigure 6. Remarkably, settingthevariablestoextremenegativevalues resultsin
‘negative‘facialexpressionsbeingdisplayed. Thesenegativefacialexpressionsareabstractoppo-
sitesoftheirpositivecounterparts. Whentheeyesareopeninoneextreme, theyareclosedinthe
oppositeextreme. Thisisconsistentregardlessoftheexpressionlabelandholdstrueforotherab-
stractfacialfeaturessuchasopen/closedmouthandsmiling/frowningface.Thedecoderhaslearned
a meaningful extrapolation of facial expression structure not explicitly present in the labeled data,
creatingasmoothsemanticallysensiblespaceforvaluesoftheobservedvariablescompletelyabsent
fromtheclasslabels.
original anger disgust fear happy sad surprise neutral no covariance cost
Figure 5: Left column: Samples from the test set displaying each of the 7 expressions. The
expression-labeled columns are generated by keeping the latent variables z constant and changing
y(expression). Therightmostsetoffacesarefromamodelwithnocovarriancecostandshowcase
theimportanceofthecostindisentanglingexpressionfromthelatentzvariables.
3.5 ManipulatingMultipleFactorsofVariation
For Multi-PIE, we use two sets of observed factors (camera pose and illumination). As shown in
Table1,wehavetwosoftmaxlayersattheendoftheencoder. Thefirstencodesthecameraposeof
theinputimageandthesecondtheilluminationcondition. Duetotheincreasedcomplexityofthese
images,wemadethisnetworksubstantiallydeeper(9layers).
7anger disgust fear happy sad surprise neutral
+
0
-
Figure6: Foreachcolumn,yissettoaone-hotvectorandscaledfrom5to-5fromtoptobottom,
welloutsideofthenaturalrangeof[0,1]. ‘Opposite’expressionsandmoreextremeexpressionscan
bemade.
InFigure7,weshowtheimagesgeneratedbythedecoderwhileiteratingthrougheachcamerapose.
Thenetworkwastiedtotheilluminationandlatentvariablesofimagesfromthetestset. Although
blurry, the generated images preserve the subject’s illumination and identity (i.e. shirt color, hair
style, skin tone) as the camera pose changes. In Figure 8, we instead fix the camera position and
iteratethroughdifferentilluminationconditions.Wealsofinditpossibletointerpolatebetweencam-
era and lighting positions by simply linearly interpolating the yˆ between two neighboring camera
positionssupportingtheinherentcontinuityintheclasslabels.
Figure7: Leftcolumn: Samplesfromtestsetwithinitialcamerapose. Thefacesontherightwere
generatedbychangingthecorrespondingcamerapose.
Figure8: Leftcolumn: Samplesfromtestset. Illuminationtransformationsareshowntotheright.
Groundtruthlightingforthefirstfaceineachblockisinthefirstrow.
4 Conclusion
With the addition of a supervised cost and an unsupervised cross-covariance penalty, an autoen-
coder can learn to disentangle various transformations using standard feedforward neural network
8components. The decoder implicitly learns to generate novel manipulations of images on multi-
ple sets of transformation variables. We show deep feedforward networks are capable of learning
higher-order factors of variation beyond the observed labels without the need to explicitly define
thesehigher-orderinteractions. Finally, wedemonstratethenaturalabilityofthesedeepnetworks
tolearnacontinuumofhigher-orderfactorsofvariationinboththelatentandobservedvariables.
Surprisingly, these networks can extrapolate intrinsic continuous variation hidden in discrete class
labels. Theseresultsgivesinsightinthepotentialofdeeplearningforthediscoveryofhiddenfac-
torsofvariationsimplybyaccountingforknownvariation. Thishasmanypotentialapplicationsin
exploratorydataanalysisandsignaldenoising.
Acknowledgments
We would like to acknowledge everyone at the Redwood Center for their helpful discussion and
comments. WethankNervanaSystemsforsupportingBrianCheungduringthesummerwhenthis
project originated and for their continued collaboration. We gratefully acknowledge the support
of NVIDIA Corporation with the donation of the Tesla K40 GPUs used for this research. Bruno
OlshausenwassupportedbyNSFgrantIIS-1111765.
References
[1] AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton,“Imagenetclassificationwithdeepconvolutionalneuralnetworks,”inAdvances
inneuralinformationprocessingsystems,2012,pp.1097–1105.
[2] DavidEigen,DilipKrishnan,andRobFergus, “Restoringanimagetakenthroughawindowcoveredwithdirtorrain,” inComputer
Vision(ICCV),2013IEEEInternationalConferenceon.IEEE,2013,pp.633–640.
[3] JoshuaBTenenbaumandWilliamTFreeman,“Separatingstyleandcontentwithbilinearmodels,”Neuralcomputation,vol.12,no.6,
pp.1247–1283,2000.
[4] DavidBGrimesandRajeshPNRao,“Bilinearsparsecodingforinvariantvision,”Neuralcomputation,vol.17,no.1,pp.47–73,2005.
[5] BrunoAOlshausen,CharlesCadieu,JackCulpepper,andDavidKWarland,“Bilinearmodelsofnaturalimages,”inElectronicImaging
2007.InternationalSocietyforOpticsandPhotonics,2007,pp.649206–649206.
[6] GeoffreyEHinton,AlexKrizhevsky,andSidaDWang, “Transformingauto-encoders,” inArtificialNeuralNetworksandMachine
Learning–ICANN2011,pp.44–51.Springer,2011.
[7] RolandMemisevicandGeoffreyEHinton, “Learningtorepresentspatialtransformationswithfactoredhigher-orderboltzmannma-
chines,”NeuralComputation,vol.22,no.6,pp.1473–1492,2010.
[8] PietroBerkes,RichardETurner,andManeeshSahani, “Astructuredmodelofvideoreproducesprimaryvisualcorticalorganisation,”
PLoScomputationalbiology,vol.5,no.9,pp.e1000495,2009.
[9] CharlesFCadieuandBrunoAOlshausen, “Learningintermediate-levelrepresentationsofformandmotionfromnaturalmovies,”
Neuralcomputation,vol.24,no.4,pp.827–866,2012.
[10] ScottReed,KihyukSohn,YutingZhang,andHonglakLee,“Learningtodisentanglefactorsofvariationwithmanifoldinteraction,”in
ProceedingsofThe31stInternationalConferenceonMachineLearning.ACM,2014,p.14311439.
[11] RuslanSalakhutdinovandGeoffreyEHinton, “Learninganonlinearembeddingbypreservingclassneighbourhoodstructure,” in
InternationalConferenceonArtificialIntelligenceandStatistics,2007,pp.412–419.
[12] DiederikPKingma,ShakirMohamed,DaniloJimenezRezende,andMaxWelling, “Semi-supervisedlearningwithdeepgenerative
models,”inAdvancesinNeuralInformationProcessingSystems,2014,pp.3581–3589.
[13] SalahRifai,PascalVincent,XavierMuller,XavierGlorot,andYoshuaBengio, “Contractiveauto-encoders:Explicitinvarianceduring
featureextraction,”inProceedingsofthe28thInternationalConferenceonMachineLearning(ICML-11),2011,pp.833–840.
[14] PascalVincent,HugoLarochelle,IsabelleLajoie,YoshuaBengio,andPierre-AntoineManzagol, “Stackeddenoisingautoencoders:
Learningusefulrepresentationsinadeepnetworkwithalocaldenoisingcriterion,”TheJournalofMachineLearningResearch,vol.11,
pp.3371–3408,2010.
[15] QuocVLe,“Buildinghigh-levelfeaturesusinglargescaleunsupervisedlearning,”inAcoustics,SpeechandSignalProcessing(ICASSP),
2013IEEEInternationalConferenceon.IEEE,2013,pp.8595–8598.
[16] YoshuaBengio,PascalLamblin,DanPopovici,andHugoLarochelle, “Greedylayer-wisetrainingofdeepnetworks,” Advancesin
neuralinformationprocessingsystems,vol.19,pp.153,2007.
[17] SalahRifai,YoshuaBengio,AaronCourville,PascalVincent,andMehdiMirza,“Disentanglingfactorsofvariationforfacialexpression
recognition,”inComputerVision–ECCV2012,pp.808–822.Springer,2012.
[18] MatthewDZeiler,“Adadelta:Anadaptivelearningratemethod,”arXivpreprintarXiv:1212.5701,2012.
[19] IanJGoodfellow,DavidWarde-Farley,PascalLamblin,VincentDumoulin,MehdiMirza,RazvanPascanu,JamesBergstra,Fre´de´ric
Bastien,andYoshuaBengio,“Pylearn2:amachinelearningresearchlibrary,”arXivpreprintarXiv:1308.4214,2013.
[20] IanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,SherjilOzair,AaronCourville,andYoshuaBengio,
“Generativeadversarialnets,”inAdvancesinNeuralInformationProcessingSystems,2014,pp.2672–2680.
[21] YannLeCunandCorinnaCortes,“Themnistdatabaseofhandwrittendigits,”1998.
[22] IanJGoodfellow,DavidWarde-farley,MehdiMirza,AaronCourville,andYoshuaBengio, “Maxoutnetworks,” inProceedingsofthe
30thInternationalConferenceonMachineLearning.ACM,2013,pp.1319–1327.
[23] J.Susskind,A.Anderson,andG.E.Hinton,“Thetorontofacedatabase,”Tech.Rep.,UniversityofToronto,2010.
9[24] RalphGross,IainMatthews,JeffreyCohn,TakeoKanade,andSimonBaker,“Multi-pie,”ImageandVisionComputing,vol.28,no.5,
pp.807–813,2010.
[25] ChristianSzegedy,WeiLiu,YangqingJia,PierreSermanet,ScottReed,DragomirAnguelov,DumitruErhan,VincentVanhoucke,and
AndrewRabinovich,“Goingdeeperwithconvolutions,”arXivpreprintarXiv:1409.4842,2014.
[26] JaschaSohl-Dickstein,BenPoole,andSuryaGanguli, “Fastlarge-scaleoptimizationbyunifyingstochasticgradientandquasi-newton
methods,”inProceedingsofthe31stInternationalConferenceonMachineLearning(ICML-14),2014,pp.604–612.
[27] NitishSrivastava,GeoffreyHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdinov, “Dropout: Asimplewaytoprevent
neuralnetworksfromoverfitting,”TheJournalofMachineLearningResearch,vol.15,no.1,pp.1929–1958,2014.
[28] OlivierBreuleux,YoshuaBengio,andPascalVincent,“Quicklygeneratingrepresentativesamplesfromanrbm-derivedprocess,”Neural
Computation,vol.23,no.8,pp.2058–2073,2011.
[29] YoshuaBengio,GregoireMesnil,YannDauphin,andSalahRifai,“Bettermixingviadeeprepresentations,”inProceedingsofThe30th
InternationalConferenceonMachineLearning,2013,pp.552–560.
[30] YoshuaBengio,EricLaufer,GuillaumeAlain,andJasonYosinski, “Deepgenerativestochasticnetworkstrainablebybackprop,” in
Proceedingsofthe31stInternationalConferenceonMachineLearning(ICML-14),2014,pp.226–234.
10"
73,75,Disentangling factors of variation for facial expression recognition,"['S Rifai', 'Y Bengio', 'A Courville', 'P Vincent']",2012,264,Toronto Face Database,"classification, classifier, deep learning, facial expression recognition, machine learning, neural network","This system beats the state-of-the-art on a recently proposed dataset for facial expression  recognition, the Toronto Face Database, moving the state-of-art accuracy from 82.4% to 85.0%",No DOI,Computer Vision–ECCV …,https://link.springer.com/chapter/10.1007/978-3-642-33783-3_58,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,springer.com,
74,76,Disgust-specific impairment of facial expression recognition in Parkinson's disease,"['A Suzuki', 'T Hoshino', 'K Shigemasu', 'M Kawamura']",2006,259,Japanese Female Facial Expression,facial expression recognition,impair the recognition of facial expressions of disgust; this provides concrete evidence for  emotion- (eight photographs for each emotion; Japanese and Caucasian facial expressions of,No DOI,Brain,https://pubmed.ncbi.nlm.nih.gov/16415306/,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,nih.gov,
75,77,Do deep neural networks learn facial action units when doing expression recognition?,"['P Khorrami', 'T Paine', 'T Huang']",2015,360,"Acted Facial Expressions In The Wild, Extended Cohn-Kanade, Toronto Face Database","CNN, classification, classifier, deep learning, facial expression recognition, machine learning, neural network","Cohn-Kanade (CK+) dataset [18], and the Multi-PIE dataset [10]. However, the recent success  of deep neural networks  in our experiments: the extended Cohn-Kanade database (CK+) [",No DOI,Proceedings of the IEEE …,http://arxiv.org/abs/1510.02969,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,arxiv.org,"Do Deep Neural Networks Learn Facial Action Units
When Doing Expression Recognition?
PooyaKhorrami TomLePaine ThomasS.Huang
BeckmanInstituteforAdvancedScienceandTechnology
UniversityofIllinoisatUrbana-Champaign
{pkhorra2,paine1,t-huang1}@illinois.edu
Abstract
Despitebeingtheappearance-basedclassifierofchoice
in recent years, relatively few works have examined how
much convolutional neural networks (CNNs) can improve
performance on accepted expression recognition bench-
marks and, more importantly, examine what it is they ac-
tuallylearn. Inthiswork, notonlydoweshowthatCNNs
can achieve strong performance, but we also introduce an
approach to decipher which portions of the face influence
the CNN’s predictions. First, we train a zero-bias CNN
on facial expression data and achieve, to our knowledge,
state-of-the-artperformanceontwoexpressionrecognition
benchmarks: the extended Cohn-Kanade (CK+) dataset
and the Toronto Face Dataset (TFD). We then qualita-
tively analyze the network by visualizing the spatial pat-
terns that maximally excite different neurons in the convo-
lutional layers and show how they resemble Facial Action
Units(FAUs).Finally,weusetheFAUlabelsprovidedinthe
CK+ dataset to verify that the FAUs observed in our filter
Figure1.Visualizationoffacialregionsthatactivatefiveselected
visualizations indeed align with the subject’s facial move-
filters in the 3rd convolutional layer of a network trained on the
ments.
ExtendedCohn-Kanade(CK+)dataset. Eachrowcorrespondsto
one filter in the conv3 layer and we display the spatial patterns
fromthetop5images.
1.Introduction
Facial expressions provide a natural and compact way accurately learn the parts of the face that convey emotion
forhumanstoconveytheiremotionalstatetoanotherparty. hasproventobeanon-trivialtask.
Therefore,designingaccuratefacialexpressionrecognition Previous work in facial expression recognition can be
algorithmsiscrucialtothedevelopmentofinteractivecom- splitintotwobroadcategories: AU-based/rule-basedmeth-
puter systems in artificial intelligence. Extensive work in ods and appearance-based methods. AU-based methods
this area has found that only a small number of regions [29, 30] would detect the presence of individual AUs ex-
changeasahumanchangestheirexpressionandarelocated plicitly and then classify a person’s emotion based on the
around the subject’s eyes, nose and mouth. In [7], Paul combinations originally proposed by Friesen and Ekman
EkmanproposedtheFacialActionCodingSystem(FACS) in [8]. Unfortunately, each AU detector required careful
which enumerated these regions and described how every hand-engineeringtoensuregoodperformance.Ontheother
facial expression can be described as the combination of hand, appearance-based methods [1, 2, 31, 33] modeled a
multipleactionunits(AUs),eachcorrespondingtoapartic- person’sexpressionfromtheirgeneralfacialshapeandtex-
ularmusclegroupintheface. However,havingacomputer ture.
7102
raM
61
]VC.sc[
3v96920.0151:viXraInthelastfewyears,manywell-establishedproblemsin Gaborwavelets[1,2],Haarfeatures[31]andLBPfeatures
computervisionhavegreatlybenefitedfromtheriseofcon- [33], in order to make representations of different expres-
volutionalneuralnetworks(CNNs)asanappearance-based sionclassesmorediscriminative.
classifier. Taskssuchasobjectrecognition[14],objectde- For some time, systems based on hand-crafted features
tection[9],andfacerecognition[28]haveseenhugeboosts were able to achieve impressive results on accepted ex-
in performance on several accepted benchmarks. Unfor- pression recognition benchmarks such as the Japanese Fe-
tunately, other tasks such as facial expression recognition male Facial Expression (JAFFE) database [19], the ex-
have not experienced performance gains of the same mag- tendedCohn-Kanade(CK+)dataset[18],andtheMulti-PIE
nitude. Little work has been done to see how much deep dataset [10]. However, the recent success of deep neural
CNNscanhelponacceptedexpressionrecognitionbench- networks has caused many researchers to explore feature
marks. representationsthatarelearnedfromdata. Notsurprisingly,
Inthispaper,weseektheanswertothefollowingques- almostallofthemethodsusedsomeformofunsupervised
tions: CanCNNsimproveperformanceonemotionrecog- pre-training/learningtoinitializetheirmodels. Wehypoth-
nition datasets/baselines and what do they learn? We pro- esize this may be because the scarcity of labeled data pre-
posetodothisbytrainingaCNNonestablishedfacialex- vented the authors from training a completely supervised
pressiondatasetsandthenanalyzingwhattheylearnbyvi- modelthatdidnotexperienceheavyoverfitting.
sualizingtheindividualfiltersinthenetwork. Inthiswork,
In [17], the authors trained a multi-layer boosted deep
we apply the visualization techniques proposed by Zeiler
beliefnetwork(BDBN)andachievedstate-of-the-artaccu-
and Fergus [32] and Springenberg et al. [25] where indi-
racy on the CK+ and JAFFE datasets. Meanwhile in [23],
vidual neurons in the network are excited and their corre-
the authors used a convolutional contractive auto-encoder
sponding spatial patterns are displayed in pixel space us-
(CAE)astheirunderlyingunsupervisedmodel. Theythen
ingadeconvolutionalnetwork. Whenvisualizingthesedis-
performedasemi-supervisedencodingfunctioncalledCon-
criminativespatialpatterns,wefindthatmanyofthefilters
tractiveDiscriminantAnalysis(CDA)toseparatediscrimi-
are excited by regions in the face that corresponded to Fa-
nativeexpressionfeaturesfromtheunsupervisedrepresen-
cialActionUnits(FAUs). Asubsetofthesespatialpatterns
tation.
isshowninFigure1.
Afewworksbasedonunsuperviseddeeplearninghave
Thus,themaincontributionsofthispaperareasfollows:
also tried to analyze the relationship between FAUs and
1. We show that CNNs trained for the emotion recogni- the learned feature representations. In [15, 16], the au-
tion task learn features that correspond strongly with thors learned a patch-based filter bank using K-means as
theFAUsproposedbyEkman[7].Wedemonstratethis their low-level feature. These features were then used to
resultbyfirstvisualizingthespatialpatternsthatmaxi- selectreceptivefieldscorrespondingtospecificFAUrecep-
mallyexcitedifferentfiltersintheconvolutionallayers tive fields which were subsequently passed to multi-layer
ofournetworks,andthenusingthegroundtruthFAU restricted Boltzmann machines (RBMs) for classification.
labelstoverifythattheFAUsobservedinthefiltervi- The FAU receptive fields were selected using a mutual in-
sualizationsalignwiththesubject’sfacialmovements. formation criterion between the image feature and the ex-
pression label. An earlier work by Susskind et al. [27],
2. We also show that our CNN model, based on works
showed that the first layer features a deep belief network
originally proposed by [20, 21], can achieve, to our
trained to generate facial expression images appeared to
knowledge, state-of-the-art performance on the ex-
learn filters that were sensitive to face parts. We conduct
tended Cohn-Kanade (CK+) dataset and the Toronto
a similar analysis except we use a CNN as our underly-
FaceDataset(TFD).
ing model and we visualize the spatial patterns that excite
higher-levelneuronsinthenetwork.
2.RelatedWork
To the authors’ knowledge, the only works that previ-
Inmostfacialexpressionrecognitionsystems, themain ously applied CNNs to expression data were that of Ka-
machinery matches quite nicely with the traditional ma- hou et al. [13, 12] and Jung et al. [11]. In [13, 12], the
chine learning pipeline. More specifically, a face image is authorsdevelopedasystemfordoingaudio/visualemotion
passedtoaclassifierthattriestocategorizeitasoneofsev- recognitionfortheEmotionRecognitionintheWildChal-
eral(typically7)expressionclasses: 1. anger,2. disgust,3. lenge (EmotiW) [6, 5] while in [11], the authors trained a
fear, 4. neutral, 5. happy, 6. sad, and7. surprise. Inmost network that incorporated both appearance and geometric
cases,priortobeingpassedtotheclassifier,thefaceimage features when doing recognition. However, one key point
is pre-processed and given to a feature extractor. Up until isthattheseworksdealtwithemotionrecognitionofvideo
ratherrecently,mostappearance-basedexpressionrecogni- / image sequence data and therefore, actively incorporated
tiontechniquesreliedonhand-craftedfeatures,specifically temporaldatawhencomputingtheirpredictions.Conv layer 1 Conv layer 2 Conv layer 3
Input FC So(cid:31)max
96 x 96 x 1
5 x 5 x 1 x 64 5 x 5 x 1 x 128 5 x 5 x 1 x 256
Max pooling Max pooling Quadrant pooling
Figure2.NetworkArchitecture-Ournetworkconsistsofthreeconvolutionallayerscontaining64,128,and256filters,respectively,each
ofsize5x5followedbyReLU(RectifiedLinearUnit)activationfunctions.Weadd2x2maxpoolinglayersafterthefirsttwoconvolutional
layersandquadrantpoolingafterthethird. Thethreeconvolutionallayersarefollowedbyafully-connectedlayercontaining300hidden
unitsandasoftmaxlayer.
5
In contrast, our work deals with emotion recognition mentum set to 0.9, and a weight decay parameter of 1e-5.
from a single image, and will focus on analyzing the fea- Weuseaconstantlearningrateof0.01anddonotuseany
tures learned by the network. Thus, not only will we form of annealing. The parameters of each layer are ran-
demonstratetheeffectivenessofCNNsonexistingemotion domly initialized by drawing from a Gaussian distribution
classification baselines but we will also qualitatively show with zero mean and standard deviation σ = k where
NFANIN
thatthenetworkisabletolearnpatternsinthefaceimages N is the number of input connections to each layer
FANIN
thatcorrespondtoFacialActionUnits(FAUs). andkisdrawnuniformlyfromtherange: [0.2,1.2].
Wealsousedropoutandvariousformsofdataaugmen-
3.OurApproach tationtoregularizeournetworkandcombatoverfitting. We
applydropouttothefully-connectedlayerwithaprobabil-
3.1.NetworkArchitecture
ityof0.5(i.e.eachneuron’soutputissettozerowithproba-
For all of the experiments we present in this paper, we bility0.5).Fordataaugmentation,weapplyarandomtrans-
use a classic feed-forward convolutional neural network. formation to each input image consisting of: translations,
The networks we use, shown visually in Figure 2 consist horizontal flips, rotations, scaling, and pixel intensity aug-
of three convolutional layers with 64, 128, and 256 filters, mentation. All of our models were trained using the anna
respectively,andwithfiltersizesof5x5followedbyReLU softwarelibrary1.
(Rectified Linear Unit) activation functions. Max pooling
layers are placed after the first two convolutional layers 4.ExperimentsandAnalysis
while quadrant pooling [3] is applied after the third. The
We use two facial expression datasets in our experi-
quadrantpoolinglayeristhenfollowedbyafull-connected
ments:theextendedCohn-Kanadedatabase(CK+)[18]and
layer with 300 hidden units and, finally, a softmax layer
the Toronto Face Dataset (TFD) [26]. The CK+ database
forclassification. Thesoftmaxlayercontainsanywherebe-
contains 327 image sequences, each of which is assigned
tween 6-8 outputs corresponding to the number of expres-
one of 7 expression labels: anger, contempt, disgust, fear,
sionspresentinthetrainingset.
happy,sad,andsurprise.Forfaircomparison,wefollowthe
Onemodificationthatweintroducetotheclassicalcon-
protocolusedbypreviousworks[15,17], andusethefirst
figuration is that we ignore the biases of the convolutional
frameofeachsequenceasaneutralframeinadditiontothe
layers. This idea was introduced first by Memisevic et al.
lastthreeexpressiveframestoformourdataset. Thisleads
in[20]forfully-connectednetworksandlaterextendedby
toatotalof1308imagesand8classestotal. Wethensplit
Paine et al. in [21] to convolutional layers. In our exper-
theframesinto10subjectindependentsubsetsintheman-
iments, we found that ignoring the bias allowed our net-
nerpresentedby[15]andperform10-foldcross-validation.
work to train very quickly while simultaneously reducing
TFD is an amalgamation of several facial expression
thenumberofparameterstolearn.
datasets. It contains 4178 images annotated with one of 7
3.2.NetworkTraining expression labels: anger, disgust, fear, happy, neutral, sad,
andsurprise. Thelabeledsamplesaredividedinto5folds,
When training our network, we train from scratch us-
ingstochasticgradientdescentwithabatchsizeof64,mo- 1https://github.com/ifp-uiuc/annaTable1.RecognitionaccuracyontheTorontoFaceDataset(TFD) Table 2. Recognition accuracy on the Extended Cohn-Kanade
-7classes-A:DataAugmentation,D:Dropout (CK+)Dataset-8classes-A:DataAugmentation,D:Dropout
Method Accuracy Method Accuracy
Gabor+PCA[4] 80.2% AURF[15] 92.22%
DeepmPoT[22] 82.4% AUDN[16] 93.70%
CDA[23] 85.0% Zero-biasCNN 78.2%±5.7%
Zero-biasCNN 79.0%±1.1% Zero-biasCNN+D 82.3%±4.0%
Zero-biasCNN+D 81.8%±2.1% Zero-biasCNN+A 94.6%±3.3%
Zero-biasCNN+A 88.4%±1.7% Zero-biasCNN+AD 95.1%±3.1%
Zero-biasCNN+AD 88.6%±1.5%
Table 3. Recognition accuracy on the Extended Cohn-Kanade
(CK+)Dataset-6classes-A:DataAugmentation,D:Dropout
each containing a train, validation, and test set. We train
all of our models using just the training set of each fold, Method Accuracy
pickthebestperformingmodelusingeachsplit’svalidation CSPL[34] 89.89%
set,thenweevaluateoneachsplit’stestsetandaveragethe LBPSVM[24] 95.10%
resultsoverall5folds. BDBN[17] 96.70%
Inbothdatasets,theimagesaregrayscaleandareofsize Zero-biasCNN+AD 95.7%±2.5%
96x96 pixels. In the case of TFD, the faces have already
beendetectedandnormalizedsuchthatallofthesubjects’
eyes are the same distance apart and have the same verti-
cal coordinates. Meanwhile for the CK+ dataset, we sim- the eight class model, we conduct the same study we did
ply detect the face in the 640x480 image and resize it to on the TFD and we observe rather similar results. Once
96x96. Theonlyotherpre-processingweemployispatch- again,regularizationappearstoplayasignificantroleinob-
wisemeansubtractionandscalingtounitvariance. taining good performance. Data augmentation gives a sig-
nificantboostinperformance(16.4%)andwhencombined
4.1.PerformanceonTorontoFaceDatabase(TFD)
withdropout,leadstoa16.9%increase. Fortheeightclass
First, we analyze the discriminative ability of the CNN and six class models, we achieve state-of-the-art and near
by assessing its performance on the TFD dataset. Table 1 state-of-the-artaccuracyrespectivelyontheCK+dataset.
shows the recognition accuracy obtained when training a
4.3.Visualizationofhigher-levelneurons
zero-bias CNN from a random initialization with no other
regularizationaswellasCNNsthathavedropout(D),data Now,withastrongdiscriminativemodelinhand,wewill
augmentation (A) or both (AD). We also include recogni- analyzewhichfacialregionstheneuralnetworkidentifiesas
tionaccuraciesfrompreviousmethods. Fromtheresultsin themostdiscriminativewhenperformingclassification. To
Table 1, there are two main observations: (i) not surpris- dothis,weemploythevisualizationtechniquepresentedby
ingly, regularization significantly boosts performance (ii) ZeilerandFergusin[32].
data augmentation improves performance over the regular For each dataset, we consider the third convolutional
CNN more than dropout (9.4% vs. 2.8%). Furthermore, layer and for each filter, we find the N images in the cho-
when both dropout and data augmentation are used, our sen split’s training set that generated the strongest magni-
modelisabletoexceedthepreviousstate-of-the-artperfor- tuderesponse. Wethenleavethestrongestneuronhighand
manceonTFDby3.6%. setallotheractivationstozeroandusethedeconvolutional
network to reconstruct the region in pixel space. For our
4.2. Performance on the Extended Cohn-Kanade
experiments,wechoseN=10trainingimages.
Dataset(CK+)
We further refine our reconstructions by employing a
We now present our results on the CK+ dataset. The technique called ”Guided Backpropagation” proposed by
CK+datasetusuallycontainseightlabels(anger,contempt, Springenberg et al. in [25]. ”Guided Backpropogation”
disgust, fear, happy, neutral, sad, and surprise). However, aims to improve the reconstructed spatial patterns by not
manyworks[34,24,17]ignorethesampleslabeledasneu- solely relying on the masked activations given by the top-
tral or contempt, and only evaluate on the six basic emo- levelsignalduringdeconvolutionbutbyalsoincorporating
tions. Therefore,toensurefaircomparison,wetrainedtwo knowledgeofwhichactivationsweresuppressedduringthe
separate models. We present the eight class model results forwardpass. Therefore,eachlayer’soutputduringthede-
in Table 2 and the six class model results in Table 3. For convolutionstageismaskedtwice: (i)oncebytheReLUofTable4.CorrespondencesbetweenCK+visualizationplotsshown
samplesthatdonotcontainFAUj,anddoesthatFAUaccu-
in Figure 4 and the FAU whose activation distribution had the
ratelycorrespondwiththevisualspatialpatternsthatmaxi-
highest KL divergence value. The KL divergence values of all
mallyexcitefilteri?
theFAUscomputedforeachfilterareshowninFigure5.
Given a training set of M images (X) and their corre-
Filter FAUwiththeLargest sponding FAU labels (Y), let F (cid:96)i(x) be the activations of
Number KLDivergenceValue samplexatlayer(cid:96)forfilteri. Sinceweareexaminingthe
1 AU25: LipsPart 3rdconvolutionallayerinthenetwork,weset(cid:96)=3. Then,
2 AU12: LipCornerPuller for each of the 10 filters visualized in Figure 4, we do the
3 AU9: NoseWrinkler following:
4 AU5: UpperLidRaiser
(i) WeconsideraparticularFAUjandplacethesamples
5 AU17: ChinRaiser
X thatcontainjinsetSwhere:
6 AU12: LipCornerPuller
S ={x |j ∈y }, ∀m∈{1,...,M}
7 AU24: LipPressor m m
8 AU27: MouthStretch
(ii) Wethenbuildahistogramofthemaximumactivations
9 AU12: LipCornerPuller
ofthesamplesthatcontainedFAUj:
10 AU1: InnerBrowRaiser
Q (x)=P(F (x)|S), ∀(x,y)∈(X,Y)
ij 3i
(iii) Wethen,similarly,buildadistributionovermaximum
thedeconvotionallayerand(ii)againbythemaskgenerated activationsofthesamplesthatdonotcontainFAUj:
bytheReLUofthelayer’smatchingconvolutionallayerin R (x)=P(F (x)|Sc), ∀(x,y)∈(X,Y)
ij 3i
theforwardpass.
First,wewillanalyzepatternsdiscoveredintheToronto (iv) We compute the KL divergence between Q (x) and
ij
Face Dataset (TFD). In Figure 3, we select 10 of the 256 R (x), D (Q (cid:107)R ), and repeat the process for
ij KL ij ij
filtersinthethirdconvolutionallayerandforeachfilter,we alloftheotherFAUs.
presentthespatialpatternsofthetop-10imagesinthetrain-
ingset. Fromtheseimages, thereadercanseethatseveral Figure 5 shows the bar charts of the KL divergences
ofthefiltersappeartobesensitivetoregionsthatalignwith computedforalloftheFAUsforeachofthe10filtersdis-
severaloftheFacialActionsUnitssuchas:AU12:LipCor- playedinFigure4.TheFAUwiththelargestKLdivergence
nerPuller(row1),AU4:BrowLowerer(row4),andAU15: valueisdenotedinredanditscorrespondingnameisdoc-
LipCornerDepressor(row9). umented in Table 4 for each filter. From these results, we
Next, we display the patterns discovered in the CK+ seethatinthemajorityofthecases,theFAUslistedinTa-
dataset. In Figure 4, we, once again, select 10 of the 256 ble 4 match the facial regions visualized in Figure 4. This
filters in the third convolutional layer and for each filter, meansthatthesamplesthatappeartostronglyinfluencethe
we present the spatial patterns of the top-10 images in the activations of these particular filters are indeed those that
trainingset.ThereaderwillnoticethattheCK+discrimina- possess the AU shown in the corresponding filter visual-
tivespatialpatternsareveryclearlydefinedandcorrespond izations. Thus, we show that certain neurons in the neural
nicelywithFacialActionUnitssuchas: AU12: LipCorner networkimplicitlylearntodetectspecificFAUsinfaceim-
Puller(rows2,6,and9),AU9: NoseWrinkler(row3)and ageswhengivenarelatively”loose”supervisorysignal(i.e.
AU27: MouthStretch(row8). emotiontype: anger,happy,sad,etc.).
What is most encouraging is that these results ap-
4.4.FindingCorrespondencesBetweenFilterActi-
pear to confirm our intuitions about how CNNs work as
vations and the Ground Truth Facial Action
appearance-based classifiers. For instance, filter 2, 6, and
Units(FAUs)
9appeartobeverysensitivetopatternsthatcorrespondto
Inadditiontocategoricallabels(anger,disgust,etc.),the AU12. ThisisnotsurprisingasAU12(LipCornerPuller)
CK+ dataset also contains labels that denote which FAUs isalmostalwaysassociatedwithsmilesandfromthevisual-
arepresentineachimagesequence. Usingtheselabels,we izationsinFigure4,asubjectoftenshowstheirteethwhen
nowpresentapreliminaryexperimenttoverifythatthefil- smiling,ahighlydistinctiveappearancecue. Similarly,for
ter activations/spatial patterns learned by the CNN indeed filter8,itisnotsurprisingthatFAU25(LipsPart)andFAU
matchwiththeactualFAUsshownbythesubjectintheim- 27(MouthStretch)hadthemostdifferentactivationdistri-
age.Ourexperimentaimstoanswerthefollowingquestion: butions given that the filter’s spatial patterns corresponded
Foraparticularfilteri,whichFAUjhassampleswhoseac- to the ”O” shape made by the mouth region in surprised
tivation values most strongly differ from the activations of faces,anothervisuallysalientcue.Figure3.Visualizationofspatialpatternsthatactivate10selectedfiltersintheconv3layerofournetworktrainedontheTorontoFace
Dataset(TFD).Eachrowcorrespondstoonefilterintheconv3layer. Wedisplaythetop10imagesthatelicitedthemaximummagnitude
response.NoticethatthespatialpatternsappeartocorrespondwithsomeoftheFacialActionUnits.
5.Conclusions ferentfiltersintheconvolutionallayersofourlearnednet-
works.Meanwhile,quantitatively,wecorrelatedthenumer-
ical activations of the visualized filters with the subject’s
In this work, we showed both qualitatively and quanti-
actual facial movements using the FAU labels given in the
tatively that CNNs trained to do emotion recognition are
CK+ dataset. Finally, we demonstrated how a zero-bias
indeed able to model high-level features that strongly cor-
CNN can achieve state-of-the-art recognition accuracy on
respondtoFAUs. Qualitatively,weshowedwhichportions
the extended Cohn-Kanade (CK+) dataset and the Toronto
of the face yielded the most discriminative information by
FaceDataset(TFD).
visualizing the spatial patterns that maximally excited dif-Figure4.Visualizationofspatialpatternsthatactivate10selectedfiltersintheconv3layerofournetworktrainedontheCohn-Kanade
(CK+)dataset.Eachrowcorrespondstoonefilterintheconv3layer.Onceagain,wedisplaythetop10imagesthatelicitedthemaximum
magnituderesponse.NoticethatthespatialpatternsappeartohaveveryclearcorrespondenceswithsomeoftheFacialActionUnits.
Acknowledgments References
[1] M. S. Bartlett, G. Littlewort, M. Frank, C. Lainscsek,
ThisworkwassupportedinpartbyMITLincolnLabo-
I. Fasel, and J. Movellan. Recognizing facial expression:
ratory. TheTeslaK40GPUusedforthisresearchwasdo- machine learning and application to spontaneous behavior.
natedbytheNVIDIACorporation. Theauthorswouldalso InCVPR,pages568–573,2005. 1,2
like to thank Dr. Kevin Brady, Dr. Charlie Dagli, Profes- [2] M. S. Bartlett, G. Littlewort, M. Frank, C. Lainscsek,
sorYunFu,andProfessorUsmanTariqfortheirinsightful I.Fasel,andJ.Movellan.Fullyautomaticfacialactionrecog-
commentsandsuggestionswithregardstothiswork. nition in spontaneous behavior. In FGR, pages 223–230,0.2
0.15
0.1
0.05
0
1 2 4 5 6 7 9 12 14 15 17 23 24 25 27
FAU
ecnegreviD
LK
3
2.5
2
1.5
1
0.5
0
1 2 4 5 6 7 9 12 14 15 17 23 24 25 27
FAU
Filter1
ecnegreviD
LK
Filter2
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1 2 4 5 6 7 9 12 14 15 17 23 24 25 27
FAU
ecnegreviD
LK
1
0.8
0.6
0.4
0.2
0
1 2 4 5 6 7 9 12 14 15 17 23 24 25 27
FAU
Filter3
ecnegreviD
LK
Filter4
0.25
0.2
0.15
0.1
0.05
0
1 2 4 5 6 7 9 12 14 15 17 23 24 25 27
FAU
ecnegreviD
LK
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
1 2 4 5 6 7 9 12 14 15 17 23 24 25 27
FAU
Filter5
ecnegreviD
LK
Filter6
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
1 2 4 5 6 7 9 12 14 15 17 23 24 25 27
FAU
ecnegreviD
LK
0.5
0.4
0.3
0.2
0.1
0
1 2 4 5 6 7 9 12 14 15 17 23 24 25 27
FAU
Filter7
ecnegreviD
LK
Filter8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1 2 4 5 6 7 9 12 14 15 17 23 24 25 27
FAU
ecnegreviD
LK
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1 2 4 5 6 7 9 12 14 15 17 23 24 25 27
FAU
Filter9
ecnegreviD
LK
Filter10
Figure5.BarchartsshowingwhichFAUsleadtothestrongestshiftsintheactivationdistributionsofparticularfiltersintheCNN.For
eachofthe10filtersvisualizedinFigure4,webuildhistogramsovertheactivationsoftrainingsamplesthatcontainaspecificFAUj,and
theactivationsofsamplesthatdonotcontainFAUj.WethencomputetheKLdivergencebetweenthetwodistributionsandplotthemfor
eachFAUabove.TheFAUwiththelargestKLdivergenceisdisplayedinredanditscorrespondingnameisgiveninTable4.(Bestviewed
incolor).2006. 1,2 [19] M.J.Lyons,J.Budynek,andS.Akamatsu. Automaticclas-
[3] A. Coates, A. Y. Ng, and H. Lee. An analysis of single- sificationofsinglefacialimages. PAMI,21(12):1357–1362,
layer networks in unsupervised feature learning. In Inter- 1999. 2
national conference on artificial intelligence and statistics, [20] R. Memisevic, K. Konda, and D. Krueger. Zero-bias au-
pages215–223,2011. 3 toencoders and the benefits of co-adapting features. stat,
[4] M. N. Dailey, G. W. Cottrell, C. Padgett, and R. Adolphs. 1050:10,2014. 2,3
Empath: A neural network that categorizes facial expres- [21] T.L.Paine,P.Khorrami,W.Han,andT.S.Huang. Ananal-
sions. Journalofcognitiveneuroscience,14(8):1158–1173, ysisofunsupervisedpre-traininginlightofrecentadvances.
2002. 4 arXivpreprintarXiv:1412.6597,2014. 2,3
[5] A. Dhall, R. Goecke, J. Joshi, K. Sikka, and T. Gedeon. [22] M.Ranzato,J.Susskind,V.Mnih,andG.Hinton. Ondeep
Emotion recognition in the wild challenge 2014: Baseline, generativemodelswithapplicationstorecognition. InCom-
dataandprotocol.In16thACMInternationalConferenceon puter Vision and Pattern Recognition (CVPR), 2011 IEEE
MultimodalInteraction.ACM,2014. 2 Conferenceon,pages2857–2864.IEEE,2011. 4
[6] A. Dhall, R. Goecke, J. Joshi, M. Wagner, and T. Gedeon. [23] S.Rifai,Y.Bengio,A.Courville,P.Vincent,andM.Mirza.
Emotion recognition in the wild challenge 2013. In Pro- Disentangling factors of variation for facial expression
ceedings of the 15th ACM on International conference on recognition. ECCV2012,pages808–822,2012. 2,4
multimodalinteraction,pages509–516.ACM,2013. 2 [24] C. Shan, S. Gong, and P. W. McOwan. Facial expression
[7] P. Ekman and W. V. Friesen. Facial action coding system. recognitionbasedonlocalbinarypatterns:Acomprehensive
1977. 1,2 study. ImageandVisionComputing,27(6):803–816,2009.
[8] W. V. Friesen and P. Ekman. Emfacs-7: Emotional facial 4
actioncodingsystem. Unpublishedmanuscript, University [25] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Ried-
ofCaliforniaatSanFrancisco,2:36,1983. 1 miller. Striving for simplicity: The all convolutional net.
[9] R.Girshick,J.Donahue,T.Darrell,andJ.Malik. Richfea- arXivpreprintarXiv:1412.6806,2014. 2,4
ture hierarchies for accurate object detection and semantic [26] J. M. Susskind, A. K. Anderson, and G. E. Hinton. The
segmentation. InCVPR,pages580–587,2014. 2 toronto face database. Department of Computer Science,
[10] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker. University of Toronto, Toronto, ON, Canada, Tech. Rep,
Multi-pie. Image and Vision Computing, 28(5):807–813, 2010. 3
2010. 2 [27] J. M. Susskind, A. K. Anderson, G. E. Hinton, and J. R.
[11] H.Jung,S.Lee,S.Park,I.Lee,C.Ahn,andJ.Kim. Deep Movellan. Generating facial expressions with deep belief
temporalappearance-geometrynetworkforfacialexpression nets. 2008. 2
recognition. arXivpreprintarXiv:1503.01532,2015. 2 [28] Y.Taigman,M.Yang,M.Ranzato,andL.Wolf. Deepface:
[12] S. E. Kahou, X. Bouthillier, P. Lamblin, C. Gulcehre, Closingthegaptohuman-levelperformanceinfaceverifica-
V.Michalski,K.Konda,S.Jean,P.Froumenty,A.Courville, tion. InCVPR,pages1701–1708,2014. 2
P. Vincent, et al. Emonets: Multimodal deep learning ap- [29] Y.-l.Tian,T.Kanada,andJ.F.Cohn.Recognizingupperface
proaches for emotion recognition in video. arXiv preprint actionunitsforfacialexpressionanalysis. InComputerVi-
arXiv:1503.01800,2015. 2 sionandPatternRecognition,2000.Proceedings.IEEECon-
[13] S. E. Kahou, C. Pal, X. Bouthillier, P. Froumenty, ferenceon,volume1,pages294–301.IEEE,2000. 1
C¸.Gu¨lc¸ehre,R.Memisevic,P.Vincent,A.Courville,Y.Ben- [30] Y.Tong,W.Liao,andQ.Ji.Facialactionunitrecognitionby
gio,R.C.Ferrari,etal. Combiningmodalityspecificdeep exploitingtheirdynamicandsemanticrelationships. Pattern
neuralnetworksforemotionrecognitioninvideo. InICMI, Analysis and Machine Intelligence, IEEE Transactions on,
pages543–550,2013. 2 29(10):1683–1699,2007. 1
[14] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet [31] J. Whitehill and C. W. Omlin. Haar features for facs au
classification with deep convolutional neural networks. In recognition. InFGR,2006. 1,2
NIPS,pages1097–1105,2012. 2 [32] M.D.ZeilerandR.Fergus. Visualizingandunderstanding
[15] M. Liu, S. Li, S. Shan, and X. Chen. Au-aware deep net- convolutional networks. In Computer Vision–ECCV 2014,
worksforfacialexpressionrecognition. InFG,pages1–6, pages818–833.Springer,2014. 2,4
2013. 2,3,4 [33] G. Zhao and M. Pietikainen. Dynamic texture recognition
[16] M.Liu,S.Li,S.Shan,andX.Chen. Au-inspireddeepnet- usinglocalbinarypatternswithanapplicationtofacialex-
worksforfacialexpressionfeaturelearning. Neurocomput- pressions. PAMI,29(6):915–928,2007. 1,2
ing,159:126–136,2015. 2,4 [34] L. Zhong, Q. Liu, P. Yang, B. Liu, J. Huang, and D. N.
[17] P. Liu, S. Han, Z. Meng, and Y. Tong. Facial expression Metaxas. Learningactivefacialpatchesforexpressionanal-
recognition via a boosted deep belief network. In CVPR, ysis. InComputerVisionandPatternRecognition(CVPR),
pages1805–1812,2014. 2,3,4 2012IEEEConferenceon,pages2562–2569.IEEE,2012.4
[18] P.Lucey,J.F.Cohn,T.Kanade,J.Saragih,Z.Ambadar,and
I. Matthews. The extended cohn-kanade dataset (ck+): A
complete dataset for action unit and emotion-specified ex-
pression. InCVPRW,pages94–101,2010. 2,3"
76,78,Driver's facial expression recognition in real-time for safe driving,"['M Jeong', 'BC Ko']",2018,158,MMI Facial Expression,"FER, classification, classifier, deep learning, facial expression recognition, machine learning",": An addition to the mmi facial expression database. In Proceedings of the 3rd International  Conference on Language Resources and Evaluation Workshop on EMOTION, Valletta, Malta",No DOI,Sensors,https://www.mdpi.com/1424-8220/18/12/4270,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,mdpi.com,
77,79,Dynamic facial expression recognition using longitudinal facial expression atlases,"['Y Guo', 'G Zhao', 'M Pietikäinen']",2012,116,MMI Facial Expression,facial expression recognition,"facial expression atlases of each expression, we are able to recognize expression of a new  facial expression  In this section, we evaluate the proposed method on the MMI database [24]",No DOI,"… Computer Vision, Florence, Italy, October 7 …",https://link.springer.com/chapter/10.1007/978-3-642-33709-3_45,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,springer.com,
78,80,EMPATH: A neural network that categorizes facial expressions,"['MN Dailey', 'GW Cottrell', 'C Padgett']",2002,349,Affective Faces Database,neural network,to which each face portrayed each basic emotion on a 1–5  rating’’ vectors for each face pair  as a measure of dissimilarity.  networks respond with emotion i when the intended emotion,No DOI,Journal of cognitive …,https://pubmed.ncbi.nlm.nih.gov/12495523/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,nih.gov,
79,81,Early cortical processing of natural and artificial emotional faces differs between lower and higher socially anxious persons,"['A Mühlberger', 'MJ Wieser', 'MJ Herrmann']",2009,276,Karolinska Directed Emotional Faces,classification,", emotional faces should  emotional faces regarding their processing as reflected by ERP  components. On the one hand, natural faces contain a lot more information than the emotion of",No DOI,Journal of neural …,https://pubmed.ncbi.nlm.nih.gov/18784899/,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,nih.gov,
80,82,Emonets: Multimodal deep learning approaches for emotion recognition in video,"['SE Kahou', 'X Bouthillier', 'P Lamblin', 'C Gulcehre']",2016,510,Toronto Face Database,deep learning,"learning several specialist models using deep learning techniques, each focusing on one  modality. Among these are a convolutional neural network,  is the Toronto Face Dataset (TFD) [",No DOI,Journal on Multimodal …,https://arxiv.org/abs/1503.01800,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"EmoNets: Multimodal deep learning approaches for emotion
recognition in video
Samira Ebrahimi Kahou · Xavier Bouthillier · Pascal Lamblin · Caglar Gulcehre ·
Vincent Michalski · Kishore Konda · S´ebastien Jean · Pierre Froumenty · Yann
Dauphin · Nicolas Boulanger-Lewandowski · Raul Chandias Ferrari · Mehdi Mirza ·
David Warde-Farley · Aaron Courville · Pascal Vincent · Roland Memisevic ·
Christopher Pal · Yoshua Bengio
Abstract The task of the emotion recognition in the Weexploremultiplemethodsforthecombinationof
wild(EmotiW)Challengeistoassignoneofsevenemo- cues from these modalities into one common classifier.
tions to short video clips extracted from Hollywood Thisachievesaconsiderablygreateraccuracythanpre-
style movies. The videos depict acted-out emotions un- dictions from our strongest single-modality classifier.
der realistic conditions with a large degree of variation Our method was the winning submission in the 2013
in attributes such as pose and illumination, making it EmotiW challenge and achieved a test set accuracy of
worthwhile to explore approaches which consider com- 47.67% on the 2014 dataset.
binations of features from multiple modalities for label
Keywords Emotion recognition · Deep learning ·
assignment.
Model combination · Multimodal learning
In this paper we present our approach to learn-
ing several specialist models using deep learning tech-
niques, each focusing on one modality. Among these
areaconvolutionalneuralnetwork,focusingoncaptur- 1 Introduction
ing visual information in detected faces, a deep belief
net focusing on the representation of the audio stream, This is an extended version of the paper describing our
a K-Means based “bag-of-mouths” model, which ex- winning submission [22] to the Emotion Recognition in
tractsvisualfeaturesaroundthemouthregionandare- theWildChallenge(EmotiW)in2013[11].Herewede-
lational autoencoder, which addresses spatio-temporal scribe our approach in more detail and present results
aspects of videos. on the new data set from the 2014 competition [10].
The task in this competition is to assign one of seven
emotionlabels(angry,disgust,fear,happy,neutral,sad,
S.E.Kahou,P.Froumenty,C.Pal
surprise)toeachshortvideoclipintheActedFacialEx-
E´colePolytechiquede Montr´eal,Universit´ede Montr´eal,
pression in the Wild (AFEW) dataset [12]. The video
Montr´eal,Canada
Email: {samira.ebrahimi-kahou,pierre.froumenty, clips are extracted from feature films. Given the low
christopher.pal}@polymtl.ca number of samples per emotion category, it is difficult
todealwiththelargevarietyofsubjects,lightingcondi-
V.Michalski,K.Konda
tionsandposesintheseclose-to-real-worldvideos.The
Goethe-Universit¨atFrankfurt,Frankfurt,Germany
Email: {michalskivince,konda.kishorereddy}@gmail.com clips are approximately 1 to 2 seconds long and also
feature an audio track, which might contain voices and
X.Bouthillier, P.Lamblin,C.Gulcehre,S. Jean,Y.
background music.
Dauphin,N.Boulanger-Lewandowski,R.C. Ferrari,M.
We explore different methods of combining predic-
Mirza,D.Warde-Farley,A.Courville,P.Vincent,R.
Memisevic,Y. Bengio tions of modality-specific models, including: (1) a deep
Laboratoired’Informatique desSyst`emesAdaptatifs, convolutionalneuralnetwork(ConvNet)trainedtorec-
Universit´ede Montr´eal,Montr´eal,Canada
ognize facial expressions in single frames; (2) a deep
Email: {bouthilx, lamblinp,gulcehrc,jeasebas,dauphiya,
belief net that is trained on audio information; (3) a
boulanni,chandiar,mirzamom,wardefar,courvila,vincentp,
memisevr,bengioy}@iro.umontreal.ca relational autoencoder that learns spatio-temporal fea-
tures, which help to capture human actions; and (4) a
5102
raM
03
]GL.sc[
2v00810.3051:viXra2 SamiraEbrahimiKahouet al.
shallow network that is trained on visual features ex- 3 Models for modality-specific representation
tracted around the mouth of the primary human sub- learning
ject in the video. We discuss each model, their perfor-
mance characteristics and different aggregation strate-
3.1 A convolutional network approach for faces
gies. The best single model, without considering com-
binationswithotherexperts,istheConvNettrainedto
ConvNets are artificial neural network architectures,
predict emotions given still frames. It has been trained
that assume a topological input space, e.g. a 2d image
only on additional facial expression datasets, i.e. not
plane.Asetoftwo-dimensionalorthree-dimensional(if
using the competition data. The ConvNet was then
the inputs are color images) filters is applied to small
used to extract class probabilities for the competition
regions over the whole image using convolution, yield-
data.Theextractedprobabilityvectorsofthechallenge
ing a bank of filter response maps (one map per filter),
training and validation sets were aggregated to fixed-
which also exhibit a similar 2d topology.
length vectors and then used to train and validate hy-
perparameters of a support vector machine (SVM) for Toreducethedimensionalityoffeaturebanksandto
final classification. This yielded a test set accuracy of introduce invariance with respect to slight translations
35.58%forthe2013dataset.Usingourbeststrategy(at of the input image, convolutional layers are often fol-
the time)for thecombinationof topperforming expert lowed by a pooling layer, which subsample the feature
models into a single predictor, we were able to achieve maps by collapsing small regions into a single element
an accuracy of 41.03% on the 2013 challenge test set. (for instance by choosing the maximum or mean value
The next best competitor achieved a test accuracy of in the region). ConvNets have recently been shown to
35.89%. We reran our pipeline on the 2014 challenge achieve state of the art performance in challenging ob-
datawithimprovedsettingsforourcombinationmodel ject recognition tasks [27].
and achieved a test set accuracy of 47.67%, compared
Because of the small number of training samples,
to 50.37% reported by the challenge winners [30].
our initial experiments with ConvNets showed severe
overfitting on the training set, achieving an accuracy
of 96.73% on the AFEW2 training set, compared to
2 Related work only 35.32% on the validation set. For this reason we
decided to train on a separate dataset, which we refer
The task of recognizing the emotion to associate with toas’extradata’.Itconsistsoftwofaceimagedatasets
a short video clip is well suited for methods and mod- and is described in Section 3.1.1.
els that combine features from different modalities. As
The approach for the face modality can roughly be
such, many other successful approaches in the Emo-
divided into four stages:
tion recognition in the Wild (EmotiW) 2013 and 2014
challenges focus on the fusion of modalities. These in-
clude [32], who used Multiple Kernel Learning (MKL) 1. TrainingtheConvNetonfacesfromextradata.The
for fusion of visual and audio features. The recent suc- architecture is described in Section 3.1.2.
cess of deep learning methods in challenging computer 2. Extraction of 7-class probabilities for each frame of
vision [27][31][21], language modeling [23] and speech the facetubes (described in Section 3.1.3).
recognition [18] tasks seems to carry over to emotion 3. Aggregation of single frame probabilities into fixed-
recognition,takingintoaccountthatthe2014challenge length video descriptors for each video in the com-
winners [30] also employed a deep convolutional neural petition dataset by expansion or contraction.
net, which they combined with other visual and audio 4. Classification of all video-clips using a support vec-
features using a Partial Least Squares (PLS) classifier. tor machine (SVM) trained on video descriptors of
The adoption of deep learning for visual features likely the competition training set.
playedabigroleintheconsiderableimprovementcom-
pared to their submission in the 2013 competition [29],
although the first and second runners up also reached Stage three and four are described in detail in Section
quite good performances without deep learning meth- 3.1.4. The pipeline is depicted in Figure 1. The strat-
ods;[34]usedahierarchicalclassifierforcombiningau- egy of training on extra data and using the competi-
dio and video features and [7] introduced an extension tion data only for classifier training and early stopping
of Histogram of Oriented Gradients (HOG) descriptors yielded a much lower training set accuracy of 46.87%,
for spatio-temporal data, which they fuse with other but it achieved a considerably better validation set ac-
visual and audio features using MKL. curacy of 38.96%.EmoNets:Multimodaldeeplearningapproachesforemotionrecognitionin video 3
Figure 1 Completepipelinedescribingthefinal strategyusedforourConvNet №1model.
3.1.1 Additional Face Dataset
The ’extra data’ we used for training of the deep net-
work is composed of two large static image datasets of
facial expressions for the seven emotion classes.
The first and larger one is the Google dataset [5]
consisting of 35,887 images with the seven facial ex-
pression classes: angry, disgust, fear, happy, sad, sur-
prise and neutral. The dataset was built by harvest- Figure 2 Raw images at the top and the corresponding IS-
preprocessedimagesbelow.
ing images returned from Google’s image search using
keywords related to expressions, then cleaned and la-
beled by hand. We use the grayscale 48×48 pixel ver- dataset, we used the diffusion-based approach intro-
sions of these images. The second one is the Toronto duced in [17]. We used the isotropic smoothing (IS)
Face Dataset (TFD) [35] containing 4,178 images la- function from the INface toolbox [33,38] with the de-
beled with basic emotions, essentially with only fully faultsmoothnessparameterandwithoutnormalization
frontal facing poses. aspost-processing.AcomparisonoforiginalandIS-pre-
To make the datasets compatible (there are big dif- processed face images is shown in figure 2.
ferences,forinstancevariationamongsubjects,lighting
andposes),weappliedthefollowingregistrationandil- 3.1.2 Extracting frame-wise emotion probabilites
lumination normalization strategies:
Our ConvNet uses the C++ and CUDA implemen-
tation written by Alex Krizhevsky [26] interfaced in
Registration To build a common dataset, TFD images
Python. The network’s architecture used here is pre-
and frames from the competition dataset had to be in-
sentedinFigure3.TheConvNettakesbatchesof48×48
tegrated with the Google dataset, for which we used
images as input and performs a random cropping into
thefollowingprocedure:Forimageregistrationweused
smaller40×40sub-imagesateachepoch.Theseimages
51 of the 68 facial keypoints extracted by the mixture
are then randomly flipped horizontally with a proba-
of trees method from [40]. The face contour keypoints
bility of 0.5. These two common methods allow us to
returned by this model were ignored in the registra-
expand the limited training set and avoid over-fitting.
tion process. Images from the Google dataset and the
The ConvNet architecture has 4 stages containing
AFEWdatasetshavedifferentposes,butmostfacesare
different layers. The first two stages include a convo-
frontal views.
lutional layer followed by a pooling layer, then a local
Toreducenoise,themeanshapeoffrontalposefaces
response normalization layer [27]. The third stage in-
for each dataset was used to compute the transforma-
cludes only a convolutional layer followed by a pool-
tionbetweenthetwoshapes.Forthetransformationthe
ing layer. Max-pooling is used in the first stage, while
Googledatawasconsideredasbaseshapeandthesim-
average-pooling is used in the next stages. The last
ilarity transformation was used to define the mapping.
stage consists of seven softmax units, which output
After inferring this mapping, all data was mapped to
sevenprobabilities,oneforeachofthesevenemotionla-
the Google data. TFD images have a tighter fit around
bels. The activation function used in the convolutional
faces,whileGoogledataincludesasmallborderaround
layersistherectifiedlinearunit(ReLU)activationfunc-
the faces. To make the two datasets compatible, we
tion. The two first convolutional layers use 64 filters
added a small noisy border to all images of TFD.
each, and the last one 128, all of size 5×5 pixels. Each
convolutional layer has the same learning parameters:
Illumination normalization using isotropic smoothing a 0.001 learning rate for the filters and 0.002 for bi-
To compensate for varying illumination in the merged ases, 0.9 momentum for both filters and biases and a4 SamiraEbrahimiKahouet al.
weight decay of 0.004 per epoch. The fully-connected new bounding boxes which more tightly frame the de-
layer shares the same hyperparameters except for the tected faces. To restrict the amount of motion of the
weightdecay,whichwesetto1.Thesehyperparameters bounding boxes the same kind of smoothing was also
are the same as the one provided by Krizhevsky [26] in applied to the center of the bounding boxes.
his example layers configuration files. The architecture Side lengths of the bounding boxes can vary due
is depicted in Figure 3. to changes of camera position or magnification (e.g.
Classification at test time is done using the 40×40 changing from a medium shot to a close-up shot). To
sub-images cropped from the center of the original im- be able to handle this, a further polynomial smooth-
ages. We stopped learning at 453 epochs using early- ing technique was applied directly on the bounding
stopping on the competition validation and train sets. box side lengths. Two low-order polynomials of degree
As stated earlier, we only used extra data to train the 0 (constant) and 1 (linear) were fit through the side
network, and the competition training and validation lengths of the bounding boxes. If the slope of the lin-
datasetswereonlyusedforearlystoppingandthesub- ear polynomial is above a scale threshold (slope · face-
sequent training of the SVM. tube length), we use the values of the linear polynomial
AshallowerConvNetwasexploredforthe2013com- as side lengths, else we use values from the constant
petition. It performed worse than ConvNet 1 and we smoothing polynomial. Empirically, we found that a
did not revisit it for the 2014 dataset. In the tables for threshold of 1.5 yielded reasonable results.
the AFEW2 results, it is referred to as ConvNet 2. For Thefinalfacetubeswerethengeneratedbycropping
details on the architecture see [22]. basedonthesmoothedboundingboxesandresizingthe
patchesto48×48.Per-frameemotionlabelprobabilities
were extracted for each facetube using the ConvNet.
3.1.3 Facetube extraction procedure
Forthecompetitiondatasetvideoframeswereextracted
preserving the original aspect ratio. Then the Google 3.1.4 Aggregation into video descriptors and
Picasafacedetector[14]wasusedtocropdetectedfaces classification
in each frame. To get the bounding box parameters
in the original image, we used Haar-like features for Weaggregatedtheper-frameprobabilitiesforallframes
matching, because direct pixel-to-pixel matching did ofafacetubeforwhichafacewasdetectedintoafixed-
not achieve the required performance. Picasa did not length video descriptor to be used as input to an SVM
detectfacesineveryframe.Tofixthis,wesearchedthe classifier.Forthisaggregationstepweconcatenatedthe
spatial neighborhood of the temporally closest bound- seven-dimensional probability vectors of ten successive
ing box for regions with an approximately matching frames, yielding 70 dimensional feature vectors. Most
histogram of color intensities. We used heuristics, such videos have more than ten frames and some are too
as the relative positioning, sizes and overlap, to asso- short and there are frames without detected faces. We
ciateboundingboxesofsuccessiveframesandgenerate resolved these problems using the following two aggre-
one facetube for each subject in the video. gation approaches:
For a few clips in the competition test sets, the Pi-
– Video averaging: For videos that were too long, we
casa face detector did not detect any faces. So we used
averaged the probability vectors of 10 independent
the combined landmark placement and face detection
groups of frames taken uniformly along time, con-
method described in [40] to find faces in these clips.
tracting the facetube to fit into the 10-frame video
Using the facial keypoints output by that model we
descriptors. This is depicted in Figure 4.
built bounding boxes and assembled them into face-
– Forvideosthatcontaintoofewframeswithdetected
tubes with the previously described procedure.
faces,weexpandedbyrepeatingframesuniformlyto
get 10 frames in total. This is depicted in Figure 5.
Facetube smoothing In order to get image sequences
wherefacesizesvarygradually,weappliedasmoothing The video descriptors for the training set were then
procedureonthecompetitionfacetubeboundingboxes usedtotrainanSVM(implementedby[6])witharadial
described in 3.1.3. For all images of a facetube, coor- basis function (RBF) kernel. The hyperparameters, γ
dinates of the opposite corners of the bounding boxes and c were tuned on the competition validation set.
were smoothed with a 2-sided moving average (using a The SVM type used in all experiments was a C-SVM
windowsizeof11frames).Thelargestcenteredsquares, classifier and the outputs are probability estimates so
that fit into these smoothed bounding boxes, yielded that the fusion with other results was simpler.EmoNets:Multimodaldeeplearningapproachesforemotionrecognitionin video 5
Figure 3 ThearchitectureofourConvNet №1.
validationusingthecompetitionvalidationdataset.We
initiallyusedarandomsearchforhyperparametersand
after the random search, we did manual finetuning of
hyperparameters.
3.2.1 Audio Preprocessing
Choosing the right features is a crucial aspect of the
audio classification. Mel-frequency cepstral coefficients
(MFCCs) are widely used for speech recognition; how-
Figure 4 Frame aggregation viaaveraging
ever, in this task we are mainly interested in detecting
emotions from the extracted audio features.
On the other hand emotion recognition on film au-
dioisquitedifferentfromotheraudiotasks.Inaddition
to speech in the audio track, background noise and the
soundtrackcanalsobesignificantindicatorsofemotion.
For the EmotiW challenge, we extracted 29 features
from each audio track using the yafee library1 with a
sampling rate of 48 kHz. We used all features provided
by the yafee library except “Frames”. Additionally 3
typesofMFCCfeaturesareused,thefirstused22cep-
stralcoefficients,thesecondusedafeaturetransforma-
Figure 5 Frame aggregation viaexpansion tion with the temporal first-order derivative and the
last one employed second-order temporal derivatives.
OnlinePCAwasappliedontheextractedfeatures,and
3.2 Audio & Deep Belief Networks 909 features per timescale were retained [16].
As we have described earlier, deep learning based tech-
3.2.2 DBN Pretraining
niques have ledto importantsuccessesin speech recog-
nition[18,15].Inthecontextofemotionrecognitionon
Weusedunsupervisedpre-trainingwithdeepbeliefnet-
audio features extracted from movie clips, we used a
works(DBN)ontheextractedaudiofeatures.TheDBN
deep learning approach for performing emotion recog-
has three layers of RBMs, the first layer is a Gaussian
nition just by pretraining a deep MLP as a deep belief
RBMwithnoisyrectifiedlinearunit(ReLU)nonlinear-
network (DBN) [19]. A DBN is a probabilistic gener-
ity [9], the second and third layer are both Gaussian-
ative model where each layer can be greedily trained
Bernoulli RBMs. We trained the RBMs using stochas-
as a Restricted Boltzmann Machine (RBM). Initially
ticmaximumlikelihoodandcontrastivedivergencewith
we trained the network as a DBN in an unsupervised
one Gibbs step (CD-1).
manner with greedy layerwise training procedure and
then we used supervised finetuning. In order to tune 1 Yaafe: audio features extraction toolbox: http://yaafe.
thehyperparametersofourmodel,weperformedcross- sourceforge.net/6 SamiraEbrahimiKahouet al.
EachRBMlayerhad350hiddenunits.Thefirstand of feature pooling technique worked best, if the fea-
second layer RBMs were trained with learning rates of tures are extracted from a bounded nonlinearity such
0.0006,0.0005and0.001respectively.AnL2penaltyof as sigmoid(.) or tanh(.).
2×10−3 and 2×10−4 was used for the first and sec-
ond layer, respectively. Both the first and second layer 3.2.4 Supervised Fine-tuning
RBMs were trained for 15 epochs on the competition
training dataset. We bounded the noisy ReLU activa- The competition training dataset was used for super-
tions of the first layer Gaussian RBM, specifically we visedfine-tuningandweappliedearlystoppingbymea-
used the activation function: min(α,max(0,x + ψ)), suring the error on the competition validation dataset.
where ψ ∼ N(0,σ(x)) with α = 6. Otherwise large ac- Thefeatureswerecenteredpriortotraining.Beforeini-
tivations of the first layer RBM were causing problems tiatingthesupervisedtraining,weshuffledtheorderof
trainingthesecondlayerGaussianBernoulliRBM.We clips. During the supervised fine-tuning phase, at each
used a Gaussian model of the form N(0,σ(x)), with 0 iteration on the training dataset, we randomly shuffled
mean and standard deviation of σ(x) = 1 . At the order of the features in the clip as well. At each
1+exp(−x)
the end of unsupervised pre-trainining, we initialized training iteration, we randomly dropped out 98 clips
a multilayer perceptron (MLP) with the ReLU non- fromthetrainingdatasetandwerandomlydroppedout
linearity for the first layer and sigmoid non-linearity 40% of the features in the clip. 0.121 % of the hidden
for the second layer using the weights and biases of the units are dropped out and we used a norm constraint
DBN. on the weights such that the L2 norm of the incoming
weights to a hidden unit does not exceed 1.2875 [20].
Inadditiontodrop-outandmaximumnormconstraint
3.2.3 Temporal Pooling for Audio Classification
on the weights, a L2 weight penalty with coefficient of
10−5 was used. The rmsprop adaptive learning rate al-
We used a multi-time-scale learning model [16] for the
gorithm was used to tune the learning rate with a vari-
MLP where we pooled the last hidden representation
ation of Nesterov’s Momentum [36]. RMSProp scales
layer of an MLP so as to aggregate information across
down parameter updates by a running average of the
frames before a final softmax layer. We experimented
gradient norm. At each iteration we keep track of the
with various pooling methods including max pooling
mean square of the gradients by:
and mean pooling, but we obtained the best results
withaspecificallydesignedtypeofpoolingfortheMLP RMS(∆ )=ρRMS(∆ )+(1−ρ)∆2 (2)
t+1 t t
features discussed below.
and compute the momentum, then do the stochastic
AssumethatwehaveamatrixAfortheactivations
gradient descent (SGD) update:
of the MLP’s last layer features that includes activa-
tionsofalltimescalesintheclipwhereA∈Rdt×df and ∂f(x(i);θ )
d t is the variable number of timescales, d f is the num- v t+1 =µv t−(cid:15) 0 ∂θ t , (3)
t
ber of features at each timescale. We sort the columns
of A in decreasing order and get the top N rows using
the map f :Rdt×df →RN×df. The most active N fea- µv −(cid:15) ∂f(x(i);θt)
tures are summarized with a weighted average of the θ t+1 =θ t+ (cid:112)t+1 0 ∂θt (4)
RMS(∆ )
top-N features: t+1
After performing crossvalidation, we decided to use an
1 (cid:88)N (cid:15) 0 = 0.0005 , µ = 0.46 and ρ = 0.92. We used early
F =
N
w if(i)(A;N) (1) stoppingbasedonthevalidationsetperformance,yield-
i=0 inganaccuracyof32.90%.Oncesupervisedfine-tuning
hadcompleted50iterations,ifthevalidationerrorcon-
where f(i)(A;N) is the ith highest active feature over
tinued increasing, the learning rate was decreased by a
time and weights should be:
(cid:80)N
w =N. During the
i=0 i factor of 0.99.
supervised finetuning, we feed the reduced features to
the top level softmax, we backpropagate through this
pooling function to the lower layers. We only used the 3.3 Activity recognition using a relational autoencoder
top 2 (N = 2) most active features in the weighted
average. Weights of the features were not learned and Given a video sequence with the task of extracting hu-
they were chosen as w = 1.4,w = 0.6 during train- man emotion labels, it seems reasonable to also con-
1 2
ing and w =1.3,w =0.7 during test time. This kind sider the temporal evolution of image frames. To this
1 2EmoNets:Multimodaldeeplearningapproachesforemotionrecognitionin video 7
Figure 6 Subsetoffilterslearned bySAEmodelonthe AFEW2 trainingset.Lefttoright:Frames1,3,5,7and9.
end we employ an activity recognition system for emo- the model on videos from the AFEW4 training set are
tion recognition based on local spatio-temporal feature visualized in Figure 6.
computation. Using local motion features for activity InpastworksIthasbeenshownthatspatiallycom-
recognition is a popular approach employed in many bininglocalfeatureslearnedfromsmallerinputregions
previous works [28,24,37,39]. leadstobetterrepresentationsthanfeatureslearnedon
larger regions [28,8]. Here, we utilize the same method
Traditionalmotionenergymodels[1]encodespatio-
by computing local feature descriptors for sub blocks
temporalfeaturesofsuccessivevideoframesassumsof
croppedfromthecornersofalarger14×20×20“super
squaredquadratureFourierorGaborcoefficientsacross
block”andconcatenatingthem,yieldingadescriptorof
multiplefrequenciesandorientations[28].Summingin-
motion for the region covered by the super block. PCA
duces invariance w.r.t. content, allowing the model to
was applied to this representation for dimensionality
yield a pure motion representation. In contrast to the
reduction,retainingthefirst100principalcomponents.
motion energy view, in [24] it has been shown that the
To generate descriptors for a whole video, super blocks
learning of transformations and introduction of invari-
arecroppeddenselyforeachvideowithastrideof7on
ancecanbeviewedastwoindependentaspectsoflearn-
the temporal axis and 10 on the spatial axes, i.e. with
ing. Based on that view, a single layered autoencoder
50%overlapofneighboringsuperblocks.TheK-means
based model named synchrony autoencoder (SAE) for
clustering step produces a dictionary of 3000 words,
learning motion representations was introduced. The
where each word represents a motion pattern. A nor-
classic approach is to use hand-engineered features for
malized histogram over K−means cluster assignment
spatio-temporal feature extraction [39]. In contrast to
frequencieswasgeneratedforeachvideoasinputtothe
hand-engineered features, deep learning based meth-
classifier.
odshavebeenshowntoyieldlow-levelmotionfeatures,
In our experiments we observed that the classifier
which generalize well across datasets [28,24].
trainedonthemotionfeaturesseemedtooverfitonthe
We use a pipeline commonly employed in works on training set and all investigated measures to avoid this
activity recognition [28,24,39] with the SAE model for problem(e.g.augmentingthedatasetbyrandomlyap-
local motion feature computation. We chose to use the plying affine transformations to the input videos) were
SAE model because, compared to other learning based also not helpful. This could be due to the videos show-
methods like ISA [28] and convGBM [37] with com- ing little to no motion cues that correlate heavily with
plex learning rules, it can be trained very efficiently, the emotion labels. The motion model by itself is not
while performing competitively. The activity recogni- very strong at discriminating emotions, but it is useful
tion pipeline follows a bag-of-words approach. It con- inthistask,nonetheless.Ithelpstodisambiguatecases,
sists mainly of three modules: motion feature extrac- where other modalities are not very confident, because
tion,K-meansvectorquantizationandaχ2kernelSVM
itrepresentssomecharacteristicsofthedataadditional
forclassification.TheSAEmodelactsasfeatureextrac- to those described by the other modalities.
tor.Itistrainedonsmallvideoblocksofsize10×16×16
(time×rows×columns) randomly cropped from the
competition training set. They are preprocessed using 3.4 Bag of mouth features and shallow networks
PCA for whitening and dimensionality reduction, re-
taining 300 principal components. The number of ran- Someemotionsmayberecognizedfrommouthfeatures.
domlycroppedtrainingsamplesis200,000.Thesizeof For example, a smile often indicates happiness while
theSAE’shiddenlayerwasfixedat300.Themodelwas an “O”-shaped open mouth may signal surprise. For
trained using SGD with a learning rate of 0.0001 and oursubmission,facetubes,describedinsection3.1.3,in
momentum 0.9 for 1,000 epochs. The filters learned by resolution 96×96 were cropped around a region where8 SamiraEbrahimiKahouet al.
the mouth usually lies. This region was globally chosen comparisons on AFEW2 can be found in [22], but we
byvisualizingmanytrainingimages,butamoreprecise provide some highlights here.
method, such as mouth keypoint extraction [40], could
also be applied. AFEW2 FromourexperimentswithAFEW2wefound
We mostly follow the method introduced by Coates that ConvNet1 yielded the highest validation set accu-
et al. [8], which achieved state-of-the-art performance racy. We therefore selected this model as our first sub-
ontheCIFAR-10dataset[25]in2011,eventhoughthat mission and it yielded a test set accuracy of 35.58%.
methodhassincebeensupersededbyconvolutionalnet- This is also indicated in table 1 which contains a sum-
works.Asafirststep,eachmouthimageisdividedinto mary of all our submissions. ConvNet2 was our second
16equallysizedsections,fromwhichmany8×8patches highestperformer,followedcloselybythebagofmouth
areextracted.Thesearenormalizedbyindividuallyset- and audio models at 30.81%, 30.05% and 29.29% re-
ting the mean pixel intensity to 0 and the variance to spectively.
1. After centering all patches from the same spatial re-
gion,weapplywhitening,whichwasshowntobeuseful
AFEW4 HereagainourConvNet1modelachievedthe
for this kind of approach [8], keeping 90% of the vari-
best results on the validation set for AFEW4. It was
ance.Foreachofthe16regions,400centroidsarefound
followed by our audio model which here yields higher
by applying the k-means algorithm on the whitened
performance than the bag of mouths model by a good
patches.
margin, at 34.20% and 27.42% accuracy respectively.
For any given image, patches are densely extracted
We explored the strategies outlined in Sections 4.1,
from each of the 16 regions and pre-processed as de-
4.2 and 4.3 to combine models for the AFEW2 evalua-
scribedabove.Eachpatchisassigneda400-dimensional
tion. Section 4.4 presents the strategy we used for our
vector by comparing it to the centroids with the trian-
experiments with the AFEW4.
gleactivationfunction[8],wheretheEuclideandistance
z between the patch and each centroid is computed,
k
aswellasthemeanµofthesedistances.Theactivation
4.1 Averaged Predictions – AFEW2
of each feature is given by max(0,µ−z ), so that only
k
centroids closer than the mean distance are assigned a
A simple way to make a final prediction using several
positivevalue,whiledistantonesstayat0.Aswehavea
models is to take the average of their predictions. We
400-dimensional representation for each patch, the im-
had 5 models in total, which gives
(cid:80)n (cid:0)n(cid:1)
=31 pos-
age representation would become extremely large if we i=1 i
sible combinations (order has no importance). In this
simply concatenated all feature vectors. For this rea-
contextitispossibletotestallcombinationsontheval-
son, we pool over all features of a region to get a local
idation set to find those which are the most promising.
region descriptor. The region descriptors are then con-
Through this analysis we found that the average
catenated to obtain a 6,400 dimensional representation
of all models yielded the highest validation set perfor-
of the image.
mance of 40.15% on AFEW2. The validation set con-
Thispoolinggenerallyusestheaverageactivationof
fusion matrix for this model is shown in figure 8 (a).
eachfeature,althoughwealsotriedtakingthestandard
For our third 2013 submission we therefore submitted
deviationacrosspatchesforeachfeature.Aregularized
the results of the averaged predictions of all models,
logistic regression classifier is trained on a frame-by-
yielding37.17%onthetest.Fromthisanalysiswealso
frame basis with the pooled features as input. When
found that the exact same validation set performance
classifying a test video, the predictions of the model
was also obtained with an average not including our
are averaged over all its frames.
second convolutional network, leading us to make the
conclusion that both convolutional networks were pro-
4 Experimental results viding similar information. We thus left it out for sub-
sequent strategies and experiments on the AFEW4.
In figure 7 (a-d) we show the validation set confusion The next highest performing simple average was
matrices from the models yielding the highest AFEW4 39.90% and consisted of simply combining ConvNet 1
validation set accuracy for each of the techniques dis- and our audio model. Given this observation and the
cussedinsection3.Asecondconvolutionalnetworkfor fact that the conference baselines included both video,
faces(Convnet#2),whichweexplored,isnotpresented audio and combined audio-video models we decided to
hereasitobtainedlowerperformancecomparedtoCon- submitamodelinwhichweusedonlythesetwomodels.
vnet #1 and used similar information to make its pre- However, we first explored a more sophisticated way to
dictions. A more detailed analysis of Convnet #2 and perform this combination.EmoNets:Multimodaldeeplearningapproachesforemotionrecognitionin video 9
Table 1 Our7submissionswithtraining,validationandtestaccuraciesfor theEmotiW2013competition.
Sub. Train Valid Test Method
1 45.79 38.13 35.58 Googledata&TFDused totrainConvNet1, SVMtrainedonaggregatedframescores
2 71.84 42.17 38.46 ConvNet1(fromsubmission1) combinedwithAudiomodelusing another SVM
3 97.11 40.15 37.17 Mean predictionfrom:Activity,Audio,Bagof mouth,ConvNet1, ConvNet2
4 98.68 43.69 32.69 SVM withdetailedhyperparametersearch:Activity,Audio,Bagofmouth, ConvNet1
5 94.74 47.98 39.42 Shortuniform random search:Activity, Audio,Bagofmouth,CN1,CN1 +Audio
6 94.74 48.48 40.06 Shortlocalrandomsearch:Activity,Audio,Bagof mouth,CN1,CN1+Audio
7 92.37 49.49 41.03 Moderate localrandomsearch:Activity,Audio, Bagofmouth,CN1,CN1+ Audio
Table 2 Ourselectedsubmissionswithtestaccuraciesforthe EmotiW2014competition.
Sub. Test Method
1 39.80 Trainedmodelon2013data,BoMfaileddue todifferentdata format andreplacedby uniform
2 37.84 Trainedmodelon2013data,re-learningrandomsearchwithoutfailedBofM
3 44.71 ConvNet1+Audio modelcombinedwithSVM, alltrainedontrain+valid
4 41.52 ConvNet1+Audio modelcombinedwithSVMtrained onswappedpredictions
5 37.35 Googledata&TFD usedtotrainConvNet 1,framescoresaggregatedwithSVM
6 42.26 Allmodels combinedwithSVM trainedonvalidationpredictions
7 44.72 Allmodels combinedwithrandomsearchoptimizedon validation predictions
8 42.51 Only twomodelsweretrainedontrain+validationincombination,othersusedtrainsetonly
9 47.67 Allmodelscombinedwithrandomsearch optimizedonfullswappedpredictions
10 45.45 Baggingof 350 modelssimilar tosubmission9
Angry Disgust Fear Happy Sad Surprise Neutral Angry Disgust Fear Happy Sad Surprise Neutral Angry Disgust Fear Happy Sad Surprise Neutral Angry Disgust Fear Happy Sad Surprise Neutral
Angry 32 2 7 6 7 1 9 Angry 39 0 5 11 3 1 5 Angry 31 0 0 10 4 6 8 Angry 31 0 4 7 6 3 8
Disgust 14 1 1 3 10 1 10 Disgust 5 5 1 10 10 1 8 Disgust 10 4 2 12 8 1 13 Disgust 11 10 2 8 7 1 11
Fear 6 0 10 5 12 2 10 Fear 11 0 13 12 3 1 6 Fear 15 0 9 6 2 12 10 Fear 6 2 19 4 6 5 12
Happy 6 0 2 48 5 0 2 Happy 3 1 7 26 7 1 18 Happy 3 2 1 50 1 3 2 Happy 3 2 0 50 3 2 2
Sad 4 1 4 6 29 4 12 Sad 1 3 7 12 15 0 23 Sad 12 2 6 11 14 5 14 Sad 4 3 6 6 31 3 11
Surprise 5 0 14 2 2 4 18 Surprise 9 0 8 8 2 1 18 Surprise 5 2 8 6 2 20 9 Surprise 4 4 10 3 0 20 11
Neutral 5 0 2 6 12 0 36 Neutral 2 1 3 13 10 2 32 Neutral 7 5 0 8 4 0 31 Neutral 3 1 2 5 7 2 35
(a) ConvNet 1, (45.97, 42.33, (b) Audio,(53.46, 34.20,-) (a) Simple Average of Mod- (b) Random Search on
37.35*) els,(97.11,40.15,37.17) WeightedAvg.,(92.37,49.49,
41.03)
AngryA 3ng 1ry Dis 0gust F e 9ar H 1ap 2py S 5ad Sur p 0rise Ne u 7tral AngryA 2ng 4ry Dis 5gust F e 9ar H 1ap 3py S 5ad Sur 4prise Ne u 4tral Angry Disgust Fear Happy Sad Surprise Neutral Angry Disgust Fear Happy Sad Surprise Neutral
Disgust 10 0 0 18 0 0 12 Disgust 10 7 4 5 3 4 7 Angry 33 0 2 2 5 5 11 Angry 36 4 2 6 1 2 7
Fear 15 0 10 14 3 0 4 Fear 14 3 10 5 2 8 4 Disgust 3 0 0 1 13 1 8 Disgust 3 1 0 1 13 0 8
Happy 22 0 5 15 1 1 19 Happy 11 7 3 32 3 1 6 Fear 9 2 10 2 11 6 6 Fear 11 0 17 2 7 4 5
Sad 4 0 3 24 1 1 28 Sad 16 3 4 4 13 5 16 Happy 4 1 0 50 8 1 17 Happy 4 1 0 54 9 0 14
Surprise 14 0 4 16 0 0 12 Surprise 14 4 10 3 3 3 9 Sad 3 4 2 5 19 6 14 Sad 3 8 4 6 19 5 8
Neutral 9 0 0 10 4 1 39 Neutral 18 9 3 13 2 2 16 Surprise 5 1 3 1 7 5 4 Surprise 5 1 5 3 4 4 4
Neutral 6 5 1 4 27 9 65 Neutral 10 7 8 4 20 5 63
(c) Activity Rec., (46.37, (d) Bag of mouth, (93.08,
25.07,-) 27.42,-) (c) ConvNet 1 & Audio, (d) All modalities (submis-
Figure 7 Confusion matrices for the AFEW4 validation (-,-,44.71*) sion9), (-, -,47.67*)
set. Accuracies for each method are specified in parentheses Figure 8 Confusion matrices on the test set of AFEW2
(training, validation & test sets, if applicable). *Model has (a-b) and AFEW4 (c-d). Accuracies for each method are
been retrained on both training and validation set prior to specified in parentheses (training, validation & test sets, if
testing applicable). *Model has been retained on both training and
validationsetpriorto testing10 SamiraEbrahimiKahouet al.
4.2 SVM and MLP Aggregation Techniques – AFEW2 we used the weighting that yielded the highest valida-
tion set performance (47.98%) as our 5th 2013 submis-
To further boost the performance of our combined au- sion. This yielded a test set accuracy of 39.42%. We
dioandvideomodelwesimplyconcatenatedtheresults usedtheresultsofthisinitialrandomsearchtoinitiate
of our ConvNet 1 and audio model using vectors and asecond,localsearchprocedurewhichisanalogousina
learnedaSVMwithanRBFkernelusingthechallenge sensetothetypicaltwolevelcoarse,thenfinelevelgrid
trainingset.ThehyperparametersoftheSVMwereset search used for SVMs. In this procedure we generated
viaatwostagecoarse,thenfinegridsearchoverinteger random weights using a Gaussian distribution around
powers of 10, then non-integer powers of 2 within the the best weights found so far. The weights were tested
reduced region of space. The hyperparameters corre- by calculating the accuracy of the so-weighted aver-
spondtoakernelwidthterm,γ andthecparameterof age predictions on the validation set. We also rounded
SVMs. This process yielded an accuracy of 42.17% on these random weights to 2 decimals to help to avoid
thevalidationset,whichbecameoursecondsubmission overfitting on the validation set. This strategy yielded
and produced a test accuracy of 38.46%. 40.06% test set accuracy with a short duration search
GiventhesuccessofourSVMcombinationstrategy, and41.03%withalongersearch-ourbestperforming
wetriedthesametechniqueusingthepredictionsofall 2013 submission on the test. The validation set confu-
models. However, this process quickly overfit the train- sion matrix for this model is shown in figure 8 (b) and
ing data and we were not able to produce any models the weights obtained through this process are shown in
that improved upon our best validation set accuracy figure 9 (a).
obtained via the ConvNet 1 and audio model. We ob-
served a similar effect using a strategy based upon an
ML WP eto thco em reb foin ree tt rh ie edre asu mlts oro ef sa oll pm hio std ie cl atp er ded Sic Vt Mions h.
y-
Angry Disgust
Fear
Happy
Sad
Surprise Neutral
perparameter search to re-weight different models and
activity .06 .00 .17 .10 .27 .05 .47
theirpredictionsfordifferentemotions.Weimplemented
audio .65 .13 .00 .26 .07 .35 .10
this via a search over discretized [0,1,2,3] per dimen-
sionscalingfactors.Whilethisresultedin28additional bagofmouth .00 .03 .35 .27 .18 .33 .00
hyperparametersthisdiscretizationstrategyallowedus convnet .06 .11 .00 .36 .18 .20 .42
to explore all combinations. This more detailed hyper- convnet+audio .23 .73 .48 .10 .30 .06 .00
parametertuningdidallowustoincreasethevalidation
set performance to 43.69%. This became our fourth (a) Emotiw2013
2013 submission; however, the strategy yielded a de-
creased test set performance at 32.69%. Angry Disgust
Fear
Happy
Sad
Surprise Neutral
4.3 Random Search for Weighting Models – AFEW2 activity .32 .00 .10 .33 .18 .63 .18
audio .00 .22 .00 .25 .16 .00 .00
Recent work [3] has shown that random search for hy-
bagofmouth .00 .19 .27 .10 .20 .08 .18
perparameter optimization can be an effective strat-
egy, even when the dimensionality of hyperparameters convnet .08 .00 .10 .00 .08 .00 .00
ismoderate(ex.35dimensions).Analysisofourvalida- convnet+audio .61 .58 .53 .32 .37 .29 .63
tion set confusion matrices shows that different models
(b) EmotiW2014
have very different performance characteristics across
the different emotion types. We therefore formulated
Figure9 Finalweightsusedformodelaveraginginourbest
the re-weighting of per-model and per-emotion predic- submissions.
tionsasahyperparametersearchoversimplexes,weight-
ing the model predictions for each emotion type.
Toperformtherandomsearch,wefirstsampledran-
domweightsfromauniformdistributionandthennor- 4.4 Strategies for the Emotiw 2014 Challenge and the
malizedthemtoproducesevensimplexes.Thisprocess AFEW4 Data
is slightly biased towards weights that are less extreme
compared to other well known procedures that are ca- WhilewedidnotparticipateintheEmotiW2014chal-
pable of generating uniform values on simplexes. After lenge we have performed a sequence of experiments us-
running this sampling procedure for a number of hours ing the underlying AFEW4 dataset and the training,EmoNets:Multimodaldeeplearningapproachesforemotionrecognitionin video 11
validation and test sets partitions defined by the chal- hyperparameter settings and retrain the model using
lengeorganizers.Wehaveperformedtheseexperiments the combined training and validation set. This method
afterthechallengeperiodsoastoexplorethebehaviour is known to work well in practice.
ofourgeneraltechniqueaswellassomedifferenttrain- We first used this method to train an SVM to com-
ing strategies arising from the fact that the challenge bine the predictions of the ConvNet1 model and the
is defined differently. Specifically, in the EmotiW 2014 audiomodel.Itresultedin44.71%testaccuracy,anim-
challenge it is permitted to re-train all models using pressive7%improvementoverConvNet1alone(37.35%)
the combined training and validation set if desired. We and6%improvementoverthesamecombinationtrained
correspondinglyexploredthefollowingsetofstrategies. onlyonthe2013AFEW2trainingset(38.26%).Anim-
As an initial step, we simply re-ran our best model portant factor might be that we are using predictions
fromthe2013competition,withoutretrainingitonthe ondatanotseenduringsub-modeltrainingtotrainthe
2014 competition dataset. Predictions of Bag-of-mouth combination model. That is, they are less biased than
modelwerereplacedbyuniformdistribution.OurBag- training predictions, which makes it possible for the
of-mouth model was trained on faces provided by the SVM to generalize better. The validation set alone is,
organizers which were RGB in 2013 and grayscale in however, too small to train a good combination model.
2014, this caused the model to fail on new dataset. Us- Tocapitalizeonthiseffect,wetrainedanotherSVM
ing models trained on AFEW2, we computed predic- onswappedpredictions,i.e.thepredictionsonthevali-
tions on AFEW4 test set, which gave 39.80% accuracy. dationsetcamefromsub-modelstrainedontrainingset
The 1% loss could possibly be attributed to the substi- and predictions on training set came from sub-models
tution of the Bag-of-mouth model with uniform distri- trained on the validation set. An SVM was trained on
bution. However, sound comparison with previous re- both swapped sets separately to select the best hyper-
sultscannotbemadeasAFEW2andAFEW4testsets parameters before training a final SVM on all swapped
are different. Retraining the combination model on all predictions. With 41.52% test accuracy, this model is
models trained on AFEW2 but bag-of-mouth resulted worsethenthepreviousone(44.71%).Apossiblereason
in a lower 37.84% accuracy. We used a more aggres- for this is that the training and validation sets are un-
sive random search procedure by starting hundreds of balancedandrelativelysmall.Goodsub-modelstrained
randomsearcheswithdifferentinitializations.Thegen- onthelargertrainingsettendtogenerategoodpredic-
eralization decrease from submission 1 to 2 was most tions on small validation sets, while worse sub-models
likely caused by overfitting because of this aggressive trained on the small validation set generate worse pre-
random search. Nevertheless, as AFEW4 training and dictionsonthebiggertrainingset.Anobvioussolution
validation sets are larger than their AFEW2 relatives, would be to generate swapped predictions in a manner
models trained on the latter might not be competitive similar to leave-one-out cross-validation, the drawback
in the Emotiw 2014 Challenge. Therefore, we trained is that for our setting we would need to train 5 times
our models on AFEW4 data for submission 3 to 10. 900 models on each fold to generate the predictions for
In preparation for the following sets of experiments the meta-model.
all sub-models were trained on training set and valida- Finally, similar to section 4.3, we trained the SVM
tion set alone. They were also trained on training set only on validation data. We hoped training an SVM
combined with validation set. This yields three differ- wouldyieldresultssimilartorunningrandomsearch.It
entsetsofpredictionsfromwhichonemayexploreand did not. As explained in next section, running random
compare different training and combination strategies. search on the validation set predictions gives 44.72%
Training on the training set and the validation set sep- whiletraininganSVMonsamedatagivesonly42.26%.
arately allowed us to easily do 2-fold cross-validation,
whiletrainingonalldatacombinedisacommonlyused
4.4.2 Weighting models and random search using all
strategytoexploitallavailabledatabutcaninvolvedif-
data
ferent techniques for setting model hyperparameters.
A random search procedure for determining the pa-
4.4.1 An SVM combination approach using all data rameters of a linear per-class and per-model weighting
was computed as described in section 4.3, but for the
One simple method for learning when working with a AFEW4 (EmotiW 2014 challenge data). For our first
single training set and a single validation set is to use experimentwerunarandomsearchusingthevalidation
the training set to train a model multiple times with setpredictions,thenusedtheresultingweightstocom-
different hyperparameters, then select the best model pute the weighted average of predictions of sub-models
usingthevalidationset.Onecanthensimplyusethese trained on all data. To be clear, the only difference to12 SamiraEbrahimiKahouet al.
Table 3 Test accuracies of different approaches on AFEW2 convolutionalnetwork.Thevalidationsetaccuracywas
(left)andAFEW4(right) significantlyhigherthaninourexperimentinwhichwe
trained the network directly on extracted faces from
Method % Method %
thechallengedata.Itisourintuitionthatvideoframes
MKL[32] 35.89% PLS[30] 50.37%
in isolation are not always representative of the emo-
PLS[29] 34.61% HCF[34] 47.17%
tional tag assigned to the clip, and using one label for
LinearSVM[13] 29.81% MKL[7] 45.21%
video length introduces noise to the training set. In
Ourmethod[22] 41.03% Ourmethod 47.67%
contrast, our additional data contained only still im-
ages with a clear correspondence between image and
ourbestmodelfrom2013submissions,wasthatweap-
label.Theproblemofoverfittinghadbothdirectconse-
plied the weighted average on sub-models trained on
quencesonper-modelperformanceonthevalidationset
the combined training and validation set of the 2014
as well as indirect consequences on our ability to com-
dataset. This yielded a test accuracy of 44.72%, 2%
bine model predictions. Our analysis of simple model
higher than the same procedure with SVM training,
averaging showed that no combination of models could
but no gain over the best combination of ConvNet1
yield superior performance to an SVM applied to the
with audio models (44.71%).
outputsofouraudio-videomodels.Oureffortstocreate
Randomsearchcanalsobeappliedtoswappedpre-
bothSVMandMLPaggregationmodelsleadtosimilar
dictions such as those explained in the previous sec-
observations in that models quickly overfit the train-
tion. Running random search on such predictions gave
ing data and no settings of hyperparameters could be
our best results on AFEW4, 47.67%, slightly higher
found which would yield increased validation set per-
than the first runner up in the EmotiW 2014 compe-
formance. We believe this is due to the fact that the
tition [34]. The weights found through this procedure
activity recognition and bag of mouth models severely
areshowninFigure9(b).Acomparisonoftestaccura-
overfitthechallengetrainingsetandtheSVMandMLP
cies for both the 2013 and 2014 EmotiW datasets with
aggregationtechniques-beingquiteflexible-overfitthe
other methods is shown in table 3.
data in such a way that no traditional hyperparameter
Assomemodelswereoverfittingtothetrainingdata,
tuning could yield validation set performance gains.
we tried to separate overfitters from the other models
Theseobservationsledustodevelopthenoveltech-
and combine them together. We ran a random search
nique of aggregating the per model and per class pre-
on ConvNet1, Bag-of-mouth and activity recognition
dictions via random search over simple weighted aver-
predictions of validation data. Then we ran a second
ages. The resulting aggregation technique is therefore
random search on top of their weighted average with
of extremely low complexity and the underlying pre-
our ConvNet1+Audio SVM combination of submission
diction was therefore highly constrained - using simple
3.Thisfinalweightedaveragewasusedtocomputethe
weighted combinations of complex deep network mod-
test predictions, giving only 42.51%.
els, each of which did reasonably well at this task. We
Weights found by random search varied a lot from
were therefore able to explore many configurations in
one run to another. We tried bagging of 350 indepen-
a space of moderate dimensionality quite rapidly as we
dent weighted averages found by random searches sim-
did not need to re-evaluate the predictions from the
ilar to submission 9 (which obtained 47.67%). Surpris-
neuralnetworksandwedidnotadapttheirparameters.
ingly, the bagging approach achieved a lower accuracy
As this obtained a marked increase in performance on
of 45.45%, our second best result on AFEW4.
boththechallengevalidationandtestsets,itleadusto
thefollowinginterpretation:Giventhepresenceofmod-
elsthatoverfitthetrainingdata,itmaybebetterprac-
5 Conclusions and discussion
tice to search a moderate space of simple combination
Ourexperimentswithbothcompetitiondatasets(2013 models. This is in contrast to traditional approaches
and 2014) have lead to a number of contributions and suchassearchingoverthesmallerspaceofSVMhyper-
insights which we believe may be more broadly appli- parameters or even a moderately sized space of tradi-
cable. First, we believe that our approach of using the tional MLP hyperparameters including the number of
largescaleminingofimageryfromGoogleimagesearch hidden layers and the number of units per layer.
totrainourdeepneuralnetworkhashelpedustoavoid Acknowledgements The authors would like to thank
overfitting to the provided challenge dataset. thedevelopersofTheano[2,4].WethankNSERC,Ubisoft,
We achieved better performance when we used the the German BMBF, project 01GQ0841 and CIFAR for
competition data exclusively for training the classifier their support. We also thank Abhishek Aggarwal, Em-
and used additional face image data for training of the manuel Bengio, J¨org Bornschein, Pierre-Luc Carrier,EmoNets:Multimodaldeeplearningapproachesforemotionrecognitionin video 13
Myriam Cˆot´e, Guillaume Desjardins, David Krueger, 18. Hinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed,
RazvanPascanu,Jean-PhilippeRaymond,ArjunSharma, A.R., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P.,
Sainath, T.N., et al.: Deep neural networks for acoustic
Atousa Torabi, Zhenzhou Wu, and Jeremie Zumer for
modelinginspeechrecognition:Thesharedviewsoffour
their work on 2013 submission.
researchgroups. IEEESig.Proc.Magazine,29(6),82–97
(2012)
19. Hinton, G., Osindero, S., Teh, Y.W.: A fast learning al-
References gorithm for deep belief nets. Neural computation 18(7),
1527–1554(2006)
1. Adelson,E.H.,Bergen,J.R.:Spatiotemporalenergymod- 20. Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever,
els for the perception of motion. JOSA A 2(2), 284–299 I., Salakhutdinov, R.: Improving neural networks
(1985) by preventing co-adaptation of feature detectors.
2. Bastien,F.,Lamblin,P.,Pascanu,R.,Bergstra,J.,Good- arXiv:1207.0580(2012)
fellow,I.,Bergeron,A.,Bouchard,N.,Warde-Farley,D., 21. Kahou, S.E., Froumenty, P., Pal, C.: Facial expression
Bengio, Y.: Theano: new features and speed improve- analysis based on high dimensional binary features. In:
ments. arXivpreprintarXiv:1211.5590(2012) ECCVWorkshoponComputerVisionwithLocalBinary
3. Bergstra, J., Bengio, Y.: Random search for hyper- Patterns Variants. Zurich,Switzerland (2014)
parameteroptimization. JMLR 13,281–305(2012)
22. Kahou,S.E.,Pal,C.,Bouthillier,X.,Froumenty,P.,Gul-
4. Bergstra,J.,Breuleux,O.,Bastien,F.,Lamblin,P.,Pas-
cehre,C.,Memisevic,R.,Vincent,P.,Courville,A.,Ben-
canu, R., Desjardins, G., Turian, J., Warde-Farley, D.,
gio, Y., Ferrari, R.C., Mirza, M., Jean, S., Carrier, P.L.,
Bengio,Y.:Theano:acpuandgpumathexpressioncom-
Dauphin,Y.,Boulanger-Lewandowski,N.,Aggarwal,A.,
piler. In: Proceedings of the Python for scientific com-
Zumer, J., Lamblin, P., Raymond, J.P., Desjardins, G.,
putingconference(SciPy),vol.4,p.3.Austin,TX(2010)
Pascanu, R., Warde-Farley, D., Torabi, A., Sharma, A.,
5. Carrier, P.L., Courville, A., Goodfellow, I.J., Mirza, M.,
Bengio, E., Coˆt´e, M., Konda, K.R., Wu, Z.: Combining
Bengio, Y.: FER-2013 Face Database. Tech. rep., 1365,
modalityspecificdeepneuralnetworksforemotionrecog-
Universit´e deMontr´eal(2013)
nitionin video. In: Proceedings ofthe15th ACMon In-
6. Chang, C.C., Lin, C.J.: LIBSVM: A library for support
ternationalConferenceonMultimodalInteraction,ICMI
vector machines. ACM Transactions on Intelligent Sys-
’13(2013)
temsandTechnology 2,27:1–27:27(2011)
23. Kalchbrenner, N., Grefenstette, E., Blunsom, P.: A
7. Chen, J., Chen, Z.,Chi, Z., Fu, H.: Emotion recognition
convolutional neural network for modelling sentences.
inthewildwithfeaturefusionandmultiplekernellearn-
arXiv:1404.2188(2014)
ing. In:Proceedingsofthe16thInternationalConference
24. Konda, K.R., Memisevic, R., Michalski, V.: The role of
onMultimodalInteraction, pp.508–513.ACM (2014)
8. Coates, A., Lee, H., Ng, A.Y.: An Analysis of Single- spatio-temporalsynchronyintheencodingofmotion. In:
Layer Networks in Unsupervised Feature Learning. In: ICLR(2014)
AISTATS(2011) 25. Krizhevsky,A.:Learningmultiplelayersoffeaturesfrom
9. Dahl,G.E.,Sainath,T.N.,Hinton,G.E.:Improvingdeep tiny images. Tech.rep.(2009)
neural networks for lvcsr using rectified linear units and 26. Krizhevsky, A.: Cuda-convnet Google code home page.
dropout. In:Proc.ICASSP (2013) https://code.google.com/p/cuda-convnet/ (2012)
10. Dhall, A., Goecke, R., Joshi, J., Sikka, K., Gedeon, T.: 27. Krizhevsky,A.,Sutskever,I.,Hinton,G.:Imagenetclas-
Emotionrecognitioninthewildchallenge2014:Baseline, sification with deep convolutional neural networks. In:
data and protocol. In: Proceedings of the 16th Interna- NIPS,pp.1106–1114(2012)
tional Conference on Multimodal Interaction, pp. 461– 28. Le,Q.,Zou,W.,Yeung,S.,Ng,A.:Learninghierarchical
466.ACM(2014) invariant spatio-temporal features for action recognition
11. Dhall,A.,Goecke,R.,Joshi,J.,Wagner,M.,Gedeon,T.: withindependentsubspace analysis. In:CVPR(2011)
Emotionrecognitioninthewildchallenge2013. In:ACM
29. Liu, M., Wang, R., Huang, Z., Shan, S., Chen, X.: Par-
ICMI(2013)
tialleastsquaresregressionongrassmannianmanifoldfor
12. Dhall, A., Goecke, R., Lucey, S., Gedeon, T.: Collecting
emotionrecognition.In:Proceedingsofthe15thACMon
large, richly annotated facial-expression databases from
International conference on multimodal interaction, pp.
movies. IEEEMultiMedia(3),34–41(2012)
525–530.ACM (2013)
13. Gehrig,T.,Ekenel,H.K.:Whyisfacialexpressionanaly-
30. Liu,M.,Wang,R.,Li,S.,Shan,S.,Huang,Z.,Chen,X.:
sisinthewildchallenging?In:Proceedingsofthe2013on
Combiningmultiplekernelmethodsonriemannianman-
Emotionrecognitioninthewildchallengeandworkshop,
ifoldforemotionrecognitioninthewild. In:Proceedings
pp.9–16.ACM(2013)
of the 16th International Conference on Multimodal In-
14. Google: The Google picasa face detector (2013). URL
teraction,pp.494–501.ACM(2014)
http://picasa.google.com. [accessed1-Aug-2013]
15. Graves,A.,Mohamed,A.R.,Hinton,G.:Speechrecogni- 31. Neverova, N., Wolf, C., Taylor, G.W., Nebout, F.:
tion with deep recurrent neural networks. In: Acoustics, Moddrop: adaptive multi-modal gesture recognition.
Speech and Signal Processing (ICASSP), 2013 IEEE In- arXiv:1501.00102(2014)
ternationalConferenceon,pp. 6645–6649. IEEE(2013) 32. Sikka, K., Dykstra, K., Sathyanarayana, S., Littlewort,
16. Hamel, P., Lemieux, S., Bengio, Y., Eck, D.: Temporal G., Bartlett, M.: Multiple kernel learning for emotion
pooling and multiscale learning for automatic annota- recognitioninthewild. In:Proceedingsofthe15thACM
tionandrankingofmusicaudio. In:ISMIR,pp.729–734 on International conference on multimodal interaction,
(2011) pp.517–524.ACM(2013)
17. Heusch, G., Cardinaux, F., Marcel, S.: Lighting normal- 33. Sˇtruc, V., Paveˇsi´c, N.: Gabor-based kernel partial-least-
ization algorithms for face verification. IDIAP Commu- squares discrimination features for face recognition. In-
nicationCom05-03(2005) formatica20(1),115–138(2009)14 SamiraEbrahimiKahouet al.
34. Sun,B.,Li,L.,Zuo,T.,Chen,Y.,Zhou,G.,Wu,X.:Com-
biningmultimodalfeatureswithhierarchicalclassifierfu-
sionforemotionrecognitioninthewild. In:Proceedings
of the 16th International Conference on Multimodal In-
teraction,pp.481–486.ACM(2014)
35. Susskind,J.,Anderson,A.,Hinton,G.:Thetorontoface
database. Tech.rep.,UTMLTR2010-001,Universityof
Toronto(2010)
36. Sutskever, I., Martens, J., Dahl, G., Hinton, G.: On
the importance of initialization and momentum in deep
learning. In:ICML 2013 (2013)
37. Taylor, G.W., Fergus, R., LeCun, Y., Bregler, C.: Con-
volutional learning of spatio-temporal features. In: Pro-
ceedings of the 11th European conference on Computer
vision:PartVI,ECCV’10(2010)
38. Sˇtruc, V., Paveˇsi´c, N.: Photometric normalization tech-
niques for illumination invariance, pp. 279–300. IGI-
Global(2011)
39. Wang, H., Ullah, M.M., Kl¨aser, A., Laptev, I., Schmid,
C.:Evaluationoflocalspatio-temporalfeaturesforaction
recognition. In:BMVC(2009)
40. Zhu, X., Ramanan, D.: Face Detection, Pose Estima-
tion,andLandmarkLocalizationintheWild. In:CVPR
(2012)"
81,83,Emotion distribution recognition from facial expressions,"['Y Zhou', 'H Xue', 'X Geng']",2015,161,Toronto Face Database,"FER, facial expression recognition",facial expression recognition methods assume the availability of a single emotion for each  expression in  The s-JAFFE database contains 213 facial expression images. Each image was,No DOI,Proceedings of the 23rd ACM international …,https://dl.acm.org/doi/10.1145/2733373.2806328,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,acm.org,
82,84,Emotion recognition from facial expression using deep convolutional neural network,['DY Liliana'],2019,116,Extended Cohn-Kanade,"CNN, classification, deep learning, neural network",This paper extends the deep Convolutional Neural Network ( This research uses the  extended Cohn Kanade (CK+) dataset  In this paper we contribute a Deep Learning approaches,No DOI,Journal of physics: conference series,https://iopscience.iop.org/article/10.1088/1742-6596/1193/1/012004,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,iop.org,
83,85,Emotion recognition in the wild challenge 2013,"['A Dhall', 'R Goecke', 'J Joshi', 'M Wagner']",2013,241,Acted Facial Expressions In The Wild,"classification, classifier, facial expression recognition",emotion recognition methods in real-world conditions. The database in the 2013 challenge is  the Acted Facial Expression in the Wild  ] and Static Facial Expressions In The Wild (SFEW) [,No DOI,Proceedings of the 15th …,https://dl.acm.org/doi/10.1145/2522848.2531739,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,acm.org,
84,86,"Emotion recognition in the wild challenge 2014: Baseline, data and protocol","['A Dhall', 'R Goecke', 'J Joshi', 'K Sikka']",2014,269,"Acted Facial Expressions In The Wild, Expression in-the-Wild","classification, classifier, facial expression recognition, machine learning",The Second Emotion Recognition In The Wild Challenge 2014 provides a platform for   with their emotion recognition method on the Acted Facial Expressions In The Wild database.,No DOI,Proceedings of the 16th …,https://dl.acm.org/doi/10.1145/2663204.2666275,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,acm.org,
85,87,Emotion recognition in the wild via convolutional neural networks and mapped binary patterns,"['G Levi', 'T Hassner']",2015,413,"Acted Facial Expressions In The Wild, Expression in-the-Wild, Static Facial Expression in the Wild",neural network,"version 2.0 of the Static Facial Expression in the Wild benchmark [9]. It was assembled by  selecting frames from different videos of the Acted Facial Expressions in the Wild (AFEW), and",No DOI,Proceedings of the 2015 ACM on international …,https://dl.acm.org/doi/10.1145/2818346.2830587,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,acm.org,
86,88,Emotion recognition of affective speech based on multiple classifiers using acoustic-prosodic information and semantic labels,"['CH Wu', 'WB Liang']",2010,351,Affective Faces Database,classifier,to emotion recognition of affective speech using multiple classifiers with AP and SLs. The   using multiple classifiers and the MDT was used to select an appropriate classifier to output the,No DOI,IEEE Transactions on Affective Computing,https://ieeexplore.ieee.org/document/5674019,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
87,89,Emotion recognition using deep learning approach from audio–visual emotional big data,"['MS Hossain', 'G Muhammad']",2019,442,Affective Faces Database,deep learning,"The proposed system is evaluated using two audio–visual emotional databases, one of   The histograms are obtained from the cropped face images. If there was no face detected in a",No DOI,Information Fusion,https://www.sciencedirect.com/science/article/pii/S1566253517307066,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
88,90,"Emotional facial expressions evoke faster orienting responses, but weaker emotional responses at neural and behavioural levels compared to scenes: A …","['A Mavratzakis', 'C Herbert', 'P Walla']",2016,118,Karolinska Directed Emotional Faces,neural network,"how affective neural activity during emotional face and scene perceptions translates into   of the analysis was to investigate differences in emotional face and scene processing, only the",No DOI,Neuroimage,https://www.sciencedirect.com/science/article/pii/S1053811915008873,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
89,91,Emotionet challenge: Recognition of facial expressions of emotion in the wild,"['CF Benitez-Quiroz', 'R Srinivasan', 'Q Feng']",2017,109,Expression in-the-Wild,machine learning,"recognition of facial expressions of emotion in the wild. Key  facial expressions, the  heterogeneity of expression in the wild in the wild by current computer vision and machine",No DOI,arXiv preprint arXiv …,https://arxiv.org/abs/1703.01210,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"EmotioNet Challenge: Recognition of facial expressions of emotion in the wild
C.FabianBenitez-Quiroz,RamprakashSrinivasan,QianliFeng,YanWang,AleixM.Martinez
Dept. ElectricalandComputerEngineering
TheOhioStateUniversity
Abstract thefullawarenessoftheparticipants. Also,researchershad
accesstoboththetrainingandtestingdata, allowingprac-
This paper details the methodology and results of the titionerstomodifytheiralgorithmsuntiltheseworkonthe
EmotioNet challenge. This challenge is the first to test the knowntestingdata.
abilityofcomputervisionalgorithmsintheautomaticanal- Toevaluatewhethercurrentcomputervisionalgorithms
ysis of a large number of images of facial expressions of canfullyautomaticallyanalyzeimagesoffacialexpressions
emotion in the wild. The challenge was divided into two of emotion in the wild, we collected a large database of 1
tracks. Thefirsttracktestedtheabilityofcurrentcomputer million image [1]. This datase is called EmotioNet. We
visionalgorithmsintheautomaticdetectionofactionunits also developed a computational model based on the latest
(AUs). Specifically,wetestedthedetectionof11AUs. The knowledgeofhowthehumanbrainanalyzesfacialexpres-
secondtracktestedthealgorithms’abilitytorecognizeemo- sions [15]. This algorithm was used to annotate a subset
tioncategoriesinimagesoffacialexpressions. Specifically, of 950,000 images of the EmotioNet set. The remaining
we tested the recognition of 16 basic and compound emo- 50,000imagesweremanuallylabelledbyexpertannotators.
tion categories. The results of the challenge suggest that
The 950k images that were automatically annotated by
current computer vision and machine learning algorithms
ouralgorithmdefinedthetrainingset. Thetrainingsetwas
areunabletoreliablysolvethesetwotasks. Thelimitations
madeavailabletoparticipantsofthechallenge. 25kofthe
ofcurrentalgorithmsaremoreapparentwhentryingtorec-
manuallyannotatedimageswerealsomadeavailabletopar-
ognize emotion. We also show that current algorithms are
ticipantsasavalidationset. Thiswasnecessarybecausethe
not affected by mild resolution changes, small occluders,
imagesinthetrainingsetarenotaccuratelyannotated,i.e.,
gender or age, but that 3D pose is a major limiting factor
thelabelsareunreliable. Theaccuracyofannotationsinthe
on performance. We provide an in-depth discussion of the
training set is about 81%. The validation set can thus be
pointsthatneedspecialattentionmovingforward.
used to determine how well one’s algorithm adapts to this
unreliability.
Thefinal25kofthemanuallyannotatedimagesarepart
1.Introduction
ofthetestingset. Thisisasequesteredset,i.e. notavailable
to teams participating in the challenge during training and
Much progress has been made in computer vision in
algorithm development. The testing set was the only one
the last couple decades. Of late, tremendous efforts have
usedtoprovidethefinalevaluationofparticipatingteams.
been made to design algorithms that can detect and rec-
ognize generic and specific object categories in images, Theresultsofthechallengeshowthatcurrentcomputer
withtremendousachievementsrecordedjustinthelastfew vision algorithms cannot reliably detect and recognize fa-
years. ImageNet, PASCAL-VOC and COCO are three ex- cial expressions of emotion. In fact, our analysis of the
ampledatabasesandchallenges[16,6,12]thathavehelped results suggest much progress is needed before computer
fueltheprogressandsuccessoftheseefforts. visionalgorithmscanbedeployedinuncontrolled,realistic
In the present paper, we ask if these achievements ex- conditions. For example, 3D pose is shown to be a major
tend to the detection and recognition of facial expressions factorinperformance–ingeneral,themoreafacedeviates
ofemotioninthewild. Previousstudieshavereportedgood fromfrontalview,thelessaccuratetheresultsare.
results in the automatic analysis of facial expressions of Nevertheless,therearepositiveresultstoreporttoo. For
emotion [2]. However, these results were obtained with example,smalllocaloccludersandimageresolution(scale)
the analysis of images and videos taken in the laboratory werenotfoundtoaffecttheaccuracyandrobustnessofthe
[23, 3]. That is, even when the expressions were sponta- tested algorithms. This is a significant achievement, since
neous, the filming was done in controlled conditions with justadecadeback,occlusionsandscaleweremajorfactors
1
7102
raM
3
]VC.sc[
1v01210.3071:viXraAU# Action AU# Action
1 innerbrowraiser 2 outerbrowraiser
4 browlowerer 5 upperlidraiser
6 cheekraiser 9 nosewrinkler
12 lipcornerpuller 17 chinraiser
20 lipstretcher 25 lipspart
26 jawdrop – –
Table1:The11AUsusedintrack1ofthechallenge.Listed
here are the 11 AUs alongside the definitions of their ac-
tions.
in face recognition tasks [24, 13, 8, 10]. Also, we demon-
stratethatneithergendernoragehaveaneffectondetection Figure 1: This figure illustrates the heterogeneity of shape
ofAUsorrecognitionofemotioncategories. andshadingfeaturesassociatedtothesameAU.Theimages
aboveallhaveAU1active,yettheimagefeaturesthatde-
2.Definitionofthechallenge finethisactivationarequitedifferentfromimagetoimage.
Yellowcirclesindicatetheareaeffectedbytheactivationof
Facialexpressionsofemotionareproducedbycontract-
AU1.
ingone’sfacialmuscles[15]. Musclegroupsthatleadtoa
clearlyvisibleimagechangearecalledactionunits(AUs).
EachoftheseAUsisassociatedwithauniquenumber. For
example, AU 1 specifies the contraction of the frontalis Category AUs Category AUs
Happy 12,25 Sadlydisgusted 4,10
muscle pars medialis, and AU 2 defines the contraction of
Sad 4,15 Fearfullyangry 4,20,25
the same muscle pars lateralis. These result in the upper
Fearful 1,4,20,25 Fearfullysurpd. 1,2,5,20,25
movement of the inner and outer corners of the eyebrows, Angry 4,7,24 Sadlyangry 4,7,15
respectively[5]. Surprised 1,2,25,26 Angrilysurprised 4,25,26
ThefirsttaskintheEmotioNetchallengewastoidentify Disgusted 9,10,17 Appalled 4,9,10
Happilysurpd. 1,2,12,25 Angrilydisgusted 4,10,17
elevendifferentAUs. TheseAUsandadescriptionoftheir
Happilydisgd. 10,12,25 Awed 1,2,5,25
visibleactionsaregiveninTable1.
Thisisachallengingtask,becausetheappearanceofan
AUvariessignificantlywhenusedinconjunctionwithother Table2: PrototypicalAUsusedtoproduceeachofthesix-
AUs. Figure1illustratesthis. Thetopthreeimagesinthis teenbasicandcompoundemotioncategoryoftrack2ofthe
figureshowfacialexpressionsperformedbythesameper- challenge.
son. Theseimagesweretakeninacontrolledenvironment
withacooperativesubject. ThethreeexpressionshaveAU
1 present; shown within a yellow circle. Note how differ-
ent the local shape and shading within the yellow circles
requiring computer vision algorithms to recognize sixteen
are. This problem is only exacerbated in images of facial
possible emotion categories in facial expressions. The 16
expressionsofemotioninthewildwheretheheterogeneity
emotion categories are: happy, angry, sad, surprised, fear-
ofidentity, ethnicity, illuminationandposeismuchlarger.
ful, disgusted, appalled, awed, angrily disgusted, angrily
ThisisshowninthebottomthreeimagesinFigure1. Com-
surprised, fearfully angry, fearfully surprised, happily dis-
putervisionandmachinelearningalgorithmsneedtolearn
gusted,happilysurprised,sadlyangry,andsadlydisgusted.
todetectAU1inalltheseinstances.
Thefirstsevenofthesecategoriesaretypicallyreferredtoas
Successful computer vision algorithms need to learn to
basicemotions. Theothersarecalledcompoundemotions
identify image features that are exclusively caused by the
[4].
activationofanAU.Thistaskissodifficultthatexperthu-
mancodershavetogothroughintensivetrainingandprac- Emotioncategoriesareeasilydefinedashavingaunique
tice sessions. The ones that successfully complete their setofAUs. Thatis,theAUsmustbethesame(consistent)
trainingarecalledexpertcoders. Thatwastheproblemad- for expressions of the same emotion category and differ-
dressedintrack1ofthechallenge: Canwedesignacom- ential between emotion categories. The AUs defining the
putervisionsystemthatisasgoodashumanexpertcoders? sixteen emotions included in track 2 of this challenge are
Thesecondtrackofthechallengewentastepfurtherby giveninTable2.
23.Methodology
WithregardtodetectionofAUsandrecognitionofemo-
tion categories, there are several evaluation criteria that
needtobeconsidered.
The first one is accuracy. Accuracy measures the num-
ber of true and false positives. This is important because
wewishtoknowifouralgorithmisabletodiscriminatebe-
tweensampleimageswithacertainAU/emotionpresent.In
statistics, this is generally called observational error – the Figure2: Shownherearethreetestimagesin(lefttoright)
difference between the measured and true value. In other theiroriginalscale(resolution)aswellasat1/2and1/4of
words,wewishtoknowthebiasofanalgorithm. theoriginalscale.
A second important criterion is precision. Precision,
whichisalsoknownaspositivepredictivevalue,isthefrac-
tion of correctly identified instances of an AU or emotion ofthechallengewascomputedastheaverageofthesetwo
category. Statistically,thismeasurestheextenttowhichthe criteria,
accuracy+F
distributionoferrorsisstretched. finalscore= 1, (3)
A third and final criterion to consider is recall. Re- 2
call, also called sensitivity, is the number of correctly de- where accuracy = C−1(cid:80)C accuracy , F =
i=1 i 1
tected/recognizedinstancesofaclassoverthetruenumber C−1(cid:80)C
F , and C is the number of classes. We
ofsamplesinthatclass. Inotherwords, ahighlysensitive i=1 1i
alsoprovide theresults ofF , with β = .5 and 2. But the
β
test,rarelyoverlooksatruepositive.
winnerofthechallengewasgivenbythefinalscoredefined
Formally,accuracyisdefinedas
in(3).
truepositives +truenegatives
accuracy = i i, (1) 4.Trainingandtestingsets
i totalpopulation
Thechallengewasdividedintotwotracks. Track1re-
whereispecifiestheclass,i.e.,AUiortheithemotioncat-
quiredparticipantstodetectthepresenceof11AUsinim-
egory, truepositives are correctly identified test instances
i ages of facial expressions. These AUs were listed in Ta-
of class i, truenegatives are test images correctly labeled
i ble 1. Training data was made available to participants in
asnotbelongingtoclassi,andtotalpopulationisthetotal
November of 2016. The training dataset consists of 950k
numberoftestimages.
AU-annotatedimagesoffacialexpressions. TheAUanno-
Precision is given by precision = detected /true ,
i i i tations are given by the algorithm of [1]. The accuracy of
where detected is the number of images where our algo-
i theseannotationsisabout81%.
rithm has detected (or recognized) class i, and true is the
i Participantsoftrack1weregivenaccesstoaverification
actualnumberofimagesbelongingtothatclass.
set. Theverificationsetconsistsof25kimagesoffacialex-
And,recallisrecall = correct /true ,withcorrect the
i i i i pressionswithmanualAUannotations. Theseannotations
numberofcorrectlydetected/recognizedimagesofclassi.
weregivenbyexperthumancoders.Annotationswerecross
Aconvenient, highlyemployedcriterionthatefficiently
referencedbyfellowcodersforverificationofaccuracy.
combinesprecisionandrecallistheF score, β > 0. The
β Track2ofthechallengerequiredparticipantstorecog-
F ofclassiisdefinedas,
β nize one of 16 possible emotion categories. The emotion
F
βi
=(cid:0) 1+β2(cid:1) β2pp rr ee cc ii ss ii oo nn ir +ec ra el cl i
all
. (2) c Pa at re tg ico ir pi ae ns tsin hc alu dd ae cd cei sn st th ois thc eh ta rl al ie nn ig ne ga dr ae tali os fte td rai cn kT 1a wbl he ic2 h.
i i
includes many sample images with the emotion categories
Distinctvaluesofβ areusedtomeasuredifferentbiases includedinthistrack.Table2specifieswhichAUcombina-
andlimitationsofanalgorithm. Forexample,β = .5gives tionscorrespondtoeachoftheemotionsofthechallenge.
moreimportancetoprecision. Thisisusefulinapplications Additionally,participantsofthistrackweregivenaval-
where false positives are not as important as precision. In idation set with verified expressions of each of the sixteen
contrast, β = 2 emphasizes recall, which is important in emotionsinTable2. Thevalidationsetincludes2ksample
applications where false negatives are unwarranted. And, images. This validation set was made available to partici-
β = 1 provides a measure where recall and precision are pantsinDecemberof2016.
equallyrelevant. Testingset(track1). Thetestingsetsweresequestered
ThepresentchallengeusedthevaluesofaccuracyandF and were not available to participants during training and
1
score to evaluate participating algorithms. The final score algorithmdesign.
3Algorithm finalscore↑ accuracy F score
1
I2R-CCNU-NTU-2 .729 .822 .64
JHU .71 .771 .64
I2R-CCNU-NTU-1 .702 .784 .63
I2R-CCNU-NTU-3 .696 .776 .622
Figure3: Threeexampleimageswithnaturalocclusions.
Table3: Mainresultsoftrack1. Notethatfinalscoretakes
a value between 0 and 1, with 0 being the worst and 1 the
The testing data of track 1 included 88k images. Of
best.
those, 22k were images with manual annotations given by
experthumancoders. Anadditional44kwereobtainedby
reducingtheresolutionofthese22kimages. Thiswasdone by[4]. Also, itisbelievedthatthehumanbrainfirstinter-
byreducingtheimageto1/2and1/4oftheiroriginalsize. prets AUs, before making a determination of the emotion
Thisyieldedimageswithdecreasingresolution(scale).Fig- displayed in the face [15]. To this point, a neural mecha-
ure2showsanexampleimageatthesethreescales. nismsinvolvedinthedecodingofAUsinthehumanbrain
Another 22k were obtained by adding small occluding hasbeenidentified[17].Theseresultssuggeststhatthebest
blacksquaresintheimages. Occlusionswereaddedinar- approachtorecognizeemotionistofirstdetectAUs.
eas of the face that did not occlude the local region of AU Thus, overall, the problem defined in track 2 is consid-
activation. Thiswasdonetodeterminewhethercurrental- eredsignificantlymorechallengingthanthatoftrack1.
gorithms are unaffected by small distractors. Occlusions
were black squares of size equal to 1/5th the width of the 5.Results
face. Face width was defined as the distance between the
To participate in this challenge, online registration was
far-most corners of the left and right ears. Note we only
mandatory.Detailedinstructionsonhowtoparticipatewere
testedforsmallunnaturaldistractors. Thistestisnotmeant
onlysenttogroupsthathadcompletedtheregistrationpro-
to test for large artificial occluders. Nonetheless, natural
cess.
occludersarecommoninourtrainingandtestingsets. Fig-
Thirty-eight(38)groupsregisteredtoparticipate. These
ure3showsafewexamplesofnaturalocclusionsfoundin
groups received final instructions on how to evaluate their
EmotioNet.
algorithmsonthesequesteredtestingsetsinearlyFebruary
Testingset(track2). Thetestingsetoftrack2included of2017.Participantshadafewdaystocompletetheirtasks.
40k images. Of these, 10k were images of facial expres- Oftheoriginal38groups,only5submissionsweresuc-
sions of the sixteen emotions listed in Table 2. An addi- cessfully completed before the assigned deadline. We re-
tional20kwereobtainedbyreducingthesizeoftheseim- ceivedacoupleofrequestsafterthedeadline,butthesewere
agesby1/2and1/4,asabove.Andafinal10kcorresponded notincludedinourevaluationgivenbelow.
tothesameimageswiththeadditionofthesmalloccluders
describedintheprecedingparagraph. 5.1.Track1
Challenges of track 1 versus track 2. The main chal- Table 3 shows the results of the final scores of the four
lenges in track 1 are: to be able to deal with the hetero- submissionswereceivedintrack1beforethedeadline.The
geneityoftheimagesinEmotioNet,andtodetectthesmall algorithm at the top of this table obtained the highest final
(sometimesbarelyvisible)imagechangescausedbysome score,asgivenby(3). Thiswasthewinneroftrack1.
ofthetestedAUs. Onthelatterpoint,notethatwhilesome Results of baseline algorithms are provided in Table 4.
AUsleadtolargeimagechanges,othersyieldhardlyvisible These include results obtained using: a. Kernel Subclass
ones. For instance, AU 12 results in clearly visible mouth DiscriminantAnalysis(KSDA)[22],astandardprobabilis-
changes,takingalargeareaoftheface. Incontrast,AU26 ticmethodthat,andb. AlexNet[11],astandarddeeplearn-
canyieldtinyimagechangesthatarehardtolearntodetect. ingconvolutionalneuralnetwork. Inbothcases,wetrained
The challenges of track 2 are compounded by those of aclassifierforeachoftheAUs. Thisyielded11two-class
track1. Recallthatanemotioncategoryisdefinedbyhav- classifiers, each able to detect one of the 11 tested AUs.
ingasetofAUs. ThiswasshowninTable2. Thus,unless Onlythetrainingsetwasusedtocomputetheseclassifiers.
onecansolvetheproblemofAUdetection,itwouldbevery Thefourparticipatingalgorithmsareconvolutionalneu-
difficulttorecognizeemotion. ral networks. The top algorithm uses residual blocks and
One solution to the above problem is to define an al- a sum of binary cross-entropy loss. The second algorithm
gorithmthatlearnstorecognizeemotioncategories, rather fromthetopusesamulti-labelsoftmaxlossfunction[20].
thanAUs. Theproblemisthattheresultsofsuchanalgo- Table 5 shows the F and F scores of the four partici-
.5 2
rithmcorrelatewiththoseofAUdetection,asdemonstrated patingalgorithmsplusthetwobaselinemethodsintroduced
4Algorithm finalscore↑ accuracy F score Algorithm finalscore accuracy F score
1 1
KSDA .708 .807 .615 NTechLab .597 .941 .255
AlexNet .608 .828 .388 JHU .48 .836 .142
KSDA .578 .91 .247
Table 4: Baseline algorithm results for track 1, AU detec-
tion. Table7:Mainresultsoftrack2. Thealgorithmatthetopof
thetablewasthewinnerofthistrack.
Algorithm F.5score↑ F score
2
I2R-CCNU-NTU-2 .64 .643 Algorithm F.5score F score
2
JHU .638 .635 NTechLab .258 .26
I2R-CCNU-NTU-1 .635 .625 JHU .182 .127
I2R-CCNU-NTU-3 .627 .62 KSDA .274 .242
KSDA .621 .611
AlexNet .353 .446
Table8: F andF scoresofthealgorithmstestedintrack
.5 2
2.
Table5: F andF scoresofthealgorithmstestedintrack
.5 2
1. Algorithm finalscore accuracy F score
1
NTechLab .602 .94 .267
Algorithm finalscore↑ accuracy F score JHU .465 .82 .13
1
I2R-CCNU-NTU-2 .729 .822 .641 KSDA .532 .88 .2
JHU .702 .763 .632
I2R-CCNU-NTU-1 .699 .782 .626
I2R-CCNU-NTU-3 .694 .775 .62 Table 9: Results on the set of images with different scales
andsmallunnaturaloccludersoftrack2.
KSDA .71 .807 .619
AlexNet .515 .763 .266
categoryisgivenby,
Table 6: Results on the set of images with different scales
(cid:88)
andsmallunnaturaloccludersoftrack1. emotion = w p . (4)
j ji i
∀AUs
Here, w are the weights associated to each possible AU.
intheprecedingparagraph. ji
These weights are learned using KSDA on the verification
And, Table 6 shows the final scores, accuracies and F
1
data. Thefinaldecisionofthisbaselinealgorithmissimply
scoresfortheimageswithvaryingscaleandsmallartificial
givenby,
occluders.
argmaxemotion . (5)
TheevaluationsshowninTables3-6indicatethatmostof j
j
thetestedalgorithmsyieldsimilarresults. Crucially, these
Table 8 shows the F and F scores for the same three
resultshavemuchroomforimprovement. Thefinalscores .5 2
algorithms. And Table 9 has the results for the images of
forexampleareallbelow.73,andtheF scoresare< .65,
β
different scales (1/2 an 1/4 of the original size) and with
significantlybelowthemaximumattainablevalue,whichis
smalloccluders.
1.
Twoconclusionsmustbedrawnfromtheseresults.First,
the F scores for all algorithms are very low. This assess-
5.2.Track2 β
ment illustrates the limitations of current computer vision
Table 7 provides the final scores, accuracies and F1 andmachinelearningalgorithmsontherecognitionofemo-
scoresofthetwosubmissionswereceivedbeforethedead- tion categories from images. This problem is considered
lineplusabaselinealgorithmforthistrack. solved for images in controlled lab conditions [4, 2]. But,
The baseline algorithm is defined as follows. First the theresultsoftheEmotioNetChallengedemonstratethereis
KSDAbaselinealgorithmoftrack1isusedtoestimatethe still much to be accomplished before these algorithms can
AUs present in the test images. Since KSDA is a prob- beutilizedincomplexapplicationsbroadly.
abilitic approach, the presence of each AU is given by a Second,theresultsofthischallengeshowsthatsmallar-
probability of detection. Let p be the probability of de- tificialoccludersandimageresolutionarenotlimitingfac-
i
tectingAUi. Then,theprobabilityofdetectinganemotion torsintheclassificationofemotion. Thispointsneedstobe
5stressed,becauseonlyafewyearsagothiswasstillanopen Tostudythisquestion,weusedthealgorithmof[24]to
problem[24,13,8,10]. computetheposeofallthefacesoftheEmotioNetdatabase.
Thisalgorithmwasatopperformerinarecentcompetition
5.3.Noageorgenderbiases
of3Dposeestimation,the20163DFaceAlignmentinthe
We also tested for possible biases in the database and Wild Challenge [7]. Extensive evaluation shows that the
algorithmsparticipatinginthischallenge. error of this algorithm in computing the 3D pose of a face
First, we used the regression algorithm of [21] to es-
is<.004mm[25].
timate the age and gender of the faces in the EmotioNet We use this algorithm to compute roll, pitch and yaw.
database. This regression method is a proven algorithm, Rollisgivenbythenormalvectortothecenterpointofthe
yieldingstate-of-the-artresults. tipofthenose. Thus, onlypitchareyawdefine3Ddistor-
Ageisdividedintothreegroups: 0-19yearsofage,20- tionsoftheface. Toseethis,notethatthein-planerotation
39, and 40 and above. The percentages of images in each ofrolldoesnotaffecttheoutcomeofacomputervisional-
of these three groups are: 37%, 45% and 18%, respec- gorithm, since any face detector will readily eliminate it.
tively. Not surprisingly, images found online mostly rep- Hence,3Deffectsareformallydefinedby,
resentyoungpeople.
|pitch|+|yaw|
Conversely,thepercentageofimagesineachgenderare: , (6)
2
48% male and 52% female. Thus, the database does not
haveanybiasesongender.
where|.|istheabsolutevalue.
Are any of the results described above biased by these
Figure 5 illustrates the effect that pose has on the final
percentages?
scoreofAUdetectionandemotionrecognitionforeachof
To test this, we rerun the detection and recognition re-
theparticipatingalgorithms.
sults described above for each of these five groups (3 age
As can be clearly seen in this figure, both, AU detec-
groupsand2gendergroups).
tion and emotion recognition, are significantly affected by
The results show no major biases due to either age or
pose. Clearly, this is an area that needs to be addressed if
gender. Forinstance,indetectionofAUs,theaccuracyre-
computer vision systems are ever to compete with human
sultsof theI2R-CCNU-NTU-2algorithmare: .779(in the
performance.
0-19agegroup),.837(inthe20-39group)and.843(inthe
40-abovegroup).TheaccuracyresultsoftheJHUalgorithm
6.Discussion
are: .734, .801, .764, respectively. The results of the I2R-
CCNU-NTU-1algorithmare:.773,.788,.786,respectively. Thispapersummarizestheresultsofthe2016-17Emo-
And, the results of the I2R-CCRU- NTU-3 algorithm are: tioNet Challenge. This is the first ever challenge on the
.773, .788, .786, respectively. The F β scores are equally recognition of facial expressions of emotion in the wild.
similaracrossagegroups. Hence,theseresultsonlyshowa Key factors of the challenge are: the use of an unprece-
verymilddisadvantagefortheyoungestfaces(0-19group). dentedlylargenumberofimagesoffacialexpressions, the
In emotion recognition, the NTechLab accuracy results heterogeneity of expression in the wild, a large number of
are: .938(inthe0-19group),.943(inthe20-39group)and testedAUsandemotioncategories,andthefactthattesting
.941(inthe40andabovegroup). TheJHUaccuracyresults wascompletedonasequesteredsetofimagesnotavailable
are: .83(inthe0-19yearsgroup),.83(inthe20-39group) toparticipantsduringtrainingandalgorithmdevelopment.
and.822(inthe40-abovegroup).TheF β scoresareequally The most important result of the challenge is the real-
similar to each other. Hence, no emotion classification bi- izationofthedifficultyofemotioncategorizationinexpres-
aseswereidentified. sions in the wild by current computer vision and machine
We found the same result in the gender groups, i.e, de- learning algorithms. While the results make it clear that
tectionofAUsandrecognitionofemotioncategorieswere the detection of AUs need to be significantly improved, it
unbiasedbygender. isevenmoreapparentthattherecognitionofemotionisnot
Figure4summarizestheaverageresultsofallalgorithms even close to being solved. Here, it is important to high-
in each of these groups. We run four t-tests, one for each lightthefactthatthepresentchallengetested16emotions,
oftheplotsinthisfigure,andfoundnostatisticaldifference morethandoublingthenumberofcategoriesusedinprevi-
betweeneitherageorgender(p>.2). ous experiments. Yet, additional emotion categories exist,
asshownin[4,1]. Ifcurrentalgorithmscannotaccurately
5.4.Faceposeisamajorfactor
recognize16emotions, theywillhaveanevenhardertime
Finally,weassessedthealgorithms’abilitytodealwith identifyingadditionalones.
pose. That is, are the detection and recognition results re- EmotioNet provides a large number of labelled images.
portedabovedependentontheposeoftheface? Hence, the limitations listed in the preceding paragraphs
6(a) (b) (c) (d)
Figure 4: Average and standard deviations of the final scores of participating algorithms in task 1 (AU detection) for: (a)
the three age groups, and (b) the two gender groups. Average and standard deviations of the final scores of participating
algorithmsintask2(emotionrecognition)for: (c)thethreeagegroups,and(d)thetwogendergroups.
images,e.g.,tinyresolutionsandmajorocclusions.Thehu-
man visual system is robust to these image manipulations
[15]and,hence,weknowtheproblemissolvable.
Also,anin-depthanalysisoftheresultsofparticipating
algorithmsshowednobiasesduetoageorgender. Thisis
also a positive outcome, not shared by all face recognition
challenges[9].
However, all participating algorithms are influenced by
(a) (b) the3Dposeoftheface. Frontalfacesarefoundtobemuch
easier.And,ingeneral,theaccuracyofdetectionandrecog-
Figure5: Finalscoreofparticipatingalgorithmsasafunc- nitiondecreaseasafunctionofpitchandyaw. Thisresults
tion of pose: (a) AU detection, (b) emotion recognition. callsfortheneedtonormalizefacestoafrontalviewbefore
Pose is defined in equation (6) and is given in degrees in applyingclassificationalgorithms. Indeed,thisisanareaof
the x-axis. The y-axis is the final score given by equation intenseresearch[7,25,18].
(3).
7.Conclusions
The EmotioNet Challenge was divided into two tracks.
cannot be attributed to a lack of labelled data. Nonethe-
Thefirsttracktestedcomputervisionalgorithms’abilityin
less, some of these labels were imprecise (noisy). Thus,
detecting 11 AUs. This is a task successfully completed
a major open area of interest in computer vision and ma-
by human experts only. The second track assessed com-
chinelearningishowtolearnfrominaccuratelabelleddata.
putervisionalgorithms’abilitytorecognize16emotioncat-
We believe that this can be accomplished with the help of
egories. Thisisatasksuccessfullycompletedbymostpeo-
thevalidationset,whichismanuallyannotatedandverified.
ple,expertsandnon-experts.
Unfortunately, most current algorithms do not take advan-
TheimagesintheEmotioNetchallengeareoffacialex-
tageofthisfact.
pressions of emotion in the wild, Figures 1-3. This is a
One solution to the above problem is to learn co-
majordeparturefrompreviousstudiesandchallengeswere
activationpatternsinthevalidationsetandthenimposethis
imagesoffacialexpressionsofemotionhadbeencollected
onthetrainingset.Relatedmethodswerestudiedinthepast
incontrolledconditionsinthelab.
(e.g., [19]), but these approaches have been abandoned re-
TheresultsoftheEmotioNetChallengeillustratethedif-
centlyinfavorofdeeplearningmethods. Thankfully,these
ficulty of these two task in images of facial expressions of
twoapproachesarenotmutuallyexclusiveandcouldbeeas-
emotion in the wild. Nonetheless, a surprising and inter-
ilyintegrated.
estingoutcomeofthechallengeistheextradifficultyalgo-
A positive outcome of the challenge that deserve cele- rithmshaveinsuccessfullycompletingthetaskofthesec-
brationistonotethatscaleandminorimagedistractorsdo ondtrack.
nothaveanoticeableeffectondetectionandrecognition. It Resultsofthefirsttrack,whichcanonlybecompletedby
iseasytodismissthisfact,butmuchefforthasbeenmadein experthumancoders,showcurrentalgorithmsarenotfully
thelastfifteenyearssincetheseproblemswereintroduced ready.But,therecognitionofemotion,whichanylayperson
[14]. can successfully complete, is a much harder problem for
The present results show that the community is now computervisionalgorithms. Thisisawell-knownresultin
ready to test detection and recognition in highly degraded artificial intelligence (AI). AI systems tend to be better at
7solving human tasks that require expertise. And, the same [13] Q. Liu and C. Liu. A novel locally linear knn model for
algorithmsaregenerallysignificantlyworseatsolvingday- visualrecognition. InProceedingsoftheIEEEConference
to-daytasksthathumanstakeforgranted. on Computer Vision and Pattern Recognition, pages 1329–
In summary, the results of the studies delineated in the 1337,2015.
presentpaper,defineanewfrontierforcomputervisionsys- [14] A.M.Martinez.Recognizingimpreciselylocalized,partially
occluded,andexpressionvariantfacesfromasinglesample
tems. Researchisneededtodetermineifcurrentalgorithms
per class. IEEE Transactions on Pattern analysis and ma-
need tweaking, or whether a novel set of algorithms is re-
chineintelligence,24(6):748–763,2002.
quiredtoemulatethesehumans’abilites.
[15] A.M.Martinez. Computationalmodelsoffaceperception.
References CurrentDirectionsinPsychologicalScience,2017.
[16] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
[1] C. F. Benitez-Quiroz, R. Srinivasan, and A. M. Martinez. S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
Emotionet: An accurate, real-time algorithm for the auto- etal. Imagenetlargescalevisualrecognitionchallenge. In-
maticannotationofamillionfacialexpressionsinthewild. ternationalJournalofComputerVision,pages1–42,2014.
InProceedingsoftheIEEEConferenceonComputerVision [17] R.Srinivasan,J.D.Golomb,andA.M.Martine. Aneural
andPatternRecognition,pages5562–5570,2016. basisoffacialactionrecognitioninhumans. TheJournalof
[2] C.A.Corneanu, M.O.Simo´n, J.F.Cohn, andS.E.Guer- Neuroscience,2016.
rero.Surveyonrgb,3d,thermal,andmultimodalapproaches [18] D.Tome,C.Russell,andL.Agapito. Liftingfromthedeep:
forfacialexpressionrecognition:History,trends,andaffect- Convolutional3dposeestimationfromasingleimage.arXiv
relatedapplications. IEEEtransactionsonpatternanalysis preprintarXiv:1701.00295,2017.
andmachineintelligence,38(8):1548–1568,2016. [19] Y.Tong,W.Liao,andQ.Ji.Facialactionunitrecognitionby
[3] A. Dhall, R. Goecke, T. Gedeon, and N. Sebe. Emotion exploitingtheirdynamicandsemanticrelationships. IEEE
recognitioninthewild. JournalonMultimodalUserInter- transactions on pattern analysis and machine intelligence,
faces,2(10):95–97,2016. 29(10),2007.
[4] S.Du,Y.Tao,andA.M.Martinez.Compoundfacialexpres-
[20] F.Wang,X.Xiang,C.Liu,T.D.Tran,A.Reiter,G.D.Hager,
sionsofemotion. ProceedingsoftheNationalAcademyof
H.Quon,J.Cheng,andA.L.Yuille.Transferringfaceverifi-
Sciences,111(15):E1454–E1462,2014.
cationnetstopainandexpressionregression. arXivpreprint
[5] P.EkmanandE.L.Rosenberg. WhattheFaceReveals:Ba-
arXiv:1702.06925,2017.
sicandappliedstudiesofspontaneousexpressionusingthe
[21] D.You,C.F.Benitez-Quiroz,andA.M.Martinez. Multiob-
Facial Action Coding System (FACS), 2nd edition. Oxford
jectiveoptimizationformodelselectioninkernelmethodsin
UniversityPress,USA,2006.
regression.IEEEtransactionsonneuralnetworksandlearn-
[6] M.Everingham,L.VanGool,C.K.I.Williams,J.Winn,and
ingsystems,25(10):1879–1893,2014.
A. Zisserman. The pascal visual object classes (voc) chal-
[22] D. You, O. C. Hamsici, and A. M. Martinez. Kernel op-
lenge.InternationalJournalofComputerVision,88(2):303–
timization in discriminant analysis. IEEE Transactions on
338,June2010.
PatternAnalysisandMachineIntelligence, 33(3):631–638,
[7] L. A. Jeni, S. Tulyakov, L. Yin, N. Sebe, and J. F. Cohn.
2011.
Thefirst3dfacealignmentinthewild(3dfaw)challenge. In
[23] S. Zafeiriou, A. Papaioannou, I. Kotsia, M. Nicolaou, and
EuropeanConferenceonComputerVision,pages511–520.
G.Zhao. Facialaffect“inthewild”. InProceedingsofthe
SpringerInternationalPublishing,2016.
IEEEConferenceonComputerVisionandPatternRecogni-
[8] H.JiaandA.M.Martinez. Supportvectormachinesinface
tionWorkshops,pages36–47,2016.
recognitionwithocclusions.InComputerVisionandPattern
[24] K.Zhao,W.-S.Chu,F.DelaTorre,J.F.Cohn,andH.Zhang.
Recognition,2009.CVPR2009.IEEEConferenceon,pages
Jointpatchandmulti-labellearningforfacialactionunitand
136–141.IEEE,2009.
holisticexpressionrecognition.IEEETransactionsonImage
[9] I. Kemelmacher-Shlizerman, S. M. Seitz, D. Miller, and
Processing,25(8):3931–3946,2016.
E.Brossard. Themegafacebenchmark: 1millionfacesfor
recognitionatscale. InProceedingsoftheIEEEConference [25] R.Zhao,Y.Wang,andA.M.Martinez. Asimple,fastand
onComputerVisionandPatternRecognition,2016. highly-accuratealgorithmtorecover3dshapefrom2dland-
marksonasingleimage. arXivpreprintarXiv:1609.09058,
[10] I.Kotsia,I.Buciu,andI.Pitas. Ananalysisoffacialexpres-
sionrecognitionunderpartialfacialimageocclusion. Image 2016.
andVisionComputing,26(7):1052–1067,2008.
[11] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
classification with deep convolutional neural networks. In
Advances in neural information processing systems, pages
1097–1105,2012.
[12] T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ra-
manan,P.Dolla´r,andC.L.Zitnick. Microsoftcoco: Com-
mon objects in context. In European Conference on Com-
puterVision,pages740–755.Springer,2014.
8"
90,92,"Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild","['C Fabian Benitez-Quiroz', 'R Srinivasan']",2016,685,"Affective Faces Database, Expression in-the-Wild","classification, classifier","in our set of a million face images in the wild, we  databases to successfully recognize AUs  and AU intensities on an independent database of images not used to train our classifiers. 2.",No DOI,Proceedings of the …,https://ieeexplore.ieee.org/document/7780969,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
91,93,Enhanced amygdala reactivity to emotional faces in adults reporting childhood emotional maltreatment,"['AL van Harmelen', 'MJ van Tol']",2013,302,Karolinska Directed Emotional Faces,"classification, neural network","to negative emotional faces are related to psychopathology, we investigated whether  abnormal amygdala (and/or mPFC functioning) was more apparent in emotionally maltreated",No DOI,Social cognitive and …,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3624946/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,nih.gov,
92,94,Ensemble-based discriminant learning with boosting for face recognition,"['J Lu', 'KN Plataniotis']",2006,244,Toronto Face Database,"classifier, machine learning",ensemble-based methods as they are known in the machine learning literature [18]). Globally  nonlinear methods are not without problems. Approaches such as those based on kernel,No DOI,IEEE transactions on …,https://pubmed.ncbi.nlm.nih.gov/16526485/,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,nih.gov,
93,95,Estimation of continuous valence and arousal levels from faces in naturalistic conditions,"['A Toisoul', 'J Kossaifi', 'A Bulat', 'G Tzimiropoulos']",2021,160,Affective Faces Database,"machine learning, neural network","For example, the ability to accurately extract emotional information from the face of the  person one is communicating with plays a major role in prosociality 24 and this capacity is often",No DOI,… Machine Intelligence,https://www.nature.com/articles/s42256-020-00280-0,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,nature.com,
94,96,Evidence and a computational explanation of cultural differences in facial expression recognition.,"['MN Dailey', 'C Joyce', 'MJ Lyons', 'M Kamachi', 'H Ishi']",2010,249,Japanese Female Facial Expression,"classification, classifier, facial expression recognition, machine learning, neural network",our interpretation of facial expressions of emotion is universal  EMPATH models to recognize  facial expressions in a variety  of facial expression images with different mixes of Japanese,No DOI,Emotion,https://pubmed.ncbi.nlm.nih.gov/21171759/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,nih.gov,
95,97,Excavating AI: The politics of images in machine learning training sets,"['K Crawford', 'T Paglen']",2021,442,Toronto Face Database,machine learning,"By looking at the politics of classification within machine learning systems, this article  demonstrates why the automated interpretation of images is an inherently social and political",No DOI,Ai & Society,https://link.springer.com/article/10.1007/s00146-021-01162-8,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,springer.com,
96,98,Exploiting multi-cnn features in cnn-rnn based dimensional emotion recognition on the omg in-the-wild dataset,"['D Kollias', 'S Zafeiriou']",2020,182,"Acted Facial Expressions In The Wild, Affective Faces Database, Expression in-the-Wild, Static Facial Expression in the Wild",CNN,"The Aff-Wild database served as benchmark for the Aff-Wild Challenge, organized in   the temporal dependencies of facial expressions in each utterance, we designed standard",No DOI,IEEE Transactions on Affective …,https://arxiv.org/abs/1910.01417,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"IEEETRANS.ONAFFECTIVECOMPUTING:SP.I.ONAUTOMATEDPERCEPTIONOFHUMANAFFECTFROMLONGITUDINALBEHAVIORALDATA 1
Exploiting multi-CNN features in CNN-RNN
based Dimensional Emotion Recognition on the
OMG in-the-wild Dataset
Dimitrios Kollias and Stefanos Zafeiriou
Abstract—ThispaperpresentsanovelCNN-RNNbasedapproach,whichexploitsmultipleCNNfeaturesfordimensionalemotion
recognitionin-the-wild,utilizingtheOne-MinuteGradual-Emotion(OMG-Emotion)dataset.Ourapproachincludesfirstpre-trainingwith
therelevantandlargeinsize,Aff-WildandAff-Wild2emotiondatabases.Low-,mid-andhigh-levelfeaturesareextractedfromthe
trainedCNNcomponentandareexploitedbyRNNsubnetsinamulti-taskframework.Theiroutputsconstituteanintermediatelevel
prediction;finalestimatesareobtainedasthemeanormedianvaluesofthesepredictions.Fusionofthenetworksisalsoexaminedfor
boostingtheobtainedperformance,atDecision-,oratModel-level;inthelattercaseaRNNwasusedforthefusion.Ourapproach,
althoughusingonlythevisualmodality,outperformedstate-of-the-artmethodsthatutilizedaudioandvisualmodalities.Someofour
developmentshavebeensubmittedtotheOMG-EmotionChallenge,rankingsecondamongthetechnologieswhichusedonlyvisual
informationforvalenceestimation;rankingthirdoverall.Throughextensiveexperimentation,wefurthershowthatarousalestimationis
greatlyimprovedwhenlow-levelfeaturesarecombinedwithhigh-levelones.
IndexTerms—Deepconvolutionalandrecurrentneuralarchitectures;CNNplusMultiRNN;low-,mid-,high-levelfeatures;multi-CNN
featureextractionandaggregation;multi-tasklearning;facialimageanalysis;valence;arousal;emotionrecognitionin-the-wild;
AffWildNet;AffWildandAffWild2emotiondatabases;OMG-EmotiondatabaseandChallenge.
(cid:70)
1 INTRODUCTION
AUTOMATIC analysis of facial behaviour is the corner- (measuring the power of emotion activation). Valence and
stone of many application areas, including Human- arousal relate readily to specific functions of regions of
Computer and Robot Interaction, Pervasive Computing, the brain [19], [43], [59]; the parietal region of the right
Ambient Intelligence and Virtual Reality. The research area hemisphere appears to play a special role in the mediation
offacialbehaviouranalysisincludestheproblemsof:i)the ofarousal,whereasthefrontalregionsappeartoplayaspe-
recognition of the so-called six universal expressions (i.e., cial role in emotional valence. A third dimension, tension,
Anger, Disgust, Fear, Happy, Sad, Surprise), plus Neutral, is also introduced but often excluded due to difficulties
influencedbytheseminalworkofEkman[12],ii)therecog- in consistently identifying what the dimension describes:
nition of spontaneous expressions including mental states tension, control, or potency (dominance). Fig. 1 shows the
(painintensity[20]andcompoundexpressions[10]),iii)the 2-D Valence-Arousal Space, introduced in [52]. Estimation
detection of the facial Action Units (AU) and estimation of of valence and arousal continuous values related to affect
theirintensity,accordingtotheFacialActionCodingSystem constitutestheproblemexaminedinthefollowing.
[11]whichprovidesastandardisedtaxonomyoffacialmus-
cles’movements,iv)thedetectionofmicro-expressions,and
v)theestimationoffacialaffectinacontinuousdimensional
space(e.g.,valenceandarousal).Relatedresearchcanassist
in flagging complex behavioral patterns such as deception,
depression, autism, spectrum disorders and schizophrenia
[1],[23],[28],[45],[57],[58].
The main focus of this paper is on dimensional emo-
tion models, which are appropriate to represent not only
extreme, but also subtle emotions appearing in everyday
human-computer interactions. According to the dimen-
sional approach [56] [61], affective behavior is described
by a number of latent continuous dimensions. The most
commonly used dimensions include valence (indicating
howpositiveornegativeanemotionalstateis)andarousal Fig.1:The2-DValence-ArousalSpace,asseenin[37]
In order to facilitate research on the above problems,
• D. Kollias and S.Zafeiriou are with the Department of Computing,
ImperialCollegeLondon,UnitedKingdom many databases have been generated and annotated, most
E-mail:dimitrios.kollias15@imperial.ac.uk ofwhichareinwell-controlledconditions.Inthebeginning,
E-mail:s.zafeiriou@imperial.ac.uk
data and annotations were scarce, hence research relied
0202
rpA
01
]GL.sc[
2v71410.0191:viXraIEEETRANS.ONAFFECTIVECOMPUTING:SP.I.ONAUTOMATEDPERCEPTIONOFHUMANAFFECTFROMLONGITUDINALBEHAVIORALDATA 2
on extracting highly engineered handcrafted features and from many CNN networks (but not using features from
designing ad-hoc learning strategies [14], [41], [42], [49], multiplelayersofthesamenetwork)andfusingthem.
[53]. Naturally, as the amount of data and annotations Both facial images and landmarks (after applying a
grew, research has started to capitalise on data-intensive Procrustes Analysis) are provided as inputs to these archi-
technologies, such as deep learning [5], [6], [22], [24], [25], tectures.Additionally,ensembleformulationsareproposed,
[38]. usingdifferentlevelsoffusion(Model-orDecision-level)on
It is now widely accepted, in both the computer vision the proposed architectures; these formulations are shown
and machine learning communities, that progress in a par- to further boost the obtained performance. In model-level
ticularapplicationdomainissignificantlycatalysedwhena fusion, our proposal is to perform fusion through a RNN
largenumberofdatasetsarecollectedinunconstrainedcon- insteadofthetypicalfullyconnectedlayer.
ditions (also referred as ”in-the-wild” data). Hence, facial Another contribution of this work is the approach to fit
analysiscouldnotonlyfocusonspontaneousbehaviors,but the developed architectures to the OMG-Emotion dataset
alsoonbehaviorscapturedinunconstrainedconditions.In- characteristics and in particular to the dataset’s annotation
the-wild dimensional databases have been generated, such atutterancelevel.Todealwiththis,wespliteachutterance
astheaudiovisualOMG-EmotionDataset[3],Aff-Wild[62] into sequences, which were individually processed by the
[29], Aff-Wild2 [34] [30] [32] [27] and SEWA [54] ones, as above architectures. The mean or median of the predicted
wellasAffectNet[44]whichincludesonlystaticimages. valence-arousalvalueswerecomputedpersequence.Then,
Regarding the pipeline of facial behavior analysis, the themeans/medianswereaveragedatutteranceleveltopro-
standard paradigm has been to: i) detect and/or track the videthefinalvalenceandarousalestimates.Thisprocedure
face in an image sequence, ii) detect and/or track facial deviates from related works that uniformly (or randomly)
landmarks, iii) extract handcrafted features1, either around sample a constant number of frames from each utterance,
thelandmarks,oronthefaceregionasawhole,andiv)use assigntoeachofthemtheannotationvalueoftheutterance
thefeaturesandthelandmarksforclassification/regression andcomputethepredictionperframe[8].
using affective labels. Recently this paradigm has shifted An additional contribution of this work is the pre-
from utilizing handcrafted features to utilizing features training of the proposed architectures on the large-scale
learned by deep Convolutional Neural Networks (CNNs) emotionally rich Aff-Wild database and on its larger ex-
and/orRecurrentNeuralNetworks(RNNs).Thisshiftwas tension, the Aff-Wild2. Other works [51], [60], [64] used
motivated by the striking performance achieved when uti- networks that were not pre-trained on same task (valence-
lizingdeepneuralnetworks(DNNs)inavarietyofemotion arousalestimation)butonothertasks(facerecognition,ob-
recognitiontasks[9],[16],[33],[46],[63]. ject detection). The pre-training on these specific databases
In this paper, we address the issue of estimating va- provided our developed architectures with the ability to
lence and arousal utilizing the One-Minute-Gradual Emo- effectively capture the dynamics of the OMG-Emotion in-
tionDataset(OMG-EmotionDataset),basedonvisualinfor- the-wilddatasetandthusprovidedabetterperformance.
mationonly.Wepresentnoveldeepneuralarchitecturesthat The main findings of our approach have been: i) low-
providebestperformanceinvalenceandarousalestimation, level features when combined with high-level ones in our
as well as the submissions we made to the OMG-Emotion CNNplusmulti-RNNarchitectures,helpedinboostingthe
Challenge, which were ranked very high, especially for networks’ performance in arousal estimation; ii) CNN plus
valenceestimation. multi-RNNarchitecturesoutperformedstandardCNNplus
Thefirstmaincontributionofthispaperisthedevelop- RNN ones showing that features extracted from previous
mentofCNNplusmulti-RNNarchitecturesforvalenceand layers contain useful and rich information for valence-
arousalestimationinamulti-taskoptimizationformulation. arousal prediction; iii) better results were obtained when
In this formulation, low-, mid- and high- level features the features extracted from previous layers were processed
are extracted from different layers of the CNN part and by independent RNNs instead of being concatenated and
passed as input to the RNN part. The intuition for this is fed to a single RNN; iv) better results were obtained when
that these features include rich information which can be using a RNN instead of a fully connected layer for model-
advantageousforthestudiedtask. level fusion; v) when using the visual modality, network
These architectures are of two different types; in the performanceforvalenceestimationismuchhigherthanthe
first, the features extracted from, say, K CNN layers are correspondingforarousalestimation.
concatenatedandpassedasinputtoasingleRNN,whereas The rest of this paper is organized as follows. Section
intheother,theyarepassedtoK RNNs.Intheexperimen- 2 reviews related work and existing state-of-the-art meth-
tal section, it is shown that the latter type outperformed ods for facial expression recognition with emphasis on the
all other developed architectures and even state-of-the-art dimensionalmodelofaffect.Section3givesabriefdescrip-
networks that used not only the visual, but also the audio tion of the databases used in our experiments, i.e., OMG-
modality. Our work deviates from others, such as [6], [8], EmotionDataset,Aff-WildandAff-Wild2databases.Section
[38], that either: i) use standard CNN-RNN networks in 4 presents the pre-processing steps which were essential to
which the output of the CNN is passed to the RNN, or obtain a common input representation for analysis. Section
ii) apply ensemble methodologies, using features extracted 5 presents the developed methods, i.e., the created novel
deep neural architectures, including ensembles and fusion
1.ExamplesofhandcraftedfeaturesincludeHistogramofOriented of networks, for valence-arousal estimation. Section 6 de-
Gradients(HoGs),ScaleInvariantFeatureTransform(SIFT),LocalBi-
scribes specific implementation details that we followed to
naryPatterns(LBPs)andfeaturesfrommultiscaleandmultiorientation
Gaborfilterbanks achievethebestresults.Section7providesanevaluationofIEEETRANS.ONAFFECTIVECOMPUTING:SP.I.ONAUTOMATEDPERCEPTIONOFHUMANAFFECTFROMLONGITUDINALBEHAVIORALDATA 3
TABLE1:State-of-the-artalgorithmsforvalence-arousal
ourapproachbyanalysingtheobtainedresultsandpresent-
estimation,theirperformancesandutilizeddatabases
ing comparisons with other methods, in terms of achieved
performance.Finally,Section8presentstheconclusions.
Work DatabasesUsed Methods Results
partof CNN-RNNvisualonly: Valence:
2 RELATED WORK
[22]
RECOLA (conv+max-pool)x2 RMSE=0.107
asusedinthe +conv+quadrant-pool PCC=0.554
One of the first deep learning architectures for valence and AVECChallenge +RNN CCC=0.507
(1)audio:
arousalestimationwasproposedin[22].Inthiswork,both handcrafted+SoundNetfeatures Valence-Arousal:
(2)visual:
frame-based CNN and CNN plus RNN architectures were [6] SEWA VGG-FACE+DenseNetfeatures RMSE=0.081-0.086
PCC=0.758-0.702
(3)text:
proposed and compared. The CNN consisted of 3 convo- CCC=0.756-0.672
wordvectors-features
lutional layers; the first two layers were followed by max fusionof(1),(2),(3)+LSTM
1)corelayer:seriesofconv.layers
pooling layers and the third by a quadrant pooling layer. 2)attributelayer:facialfeatures Valence-Arousal
[5] Aff-Wild MSE=0.123-0.095
3)AUlayer
A fully connected layer was then used, followed by the 4)Valence&Arousallayer CCC=0.396-0.282
outputlayer.TheCNNplusRNNarchitecturesconsistedof CCC:
Aff-Wild; Valence-Arousal
[26] AffWildNet:
thepreviouslydescribedCNNnetwork(keepingitsweights wholeRECOLA; Aff-Wild:0.570-0.430
[29] ResNet-50+FC+GRU
AFEW-VA; RECOLA:0.526-0.273
fixed)withoutthetopregressionlayer,followedbyasingle AFEW-VA:0.515-0.556
RNN layer that gave the final estimates. This methodology
achieved very high valence and arousal correlations in a
partoftheRECOLAdatabase[55]. 3 THE UTILIZED IN-THE-WILD DIMENSIONAL EMO-
The authors in [6] explored and fused different hand- TION DATABASES
craftedanddeeplearningfeaturesfromallavailablemodal-
In this Section we provide a short description of the Aff-
ities(acoustic,visual,andtextual).Theyalsoconsideredthe
WildanditsextensionAff-Wild2database,whichhavebeen
interlocutor influence (a person’s influence on the interact-
usedtopre-trainthedevelopeddeepneuralnetworkarchi-
ingpartner’sbehaviors)fortheacousticfeatures.
tectures,aswellastheOMG-Emotiondatabase,analysisof
Inmoredetail,theauthorsextracted:i)fromtheacoustic
whichisthetargetofthispaper.
modality,hand-craftedfeatures,suchasMFCCs,loundness,
F0,jitter,shimmerandfeatureslearnedfromtheSoundNet
[2],ii)fromthevisualmodality,featureslearnedfromVGG- 3.1 Aff-WildDatabase
FACE [50] and DenseNet [18] that had been pre-trained on TheAff-Wilddatabase[29][62]hasbeenthefirstlargescale
the FER+ [4] dataset (annotated in terms of the basic ex- capturedin-the-wilddatabasethathasbeenannotatedby8
pressions), and iii) from the textual modality, word vectors lay experts with regards to valence and arousal. It consists
that were used as features. All those features were fused of 298 videos and displays reactions of 200 subjects, with
and passed as input to a LSTM network that produced the a total video duration of more than 30 hours. The total
estimates for valence, arousal and likability. This approach number of frames in this database is 1,224,100. Regarding
was the winning of AVEC 2017 Challenge that utilized the subjects’ gender, 130 are male and 70 female. The Aff-Wild
SEWAdatabase. database served as benchmark for the Aff-Wild Challenge,
Theauthorsof[5]presentedtheFATAUVA-Netmethod, organized in conjunction with CVPR 2017. The aim for
which is a deep learning framework in which a core layer, thisdatabasewastocollectspontaneousfacialbehaviorsin
an attribute layer, an AU layer and a valence-arousal layer arbitraryrecordingconditions.Tothisend,thevideoswere
were trained sequentially. The core layer was a series of collected using Youtube. The main keyword that was used
convolutional layers, followed by the attribute layer which toretrievethevideoswasreaction.
extracted facial area’s features (face, eye, eyebrow, mouth).
These layers were used in supervised learning of AUs.
3.2 Aff-Wild2Database
Finally,AUswereemployedasmid-levelrepresentationsto
estimatetheintensityofvalenceandarousal.Thismethod- The Aff-Wild database has recently been augmented with
ology produced the highest results of the First Affect-in- new Youtube videos having a total length of 13 hours and
the-wildChallenge[29]whichwasthefirstchallengeonthe 5 minutes, thus forming the Aff-Wild2 database [30], [32],
estimationofvalenceandarousalin-the-wild,usingtheAff- [34]. This database has been the basis for the ABAW Com-
Wilddatabaseforrecognitionofaffect. petition [27]. The aim has been to extend the spontaneous
BestresultsintheAff-Wilddatabasehavebeenobtained facialbehaviorsinarbitraryrecordingconditionsmetinAff-
bytheauthorsof[26][29].Intheseworks,theauthorsper- Wild,whilstsignificantlyincreasingthenumberofdifferent
formed a large number of experiments, training CNN and subjectsinit.Alltheadditionalvideoshavebeenannotated
CNN-RNN networks on the Aff-Wild for emotion recog- by four experts and contain a wide range in subjects’:
nition. The best performing network, AffWildNet, consists age (from babies and young children, to elderly people);
of the convolutional and pooling parts of the ResNet-50 ethnicity (subjects are caucasian, hispanic or latino, asian,
network [17], followed by a fully connected layer, a 2- black, or african american); profession (e.g. actors, athletes,
layer GRU [7] and the output layer that provided the final politicians, journalists); head pose; illumination conditions;
valence-arousal estimates. This network was further fine- occlusions; emotions. In total, Aff-Wild2 consists of 558
tunedontheRECOLAandAFEW-VAdatabases,producing videoswith2,786,201frames.11outofthosevideosdisplay
state-of-the-artperformance. two subjects, all of which have been annotated. The total
Table 1 provides a summary of the performance of the numberofsubjectsis458,with279ofthembeingmaleand
above-describedmethodsontherespectivedatabases. 179female.IEEETRANS.ONAFFECTIVECOMPUTING:SP.I.ONAUTOMATEDPERCEPTIONOFHUMANAFFECTFROMLONGITUDINALBEHAVIORALDATA 4
Fig.2:SampleimagesfromtheOMG-Emotiondatasetshowingpeopledisplayingvariousin-the-wildemotions
3.3 OMG-EmotionDataset annotator could take into consideration not only the visual
The One-Minute Gradual-Emotional Behavior dataset and audio information but also the context of each video,
(OMG-Emotion dataset) [3] contains in-the-wild videos i.e.whatwasspokeninthecurrentandpreviousutterances
from Youtube where emotion expressions emerge and de- through the context clips provided by the annotation tool.
velop over time based on monologued scenarios. Figure In this manner, each annotation is based on multimodal
2 shows some frames from this dataset, with various information.
Eachutterancewasgivenaspecificvalenceandarousal
people displaying different emotions under many occa-
value, based on the gold standard of the five annotations.
sions/circumstances. This dataset is annotated in terms of
Valence annotations range in [−1,1], whereas arousal ones
valence and arousal and also contains a large number of
range in [0,1]. In Fig. 3, on top row are the 2-D histograms
differentidentities.
The OMG-Emotion dataset served as a benchmark oftheannotationsintheOMG-Emotiontraining,validation
for the One-Minute Gradual-Emotion Recognition (OMG- and test sets, respectively, and in the bottom row are the
Emotion) Challenge 2, held jointly with the Special Ses- correspondingdatasets’annotations’distributions.
Additionally, this dataset contains categorical annota-
sion on Neural Models for Behavior Recognition at the
tions for each utterance; transcripts of what was spoken in
WCCI/IJCNN 2018. In particular, the dataset is split into
eachofthevideosarealsoprovided.
training, validation and test sets in a subject independent
manner, meaning that each subject appears strictly in only
one of these sets. The training set consists of 231 videos 4 PRE-PROCESSING: FACE DETECTION & ALIGN-
composed of 2442 utterances, the validation set consists MENT, IMAGE RESIZING & NORMALIZATION
of 60 videos composed of 617 utterances and the test set
Datapre-processingconsistsofallprocessingstepsthatare
consists of 204 videos composed of 2229 utterances. Each
required for starting the extraction of meaningful features
utterancehasanaveragelengthof8secondsandeachvideo
from the data. The usual steps are face detection, face
hasanaveragelengthofaround1minute.
alignment, image resizing and image normalization. The
For annotating the collected data, theAmazon Mechan-
first step is to extract face bounding boxes from all video
ical Turk tool was used, resulting, on average, in five in-
frames. In order to do so, we used the Deformable Part
dependent annotations per utterance. Each annotator was
Model(DPM)detectorffld2[40]thathasproventobehighly
giventhefullcontextualinformationofthevideouptothat
efficientandaccurateforfacedetectionin-the-wild.
point when annotating the dataset. That means that each
For face alignment, we extracted facial landmarks and
implemented the Generalized Procrustes Analysis [15]. In
2.https://www2.informatik.uni-hamburg.de/wtm/
omgchallenges/omg emotion2018 session.html our implementations, we first used the facial landmark
Fig.3:Histograms(onthetoprow)andDistributions(onthebottomrow)inthe2-DValence-Arousalspaceof
utterance-levelannotationsofthetraining(ingreen),validation(inred)andtest(inblue)OMG-EmotionsetsIEEETRANS.ONAFFECTIVECOMPUTING:SP.I.ONAUTOMATEDPERCEPTIONOFHUMANAFFECTFROMLONGITUDINALBEHAVIORALDATA 5
detector inside the dlib library [21] to locate 68 facial case, the landmarks were concatenated with the outputs of
landmarks in all frames. We used as reference and rigid thelastpoolinglayerofthenetworkandweregivenasinput
points, 5 anchor points that corresponded to the location tothefirstfullyconnectedlayer,thatconsistedof4096units.
of the left eye, right eye, nose and mouth in a prototypical In this way, both outputs and landmarks were mapped to
frontalface.Foreveryframe,weusedits5faciallandmarks the same feature space, before performing the prediction.
correspondingtothelocationofthesamefacialcomponents; In the ResNet-50 (and DenseNet-121) case, the landmarks
we performed Procrustes transformation, which eliminates wereconcatenatedwiththeaveragedpooledfeaturesofthe
in-plane rotation, isotropic scaling and translation, on the ResNet-50(DenseNet-121)networkandweregivenasinput
coordinatesofthese5landmarksandthecoordinatesofthe toafullyconnectedlayerconsistingof1500units.Thislayer
5landmarksofthefrontalface;weimposedthistransforma- was followed by the output layer which provided the final
tion to the whole new frame to perform the alignment. All estimatesforvalence-arousalpair.
croppedandalignedimageswerethenresizedto96×96×3
pixelresolutionandtheirintensityvalueswerenormalized
to the range [−1,1]. Those images, along with the 68 facial
5.2 StandardCNNplusRNNarchitectures
landmarks, were then used as inputs for training our net-
works,asdescribedinthefollowingSection. In order to consider the contextual information in the data
and more specifically the temporal dependencies of facial
5 THE DEVELOPED ARCHITECTURES expressions in each utterance, we designed standard CNN
plus RNN architectures. In the following we present the
This section presents the proposed framework for dimen-
different CNN-RNN architectures that we have developed
sional emotion recognition, by describing the CNN, the
and used in the experimental study. In these architectures,
standard CNN plus RNN, the proposed CNN plus Multi
the output of the CNN’s last pooling layer is being fed
RNN architectures and then an ensemble methodology for
to a fully connected layer, whose output constitutes the
fusion.
input of the RNN layers. These architectures were pre-
In all architectures presented in this Section, we com-
trained on either the Aff-Wild, or the Aff-Wild2 databases.
pared a uni-task learning approach, independently for va-
We then used two different strategies for training these
lence and arousal, to multi-task learning approach. The
architectures:i)keepingtheCNNweightsfixedandtraining
latter provided better performance in estimation of both
the remaining architecture (i.e., the fully connected layers
affective dimensions. This result is in agreement with [48]
and the RNNs), or ii) training the whole architecture in an
which claims that there exist inter-correlations between the
end-to-end manner (by jointly training the CNN and RNN
valence and arousal emotion dimensions. This relation be-
parts).Thelatterapproachprovidedthebestresults.
tweenemotiondimensionsinisolation,(i.e.,withoutinclud-
ing features), has been well-supported by related research
in psychology [36] [35]. That is the reason why in the 5.2.1 AffWildNet
following,wefocusonthemulti-taskcase.
Atfirst,weconsideredtheAffWildNet[29]asthebestper-
forming network on the Aff-Wild database and re-trained
5.1 CNNarchitectures
it on the OMG-Emotion database. As shown in Table 2,
We experimented with three state-of-the-art networks: the AffWildNet is a CNN-RNN network consisting of the
VGG-Face, ResNet-50 and DenseNet-121. These networks convolutional and pooling parts of ResNet-50 followed by
werefirstpre-trainedeitherontheAff-WildortheAff-Wild2 a fully connected layer of 1500 units, followed by a 2-layer
database and then trained on the OMG-Emotion training GRU,witheachlayerhaving128units.Inthisarchitecture,
set.Todesignthestructureofthesenetworks,wetookinto the landmarks are concatenated with the averaged pooled
account the procedure used to annotate the OMG-Emotion features of the ResNet-50 and being fed as input to the
dataset. According to this, each utterance was labeled with fully connected layer consisting of 1500 units. Similarly to
a single pair of valence and arousal values. We split each the CNN case described in the previous Subsection, the
utterance into smaller parts-sequences, each consisting of CNN-RNN network receives an input sequence of frames,
thesamenumberofconsecutiveframes.Then,weassigned then predicts, for each frame, the valence-arousal values
to each of those parts-sequences of frames, the label of the andfinallycomputesthemean,ormedian,ofthesevalues,
correspondingutterance. whichisthefinalestimate.Thisarchitectureisthesameasin
TrainingoftheCNNnetworkswasperformedasshown Fig.4,ifonereplacestheCNNnetworkwiththeCNN-RNN
in Fig.4. In more detail, each CNN was provided with (i.e.,theAffWildNet).
an input sequence and was trained to predict, for each
frame in the sequence, the respective valence-arousal pair TABLE2:TheAffWildNetarchitecture
of values. The 68 facial landmarks (per each frame of the
input sequence) were also provided as additional inputs to block1 ResNet-50conv&poolingparts
the CNN networks. The final valence (arousal) prediction block2 fullyconnected1 1500
dropout
was computed as the mean, or median (both approaches
block3 GRUlayer1 128
were considered) of the per-frame valence (arousal) values dropout
inthatsequence. block4 GRUlayer2 128
InFig.4,theCNNstructurecanbeanyoftheVGG-FACE, block5 fullyconnected2 2
ResNet-50andDenseNet-121ones.IntheVGG-FACECNNIEEETRANS.ONAFFECTIVECOMPUTING:SP.I.ONAUTOMATEDPERCEPTIONOFHUMANAFFECTFROMLONGITUDINALBEHAVIORALDATA 6
Fig.4:ThedevelopedCNNstructure.Itgivesonlyonevalence-arousal(V-A)estimateperinputsequenceofconsecutive
frames.TheCNNcomponentcanbeanyoftheVGG-FACE,ResNet-50andDenseNet-121networks.The68landmarks
areconcatenatedwiththeextractedfeaturesfromthelastpoolinglayeroftheCNNcomponentandarepassedtothefully
connectedlayerthatprecedestheoutputlayer.
5.2.2 DenseNet-RNN predicts the valence and arousal values. Each GRU layer
WealsousedaDenseNet-RNNstructurethatisquitesimilar comprises128units.Similarlytothearchitecturesdescribed
tothatoftheAffWildNetdescribedinthepreviousSubsec- in Subsection 5.3, the CNN-3RNN networks are provided
tion. The only difference is that it uses the DenseNet-121 with an input sequence of frames (and the corresponding
network’sconvolutionalandpoolinglayers. landmarks of each frame), predicting, for each frame, the
valence-arousalvalues;theirmean,ormedianconstitutethe
finalestimates.
5.3 CNNplusMulti-RNNnetworks
Fig.5, presents an example of CNN-3RNN networks,
In general, features extracted from the low CNN layers namedCNN-3RNN-2nd-pool last-pool fc.Inthisnetwork:
containrich,completeandtimevaryinginformation,whilst i) the features extracted from the fully connected layer are
high-level features are highly specific and characteristic of passed as input to a RNN network, denoted RNN 1 in
the specific problem studied. Taking this into account, we Fig.5; ii) the features extracted from the last pooling layer
havedevelopedandusedCNNplusMulti-RNNnetworks; (before being concatenated with the landmarks) are passed
these networks extract low-, mid- and high- level features asinputtoasecondRNNnetwork,denotedRNN 2inFig.5;
from different layers of the CNN and pass them through iii) the features extracted from the second pooling layer
RNNs. These networks are split into two different types (following the fourth convolutional layer) are passed as
throughdifferentmethodologies:thefirst,referredasCNN- input to another RNN network, denoted RNN 3 in Fig.5.
1RNN, concatenates the extracted features from 3 CNN Fig.6 depicts the exact structure of the afore-mentioned
layers and passes them to a single RNN, whereas the RNN i,i ∈ {1,2,3},networks.Allnetworkshavethesame
other, referred as CNN-3RNN, processes them indepen- structure; a 2-layer GRU network, with each layer having
dentlythrough3RNNsubnets. 128units.Next,theoutputsofthe3RNNsareconcatenated
It should be mentioned that we also tested other net- and passed to the output layer that performs the valence-
works:CNN-2RNN(extractingfeaturesfrom2CNNlayers arousal prediction. As shown in the experimental Section
andpassthemindependentlyto2RNNs);CNN-2RNN-1FC 7, this network based on the features extracted from these
(similarly as before, with the outputs from the 2 RNNs specific layers provided the best results in these type of
being concatenated and passed to a fully connected layer; networks.
inthiswaytheyarebothmappedtothesamefeaturespace,
before performing the final prediction); CNN2-to-1RNN 5.3.2 CNN-1RNNnetworks
(extractingfeaturesfrom2CNNlayers,concatenatingthem The CNN-1RNN types of networks consist of the convo-
and passing them as input to a single RNN); CNN-3RNN- lutional and pooling layers of VGG-FACE, followed by a
1FC (the outputs from the 3 RNNs being concatenated and fullyconnectedlayerof4096units.The68faciallandmarks
passedtoafullyconnectedlayer,beforeperformingthefinal are concatenated with the features extracted from the last
prediction). These architectures provided performance that pooling layer of VGG-FACE and are passed to this fully
was around 4-5% lower than the performance of the CNN- connected layer. Then, low-, mid- and high-level features
1RNN and CNN-3RNN networks, presented next in this are extracted, concatenated and passed to a 2-layer GRU
Section. network that predicts the valence and arousal values. Each
GRU layer comprises 128 units. Similarly to the other ar-
5.3.1 CNN-3RNNnetworks
chitectures described above, the CNN-1RNN networks are
The CNN-3RNN networks include the convolutional and provided with an input sequence of frames (and the cor-
poolinglayersofVGG-FACE,followedbyafullyconnected responding landmarks of each frame), predicting, for each
layerof4096units.The68faciallandmarksareconcatenated frame, the valence-arousal values; their mean, or median,
with the features extracted from the last pooling layer of arethefinalestimates.
VGG-FACE and are passed to this fully connected layer. Fig.7 presents one example of CNN-1RNN networks,
Then, low-, mid- and high-level features are extracted and which we call CNN-1RNN-2nd-pool last-pool fc. In this
each one is processed by a 2-layer GRU network that network, the features extracted from: i) the second pool-IEEETRANS.ONAFFECTIVECOMPUTING:SP.I.ONAUTOMATEDPERCEPTIONOFHUMANAFFECTFROMLONGITUDINALBEHAVIORALDATA 7
Fig.5:TheCNN-3RNN-2nd-pool last-pool fc.Itprovided Fig.7:TheCNN-1RNN-2nd-pool last-pool fcarchitecture.
avalence-arousal(V-A)estimateperinputsequenceof Itprovidesavalence-arousal(V-A)estimateperinput
consecutiveframes.The’68landmarks’areconcatenated sequenceofconsecutiveframes.The’68landmarks’are
withthefeaturesofthelast’pool’layerandpassedasinput concatenatedwiththefeaturesofthelast’pool’layerand
tothe’fc’layer.Thisarchitectureprovidedthebestresults. passedasinputtothe’fc’layer.
5.4 EnsembleMethodology
InthisSubsectionwedescribeanensembleapproachwhich
fuses the developed networks at: i) Model-level and ii)
Decision-level.Model-levelfusionisbasedonconcatenating
the high level features extracted by different networks,
whilstDecision-levelfusionisbasedonweightedaveraging
Fig.6:ThestructureofeachRNNnetworkinthe
thepredictionsprovidedbydifferentnetworks.Ontheone
CNN-3RNNarchitecturedisplayedinFig.5.
handside,Model-levelfusiontakesadvantageofthemutual
informationinthedata.Ontheotherhandside,theaverag-
ing procedure in Decision-level fusion reduces variance in
ing layer (following the fourth convolutional), ii) the last the ensemble regressor (thus achieving higher robustness),
pooling layer (following the 13th convolutional and before whilepreservingtherelativeimportanceofeachindividual
being concatenated with the landmarks) and iii) the fully model.
connected layer, are concatenated and passed to the RNN.
AsshownintheexperimentalSection7,thisnetworkbased 5.4.1 Model-levelFusion
onthefeaturesextractedfromthesespecificlayersprovided Let us consider the CNN-1RNNs and CNN-3RNNs de-
thebestresultsinthesetypeofnetworks. scribed in the previous Subsection. We concatenate theIEEETRANS.ONAFFECTIVECOMPUTING:SP.I.ONAUTOMATEDPERCEPTIONOFHUMANAFFECTFROMLONGITUDINALBEHAVIORALDATA 8
outputsofalltheRNNsintheabovenetworksandprovide Whentrainingend-to-endtheCNNplusRNNarchitectures,
them, as input, either: i) to another single RNN layer with the learning rate was either 10−4 or 10−5; when training
128 GRU units, or ii) to a fully connected layer with 128 them,keepingtheirrespectiveCNNpartsfixed,itwas10−3.
units; the output layer follows. We denote the resulting All networks were trained using Tensorflow on a Quadro
networks as Model-level Fusion + RNN and Model-level GV100VoltaGPUandthetrainingtimewasaboutaday.
Fusion + FC, respectively. Similarly to the previous Sub-
sections, for each frame in the input sequence of frames, 6.2 ObjectiveFunction
thismodel-levelfusionnetworkpredictsthevalence-arousal
Since the evaluation criterion of the OMG-Emotion Chal-
values and then computes their mean, or median, as final
lenge was the CCC, our loss function was based on that
estimates.
criterionandwasdefinedas:
5.4.2 Decision-levelFusion ρ +ρ
Let us consider again the CNN-1RNNs and CNN-3RNNs L total =1− a 2 v, (3)
described above. The final valence (arousal) estimate whereρ a andρ v aretheCCCforthearousalandvalence.
Odec.−level(Odec.−level),iscomputedasaweightedaverage
v a
of the final valence (arousal) estimates, on(on), of these
v a 6.3 Post-Processing
networks;eachweightisproportionaltothecorresponding
Finally, for all investigated methods, a chain of post-
networkperformanceonthevalidationset:
processing steps was applied. These steps included: i) me-
O idec.−level = (cid:80)1
tn
(cid:88) tn
i
·on i, (1) d quia en ncfi elt ae nri dn ig i)o sf mt oh oe th- inp ger of fr ta hm ee -p- ep rr ued tti ec rt aio nn ces -w pi rt eh din icta ios ne s-
i n
n (especiallytothosethatconsistedoftoofewframes).Anyof
where i ∈ {v,a} (v stands for valence, a stands for thesepost-processingstepswaskeptwhenanimprovement
arousal), tn is equal to the Concordance Correlation Coef- was observed on the CCC over the validation set, and ap-
i
ficient (CCC), ρ i, for valence or arousal, computed on the pliedthen,withthesameconfigurationtothetestpartition.
validation set, with n denoting the CNN-1RNNs or CNN-
3RNNs; the CCC has been the evaluation criterion of the 7 EXPERIMENTAL RESULTS
OMG-Emotion Challenge, taking values in [−1,1] and is
In all conducted experiments, best results were obtained
definedasfollows:
whenthefinalestimateswerethemedianofthe,perframe,
2s valence and arousal estimates within a sequence. In all
ρ i = s2 +s2 +i,x (y x¯ −y¯)2, (2) developments, we trained the DNNs with the training set,
i,x i,y i i
evaluatedthemontherespectivevalidationsetandselected
where i ∈ {v,a}, s i,x and s i,y are the variances of the the best networks according to the validation performance.
valence/arousallabelsandpredictedvaluesrespectively,x¯ i There were no significant differences between training the
and y¯ i are the corresponding mean values and s i,xy is the DNNmultipletimesandthenaveragingthepredictions,or
covariancevalue. usinga10-foldcrossvalidation.
Weexaminedtoincludealevelofencodingformatching
the size of landmarks with the size of the CNN features
6 NETWORK TRAINING DETAILS
before fusing them. We first passed the 68 landmarks to
Inthefollowing,weprovidefurtherinformationregarding a fully connected layer of 512, 1024, or 2048 units and
the parameters used in the developed architectures (learn- then fused this output with the features extracted from the
ing rate, dropout probability value, batch size, sequence CNN.However,wedidnotnoticeanysignificantdifference
length),thelossfunctionthatwasformulatedforourprob- in performance, although the developed architectures were
lemandtheseriesofpost-processingstepsthatwereapplied morecomplexandbiggerintermsoflearnableparameters.
totheobtainedestimatesofvalenceandarousal.
7.1 CNN-RNNComponentAnalysis
6.1 ImplementationDetails
Table3showstheperformanceofthedevelopedCNN,stan-
In all developed CNN, CNN plus RNN and CNN plus dardCNNplusRNN,CNNplusMulti-RNNandensemble
Multi-RNN architectures, dropout with 0.5 probability architectures, pre-trained on the Aff-Wild2 database, with
value was applied on the fully connected layers that were and withoutthe post-processing steps described inSubsec-
on top of the convolutional and pooling layers of CNN tion 6.3 (for all networks: p-value (cid:54) 10−20 (cid:28) 0.05). The
networks (VGG-FACE, ResNet-50 and DenseNet-121). Ad- VGG-FACEhasachievedthebestperformancecomparedto
ditionally, dropout with 0.8 probability value was applied the ResNet-50 and DesNet-121 networks. This is expected
afterthefirstGRUlayeroftheRNNs. as the VGG-FACE network has been pre-trained with a
FortrainingourCNNnetworks,differentsequencesizes large dataset for face recognition (many human faces have
were used, ranging from 40 to 100, with the size of 80 been, therefore, used in its construction), thus better filters
framesprovidingthebestresults.IntheCNNplusRNNand arealreadyestablishedincomparisontotheResNet-50and
CNN plus Multi-RNN cases, we used a batch size of 4 and DesNet-121thathavebeenpre-trainedonobjects.Addition-
sequencelengthof80consecutiveframes.Whentrainingthe ally,afterfurtherpre-trainingonAff-Wild2,abettertuning
CNNarchitectures,thelearningratewaschosentobe10−4. ofthesefiltersisattainedintheVGG-FACEcase.IEEETRANS.ONAFFECTIVECOMPUTING:SP.I.ONAUTOMATEDPERCEPTIONOFHUMANAFFECTFROMLONGITUDINALBEHAVIORALDATA 9
TABLE3:CCCbasedevaluation,ontheOMGtestset,of
Additionally,AffWildNetandDenseNet-RNNnetworks
valence&arousalpredictionsprovidedbyourdeveloped
achievedabetterperformancethanallCNNnetworks.The
CNN,CNNplusRNN,CNNplusMulti-RNNand
formernetworksarestandardCNNplusRNNsinwhichthe
ensemblearchitectures.Allnetworksarepre-trainedon
RNNisusedinordertomodelthecontextualinformationin
Aff-Wild2with(without)post-processing.AhigherCCC
thedata,takingintoaccounttemporalvariationsandthusa
valueindicatesabetterperformance.
betterperformanceisexpected.
One can also note that both CNN-1RNN-2nd-pool last-
With(Without)
pool fc and CNN-3RNN-2nd-pool last-pool fc exhibit a CCC Post-Processing Mean
muchimprovedperformance(between6%and10%onaver- Valence Arousal
VGG-Face 0.378(0.361) 0.203(0.193) 0.291(0.277)
age)whencomparedtoCNNplusRNNarchitectures.This DenseNet-121 0.365(0.350) 0.191(0.184) 0.278(0.267)
validates our essence that low-level CNN features together ResNet-50 0.359(0.344) 0.195(0.189) 0.277(0.267)
AffWildNet 0.409(0.390) 0.224(0.219) 0.317(0.305)
with high-level ones provide useful information for our
DenseNet-RNN 0.394(0.378) 0.211(0.209) 0.303(0.294)
task. Additionally, CNN-3RNN-2nd-pool last-pool fc out- CNN-1RNN-
0.449(0.441) 0.303(0.297) 0.376(0.369)
2nd-pool last-pool fc
performedCNN-1RNN-2nd-pool last-pool fcshowingthat
CNN-3RNN-
it is better to exploit the low- and high-level features’ time 2nd-pool last-pool fc 0.472(0.463) 0.329(0.322) 0.401(0.393)
variations via RNNs, independently, and then concatenate Decision-LevelFusion 0.501(0.482) 0.332(0.321) 0.417(0.402)
Model-LevelFusion+FC 0.518(0.500) 0.348(0.328) 0.433(0.414)
them, rather than concatenate them first and process them Model-LevelFusion+RNN 0.535(0.512) 0.365(0.340) 0.450(0.426)
throughtheuseofasingleRNN.
Table3validatesthatusingtheensemblemethodologyis TABLE4:CCCbasedevaluation,ontheOMGtestset,of
betterthanusingasinglenetwork.Thisisbecausedifferent valence&arousalpredictionsprovidedbyvarious
networks produce quite different features; fusing them ex- networkswhen:theyaretrainedfromscratchorare
ploitsalltheserepresentationsthatincluderichinformation. pre-trainedwiththeAff-WildandAff-Wild2databases.A
ItcanalsobeobservedthatModel-levelfusionmethodhas higherCCCvalueindicatesabetterperformance.
a superior performance compared to that of the Decision-
Trained Pre-trained Pre-trained
levelone,sincethefeaturesfromdifferentnetworksthatare Methods fromScratch onAff-Wild onAff-Wild2
Valence Arousal Valence Arousal Valence Arousal
concatenated,containricherinformationabouttherawdata
CNN-1RNN-
0.371 0.210 0.419 0.278 0.449 0.303
than the final decision. In particular, in Model-level fusion, 2nd-poollast-poolfc
CNN-3RNN-
0.385 0.192 0.448 0.302 0.472 0.329
we concatenate these features and pass them through an 2nd-poollast-poolfc
Model-levelFusion+RNN 0.431 0.265 0.511 0.342 0.535 0.365
RNN and the whole ensemble is trained end-to-end and
optimizedsothattheconcatenationoffeaturescanprovide
the best overall result. Moreover, in Model-level fusion, a latterone.Next,wepresentanablationstudyonextracting
better performance is achieved when a RNN, instead of a different CNN low-, mid- and high-level features in CNN-
fullyconnectedlayer,isusedforthefusion. 3RNNnetworks.Table5comparestheirperformance(inall
Onecanalsonoticethatthepost-processingstepshelped cases:p-value(cid:54)10−25 (cid:28)0.05).ThefirstfourrowsofTable
to achieve a better performance, mainly in valence esti- 5 show the performance of networks where a combination
mation. The median filter size that we used was 81 for oflow-,mid-andhigh-levelfeaturesareextracted,whereas
valence (similar to the sequence length), whereas only 3 thenextrowsshowtheperformanceofnetworkswhereonly
for the arousal. The arousal window size was small, but, low-,oronlymid-,oronlyhigh-levelfeaturesareextracted.
when it was increased, the performance decreased. Our Letusnotethatworstperformancesamongallthesetypesof
finalobservationisthattheperformanceofthenetworksin networkswereobtainedwhenfeatureswereextractedfrom
arousal was worse than their performance in valence. This mid- CNN levels (convolutional layers 6-9). Generally, best
is expected because we only used the visual modality for performances were obtained when features were extracted
trainingournetworks;forarousaltheaudiocuesappearto from high- and from low-levels. The optimal combination
includemorediscriminatingcapabilitiesthanfacialfeatures (thatprovidedthebestperformance)wasthroughtheuseof
in terms of correlation coefficient; this conclusion confirms CNN-3RNN-2nd-pool last-pool fc. One more observation
previousfindings[47]. is that low-level features (convolutional layers 3-5), espe-
Inthefollowing,wecomparetheperformanceofthebest ciallywhencombinedwithhigh-level,significantlyaffected
performingnetworksofTable3withpost-processingtothat theperformanceinpredictingbothvalenceandarousal.
ofnetworkstrainedfromscratch,orbeingpre-trainedwith Next, we present an ablation study on the use of
the Aff-Wild or the Aff-Wild2 database. Table 4 presents landmarks as additional input to various networks. Ta-
theresultsofthiscomparison.TheAff-Wild2database,due ble 6 compares the performance of the CNN-1RNN-2nd-
to its big size and emotion diversity, boosted the perfor- pool last-pool fc, CNN-3RNN-2nd-pool last-pool fc and
manceofallnetworkspre-trainedwithit,incomparisonto Model-level Fusion + RNN networks when the landmarks
the performance of the networks trained directly with the areandarenotusedasadditionalinput.Inallcases,using
OMG-Emotion set. This was also the case when we pre- landmarksincreasestheirperformanceby1.2%-1.9%.
trained the networks with the Aff-Wild database. Overall, Finally, to give more insight on the performance of
networks pre-trained with the Aff-Wild2 achieved a better the best CNN-3RNN (CNN-3RNN-2nd-pool last-pool fc),
performance in comparison to networks pre-trained with we analyzed its performance at different parts of the 2D
theAff-Wilddatabase. Valence-Arousal Space. Table 7 presents the obtained va-
Between CNN-1RNN and CNN-3RNN types of archi- lence and arousal performance in terms of Mean Squared
tectures,abetterperformancewasacquiredwhenusingthe Error(MSE)across4differentregionsofthisSpace.ItcanbeIEEETRANS.ONAFFECTIVECOMPUTING:SP.I.ONAUTOMATEDPERCEPTIONOFHUMANAFFECTFROMLONGITUDINALBEHAVIORALDATA 10
TABLE5:EffectonCCC(ontheOMGtestset)ofusing
The authors of [60] developed two models. In the first
featuresfromdifferentlayersintheCNN-3RNNcase.All
model,denotedasopenSMILE+LSTMs,featuresextracted
networksarepost-processed&pre-trainedonAff-Wild2.A
fromaudiousingopenSMILE[13]werepassedthroughsix
higherCCCvalueindicatesabetterperformance.
2-layerLSTMs,eachpredictingvalence,arousalorboth;the
final prediction was their average. In the second model,
CNN-3RNN CCC Mean
Valence Arousal denoted as VGG-FACE-BLSTM, the visual modality was
8thconv+lastpool+fc 0.416 0.261 0.339 used; frames from the utterances were passed through a
5thconv+lastpool+fc 0.455 0.322 0.389 fixed and pre-trained VGG-FACE followed by a 2-layer
2ndpool+lastpool+fc 0.472 0.329 0.401
BLSTMthatgavethefinalvalenceprediction.
3rdconv+7thconv+fc 0.402 0.267 0.335
lastconv+lastpool+fc 0.440 0.248 0.344 Theauthorsof[64]developedbothsingleandensemble
6thconv+7thconv+8thconv 0.328 0.162 0.245 networks, consisting of three models. In the first model,
7thconv+8thconv+9thconv 0.334 0.172 0.253 denoted as Single Multi-Modal, acoustic features were ex-
3rdconv+4thconv+5thconv 0.345 0.185 0.265
tractedusingopenSmile;visualfeatureswereextractedfrom
a fixed and pre-trained VGG16 followed by 1-layer LSTM
TABLE6:EffectonCCC(ontheOMGtestset)of(not)
withattentionmechanism;visualandacousticfeatureswere
usinglandmarksasadditionalinputtovariousnetworks.
passed into an SVM that performed the final predictions.
Allnetworksarepost-processed&pre-trainedon
The second model was similar to the first and extracted
Aff-Wild2.AhigherCCCvalueindicatesabetter
similar visual and acoustic features, but it also extracted
performance.V,AstandforValenceandArousal
acoustic features from SoundNet. All these features were
CCC WithoutLandmarks WithLandmarks passed to an SVM that performed the predictions. The late
V A Mean V A Mean fusion of the two afore-mentioned models, is denoted as
CNN-1RNN-
2nd-pool last-pool fc 0.429 0.291 0.360 0.449 0.303 0.376 Ensemble I; the final predictions were a weighted sum of
CNN-3RNN- 0.454 0.310 0.382 0.472 0.329 0.401 themodels’predictions.Thethirdmodelwasanend-to-end
2nd-pool last-pool fc
Model-levelFusion+RNN 0.524 0.352 0.438 0.535 0.365 0.450 trained VGG16 followed by 1-layer LSTM with attention
mechanism that takes as input only visual data. The late
fusionofthethreedevelopedmodels,isdenotedasEnsem-
seenthatbetterresultshavebeenobtainedintheregionwith
ble II; again the final predictions were a weighted sum of
high arousal and positive valence; however the obtained
themodels’predictions.
MSE are not far away from the MSE across the whole 2D
Table 8 shows that our Model-level Fusion + RNN
Valence-ArousalSpace.
method outperforms all other methods -even those that
have been trained using the audio modality as well- on
TABLE7:ValenceandArousalMSEinareasofthe2DVA
boththevalenceandarousalestimation.Table8alsoshows
SpaceforthebestCNN-3RNN.AlowerMSEindicatesa
that the CNN-3RNN-2nd-pool last-pool fc outperformed
betterperformance.V,AstandforValenceandArousal
all state-of-the-art networks, regardless whether they addi-
V∈[0,1] V∈[0,1] V∈[-1,0) V∈[-1,0) V∈[-1,1] tionally used the audio modality, except for: i) the Single
2DVA-Space A∈[0,0.5) A∈[0.5,1] A∈[0,0.5) A∈[0.5,1] A∈[0,1]
CNN-3RNN- Multi-Modal method that outperformed it on average by
2nd-pool MSE-V=0.101 MSE-V=0.055 MSE-V=0.154 MSE-V=0.110 MSE-V=0.110
last-poolfc MSE-A=0.031 MSE-A=0.021 MSE-A=0.061 MSE-A=0.040 MSE-A=0.041 0.015 (however this network used the audio modality as
well;sincetheaudioandspeechcontributemoretoarousal
estimation, this small difference is justified) and ii) Ensem-
7.2 SubmissionstotheOMG-EmotionChallenge blesIandII,whichareafusionofmanydifferentnetworks
FortheOMG-EmotionChallengeeachteamwasallowedto that used the visual and audio modalities and thus again
haveupto3submissions.WehavesubmittedtheCNN2-to- thedifferenceinperformancewasexpected.
1RNN and CNN-3RNN-last-conv last-pool fc pre-trained
on Aff-Wild models’ predictions without post-processing TABLE8:CCCbasedevaluation,ontheOMGtestset,of
(submission I) and with post-processing: either with me- VApredictionsprovidedbyourbestperformingnetworks
dian filtering (submission II) or with median filtering and vsthestate-of-the-art.V,Astandforvalenceandarousal.A
smoothing (submission III; our best one). More details re- higherCCCvalueindicatesabetterperformance.
gardingoursubmissionscanbefoundin[31].
Methods Modality CCC
Valence Arousal
VNet[51] V,A:visual 0.438 0.244
7.3 ComparisonwithState-of-the-Art
ANet+VNet[51] V,A:audio+visual 0.442 0.236
Here we compare the performance of our best networks to openSMILE+LSTMs, [60] A:audio, 0.258 0.277
VGG-FACE-BLSTM V:visual
the performances of state-of-the-art methods submitted to openSMILE+LSTMs,
A:audio,
theOMG-EmotionChallenge.Theauthorsof[51]developed VGG-FACE-BLSTM+[60] 0.369 0.286
V:audio+visual
openSMILE+LSTMs
the VNet and ANet models. VNet is a SphereFace [39]
openSMILE+LSTMs[60] V,A:audio 0.361 0.293
network, followed by a BLSTM, followed by a temporal SingleMulti-Modal[64] V,A:audio+visual 0.484 0.345
pooling and the output layer. ANet is a VGG16 network EnsembleI[64] V,A:audio+visual 0.496 0.356
EnsembleII[64] V,A:audio+visual 0.499 0.361
with average pooling and accepts as input STFT maps ex-
CNN-3RNN-
tractedfromtheaudio.Intheirfusion,thefeaturesextracted 2nd-pool last-pool fc V,A:visual 0.472 0.329
fromVNet’stemporalpoolingandANet’saveragepooling Model-levelFusion+RNN V,A:visual 0.535 0.365
layers,areconcatenatedandpassedtotheoutputlayer.IEEETRANS.ONAFFECTIVECOMPUTING:SP.I.ONAUTOMATEDPERCEPTIONOFHUMANAFFECTFROMLONGITUDINALBEHAVIORALDATA 11
8 CONCLUSIONS [15] Gower,J.C.:Generalizedprocrustesanalysis.Psychometrika40(1),
33–51(1975)
This paper presented the development of novel architec-
[16] Han, S., Meng, Z., Khan, A.S., Tong, Y.: Incremental boosting
tures for predicting valence-arousal, by utilizing the OMG- convolutionalneuralnetworkforfacialactionunitrecognition.In:
Emotion dataset. The proposed approach was based on Advancesinneuralinformationprocessingsystems,pp.109–117
visual information and achieved very good performance (2016)
[17] He,K.,Zhang,X.,Ren,S.,Sun,J.:Deepresiduallearningforimage
whentestedontheOMG-Emotiontestset.Inthedeveloped
recognition. In:ProceedingsoftheIEEEConferenceonComputer
networks, features extracted from low-, mid- and high- VisionandPatternRecognition,pp.770–778(2016)
CNN layers were either concatenated and fed to a single [18] Huang,G.,Liu,Z.,VanDerMaaten,L.,Weinberger,K.Q.:Densely
connected convolutional networks. In: Proceedings of the IEEE
RNN,orprocessedbyRNNsubnetsandthenconcatenated.
conferenceoncomputervisionandpatternrecognition,pp.4700–
Moreoveranensembleapproachwasproposed;theModel-
4708(2017)
level fusion through a RNN produced the best results. All [19] Iordan, A., Dolcos, F.: Brain activity and network interactions
developed networks were first pre-trained on the rich and linked to valence-related differences in the impact of emotional
distraction. Cerebralcortex27(1),731–749(2017)
largeAff-WildorAff-Wild2databases.
[20] Kaltwang,S.,Rudovic,O.,Pantic,M.:Continuouspainintensity
estimationfromfacialexpressions. In:InternationalSymposium
ACKNOWLEDGMENTS onVisualComputing,pp.368–377.Springer(2012)
[21] Kazemi, V., Sullivan, J.: One millisecond face alignment with an
The work of Dimitris Kollias was funded by a Teaching ensembleofregressiontrees. In:ProceedingsoftheIEEEConfer-
Fellowship of Imperial College London. Additionally, we enceonComputerVisionandPatternRecognition,pp.1867–1874
(2014)
would like to thank the reviewers for their valuable com-
[22] Khorrami, P., Le Paine, T., Brady, K., Dagli, C., Huang, T.S.:
mentsthathelpedustoimprovethispaper. Howdeepneuralnetworkscanimproveemotionrecognitionon
videodata. In:ImageProcessing(ICIP),2016IEEEInternational
Conferenceon,pp.619–623.IEEE(2016)
REFERENCES
[23] Kim, J., Calhoun, V.D., Shim, E., Lee, J.H.: Deep neural network
with weight sparsity control and pre-training extracts hierarchi-
[1] Acharya,U.R.,Oh,S.L.,Hagiwara,Y.,Tan,J.H.,Adeli,H.,Subha,
cal features and enhances classification performance: Evidence
D.P.: Automated eeg-based screening of depression using deep
fromwhole-brainresting-statefunctionalconnectivitypatternsof
convolutionalneuralnetwork. Computermethodsandprograms
schizophrenia. Neuroimage124,127–146(2016)
inbiomedicine161,103–113(2018)
[24] Kollias,D.,Cheng,S.,Ververas,E.,Kotsia,I.,Zafeiriou,S.:Deep
[2] Aytar, Y., Vondrick, C., Torralba, A.: Soundnet: Learning sound
neuralnetworkaugmentation:Generatingfacesforaffectanalysis.
representations from unlabeled video. In: Advances in Neural
InternationalJournalofComputerVisionpp.1–30(2020)
InformationProcessingSystems,pp.892–900(2016)
[25] Kollias, D., Marandianos, G., Raouzaiou, A., Stafylopatis, A.G.:
[3] Barros,P.,Churamani,N.,Lakomkin,E.,Siqueira,H.,Sutherland,
Interweavingdeeplearningandsemantictechniquesforemotion
A.,Wermter,S.:Theomg-emotionbehaviordataset.arXivpreprint
analysisinhuman-machineinteraction.In:201510thInternational
arXiv:1803.05434(2018)
WorkshoponSemanticandSocialMediaAdaptationandPerson-
[4] Barsoum, E., Zhang, C., Canton Ferrer, C., Zhang, Z.: Training
alization(SMAP),pp.1–6.IEEE(2015)
deep networks for facial expression recognition with crowd-
[26] Kollias, D., Nicolaou, M.A., Kotsia, I., Zhao, G., Zafeiriou, S.:
sourcedlabeldistribution. In:ACMInternationalConferenceon
Recognitionofaffectinthewildusingdeepneuralnetworks. In:
MultimodalInteraction(ICMI)(2016)
ComputerVisionandPatternRecognitionWorkshops(CVPRW),
[5] Chang, W.Y., Hsu, S.H., Chien, J.H.: Fatauva-net : An integrated
2017IEEEConferenceon,pp.1972–1979.IEEE(2017)
deep learning framework for facial attribute recognition, action
[27] Kollias,D.,Schulc,A.,Hajiyev,E.,Zafeiriou,S.:Analysingaffec-
unit(au)detection,andvalence-arousalestimation. In:Proceed-
tivebehaviorinthefirstabaw2020competition. arXivpreprint
ings of the IEEE Conference on Computer Vision and Pattern
arXiv:2001.11409(2020)
RecognitionWorkshop(2017)
[28] Kollias, D., Tagaris, A., Stafylopatis, A., Kollias, S., Tagaris, G.:
[6] Chen,S.,Jin,Q.,Zhao,J.,Wang,S.:Multimodalmulti-tasklearning
Deepneuralarchitecturesforpredictioninhealthcare.Complex&
for dimensional and continuous emotion recognition. In: Pro-
IntelligentSystems4(2),119–131(2018)
ceedingsofthe7thAnnualWorkshoponAudio/VisualEmotion
Challenge,pp.19–26.ACM(2017) [29] Kollias, D., Tzirakis, P., Nicolaou, M.A., Papaioannou, A., Zhao,
[7] Chung,J.,Gulcehre,C.,Cho,K.,Bengio,Y.:Empiricalevaluation G.,Schuller,B.,Kotsia,I.,Zafeiriou,S.:Deepaffectpredictionin-
ofgatedrecurrentneuralnetworksonsequencemodeling. arXiv the-wild:Aff-wilddatabaseandchallenge,deeparchitectures,and
preprintarXiv:1412.3555(2014) beyond. InternationalJournalofComputerVision127(6-7),907–
[8] Deng, D., Zhou, Y., Pi, J., Shi, B.E.: Multimodal utterance-level 929(2019)
affectanalysisusingvisual,audioandtextfeatures.arXivpreprint [30] Kollias, D., Zafeiriou, S.: Aff-wild2: Extending the aff-wild
arXiv:1805.00625(2018) database for affect recognition. arXiv preprint arXiv:1811.07770
[9] Ding,H.,Zhou,S.K.,Chellappa,R.:Facenet2expnet:Regularizing (2018)
a deep face recognition net for expression recognition. In: 2017 [31] Kollias, D., Zafeiriou, S.: A multi-component cnn-rnn approach
12thIEEEInternationalConferenceonAutomaticFace&Gesture for dimensional emotion recognition in-the-wild. arXiv preprint
Recognition(FG2017),pp.118–126.IEEE(2017) arXiv:1805.01452(2018)
[10] Du, S., Tao, Y., Martinez, A.M.: Compound facial expressions [32] Kollias, D., Zafeiriou, S.: A multi-task learning & generation
of emotion. Proceedings of the National Academy of Sciences framework:Valence-arousal,actionunits&primaryexpressions.
111(15),E1454–E1462(2014) arXivpreprintarXiv:1811.07771(2018)
[11] Ekman,P.:Facialactioncodingsystem(facs).Ahumanface(2002) [33] Kollias,D.,Zafeiriou,S.:Trainingdeepneuralnetworkswithdif-
[12] Ekman,P.:Darwin,deception,andfacialexpression.Annalsofthe ferentdatasetsin-the-wild:Theemotionrecognitionparadigm.In:
NewYorkAcademyofSciences1000(1),205–221(2003) 2018InternationalJointConferenceonNeuralNetworks(IJCNN),
[13] Eyben,F.,Wo¨llmer,M.,Schuller,B.:Opensmile:themunichversa- pp.1–8.IEEE(2018)
tileandfastopen-sourceaudiofeatureextractor.In:Proceedingsof [34] Kollias, D., Zafeiriou, S.: Expression, affect, action unit recogni-
the18thACMinternationalconferenceonMultimedia,pp.1459– tion: Aff-wild2, multi-task learning and arcface. arXiv preprint
1462.ACM(2010) arXiv:1910.04855(2019)
[14] Glodek, M., Tschechne, S., Layher, G., Schels, M., Brosch, T., [35] Kuppens,P.,Tuerlinckx,F.,Yik,M.,Koval,P.,Coosemans,J.,Zeng,
Scherer, S., Ka¨chele, M., Schmidt, M., Neumann, H., Palm, G., K.J., Russell, J.A.: The relation between valence and arousal in
et al.: Multiple classifier systems for the classification of audio- subjectiveexperiencevarieswithpersonalityandculture. Journal
visual emotional states. In: International Conference on Affec- ofpersonality85(4),530–542(2017)
tiveComputingandIntelligentInteraction,pp.359–368.Springer [36] Lane,R.D.,Nadel,L.:Cognitiveneuroscienceofemotion. Oxford
(2011) UniversityPress(1999)IEEETRANS.ONAFFECTIVECOMPUTING:SP.I.ONAUTOMATEDPERCEPTIONOFHUMANAFFECTFROMLONGITUDINALBEHAVIORALDATA 12
[37] Little, W., Vyain, S., Scaramuzzo, G., Cody-Rydzewski, S., Grif- of practices and launch of benchmark dataset. International
fiths, H., Strayer, E., Keirns, N.: Introduction to sociology-1st JournalonArtificialIntelligenceTools27(03),1850,011(2018)
canadianedition. BCOpenTextbookproject(2012) [59] Tom,N.L.S.B.L.,etal.:Psychologicalandbiologicalapproachesto
[38] Liu, C., Tang, T., Lv, K., Wang, M.: Multi-feature based emotion emotion. PsychologyPress(1990)
recognitionforvideoclips.In:Proceedingsofthe2018onInterna- [60] Triantafyllopoulos, A., Sagha, H., Eyben, F., Schuller, B.: audeer-
tionalConferenceonMultimodalInteraction,pp.630–634.ACM ing’s approach to the one-minute-gradual emotion challenge.
(2018) arXivpreprintarXiv:1805.01222(2018)
[39] Liu,W.,Wen,Y.,Yu,Z.,Li,M.,Raj,B.,Song,L.:Sphereface:Deep [61] Whissel,C.:Thedictionaryofaffectinlanguage,emotion:Theory,
hypersphereembeddingforfacerecognition. In:Proceedingsof researchandexperience:vol.4,themeasurementofemotions,r.
theIEEEconferenceoncomputervisionandpatternrecognition, PlutchikandH.Kellerman,Eds.,NewYork:Academic(1989)
pp.212–220(2017) [62] Zafeiriou,S.,Kollias,D.,Nicolaou,M.A.,Papaioannou,A.,Zhao,
[40] Mathias, M., Benenson, R., Pedersoli, M., Van Gool, L.: Face G.,Kotsia,I.:Aff-wild:Valenceandarousal’in-the-wild’challenge.
detection without bells and whistles. In: European Conference In:ProceedingsoftheIEEEConferenceonComputerVisionand
onComputerVision,pp.720–735.Springer(2014) PatternRecognitionWorkshops,pp.34–41(2017)
[41] Meng,H.,Bianchi-Berthouze,N.:Naturalisticaffectiveexpression [63] Zhao,X.,Liang,X.,Liu,L.,Li,T.,Han,Y.,Vasconcelos,N.,Yan,S.:
classificationbyamulti-stageapproachbasedonhiddenmarkov Peak-piloted deep network for facial expression recognition. In:
models.In:InternationalConferenceonAffectiveComputingand European conference on computer vision, pp. 425–442. Springer
IntelligentInteraction,pp.378–387.Springer(2011) (2016)
[42] Meng,H.,Huang,D.,Wang,H.,Yang,H.,Ai-Shuraifi,M.,Wang, [64] Zheng,Z.,Cao,C.,Chen,X.,Xu,G.:Multimodalemotionrecog-
Y.: Depression recognition based on dynamic facial and vocal nitionforone-minute-gradualemotionchallenge. arXivpreprint
expressionfeaturesusingpartialleastsquareregression. In:Pro- arXiv:1805.01060(2018)
ceedingsofthe3rdACMinternationalworkshoponAudio/visual
emotionchallenge,pp.21–30.ACM(2013)
[43] Mickley Steinmetz, K.R., Kensinger, E.A.: The effects of valence
andarousalontheneuralactivityleadingtosubsequentmemory.
Psychophysiology46(6),1190–1199(2009)
[44] Mollahosseini,A.,Hasani,B.,Mahoor,M.H.:Affectnet:Adatabase
forfacialexpression,valence,andarousalcomputinginthewild. DimitriosKollias,FellowoftheHigherEduca-
arXivpreprintarXiv:1708.03985(2017) tionAcademy(HEA),holderofaPostGraduate
[45] Nasser,I.M.,Al-Shawwa,M.O.,Abu-Naser,S.S.:Artificialneural Certificate(PGCert)andstudentmemberofthe
networkfordiagnoseautismspectrumdisorder(2019) IEEE, received the Diploma/M.Sc. in Electrical
andComputerEngineeringfromtheECESchool
[46] Ng,H.W.,Nguyen,V.D.,Vonikakis,V.,Winkler,S.:Deeplearning
of the National Technical University of Athens,
foremotionrecognitiononsmalldatasetsusingtransferlearning.
Greece, in 2015 and the M.Sc. in Advanced
In:Proceedingsofthe2015ACMonInternationalConferenceon
Computing from the Department of Computing
MultimodalInteraction,pp.443–449.ACM(2015)
of Imperial College London, U.K., in 2016. He
[47] Nicolaou,M.A.,Gunes,H.,Pantic,M.:Continuouspredictionof
iscurrentlyworkingtowardsthePh.D.degreein
spontaneousaffectfrommultiplecuesandmodalitiesinvalence-
the intelligent Behaviour Understanding Group
arousal space. IEEE Transactions on Affective Computing 2(2),
(iBUG),havingreceivedtheprestigiousTeachingFellowshipofImperial
92–105(2011)
College London. During the course of his Ph.D., he has published
[48] Nicolaou,M.A.,Zafeiriou,S.,Pantic,M.:Correlated-spacesregres-
his research in the top venues for machine learning, perception and
sionforlearningcontinuousemotiondimensions.In:Proceedings
computervisionsuchastheInternationalJournalofComputerVision,
ofthe21stACMinternationalconferenceonMultimedia,pp.773–
theCVPR,ECCVandBMVCConferences,theIEEEIJCNNConference
776.ACM(2013)
and the IEEE SSCI. His research interests span the areas of deep
[49] Nicolle, J., Rapp, V., Bailly, K., Prevost, L., Chetouani, M.: Ro-
learninganddeepneuralnetworks,computervision,affectivecomputing
bust continuous prediction of human emotions using multiscale
andmedicalimaging.
dynamic cues. In: Proceedings of the 14th ACM international
conferenceonMultimodalinteraction,pp.501–508.ACM(2012)
[50] Parkhi, O.M., Vedaldi, A., Zisserman, A.: Deep face recognition.
In:BMVC,vol.1,p.6(2015)
[51] Peng,S.,Zhang,L.,Ban,Y.,Fang,M.,Winkler,S.:Adeepnetwork
forarousal-valenceemotionpredictionwithacoustic-visualcues.
arXivpreprintarXiv:1805.00638(2018)
StefanosZafeiriouiscurrentlyaReaderinMa-
[52] Plutchik, R.: Emotion: A psychoevolutionary synthesis. Harper- chine Learning and Computer Vision with the
collinsCollegeDivision(1980) DepartmentofComputing,ImperialCollegeLon-
[53] Ramirez, G.A., Baltrusˇaitis, T., Morency, L.P.: Modeling latent don,andaDistinguishingResearchFellowwith
discriminativedynamicofmulti-dimensionalaffectivesignals.In: the University of Oulu. He received the Presti-
InternationalConferenceonAffectiveComputingandIntelligent giousJuniorResearchFellowshipsfromImperial
Interaction,pp.396–406.Springer(2011) CollegeLondonin2011tostarthisownindepen-
[54] Ringeval,F.,Schuller,B.,Valstar,M.,Gratch,J.,Cowie,R.,Scherer, dentresearchgroup.HereceivedthePresidents
S.,Mozgai,S.,Cummins,N.,Schmitt,M.,Pantic,M.:Avec2017: Medal for Excellence in Research Supervision
Real-life depression, and affect recognition workshop and chal- for2016.Hereceivedvariousawardsduringhis
lenge. In: Proceedings of the 7th Annual Workshop on Au- doctoralandpostdoctoralstudies.Hehasbeen
dio/VisualEmotionChallenge,pp.3–9.ACM(2017) aGuestEditorofmorethan6journalspecialissuesandco-organized
[55] Ringeval, F., Sonderegger, A., Sauer, J., Lalanne, D.: Introducing more than 15 workshops/special sessions on specialized computer
therecolamultimodalcorpusofremotecollaborativeandaffective vision topics in top venues, such as CVPR/FG/ICCV/ECCV. He has
interactions. In: Automatic Face and Gesture Recognition (FG), coauthored more than 70 journal papers mainly on novel statistical
201310thIEEEInternationalConferenceandWorkshopson,pp. machinelearningmethodologiesappliedtocomputervisionproblems,
1–8.IEEE(2013) such as 2-D/3-D face analysis, deformable object fitting and tracking,
[56] Russell, J.A.: Evidence of convergent validity on the dimensions shape from shading, and human behavior analysis, published in the
ofaffect.Journalofpersonalityandsocialpsychology36(10),1152 mostprestigiousjournalsinhisfieldofresearch,suchasIEEETPAMI,
(1978) IJCV,IEEETIP,IEEETNNLSandmanypapersintopconferences,such
[57] Tagaris,A.,Kollias,D.,Stafylopatis,A.:Assessmentofparkinsons as CVPR, ICCV, ECCV, ICML. His students are frequent recipients of
diseasebasedondeepneuralnetworks. In:InternationalConfer- veryprestigiousandhighlycompetitivefellowships,suchastheGoogle,
enceonEngineeringApplicationsofNeuralNetworks,pp.391– IntelandQualcommones.Hehasmorethan7200citationstohiswork,
403.Springer(2017) h-index44.HewastheGeneralChairofBMVC2017.
[58] Tagaris, A., Kollias, D., Stafylopatis, A., Tagaris, G., Kollias, S.:
Machinelearningforneurodegenerativedisorderdiagnosissurvey"
97,99,"Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface","['D Kollias', 'S Zafeiriou']",2019,345,Expression in-the-Wild,CNN,"expression classification). To address these, we substantially extend the largest available  in-the-wild  We conduct extensive experiments with CNN and CNN-RNN architectures that use",No DOI,arXiv preprint arXiv:1910.04855,https://arxiv.org/abs/1910.04855,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"KOLLIAS,ZAFEIRIOU:AFF-WILD2,MULTI-TASKLEARNING&ARCFACE 1
Expression, Affect, Action Unit Recognition:
Aff-Wild2, Multi-Task Learning and ArcFace
DimitriosKollias1,2 1 DepartmentofComputing
dimitrios.kollias15@imperial.ac.uk ImperialCollegeLondon
London,UK
StefanosZafeiriou1,2 2 FaceSoft.io
s.zafeiriou@imperial.ac.uk
Abstract
Affective computing has been largely limited in terms of available data re-
sources. The need to collect and annotate diverse in-the-wild datasets has be-
come apparent with the rise of deep learning models, as the default approach
toaddressanycomputervisiontask. Somein-the-wilddatabaseshavebeenre-
cently proposed. However: i) their size is small, ii) they are not audiovisual,
iii) only a small part is manually annotated, iv) they contain a small number
of subjects, or v) they are not annotated for all main behavior tasks (valence-
arousalestimation,actionunitdetectionandbasicexpressionclassification). To
addressthese,wesubstantiallyextendthelargestavailablein-the-wilddatabase
(Aff-Wild)tostudycontinuousemotionssuchasvalenceandarousal. Further-
more,weannotatepartsofthedatabasewithbasicexpressionsandactionunits.
Asaconsequence,forthefirsttime,thisallowsthejointstudyofallthreetypes
of behavior states. We call this database Aff-Wild2. We conduct extensive ex-
periments with CNN and CNN-RNN architectures that use visual and audio
modalities; these networks are trained on Aff-Wild2 and their performance is
then evaluated on 10 publicly available emotion databases. We show that the
networksachievestate-of-the-artperformancefortheemotionrecognitiontasks.
Additionally,weadapttheArcFacelossfunctionintheemotionrecognitioncon-
text and use it for training two new networks on Aff-Wild2 and then re-train
theminavarietyofdiverseexpressionrecognitiondatabases.Thenetworksare
showntoimprovetheexistingstate-of-the-art. Thedatabase,emotionrecogni-
tionmodelsandsourcecodeareavailableathttp://ibug.doc.ic.ac.uk/
resources/aff-wild2.
1 Introduction
Until recently affective computing has been mostly studied in controlled settings
ofthe environments[14, 25, 39, 47, 48], with limitedamountof participants[3, 26,
31, 33, 40], using pre-defined scenarios that users have to follow, depicting posed
expressions [1, 28]. However, with the development of large and diverse datasets
in the field of computer vision (and the accompanying performance gains), it has
(cid:13)c 2019.Thecopyrightofthisdocumentresideswithitsauthors.
Itmaybedistributedunchangedfreelyinprintorelectronicforms.
9102
peS
52
]VC.sc[
1v55840.0191:viXra2 KOLLIAS,ZAFEIRIOU:AFF-WILD2,MULTI-TASKLEARNING&ARCFACE
becomeapparentthatthediversityofhumanparticipantsandspontaneousexpres-
sions have to become the prerogatives in deployment of the affective computing
modelsinpractice. Largedatasetswithin-the-wildsettingshavebeenrecentlycol-
lectedtostudyfacialexpression(Expr)analysis[5,7],facialactionunits(AUs)[11]
andcontinuousemotionsofvalenceandarousal(VA)[35,45]in-the-wild.
In [29] a static in-the-wild database (AffectNet) has been created that contains
VA annotations for 1M images. However, from those images, only around 450K
are manually annotated and from those only 350K are valid faces. This number is
moderate for training deep neural networks (DNNs). Also this database is static,
meaning that it contains only images and no video/audio. It is worth to mention
thatthisdatabasealsocontainsannotationsforthesevenbasicexpressionsplusthe
contemptclassfor290Kimages. In[2]astaticin-the-wilddatabase(Emotionet)has
been created that contains AU annotations for 1M images. However, from those
images, only 50K are manually annotated. Half of those consist the validation set
andtheotherhalfthetestset. Again,thesesetshaveasmallsizetoadequatelytrain
DNNsandgeneralizeonotherdatabases. Emotionetdatabasealsocontainsanno-
tations for 6 basic and 10 compound emotion categories. Nevertheless, their total
sizeislessthan3Kandtheclassesareheavilyimbalanced,makingitimpossibleto
trainDNNs. In[20,50],theauthorshavedevelopedthelargestexistingaudiovisual
(A/V) in-the-wild database annotated in terms of VA for around 1.25M. All these
annotations are manual. However this database contains annotations only for VA
andthenumberofsubjectsinthevideosismoderate(298subjectsintotal).
Up to the present, there is no database that contains annotations for all main
behaviortasks(VAestimation,AUdetection,Exprclassification). Also,mostofthe
existing databases do not contain sufficiently large numbers of annotated samples
foreffectivelytrainingDNNs. Thefirstcontributioninthispaperisthecreationof
anewdatasetthatcontains260videoswitharound1.4Mframes,annotatedforVA.
We merge this dataset with Aff-Wild (since this database also contains annotated
videos), generating the so-called Aff-Wild2 database. Next, we annotate parts of
Aff-Wild2 with AUs and seven basic expression labels, creating about 398K and
403K AU and Expr annotations, respectively. To the best of our knowledge, Aff-
Wild2 is the first large scale in-the-wild database containing annotations for all 3
main behavior tasks. It is also the first audiovisual database with annotations for
AUs. AllAUannotateddatabasesdonotcontainaudio,butonlyimagesorvideos.
Next,weconductmulti-taskexperimentsonthisdatabase,foremotionrecogni-
tion. Manyquestionsarisehereafter: howcanwecombinethesethreetasks? what
loss function should we use? The first apparent answer is to use a loss function
equal to the sum of the loss functions of each task. The binary cross entropy loss
isusedforAUdetection. TheMSEandConcordanceCorrelationCoefficient(CCC)
losses are used for VA estimation. The standard loss for expression classification
is the categorical cross entropy. The second contribution of the paper is the devel-
opment of multi-task CNNs, multi-task CNN-RNNs and multi-modal, multi-task
CNN-RNNs,whicharetrainedonAff-Wild2andthenappliedto10publiclyavail-
able databases (including the Aff-Wild one). The results are very promising, beat-
ing the state-of-the-art on emotion recognition in these databases; exceptions are
two databases annotated for Expr Recognition. In one of them, a best performing
networkusedalocalitypreservinglossfunction[22]. This,aswellastherecentten-
dencytodevelopelaboratelossfunctionsforspecifictasks[18],hasledustosearchKOLLIAS,ZAFEIRIOU:AFF-WILD2,MULTI-TASKLEARNING&ARCFACE 3
Figure1: FramesofAff-Wild2,showingsubjectsofdifferentethnicities,agegroups,
emotionalstates,headposes,illuminationconditionsandocclusions
forabetterlossfunctionthanthecategoricalcrossentropy.
In fact, in the related face recognition field, it has been shown [24, 44] that cat-
egorical cross entropy loss is insufficient to acquire discriminating power for face
classification. Severallossfunctionshavebeenproposedformaximizinginter-class
andminimizingintra-classvariance. [4,15]proposemulti-losslearningtoincrease
featurediscriminatingpower. These,requirethoroughminingofpair/tripletsam-
ples, which is a time-consuming procedure. [24] projects the original Euclidean
spaceoffeaturestoanangularspace,introducinganangularmarginforlargerinter-
classvariance.[43]directlyaddsacosinemarginpenaltytothetargetlogit,showing
betterperformancethan[24]. [8]furtherimprovedthediscriminativepowerofface
recognition models, stabilising the training process. As these losses boosted face
recognitionmodelsperformance,inthiswork,wechoosetoadopttheArcFaceloss
[8]andadaptitforemotionrecognition.Tothebestofourknowledge,thisisthefirst
timethatsuchalossdesignedforfacerecognition,isusedinthecontextofemotion
recognition. Ourfinalcontributioninthispaperisthedesignof2networkstrained
withtheArcFaceloss.AftertrainingthemonAff-Wild2,were-trainedthemoneach
oftheexamineddatabases. Ourresultsoutperformedallstate-of-the-artnetworks,
illustrating: i)therichnessofAff-Wild2(providingitwiththeabilitytobeusedas
robust prior for network pre-training) and ii) that the ArcFace loss can be used in
theemotionrecognitionfield,yieldingstate-of-the-artresults.Infact,thisisthevery
firstproofoftheeffectivenessofadditiveangularmargininemotionrecognition.
2 The Aff-Wild2 database
Aff-Wild2isdescribednext,presentingthenewcollecteddatasetanditsproperties,
thegeneratedpartitionsets,theirdistributionsandtheannotationprocedure.
Collected dataset and properties We extend the Aff-Wild database [20, 50], by
collecting a new dataset consisting of 260 YouTube videos, with 1,413,000 frames
andatotallengthof13hoursand5minutes. Thevideoshavebeencollectedusing
the Youtube video sharing website. All of the collected videos are in MP4 format,
with a frame rate of 30, provided under the CC licence. Keywords for retrieving
thevideoswereselectedfromthe2-DEmotionWheel,showninFigure2. Thenew
videos have wide range in subjects’: age (from babies to elderly people); ethnicity
(caucasian/hispanic/latino/asian/black/africanamerican);profession(e.g. actors,
athletes, politicians, journalists); head pose; illumination conditions; occlussions;
emotions.Figure1showsframesofAff-Wild2verifyingtheabovedescribedranges.4 KOLLIAS,ZAFEIRIOU:AFF-WILD2,MULTI-TASKLEARNING&ARCFACE
Thesevideosshowsubjectswho:reactonasurprise,onsomethingthatbringsthem
happiness or fulfillment, on flirting or rejection, on important political issues, on
funny or mean tweets; are stand-up comedians; give a really interesting speech in
ceremonies; are taking an oral exam; are giving lectures on depression, or other
seriousdisorders;areperformingpassive,boring,apathetic,intenseactivities,etc.
Fourexpertsannotatedthenewdatasetintermsofvalenceandarousal,asinthe
caseofAff-Wild. WethenconcatenatedtheAff-Wilddatabasewiththenewdataset,
formingAff-Wild2. Intotal,Aff-Wild2consistsof558videoswith2,786,201frames,
showing both subtle and extreme human behaviours in real-world settings. The
totalnumberofsubjectsis458;279ofwhicharemalesand179females.
Two more tasks were implemented, in which we annotated parts of Aff-Wild2
with AUs and Exprs. In the first, three very experienced annotators annotated 63
videos, with 397,800 frames and a total length of 3 hours and 41 mins, in terms of
AUs 1,2,4,6,12,15,20,25 - described in Figure 2. These videos contain 31 male and
31 female subjects. In the second, three experts annotated 84 videos consisting of
403,758 frames, with a total length of 3 hours and 45 mins, in terms of the 7 basic
expressions. The videos show 42 male and 42 female subjects. Consequently, Aff-
Wild2contains3datasets(VA,AU,Expr);eachcontainsannotationsforarespective
behaviortask(preliminaryworkregardingthe3setscanbefoundin[16,17]). Table
1summarizestheattributesandpropertiesofthethreeannotatedsetsofAff-Wild2.
Figure2: The2DEmotionWheel(left);theAUsannotatedinAff-Wild2(right)
Table 1: General Attributes of Aff-Wild2; in the VA set, top row refers to the new
dataset,whilebottomrowreferstoAff-Wild
Aff-Wild2 #frames #videos #annotators VideoLength MeanResolution
1,413,000 260 4 0.03−26.22mins 1450×900
VAset
1,373,201 298 8 0.10−14.47mins 607×359
AUset 397,800 63 3 0.03−26.22mins 1500×900
Exprset 403,758 84 3 0.04−26.22mins 1350×800
Partition Sets and Distributions Each set (VA, AU, Expr) is split into three sub-
sets: training, validation and test. Partitioning is done in a subject independent
manner,inthesensethatapersoncanappearonlyinoneofthosethreesubsets. In
theVAset,theresultingtraining,validationandtestsubsetsconsistof350,70and
138 videos respectively. In the AU set, the respective subsets consist of 42, 7 and
14 videos respectively. In the Expr set, the corresponding subsets consist of 51, 11
and22videosrespectively. Figure3showsthe2DVAhistogramofthenewdataset,
whichwasaddedtoAff-Wild. Figure3showsthedistributionofthesevenemotion
categories in Aff-Wild2. Table 2 shows the distribution of the activated AUs. WeKOLLIAS,ZAFEIRIOU:AFF-WILD2,MULTI-TASKLEARNING&ARCFACE 5
note that the Expr Set of images can be extended to also contain AU annotations,
accordingtoTable1of[10]. However,thisisoutofthescopeofthecurrentpaper.
Figure3: 2DVAHistogramofthenewdataaddedtoAff-Wild(left);Histogramof
thesevenbasicexpressionsinAff-Wild2(right)
Table2: DistributionofAUannotationsinAff-Wild2
ActionUnit# AU1 AU2 AU4 AU6 AU12 AU15 AU20 AU25
86,677 4,166 56,327 25,226 35,675 3,340 5,695 9,048
TotalNumberofActivatedAUs
43.9% 2.1% 28.5% 12.8% 18.1% 1.7% 2.9% 4.6%
Annotation FourexpertshaveperformedtheVAsetannotation,usingthemethod
proposedin[6]. Valenceandarousalvaluesrangecontinuouslyin[-1,1]. Thefinal
label values are the mean of those four annotations. The mean inter-annotation
correlation is 0.63 for valence and 0.60 for arousal. For the AU set, three experts
haveperformedtheannotation. FortheExprset,threemoreexpertsperformedthe
annotation. In both cases, agreement between the annotators has not always been
100%. Weonlykepttheannotations,onwhichallexpertsagree.
3 Proposed Methods
Two pre-processing steps, on the visual and audio modalities, have been applied
togeneratetheinputdataforDNNbasedemotionanalysis. Wedevelopedthefol-
lowingdeepnetworkarchitecturesforemotionrecognition: i)CNN,single-/multi-
task; ii) CNN-RNN multi-task; iii) CNN-RNN multi-modal (A/V) and multi-task;
iv)newArcFacenetworkswithrespectivelossfunction,asdescribedbelow.
Visual Modality Pre-Processing The SSH detector [30] based on the ResNet and
trainedontheWiderFacedataset[46]wasusedtoextractfaceboundingboxesfrom
allimages. Also, 5faciallandmarks(twoeyes, noseandtwomouthcorners)were
extractedandusedtoperformsimilaritytransformation(forfacealignment). After
thatweobtainthecroppedfaceswhicharethenresizedtodimension96×96×3.
Thepixelintensitiesarenormalizedtotakevaluesin[-1,1].
AudioModalityPre-Processing Theaudiosignal(mono)issampledat44,100Hz.
Then spectrograms are extracted; spectrogram frames are computed over a 33ms6 KOLLIAS,ZAFEIRIOU:AFF-WILD2,MULTI-TASKLEARNING&ARCFACE
windowwith11msoverlap. Theresultingintensityvaluesarenormalizedin[-1,1]
tobeconsistentwiththevisualmodality.
CNN Single- & Multi-Task We employ 3 state-of-the-art networks, SphereFace-
20[24],VGGFace[32],andInceptionResNet[37](denotedasInc.ResNet). Wetrain
thesenetworkstoperformonebehaviortask(VAestimation,AUdetection,orExpr
classification),orjointlyperformall3tasks. Wecallthemulti-taskVGGFACEnet-
work,MT-VGG.Thepredictionsforalltasksarepooledfromthesamefeaturespace.
CNN-RNN Multi-Task As shown in the experimental section, MT-VGG has the
best performance; thus we construct a CNN-RNN multi-task network, based on
MT-VGG.Inmoredetail,a2-layerGRUwith128cellseachisstackedontopofthe
first fc layer of MT-VGG for capturing the temporal dynamics; the output layer is
ontopoftheGRU.WecallthisnetworkMT-VGG-RNN.
CNN-RNN Multi-Modal (A/V) & Multi-Task To handle both video and audio
modalities, we use a feature level fusion strategy in our developed deep learning
model, that we illustrate in Figure 4. This model consists of two identical streams
thatextractfeaturesdirectlyfromrawinputimagesandspectrograms,respectively.
EachstreamconsistsofaMT-VGG-RNN,describedabove,withouttheoutputlayer.
Thefeaturesfromthetwostreamsareconcatenated,forminga256-dimensionalfea-
turevectorthatispassedthrougha2-layerGRUlayerwith128unitsineachlayer,
inordertofusetheinformationoftheaudioandvisualstreams. Theoutputlayer
followsontopofit. WecallthisnetworkA/V-MT-VGG-RNN.
Figure4: A/V-MT-VGG-RNN:theMulti-ModalandMulti-Taskdevelopedmodel
StandardLossFunctions Theobjectivefunctionminimizedduringtrainingofthe
multi-tasknetworksisthesumoftheindividualtasklosses:
L =E[−logepp +log ∑7 epi] (1)
CCE i=1
L
=E[−∑17
(t ·log p +(1−t )·log(1−p ))] (2)
BCE i=1 i i i i
L =1−0.5·(ρ +ρ ),withρ =2s ÷[s2+s2+(x¯−y¯)2] (3)
CCC a v a,v xy x y
where L is the categorical cross entropy loss, L is the binary cross entropy
CCE BCE
loss,p isthepredictionofpositiveclass,p isthepredictionofAU,t ∈{0,1}isthe
p i i i
labelofAU,ρ istheConcordanceCorrelationCoefficient(CCC)ofarousal/valence,
i a,v
s and s are the variances of arousal/valence labels and predicted values respec-
x y
tivelyands isthecorrespondingcovariancevalue.
xyKOLLIAS,ZAFEIRIOU:AFF-WILD2,MULTI-TASKLEARNING&ARCFACE 7
ArcFace Loss Function & Networks Next, we focus on Expr recognition and in-
troduceanewlossfunction. Thesoftmaxcross-entropylossismodifiedasfollows:
L=
−1∑N
log
eWyT ixi
=
−1∑N
log
e(cid:107)Wyi(cid:107)·(cid:107)xi(cid:107)·cosθyi =(cid:107) =W(cid:107) =x ji (cid:107)(cid:107) == = =s, =1 −1∑N
log
es·cosθyi
(4)
N i=1 ∑7 j=1eW jTxi N i=1 ∑7 j=1e(cid:107)Wj(cid:107)·(cid:107)xi(cid:107)·cosθj N i=1 ∑7 j=1es·cosθj
where the embedding feature x ∈Rd denotes the deep feature of the i-th sample
i
belonging to the y-th class, W ∈ Rd denotes the j-th column of the weight W ∈
i j
Rd×7,Nisthebatchsize,θ istheanglebetweenweightW andfeature x,(cid:107)W(cid:107)is
j j i j
fixedto1byl normalization,(cid:107)x (cid:107)isfixedbyl normalizationandre-scaledtos.
2 i 2
Fromeq.4,itcanbeseenthattheembeddingfeaturesaredistributedaroundeach
featurecentreonthehypersphere. Inourcase,weadopttheArcFaceloss,wherean
angularmarginpenaltymbetween x andW isaddedtosimultaneouslyenhance
i yi
the intra-class compactness and inter-class discrepancy (eq.4: θ −→θ +m). m is
yi yi
equal to the geodesic distance margin penalty in the normalised hypersphere. We
refertheinterestedreaderto[8]formoredetailsandexplanationofthisloss.
Next, we develop two networks to account for this loss. The first CNN archi-
tecture,calledMulti-Task-ArcFace-Residual(MT-ArcRes)usesresidualunitsandis
depictedinFig.5;’bn’standsforbatchnormalization,theconvolutionlayerisinthe
format: filterheight×filterwidthconv.,numberofoutputfeaturemaps;thestride
isequalto2,everywhere;thefclayeristheembeddinglayer;theoutputlayerpro-
videsthesevenexpresionclasslogits(WTx, j=1..7). Thesecondnetworkiscalled
j i
Multi-Task-Arcface-VGG (MT-ArcVGG); the difference with MT-ArcRes is that the
rectangularareaintheFigurecontainsVGGFace’slayers.
Figure5: TheMT-ArcResnetworkthathasbeentrainedwiththeArcFaceloss
4 Experimental Study
Theexperimentalstudyconsistsoftwoparts.Inthefirst,wetrainMTCNNs,CNN-
RNNs & Multi-Modal CNN-RNNs, for VA, AU and Expr Recognition on the Aff-
Wild2;then,wetestthenetworkson10differentdatabases,showingthatAff-Wild2
andtheMulti-Tasknetworksprovidethebestpre-trainedframeworkforalargeva-
rietyofemotionrecognitionsettings. Inthesecond, focusingonexpressionrecog-
nition,wefirsttrainArcFacenetworkswithAff-Wild2andthenre-trainthemwith
eachexpressiondatabase;weevaluatethem,achievingstate-of-the-artperformance.
ImplementationDetails&Settings Specificdetailsabouthyperparametersofthe
developedarchitecturescanbefoundinTable3. Allexperimentsinthispaperare8 KOLLIAS,ZAFEIRIOU:AFF-WILD2,MULTI-TASKLEARNING&ARCFACE
implemented in TensorFlow, on a Tesla V100 32GB GPU, using Adam optimizer
(withdefaultvalues)orSGDwithmomentum(0.9)intheArcFaceNetworks’case.
Additionaldetailsfollow:
I) CNNSingle-&Multi-Task: The networks have first been pre-trained for VA esti-
mationontheAff-Wilddatabase,thentheoutputlayerisdiscardedandsubstituted
byanewoneforsingle-ormulti-task, dependingonthenetworktype. Thenthey
aretrainedend-to-endonAff-Wild2.
II)CNN-RNNMulti-Task: TheCNNpartisinitializedwiththeweightsoftheCNN
MT-VGG.Thenthewholearchitectureistrainedend-to-endonAff-Wild2.
III) CNN-RNNMulti-Modal(A/V)&Multi-Task: Training is divided in two phases:
first the audio/visual streams are trained independently and then the audiovisual
network is trained end-to-end. To train each stream individually, we follow the
sameprocedureasintheCNN-RNNMulti-Taskcase. Oncethesinglestreamsare
trained,theyareusedforinitializingthecorrespondingstreamsinthemulti-stream
architecture. Finally,theentireaudiovisualnetworkistrainedend-to-end.
IV)ArcFaceNetworks: BothnetworksarefirsttrainedonAff-Wild2. Then, theyare
re-trained end-to-end on each of the examined databases. During testing we keep
the feature embedding layer, discarding the output layer. For all training images,
weextractfeaturesfromtheembeddinglayerandsplitthemin7clusters. Then,for
eachtestimage,wecomputeitsdistance(basedoncosinesimilarity)fromallcluster
centersandassignittothecenterforwhichthisdistanceisminimum.
Table3: NetworkConfigurations: ST=SingleTask,MT=Multi-Task
ST-&MT-CNN MT-CNN-RNN A/V-MT-CNN-RNN MT-ArcRes/MT-ArcVGG
learningrate [10−4,10−5],best:10−4 [10−4,10−6],best:10−5 [10−3,10−6],best:10−5 [10−4,10−5],best:10−4
batchsize/seq.length 256/- 10/90 5/90 300/-
dropout=0.4,d∈{32,512},s∈{32,64},
parameters dropout=0.4 dropout=0.4 dropout=0.4 m∈{0.1,0.5,1,1.5,2,2.5,3},best:0.1/1
Databases Table4showsthedatabasesusedinourexperimentsalongwiththeir
properties. BP4DS and BP4D+ datasets correspond to the ones used in the FERA
2015[41]and2017[42]Challenges,respectively. Alldatabasesarein-the-wild,apart
from DISFA, BP4DS, BP4D+, which are spontaneous. Let us note that for the Af-
fectNet,BP4DSandBP4D+databases,thetestsetisnotreleased;thuswereportthe
performancesonthevalidationset,whichweusefortesting.
Table4: PropertiesofDatabasesusedinourExperiments
Databases AFEW-VA[21] AffectNet RAF-DB[22] FER2013[13] IMFDB[36] Emotionet DISFA[27] BP4DS[51] BP4D+[52]
ModelofAffect VA VA,Expr Expr Expr Expr AUs AUs AUs AUs
#ofvideos 600 - - - - - 54 1,640 5,463
#offrames 30,050 450,000 15,200 35,887 34,512 50,000 261,630 222,573 967,570
EvaluationMetrics CCC,definedineq.3isusedforVAestimation,asithasbeen
the evaluation criterion in all related Challenges [20, 34]. The usual F1 score is
adoptedforevaluationofAUdetectionandExprclassification. Exceptionsarethe
RAF-DB and FER2013, in which the mean diagonal value of the confusion matrix
andtheaccuracymetric,respectively,arethedefaultperformancemeasures.KOLLIAS,ZAFEIRIOU:AFF-WILD2,MULTI-TASKLEARNING&ARCFACE 9
Results on static databases for VA & Expr Recognition Table 5 presents the re-
sults of different CNN Single- and Multi-task (ST- and MT-) networks in a cross-
databasesetting(networksaretrainedonAff-Wild2andtestedonAffectNet,RAF-
DB,FER2013andIMFDB).TheMT-VGGhasthebestperformanceforbothVAand
Exprrecognition. InTable5,wealsocompareMT-VGG’sperformancewiththatof
thestate-of-the-artineachofthetesteddatabases(theresultsshownaretakenfrom
the respective papers). It can be seen that MT-VGG beats the state-of-the-art in all
databases,illustratingtheexcellentcross-performanceofthegeneratedframework.
Only, inexpressionrecognitioninAffectNet, theobtainedperformanceislowerto
thestate-of-the-art.
Table 5: Cross-database evaluation (models trained on Aff-Wild2 and tested on
other databases) for VA and Expr on static databases: VA evaluation is shown as
(CCC -CCC );singlevaluescorrespondtoexpressions’performancemetrics
V A
MT- ST- MT-Inc. ST-Inc.
Databases MT-VGG ST-VGG AlexNet[29] VGGFACE[22] VGG[12]
SphereFace SphereFace ResNet ResNet
FER2013 0.76 0.73 0.72 0.72 0.74 0.71 - - 0.75
RAF-DB 0.61 0.57 0.53 0.52 0.57 0.55 - 0.58 -
IMFDB 0.42 0.39 0.39 0.38 0.4 0.39 - - -
(0.61-0.46) (0.51-0.42) (0.5-0.43) (0.5-0.4) (0.52-0.45) (0.5-0.42) (0.6-0.34)
AffectNet - -
0.54 0.52 0.5 0.51 0.52 0.51 0.58
Results on video databases for VA & Expr Recognition Table 6 presents the re-
sultsofCNN,CNN-RNNmulti-task,single-andmulti-modalnetworksinacross-
databasesetting,testingonAff-Wild,Aff-Wild2andAFEW-VAdatabases. Itcanbe
seenthattheMT-VGG-RNN(trainedonthevisualmodality)displaysabetterper-
formancethantheMT-VGG,forVAandExprRecognition,inalldatabases. More-
over, MT-VGG-RNN performs best for valence estimation when trained with the
visual modality, whereas performs best for arousal when trained with the audio
modality. Thisisbecauseaudiotendstohavethematicconstancy. Consider,forex-
ample,twofightsequencesinamovie,onebeingaflashyfightsceneandtheother
a one-sided fight with a person being injured. In both cases, arousal can be high
duetoloudandpronouncedmusic,butvalencewillbepositiveintheformerand
negativeinthelattersequence. ItcanalsobeseenthattheA/V-MT-VGG-RNNout-
performs the MT-VGG-RNN, illustrating that the A/V combination improves net-
workperformance,invalence,arousalandexpressionestimation. Table6compares
theperformanceoftheproposednetworkswiththestate-of-the-artintheexamined
databases . It is evident that MT-VGG and MT-VGG-RNN outperform the respec-
tivestate-of-the-art.
Table6: Cross-databaseevaluationforVAandExpronvideodatabases: VAevalu-
ationisshownas(CCC -CCC );singlevaluescorrespondtoF1score
V A
MT-VGG-RNN MT-VGG-RNN bestCNN AffWildNet
Databases MT-VGG A/V-MT-VGG-RNN
visualmodality audiomodality [19,20] [19,20]
Aff-Wild (0.56-0.35) (0.60-0.45) (0.51-0.47) (0.62-0.49) (0.51-0.33) (0.57-0.43)
(0.38-0.3) (0.40-0.33) (0.34-0.36) (0.42-0.38) (0.33-0.25) (0.35-0.28)
Aff-Wild2
0.4 0.43 0.43 0.46 - -
AFEW-VA (0.58-0.53) (0.6-0.6) - - (0.49-0.52) (0.52-0.56)
Results for AU Detection Table 7 compares the performance between MT-VGG
andstate-of-the-artnetworksinacross-databasesettingamongEmotionet,DISFA,
BP4DSandBP4D+;allreportedresultsareforthecommonAUsbetweenthetesting10 KOLLIAS,ZAFEIRIOU:AFF-WILD2,MULTI-TASKLEARNING&ARCFACE
database and Aff-Wild2. It is clear that MT-VGG outperforms the winner [9] of
Emotionet2017Challenge,thebaseline[41]andthewinner[49]ofFERA2015,the
baseline[42]ofFERA2017andthefine-tunedVGG(FVGG)andR-TImethodof[23].
ApartfromEmotionet,inallothercases,thereisaboostinperformance. MT-VGG
displaysaslightlyworseperformancethanthewinner[38]ofFERA2017.
Table7: Cross-databaseevaluationforAUDetection: evaluationmetricisF1score
Databases MT-VGG MT-VGG-RNN [9] [49] [41] [38] [42] FVGG[23] R-T1[23]
Aff-Wild2 0.42 0.44 - - - - - - -
Emotionet 0.52 - 0.51 - - - - - -
DISFA 0.61 - - - - - - 0.52 0.60
BP4DS 0.66 - - 0.54 0.53 - - - -
BP4D+ 0.49 - - - - 0.51 0.34 - -
FromtheaboveTables,itisevidentthatAff-Wild2constitutesaveryrichdatabase
fordeepnetworktrainingandfurthertestingonverydifferentanddiverseemotion
databases;thepresentedcross-databaseresultsvalidateournetworkdevelopments.
ResultswithArcFaceLossforExprRecognition Table8presentsaperformance
comparisonbetween: i)MT-VGG(trainedonAff-Wild2), ii)afine-tunedMT-VGG
(FT-MT-VGG; pre-trained on Aff-Wild2, then re-trained on each of the examined
databases), iii) the two networks trained with the ArcFace loss (MT-ArcRes, MT-
ArcVGG) on Aff-Wild2 and re-trained on each of the examined databases, iv) the
state-of-the-art in these databases (whose results are taken from the respective pa-
pers). TheFT-MT-VGGoutperformsthestate-of-the-artinalldatabases,apartfrom
RAF-DB, where DLP-CNN performs better, however, trained with a locality pre-
servinglossfunction. Table8alsocomparesthisnetwork’sperformancetotheper-
formance of MT-ArcRes and MT-ArcVGG networks, trained with the ArcFace loss
function. Thesenetworksoutperformallothers,includingDLP-CNN.
Table8:Retrainedmulti-tasknetworkswithArcFaceloss,forexpressionrecognition
Databases MT-ArcRes MT-ArcVGG FT-MT-VGG MT-VGG AlexNet[29] DLP-CNN[22] VGG[12]
AffectNet 0.63 0.62 0.59 0.54 0.58 - -
RAF-DB 0.75 0.76 0.71 0.61 - 0.74 -
IMFDB 0.55 0.56 0.51 0.42 - - -
FER2013 0.8 0.79 0.78 0.76 - - 0.75
5 Conclusions
In this paper, we present the first, largest, in-the-wild, A/V database, called Aff-
Wild2,thatisannotatedforVA,AUsandExprs. Webuildandtrainmulti-taskand
multi-modal CNNs and CNN-RNNs on Aff-Wild2 and test their performances on
10 databases, beating the state-of-the-art. We further train two new networks on
Aff-Wild2,adoptingtheArcFacelossfunction,andthenre-trainthemonavariety
ofexpressiondatabases;theresultsimprovetheexistingstate-of-the-art.
Acknowledgements WewouldliketothankViktoriiaSharmanskaforourfruitful
conversations during preparation of this work. The work of S. Zafeiriou has been
partially funded by the EPSRC Fellowship Deform (EP/S010203/1). The work of
DimitriosKolliaswasfundedbyaTeachingFellowshipofImperialCollegeLondon.KOLLIAS,ZAFEIRIOU:AFF-WILD2,MULTI-TASKLEARNING&ARCFACE 11
References
[1] NikiAifanti,ChristosPapachristou,andAnastasiosDelopoulos. Themugfa-
cial expression database. In 11th International Workshop on Image Analysis for
MultimediaInteractiveServicesWIAMIS10,pages1–4.IEEE,2010.
[2] C.F. Benitez-Quiroz, R. Srinivasan, and A.M. Martinez. Emotionet: An accu-
rate, real-time algorithm for the automatic annotation of a million facial ex-
pressionsinthewild.InProceedingsofIEEEInternationalConferenceonComputer
Vision&PatternRecognition(CVPR’16),LasVegas,NV,USA,June2016.
[3] Sanjay Bilakhia, Stavros Petridis, Anton Nijholt, and Maja Pantic. The mah-
nobmimicrydatabase: Adatabaseofnaturalistichumaninteractions. Pattern
recognitionletters,66:52–61,2015.
[4] Sumit Chopra, Raia Hadsell, Yann LeCun, et al. Learning a similarity metric
discriminatively,withapplicationtofaceverification. InCVPR(1),pages539–
546,2005.
[5] RoddyCowieandRandolphRCornelius. Describingtheemotionalstatesthat
areexpressedinspeech. Speechcommunication,40(1):5–32,2003.
[6] RoddyCowie,EllenDouglas-Cowie,SusieSavvidou*,EdelleMcMahon,Mar-
tin Sawey, and Marc Schröder. ’feeltrace’: An instrument for recording per-
ceived emotion in real time. In ISCA tutorial and research workshop (ITRW) on
speechandemotion,2000.
[7] TimDalgleishandMickPower. Handbookofcognitionandemotion. JohnWiley
&Sons,2000.
[8] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface:
Additive angular margin loss for deep face recognition. arXiv preprint
arXiv:1801.07698,2018.
[9] WanDing,Dong-YanHuang,ZhuoChen,XinguoYu,andWeisiLin. Facialac-
tion recognition usingvery deep networks for highly imbalancedclass distri-
bution. In2017Asia-PacificSignalandInformationProcessingAssociationAnnual
SummitandConference(APSIPAASC),pages1368–1372.IEEE,2017.
[10] ShichuanDu,YongTao,andAleixMMartinez. Compoundfacialexpressions
ofemotion.ProceedingsoftheNationalAcademyofSciences,111(15):E1454–E1462,
2014.
[11] PaulEkman. Facialactioncodingsystem(facs). Ahumanface,2002.
[12] Mariana-IulianaGeorgescu, RaduTudorIonescu, andMariusPopescu. Local
learningwithdeepandhandcraftedfeaturesforfacialexpressionrecognition.
IEEEAccess,7:64827–64836,2019.
[13] IanJGoodfellow,DumitruErhan,PierreLucCarrier,AaronCourville,Mehdi
Mirza,BenHamner,WillCukierski,YichuanTang,DavidThaler,Dong-Hyun
Lee, et al. Challenges in representation learning: A report on three machine12 KOLLIAS,ZAFEIRIOU:AFF-WILD2,MULTI-TASKLEARNING&ARCFACE
learning contests. In International Conference on Neural Information Processing,
pages117–124.Springer,2013.
[14] Ralph Gross, Iain Matthews, Jeffrey Cohn, Takeo Kanade, and Simon Baker.
Multi-pie. ImageandVisionComputing,28(5):807–813,2010.
[15] EladHofferandNirAilon.Deepmetriclearningusingtripletnetwork.InInter-
nationalWorkshoponSimilarity-BasedPatternRecognition,pages84–92.Springer,
2015.
[16] Dimitrios Kollias and Stefanos Zafeiriou. Aff-wild2: Extending the aff-wild
databaseforaffectrecognition. arXivpreprintarXiv:1811.07770,2018.
[17] Dimitrios Kollias and Stefanos Zafeiriou. A multi-task learning & generation
framework:Valence-arousal,actionunits&primaryexpressions.arXivpreprint
arXiv:1811.07771,2018.
[18] DimitriosKolliasandStefanosZafeiriou. Trainingdeepneuralnetworkswith
differentdatasetsin-the-wild: Theemotionrecognitionparadigm. In2018In-
ternationalJointConferenceonNeuralNetworks(IJCNN),pages1–8.IEEE,2018.
[19] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoying Zhao, and Ste-
fanosZafeiriou. Recognitionofaffectinthewildusingdeepneuralnetworks.
InComputerVisionandPatternRecognitionWorkshops(CVPRW),2017IEEECon-
ferenceon,pages1972–1979.IEEE,2017.
[20] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou, Athanasios Pa-
paioannou,GuoyingZhao,BjörnSchuller,IreneKotsia,andStefanosZafeiriou.
Deep affect prediction in-the-wild: Aff-wild database and challenge, deep ar-
chitectures, andbeyond. InternationalJournalofComputerVision, 127(6-7):907–
929,2019.
[21] Jean Kossaifi, Georgios Tzimiropoulos, Sinisa Todorovic, and Maja Pantic.
Afew-va database for valence and arousal estimation in-the-wild. Image and
VisionComputing,2017.
[22] Shan Li, Weihong Deng, and JunPing Du. Reliable crowdsourcing and deep
locality-preservinglearningforexpressionrecognitioninthewild. InProceed-
ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
2852–2861,2017.
[23] Wei Li, Farnaz Abtahi, and Zhigang Zhu. Action unit detection with region
adaptation,multi-labelinglearningandoptimaltemporalfusing.InProceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1841–
1850,2017.
[24] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song.
Sphereface: Deephypersphereembeddingforfacerecognition. InProceedings
of the IEEE conference on computer vision and pattern recognition, pages 212–220,
2017.KOLLIAS,ZAFEIRIOU:AFF-WILD2,MULTI-TASKLEARNING&ARCFACE 13
[25] Patrick Lucey, Jeffrey F Cohn, Takeo Kanade, Jason Saragih, Zara Ambadar,
and Iain Matthews. The extended cohn-kanade dataset (ck+): A complete
dataset for action unit and emotion-specified expression. In Computer Vision
andPatternRecognitionWorkshops(CVPRW),2010IEEEComputerSocietyConfer-
enceon,pages94–101.IEEE,2010.
[26] Michael J Lyons, Shigeru Akamatsu, Miyuki Kamachi, Jiro Gyoba, and Julien
Budynek. Thejapanesefemalefacialexpression(jaffe)database. InProceedings
ofthirdinternationalconferenceonautomaticfaceandgesturerecognition,pages14–
16,1998.
[27] SMohammadMavadati, MohammadHMahoor, KevinBartlett, PhilipTrinh,
andJeffreyFCohn. Disfa: Aspontaneousfacialactionintensitydatabase. Af-
fectiveComputing,IEEETransactionson,4(2):151–160,2013.
[28] Gary McKeown, Michel Valstar, Roddy Cowie, Maja Pantic, and Marc
Schroder. The semaine database: Annotated multimodal records of emotion-
allycoloredconversationsbetweenapersonandalimitedagent. IEEETrans-
actionsonAffectiveComputing,3(1):5–17,2011.
[29] Ali Mollahosseini, Behzad Hasani, and Mohammad H Mahoor. Affectnet: A
database for facial expression, valence, and arousal computing in the wild.
arXivpreprintarXiv:1708.03985,2017.
[30] Mahyar Najibi, Pouya Samangouei, Rama Chellappa, and Larry Davis. SSH:
Singlestageheadlessfacedetector. InTheIEEEInternationalConferenceonCom-
puterVision(ICCV),2017.
[31] Maja Pantic, Michel Valstar, Ron Rademaker, and Ludo Maat. Web-based
database for facial expression analysis. In Multimedia and Expo, 2005. ICME
2005.IEEEInternationalConferenceon,pages5–pp.IEEE,2005.
[32] OmkarMParkhi,AndreaVedaldi,andAndrewZisserman. Deepfacerecogni-
tion. InBritishMachineVisionConference(BMVC),2015.
[33] Fabien Ringeval, Andreas Sonderegger, Jens Sauer, and Denis Lalanne. In-
troducing the recola multimodal corpus of remote collaborative and affective
interactions. InAutomaticFaceandGestureRecognition(FG),201310thIEEEIn-
ternationalConferenceandWorkshopson,pages1–8.IEEE,2013.
[34] Fabien Ringeval, Björn Schuller, Michel Valstar, Jonathan Gratch, Roddy
Cowie,StefanScherer,SharonMozgai,NicholasCummins,MaximilianSchmi,
andMajaPantic. Avec2017–real-lifedepression,andaffectrecognitionwork-
shopandchallenge. 2017.
[35] JamesARussell. Evidenceofconvergentvalidityonthedimensionsofaffect.
Journalofpersonalityandsocialpsychology,36(10):1152,1978.
[36] Shankar Setty, Moula Husain, Parisa Beham, Jyothi Gudavalli, Menaka Kan-
dasamy, RadhesyamVaddi, VidyagouriHemadri, JCKarure, RajaRaju, BRa-
jan,etal. Indianmoviefacedatabase: abenchmarkforfacerecognitionunder
widevariations. In2013FourthNationalConferenceonComputerVision,Pattern
Recognition,ImageProcessingandGraphics(NCVPRIPG),pages1–5.IEEE,2013.14 KOLLIAS,ZAFEIRIOU:AFF-WILD2,MULTI-TASKLEARNING&ARCFACE
[37] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi.
Inception-v4,inception-resnetandtheimpactofresidualconnectionsonlearn-
ing. InAAAI,volume4,page12,2017.
[38] ChuangaoTang,WenmingZheng,JingweiYan,QiangLi,YangLi,TongZhang,
andZhenCui.View-independentfacialactionunitdetection.In201712thIEEE
InternationalConferenceonAutomaticFace&GestureRecognition(FG2017),pages
878–882.IEEE,2017.
[39] Ying-li Tian, Takeo Kanade, and Jeffrey F Cohn. Recognizing action units for
facialexpressionanalysis. PatternAnalysisandMachineIntelligence,IEEETrans-
actionson,23(2):97–115,2001.
[40] MichelValstarandMajaPantic. Induceddisgust, happinessandsurprise: an
additiontothemmifacialexpressiondatabase. InProc.3rdIntern.Workshopon
EMOTION(satelliteofLREC):CorporaforResearchonEmotionandAffect,page65,
2010.
[41] MichelFValstar,TimurAlmaev,JeffreyMGirard,GaryMcKeown,MarcMehu,
LijunYin,MajaPantic,andJeffreyFCohn. Fera2015-secondfacialexpression
recognition and analysis challenge. In Automatic Face and Gesture Recognition
(FG),201511thIEEEInternationalConferenceandWorkshopson,volume6,pages
1–8.IEEE,2015.
[42] Michel F Valstar, Enrique Sánchez-Lozano, Jeffrey F Cohn, László A Jeni, Jef-
freyMGirard,ZhengZhang,LijunYin,andMajaPantic. Fera2017-addressing
headposeinthethirdfacialexpressionrecognitionandanalysischallenge. In
201712thIEEEInternationalConferenceonAutomaticFace&GestureRecognition
(FG2017),pages839–847.IEEE,2017.
[43] HaoWang,YitongWang,ZhengZhou,XingJi,DihongGong,JingchaoZhou,
ZhifengLi,andWeiLiu.Cosface:Largemargincosinelossfordeepfacerecog-
nition.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecog-
nition,pages5265–5274,2018.
[44] Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative
featurelearningapproachfordeepfacerecognition. InEuropeanconferenceon
computervision,pages499–515.Springer,2016.
[45] CM Whissel. The dictionary of affect in language, emotion: Theory, research
andexperience: vol.4,themeasurementofemotions,r. PlutchikandH.Keller-
man,Eds.,NewYork: Academic,1989.
[46] Shuo Yang, Ping Luo, Chen Change Loy, and Xiaoou Tang. Wider face: A
face detection benchmark. In IEEE Conference on Computer Vision and Pattern
Recognition(CVPR),2016.
[47] LijunYin,XiaozhouWei,YiSun,JunWang,andMatthewJRosato. A3dfacial
expression database for facial behavior research. In Automatic face and gesture
recognition,2006.FGR2006.7thinternationalconferenceon,pages211–216.IEEE,
2006.KOLLIAS,ZAFEIRIOU:AFF-WILD2,MULTI-TASKLEARNING&ARCFACE 15
[48] Lijun Yin, Xiaochen Chen, Yi Sun, Tony Worm, and Michael Reale. A high-
resolution3ddynamicfacialexpressiondatabase. InAutomaticFace&Gesture
Recognition,2008.FG’08.8thIEEEInternationalConferenceOn,pages1–6.IEEE,
2008.
[49] AnılYüce,HuaGao,andJean-PhilippeThiran. Discriminantmulti-labelman-
ifoldembeddingforfacialactionunitdetection. In201511thIEEEInternational
Conference and Workshops on Automatic Face and Gesture Recognition (FG), vol-
ume6,pages1–6.IEEE,2015.
[50] Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou, Athanasios Pa-
paioannou,GuoyingZhao,andIreneKotsia. Aff-wild:Valenceandarousal’in-
the-wild’challenge. InProceedingsoftheIEEEConferenceonComputerVisionand
PatternRecognitionWorkshops,pages34–41,2017.
[51] Xing Zhang, Lijun Yin, Jeffrey F Cohn, Shaun Canavan, Michael Reale, Andy
Horowitz, Peng Liu, and Jeffrey M Girard. Bp4d-spontaneous: a high-
resolutionspontaneous3ddynamicfacialexpressiondatabase.ImageandVision
Computing,32(10):692–706,2014.
[52] Zheng Zhang, Jeff M Girard, Yue Wu, Xing Zhang, Peng Liu, Umur Ciftci,
Shaun Canavan, Michael Reale, Andy Horowitz, Huiyuan Yang, et al. Mul-
timodal spontaneous emotion corpus for human behavior analysis. In Pro-
ceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,pages
3438–3446,2016."
98,100,Extended deep neural network for facial emotion recognition,"['DK Jain', 'P Shamsolmoali', 'P Sehdev']",2019,416,"Extended Cohn-Kanade, Japanese Female Facial Expression","classification, deep learning, facial expression recognition, neural network","This paper proposed a new deep learning model for the  size and variety of datasets, deep  neural network is the most  datasets are publicly available such as Cohn–Kanade (CK+) [7]",No DOI,Pattern Recognition Letters,https://www.sciencedirect.com/science/article/pii/S016786551930008X,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
99,101,"FACES—A database of facial expressions in young, middle-aged, and older women and men: Development and validation","['NC Ebner', 'M Riediger', 'U Lindenberger']",2010,1382,Karolinska Directed Emotional Faces,FER,"to neutral faces (Isaacowitz et al., 2006). These studies have almost exclusively used  emotional faces of young individuals and have not systematically varied the age of the face, even",No DOI,Behavior research methods,https://link.springer.com/article/10.3758/BRM.42.1.351,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,springer.com,
100,102,"Face behavior a la carte: Expressions, affect and action units in a single network","['D Kollias', 'V Sharmanska', 'S Zafeiriou']",2019,206,Acted Facial Expressions In The Wild,neural network,manifestation of complex facial expressions. The dataset  We train a multi-task neural  network model to jointly perform ( It served as benchmark for the ABAW Competition organized,No DOI,arXiv preprint arXiv:1910.11111,https://arxiv.org/abs/1910.11111,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"NonamemanuscriptNo.
(willbeinsertedbytheeditor)
Face Behavior a` la carte:
Expressions, Affect and Action Units in a Single Network
DimitriosKollias(cid:63) · ViktoriiaSharmanska† · StefanosZafeiriou2
Received:/Accepted:
Abstract Automaticfacialbehavioranalysishasalonghis- learning setting. The model and source code will be made
toryofstudiesintheintersectionofcomputervision,phys- publiclyavailable.
iology and psychology. However it is only recently, with
thecollectionoflarge-scaledatasetsandpowerfulmachine
learning methods such as deep neural networks, that auto- 1 Introduction
maticfacialbehavioranalysisstartedtothrive.Threeofits
iconic tasks are automatic recognition of basic expressions Holisticframeworks,whereseveralparts,e.g.learningtasks,
(e.g. happiness, sadness, surprise), estimation of continu- are interconnected and explicable by the reference to the
ous affect (e.g., valence and arousal), and detection of fa- whole,arecommonincomputervision.Thediverseexam-
cial action units (activations of e.g. upper/inner eyebrows, plesrangefromthesceneunderstandingframeworkthatrea-
nosewrinkles).Upuntilnowthesetaskshavebeenstudied sons about 3D object detection, pose estimation, semantic
independently by collecting a dedicated dataset and train- segmentationanddepthreconstruction[39],thefaceanaly-
ingasingle-taskmodel.Wepresentthefirstandthelargest sisframeworkthataddressesfacedetection,landmarklocal-
study of all facial behaviour tasks learned jointly in a sin- ization,genderrecognition,ageestimation[34],totheuni-
gleholisticframework,whichwecallFaceBehaviorNet.For versal networks for low-, mid-, high-level vision [20] and
this we utilize all publicly available datasets in the com- for various visual tasks [47]. Most if not all of these prior
munity (over 5M images) that study facial behaviour tasks worksrelyonbuildingamulti-taskframeworkwherelearn-
in-the-wild.Wedemonstratethattrainingjointlyanend-to- ingisdonebasedonthegroundtruthannotationswithfullor
endnetworkforalltaskshasconsistentlybetterperformance partialoverlapacrosstasks.Duringtraining,allthetasksare
thantrainingeachofthesingle-tasknetworks.Furthermore, optimisedsimultaneouslyaimingforrepresentationlearning
weproposetwosimplestrategiesforcouplingthetasksdur- thatsupportstheholisticview.
ing training, co-annotation and distribution matching, and Inthisworkweproposethefirstholisticframeworkfor
showtheadvantagesofthisapproach.Finallyweshowthat emotional behaviour analysis in-the-wild, where different
FaceBehaviorNet has learned features that encapsulate all emotionalstatessuchasbinaryactionunitsactivations,ba-
aspectsoffacialbehaviour,andcanbesuccessfullyapplied sic categorical emotions and continuous dimensions of va-
to perform tasks (compound emotion recognition) beyond lenceandarousalconstitutetheinterconnectedtasksthatare
the ones that it has been trained in a zero- and few-shot explicablebythehuman’saffectivestate.Whatmakesitdif-
ferent from the aforementioned holistic approaches is ex-
(cid:63)dimitrios.kollias15@imperial.ac.uk
ploring the idea of task-relatedness, given explicitly either
†sharmanska.v@imperial.ac.uk
2s.zafeiriou@imperial.ac.uk as external expert knowledge or from empirical evidence.
Inthisform,itissimilarlymotivatedtotheclassicalmulti-
(cid:63),†,2Department of Computing, Imperial College London, Queens task literature exploring feature sharing [1] and task relat-
Gate,LondonSW72AZ,UK
edness[18]duringtraining;moreexamplescanbefoundin
(cid:63),†,2FaceSoftLtd
2CenterforMachineVisionandSignalAnalysis,UniversityofOulu, thesurveys[49,32].Howeverinthemulti-tasksetting,one
Oulu,Finland typicallyassumeshomogeneityofthetasks,i.e.tasksofthe
same type such as object classifiers or attribute detectors.
0202
yaM
92
]VC.sc[
3v11111.0191:viXra2 D.Kollias
Themaindifferenceandnoveltyofourworkisthatthepro- b)inferredempiricallyfromexternaldatasetannotations;
posedholisticframework(i)explorestherelatednessofnon- theannotationswillbemadepubliclyavailable.
homogeneous tasks, e.g. tasks for classification, detection, – Weproposeaneffectivealgorithmicapproachofcoupling
regression; (ii) operates over datasets with partial or non- thetasksviaco-annotationanddistributionmatchingand
overlapping annotations of the tasks; (iii) encodes explicit showitseffectivenessforfacialbehaviouranalysis;
relationship between tasks to improve transparency and to – Wepresentthefirst,tothebestofourknowledge,holistic
enableexpertinput. networkforfacialbehaviouranalysis(FaceBehaviorNet)
Recently,alotofefforthasbeenmadetowardscollect- andtrainitend-to-endforpredictingsimultaneously7ba-
inglargescaledatasetsofnaturalisticbehaviourcapturedin sic expressions, 17 action units and continuous valence-
uncontrolledconditions,in-the-wild [23,46,30,3],whichis arousal in-the-wild. For network training we utilize all
thefocusofourstudy.Thereisarichliteratureonrecogni- publiclyavailablein-the-wilddatabasesthat,intotal,con-
tionofbasicemotioncategoriesorexpressions[13]suchas sistofover5Mimageswithpartialand/ornon-overlapping
anger,disgust,fear,happiness,sadness,surpriseandneutral annotationsfordifferenttasks.
in-the-wild[8,7].Continuousaffectdimensionssuchasva- – WeshowthatFaceBehaviorNetgreatlyoutperformseach
lence (how positive/negative a person is) and arousal (how ofthesingle-tasknetworks,validatingthatournetwork’s
active/passiveapersonis)haveattractedattention(VA)re- emotion recognition capabilities are enhanced when it is
cently, as they are naturally suited to represent emotional jointly trained for all related tasks. We further explored
stateanditschangesovertime.Datasetsforcontinuousaf- thefeaturerepresentationlearnedinthejointtrainingand
fectaresimplertocollectwhilebenefitingfromhumancom- showitsgeneralizationabilitiesonthetaskofcompound
puterinteractionstechniques.Automaticfacialanalysishas expressions recognitionwhen no orlittle training data is
been also studied in terms of the facial action units (AUs) available(zero-shotandfew-shotlearning).
codingsystem[14].Thissystemisasystematicwaytocode
the facial motion with respect to activation of facial mus-
2 Relatedwork
cles. It has been widely adopted as a common standard to-
wardssystematicallycategorisingphysicalmanifestationof
Works exist in literature that use emotion labels to com-
complexfacialexpressions.Thedatasetcollectionofaction
plementmissingAUannotationsorincreasegeneralization
units is very costly, as it requires skilled annotators to per-
of AU classifiers [35,43,40]. Our work deviates from such
formthetask.Neverthelesstherehasbeenalotofeffortto
methods, as we target a joint learning of three facial be-
collect action unit annotations and develop automatic AUs
haviour tasks via a single holistic framework, whilst these
annotationtoolboxes[3,2].
worksperformonlyAUdetectionandnotemotionrecogni-
Upuntilnowfacialbehaviourin-the-wildhasbeenpri-
tion(norvalence-arousalestimation).
marilyaddressedbycollectingin-the-wilddatasetstosolve
Multi-tasklearning(MTL)wasfirststudiedin[5],where
individual tasks. However the three aforementioned tasks
the authors propose to jointly learn parallel tasks sharing a
offacialbehaviouranalysisareinterconnected.In[14],the
common representation and transferring part of the knowl-
facial action coding system (FACS) has been built to indi-
edge learned to solve one task to improve the learning of
cateforeachofthebasicexpressionsitsprototypicalaction
theotherrelatedtasks.Sincethen,severalapproacheshave
units. In [12], a dedicated user study has been conducted
adoptedMTLforsolvingdifferentproblemsincomputervi-
tostudytherelationshipbetweenAUsactivationsandemo-
sionandmachinelearning.Inthefaceanalysisdomain,the
tion expressions beyond basic types – compound emotions
useofMTLissomewhatlimited.In[42],MTLwastackled
(e.g.happilysurprised).In[19],theauthorsshowthatneural
throughaneuralnetworkthatjointlyhandledfacerecogni-
networkstrainedforexpressionrecognitionimplicitlylearn
tion and facial attribute prediction tasks. MTL helped cap-
facialactionunits.
ture global feature and local attribute information simulta-
Also, in [29] the authors have discovered that valence neously.
and arousal dimensions could be interpreted by AUs. For One of the closest goals to ours is [6], where an in-
example, AU12 (lip corner puller) is related to positive va- tegrated deep learning framework (FATAUVA-Net) for se-
lence. quentialfacialattributerecognition,AUdetection,andvalence-
Ourmaincontributionsareasfollows: arousalestimationwasproposed.Thisframeworkemployed
– Weproposeaflexibleholisticframeworkthatcanaccom- face attributes as low-level (first component) and AUs as
modatenon-homogeneoustaskswithencodingpriorknowl- mid-level (second component) representations for predict-
edgeoftasksrelatedness.Inourexperimentsweevaluate ingquantizedvalence-arousalvalues(thirdcomponent).How-
two effective strategies of task relatedness: a) obtained evertrainingofthismodelismadeoftransferlearningand
from a cognitive and psychological study, e.g. how ac- fine-tuning steps, is hierarchical and not end-to-end. In a
tionunitsarerelatedtobasicemotioncategories[12],and similar work of [41], a two-level attention with two stageFaceBehaviora`lacarte:Expressions,AffectandActionUnitsinaSingleNetwork 3
multi-tasklearningframeworkwasconstructedforemotion Table1:Basicemotionsandtheirprototypicalandobserva-
recognition and valence-arousal estimation; this work was tionalAUsfrom[12].Theweightswinbracketscorrespond
basedonadatabase(AffectNet[30])annotatedforbothtasks. tothefractionofannotatorsthatobservedtheAUactivation.
In the first attention level, a CNN extracted position-level
Emotion Protot.AUs Observ.AUs(withweightsw)
featuresandtheninthesecondanRNNwithself-attention
happiness 12,25 6(0.51)
wasproposedtomodeltherelationshipbetweenlayer-level
sadness 4,15 1(0.6),6(0.5),11(0.26),17(0.67)
features.
fear 1,4,20,25 2(0.57),5(0.63),26(0.33)
anger 4,7,24 10(0.26),17(0.52),23(0.29)
surprise 1,2,25,26 5(0.66)
3 TheProposedApproach disguste 9,10,17 4(0.31),24(0.26)
We start with the multi-task formulation of the facial be-
a table of the emotions and their prototypical and observa-
haviour model. In this model we have three objectives: (1)
tional actions units (Table 1 in [12]) which we include in
learning seven basic emotions, (2) detecting activations of
Table1forcompleteness.Prototypicalareactionunitsthat
17binaryfacialactionunits,(3)learningtheintensityofthe
arelabelledasactivatedacrossallannotators’responses,ob-
valenceandarousalcontinuousaffectdimensions.Wetrain
servationalareactionunitsthatarelabelledasactivatedby
amulti-taskneuralnetworkmodeltojointlyperform(1)-(3).
afractionofannotators.Forexample,inemotionhappiness
Foragivenimagex ∈ X,wecanhavelabelannotationsof
the prototypical are AU12 and AU25, the observational is
eitheroneofsevenbasicemotionsy ∈ {1,2,...,7},or
emo AU6withweight0.51(observedby51%oftheannotators).
171 binary action units activations y ∈ {0,1}17, or two
au Here let us mention that Table 1 constitutes the relat-
continuous affect dimensions, valence and arousal, y ∈
va ednessbetweentheemotioncategoriesandactionunitsob-
[−1,1]2.Forsimplicityofpresentation,weusethesameno-
tainedfromacognitivestudy.Inourexperiments,inSection
tationxforallimagesleavingthecontexttobeexplainedby
4.2,wealsoshowthatsuchrelatednesscanbeinferredem-
the label notations. We train the multi-task model by mini-
pirically fromexternal datasetannotations. Othermeans of
mizingthefollowingobjective:
describing task relatedness in a holistic framework will be
furtherexploredinthefuture.
L =L +λ L +λ L (1)
MT Emo 1 AU 2 VA
Weproposeasimplestrategyofco-annotationtocouple
L =E [−logp(y |x)]
Emo x,yemo emo the training of emotions and action unit predictions. Given
L AU =E x,yau[−logp(y au|x)] animagexwiththegroundtruthbasicemotiony emo,ween-
L =1−CCC(y ,y¯ ), forcetheprototypicalandobservationalAUsofthisemotion
VA va va
to be activated. We co-annotate the image (x,y ) with
emo
wherethefirsttermisthecrossentropylosscomputedover y ; this image contributes to both L and L 2 in eq.
au Emo AU
imageswithabasicemotionlabel,thesecondtermisthebi- 1.Were-weightthecontributionsoftheobservationalAUs
narycrossentropylosscomputedoverimageswith17AUs withtheannotators’agreementscore(fromTable1).
activations,logp(y au|x):=[(cid:80)1 k7 =1δ k]−1·(cid:80)1 i=7 1δ i· Similarly, for an image x with the ground truth action
[y ai ulogp(y ai u|x)+(1−y ai u)log(1−p(y ai u|x))],whereδ
i
∈ units y au, we check whether we can co-annotate it with an
{0,1} indicates whether the image contains annotation for emotion label. For an emotion to be present, all its proto-
AU . The third term measures the concordance correlation typical and observational AUs have to be present. In cases
i
coefficientbetweenthegroundtruthvalenceandarousaly whenmorethanoneemotionispossible,weassignthelabel
va
andthepredictedy¯ va,CCC(y va,y¯ va) = ρa+ 2ρv,wherefor y emo of the emotion with the largest requirement of pro-
i ∈ {v,a}, y
i
is the ground truth, y¯
i
is the predicted value totypical and observational AUs. The image (x,y au) that
andρ i = is co-annotated with the emotion label y emo contributes to
both L and L in eq. 1. We call this approach the
AU Emo
2·E[(y −E )·(y¯ −E )]
i yi i y¯i . FaceBehaviorNetwithco-annotation.
E2[(y −E )2]+E2[(y¯ −E )2]+(E −E )2
i yi i y¯i yi y¯i
CouplingofbasicemotionsandAUsviadistributionmatch-
Coupling of basic emotions and AUs via co-annotation In ing Theaimhereistoalignthepredictionsoftheemotions
the seminal work [12], the authors conduct a study on the andactionunitstasksduringtraining.Foreachsamplexwe
relationship between emotions (basic and compound) and have the predictions of emotions p(y |x) as the softmax
emo
facialactionunitsactivations.Thesummaryofthestudyis scoresoversevenbasicemotionsandwehavetheprediction
1 Infact,17isanaggregateofactionunitsinalldatasets;typically 2 Hereweoverloadslightlyournotations;forco-annotatedimages,
eachdatasethasfrom10to12AUslabelledbypurposelytrainedan- yau has variable length and only contains prototypical and observa-
notators. tionalAUs.4 D.Kollias
of AUs activations p(yi |x), i = 1,...,17 as the sigmoid Coupling of categorical emotions, AUs with continuous af-
au
scoresover17AUs. fect In our work, continuous affect (valence and arousal)
The distribution matching idea is simple: we match the is implicitly coupled with the basic expressions and action
distributionoverAUpredictionsp(yi |x)withthedistribu- unitsviaajointtrainingprocedure.Alsooneofthedatasets
au
tionq(yi |x),wheretheAUsaremodeledasamixtureover weusedhasannotationsforcategoricalandcontinuousemo-
au
thebasicemotioncategories: tions(AffectNet[30]).Studyinganexplicitrelationshipbe-
tween them is a novel research direction beyond the scope
(cid:88)
q(y ai u|x)= p(y emo|x)p(y ai u|y emo), (2) ofthiswork.
yemo∈{1,...,7}
wherep(yi |y )isdefineddeterministicallyfromTable1 FaceBehaviorNetstructure Fig.1showsthestructureofthe
au emo
andis1forprototypical/observationalactionunits,or0oth- holistic(multi-task,multi-domainandmulti-label)FaceBe-
erwise. For example, AU2 is prototypical for emotion sur- haviorNet, based on the 13 convolutional and pooling lay-
priseandobservationalforemotionfearandthusq(y |x)= ers of VGG-FACE [33] (its fully connected layers are dis-
AU2
1(p(y |x)+p(y |x))3. carded), followed by 2 fully connected layers, each with
2 surprise fear
Thismatchingaimstomakethenetwork’spredictedAUs 4096hiddenunits.A(linear)outputlayerfollowsthatgives
consistent with the prototypical and observational AUs of finalestimatesforvalenceandarousal;italsogives7basic
the network’s predicted emotions. So if, e.g., the network expressionlogitsthatarepassedthroughasoftmaxfunction
predictstheemotionhappinesswithprobability1,i.e., togetthefinal7basicexpressionpredictions;lastly,itgives
p(y |x) = 1, then the prototypical and observational 17AUlogitsthatarepassedthroughasigmoidfunctionto
happiness
AUsofhappiness-AUs12,25and6-needtobeactivatedin getthefinal17AUpredictions.Onecanseethatthepredic-
thedistributionq:q(y |x) = q(y |x) = q(y |x) = tionsforalltasksarepooledfromthesamefeaturespace.
AU12 AU25 AU6
1;q(yi |x)=0,i∈{1,..,14}.
au
In spirit of the distillation approach [17], we match the
distributionsp(yi |x)andq(yi |x)byminimizingthecross 4 ExperimentalStudy
au au
entropywiththesofttargetslossterm4:
Databases Let us first describe the databases that we uti-
17
(cid:88) lizedinallourexperiments.Weselectedtoworkwiththese
L =E [−p(yi |x)logq(yi |x)], (3)
DM x au au databases because they provide a large number of samples
i=1
with accurate annotations of valence-arousal, basic expres-
where all available training samples are used to match the sionsandAUs.Trainingwiththesedatasetsallowsournet-
predictions. We call this approach FaceBehaviorNet with works to learn to recognize affective states under a large
distr-matching. number of image conditions (e.g., each database includes
Amixofthetwostrategies,co-annotationanddistribu- imagesatdifferentresolutions,poses,orientationsandlight-
tion matching, is also possible. Given an image x with the ingconditions).Thesedatasetsalsoincludeavarietyofsam-
groundtruthannotationoftheactionunitsy ,wecanfirst plesinbothgenders,ethnicitiesandraces.
au
co-annotate it with a soft label in form of the distribution TheAff-Wilddatabase[23][46]hasbeenthefirstlarge
overemotionsandthenmatchitwiththepredictionsofemo- scale captured in-the-wild database, containing 298 videos
tionsp(y |x).Morespecifically,foreachbasicemotion, (200subjects)ofaround1.25Mframes,annotatedinterms
emo
wecomputethescoreoveritsprototypicalandobservational ofvalence-arousal.ItservedasbenchmarkfortheAff-Wild
AUsbeingpresent.Forexample,foremotionhappiness,we ChallengeorganizedinCVPR2017.TheAffectNetdatabase
compute(y +y +0.51·y )/(1+1+0.51),or [30]containsaround1Mfacialimages,400Kofwhichwere
AU12 AU25 AU6
all weights equal 1 if without reweighting. We take a soft- manuallyannotatedintermsof7discreteexpressions(plus
maxoverthescorestoproducetheprobabilitiesoveremo- contempt) and valence-arousal. The AFEW database[9] is
tion categories. In this variant, every single image that has used in the EmotiW Challenges that focus on audiovisual
groundtruthannotationofAUswillhaveasoftemotionla- classificationofeachofthe1,809videoclipsintothe7ba-
bel assigned. Finally we match the predictions p(y |x) sicemotioncategories.TheRAF-DBdatabase[27]contains
emo
andthesoftlabelbyminimizingthecrossentropywiththe 15.2Kfacialimagesannotatedintermsofthe7basicand11
softtargetssimilarlytoeq.3.WecallthisapproachFaceBe- compoundemotioncategories.
haviorNetwithsoftco-annotation. The EmotioNet database [15] is a large-scale database
witharound1Mfacialexpressionimages;950Kimageswere
3 WealsotriedavariantwithreweightingforobservationalAUs,i.e.
automaticallyannotatedandtheremaining50Kimageswere
p(y ai u|yemo)=w
4 This can be seen as minimizing the KL-divergence KL(p||q) manually annotated with 11 AUs. Additionally, a subset of
acrossthe17actionunits. about 2.5K images was annotated with the 6 basic and 10FaceBehaviora`lacarte:Expressions,AffectandActionUnitsinaSingleNetwork 5
Fig. 1: The holistic (multi-task, multi-domain, multi-label) FaceBehaviorNet; ’VA/AU/EXPR-BATCH’ refers to batches
annotatedintermsofVA/AU/7basicexpressions
compoundemotions.ItwasreleasedfortheEmotioNetChal- lenges);forAUdetectioninEmotioNettheChallenge’smet-
lenge in 2017 [4]. The DISFA database [28] is a lab con- ric was the average between: a) the mean (across all AUs)
trolleddatabasewithspontaneousemotionexpressions,an- F1scoreandb)themean(acrossallAUs)accuracy;forthe
notated for the presence, absence and intensity of 12 AUs. expressionclassification,itwastheaveragebetween:a)the
Itconsistsof260Kvideoframesof27subjectsrecordedby mean(acrossallemotions)F1scoreandb)theunweighted
twocameras.TheBP4D-Spontaneousdatabase[48](inthe averagerecall(UAR)overallemotioncategories.
rest of the paper we refer to it as BP4D) contains 61 sub-
jectswith223Kframesandisannotatedfortheoccurrence
Pre-Processing We used the SSH detector [31] based on
and intensity of 27 AUs. It has been used as a part of the
ResNetandtrainedontheWiderFacedataset[44]toextract,
FERA 2015 Challenge [37]. The BP4D+ database [50] is
from all images, face bounding boxes and 5 facial land-
anextensionofBP4Dincorporatingdifferentmodalitiesas
marks;thelatterwereusedforfacealignment.Allcropped
well as more subjects (140). It is annotated for occurrence
andalignedimageswereresizedto96×96×3pixelreso-
of34AUsandintensityfor5ofthem.Ithasbeenusedasa
lutionandtheirintensityvalueswerenormalizedto[−1,1].
partoftheFERA2017Challenge[38].
Here let us note that for AffectNet, AFEW, BP4D and
BP4D+,notestsetisreleased;thusweusethereleasedvali-
4.1 TrainingImplementationDetails
dationsettotestonandrandomlydividethetrainingsetinto
atrainingandavalidationsubset(witha85/15split).
At this point let us describe the strategy that was used for
feeding images from different databases to FaceBehavior-
Performance Measures We use: i) the CCC for Aff-Wild Net. At first, the training set was split into three different
(CCC was the evaluation criterion of Aff-Wild Challenge) sets,eachofwhichcontainedimagesthatwereannotatedin
andAffectnet,ii)thetotalaccuracyforAFEW(thismetric termsofeithervalence-arousal,oractionunits,orsevenba-
wastheevaluationcriterionoftheEmotiWChallenges),the sic expressions; let us denote these sets as VA-Set, AU-Set
mean diagonal value of the confusion matrix for RAF-DB and EXPR-Set, respectively. During training, at each itera-
(this criterion was selected for evaluating the performance tion, three batches, one from each of these sets (as can be
on this database by [27]), the F1 score for AffectNet, iii) seeninFig.1),wereconcatenatedandfedtoFaceBehavior-
theF1scoreforDISFA,BP4DandBP4D+(thismetricwas Net.Thisstepisimportantfornetworktraining,because:i)
the evaluation criterion of the FERA 2015 and 2017 Chal- the network minimizes the objective function of eq. 1; at6 D.Kollias
each iteration, the network has seen images from all cat- 2showstheresultsforalltheseapproaches,whenTables1
egories and thus all loss terms contribute to the objective and3areusedforthetaskrelatedness.
function, ii) since the network sees an adequate number of Many deductions can be made. Firstly, when FaceBe-
imagesfromallcategories,theweightupdates(duringgra- haviorNet is trained with any coupling loss, or any com-
dientdescent)arenotbasedonnoisygradients;thisinturn bination of these, it displays a better (or in the worst case
preventspoorconvergencebehaviors;otherwise,wewould equal) performance on all databases, in both different task
needtotackletheseproblems,e.g.doasynchronousSGDas relatedness scenarios. This validates the fact that the pro-
proposedin[20]tomakethetaskparameterupdatesdecou- posed losses help to couple the three studied tasks regard-
pled,iii)theCCCcostfunction(definedinSection3)needs lessofwhichrelatednessscenariowasfollowed;thisshows
anadequatesequenceofpredictions. the generality of the proposed losses that boosted the per-
SinceVA-Set,AU-SetandEXPR-Sethaddifferentsizes, formance of the network. Secondly, the performance in es-
theyneededtobe’aligned’.Todoso,weselectedthebatches timationofvalenceandarousalimproved,althoughwedid
of these sets in such a manner, so that after one epoch we notexplicitlydesignedacouplinglossforthis;weonlycou-
will have sampled all images in the sets. In particular, we pledemotioncategoriesandactionunits.Weconjecturethat
chosebatchesofsize401,247and103fortheVA-Set,AU- whenactionunitdetectionandexpressionclassificationac-
SetandEXPR-Set,respectively.ThetrainingofFaceBehav- curacy is improving (due to coupling), valence and arousal
iorNetwasperformedinanend-to-endmanner,withalearn- performancealsoimproves,becausevalenceandarousalare
ingrateof10−4.A0.5Dropoutvaluewasusedinthefully implicitly coupled with emotions via joint dataset annota-
connectedlayers.TrainingwasperformedonaTeslaV100 tionsforbothemotiontypes.
32GBGPU;trainingtimewasabout2days. Thirdly,inallscenarios,theco-annotationlossresultsin
FaceBehaviorNethavingtheworstperformancewhencom-
paredtoallothercouplinglosses.Furthermore,inbothset-
tings,whenthenetworkwastrainedwiththesoftco-annotation
4.2 Task-RelatednessfromEmpiricalEvidences
loss, the performance increase in AUs was bigger than the
Table 1 was created using a cognitive and psychological corresponding increase in expressions, whereas when the
studywithhumanparticipants.Here,wecreateanotherTa- network was trained with the distr-matching loss the per-
bleinferredempiricallyfromexternaldatasetannotations.In formanceincreaseinexpressionswasbiggerthanthecorre-
particular,weusetherecentlyproposedAff-Wild2database
spondingincreaseinAUs.Finally,overallbestresultshave
[26,24,25,22], which is the first in-the-wild database that been achieved, in both scenarios, when FaceBehaviorNet
containsannotationsforallthreebehaviortasksthatweare wastrainedwithbothsoftco-annotationanddistr-matching
dealingwithinthispaper.Itconsistsof558videos:allcon- losses.Inparticular,inbothsettings,anaverageperformance
tainVAannotations,63containAUannotationsand84con- increase of more than 2% has been observed when using
tainbasicexpressionannotations.Itservedasbenchmarkfor bothcouplinglosses,comparedtothe(two)caseswhenonly
theABAWCompetitionorganizedinIEEEFG2020. oneofthemwasused.
At first, we trained a network for AU detection on the
unionofAff-Wild2andGFTdatabases[16].Next,thisnet-
4.4 Results:ComparisonwithState-of-the-Artand
work was used for automatically annotating all Aff-Wild2
Single-TaskMethods
videos with AUs. These annotations will be made publicly
available.Table3showsthedistributionofAUsforeachba- Next, we trained a VGG-FACE network on all the dimen-
sic expression. In parenthesis next to each AU (e.g. AU12) sionallyannotateddatabasestopredictvalenceandarousal;
is the percentage of images (0.82) annotated with the spe- wealsotrainedanotherVGG-FACEnetworkonallcategori-
cific expression (happiness) in which this AU (AU12) was callyannotateddatabases,toperformsevenbasicexpression
activated. classification;finallywetrainedathirdVGG-FACEnetwork
onalldatabasesannotatedwithactionunits,soastoperform
AU detection. For brevity these three single-task networks
4.3 Results:AblationStudy aredenotedas’(3×)VGG-FACEsingle-task’inonerowof
Table4.
At first, we compare the performance of FaceBehaviorNet Wecomparedthesenetworks’performanceswiththeper-
when trained: i) with only the losses of eq. 1 and without formanceofFaceBehaviorNetwhentrainedwithandwith-
usingthecouplinglossesdescribedinSection3,ii)withco- outthecouplinglosses.Wealsocomparethemwiththeper-
annotation coupling loss, iii) with soft co-annotation cou- formancesofthestate-of-the-artmethodologiesofeachuti-
plinglossandiv)withdistr-matchingcouplingloss,vi)with lized database: i) FATAUVA-Net [6] (described in Section
softco-annotationanddistr-matchingcouplinglosses.Table 2),whichwasthewinnerofAff-WildChallenge;ii)thebestFaceBehaviora`lacarte:Expressions,AffectandActionUnitsinaSingleNetwork 7
Table2:Performanceevaluationofvalence-arousal,sevenbasicexpressionandactionunitspredictionsonalluseddatabases
providedbytheFaceBehaviorNetwhentrainedwith/withoutthecoupledlosses,underthetwotaskrelatednessscenarios.
Databases Relatedness Aff-Wild AffectNet AFEW RAF-DB EmotioNet DISFA BP4D BP4D+
F1 Total Meandiag. F1 F1 F1 F1
FaceBehaviorNet CCC-V CCC-A CCC-V CCC-A Accuracy
Score Accuracy ofconf.matrix Score Score Score Score
nocouplingloss - 0.55 0.36 0.56 0.46 0.54 0.38 0.67 0.49 0.94 0.52 0.61 0.57
co-annotation [12] 0.56 0.38 0.56 0.46 0.55 0.40 0.67 0.49 0.94 0.54 0.64 0.58
softco-annotation [12] 0.56 0.39 0.57 0.47 0.57 0.41 0.67 0.50 0.94 0.54 0.64 0.60
distr-matching [12] 0.56 0.37 0.57 0.49 0.57 0.42 0.68 0.50 0.94 0.56 0.66 0.58
softco-annotation
[12] 0.59 0.41 0.59 0.50 0.60 0.43 0.70 0.51 0.95 0.57 0.67 0.60
anddistr-matching
co-annotation Aff-Wild2 0.55 0.37 0.56 0.47 0.54 0.40 0.67 0.50 0.93 0.54 0.61 0.57
softco-annotation Aff-Wild2 0.56 0.37 0.57 0.47 0.55 0.42 0.68 0.52 0.94 0.58 0.63 0.59
distr-matching Aff-Wild2 0.57 0.39 0.60 0.51 0.57 0.42 0.69 0.50 0.94 0.57 0.62 0.58
softco-annotation
Aff-Wild2 0.60 0.40 0.61 0.51 0.60 0.42 0.71 0.54 0.94 0.60 0.66 0.60
anddistr-matching
Table 3: Relatedness between basic emotions and AUs, in- which forces different classes to stay apart - and a newly
ferredfromAff-Wild2. createdloss-thatpullsthelocallyneighboringfacesofthe
sameclasstogether.Forthetaskofexpressionrecognition,
Emotion AUs(withweightsw)
ourapproachusedthestandardcrossentropyloss;therefore
happy 12(0.82),25(0.7),6(0.57),7(0.83),10(0.63)
a fair comparison cannot be made with our model because
sad 4(0.53),15(0.42),1(0.31),7(0.13),17(0.1)
DLP-CNNusesadifferentcostfunctionthatwedonotuse
fearful 1(0.52),4(0.4),25(0.85),5(0.38),7(0.57),10(0.57)
angry 4(0.65),7(0.45),25(0.4),10(0.33),9(0.15) andthusDLP-CNNisnotlistedinTable4.
surprised 1(0.38),2(0.37),25(0.85),26(0.3),5(0.5),7(0.2)
disgusted 9(0.21),10(0.85),17(0.23),4(0.6),7(0.75),25(0.8) It might be argued that the more data used for network
training(eveniftheycontainpartialornon-overlappingan-
notations),thebetternetworkperformancewillbeinalltasks.
performing CNN (VGG-FACE) on Aff-Wild [21][23]; iii) However this may not be true, as the three studied tasks
thebaselinenetworks(AlexNet)onAffectNet[30](inTable are non-homogeneous and each one of them contains am-
4 they are denoted as ’(2 ×) AlexNet’ as they are two dif- biguous cases: i) there is generally discrepancy in the per-
ferentnetworks:oneforVAestimationandanotherforex- ceptionofthedisgust,fear,sadnessand(negative)surprise
pressionclassification);iv)thebaselinenetwork(non-linear emotions across different people and across databases; ii)
Chi-squarekernelbasedSVM)[10]onEmotiWChallenges; the exact valence and arousal value for a particular affect
v) VGG-FACE-mSVM [27] on RAF-DB; vi) the baseline is also not consistent among databases; iii) the AU anno-
network(AlexNet)onEmotioNet[4];vii)ResNet-34,which tation process is a hard to do and error prone one. Nev-
was the best performing network on EmotioNet [11]; viii) ertheless, from Table 4, it can be verified that FaceBehav-
DiscriminantLaplacianEmbeddingextension(DLEextension)[4i5o]r,Net achieved a better performance on all databases than
which was the winner of FERA 2015 on BP4D; ix) [36], the independently trained VGG-FACE single-task models.
which was the winner of FERA 2017 on BP4D+. Table 4 This shows that, all described facial behavior understand-
displaystheperformancesofallthesenetworks. ing tasks are coherently correlated to each other; training
anend-to-endarchitecturewithheterogeneousdatabasessi-
Here, letus mention that inAff-Wild the best perform-
multaneously,therefore,leadstoimprovedperformance.
ingnetworkisAffWildNet[23][21],thathasaCCCof0.57
and0.43invalenceandarousalrespectively;thisnetworkis InTable4,itcanbeobservedthatFaceBehaviorNettrained
aCNN-RNNthatexploitsthefactthattheAff-Wilddatabase withnocouplingloss:i)ouperformsthestate-of-the-artby
isanaudio-visualone.Additionally,faciallandmarkswere 3.5% (average CCC) on Aff-Wild, 4% (average CCC) on
provided as additional inputs to this network, thus improv- AffectNet, 9% on RAF-DB and 2% on BP4D; ii) has the
ing its performance. The latter is not included in Table 4. sameperformanceonAFEW;iii)showsinferiorperformance
However,althoughbeingaCNN-RNNnetwork,itsaverage by4%onAffectNetand1.5%(onaverage)onEmotioNet,
CCCisthesameastheaverageCCCofourCNNnetwork, 1%onBP4D+.However,whenFaceBehaviorNetistrained
FaceBehaviorNet trained with the two coupling losses (in withsoftco-annotationanddistr-matchinglosses(eitherwhen
bothtaskrelatednesssettings). taskrelatednessisinferredfromAff-Wild2orfrom[12]),it
Let us also mention that on RAF-DB the best perform- showssuperiorperformancetoallstate-of-the-artmethods.
ing network is the Deep Locality-preserving CNN (DLP- The fact that it outperforms these methods and the single-
CNN)of[27]withaperformancemetricvalueof0.74;this tasknetworks,inbothtaskrelatednesssettings,verifiesthe
network was trained using a joint classical softmax loss - generality of the proposed losses; network performance is8 D.Kollias
Table 4: Performance evaluation of valence-arousal, seven basic expression and action units predictions on all utilized
databasesprovidedbytheFaceBehaviorNetandstate-of-the-artmethods.
Databases Aff-Wild AffectNet AFEW RAF-DB EmotioNet DISFA BP4D BP4D+
F1 Total Meandiagonal F1 Mean F1 F1 F1
CCC-V CCC-A CCC-V CCC-A
Score Accuracy ofconf.matrix Score Accuracy Score Score Score
bestperformingCNN[21][23] 0.51 0.33 - - - - - - - - - -
FATAUVA-Net[6] 0.40 0.28 - - - - - - - - - -
(2×)AlexNet[30] - - 0.60 0.34 0.58 - - - - - - -
non-linearSVM[9] - - - - - 0.38 - - - - - -
VGG-FACE-mSVM[27] - - - - - - 0.58 - - - - -
AlexNet[4] - - - - - - - 0.39 0.83 - - -
ResNet-34[11] - - - - - - - 0.64 0.82 - - -
DLEextension[45] - - - - - - - - - - 0.59 -
[36] - - - - - - - - - - - 0.58
(3×)VGG-FACEsingle-task 0.52 0.31 0.53 0.43 0.51 0.37 0.59 0.41 0.92 0.47 0.56 0.54
FaceBehaviorNet,nocouplingloss 0.55 0.36 0.56 0.46 0.54 0.38 0.67 0.49 0.94 0.52 0.61 0.57
FaceBehaviorNet,softco-annotation
0.59 0.41 0.59 0.50 0.60 0.43 0.70 0.51 0.95 0.57 0.67 0.60
anddistr-matching,[12]
FaceBehaviorNet,softco-annotation
0.60 0.40 0.61 0.51 0.60 0.42 0.71 0.54 0.94 0.60 0.66 0.60
anddistr-matching,Aff-Wild2
boostedindependentlyoftheTableoftaskrelatednesswhich (probability)ofAUsthatthiscompoundemotionispresent;
wasused. ii) p(y ) and p(y ) are FaceBehaviorNet’s predic-
emo1 emo2
tions of only the basic expression classes emo1 and emo2
that are mixed and form the compound class (e.g., if the
4.5 Results:Zero-ShotandFew-ShotLearning compoundclassishappilysurprisedthenemo1ishappyand
emo2issurprised);iii)thelasttermofthesumisaddedonly
In order to further prove and validate that FaceBehavior-
tothehappilysurprisedandhappilydisgustedclassesandis
Net learned good features encapsulating all aspects of fa-
either 0 or 1 depending on whether FaceBehaviorNet’s va-
cialbehavior,weconductedzero-shotlearningexperiments
lencepredictionisnegativeorpositive,respectively;thera-
forclassifying compoundexpressions.Given thatthere ex-
tionale is that only happily surprised and (maybe) happily
istonly2datasets(EmotioNetandRAF-DB)annotatedwith
disgustedclasseshavepositivevalence;allotherclassesare
compoundexpressionsandthattheydonotcontainalotof
expectedtohavenegativevalenceastheycorrespondtoneg-
samples(lessthan3,000each),atfirst,weusedthepredic-
ative emotions. Our final prediction was the class that had
tionsofFaceBehaviorNettogetherwiththerulesfrom[12]
themaximumcandidatescore.
togeneratecompoundemotionpredictions.Additionally,to
Table5showstheresultsofthisapproachwhenweused
demonstratethesuperiorityofFaceBehaviorNet,weusedit
thepredictionsofFaceBehaviorNettrainedwithandwithout
asapre-trainednetworkinafew-shotlearningexperiment.
thesoftco-annotationanddistr-matchinglosses.Bestresults
Wetookadvantageofthefactthatournetworkhaslearned
have been obtained when the network was trained with the
good features and used them as priors for fine-tuning the
couplinglosses.Onecanobserve,thatthisapproachoutper-
networktoperformcompoundemotionclassification.
formedby4.8%theVGG-FACE-mSVM[27]whichhasthe
samearchitectureasournetworkandithasbeentrainedfor
RAF-DBdatabase Atfirst,weperformedzero-shotexperi-
compoundemotionclassification.
mentsonthe11compoundcategoriesofRAF-DB.Wecom-
Next,wetargetfew-shotlearning.Inparticular,wefine-
putedacandidatescore,C (y ),foreachclassy :
s emo emo tunetheFaceBehaviorNet(trainedwithandwithoutthesoft
co-annotationanddistr-matchinglosses)onthesmalltrain-
17 17 ingsetofRAF-DB.InTable5wecompareitsperformance
(cid:88) (cid:88)
C (y )=[ p(yk |y )]−1· p(yk |x)p(yk |y )
s emo au emo au au emo to a state-of-the-art network. It can be seen that our fine-
k=1 k=1 tuned FaceBehaviorNet, trained with and without the cou-
+p(y )+p(y )
emo1 emo2 plinglosses,outperformedby1.2%and3.7%,respectively,
p(y |x) the best performing network, DLP-CNN, that was trained
+0.5·( v +1),p(y |x)(cid:54)=0,
|p(y |x)| v withalossdesignedforthisspecifictask.
v
where:i)thefirsttermofthesumisFaceBehaviorNet’spre-
dictions of only the prototypical (and observational) AUs EmotioNetdatabase Next,weperformedzero-shotexperi-
that are associated with this compound class according to ments on the EmotioNet basic and compound set that was
[12]; in this manner, every AU acts as an indicator for this releasedfortherelatedChallenge.Thissetincludes6basic
particularemotionclass;thistermsdescribestheconfidence plus10compoundcategories,asdescribedatthebeginningFaceBehaviora`lacarte:Expressions,AffectandActionUnitsinaSingleNetwork 9
Table5:PerformanceevaluationofgeneratedcompoundemotionpredictionsonEmotioNetandRAF-DBdatabases.
Databases EmotioNet RAF-DB
F1 Unweighted Meandiagonal
Methods
Score AverageRecall ofconf.matrix
zero-shot,FaceBehaviorNet,nocouplingloss 0.243 0.260 0.342
zero-shot,FaceBehaviorNet,bothcouplinglosses 0.312 0.329 0.364
NTechLab[4] 0.255 0.243 -
VGG-FACE-mSVM[27] - - 0.316
DLP-CNN[27] - - 0.446
fine-tunedFaceBehaviorNet,nocouplingloss - - 0.458
fine-tunedFaceBehaviorNet,bothcouplinglosses - - 0.483
of this Section. Our zero-shot methodology was similar to ofIEEEInternationalConferenceonComputerVision&Pattern
theonedescribedabovefortheRAF-DBdatabase. Recognition(CVPR’16),LasVegas,NV,USA,June2016. 2
4. C Fabian Benitez-Quiroz, Ramprakash Srinivasan, Qianli Feng,
The results of this experiment can be found in Table 5.
YanWang,andAleixMMartinez. Emotionetchallenge:Recog-
Bestresultshavealsobeenobtainedwhenthenetworkwas nitionoffacialexpressionsofemotioninthewild. arXivpreprint
trainedwiththetwocouplinglosses.Itcanbeobservedthat arXiv:1703.01210,2017. 5,7,8,9
this approach outperformed by 5.7% and 8.6% in F1 score 5. RichCaruana. Multitasklearning. Machinelearning,28(1):41–
75,1997. 2
and Unweighted Average Recall (UAR), respectively, the
6. Wei-YiChang,Shih-HuanHsu,andJen-HsienChien.Fatauva-net
state-of-the-art NTechLab’s [4] approach, which used the :Anintegrateddeeplearningframeworkforfacialattributerecog-
Emotionet’simageswithcompoundannotation. nition,actionunit(au)detection,andvalence-arousalestimation.
InProceedingsoftheIEEEConferenceonComputerVisionand
PatternRecognitionWorkshop,2017. 2,6,8
7. Roddy Cowie and Randolph R Cornelius. Describing the emo-
5 Conclusions tionalstatesthatareexpressedinspeech. Speechcommunication,
40(1):5–32,2003. 2
Inthispaper,wepresentedFaceBehaviorNet,thefirstholis- 8. TimDalgleishandMickPower.Handbookofcognitionandemo-
tion. JohnWiley&Sons,2000. 2
ticframeworkforemotionalbehaviouranalysisin-the-wild.
9. AbhinavDhall,RolandGoecke,ShreyaGhosh,JyotiJoshi,Jesse
FaceBehaviorNetisanend-to-endnetworktrainedforjoint:
Hoey,andTomGedeon. Fromindividualtogroup-levelemotion
basicexpressionrecognition,actionunitdetectionandvalence- recognition:Emotiw5.0. InProceedingsofthe19thACMInter-
arousal estimation. All publicly available databases, con- nationalConferenceonMultimodalInteraction,pages524–528.
ACM,2017. 4,8
taining over 5M images, that study facial behaviour tasks
10. AbhinavDhall,AmanjotKaur,RolandGoecke,andTomGedeon.
in-the-wild, have been utilized. Additionally we proposed Emotiw2018:Audio-video,studentengagementandgroup-level
twosimplestrategiesforcouplingthetasksduringtraining, affect prediction. In Proceedings of the 2018 on International
namely co-annotation and distribution matching. We per- Conference on Multimodal Interaction, pages 653–656. ACM,
2018. 7
formedexperimentscomparingtheperformanceofFaceBe-
11. WanDing,Dong-YanHuang,ZhuoChen,XinguoYu,andWeisi
haviorNettosingle-tasknetworks,aswellasstate-of-the-art Lin.Facialactionrecognitionusingverydeepnetworksforhighly
methodologies.FaceBehaviorNetconsistentlyoutperformed imbalancedclassdistribution.In2017Asia-PacificSignalandIn-
formationProcessingAssociationAnnualSummitandConference
all of them. Finally, we explored the feature representation
(APSIPAASC),pages1368–1372.IEEE,2017. 7,8
learned in the joint training and showed its generalization
12. ShichuanDu,YongTao,andAleixMMartinez.Compoundfacial
abilities on the task of compound expressions, under zero- expressionsofemotion. ProceedingsoftheNationalAcademyof
shotorfew-shotlearningsettings. Sciences,111(15):E1454–E1462,2014. 2,3,7,8
13. PaulEkmanandWallaceVFriesen. Constantsacrossculturesin
thefaceandemotion. Journalofpersonalityandsocialpsychol-
ogy,17(2):124,1971. 2
References 14. RosenbergEkman.Whatthefacereveals:Basicandappliedstud-
iesofspontaneousexpressionusingtheFacialActionCodingSys-
1. AndreasArgyriou,TheodorosEvgeniou,andMassimilianoPontil. tem(FACS). OxfordUniversityPress,USA,1997. 2
Multi-task feature learning. In Advances in neural information 15. C Fabian Benitez-Quiroz, Ramprakash Srinivasan, and Aleix M
processingsystems,pages41–48,2007. 1 Martinez.Emotionet:Anaccurate,real-timealgorithmfortheau-
2. TadasBaltrusˇaitis,MarwaMahmoud,andPeterRobinson.Cross- tomaticannotationofamillionfacialexpressionsinthewild. In
datasetlearningandperson-specificnormalisationforautomatic ProceedingsoftheIEEEConferenceonComputerVisionandPat-
actionunitdetection.In201511thIEEEInternationalConference ternRecognition,pages5562–5570,2016. 4
andWorkshopsonAutomaticFaceandGestureRecognition(FG), 16. JeffreyMGirard,Wen-ShengChu,La´szlo´ AJeni,andJeffreyF
volume6,pages1–6,2015. 2 Cohn. Sayettegroupformationtask(gft)spontaneousfacialex-
3. C.F. Benitez-Quiroz, R. Srinivasan, and A.M. Martinez. Emo- pressiondatabase. In201712thIEEEInternationalConference
tionet:Anaccurate,real-timealgorithmfortheautomaticanno- onAutomaticFace&GestureRecognition(FG2017),pages581–
tationofamillionfacialexpressionsinthewild. InProceedings 588.IEEE,2017. 610 D.Kollias
17. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the 35. AdriaRuiz,JoostVandeWeijer,andXavierBinefa. Fromemo-
knowledgeinaneuralnetwork. arXiv:1503.02531,2015. 4 tionstoactionunitswithhiddenandsemi-hidden-tasklearning.In
18. DineshJayaraman,FeiSha,andKristenGrauman. Decorrelating ProceedingsoftheIEEEInternationalConferenceonComputer
semanticvisualattributesbyresistingtheurgetoshare. InPro- Vision,pages3703–3711,2015. 2
ceedingsoftheIEEEConferenceonComputerVisionandPattern 36. ChuangaoTang,WenmingZheng,JingweiYan,QiangLi,Yang
Recognition,pages1629–1636,2014. 1 Li,TongZhang,andZhenCui. View-independentfacialaction
19. Pooya Khorrami, Thomas Paine, and Thomas Huang. Do deep unitdetection. In2017 12thIEEEInternational Conferenceon
neuralnetworkslearnfacialactionunitswhendoingexpression Automatic Face & Gesture Recognition (FG 2017), pages 878–
recognition?InProceedingsoftheIEEEInternationalConference 882.IEEE,2017. 7,8
onComputerVisionWorkshops,pages19–27,2015. 2 37. Michel F Valstar, Timur Almaev, Jeffrey M Girard, Gary McK-
20. Iasonas Kokkinos. Ubernet: Training a universal convolutional eown,MarcMehu,LijunYin,MajaPantic,andJeffreyFCohn.
neuralnetworkforlow-,mid-,andhigh-levelvisionusingdiverse Fera2015-secondfacialexpressionrecognitionandanalysischal-
datasetsandlimitedmemory.InProceedingsoftheIEEEConfer- lenge. In Automatic Face and Gesture Recognition (FG), 2015
enceonComputerVisionandPatternRecognition,pages6129– 11thIEEEInternationalConferenceandWorkshopson,volume6,
6138,2017. 1,6 pages1–8.IEEE,2015. 5
21. Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoying 38. Michel F Valstar, Enrique Sa´nchez-Lozano, Jeffrey F Cohn,
Zhao,andStefanosZafeiriou.Recognitionofaffectinthewildus- La´szlo´ A Jeni, Jeffrey M Girard, Zheng Zhang, Lijun Yin, and
ingdeepneuralnetworks.InComputerVisionandPatternRecog- MajaPantic. Fera2017-addressingheadposeinthethirdfacial
nition Workshops (CVPRW), 2017 IEEE Conference on, pages expressionrecognitionandanalysischallenge.In201712thIEEE
1972–1979.IEEE,2017. 7,8 InternationalConferenceonAutomaticFace&GestureRecogni-
22. Dimitrios Kollias, Attila Schulc, Elnar Hajiyev, and Stefanos tion(FG2017),pages839–847.IEEE,2017. 5
Zafeiriou. Analysing affective behavior in the first abaw 2020 39. Shenlong Wang, Sanja Fidler, and Raquel Urtasun. Holistic 3d
competition. arXivpreprintarXiv:2001.11409,2020. 6 scene understanding from a single geo-tagged image. In Pro-
23. Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A. Nicolaou, ceedingsoftheIEEEConferenceonComputerVisionandPattern
AthanasiosPapaioannou,GuoyingZhao,BjrnSchuller,IreneKot- Recognition,pages3964–3972,2015. 1
40. ShangfeiWang,QuanGan,andQiangJi. Expression-assistedfa-
sia, and Stefanos Zafeiriou. Deep affect prediction in-the-wild:
cialactionunitrecognitionunderincompleteauannotation. Pat-
Aff-wilddatabaseandchallenge,deeparchitectures,andbeyond.
ternRecognition,61:78–91,2017. 2
InternationalJournalofComputerVision,feb2019. 2,4,7,8
41. XiaohuaWang,MuziPeng,LijuanPan,MinHu,ChunhuaJin,and
24. Dimitrios Kollias and Stefanos Zafeiriou. Aff-wild2: Extend-
FujiRen. Two-levelattentionwithtwo-stagemulti-tasklearning
ing the aff-wild database for affect recognition. arXiv preprint
forfacialemotionrecognition. arXivpreprintarXiv:1811.12139,
arXiv:1811.07770,2018. 6
2018. 2
25. DimitriosKolliasandStefanosZafeiriou. Amulti-tasklearning
42. ZhanxiongWang,KekeHe,YanweiFu,RuiFeng,Yu-GangJiang,
&generationframework:Valence-arousal,actionunits&primary
andXiangyangXue.Multi-taskdeepneuralnetworkforjointface
expressions. arXivpreprintarXiv:1811.07771,2018. 6
recognitionandfacialattributeprediction. InProceedingsofthe
26. DimitriosKolliasandStefanosZafeiriou. Expression,affect,ac-
2017ACMonInternationalConferenceonMultimediaRetrieval,
tionunitrecognition:Aff-wild2,multi-tasklearningandarcface.
pages365–374.ACM,2017. 2
arXivpreprintarXiv:1910.04855,2019. 6
43. Jiajia Yang, Shan Wu, Shangfei Wang, and Qiang Ji. Multi-
27. ShanLi,WeihongDeng,andJunPingDu.Reliablecrowdsourcing
plefacialactionunitrecognitionenhancedbyfacialexpressions.
and deep locality-preserving learning forexpression recognition
In 2016 23rd International Conference on Pattern Recognition
inthewild. InProceedingsoftheIEEEConferenceonComputer
(ICPR),pages4089–4094.IEEE,2016. 2
VisionandPatternRecognition,pages2852–2861,2017. 4,5,7,
44. ShuoYang,PingLuo,ChenChangeLoy,andXiaoouTang.Wider
8,9
face:Afacedetectionbenchmark. InIEEEConferenceonCom-
28. SMohammadMavadati,MohammadHMahoor,KevinBartlett,
puterVisionandPatternRecognition(CVPR),2016. 5
Philip Trinh, and Jeffrey F Cohn. Disfa: A spontaneous facial
45. Anıl Yu¨ce, Hua Gao, and Jean-Philippe Thiran. Discriminant
actionintensitydatabase.AffectiveComputing,IEEETransactions
multi-label manifold embedding for facial action unit detection.
on,4(2):151–160,2013. 5
In201511thIEEEInternationalConferenceandWorkshopson
29. Marc Mehu and Klaus R Scherer. Emotion categories and di-
AutomaticFaceandGestureRecognition(FG),volume6,pages
mensionsinthefacialcommunicationofaffect:Anintegratedap-
1–6.IEEE,2015. 7,8
proach. Emotion,15(6):798,2015. 2 46. Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,
30. Ali Mollahosseini, Behzad Hasani, and Mohammad H Mahoor. AthanasiosPapaioannou,GuoyingZhao,andIreneKotsia. Aff-
Affectnet:Adatabaseforfacialexpression,valence,andarousal wild:Valenceandarousalin-the-wildchallenge. InComputerVi-
computinginthewild.arXivpreprintarXiv:1708.03985,2017.2, sion and Pattern Recognition Workshops (CVPRW), 2017 IEEE
3,4,7,8 Conferenceon,pages1980–1987.IEEE,2017. 2,4
31. MahyarNajibi,PouyaSamangouei,RamaChellappa,andLarry 47. AmirRZamir,AlexanderSax,WilliamShen,LeonidasJGuibas,
Davis. SSH: Single stage headless face detector. In The IEEE JitendraMalik,andSilvioSavarese. Taskonomy:Disentangling
InternationalConferenceonComputerVision(ICCV),2017. 5 task transfer learning. In Proceedings of the IEEE Conference
32. Sinno Jialin Pan and Qiang Yang. A survey on transfer learn- onComputerVisionandPatternRecognition,pages3712–3722,
ing. IEEE Transactions on knowledge and data engineering, 2018. 1
22(10):1345–1359,2010. 1 48. XingZhang,LijunYin,JeffreyFCohn,ShaunCanavan,Michael
33. OmkarMParkhi,AndreaVedaldi,andAndrewZisserman. Deep Reale,AndyHorowitz,PengLiu,andJeffreyMGirard. Bp4d-
facerecognition. InBritishMachineVisionConference(BMVC), spontaneous:ahigh-resolutionspontaneous3ddynamicfacialex-
2015. 4 pressiondatabase.ImageandVisionComputing,32(10):692–706,
34. RajeevRanjan,SwamiSankaranarayanan,CarlosDCastillo,and 2014. 5
RamaChellappa. Anall-in-oneconvolutionalneuralnetworkfor 49. YuZhangandQiangYang.Asurveyonmulti-tasklearning.arXiv
face analysis. In 2017 12th IEEE International Conference on preprintarXiv:1707.08114,2017. 1
AutomaticFace&GestureRecognition(FG2017),pages17–24. 50. Zheng Zhang, Jeff M Girard, Yue Wu, Xing Zhang, Peng Liu,
IEEE,2017. 1 Umur Ciftci, Shaun Canavan, Michael Reale, Andy Horowitz,FaceBehaviora`lacarte:Expressions,AffectandActionUnitsinaSingleNetwork 11
HuiyuanYang,etal.Multimodalspontaneousemotioncorpusfor
humanbehavioranalysis.InProceedingsoftheIEEEConference
onComputerVisionandPatternRecognition,pages3438–3446,
2016. 5"
101,103,Face detection using mixtures of linear subspaces,"['MH Yang', 'N Abuja', 'D Kriegman']",2000,121,Toronto Face Database,classification,"in face recognition [2]. The reason for this is that FLD provides a better projection than PCA  for pattern classification. In the second proposed method, we decompose the training face",No DOI,… on Automatic Face and …,https://ieeexplore.ieee.org/document/840614,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
102,104,Face expression recognition with a 2-channel convolutional neural network,"['D Hamester', 'P Barros', 'S Wermter']",2015,164,Toronto Face Database,"facial expression recognition, neural network","CNN and train it with the Acted Facial Expression in the Wild (AFEW) dataset [17]. In the sec   with the Toronto Face Dataset. After that, a softmax layer is trained with the AFEW dataset.",No DOI,2015 international joint …,https://ieeexplore.ieee.org/document/7280539,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
103,105,Face recognition using kernel direct discriminant analysis algorithms,"['J Lu', 'KN Plataniotis']",2003,836,Toronto Face Database,"classification, classifier",") [20] have in pattern regression and classification tasks, we propose a new kernel discriminant  analysis algorithm for face recognition. The algorithm generalizes the strengths of the",No DOI,IEEE transactions on …,https://ieeexplore.ieee.org/document/1176132,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,ieee.org,
104,106,Face recognition with radial basis function (RBF) neural networks,"['MJ Er', 'S Wu', 'J Lu', 'HL Toh']",2002,975,Toronto Face Database,"classification, classifier, neural network",excellent performance both in terms of error rates of classification and learning efficiency.   Database Our experiments were performed on the face database which contains a set of face,No DOI,IEEE transactions on neural …,https://ieeexplore.ieee.org/document/1000134,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
105,107,Face recognition: A convolutional neural-network approach,"['S Lawrence', 'CL Giles', 'AC Tsoi']",1997,4525,Toronto Face Database,"classification, neural network",We are interested in rapid classification and hence we do not assume that time is  available for extensive preprocessing and normalization. Good algorithms for locating,No DOI,IEEE transactions on …,https://ieeexplore.ieee.org/document/554195,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
106,108,Facenet2expnet: Regularizing a deep face recognition net for expression recognition,"['H Ding', 'SK Zhou', 'R Chellappa']",2017,489,Toronto Face Database,"CNN, classification, deep learning, facial expression recognition, machine learning, neural network","adapt to the new domain task (facial expression recognition), we attach the fully- databases:  CK+ [27], Oulu-CASIA [28], Toronto Face Database (TFD) [29] and Static Facial Expression",No DOI,… face & gesture recognition (FG …,https://arxiv.org/abs/1609.06591,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"FaceNet2ExpNet: Regularizing a Deep Face Recognition Net for
Expression Recognition
Hui Ding1, Shaohua Kevin Zhou2 and Rama Chellappa1
1 University of Maryland, College Park
2 Siemens Healthcare Technology Center, Princeton, New Jersey
Abstract—Relatively small data sets available for expression
recognition research make the training of deep networks for
expression recognition very challenging. Although fine-tuning
can partially alleviate the issue, the performance is still below
acceptable levels as the deep features probably contain redun-
dant information from the pre-trained domain. In this paper,
wepresentFaceNet2ExpNet,anovelideatotrainanexpression
recognition network based on static images. We first propose
a new distribution function to model the high-level neurons
of the expression network. Based on this, a two-stage training
algorithm is carefully designed. In the pre-training stage, we
traintheconvolutionallayersoftheexpressionnet,regularized
by the face net; In the refining stage, we append fully-
connected layers to the pre-trained convolutional layers and
train the whole network jointly. Visualization shows that the
model trained with our method captures improved high-level
expression semantics. Evaluations on four public expression
databases, CK+, Oulu-CASIA, TFD, and SFEW demonstrate
that our method achieves better results than state-of-the-art.
I. INTRODUCTION
Deep Convolutional Neural Networks (DCNN) have
Fig.1. Thered-boxedimagesaregeneratedbythemodeltrainedwithour
demonstrated impressive performance improvements for
method,whiletheblack-boxedimagesarefromthefacenetworkfine-tuned
many problems in computer vision. One of the most im- ontheexpressiondataset.Wecanseetheimagesproducedbythefacenet
portant reasons behind its success is the availability of aredominatedwithfaces,whileourmodelrepresentsthefacialexpressions
better.ModelsarevisualizedbyDeepDraw[11].
large-scale training databases, for example, ImageNet [1]
for image classification, Places [2] for scene recognition,
CompCars[3]forfine-grainedrecognitionandMegaFace[4] face and expression datasets. As we can see from Fig. 1,
for face recognition. the images (black-boxed) generated by the face net are
However, it is not uncommon to have small datasets in dominated by faces as they should, which weakens the
many application areas, facial expression recognition being network’s ability to represent the different expressions. (ii)
one of them. With a relatively small set of training images, thenetworkdesignedforthefacerecognitiondomainisoften
even when regularization techniques such as Dropout [5] too big for the expression task, thus the overfitting issue is
and Batch Normalization [6] are used, the results are not still severe.
satisfactory. The mostly used method is to fine-tune a net- Inthispaper,wepresentFaceNet2ExpNet,anovellearn-
work that has been pre-trained on a large dataset. Because ing algorithm that incorporates face domain knowledge to
of the generality of the pre-learned features, this approach regularize the training of an expression recognition network.
has achieved great success [7]. Specially we first propose a new distribution function to
Motivated by this observation, several previous works [8], model the high-level neurons of the expression net using
[9]onexpressionrecognitionutilizefacerecognitiondatasets the information derived from the fine-tuned face net. Such
to pre-train the network, which is then fine-tuned on the modeling naturally leads to a regression loss which serves
expressiondataset.Thelargeamountoflabeledfacedata[4], as feature-level regularization that pushes the intermediate
[10],makesitpossibletotrainafairlycomplicatedanddeep features of the expression net to be close to those of the
network. Moreover, the close relationship between the two fine-tuned face net. Next, to further improve the discrim-
domains facilites the transfer learning of features. inativeness of the learned features, we refine the network
Although this strategy performs well, it has two notable with strong supervision from the label information. We
problems: (i) the fine-tuned face net may still contain infor- adopt a conventional network architecture, consisting of
mation useful for subject identification. This is because of convolutional blocks followed by fully-connected layers, to
the large size gap (several orders of magnititudes) between design our expression net. The training is carried out in two
6102
peS
22
]VC.sc[
2v19560.9061:viXrastages: in the first stage, only the convolutional layers are layersofrestrictedBoltzmanmachines(RBM)arestackedto
trained. We utilize the deep features from the face net as learnhierarchicalfeatures.Tofurtherboosttheperformance,
the supervision signal to make the learning easier. It also a loopy boosted deep belief network (DBN) framework was
containsmeaningfulknowledgeabouthumanfaces,whichis explored in [19]. Feature learning, feature selection and
importantforexpressionrecognition,too.Afterthefirststage classifier design are learned jointly. In the forward phase,
of learning is completed, we add randomly initialized fully- several DBNs extract features from the overlapped facial
connected (FC) layers and jointly train the whole network patches. Then, AdaBoosting is adopted to combine these
using the label information in the second stage. As observed patch-based DBNs. In the fine-tuning phase, the loss from
bypreviousworks[12],FClayersgenerallycapturedomain- both weak and strong classifiers are backproped. In [20],
specific semantics. So we only utilize the face net to guide to utilize the temporal information for video-based expres-
thelearningoftheconvolutionallayersandtheFClayersare sion recognition, 3D CNN was applied to learn low-level
trained from scratch. Moreover, we empirically find that late features. Then, a GMM model is trained on the features,
middle layer (e.g. pool5 for VGG-16 [13]) is more suitable and the covariance matrix for each component composes
for training supervision due to the richness of low entropy the expressionlet. Motivated by the domain knowledge that
neurons. In both training stages, only expression images are facial expression can be decomposed into a combination of
used. facial action units (AU), a deformable facial part model was
From Fig. 1, we can see that the models trained with our exploredin[21].Multiplepartfiltersarelearnedtodetectthe
method capture the key properties of different expressions. location of discriminative facial parts. To further cope with
For example, the angry expression is displayed by frowned theposeandidentityvariations,aquadraticdeformationcost
eye brows and a closed mouth; the surprise expression is is used.
represented by a large opened mouth and eyes. This method More recently, Jung et al. [22] trained a deep temporal
isdifferentfromknowledgedistillation[14].Herewedonot geometry network and a deep temporal appearance network
have a large accurate network trained on the same domain with facial landmarks and images. To effectively fuse these
to produce reliable outputs from softmax. It is also different twonetworks,ajointfine-tuningmethodisproposed.Specif-
from FitNets [15], which is mainly used to train a thinner ically, the weight values are frozen and only the top layers
and deeper network. are trained. In [23], Mollahosseini et al. discovered that
To validate the effectiveness of our method, we perform the inception network architecture works very well for ex-
experiments on both constrained (CK+, Oulu-CASIA, TFD) pressionrecognitiontask.Multiplecrossdatasetexperiments
andunconstraineddatasets(SFEW).Forallthefourdatasets, are performed to show the generality of the learned model.
we achieve better results than the current state-of-the-art. In [24], [25], a two-step training procedure is suggested,
The remainder of this paper is organized as follows. Sec- where in the first step, the network was trained using a
tion2brieflyintroducesrelatedworks.TheFaceNet2ExpNet relativelylargeexpressiondatasetfollowedbytrainingonthe
algorithm is presented in Section 3. Experiments and com- target dataset. Even though the image is of low resolution
putational analysis are discussed in Section 4 and Section 5. and the label of the relatively large dataset is noisy, this
We conclude this work in Section 6. approachiseffective.Theworkcloselyrelatedtooursis[9],
which proposed to employ a peak expression image (easy
II. RELATEDWORKS
sample) to help the training of a network with input from a
In [16], Zhong et al. observed that only a few active weak expression image (hard sample). This is also achieved
facial patches are useful for expression recognition. These by a regression loss between the intermediate feature maps.
active patches include: common patches for the recognition However,apairofthesamesubjectandthesameexpression
ofallexpressionsandspecificpatchesthatareonlyimportant image is required as input for training. This is not always
for single expression. To locate these patches, a two-stage possible, especially in unconstrained expression recognition
multi-tasksparselearningframeworkisproposed.Inthefirst scenario, where the subject identities are usually unknown.
stage, multi-task learning with group sparsity is performed
to search for the common patches. In the second stage, III. APPROACH
face recognition is utilized to find the specific patches. A. Motivation
However, the sequential search process is likely to find
We write our expression net as:
overlapped patches. To solve this problem, Liu et al. [17]
integrated the sparse vector machine and multi-task learning
O =h (g (I))
into a unified framework. Instead of performing the patch
θ2 θ1
selection in two separate phrases, an expression specific where h represents the fully connected layers, and g cor-
feature selection vector and a common feature selection responds to the convolutional layers. θ and θ are the
2 1
vector are employed together. To get more discriminative parameters to be learned. I is the input image, and O is the
featuresinsteadofhand-craftedfeatures,Liuetal.[18]used output before softmax.
patch-based learning method. Subsequently, a group feature First, the parameters θ of the convolutional layers are
1
selection scheme based on the maximal mutual information learned. In [26], Xie et al. observed that the high-level
and minimal redundancy criterion is presented. Lastly, three neurons are exponentially decayed. To be more specific,by denoting the outputs of the l layer as x , and the
th c,w,h
average response value over the spatial dimension as
W−1H−1
1 (cid:88) (cid:88)
x = x (1)
c W ×H c,w,h
w=0 h=0
where C is the number of output channels in the l layer,
th
and W, H is the width and height of the response maps,
respectively.Thenthedistributionfunctioncanbeformulated
as follows:
f(Xl)=C p·e−||Xl||p p (2)
where Xl = [x ,...,x ] ∈ RC, and C is a normalization
1 C p
constant. ||·||p is the p norm.
p th
To incorporate the knowledge of a face net, we propose
to extend (2) to have the following form, i.e., :
f(Xl)=C p·e−||Xl−µ||p p (3)
The mean is modeled by the face net, µ = G(I). And
G represents the face net’s convolutional layers. This is
motivated by the observation that the fine-tuned face net Fig.2. Two-stageTrainingAlgorithm.Instage(a),thefacenetisfrozen
already achieves competitive performance on the expression and provides supervision for the expression net. The regression loss is
backpropedonlytotheexpressionnet.Theconvolutionallayersaretrained
dataset, so it should provide a good initialization point for
inthisstage.Instage(b),therandomlyinitializedfully-connectedlayersare
theexpressionnet.Thus,wedonotwantthelattertodeviate attachedtothetrainedconvolutionalblocks.Thewholenetworkistrained
much from the former. jointlywithcross-entropyloss.Thefacenetisnormallymuchdeeperthan
theexpressionnet.
Using the maximum likelihood estimation (MLE) proce-
dure, we can derive the loss function as:
maxL =maxlogf(Xl) layers is a 3×3 window. For the pooling layer, it is 3×3
1
θ1 θ1
with stride 2. The numbers of the output channels are 64,
=maxlogC
p·e−||Xl−µ||
(4) 128, 256, 512, 512. After the last pooling layer, we add
θ1
another 1 × 1 convolutional layer, which serves to bridge
=min||g (I)−G(I)||p
θ1 θ1 p the gap between face and expression domains. Moreover, it
also helps to adapt the dimension if the last pooling layer of
Note that if p = 2 and without G, this is the normal
the expression net does not match the face net. To reduce
l regularizer. Thus we can also view the face net acting
2
overfitting, we have only one fully-connected layer with
as a regularizer, which stabilizes the training step of the
dimension 256. Note, if the spatial size of the last pooling
expression net.
layerbetweenthefacenetandexpressionnetdoesnotmatch
B. Training Algorithm exactly,thendeconvolution(fractionallystridedconvolution)
The training algorithm has the following two steps: can be used for upsampling.
Inthefirststage,wetraintheconvolutionallayerswiththe
C. Which Layer to Transfer?
loss function in (4). The face net is frozen, and the outputs
from the last pooling layer are used to provide supervision In this section, we explore the layer selection problem for
fortheexpressionnet.Weprovidemoreexplanationsonthis the first stage supervision transfer. Since the fine-tuned face
choice in the next section. network outperforms the pre-trained network on expression
In the second stage, we append the fully connected layers recognition, we hypothesize that there may be interesting
to the trained convolutional layers. The whole network differencesinthenetworkbeforeandafterfine-tuning.These
is jointly learned using the cross-entropy loss, defined as differences might help us understand better which layer is
follows: moresuitabletoguidethetrainingoftheexpressionnetwork.
(cid:88)N (cid:88)M To this end, we first investigate the expression sensitivity
L =− y logyˆ (5)
2 i,j i,j of the neurons in the network, using VGG-16 as a working
i=1j=1 example. For each neuron, the images are ranked by the
Where y is the ground truth for the image, and yˆ is maximum response values. Then the top K (K = 100
i,j i,j
the predicated label. The complete training algorithm is in our experiments) images are binned according to the
illustrated in Fig. 2. expression labels. We compute the entropy for the neuron
Our expression net consists of five convolutional layers, x as H(x) =
−(cid:80)n
p(i)logp(i), where p(i) denotes the
i=1
eachfollowedbyanon-linearactivationfunction(ReLU)and histogram count for bin i and n denotes the number of
a max-pooling layer. The kernel size of all the convolutional quantized label bins (we normalize the histogram to sumTABLEI
THENUMBEROFLOWEXPRESSIVESCORENEURONSFORPRE-TRAINED
NETWORKANDFINE-TUNEDNETWORK
Model Pool4 Pool5 FC6 FC7
Pre-trained(CK) 7763 2011 338 248
Fine-tuned(CK) -57 +511 +658 +610
Pre-trained(Oulu-CASIA) 3009 605 48 33
Fine-tuned(Oulu-CASIA) +194 +895 +952 +1086
TABLEII
THENUMBEROFIMAGESFORDIFFERENTEXPRESSIONCLASSES
An Co Di Fe Ha Sa Su Ne Total
CK+ 135 54 177 75 147 84 249 327 1308 Fig.4. Visualizesseveralneuronsinthetophiddenlayerofourmodelon
Oulu-CASIA 240 240 240 240 240 240 1444 CK+dataset.
TFD 437 457 424 758 441 459 1202 4178
SFEW 255 75 124 256 234 150 228 1322
to 1). If the neuron has a low entropy, then it should
be more expression sensitive since its label distribution is
peaky.Tovalidateourassumption,wehistogramtheentropy
for pool4, pool5, FC6 and FC7 layers. In Fig. 3, it is
interesting to notice that as the layer goes deeper, more low
entropy neurons start to emerge in the fine-tuned network
compared with the pre-trained network. This phenomenon is
particularly obvious in the fully-connected layers, which are
often treated as discriminative features. While for pool4, the
distribution does not change too much.
Fig.5. Visualizesseveralneuronsinthetophiddenlayerofourmodelon
Since these low entropy neurons indicate layer discrimi- Oulu-CASIAdataset.
nativeness, we next compute the number of low expressive
score (LES) neurons for each layer (here low expressive For network training, in the first stage, the regression loss
scoreistheentropylowerthantheminimumaverageentropy is very large. So we start with a very small learning rate 1e-
score among the four selected layers). In Table I., we find 7,anddecreaseitafter100epochs.Thetotaltrainingepochs
that in comparison with the pre-trained network, the LES for this stage is 300. We also try gradient clipping, and find
neurons increase dramatically in the fine-tuned network, that though it enables us to use a bigger learning rate, the
especiallystartingfrompool5layer.Moreover,convolutional results are not better compared to when a small learning
layershavealargernumberoftheseneuronsthanFClayers. rate was used. In the second stage, the fully connected layer
These results suggest that maybe late middle layer, such as is randomly initialized from a Gaussian distribution, and
pool5, is a good tradeoff between supervision richness and the convolutional layers are initialized from the first stage.
representation discriminativeness. The learning rate is 1e-4, and decreased by 0.1 after 20
epochs.Wetrainitfor50epochsintotal.StochasticGradient
IV. EXPERIMENTS
Descent (SGD) is adopted as the optimization algorithm.
Wevalidatetheeffectivenessofourmethodonfourwidely For testing, a single center crop with size 224 × 224 is
used databases: CK+ [27], Oulu-CASIA [28], Toronto Face used. The settings are same for all the experiments. We
Database (TFD) [29] and Static Facial Expression in the use the face net from [33], which is trained on 2.6M face
Wild (SFEW) [30]. The numbers of images for different images. All the experiments are performed using the deep
expressions are shown in Table. II. In the following, we learningframeworkCaffe[34].Uponpublication,thetrained
reference our method FaceNet2ExpNet as FN2EN. expression models will be made publicly available.
A. Implementation B. Neuron Visualization
We apply the Viola Jones [31] face detector and In- We first show that the model trained with our algorithm
traFace [32] for face detection and landmark detection. The captures the semantic concepts related to facial expression
faces are normalized, cropped, and resized to 256×256. We very well. Given a hidden neuron, the face images that
utilizeconventionaldataaugmentationintheformofrandom obtain high response are averaged. We visualize these mean
sampling and horizontal flipping. The min-batch size is 64, images for several neurons in Fig. 4 and Fig. 5 on CK+ and
the momentum is fixed to be 0.9 and the dropout is set at Oulu-CASIA, respectively. Human can easily assign each
0.5. neuron with a semantic concept it measures (i.e. the textFig. 3. Histograms of neuron entropy scores from four different layers for pre-trained network (red) and fine-tuned network (blue). The X axis is the
entropyvalueandtheYaxisisthenumberofneurons.ThefirstrowisonCK+dataset,whilethesecondrowisonOulu-CASIAdataset.
TABLEIII
in black). For example, the neuron 11 in the first column
THEAVERAGEACCURACYONCK+DATASET
in Fig. 4 corresponds to “Anger”, and the neuron 53 in Fig.
5 represents “Happy”. Interestingly, the high-level concepts
Method AverageAccuracy #Exp.Classes
learned by the neurons across the two datasets are very
CSPL[16] 89.9% SixClasses
consistent. AdaGabor[35] 93.3%
LBPSVM[36] 95.1%
3DCNN-DAP[21] 92.4%
BDBN[19] 96.7%
STM-ExpLet[20] 94.2%
DTAGN[22] 97.3%
Inception[23] 93.2%
LOMo[37] 95.1%
PPDN[9] 97.3%
FN2EN 98.6%
AUDN[18] 92.1% EightClasses
TrainFromScratch(BN) 88.7%
VGGFine-Tune(baseline) 89.9%
FN2EN 96.8%
In Table III, we compare our approach with both tradi-
tional and deep learning-based methods in terms of average
accuracy. We consider the fine-tuned VGG-16 face net as
our baseline. To further show the superiority of our method,
we also include the results on training from scratch with
batch normalization. The network architecture is same as
FNEN.Thefirstblockshowstheresultsforsixclasses,while
the second block shows the results for eight classes, includ-
Fig. 6. Confusion Matrix of CK+ for the Eight Classes problem. The
darkerthecolor,thehighertheaccuracy. ing both contempt and neutral expressions. Among them,
3DCNN-DAP [21], STM-ExpLet [20] and DTAGN [22]
C. CK+ are image-sequence based methods, while others are image-
CK+ consists of 529 videos from 123 subjects, 327 of based. For both cases, our method significantly outperforms
them annotated with eight expression labels. Each video all others, achieving 98.6% vs the pervious best of 97.3%
starts with a neutral expression, and reaches the peak in the for six classes, and 96.8% vs 92.1% for eight classes.
last frame. As in other works [20], we extract the last three Because of the high accuracy on the six class problem,
frames and the first frame of each video to compose our here we only show the confusion matrix for eight class
image-based CK+ database. The total number of the images problem. From Fig. 6 we can see that both disgust and fear
is1308,whichissplitinto10folds.Thesubjectsaredivided expressions are perfectly classified, while contempt is the
into ten groups by ID in ascending order. mostdifficulttoclassify.ItisbecausethisexpressionhastheTABLEIV TABLEV
THEAVERAGEACCURACYONOULU-CASDATASET THEAVERAGEACCURACYONTFDDATASET
Method AverageAccuracy Method AverageAccuracy
HOG3D[38] 70.63% Gabor+PCA[40] 80.2%
AdaLBP[28] 73.54% DeepmPoT[41] 82.4%
Atlases[39] 75.52% CDA+CCA[42] 85.0%
STM-ExpLet[20] 74.59% disRBM[43] 85.4%
DTAGN[22] 81.46% bootstrap-recon[44] 86.8%
LOMo[37] 82.10% TrainFromScratch(BN) 82.5%
PPDN[9] 84.59% VGGFine-Tune(baseline) 86.7%
TrainFromScratch(BN) 76.87% FN2EN 88.9%
VGGFine-Tune(baseline) 83.26%
FN2EN 87.71%
wellinrecognizingfearandhappy,whileangryisthehardest
expression, which is mostly confused with disgust.
least number of training images, and the way people show it
is very subtle. Surprisingly, from the visualization in Fig. 1,
E. TFD
thenetworkisstillabletocapturethespecialityofcontempt:
the conner of the mouth is pulled up. This demonstrates the The TFD is the largest expression dataset so far, which
effectiveness of our training method. is comprised of images from many different sources. It
contains4178images,eachofwhichisassignedoneofseven
D. Oulu-CAS VIS expression labels. The images are divided into 5 separate
folds,eachcontainingtrain,validandtestpartitions.Wetrain
Oulu-CASIA has 480 image sequences taken under Dark,
our networks using the training set and report the average
Strong, Weak illumination conditions. In this experiment,
results over five folds on the test sets.
onlyvideoswithStrongconditioncapturedbyaVIScamera
Table V summarizes our TFD results. As we can see, the
are used. There are 80 subjects and six expressions in total.
fine-tuned VGG face is a fairly strong baseline, which is
Similar to CK+, the first frame is always neutral while the
almost on par with the current state-of-the-art, 86.7% vs
lastframehasthepeakexpression.Onlythelastthreeframes
86.8%. Our method performs the best, significantly outper-
are used, and the total number of images is 1440. A ten-
forming bootstrap-recon [44] by 2%. From the confusion
fold cross validation is performed, and the split is subject
matrix, we find that fear has the lowest recognition rate and
independent.
is easy to be confused with surprise. When inspecting the
dataset, we find the images from the two expressions indeed
have very similar facial appearances: mouth and eyes are
wide open.
Fig.7. ConfusionMatrixofOulu-CASIA.Thedarkerthecolor,thehigher
theaccuracy.
Table IV reports the results of average accuracy for the
different approaches. As can be seen, our method achieves
substantialimprovementsoverthepreviousbestperformance
Fig. 8. Confusion Matrix of TFD. The darker the color, the higher the
achieved by PPDN [9], with a gain of 3.1%. The confusion
accuracy.
matrix is shown in Fig. 7. The proposed method performsTABLEVI
V. COMPUTATIONALSPEEDANALYSIS
THEAVERAGEACCURACYONSFEWDATASET
Compared with networks adopted in previous works [9],
[23], [25], AlexNet [47] or VGG-M [48], the size of our
Method AverageAccuracy ExtraTrainData network is fairly small. The number of parameters is 11M
AUDN[18] 26.14% None vs. VGG-16 baseline 138M. For testing, our approach takes
STM-ExpLet[20] 31.73%
only 3ms per image using a single Titan X GPU.
Inception[23] 47.70%
MappedLBP[8] 41.92%
TrainFromScratch(BN) 39.55% VI. CONCLUSIONSANDFUTUREWORKS
VGGFine-Tune(baseline) 41.23%
FN2EN 48.19% In this paper, we present FaceNet2ExpNet, a novel two-
TransferLearning[25] 48.50% FER2013 stage training algorithm for expression recognition. In the
MultipleDeepNetwork[24] 52.29%
firststage,weproposeaprobabilisticdistributionfunctionto
FN2EN 55.15%
model the high level neuron response based on already fine-
tunedfacenet,therebyleadingtofeaturelevelregularization
F. SFEW that exploits the rich face information in the face net. In the
secondstage,weperformlabelsupervisiontoboostthefinal
Different from the previous three datasets, SFEW is
discriminative capability. As a result, FaceNet2ExpNet im-
targeted for unconstrained expression recognition. So the
proves visual feature representation and outperforms various
images are all extracted from films clips, and labeled with
state-of-the-art methods on four public datasets. In future,
seven expressions. The poses are large, and the expression
we plan to apply this training method to other domains with
ismuchmoredifficulttorecognize.Furthermore,ithasonly
small datasets.
891 training images. Because we do not have access to the
test data, here we report the results on the validation data.
VII. ACKNOWLEDGMENTS
InTableVI,wedividethemethodsintotwoblocks,where
the first block only uses the training images from SFEW, This research is based upon work supported by the Office
while the second block utilizes FER2013 [45] as additional of the Director of National Intelligence (ODNI), Intelli-
training data. For both settings, our method achieves best gence Advanced Research Projects Activity (IARPA), via
recognition rates. Especially with more training data, we IARPA R&D Contract No. 2014-14071600012. The views
surpass Multiple Deep Network Learning [24] by almost and conclusions contained herein are those of the authors
3%, which is the runner-up in EmotiW 2015. We do not and should not be interpreted as necessarily representing the
compare the result with the winner [46] since they use 216 officialpoliciesorendorsements,eitherexpressedorimplied,
deep CNNs to get 56.40%, while we only use a single CNN of the ODNI, IARPA, or the U.S. Government. The U.S.
(1.25% higher than our method). From the confusion matrix Governmentisauthorizedtoreproduceanddistributereprints
Fig. 9, we can see the accuracy for fear is much lower than for Governmental purposes notwithstanding any copyright
other expressions. This is also observed in other works [25]. annotation thereon.
REFERENCES
[1] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,
“ImageNet:ALarge-ScaleHierarchicalImageDatabase,”inCVPR09,
2009.
[2] B.Zhou,A.Lapedriza,J.Xiao,A.Torralba,andA.Oliva,“Learning
deepfeaturesforscenerecognitionusingplacesdatabase,”inAdvances
inneuralinformationprocessingsystems,pp.487–495,2014.
[3] L. Yang, P. Luo, C. Change Loy, and X. Tang, “A large-scale car
datasetforfine-grainedcategorizationandverification,”inProceedings
oftheIEEEConferenceonComputerVisionandPatternRecognition,
pp.3973–3981,2015.
[4] I.Kemelmacher-Shlizerman,S.Seitz,D.Miller,andE.Brossard,“The
megafacebenchmark:1millionfacesforrecognitionatscale,”arXiv
preprintarXiv:1512.00596,2015.
[5] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and
R.Salakhutdinov,“Dropout:asimplewaytopreventneuralnetworks
from overfitting.,” Journal of Machine Learning Research, vol. 15,
no.1,pp.1929–1958,2014.
[6] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep
networktrainingbyreducinginternalcovariateshift,”arXivpreprint
arXiv:1502.03167,2015.
[7] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature
hierarchiesforaccurateobjectdetectionandsemanticsegmentation,”
inProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pp.580–587,2014.
[8] G.LeviandT.Hassner,“Emotionrecognitioninthewildviaconvolu-
Fig.9. ConfusionMatrixofSFEW.Thedarkerthecolor,thehigherthe tionalneuralnetworksandmappedbinarypatterns,”inProceedingsof
accuracy. the2015ACMonInternationalConferenceonMultimodalInteraction,
pp.503–510,ACM,2015.[9] X.Zhao,X.Liang,L.Liu,T.Li,N.Vasconcelos,andS.Yan,“Peak- 11thIEEEInternationalConferenceandWorkshopson,vol.1,pp.1–
piloteddeepnetworkforfacialexpressionrecognition,”arXivpreprint 8,IEEE,2015.
arXiv:1607.06997,2016. [33] O.M.Parkhi,A.Vedaldi,andA.Zisserman,“Deepfacerecognition,”
[10] D. Yi, Z. Lei, S. Liao, and S. Z. Li, “Learning face representation inBMVC,2015.
fromscratch,”arXivpreprintarXiv:1411.7923,2014. [34] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
[11] Deepdraw.,“Deepdrawongithub.com/auduno/deepdraw,” S.Guadarrama,andT.Darrell,“Caffe:Convolutionalarchitecturefor
[12] S. Vittayakorn, T. Umeda, K. Murasaki, K. Sudo, T. Okatani, and fastfeatureembedding,”inProceedingsofthe22ndACMinternational
K.Yamaguchi,“Automaticattributediscoverywithneuralactivations,” conferenceonMultimedia,pp.675–678,ACM,2014.
arXivpreprintarXiv:1607.07262,2016. [35] M. S. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. Fasel, and
[13] K. Simonyan and A. Zisserman, “Very deep convolutional networks J. Movellan, “Recognizing facial expression: machine learning and
forlarge-scaleimagerecognition,”CoRR,vol.abs/1409.1556,2014. applicationtospontaneousbehavior,”in2005IEEEComputerSociety
ConferenceonComputerVisionandPatternRecognition(CVPR’05),
[14] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a
vol.2,pp.568–573,IEEE,2005.
neuralnetwork,”arXivpreprintarXiv:1503.02531,2015.
[36] X. Feng, M. Pietika¨inen, and A. Hadid, “Facial expression recogni-
[15] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and
tion based on local binary patterns,” Pattern Recognition and Image
Y. Bengio, “Fitnets: Hints for thin deep nets,” arXiv preprint
Analysis,vol.17,no.4,pp.592–598,2007.
arXiv:1412.6550,2014.
[37] K. Sikka, G. Sharma, and M. Bartlett, “Lomo: Latent ordinal model
[16] L. Zhong, Q. Liu, P. Yang, B. Liu, J. Huang, and D. N. Metaxas,
forfacialanalysisinvideos,”arXivpreprintarXiv:1604.01500,2016.
“Learningactivefacialpatchesforexpressionanalysis,”inComputer
[38] A.Klaser,M.Marszałek,andC.Schmid,“Aspatio-temporaldescrip-
Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on,
torbasedon3d-gradients,”inBMVC2008-19thBritishMachineVision
pp.2562–2569,IEEE,2012.
Conference,pp.275–1,BritishMachineVisionAssociation,2008.
[17] P. Liu, J. T. Zhou, I. W.-H. Tsang, Z. Meng, S. Han, and Y. Tong,
[39] Y. Guo, G. Zhao, and M. Pietika¨inen, “Dynamic facial expression
“Featuredisentanglingmachine-anovelapproachoffeatureselection
recognition using longitudinal facial expression atlases,” in ECCV,
anddisentanglinginfacialexpressionanalysis,”inEuropeanConfer-
pp.631–644,2012.
enceonComputerVision,pp.151–166,Springer,2014.
[40] M.N.Dailey,G.W.Cottrell,C.Padgett,andR.Adolphs,“Empath:
[18] M. Liu, S. Li, S. Shan, and X. Chen, “Au-aware deep networks A neural network that categorizes facial expressions,” Journal of
for facial expression recognition,” in Automatic Face and Gesture cognitiveneuroscience,vol.14,no.8,pp.1158–1173,2002.
Recognition (FG), 2013 10th IEEE International Conference and
[41] J.Susskind,V.Mnih,G.Hinton,etal.,“Ondeepgenerativemodels
Workshopson,pp.1–6,IEEE,2013.
with applications to recognition,” in Computer Vision and Pattern
[19] P.Liu,S.Han,Z.Meng,andY.Tong,“Facialexpressionrecognition Recognition (CVPR), 2011 IEEE Conference on, pp. 2857–2864,
via a boosted deep belief network,” in Proceedings of the IEEE IEEE,2011.
Conference on Computer Vision and Pattern Recognition, pp. 1805– [42] S. Rifai, Y. Bengio, A. Courville, P. Vincent, and M. Mirza, “Dis-
1812,2014. entangling factors of variation for facial expression recognition,” in
[20] M.Liu,S.Shan,R.Wang,andX.Chen,“Learningexpressionletson European Conference on Computer Vision, pp. 808–822, Springer,
spatio-temporal manifold for dynamic facial expression recognition,” 2012.
in Proceedings of the IEEE Conference on Computer Vision and [43] S. Reed, K. Sohn, Y. Zhang, and H. Lee, “Learning to disentangle
PatternRecognition,pp.1749–1756,2014. factors of variation with manifold interaction,” in Proceedings of
[21] M. Liu, S. Li, S. Shan, R. Wang, and X. Chen, “Deeply learning the 31st International Conference on Machine Learning (ICML-14),
deformablefacialactionpartsmodelfordynamicexpressionanalysis,” pp.1431–1439,2014.
inACCV2014,pp.143–157,2014. [44] S. Reed, H. Lee, D. Anguelov, C. Szegedy, D. Erhan, and A. Ra-
[22] H.Jung,S.Lee,S.Park,I.Lee,C.Ahn,andJ.Kim,“Deeptemporal binovich, “Training deep neural networks on noisy labels with boot-
appearance-geometry network for facial expression recognition,” in strapping,”arXivpreprintarXiv:1412.6596,2014.
ICCV,2015. [45] I. J. Goodfellow, D. Erhan, P. L. Carrier, A. Courville, M. Mirza,
[23] A. Mollahosseini, D. Chan, and M. H. Mahoor, “Going deeper in B. Hamner, W. Cukierski, Y. Tang, D. Thaler, D.-H. Lee, et al.,
facial expression recognition using deep neural networks,” in 2016 “Challenges in representation learning: A report on three machine
IEEEWinterConferenceonApplicationsofComputerVision(WACV), learningcontests,”inInternationalConferenceonNeuralInformation
pp.1–10,IEEE,2016. Processing,pp.117–124,Springer,2013.
[24] Z.YuandC.Zhang,“Imagebasedstaticfacialexpressionrecognition [46] B.-K.Kim,J.Roh,S.-Y.Dong,andS.-Y.Lee,“Hierarchicalcommittee
withmultipledeepnetworklearning,”inProceedingsofthe2015ACM of deep convolutional neural networks for robust facial expression
onInternationalConferenceonMultimodalInteraction,pp.435–442, recognition,”JournalonMultimodalUserInterfaces,pp.1–17,2016.
ACM,2015. [47] A.Krizhevsky,I.Sutskever,andG.E.Hinton,“Imagenetclassification
[25] H.-W.Ng,V.D.Nguyen,V.Vonikakis,andS.Winkler,“Deeplearning with deep convolutional neural networks,” in Advances in neural
for emotion recognition on small datasets using transfer learning,” informationprocessingsystems,pp.1097–1105,2012.
in Proceedings of the 2015 ACM on International Conference on [48] K.Chatfield,K.Simonyan,A.Vedaldi,andA.Zisserman,“Returnof
MultimodalInteraction,pp.443–449,ACM,2015. the devil in the details: Delving deep into convolutional nets,” arXiv
[26] L.Xie,L.Zheng,J.Wang,A.Yuille,andQ.Tian,“Interactive:Inter- preprintarXiv:1405.3531,2014.
layeractivenesspropagation,”arXivpreprintarXiv:1605.00052,2016.
[27] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and
I. Matthews, “The extended cohn-kanade dataset (ck+): A complete
dataset for action unit and emotion-specified expression,” in CVPR
Workshop,pp.94–101,2010.
[28] G. Zhao, X. Huang, M. Taini, S. Z. Li, and M. Pietika¨inen, “Facial
expression recognition from near-infrared videos,” Image and Vision
Computing,vol.29,no.9,pp.607–619,2011.
[29] J.M.Susskind,A.K.Anderson,andG.E.Hinton,“Thetorontoface
database,” Department of Computer Science, University of Toronto,
Toronto,ON,Canada,Tech.Rep,vol.3,2010.
[30] A. Dhall, O. Ramana Murthy, R. Goecke, J. Joshi, and T. Gedeon,
“Videoandimagebasedemotionrecognitionchallengesinthewild:
Emotiw 2015,” in Proceedings of the 2015 ACM on International
ConferenceonMultimodalInteraction,pp.423–426,ACM,2015.
[31] P. Viola and M. J. Jones, “Robust real-time face detection,” Interna-
tionaljournalofcomputervision,vol.57,no.2,pp.137–154,2004.
[32] F.DelaTorre,W.-S.Chu,X.Xiong,F.Vicente,X.Ding,andJ.Cohn,
“Intraface,” in Automatic Face and Gesture Recognition (FG), 2015"
107,109,Faces of pain: automated measurement of spontaneousallfacial expressions of genuine and posed pain,"['GC Littlewort', 'MS Bartlett', 'K Lee']",2007,232,Toronto Face Database,machine learning,"machine learning approach, previously used successfully to categorize basic emotional facial   Here we applied machine learning on a 20-channel output stream of facial action detectors",No DOI,… of the 9th international conference on …,https://www.researchgate.net/publication/221052396_Faces_of_pain_automated_measurement_of_spontaneousallfacial_expressions_of_genuine_and_posed_pain,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,researchgate.net,
108,110,Facial Affect``In-The-Wild,"['S Zafeiriou', 'A Papaioannou', 'I Kotsia']",2016,122,"Acted Facial Expressions In The Wild, Expression in-the-Wild, Static Facial Expression in the Wild","CNN, FER, classification, classifier, deep learning, facial expression recognition, machine learning, neural network","of compound expressions [20]). A recent survey on facial expression recognition can be   In particular, the network exploits the fact that facial expressions can be decomposed to FAU.",No DOI,… Pattern Recognition …,https://openaccess.thecvf.com/content_cvpr_2016_workshops/w28/papers/Zafeiriou_Facial_Affect_In-The-Wild_CVPR_2016_paper.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,thecvf.com,
109,111,Facial emotion detection using deep learning,"['A Jaiswal', 'AK Raju', 'S Deb']",2020,150,Japanese Female Facial Expression,deep learning,"basically three main steps: face detection, features extraction,  a convolutional neural networks  (CNN) based deep learning  the Japanese Female Face Expression (JAFFE) [15], Facial",No DOI,2020 international conference for …,https://ieeexplore.ieee.org/document/9154121,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
110,112,Facial emotion processing in borderline personality disorder: a systematic review and meta-analysis,"['AE Mitchell', 'GL Dickens', 'MM Picchioni']",2014,220,Karolinska Directed Emotional Faces,FER,A body of work has developed over the last 20 years that explores facial emotion perception  in Borderline Personality Disorder (BPD). We identified 25 behavioural and functional,No DOI,Neuropsychology review,https://pubmed.ncbi.nlm.nih.gov/24574071/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,nih.gov,
111,113,Facial emotion recognition using convolutional neural networks (FERC),['N Mehendale'],2020,328,Extended Cohn-Kanade,"FER, neural network","FERC was extensively tested with more than 750K images using extended Cohn–Kanade  expression, Caltech faces, CMU and NIST datasets. We expect the FERC emotion detection",No DOI,SN Applied Sciences,https://link.springer.com/article/10.1007/s42452-020-2234-1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,springer.com,
112,114,Facial expression analysis under partial occlusion: A survey,"['L Zhang', 'B Verma', 'D Tjondronegoro']",2018,121,Toronto Face Database,facial expression recognition,"While face and facial expression recognition systems  database includes four typical complex  facial expressions: smile while hands obscure the face, anger while hand obscure the face",No DOI,ACM Computing Surveys …,https://arxiv.org/abs/1802.08784,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"Facial Expression Analysis under Partial Occlusion: A
*1
Survey
Ligang Zhang1, Brijesh Verma1, Dian Tjondronegoro2 and Vinod Chandran3
1Central Queensland University
2Southern Cross University
3Queensland University of Technology
Automatic machine-based Facial Expression Analysis (FEA) has made substantial progress in the past few decades driven
by its importance for applications in psychology, security, health, entertainment and human computer interaction. The
vast majority of completed FEA studies are based on non-occluded faces collected in a controlled laboratory environment.
Automatic expression recognition tolerant to partial occlusion remains less understood, particularly in real-world
scenarios. In recent years, efforts investigating techniques to handle partial occlusion for FEA have seen an increase. The
context is right for a comprehensive perspective of these developments and the state of the art from this perspective. This
survey provides such a comprehensive review of recent advances in dataset creation, algorithm development, and
investigations of the effects of occlusion critical for robust performance in FEA systems. It outlines existing challenges in
overcoming partial occlusion and discusses possible opportunities in advancing the technology. To the best of our
knowledge, it is the first FEA survey dedicated to occlusion and aimed at promoting better informed and benchmarked
future work.
Categories and Subject Descriptors: [General and Reference]: Surveys and Overviews; [Artificial Intelligence]:
Computer Vision – Computer Vision Tasks – Scene Understanding
General Terms: Experimentation, Performance, Algorithms
Additional Key Words and Phrases: Facial expression analysis, emotion recognition, partial occlusion, overview, survey
1 Introduction
Facial expressions of emotion are a major channel in daily human-human communication. Machine-based automatic
analysis of expression from human faces is an important part of artificial intelligence capabilities. It has potential
applications in various fields, such as intelligent tutoring systems, emotionally sensitive robots, driver fatigue monitoring,
personalized service provision, interactive game design, and emotion based data retrieval, categorization and management.
However, automatic Facial Expression Analysis (FEA) in an unconstrained real-life situation is still difficult. It encounters
a variety of challenges arising factors such as occlusion, face pose variations, illumination changes, head motion, and
differences in the age, gender, skin color and culture of the subject between training and testing phases of a system. An
ideal FEA system should be able to handle all these challenges. While face and facial expression recognition systems have
addressed most of these factors systematically, occlusion is often overlooked and assumed to be taken care of by
controlled acquisition – which is only true for laboratory or prescribed acquisition conditions.
Partial occlusion presented in the face is one of the major obstacles for accurate FEA in real-world conditions. In real-
life situations, there is a high likelihood that some parts of the face become obstructed by sunglasses, a hat, a scarf, hands
moving over the mouth, a moustache or hair, etc. Occlusion can substantially change the visual appearance of the face and
severely deteriorate the performance of FEA systems. The presence of occlusion increases the difficulty of extracting
discriminative features from occluded facial parts due to inaccurate feature location, imprecise face alignment or face
registration error [Ekenel and Stiefelhagen 2009]. It also introduces noise and outliers to extracted features leading to
higher intra-expression variations. An FEA system with occlusion handling capacity aims to achieve accurate emotion
recognition even when a portion of the face is occluded. The system can be useful in various real-life scenarios,
particularly those with frequently occurring occlusion, such as students wearing glasses in online tutoring, patients
wearing medical masks in medical diagnosis, and players with pose variations in game entertainment.
Although the significance of handling facial occlusion has been widely recognized in the research community for a
long time, there has been a significant delay in the development of relevant algorithms and systems. In earlier surveys on
FEA [Pantic and Rothkrantz 2000], [Fasel and Luettin 2003], no study reported specifically designed techniques to
overcome facial occlusion. This situation remained the same in the survey on audio-visual affect recognition in 2009 [Zeng
et al. 2009], which concluded that most human affect recognizers are evaluated using non-occluded facial data and that
1* This material is based on work supported under Australian Research Council's Linkage Project (LP140100939).
Author’s address: Ligang Zhang and Brijesh Verma, Centre for Intelligent Systems, Central Queensland University, Brisbane,
QLD, Australia 4000; emails: {l.zhang, b.verma}@cqu.edu.au; Dian Tjondronegoro,School of Business and Tourism, Southern
Cross University, Bilinga, QLD, Australia 4225; email: dian.tjondronegoro@scu.edu.au; Vinod Chandran, Science and Engineering
Faculty, Queensland University of Technology, Brisbane, QLD, Australia 4000; email: v.chandran@qut.edu.au.
1developing methods that are robust to occlusion is an important issue that is yet to be addressed. In recent surveys on FEA
in 2012 [Bettadapura 2012], 2015 [Owusu et al. 2015], [Sariyanidi et al. 2015] and 2016 [Corneanu et al. 2016], several
papers were cited that exploited the most informative facial parts or developed automatic systems for FEA. Current
literature still lacks a comprehensive and focused survey of existing efforts in overcoming partial occlusion for FEA.
This paper aims to bridge this gap. It is expected that it can serve as a good reference for developing techniques toward
robust FEA in the presence of occlusion. The outline of this paper follows the concept map illustrated in Fig.1, and the
remainder of the paper is organized as follows: Section 2 introduces background knowledge about FEA under partial
occlusion, including its brief history, methods for representing emotions, and the major types and characteristics of facial
occlusion. Section 3 examines related databases for performance evaluation of FEA systems. Section 4 reviews existing
FEA approaches that can automatically recognize emotional states from occluded faces. We identify top five techniques
that can be used as baselines for performance evaluations of future algorithms. Section 5 summarizes investigations on the
effect of occlusion on the performance of recognizing facial expressions based on computer vision or human perception.
In Section 6, we present discussions about existing challenges and possible opportunities, covering the aspects of data
creation, occlusion detection, feature extraction, context information, and multiple modalities and disciplines. Finally,
Section 7 draws some conclusions.
Feature Sparse
HF isE toA ry RepE rm eso et ni to an ti on Re Aco pn ps rt oru acc htion Re Apr pe ps re on ata ct hion VisC ioo nm Ap nu ate lyr sis CrD eaa tt ia o n O Dc ec telu cs tii oo nn Context
Background Related Facial Expression Investigations on Challenges &
Information Databases Analysis Approaches Effect of Occlusion Opportunities
Major Type Main Characteristics Sub-region Model Based Human Perception Feature Multiple Multiple
of Occlusion of Occlusion Based Approach Approach Analysis ExtractionModalities Disciplines
Fig. 1. Overview of the structure of the survey on FEA under partial occlusion.
2 Background
2.1 Brief History of Facial Expression Analysis (FEA) under Occlusion
Research on FEA can be tracked back to the study of physiognomy in the 4th century BC, which assessed the character
or personality of a person from his/her outer appearance, primarily the face. This study was later extensively extended to
explore the relationship between facial expressions and the movements of head muscles in the 17th century [Bettadapura
2012]. Since then, one pioneering work that has significantly impacted today’s automatic FEA systems was done by
Charles Darwin, who provided evidence to the existence of some basic emotions universally across cultures and ethnics.
Another work was done by Ekman and his colleagues [Ekman 1978], who designed the Facial Action Coding System
(FACS) to encode the states of facial expressions using facial Action Units (AUs). Until the 1980s, most FEA work was
conducted by philosophers and psychologists (see a review of early work [Keltner et al. 2003]). Arguably, Kenade [1973]
and Suwa et al. [1978] are two earliest investigators on recognizing facial expressions using computer technologies, and
they developed computer programs to extract facial points for analyzing human faces and representing facial expressions.
After relatively slow progress in the 1970s and 1980s, the 90s witnessed increased development of automatic FEA systems,
moving from analyzing deliberately posed prototypical emotions of near-frontal faces collected in controlled laboratory
settings to spontaneously evoked emotions collected from non-constrained settings (see reviews [Pantic and Rothkrantz
2000],[Fasel and Luettin 2003],[Zeng, et al. 2009],[Bettadapura 2012]).
On the recognition of the big impact of facial occlusion on FEA, since the 1920s, a host of psychosocial studies (e.g.
Ruckmick [Ruckmick 1921], Dunlap [Dunlap 1927], Boucher and Ekman [Boucher and Ekman 1975]) have investigated the
facial parts that are most important for human perception and recognition of facial expressions from partially occluded
faces. However, the first machine system for FEA in the presence of occlusion was presented in 2001 by Bourel et al.
[Bourel 2001], who recovered geometric facial points for overcoming occlusion in regions of the upper face, mouth, and
left/right half of the face. Inspired by this work, most initial efforts [Bourel et al. 2002], [Towner and Slater 2007] focused
on the recovering of geometric features from occluded faces in static images and the classification of facial expressions
using a single global classifier. More recent advancements [Xia et al. 2009], [Cotter 2011], [Liu et al. 2014c] have shifted to
the adoption of texture features and their combination with geometric features in temporal 2D or 3D video sequences, as
well as the fusion of multiple local classifiers from different facial regions to derive a final classification decision for the
whole face. Kotsia et al. [2008] presented, to our best knowledge, the most comprehensive analysis on the impact of facial
occlusion on the recognition of six basic emotions based on FER systems. The results were found to agree in overall with
those from human observers. With the popularity of deep learning techniques, recent studies [Cheng et al. 2014], [Tősér et
al. 2016], [Batista et al. 2017] have focused on the use of deep neural networks to directly perform FEA on occluded facial
images without involving steps of occlusion detection, hand-engineering feature extraction, and classifier design.
2Although more types of occlusion and datasets have been included in recent studies, most of existing studies are primarily
based on a limited number of artificially generated types of occlusion and the present progress is relatively slow.
2.2 Methods for Representing Facial Expressions
Representing facial expressions is a prerequisite for evaluating the effectiveness of FEA systems, particularly in the
presence of occlusion. Facial expressions are generally represented using two methods in exiting studies: message based
and movement of facial components based.
The message based method can be further divided into discrete categorical and continuous dimensional methods. The
discrete categorical method is perhaps the most long-standing and widely used way for describing facial expressions by
psychologists. In this method, an expression is assigned to one of pre-defined prototypical categories, including six basic
emotional states - anger (AN), disgust (DI), fear (FE), happiness (HA), sadness (SA), and surprise (SU) [Ekman 1994], and
non-basic emotional states such as depression, agreement, distress and disappointment . This intuitive theory creates a
convenient way of representing observed facial expressions in daily lives using a list of emotion words with concrete
meaning, which is highly consistent with human understanding. However, the theory can represent only a small portion
of possible complicated and mixed emotions in natural communication conditions. Studies [Maja et al. 2005] have shown
that “pure expressions of prototypical emotions are less frequently elicited and blends of emotional displays are often
shown by humans in real-life situations”.
The continuous dimensional method was derived from the field of psychology [Russell 1980]. It describes facial
expressions using continuous axes in a multiple dimensional space and represents each expression as a point or a region
in the space. The most commonly used spaces are composed of two- and three- dimensional representations, such as
activation or arousal, valence, power, and expectancy. The advantage of dimensional spaces over discrete categories and
AUs lies in the use of a set of continuous axis values to represent a large amount of different types of emotions, including
those naturalistic non-prototypical ones that often occur in realistic conditions. They can provide useful insights into the
intensity of emotions, as well as the similarity and contrast between categorical emotions. However, as the dimensional
space is not intuitive, it requires specially trained annotators in emotion labeling using continuous axes. In addition, some
emotions become indistinguishable in a limited number of dimensions and it is not straightforward in directly applying
the analysis results into practical applications.
The movement of facial components based method uses the movements of individual facial muscles to encode facial
expression states. Examples of this method include FACS [Ekman 1978], Emotional Facial Action Coding System
(EMFACS), MAXimally discriminative facial movement coding system (MAX) [Izard et al. 1979], and probability-based AU
space [Zhao et al. 2016]. The FACS, which was originally developed by Paul Ekman and his colleagues in 1978, defines a
total of 44 AUs to encode movements of facial muscles. Each AU corresponds to a contraction or relaxation of individual
or multiple facial muscles that can generate a certain facial action with regard to its location and intensity. A revised
version was published in 2002 [Ekman et al. 2002] comprising of 32 AUs. One advantage of AUs is that a combination of
relatively few AUs can effectively represent a large number (e.g., thousands) of expression states and subtle facial signals
such as wink and frown. Thus, AUs are able to represent a wide variety of emotions and are suitable for describing,
modeling and analyzing real-life facial expressions. In addition, AUs are objective descriptors and thus they are
independent of human interpretation, which consequently reduces the subjectivity of emotion labeling. However, the
challenge lies in the difficulty to accurately choose a set of relevant AUs and their combination for arbitrary naturalistic
emotions, and vice versa it is also challenging to translate emotion related AUs into affective meanings. The EMFACS, as a
selective application of FACS scoring, focuses on scoring only facial actions that are likely to have emotional significance.
The MAX was designed to code discrete emotional states such as interest, joy, surprise, contempt, and physical distress or
pain, based on a set of facial movement formulas. Rather than using binarized AU occurrence in FACS, EMFACS and
MAX, the probability-based AU space treats each basic AU as an individual dimension and uses continuous coordinates on
an AU axis to represent the probability of this AU occurring on a face. A hyperplane can be constructed to divide the
space into regions of emotions or affective states. The space has the advantages of not requiring manual labelling of AUs
from skilled experts and being more robust for AU detection in margin areas.
Another category of facial component movement based methods focuses on the recognition of Micro-Expressions
(MEs). MEs are brief, involuntary facial expressions which reveal hidden emotions and are important for understanding
humans’ deceitful behaviours. Unlike general facial expressions, MEs are very short (i.e., 1/25 to 1/3 second), involve
subtle muscle movements and are difficult to control through one’s willpower. It is still a difficult task, even for humans,
to precisely recognize MEs in real-life environments. Although MEs have been given relatively less attention compared to
general expressions, various types of tools and systems have been developed for recognizing MEs, such as the Micro
Expression Training Tool developed by Ekman [Ekman 2003], the MR recognition system [Wu et al. 2011], and the ME
analysis system (MESR) [Li et al. 2017b]. Encouragingly, some systems such as MESR, were reported to outperform
humans in ME recognition. One possible benefit of using MEs for FEA under occlusion is that, even a part of the face is
occluded intentionally by a subject to hide his/her emotions, the true emotional state may still can be automatically
revealed by recognizing those MEs hidden in unoccluded parts of the face.
32.3 Major Types and Characteristics of Facial Occlusion
Due to the complexity and variability of specific environments where the face presents, the types of facial occlusion
that occur may vary significantly. Generally, there are two major types of partial occlusion: systematic and temporary
[Towner and Slater 2007]. Systematic occlusions are caused by the existence of individual facial components (e.g., hair,
mustache, or a scar), or by people wearing adornments (e.g. glasses, clothes, a hat or surgical mask, or mark-ups) as
displayed in Fig. 2 (a). Temporary occlusions arise from a portion of the face being temporarily obscured by other objects
(e.g., people moving across the face or hands covering the face), or from environmental condition changes (e.g., lighting
and shadows), or self-occlusion due to changes in head pose (e.g., out-of-plane pose variations) or temporarily placing
objects in front of the face as shown in Fig. 2 (b). Due to the necessity of constantly interacting with the environment, self-
occlusion might occur more frequently than other types of temporary occlusions in daily lives. Facial occlusion is not
necessarily restricted to be either systematic or temporary; instead, it can be composed of multiple types of occlusion as
shown in Fig.2(c). In special cases as shown in Fig. 2 (d), blurring, pixellation, artificial masks or texts are added
specifically into the face to hide personal identity or provide helpful information.
Fig. 2. Real examples with different types of facial occlusion.
Different from other challenges, such as pose and illumination variations whose main characteristics and the
associated impact can be inferred beforehand, facial occlusion has several distinguishing characteristics that make it
particularly difficult to be handled:
1) Varying type. The type of occlusion may vary significantly dependent on the situation where the face presents.
Unless there is prior knowledge about what type of occlusion is going to occur in a specific context, FEA systems should
consider all possible types, which is very challenging and technically infeasible at least for now.
2) Mixed type. Multiple types of occlusion may co-exist in the face. The existence and prevalence of mixed occlusion
increases the difficulty of handling occlusion and investigating the effect of individual occlusion because the separation of
mixed occlusion is still an unexplored field.
3) Non-fixed location. Most types of occlusion are generally not fixed to a certain location of the face. Although the
location is roughly predictable for some systematic occlusion, such as glasses and a hat, it is still difficult to predict their
precise locations and it is more difficult for temporary occlusion such as hand covering.
4) Varied duration. Different types of occlusion may exist for a different length of duration. Occlusion due to a hand
moving over the face is anticipated to last for only few seconds, while occlusion of sunglasses often exist for the total
duration of the video data. Thus, the duration of occlusion is largely dependent on the nature type of the occlusion.
5) Unpredictable property. Due to the variability of objects leading to occlusion, the visual properties (e.g. shape,
appearance and size) of the resulting occlusion are often unpredictable. For example, there might be big differences in the
visual properties between sunglasses, vision correction glasses, and protective glasses.
6) Local impact. Unlike pose and illumination variations, which often impact the holistic facial region, most types of
occlusion impact only a small portion of the face. This property can be somehow treated as a merit because the effect of
occlusion can be compensated using un-occluded facial parts.
4It should be noted that in specific cases, it might be possible to roughly predict the parameters (e.g. type, location,
shape, appearance and time) of occlusion that is likely to occur. For instance, a person’s face is highly likely to be partially
and temporally occluded by a cup and moving hands when he/she is drinking coffee. Prior knowledge about the occlusion
is critical in developing techniques to overcome its impact.
3 Related Databases
Being able to access public facial expression datasets with well-annotated occlusion is a pre-requisite in evaluating FEA
systems. Many databases [Patil et al. 2015] with facial occlusion were generated for face recognition, but very few have
been created specifically for FEA. This section introduces related public databases (samples shown in Fig. 3) that either are
widely used in existing studies with artificially superimposed occlusion, or can be potentially used for future system
evaluations with naturally occurring occlusion. It is worth mentioning that there are also recently released datasets such
as AFEW [Dhall et al. 2012], QUTFER [Zhang et al. 2014a] and BAUM-1 [Zhalehpour et al. 2016] that contain a certain
level of realistic facial occlusion caused by factors such as pose variations and lighting changes, but as these datasets are
not specifically created for evaluating FEA under occlusion and only limited types of occlusion occur occasionally in few
cases, they are not covered here. Readers are referred to [Zafeiriou et al. 2016] for a survey of FEA databases collected in a
wild environment.
1) The JApanese Female Facial Expression (JAFFE) database [Lyons et al. 1998] is a widely used benchmark dataset for
FEA in early work and it includes 213 images of six basic emotions plus neutral that were posed by 10 Japanese female
subjects. Each subject has three or four non-occluded frontal face images per expression and the face is roughly located at
the center of the images. The images have a resolution of 256×256 pixels and have been rated on seven emotion categories
by 60 subjects. Although the JAFFE database has been used less frequently in recent studies, it is still a popular dataset for
evaluating the effect of artificially superimposed occlusion on the FEA.
2) The Cohn-Kanade (CK) database [Kanade et al. 2000] is a popular comprehensive dataset for facial expression
benchmark tests. It is composed of 486 video sequences collected from 97 subjects with neutral to target displays in each
sequence. The frames have a resolution of 640×480 or 640×490 pixels and are fully FACS coded. Annotation of six basic
expressions has also been provided. The extended version (CK+) [Lucey et al. 2010] includes 593 posed expression
sequences from 123 subjects and 122 spontaneous smile sequences from 66 subjects. Similar to JAFFE, CK and CK+ do not
include occluded faces.
3) The BeiHang University Facial Expression (BHUFE) database [Yu-Li et al. 2006] includes 1,600 color videos for the
frontal and 30-degree profile of 25 facial expressions from 32 college students aged 21 to 25. Each video lasts about 6
seconds and has a frame resolution of 480×640 pixels. The database includes four typical complex facial expressions: smile
while hands obscure the face, anger while hand obscure the face, smile while talking, and anger while talking.
4) The Caltech Occluded Faces in the Wild (COFW) dataset [Burgos-Artizzu et al. 2013] comprises of 1,007 facial
images collected from a variety of real-world sources by four people. It was designed to evaluate the performance of face
landmark algorithms in realistic conditions, which have heavy occlusion and large shape variations that arise from the
differences in expression or head pose, using accessories (e.g. hats and sunglasses), and interacting with objects. There are
substantial variations in the type of occlusion and different degrees of occlusion in the faces. The average face occlusion is
over 23%. The occluded/unoccluded state in all images was hand annotated.
5) The Acted Facial Expressions in the Wild (AFEW) database [Dhall, et al. 2012] is a dynamic temporal facial
expressions data corpus that comprises of a total of 957 audio-visual clips extracted from 37 movies. The clips are searched
by their subtitles using a list of expression keywords such as ‘laughs’, ‘smiles’, ‘scared’, etc. The clips are labeled with six
basic emotions and actor information by human observers. The Static Facial Expressions in the Wild (SFEW) database
[Dhall et al.] is a subset of the AFEW database, and it is composed of static frames selected from AFEW video clips. The
SFEW includes 700 images labelled with six basic emotions. The images have unconstrained facial expressions with
variations in pose, subject age and image resolution, as well as real-life occlusions such as glasses, eye mask, beard and hat.
The AFEW (or SFEW) has been used as a benchmark dataset in the Emotion recognition in the Wild (EmotiW) challenge
from 2013 to 2017 [Dhall et al. 2017].
6) The HAPpy PeoplE Images (HAPPEI) database [Dhall et al. 2013] is created to evaluate the happiness intensity of a
group of people. It includes 4,886 images that are collected from Flickr by searching keywords associated with groups of
people and events, such as ‘party + people’, ‘marriage’, ‘reunion’, ‘bar’, etc. All images have more than one subject and are
annotated with group-level mood intensity (from neutral to thrilled). In addition, 8,500 faces in these images are also
annotated for six intensities of happiness (i.e., neutral, small smile, large smile, small laugh, large laugh and thrilled), and
three intensities of occlusion (i.e., face visible, partial occlusion and high occlusion). The dataset has been used as the
benchmark dataset for group-level emotion recognition in the EmotiW 2016 [Dhall et al. 2016b]. Instead of focusing on
only happiness in the HAPPEI database, the Group Affect database [Dhall et al. 2015b] covers images of a group of people
with three emotion categories of positive, negative, and neutral. It has been used in the EmotiW 2017 [Dhall, et al. 2017].
5Although partial occlusion such as beard, glasses and hat frequently present in some images, no annotation labels of the
occlusion are provided in the database.
7) The ‘in-the-wild’ database [Zafeiriou, et al. 2016] is a recently developed dataset for facial expression analysis in
naturalistic conditions. It comprises of more than 500 video collected from Youtube with people reacting to different
emotional scenarios including performing an activity, a practical joke, a positive surprise, a particular video, etc. The facial
responses have been annotated with arousal and valance values by three rates using the FeelTrack tool. In addition, the
database also includes more than 10,000 facial images from more than 2,000 people that were collected by performing a tag
based search in Google Image using keywords such as anger, feeling, fear, and pain. The facial images were annotated
with 16 AUs by a trained AU coder. This database contains various types of natural occlusion caused by hands, pose
variations, lighting changes, etc., but no ground truth data regarding the presence/state of occlusion was provided.
8) The Bosphorus database [Savran et al. 2008] is a large multi-expression and multi-pose 3D face dataset with different
types of real-life face occlusions. It comprises of 4,652 face scans from 105 subjects mostly aged between 25 and 35. Facial
expressions were encoded using 28 AUs and six basic emotions, and occlusion of the eyes and mouth were formed
naturally by subjects rubbing their eyes, or putting hands over their mouths. Occlusion of eyeglasses was generated by
asking each subject to wear a pair of eyeglasses chosen from a pool of different eyeglasses. The dataset also includes a
small portion of facial images with partial occlusion by long hair.
9) The University of Cambridge 3D (Cam3D) multimodal corpus [Mahmoud et al. 2011] was specifically collected to
analyze hand-over-face gestures. It includes 108 audio/visual segments of spontaneous facial expressions and hand-over-
face gestures from 16 participants with 12 natural mental states, such as thinking, unsure, happy, surprise, anger,
frustrated and confused. The data was captured using three sensors of HD cameras, Microsoft Kinect and microphones,
and has a frame resolution of 640×480 or 720×576 pixels. The emotional state was annotated using crowd-sourcing
techniques, and the hand-over-face gestures were annotated into three states of action, hand shape and facial region
occluded.
10) The University of Milano Bicocca (UMB) 3D database [Colombo et al. 2011] consists of 1,473 2D color images and
3D depth images collected from 143 subjects. Each subject has at least four facial expressions (neutral, smiling, anger and
bored) and occluded faces by scarf, hat or hands in random positions. Most subjects also have partial occlusion arising
from eyeglasses, holding phones, hair, or other miscellaneous objects. 42% of the face area is occluded on average, with the
largest coverage of about 84%. Each acquisition was described by labels such as occluded or non-occluded, occluding
object (if any) and facial expression. In total, there are 578 occluded faces with an image resolution of 640×480 pixels. One
drawback of this database is that all subjects were asked to keep their eyes closed during recordings.
11) The Binghamton University 3D Facial Expression (BU-3DFE) [Lijun et al. 2006] database includes both prototypical
3D facial expression shapes and 2D facial textures of 2,500 models of 100 subjects. For each shape model, the texture
images were captured at two views of approximately +45 and -45 degrees. There are six basic emotions plus neural with
four levels of emotion intensity. The BU-3DFE was later extended to a high-resolution 3D dynamic database – BU-4DFE
[Yin et al. 2008]. The BU-4DFE includes 606 3D facial expression sequences from 101 subjects with 60,600 frame models in
total. Each subject has six model sequences and each sequence shows one of six basic emotions. The databases are useful
for simulating self-occlusion by rotating the 3D model by certain degrees of yaw, pitch, or roll.
12) The Binghamton Pittsburgh 4D spontaneous expression Database (BP4D) [Zhang et al. 2014c] was collected to
analyze facial actions that are not deliberately posed. It contains a total of 328 sequences of high-resolution 3D images
plus 2D texture videos of 1040×1392 pixels. There are eight categorical emotions, including six basic emotions,
embarrassment and pain, from 41 participants (23 women and 18 men). These emotions were elicited from eight tasks. For
each sequence, three types of meta data are provided, including 27 manually annotated AUs, automatically tracked head
pose (pitch, yaw, and roll), and 83 2D/3D facial landmarks. It is noted that the dataset in the Facial Expression Recognition
and Analysis (FERA) 2017 challenge [Valstar et al. 2017] was derived from the 3D model of the BP4D database. The dataset
comprises of 2,952 videos for training, 1,431 videos for validation and 1,080 videos for test. The challenge focuses on the
recognition of 10 frequently occurring AUs and the estimation of six intensity levels (i.e., 0-5) of seven AUs. Nine different
face orientations were also created by rotating 3D sequences by -40, -20 and 0 degrees pitch and -40, 0 and 40 degrees yaw
from a frontal pose.
Table 1 lists main characteristics of related databases. We can see that all databases:
 primarily use discrete categories or AUs to represent facial expressions (except ‘in-the-wild’);
 are largely collected in a laboratory environment (except COFE and ‘in-the-wild’);
 contain both artificially posed and spontaneously elicited;
 focus on occlusion by hands, glasses and hair;
 focus on self-occlusion by head pose variations (3D databases).
6Fig. 3. Facial samples from existing related databases.
Table 1. Existing Databases for Validating FEA Algorithms under Facial Occlusion.
Occlusion Facial Expression Data
Database
Type A/N Category P/S Size Subject 2D/3D Env
JAFFE1 [Lyons, et al. 1998] - A six basic, neutral P 213 I 10 2D lab
CK2 [Kanade, et al. 2000] - A six basic, AUs S 486 V 97 2D lab
CK+2 [Lucey, et al. 2010] - A six basic, AUs PS 593 V, 122 V 123 2D lab
BHUFE3 [Yu-Li, et al. 2006] hands N smile, anger P 1,600 V 32 2D lab
COFW4 sunglass, hat, food, real
N various emotions (e.g. HA) S 1,007 I - 2D
[Burgos-Artizzu, et al. 2013] hands, hair etc.
In-the-wild hands, hat, pose, +500 V, -
N arousal, valance, 16 AUs S 2D real
[Zafeiriou, et al. 2016] lighting etc. +10,000 I +2,000
AFEW/SFEW5 glasses, eye mask, 957 V,
N six basic S 330 2D real
[Dhall, et al. 2012] beard, hand, hat etc. 700 I
HAPPEI glasses, beard, people
N HA (six intensities) S 4,886 I - 2D real
[Dhall, et al. 2013] standing front, hat etc.
Bosphorus6 hands, eyeglasses, hair, lab
N six basic, 28 AUs P 4,652 scans 105 3D
[Savran, et al. 2008] beard, moustache
Cam3D7 12 mental states, e.g.
hands N S 108 V 16 3D lab
[Mahmoud, et al. 2011] thinking, unsure, happy.
UMB8 Scarf, hat, hands, neutral, smile, bored,
N P 1,473 I 143 2D/3D lab
[Colombo, et al. 2011] eyeglasses, phones, hair hungry
BU-3DFE9
head pose variations N six basic, neutral P 2,500 models 100 2D/3D lab
[Lijun, et al. 2006]
BU-4DFE9 [Yin, et al. 2008] head pose variations N six basic, neutral P 60,600 models 101 2D/3D lab
BP4D9 [Zhang, et al. 2014c] head pose variations N eight emotions, 27 AUs S 328 V 41 2D/3D lab
Note: ‘-’ means not available.
Abbreviations: A- Artificially imposed; N – Naturally occurring; P – Posed emotion; S – Spontaneous; I – Image; V – Video.
1: http://www.kasrl.org/jaffe.html; 2: http://www.pitt.edu/~emotion/ck-spread.htm;
3: http://www.ee.buaa.edu.cn/oldeeweb/html/zykj/teachers/mx/news/7.html; 4: http://www.vision.caltech.edu/xpburgos/ICCV13;
5: https://cs.anu.edu.au/few/; 6: http://bosphorus.ee.boun.edu.tr/default.aspx; 7: http://www.cl.cam.ac.uk/research/rainbow/projects/cam3d;
8: http://www.ivl.disco.unimib.it/umbdb; 9: http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html.
74 Automatic Facial Expression Analysis Approaches
Rather than providing a comprehensive survey on all previous approaches on automatic FEA, this paper limits its
attention to only those that have used or investigated face data with partial occlusion. For exhaustive surveys on the past
approaches on FEA and affect recognition in non-occluded faces, as well as face recognition or human detection under
occlusion, readers are referred to the following work:
 Introduction of early FEA approaches [Pantic and Rothkrantz 2000], [Fasel and Luettin 2003].
 Surveys of recent 2D/3D FEA methods [Bettadapura 2012], [Sandbach et al. 2012], [Owusu, et al. 2015], [Sariyanidi,
et al. 2015], [Corneanu, et al. 2016].
 Summaries of affect recognition work using audio, visual, text or physiological modalities [Zeng, et al. 2009], [Calvo
and D'Mello 2010].
 Reviews of dimensional affect analysis methods using audio, visual, and biological modalities [Gunes et al. 2011],
[Gunes and Schuller 2013].
 Overviews of face recognition or human detection methods under partial occlusion [Azeem et al. 2014], [Nguyen et
al. 2016].
 A survey of face detection methods in a wild environment [Zafeiriou et al. 2015].
 A survey of FEA methods using FACS [Martinez et al. 2017].
 A brief review of FEA using deep learning methods [Zafeiriou, et al. 2016].
According to the strategies used for handing facial occlusion, existing FEA approaches can be roughly divided into
feature reconstruction approach, sparse representation approach, sub-region based approach, statistical model based
approach, 3D data based approach, and deep learning approach. Table 2 provides an overview of these approaches with
respect to occlusion detection, feature, classifier, emotion category, occlusion type, occlusion simulation, and performance.
Table 3 summarizes main characteristics of these approaches regarding their pros and cons. It should be noted that sparse
representation and sub-region approaches are often used for static image based FEA, while model based approach is often
employed for temporal video based FEA. The feature reconstruction, 3D data based, and deep learning approaches can be
used for both types of analysis.
8Table 2. Summary of Six Categories of FEA approaches in the Presence of Occlusion.
Algorithm Database Occlusion
Ref. Acc. (%)
Ocu. Det. Feature Classifier Emotion Size Simulation Type
[Bourel 2001] N Spatio-temp point vector RW-KNN AN, HA, SA, SU CK: 100 V missing region upper/left/right face, mouth >80(except SA)
upper/left/right face, mouth,
[Bourel, et al. 2002] N discrete state of point RW-KNN 6 basic CK: 300 V missing region >80(except mouth)
noise
[Towner and Slater
N point coordinate SVM 6 basic CK: 376 V missing point upper/lower face 70, 82
2007]
n
o 69 (each AU)
itc [Kapoor et al. 2003] N point coordinate SVM 5 AUs, NE natural: 80 I real-life N/A
63 (all AU)
u
r ts [Zhang et al. 2015] N point coordinate SVR+ANN 18 AUs, 6 basic, CK+: 250 I black mask, real- eyes, mouth, N/A
n NE, contempt life upper/lower face
o
c e r [Xia, et al. 2009] SD+ Haar-like Adaboost 6 basic JAFFE & BHUFE: real-life hand, hair, sunglass N/A
e RPCA 1200 I
ru
[Jiang and Jia] N Eigen-/Fisher face NN/SVM 6 basic, NE JAFFE: 213 I black bar mouth, eyes 73, 78
ta
e JAFFE: 213 I 98 (JAFFE)
F
[Cornejo et al. 2015] N Gabor/point coordinate SVM/KNN 6 basic, NE CK+: 593 I black mask 99 (CK+)
MUG: I left/right eye, eyes, bottom 99 (MUG)
left/right face, bottom face
[Cornejo and Pedrini JAFFE: 213 I 92 (JAFFE)
N CENTRIST SVM 6 basic, NE black mask
2016] CK+: 593 I 90 (CK+)
adding noise,
[Cotter 2010a] N raw pixel SRC 6 basic, NE JAFFE: 213 I black/white noise, block 95, >91
mask
black mask,
[Cotter 2011]* N raw pixel FLSRC 6 basic, NE JAFFE: 213 I mouth, block 93, 85
random block
n
o random block,
ita [Cotter 2010b]* N raw pixel WVSRC 6 basic, NE JAFFE: 213 I
black bar
upper/lower face, block 64, 77, 68
tn
e s e [Ouyang et al. 2013]* N LBP map SRC 6 basic CK: 1017 I blac nk o im sea sk, eyes, corruption 72, 87
r
p
e noise, replaced
r
e
[Zhang et al. 2012] N raw pixel, Gabor, LBP SRC 6 basic, NE CK: 470 I
block
corruption, block 68, 42
s
ra
p S [Zhi et al. 2011]* N GSNMF nn ee iga hre bs ot r 6 basic CK: I black mask eyes, nose, mouth 93, 94, 91
STLBP/ black mask, eyes, mouth, lower-face,
[Huang et al. 2012]* SRC WLFF 6 basic, contempt CK: 325 I + V 93, 79, 74, 80
edge map random mask block
JAFFE: 213 I
[Liu et al. 2014d] N raw pixel MLESR 6 basic, NE black mask random block 87 (JAFFE), 85 (CK)
CK: 420 I
[Zhang et al. 2014b]*, JAFFE: 213 I eyes, mouth, random block, 80,78,49,80, 75(JAFFE)
N Gabor-based template SVM 6 basic, NE white mask
[Zhang et al. 2011] CK: 1,615 I clear/solid glasses 95,90,75,95, 92 (CK)
d e s [Song and QiuQi 2011] N LBCM nearest 6 basic, NE JAFFE: I black mask eyes, mouth 94, 93
a neighbor
b
n black bar, real-
o JAFFE: 213 I eyes, mouth, left/right face,
i bg ue r- [Sh [Lu ia ui ,- S eh
t
i
a
e l.t 2a 0l.
1
42 c0 ]1 3] N
N
L WG LB DPH HS SVS MV +M
D F
6
6
b ba as si ic c,
,
N NE
E
JAn Fa Ft Eu :r a 2l
1
I
3 I
bsl lui af ne cg km l a ma ss ask e s, ks
eyes,
msc oa ur tf h, ,s u len fg t/l ra is gs
h t face
86, 9 82 7,
,
9 90 0,
,
9 91 0,
,
8 98 1, 83
S
[Lin et al. 2013] GMM point displacement EWCCM 5 AUs, AU comb. CK+: V grey block lower face 81
[Dapogny et al. 2016] N point distance + HOG WLSRF 6 basic, NE, 14 CK+: 1308 I noisy mask eyes, mouth 72,67 (CK+)
9AUs BU4D: 1212 I 57,49 (BU4D)
[Yongmian and Qiang point displacement, missing feature,
le 2005]
N
furrow
DBN 6 basic, AU Sample V
real-life hand
missing frame, hand N/A
d
o [Miyakoshi and Kato
la
c its
ita
tm
Sd
e
s a b [Ta 2n
00
SD
8
u]a
r,
et
[2
na
T0
dn
a1
rd
n1
a
]
R
D
2*
a 0an 0tg
8
a
]a
n
n da th
KLTN
+ BN
po pin ot
i
nd ti s dp il sa tc ae nm cee nt
H
AMB NN
M
N
+
4
gr6
a
mba msi ac
t ical
JA DF HCF HKE F:
:
S1
V
:8 3
V
I m reis as l-in
l
big
f le
u
f re
h
a at nu dr ,e eyes h, ab nro dw
,
b, lm uro uth 67, 5 66 2, 50
[Hammal et al. 2009] N point distance TBM 6 basic, NE CFE: 100,800 I bubble bubble N/A
SIFT+LPP:
HOG, LBP SIFT + PCA,
view nearest 6 basic, 4 yaw 73.1% (avg.)
[Hu et al. 2008] classifier LDA, LPP neighbor intensity BU3DFE: 12,000 (0, 30, 45, 60, 90) 73.9% (30o)
71.4% (90o)
BU3DFE: (0, 30, 45, 60, 90)
[Moore and Bowden view 6 basic, 4 BU3DFE: 48,000 yaw LGBP: 68% (BU3DFE)
d e s 2011] classifier
LBP, LGBP, MSLBP, etc. SVM
intensity Multi-pie: 4,200
Multi-pie: 7(0 5,
,
1 95 0,
)
30, 45, 60,
81% (Multi-pie)
a
b 6 basic,
a view generic sparse coding pan; (0, ±15, ±30, ±45); 69.1% (4 intensity)
Dt da [Tariq et al. 2012] classifier features SVM 4/ inst tr eo nn sg ite ys t BU3DFE: 21,000 title (0, ±15, ±30) 76.1% (strongest)
3 random
forest + 6 basic, NE, 4 yaw; (-90, 90);
[Vieriu et al. 2015] N haar on 9 channels BU3DFE: 1400 >66%
decision intensity tilt (-60, 60)
fusion
yaw; (-180, 180);
[Sun and Yin 2008] N shape map HMM 6 basic BU4DFE: 34,200 >80%
pitch (-180, 180)
similarity normalized yaw; (-18, 90);
[Tősér, et al. 2016] N CNN 11 AUs BP4D 55% (F1)
images pitch (-54, 54)
non, eyes, mouth, 85.7%, 82.9%, 82.9%,
[Cheng, et al. 2014] N Gabor DBM Six basic, NE JAFFE: 213 I black bar
g lower/upper face 82.9%, 77.1%
n in linear CK: 327 I eyes, mouth, nose, 70%
pra
e
l
[Ranzato et al. 2011] N raw pixel + MRF + DBN
classifier
6 basic, NE
TF: 104,000 I
grey block random, rig fah ct/ eb ottom/top N/A
e FERA 2017 0.506(F1)
e [Batista, et al. 2017] N raw pixel AUMPNet 7 AUs, 6 intensity pose 9 different poses
D challenge (BP4D) 0.399(ICC)
multi-task
view FERA 2017 0.879 (RMSE)
[Zhou et al. 2017]
classifier
raw pixel deep 7 AUs, 6 intensity
challenge (BP4D)
pose 9 different poses
0.446(ICC)
network
Note: “Ocu. Det.” stands for occlusion detection. References with * are the top benchmark approaches identified in Section 4.6.
Abbreviations: N – No, T – Texture, G – Geometry, I – Image, V – Video, N/A – Not Available, ANN – Artificial Neural Network, AU – Action Unit, AUMPNet - Unified CNN, BN – Bayesian Network, CFE –
California Facial Expression database, DBN – Dynamic Bayesian Network, DF – Decision Fusion, DHHFS - Deaf & Hard-of-Hearing Federation of Singapore, EWCCM – Error Weighted Cross-Correlation Model,
HMM – Hidden Markov Model, ICC - Intraclass Correlation Coefficient, KLT – Kanade Lucas Tomasi tracker, KNN – K-Nearest Neighbor, LBCM – Local Binary Covariance Matrices, LGBP - Local Gabor Binary
Pattern, LGBPHS – Local Gabor Binary Pattern Histogram Sequence, MLESR – Maximum Likelihood Estimation Sparse Representation, MKL – Mean rule and multiple Kernel Learning, MUG – Multimedia
Understanding Group database, MSLBP - Multi-Scale LBP, PCC - Pearson Correlation Coefficient, RMSE - Root Mean Square Error, RW-KNN – Rank Weighted KNN, SD – Salient Detector, SRC – Sparse
Representation Classifier, SVM – Support Vector Machine, TBM – Transferable Belief Model, WLDH – Weber Local Descriptor Histogram; WLFF – Weight Learning based Feature Fusion; WLSRF – Weighted Local
Subspace Random Forest Model; WVSRC – ρ-Weighted Voting SRC.
10Table 3. Main Characteristics of Six Categories of FEA Approaches in the Presence of Occlusion.
Face Ocu. Face Feat.
Approach I/V Pros Cons
Det. Det. Reg. Track.
require reliable feature
detector/tracker (G);
Feature Y(T) N(T) I(T) robust feature reconstruction require precise face
Y Y
reconstruction N(G) Y(G) V(G) based on face configuration. alignment/normalization (T);
need occlusion detection (T);
loss of texture.
require precise face
Sparse optimal feature representation; alignment/normalization;
Y Y/N Y N I
representation estimate occlusion location. assume test and training data are
linearly correlated.
require precise face
easy to implement;
Sub-region alignment/normalization;
Y N Y N I good result for small occlusion;
based dependent on face subdivision;
not need occlusion detection.
dependent on decision fusion.
Statistical robust via temporal reasoning; require robust feature trackers;
Y N N Y I/V
model based close to real situation. difficult to create ground data.
require face view classifier & view-
depth information; dependent emotion classifier;
3D data based Y Y/N Y Y I/V
robust to pose variations. require mapping to frontal view;
heavy computation.
require large training data;
Automatic feature extraction;
Deep learning Y N N Y I/V fine-tuning large system parameters;
no need occlusion detection.
heavy computation.
Note: “Face Det.”, “Ocu. Det.”, “Face Reg.”, “Feat. Track.” stand for face detection, occlusion detection, face registration and feature
tracking respectively.
Abbreviations: Y – Yes, N – No, T – Texture, G – Geometry, I – Image, V – Video. Take the step of occlusion detection for instance, ‘N(G)'
means that occlusion detection has not been adopted for constructing geometric features in existing studies, and ‘Y/N’ means that
occlusion detection have been used in some existing studies, while not used in other studies.
4.1 Feature Reconstruction Approach
The feature reconstruction approaches attempt to overcome the effect of occlusion by reconstructing missing
geometric and (or) texture features caused by partial occlusion based on the visual configuration of the face, and they are
the most popular approach in early FEA studies on handling occlusion. Approaches in this category can be further
grouped into geometry based and texture based, according to the type of the features used for representing emotions.
4.1.1 Geometry based Approach
One group of geometry based approaches focuses on utilizing coordinates of facial points in static images. Towner and
Slater [2007] compared the performance of three PCA based methods in the reconstruction of the positions of missing
feature points at the top and bottom parts of the face. They found that the conditional mean method produced the best
accuracy for recovering random subsets of 22 points on the CK database. The feature points reconstructed from partially
occluded images were then used as the input of a Support Vector Machine (SVM) for classifying six basic emotions. There
is only a 5% and a 3% reduction in overall classification rates for the top and bottom occlusion respectively, compared to
using all facial points. Zhang et al. [2015] combined Iterative Closest Point (ICP) features and the Fuzzy C-Means (FCM)
algorithm to reconstruct 54 facial points in an occluded face using prior knowledge of facial elements. The geometry of
facial points was used to predict 18 AUs and eight emotions using Support Vector Regression (SVR) and Artificial Neural
Networks (ANNs). The experiments showed more than 78% point detection accuracy under occlusion of the top and
bottom parts of the face. However, the performance for facial expression recognition was not evaluated.
The other group is based on temporal geometric coordinates of facial points in video sequences. Bourel et al. [2001]
proposed using recovered geometric features to handle occlusion in regions of the upper face, mouth, and left/right half of
the face in video sequence (Fig. 4). Their approach adopted an enhanced version of the Kanade-Lucas tracker to
reconstruct drifting or lost facial points during face tracking, and then generated local spatiotemporal vectors based on
geometrical positions of 12 facial points. A rank-weighted K-Nearest Neighbor (KNN) classifier was further applied
independently to local facial regions to ensure that the occluded region does not affect other regions. A sum fusion of
classifier outputs was finally used to obtain an emotion label for each sequence. The method achieved more than 80%
accuracy for classifying four emotions under four types of occlusion using 100 CK sequences, except for sadness under an
occluded mouth with around 20% accuracy. However, the spatiotemporal vectors in [Bourel 2001] suffer from a lot of
noise. To reduce the noise, Bourel et al. [2002] further converted continuous values into three discrete states (increase,
stable and decrease) based on the average motion amplitude of a sequence representing a facial expression. It resulted in
higher accuracy for classifying six basic emotions under the four types of occlusion plus random noise using 300 CK video
11sequences. However, both approaches require manual annotation of facial points in the first frames of video sequences,
and need to manually omit the occluded features from feature vectors.
Fig. 4. (left) Recovery of lost or missed geometric facial landmarks using an enhanced Kanade-Lucas tracker
in video sequences. (right) Multiple local classifiers are generated from geometric features and fused to derive
a class label [Bourel 2001].
Different from the above approaches which often depend on data from normal digital camera, Kapoor et al. [2003]
proposed an AU recognition system based an infrared sensitive camera. The camera was employed to robustly detect the
pupil location even under unfavorably lighting conditions, and the pupil location was used to find and normalize the eyes
and brow regions. The x and y coordinates of landmark points from the eyes and brows were then extracted as shape
parameters. The parameters in occluded regions in test images were recovered by finding a linear combination of example
images using Principal Component Analysis (PCA). 69.3% accuracy was achieved for each individual AU classification in
real-life video frames with head motions, occlusion and pose changes. Details about occlusion, however, were not given
by the authors.
4.1.2 Texture based Approach
Existing texture based approaches are mainly based on the Robust PCA (RPCA) algorithm. Xia et al. [2009] combined
RPCA and saliency detection for FEA under occlusion. The occlusion was located using a saliency detector by setting a
threshold to the pixel difference between the occluded image and the reconstructed image using RPCA. The occluded
region was then replaced by the corresponding region in the reconstructed face. Haar-like features were further extracted
and fed into AdaBoost for classifying six basic emotions. The method produced 5% and 16% higher accuracies for hands
and sunglasses occlusion respectively than using Adaboost alone on the JAFFE and BHUFE databases. Rather than using
Haar-like features, Cornejo et al. [2015] extracted Gabor wavelets and a geometric representation of 22 facial points from
the recovered regions of occlusion using RPCA. A KNN or SVM classifier was adopted for recognizing expressions in the
presence of five types of occlusion, including two eyes, left eye, right eye, bottom left or bottom right part of the face, and
bottom part of the face. The approach achieved more than 98% accuracy for random partial occlusion on the JAFFE, CK+
and MUG databases. In another similar work by Cornejo et al. [2016], the CENsus Transform hISTogram (CENTRIST)
features were extracted from RPCA reconstructed facial regions, and further fed into PCA plus LDA for feature reduction,
and finally SVM for emotion recognition. The approach showed 92% and 90% accuracies on the JAFFE and CK+ datasets.
The RPCA was also found to outperform PCA and perform similarly to probabilistic PCA in reconstructing occluded eyes
and mouth [Jiang and Jia 2011].
The feature reconstruction approaches retain promising robustness against occlusion by recovering the features in
occluded regions based on facial configuration characteristics. However, they are still heavily dependent on reliable face
detection and facial feature tracking. Approaches used for reconstructing geometric features face the challenge of
accurately detecting and robustly tracking geometric points in the presence of partial occlusion. Approaches [Bourel 2001],
[Bourel, et al. 2002] which depend on manual annotation of the coordinates of facial points in the first video frames have
problems in automatic recognition. Approaches to reconstructing texture require pre-locating occluded regions and
precise face alignment, which are still challenging issues. PCA based techniques have difficulty of precisely recovering
dynamically varied local and subtle texture in an unseen facial region based on information learnt from the training data,
which may significantly impact the performance. In brief, current feature reconstruction approaches are heavily
dependent on the accuracy of face detection and tracking. As a result, it is still difficult to achieve fully automatic
implementation of these approaches in real-life applications.
4.2 Sparse Representation Approach
The sparse representation approach was firstly proposed for face recognition tasks in [Wright et al. 2009], and later
applied into FEA, especially from occluded faces [Cotter 2010a],[Cotter 2010b],[Cotter 2011],[Zhang, et al. 2012]. The
approach treats all training samples as a dictionary and performs robust object recognition using a sparse representation
of a test image that is formed by finding a linear combination of training images from the same class. The optimal weights
for combining training images are searched through solving a convex optimization problem via l minimization. For the
1
12purpose of handling occlusion, the error caused by occlusion is represented by an individual identity matrix that can be
isolated from the feature matrix of un-occluded training images. For occlusion that is not overly large, the error matrix can
be calculated using l minimization. A clear image can be then recovered by subtracting the calculated sparse solution of
1
the error matrix from the test image at the emotion classification stage.
Cotter [2010a] is one of the first work in applying the Sparse Representation Classifier (SRC) into FEA in noise
corrupted or occluded JAFFE images. The SRC obtains 95% accuracy when 50% pixels are corrupted by noise and over 91%
accuracy for block occlusion with a size ranging from 10×10 to 40×40 pixels. The SRC outperformed Eigenfaces or Gabor
features with an ANN or SVM classifier on average. It was found that the use of a black or a white color in simulating
block occlusion also impacts the performance. To more effectively utilize the local characteristics of facial occlusion, the
SRC was later extended to perform on sub-regions of the face. Cotter [2011] proposed the Fusion of Local SRC (FLSRC),
which performs SRC separately in each of three facial regions – the mouth, left eye and right eye. Unoccluded regions are
dynamically determined by setting a threshold to representation errors in SRC. Features from only unoccluded regions are
used to select the most important individual region (or fused region) to make a final classification decision. The tests using
JAFFE images with an occluded mouth and randomly placed block occlusion showed that fusion of all regions leads to
higher accuracy than using each alone. The FLSRC significantly outperformed SRC for an occluded mouth (93.4% vs.
72.8%). For small random block occlusion, they performed similarly, but FLSRC was superior for block occlusion larger
than 35×35 pixels in 96×72 facial regions. Unlike the FLSRC method in [Cotter 2011], which selects only the most
important facial region in making a decision, a ρ-Weighted Voting SRC (WVSRC) scheme was used in [Cotter 2010b]. The
scheme assigns different weights to decisions from nine equally divided facial sub-regions and then combines weighted
decisions. The weights are assigned based on the class representation error of SRC in each sub-region. The WVSRC
significantly outperformed both SRC and Gabor based approaches for occluded upper and lower halves of the face, and
large randomly placed block occlusion. However, a direct comparison between WVSRC and FLSRC was not reported.
The original sparse representation assumes a Gaussian distribution of the coding residual, which may suffer from
inaccuracy in describing errors in practice. Liu et al. [2014d] proposed the Maximum Likelihood Estimation Sparse
Representation (MLESR), which models the sparse coding task as a sparsely constrained regress problem and iteratively
assigns lower weights to pixels in occluded regions until the result converges. A test image was classified into an emotion
which has the minimal spare representation residual with this image. The MLESR outperformed SRC and Gabor based
SRC (GSRC) for all degrees of simulated random occlusion ranging from 0 to 90% of image pixels on the JAFFE and CK
databases. Zhi et al. [2011] presented a Graph-preserving Sparse Non-negative Matrix Factorization (GSNMF) algorithm to
utilize both the sparse and graph-preserving properties of facial images. The GSNMF transforms a high-dimensional image
into a low-dimensional locality-preserving subspace to achieve robustness to partial occlusion. The GSNMF with the
nearest neighbor classifier achieved 93.3%, 94.0% and 91.4% accuracies for classifying six basic emotions under the eyes,
nose and mouth occlusion respectively on the CK database.
Unlike the above work that applied SRC directly on raw image pixels, studies also exploited other feature descriptors.
Ouyang et al. [2013] suggested using Local Binary Pattern (LBP) maps that generally have good robustness against
illumination variations. Fusion of LBP maps and SRC was found to outperform SRC under non-occlusion, occluded eyes,
and partial corruption conditions on the CK database. The fusion achieved 87.0% accuracy when 35% of the face is
replaced with uniform distributed values and 72.4% accuracy when 30% of the face is occluded in the eyes region. Zhang et
al. [2012] performed performance comparisons among raw pixels, Gabor wavelets and LBP features, as well as SRC, ANN,
SVM and the nearest subspace classifiers. Results on CK images indicated that Gabor features with SRC performed the
best for non-occluded images. SRC significantly outperformed ANN, SVM and the nearest subspace at various levels of
random pixel corruption and random block occlusion. Huang et al. [2012] investigated the use of Spatio-Temporal LBP
(STLBP) and edge map features for FEA under occlusion in video sequences (Fig. 5). The features were extracted from the
mouth, nose and eyes components and further integrated in sparse representation to generate a binary codebook for
determining occluded components. For expression recognition, the features were also combined via a feature-level fusion
and their optimal weights were learned using multiple kernel learning. The system yielded 93%, 79.1% and 73.5%
accuracies for classifying six basic emotions plus contempt under occlusion of the eyes, mouth and lower-face respectively
in CK+ frames. The integration of occlusion detection led to higher accuracies for most emotions. A recent study [Amirian
et al. 2017] using sparse coding also achieved promising results of AU intensity estimation in the FERA 2017 challenge.
The proposed approach first estimated the head pose using dictionary learning and then computed a sparse representation
of image patches to train a Support Vector Regression for AU intensity estimation. The approach produced Intraclass
Correlation Coefficient (ICC) of 0.295 and Root Mean Square Error (RMSE) of 0.970 on the test subset of the challenge.
To summarize, the great advantage of the sparse representation approaches is that they are not only robust to small
occlusion and corruption, but also can be used to estimate the occluded or corrupted parts of the face. The approaches
have been demonstrated as one of the most promising techniques in overcoming occlusion for FEA. However, their
performance is largely dependent on whether the test data can be accurately represented using a linear combination of a
subset of training samples from the same emotion, and the availability of a reasonable large number of training samples
with sufficient variations for the emotion. The training dictionary needs not only sufficient information to effectively
represent the test data, but also abundant characteristics to reduce the correlations of training samples from different
13classes [Ouyang, et al. 2013]. To ensure accurate feature extraction, the approaches require precise face location,
alignment and normalization, which are done primarily manually in existing work [Cotter 2010a],[Cotter 2010b],[Zhang,
et al. 2012],[Ouyang, et al. 2013]. One important factor in the use of SRC is to choose a proper type of feature descriptor.
Although several feature descriptors such as raw pixels, Gabor and LBP, have been investigated in existing studies, it is
still a largely unexplored field regarding which descriptor works best for handling facial occlusion. It is still worth
investigating other types of descriptors such as Scale-Invariant Feature Transform (SIFT) and Histogram of Oriented
Gradients (HOG) to further improve the performance.
Fig. 5. An automatic system comprising of occlusion detection using STLBP based sparse representation, and
emotion recognition via multiple feature fusion with weights obtained using multiple kerning learning
[Huang, et al. 2012].
4.3 Sub-region based Approach
The sub-region based approaches treat the whole facial region as comprising of a set of local sub-regions and attempt
to fuse information from only non-occluded sub-regions for FEA. The approaches can be approximately categorized into 1)
feature fusion approach which fuses features from sub-regions, and approaches in this category often adopt a feature
selection mechanism to remove the features extracted from occluded sub-regions, and 2) decision fusion approach which
fuses classification decisions from sub-regions, and approaches in this category often employ a voting strategy to reduce
the weights of decisions derived from occluded sub-regions.
4.3.1 Feature Fusion Approach
Zhang et al. [2011],[2014b] proposed Gabor based templates for FEA under the eyes, mouth, glasses and randomly
placed block occlusion. A Monte Carlo algorithm was used to collect a group of multi-scale 3D Gabor templates from
randomly selected locations in the gallery images. The collected templates form a pool of local texture features, and thus it
is anticipated that only a small portion of them are significantly impacted by partial occlusion. A template-based matching
process was then performed over a local search area to create distance features which encode high-level expression
information of the face and suffer from limited impact by occlusion. A linear SVM was further used to select a small set of
most effective distance features from the whole feature set. The SVM employs normal based feature selection [Dunja et al.
2004] by treating the normal to the classification hyperplane as weights of features and keeping only those features with
high weights. Those features often have the biggest impact on the classification results and thus are most important. The
six basic emotions plus neutral were finally classified using another linear SVM classifier. The experiments showed that a
larger occlusion has bigger influence on the overall performance of both JAFFE and CK databases. The eyes region and the
mouth region have big effects on JAFFE and CK, respectively. The system showed robustness to changes in parameters of
Gabor filters and template sizes. The work [Zhang, et al. 2011],[Zhang, et al. 2014b] adopted features extracted from a
large number of random local patches to overcome facial occlusion, while another commonly used way is to extract
features from a set of facial sub-regions. Guo and Ruan [2011] used a sum function to combine Local Binary Covariance
Matrices (LBCM) features from nine equally sized facial sub-regions. The robustness to occlusion was achieved by
removing the sub-region with the maximal distance between covariance matrices from the gallery and the probe sets.
Using the nearest neighbor classifier, the LBCM outperformed Gabor filters and SRC, and achieved 93.7% and 92.9%
accuracies for occluded eyes and mouth respectively using JAFFE images. Liu et al. [2013] investigated a feature-level
fusion of Local Gabor Binary Pattern (LGBP) maps from equally divided facial sub-regions. Tests on occlusion of the
mouth, eyes, left and right face gave 92.1%, 85.5%, 89.5% and 90.8% accuracies respectively on the JAFFE database. An
additional test on 24 natural images from a person with real-life medical mask and sunglasses occlusion showed 87.5% and
83.3% accuracies for classifying four expressions.
4.3.2 Decision Fusion Approach
Liu et al. [2014c] employed a maximum decision fusion of equally sized facial sub-regions (Fig. 6). From each sub-
region, Weber Local Descriptor Histogram (WLDH) features were extracted and fed into an SVM classifier. The SVM
outputs from all sub-regions were fused using a maximum function. The approach achieved more than 87% accuracy for
14occlusion of the mouth, eyes, left and right sides of the face using JAFFE images. Dapogny et al. [2016] presented Local
Expression Predictions (LEPs) for categorical FER and AU prediction under partial occlusions. The LEPs were generated by
locally averaging predictions by local trees in random forests which are trained using random facial masks generated in
specific parts of the face. HOG features extracted from facial landmarks were used as the local descriptor of facial masks.
For occlusion-robust FER, local confidence measurements were obtained based on the reconstruction error outputted by a
two-layer autoencoder network to weight LEPs in different facial parts. The network was trained to model the local
manifold of non-occluded faces and to reconstruct feature patterns in an occluded face, providing a confidence
measurement based on the reconstruction error. Occlusions were simulated by overlaying noisy patterns to regions of
eyes and the mouth in facial images. Evaluations on the CK+ and BU4D databases showed around 72% and 57% under
occluded eyes, and 67% and 49% under occluded mouth for categorical FER. Instead of using texture features, Lin et al.
[2013] predicted AUs under mouth occlusion using geometric features of displacements of facial points. A Gaussian
Mixture Model (GMM) was employed to model the gray pixel distribution in facial regions for detecting the occluded
region. Facial Deformation Parameters (FDPs) were represented using the displacements of 74 landmark points in six
regions, including the mouth, eyes, eyebrows, nose, cheeks, and jaw. The FDPs in each region and the relationships among
paired regions were modelled using a Cross-Correlation Model (CCM), and the prediction decisions of CCM in all paired
regions were finally combined using a Bayesian weighting scheme. 80.7% accuracy was obtained in predicting five AUs or
AU combinations in CK+ sequences. To optimize the fusion of decisions from nine equally divided facial sub-regions, a ρ
-weighted voting SRC scheme was presented in [Cotter 2010b] to assign a different weight to each sub-region.
Fig. 6. WLDP features and SVM classifier are used to derive a classification decision for each facial sub-region.
The expression classification is achieved using a maximum decision fusion of all sub-regions [Liu, et al. 2014c].
A basic assumption of sub-region based approaches is that occlusion presents only in a small portion of the face, and
thus its effect can be minimized via a feature selection method or a decision voting strategy over all sub-regions. The
approaches are often capable of producing satisfactory performance for small occlusion. However, the granularity of
subdividing the face into local regions, and its effect on the performance is still an open issue, particularly for random
occlusion without a fixed location, shape and size. Similar to feature reconstruction and sparse representation approaches,
the sub-region based approaches are also sensitive to noise due to inaccurate face location, alignment and normalization.
It is still an issue to choose a proper fusion strategy for multiple results [Bourel 2001], such as a linear combination, fuzzy
logic, and ANN. State-of-the-art sub-region based approaches are still at the beginning stage of exploiting the most
effective facial region division method and the best decision or feature fusion strategy.
4.4 Statistical Model based Approach
Unlike the above approaches, the statistical model based approaches do not directly reconstruct the features in
occluded regions nor divide face into sub-regions. Instead, they try to infer occluded features using statistical prediction
models by utilizing the temporal correlations between neighboring video frames or spatial dependent information in non-
occluded parts in static images. A unique feature of statistical model approaches lies in the capacity of robustly inferring
facial features in a current frame based on facial information in neighboring frames, even when the current frame contains
a partially occluded face or is a completely missing frame.
Hammal et al. [2009] exploited the usefulness of facial point deformations with a modified Transferable Belief Model
(TBM) for recognizing facial expressions from images with partial occlusion. Five distances from the contours of the
mouth, eyes, and eyebrows were normalized and mapped to symbolic states. The TBM has the advantage of automatically
integrating features from multiple local facial regions and handle uncertain or imprecise data such as occluded facial
regions, and thus it is adopted for modeling the correlation between expressions and symbolic states. The results on 70
facial images with occlusion simulated by bubble masks showed that the use of all five distances obtained the highest rates
for recognizing happiness (100%), anger (100%), surprise (75%) and disgust (75%), but a much lower rate for recognizing
sadness (25%). The behaviors of human observers are different from TBM based models and the human tends to use
“suboptimal” features for FEA under occlusion. Rather than using distances between facial components in static images
[Hammal, et al. 2009], Miyakoshi and Kato [2011] improved this by using movement magnitudes of 14 points from a
15neutral face to an emotional face in static images. A Bayesian network classifier was employed to learn the dependencies
between target facial expressions and facial features without involving a process of filling in the facial gap due to
occlusion. The causal relationships among facial features and the structural associations between expressions and facial
features were learnt using the K algorithm and stepwise feature selection respectively. 67.1%, 56.0%, and 49.5% accuracies
2
were observed for occlusion of the eyes, brows and mouth in JAFFE images.
Given the limited information available in static images, other statistical model based approaches have focused on
directly processing video sequences. Work [Tan Dat and Ranganath 2008],[Tan Dat and Surendra 2008] designed a
Bayesian tracker to reliably track facial points in the presence of temporal occlusion by head motions or hands (Fig. 7).
The tracker augments the Kanade Lucas Tomasi (KLT) tracker by additionally incorporating a Bayesian feedback
mechanism. Seven eyebrow and four eye distances were extracted from the tracked points and fed into seven individual
Hidden Markov Models (HMMs) for classifying four face movement categories and three head motions. The likelihood
outputs of HMMs were further used as inputs into an ANN for classifying four grammatical expressions (Yes/no, Wh,
Topic and Negation). The proposed tracker showed more stable tracking results than KLT. The proposed system had an
accuracy drop from 69% to 62% in the presence of hand occlusion and performed the best when it was trained and tested
using the same tracker. However, the system requires manual annotation of facial features in the first video frames. Unlike
[Tan Dat and Ranganath 2008],[Tan Dat and Surendra 2008], which aimed to generate robust facial point trackers,
Yongmian and Qiang [2005] focused on recognizing AUs directly from occluded video frames (Fig. 8). They utilized
temporal reasoning via Dynamic Bayesian Networks (DBNs), which are capable of accounting for information from both
current visual observations and previous visual evidences. Facial features missed in an occluded frame were compensated
by modeling its temporal correlations with neighboring frames. Although no systematic evaluation was conducted,
impressive results were achieved using video examples with undetected or untracked frames and hand occlusion.
Fig. 7. Distance features between facial points are extracted based on a Bayesian tracker, and they are fed into
individual HMMs and ANNs subsequently for classifying four grammatical expressions [Tan Dat and
Ranganath 2008].
Fig. 8. (left) Dynamic Bayesian networks generated for FEA and (right) its accuracy for classifying happiness
under temporal occlusion caused by a moving hand [Yongmian and Qiang 2005].
Image based statistical model approaches generally depend on spatial relationships in facial parts learnt from training
data to recover facial occlusions, but they also require large training data, and heavy computation. On the other hand, the
greatest advantage of video based statistical model approaches is that they are able to utilize temporal information in
video sequences to infer features in occluded facial regions, and thus the results are anticipated to be more robust than
static image based approaches because facial expressions often exhibit strong and unique temporal patterns and
correlations. This approach also shows a good capacity of handling missed frames and is closer to the situation of
handling facial occlusion in real-life applications. The drawbacks lie in the difficulty of creating suitable ground truth
16video data that represents a full sequence of a facial expression to train the model, and the requirement of robust facial
feature trackers in occluded frames. To conclude, statistical model based approaches seem to be a robust method for
handling facial occlusion during FEA. However, due to the lack of suitable benchmark databases, direct comparisons of
statistical model based approaches with other approaches are still largely unexplored in existing work. As a result, their
performance advantages over other approaches still need to be further validated.
4.5 3D Data based Approach
Most approaches discussed so far are based on 2D facial data. The 3D data based approaches include the additional
depth information about the facial structure and appearance on top of the normal 2D data. The depth information can be
potentially utilized for generating more robust, discriminative, and view-independent features under facial occlusion,
particularly those caused by head pose changes, or missing parts [Drira et al. 2013]. Although FEA using 3D data has been
researched intensively (see [Sandbach, et al. 2012] for a recent survey on existing studies), the approaches that are
specifically designed to overcome facial occlusion are still limited and largely focus on handling self-occlusion caused by
head pose variations. This part introduces several typical 3D based approaches for FEA with or without facial occlusion.
One typical 3D data based approach is multiple views method, which first estimates the face’s current view angle and
then builds a separate emotion classifier for each angle. Accordingly, there are generally two sequential steps, including
view classification and view dependent FER. Hu et al. [2008] adopted a five-class view classifier to determine the view and
trained a separate emotion classifier for each view. Three descriptors - HOG, LBP and SIFT, and three feature dimension
reductions – PCA, LDA and Locality Preserving Projection (LPP), are comparatively used with the same nearest neighbor.
Experiments were conducted on recognizing 6 emotions from 12,000 face data with five yaw angles (0, 30, 45, 60 and 90
degrees) on the BU-3DFE database. The results showed a fusion of SIFT and LPP produced the lowest average error rate of
26.9% for all views. Moore and Bowden [2011] compared LBP features with their variations as texture descriptors for both
facial view and expression classification using a multi-class SVM. Face images with five yaw angles (0, 30, 45, 60 and 90
degrees) are projected from a 3D textured model on the BU-3DFE database. Experiments indicated that Local Gabor Binary
Patterns (LGBPs) perform the best, particularly at large yaw angles. Tariq et al. [2012] utilized generic sparse coding
features with a linear SVM for multi-view FER. Experiments used 84,000 face data with seven pan angles (0, ±15, ±30 and
±45 degrees) and five title angles (0, ±15 and ±30 degrees) from the BU-3DFE database. The approach achieved 69.1% and
76.1% accuracies of classifying six emotions with four emotion intensities and only the strongest intensity, respectively.
Another type of 3D-based approach is simulated 3D features method. Different from multiple views methods, this
method directly performs FER on 3D non-frontal facial data by feature mapping. The mapping is achieved by transferring
non-frontal features to their counterparts in a frontal view of the same face. Vieriu et al. [2015] transformed 3D data of the
face onto a pose invariant 2D cylindrical representation (Fig. 9), where self-occlusion was treated as missing information
in this representation. The representation was later split into multiple overlapping patches, and from each patch, Haar
features were extracted from 9 channel maps and a local random forests classifier was generated. The emotional state of
the face was recognized via a weighting decision scheme which fuses probabilities from patch-specific random forests.
Evaluations showed that the method achieved a recognition rate of 66.2% on faces with (-90, 90) degrees of yaw rotation
and (-60, 60) degrees of tilt rotation on the BU-3DFE database. Rudovic et al. [2010] mapped locations of 39 points in a
non-frontal face to their corresponding locations in a frontal view using four regression functions, which were fed into a
frontal face SVM emotion classifier. Evaluations on classifying four emotions from 800 facial images with four views (0, 15,
30 and 45 degrees) on the Multi-PIE database showed that the approach outperformed view-specific classifiers.
Fig. 9. Projection of sampled 3D faces under varying head poses in (f) to a frontal pose-invariant face
representation in (e) using a cylindrical head model in (b) [Vieriu, et al. 2015].
17There are also other 3D-based methods that are not specifically designed to handling facial occlusion, but can be
potentially used for FEA under occlusion. Sun and Yin [2008] presented a spatio-temporal 3D model-based approach
which integrates 3D facial surface descriptor and HMM for emotion recognition. The surface descriptor included eight
primitive shape types and was generated based on 83 facial landmarks and LDA based feature reduction. The approach
was evaluated on simulated partial occlusion by changing the yaw and pitch angles of a 3D face model. The results
showed over 80% accuracy for raw and pitch angles of 60 degrees, and only limited decreased accuracy for a yaw of close
to 90 degree. However, the accuracy degrades to zero when either pitch or yaw changes to a 150 degree where useful
facial information is completely occluded. The results also indicated that temporal approaches outperformed image based
statistic approaches, implying that motion information helps to compensate the loss of spatial information. In [Zhao et al.
2011], a 3D Statistical Facial feAture Model (SFAM) was presented to locating facial landmarks in the presence of facial
expressions and occlusion. The SFAM combined global variations in 3D face morphology and local variations of texture
and geometry features around each facial landmark, and integrated them in an objective optimization function. To identity
the type of occlusion, a histogram of the similarity map between local shapes of the target face and shape instance from
the SFAM was used with a KNN classifier. The type of occlusion was also integrated in the optimization function to
localize facial landmarks by setting a binary weight (0 for occluded and 1 for unoccluded) to each landmark region.
Experiments on the Bosphorus database demonstrated an accuracy of 93.8% for classifying four types of occlusion (i.e.,
eyes, mouth, glass and non-occlusion) and a precision of 20-mm for locating 97% landmarks. The SFAM also achieved a
precision of less than 10mm for most landmarks on the FRGC V1, V2, and BU-3DFE databases. However, the SFAM was
designed for landmark detection and no result on FER was reported.
Studies also investigated fusion of 2D with 3D feature descriptors to improve FER results. Li et al. [2015] represented
the local texture around 2D facial landmarks using histogram of second order gradients and first-order gradient based
SIFT descriptor, and the local geometry around 3D facial landmarks using histogram of mesh gradients and histogram of
mesh shape index. The texture and geometry were fused at both feature-level and score-level to improve FER based on an
SVM classifier. Evaluations on the BU-3DFE and Bosphorus databases show that 2D and 3D descriptors provide
complementary characteristics (Fig. 10). Zhao et al. [2016] localized facial landmarks from 2.5D facial data using a
deformable partial face model. Global and local features were extracted from those landmarks and used to represent
coordinates in an AU space, where each region was classified to a specific affective state using SVM. The approach
achieved promising FEA results on the EURECOM, FRGC and Bosphorus databases. However, those studies were not
designed for handling facial occlusion.
Fig. 10. Distribution of top 15 most discriminative 2D (top) and 3D (bottom) landmarks. The 2D and 3D
features provide complementary characteristics for FEA [Li, et al. 2015].
A benefit of using 3D data is the possibility of using a richer set of information about facial structure, texture, and
depth. Thus, 3D data based approaches are more robust in handling facial occlusion, particularly self-occlusion caused by
head pose variations. However, the disadvantages are heavy computational time required and the necessity of designing
algorithms to accurately map non-frontal facial features into a frontal face representation. Existing studies have been
heavily dependent on several available databases collected in controlled laboratory environments. The lack of naturalistic
3D FEA databases with different types of occlusion has largely restricted the current progress on designing 3D data based
approaches and evaluating their performance in realistic conditions.
4.6 Deep Learning Approach
In recent years, deep learning approaches such as Boosted Deep Belief Network [Liu et al. 2014b], Convolutional
Neural Networks (CNNs) [Chang et al. 2017], Long Short-Term Memory [Rodriguez et al. 2017], and fusion of multiple
deep neural networks [Zhang et al. 2017], have gained increasing popularity due to their ability to deliver state-of-the-art
performance on FEA (see a recent review in [Zafeiriou, et al. 2016]). Deep learning has advantages of learning more
abstract patterns progressively and automatically from raw image pixels in a multiple layer architecture rather than using
hand-engineered features. It is better suited to learn embedded patterns in facial regions and encode geometric
relationships between facial components, and thus it can be used in recovering occluded facial parts in an inherent and
automatic manner.
18Existing studies on utilizing deep learning for FEA under occlusion are still very limited, and they mainly focus on
using a single deep learning architecture. Toser et al. [2016] employed a 8-layer CNN for AU detection on 3D facial data
with self-occlusion caused by large pose variations. The CNN accepted as input similarity normalized images and
combined gradient descent with selective methods to improve the convergence of its optimization. An augmented data
was created based on the BP4D database using 3D face information and renderings of the face with different rotations,
yielding faces with head poses ranging (-18, 90) degrees for yaw and (-54, 54) degrees for pitch. The CNN produced a mean
F1 score of 0.55 for recognizing 11 AUs on the augmented data. Cheng et al. [2014] presented a deep structure for FER
under partial occlusion. Multi-scale and multi-orientation Gabor magnitudes were extracted from facial images using
Gabor filters, and were taken as input to a three-layer Deep Boltzmann Machine (DBM) for emotion classification. The
DBM had a structure of 3,500-600-600-7 elements and incorporated a pre-training and a fine-turning process to learn the
best weights for encoding the structure of the face including occlusion. The impact of occlusion was then reduced by
compressing the features from occluded parts in the DBM. The DBM was evaluated for classifying six basic and neutral
emotions from JAFFE images with occlusion simulated by artificially added black bars. It achieved 85.7% and 77.1% for
non-occlusion and occluded upper part of the face, as well as the same accuracy of 82.9% for occluded eyes, mouth and
lower part of the face. Ranzato et al. [2011] presented a deep generative model that uses a gated Markov Random Field
(MRF) as the first layer of a DBN for FER under occlusion. The MRF learnt the covariance structure and intensities of
image pixels, while the DBN included several layers of Bernouilli hidden variables to model the statistical structure in the
hidden activities of the MRF. The missing pixels in facial occlusion were filled in by propagating the occluded image
through all layers in the model using a sequence of posterior distributions learnt from training data. From the
reconstructed facial region, features were extracted and used for FER with a linear classifier. Occlusions of the eyes, mouth,
nose, right/bottom/top halves, and 70% of the pixels at random were artificially generated based on the CK and Toronto
Face (TF) databases. The results showed a higher overall accuracy than a Gabor-based, sparse coding, SVM-based
approaches. However, the deep generative model assumes prior knowledge of the location of occlusion, requires large
training data, and is computationally expensive.
Deep learning techniques also showed state-of-the-art performance in the FERA 2017 challenge. The challenge used a
dataset from the BP4D database having six intensity levels of seven AUs and nine head poses, as described in Section 3.
Batista et al. [2017] presented a unified CNN (AUMPNet) to detect AUs and estimate their intensity simultaneously from
multi-view facial images through a multi-task loss. The pose estimate was added to the multitask loss to generate features
invariant to head pose changes. The AUMPNet demonstrated better performance than the FERA 2017 baseline, and it
achieved mean F1 scores of 0.521 and 0.506 for AU detection, and mean ICCs of 0.499 and 0.399 for intensity estimation on
the validation and test subsets, respectively. Zhou et al. [2017] proposed a Multi-Task Deep Network (MTDN) for AU
intensity estimation. Three pose-dependent AU regressors and one pose estimator were trained, and they shared the same
bottom layers of a deep CNN. The final AU estimate was taken as the dot product between a winning pose regressor and
the output of the pose estimator. The MTDN network achieved a mean RMSE of 0.823, ICC of 0.601, and Pearson
Correlation Coefficient (PCC) of 0.620 on estimating AU intensity on the validation subset, as well as RMSE of 0.879 and
ICC of 0.446 on the test subset. Tang et al. [2017] fine-tuned the VGG network for AU detection from multi-view facial
regions that were segmented from facial images using morphology operations. The network achieved F1 of 0.574 and
accuracy of 0.778 on the test subset. Li et al. [2017a] proposed a multi-AUs late fusion approach. The hand-crafted LBP-
TOP features and automatically extracted CNN features were used separately for training three AU classifiers. The
predictions of all classifiers were concatenated and taken as the input of a second-level AU classifier. The approach
achieved F1 of 0.498 and accuracy of 0.694 on the test subset. Similarly, state-of-the-art results have also been observed for
deep learning techniques in the EmotiW 2015 and 2016, where improved versions of RNN, CNN, and LSTM were reported
as the top performers in video based, image based, and group-level emotion recognition.
The power of deep learning techniques is that they can automatically learn the most discriminative feature patterns of
facial expressions from the raw face data. They normally do not require a separate process of occlusion detection or
reconstruction. Specifically, the occlusion information can be inherently embedded in the feature set automatically learnt
by deep learning architectures. Given the state-of-the-art performance of deep learning techniques in various compute
vision tasks, we can anticipate that deep learning will potentially be one of the most effective approaches to handling
occlusion for FEA. However, the use of deep architectures for FEA has to overcome several constraints such as the need of
a large amount of training data to ensure proper feature learning, the difficulty of tuning a large number of system
parameters, and the requirement of expensive computation.
4.7 Summary of FEA Approaches
The state of the art on automatic FEA approaches under partial occlusion can be summarized as follows.
1) The primary types of occlusion include occluded eyes, mouth, left/right, upper/lower face, random placed blocks,
hands, glasses, noise (as shown in Fig. 11), and self-occlusion caused by head pose variations. Few studies have tested
occlusion in brows [Miyakoshi and Kato 2011] or arising from hair [Xia, et al. 2009] and blur [Tan Dat and Ranganath
2008],[Tan Dat and Surendra 2008], as well as missed frames [Yongmian and Qiang 2005].
192) The majority of existing studies focus on artificial occlusion simulated by removing occlusion related features or by
superimposing graphically generated masks into a certain region of the face. Few attempts have been made toward using
naturally occurring occlusion from real-life data, such as mouth masks and sunglasses [Shuai-Shi, et al. 2013], and hand
occlusion [Yongmian and Qiang 2005],[Tan Dat and Ranganath 2008],[Tan Dat and Surendra 2008].
3) Most existing evaluations are based on the JAFFE, CK, and CK+ databases with non-occluded faces. Some initial
efforts were reported on real-life data with natural occlusion arising from sunglasses, medical mask [Shuai-Shi, et al. 2013]
and hands [Yongmian and Qiang 2005], but the results were primarily used for demonstrating the performance of the
system on sample occluded data and no thorough evaluation outcomes such as emotion classification accuracy were
reported on the whole dataset. No work has been found to investigate FEA under occlusion from 3D face data.
4) All existing studies focus on a single type of occlusion in frontal faces, and to our best knowledge, the presence of
co-existed occlusion or multiple faces has not been investigated yet.
5) Most work extracts features directly from occluded facial images without incorporating a pre-processing step of
occlusion detection. Only few attempts [Xia, et al. 2009],[Huang, et al. 2012],[Lin, et al. 2013] have investigated occlusion
detection techniques and integrated them in complete FEA systems. Most approaches require accurate face location and
alignment, and robust facial feature tracking.
6) Features used are largely restricted to texture or geometry from the 2D visual face modality only, and few studies
have investigated fusion of them [Huang, et al. 2012] and utilized skin color features [Lin, et al. 2013]. Existing works on
3D data primarily focus on self-occlusion generated by varying head poses of a 3D face model. Only few studies have been
found on exploiting the fusion of features from multiple modalities.
7) Most studies place emphasis on the six basic emotions plus neutral. Only few studies have been reported on
exploiting non-basic emotions, such as contempt [Huang, et al. 2012], grammatical expressions [Tan Dat and Ranganath
2008],[Tan Dat and Surendra 2008] and AUs [Kapoor, et al. 2003],[Lin, et al. 2013],[Yongmian and Qiang 2005]. No
occlusion-specific work has been found on using dimensional spaces.
8) Although various types of feature descriptors have been used in existing work, there is yet an agreement on the
most effective feature descriptor for handling facial occlusion. For instance, which descriptor works best with the sparse
representation is still largely an unanswered question? From this perspective, it seems that deep learning presents a
unique advantage by automatically leaning the most discriminative features for FEA.
Fig. 11. Illustration of various types of facial occlusion in existing FEA studies.
4.8 Top Five Benchmark Approaches
To facilitate direct performance comparisons of future algorithms with the state-of-the-art results, we identified the
top five approaches from existing studies on the JAFFE and CK/CK+ databases respectively. The criterion for selecting
these approaches is that they achieved the so-far highest overall accuracies of classifying six basic and neutral emotions
on the two databases between 2001 and 2016. In addition, we also included recent approaches from the FERA 2017
challenge. Table 4 lists the accuracies of these approaches under different types of facial occlusion. It should be noted that
the accuracies on the JAFFE and CK/CK+ databases provide a simple indicator of the performance and may not be directly
comparable due to the differences in train-test strategies, face pre-processing steps, occlusion simulation methods, etc. in
these approaches.
20Table 4. Accuracy (%) of Top Five Benchmark Approaches under Different Types of Facial Occlusion.
(a) the JAFFE Database
Face Emo Occlusion
Ref. Approach
size No. Non Eyes Mouth Brow Upper Lower Block Glass
[Cotter 2011] FLSRC 96×72 7 96.7 - 93.4 - - - 85.0 -
[Cotter 2010b] WVSRC 96×72 7 95.3 - - - 63.8 77.0 86.4 -
[Cheng, et al. 2014] DBM 80×70 7 85.7 82.9 82.9 - 77.1 82.9 - -
[Kotsia, et al. 2008] DNMF 80×60 6 85.2 82.5 81.5 - - - - -
[Zhang, et al. 2014b] Gabor Temp. 48×48 7 81.2 80.3 78.4 - - - 48.8 75.1,79.8
(b) the CK/CK+ Database
Face Emo. Occlusion
Ref. Approach
size No. Non Eyes Mouth Nose Lower Block Noise Glass
[Zhang, et al. 2014b] Gabor Temp. 48×48 7 95.3 95.1 90.8 - - 75 - 91.5,95.0
[Huang, et al. 2012] CFD-OD-WL orig 7 93.2 93.0 79.1 - 73.5 - 79.7 -
[Zhi, et al. 2011] GSNMF+NN 60×60 6 - 93.3 91.4 94.0 - - - -
[Kotsia, et al. 2008] Shape+SVM 80×60 6 91.4 88.4 86.7 - - - - -
[Ouyang, et al. 2013] LBPM+SRC 64×64 6 - 72.4 - - - - 87.0 -
(c) the BP4D Database (FERA 2017 Challenge)
Test AU Inten. Pose AU recognition Intensity estimation
Ref. Approach
size No. level no. F1 Acc. 2AFC RMSE ICC PCC
[Zhou, et al. 2017] MTDN 1080 V 7 6 9 - - - 0.879 0.446 -
[Batista, et al. 2017] AUMPNet 1080 V 7 6 9 0.506 - - - 0.399 -
[Li, et al. 2017a] multi-AU fusion 1080 V 7 6 9 0.498 0.694 - - - -
[Tang, et al. 2017] VGG 1080 V 7 6 9 0.574 0.778 - - - -
[Amirian, et al. 2017] sparse coding 1080 V 7 6 9 - - - 0.970 0.295 -
[Valstar, et al. 2017] CRF or CORF 1080 V 7 6 9 0.452 0.561 0.537 1.403 0.217 0.221
Note: ‘-’ means values not available, the facial region in [Miyakoshi and Kato 2011] is set to 30×30 pixels between two eyes, and Ref.
[Valstar, et al. 2017] is the baseline approach for the FERA 2017 Challenge. Abbreviations: CRF – Conditional Random Field (AU
occurrence sub-challenge), CORF – Conditional Ordinal Random Field (intensity sub-challenge), DBM - Deep Boltzmann Machine, VGG -
Oxford Visual Geometry Group network.
5 Effect of Occlusion on Facial Expression
Investigations into the effect of occlusion on the classification performance of facial expressions can provide useful
insights into the most informative facial parts for an expression, and thus can be useful for designing FEA systems.
Existing investigations are from either computer vision or human perception.
5.1 Computer Vision Investigation
Computer vision investigations are generally based on the recognition performance of FEA systems. Buciu et al. [2005]
showed that occlusion of the eyes and mouth have a similar effect on the overall performance on the JAFFE database,
while occlusion of the mouth exerts a larger effect than occluded eyes on the CK database. Occluded eyes and mouth
affect the most sadness and neutral respectively on the JAFFE database, and both anger and sadness on the CK database.
Buciu et al. [Kotsia, et al. 2008] also conducted a machine experiment and a human observer experiment (2 experts and 13
non-experts) on the JAFFE and CK databases. The results demonstrated that an occluded mouth results in a decrease of
more than 50% in the overall classification accuracy than occluded eyes, indicating a more important role of the mouth
than the eyes for FER. Occlusion on the left or right side of the face has little effect on the accuracy. An occluded mouth
affects the classification accuracies of anger, fear, happiness and sadness more, while an occluded eye affects those of
disgust and surprise more. The results are in line with those from human observers, indicating the consistency between
computer vision and human vision in discriminating facial expressions from occluded faces. Azmi and Yegane [Azmi and
Yegane 2012] investigated the effect of occlusion of the mouth, eyes, upper and lower parts of the face. Occluded eyes
were found to affect the overall accuracy more than an occluded mouth. Occluded eyes have more effect on the
classification accuracies of surprise, anger and sadness, while an occluded mouth affects more on those of disgust and fear.
Upper face occlusion affects the classification of sadness, surprise and anger most, while lower face occlusion impacts
more that of disgust, fear and neutral. Regarding train-test strategies, studies [Ranzato, et al. 2011], [Zhang, et al. 2014b]
found that using both training and test data with occlusion produces higher accuracy than using non-occluded training
data and occluded testing data. This indicate the importance of informing the learning model of the presence of occlusion
patterns at the training stage. From the above results, we can find that there is no direct agreement on a more important
role between the mouth and eyes. Take the JAFFE database as an example, some studies [Buciu, et al. 2005] found an
equally important role between them. By contrast, some studies [Kotsia, et al. 2008] found that the mouth is more
important, while some [Shuai-Shi, et al. 2013] indicated that the eyes are more important. One primary reason for the
21contrary results is due to different computer vision systems used in those work. Those systems may vary dramatically in
aspects such as data pre-processing step, feature type, classification algorithm, system setting, and train-test strategy.
These differences can have direct big impact on the results of the systems even when the same database with the same
type of occlusion is used for evaluation.
Table 5 presents a summary of the reductions in accuracy owing to facial occlusion in comparison with non-occlusion
for six basic emotions plus neutral, and the overall performance. Almost all present investigations are based on the JAFFE
and CK databases. For studies that have not provided the reductions in accuracy, they are not listed.
From Table 5, we can see that:
 Occlusion of the mouth results in more reduction in accuracy than occlusion of eyes for anger, fear, happiness,
sadness, and the overall performance.
 For disgust, occlusion of eyes leads to more reduction in accuracy than occlusion of the mouth on the CK database,
but less reduction on the JAFFE database.
 Occlusion of the lower face causes significant reduction of the classification accuracies of six basic emotions plus
neutral, and of the overall accuracy.
 Occlusion of the upper face produces large reduction in accuracy for anger and happiness, but small reduction for
neutral and fear.
 Occlusion of the nose leads to little reduction in accuracy for all six basic emotions.
 Occlusion of random blocks causes large reduction of the overall accuracy.
5.2 Human Perception Investigation
Human perception investigations often asked recruited subjects to identify pre-defined facial expressions from a face
with certain parts occluded. The results reveal the visual information that is perceptually necessary and sufficient in the
human recognition. Instead of providing a comprehensive review on previous work (which is not the focus of this survey),
we introduce several typical work.
Early psychological studies [Dunlap 1927],[Ruckmick 1921] focused on the question of whether there is one facial area
which can best distinguish among facial expressions. This question was later largely answered by the predominant
evidence from researchers such as Ekman [Boucher and Ekman 1975],[Ekman et al. 2013] and Hanawalt [Hanawalt 1942]
that the most distinctive facial component varies with each emotion. Using static photographs of posed facial expressions,
it was generally found that the most important facial components are mouth/cheeks, eyes/eyelids, and brows/forehead,
and that disgust is best distinguished from the mouth, fear from the eyes, sadness from both brows and eyes, happiness
from both mouth and eyes, anger from mouth and brows, and surprise from all and eyes, anger from mouth and brows,
and surprise from all three components.
Recent studies on investigating the effect of occlusion on human perception tend to use spontaneous facial expressions,
video sequences, and subjects with different ages. Halliday [Halliday 2008] asked 56 female participants to identity
happiness, sadness and fear from static photographs of genuine and posed facial expressions from a single displayer, with
four regions occluded: the forehead and eyebrows, the nose and cheeks, the eyes, and the mouth. The results revealed that
participants can accurately identify emotions from limited information, and the mouth and eyes are the two most crucial
regions for recognizing genuine emotions. Bassili [Bassili 1979] found that the participants more accurately recognize six
basic emotions using temporal displays in video sequences than static displays of peak emotions, implying the importance
of facial motion in assisting the recognition. The bottom part of the face is able to generate a higher overall recognition
rate than the top part. Using 11 participants, Nusseck [Nusseck et al. 2008] showed that the fusion of the mouth, eyes and
eyebrows regions is sufficient to generate acceptable results for most of nine conversational expressions. By comparing
the recognition rates of facial expressions with simulated sunglasses or masks between younger children, children and
adult students, Roberson et al. [Roberson et al. 2012] observed that the capacity of accurate decoding of facial expressions
under eyes and mouth occlusion grows slowly with ages, but the inversion of sunglasses does not affect the performance
of 5–6 year olds.
22Table 5. Summary of Accuracy Deduction (% in Parentheses) for Each Emotion and Overall Performance due to Facial Occlusion.
Occlusion
Database Approach
Non Eyes Mouth Upper Lower Nose Random
Gabor [Kotsia, et al. 2008] 82 79.3(-2.7) 70(-12) - - - -
DNMF [Kotsia, et al. 2008] 78.3 74(-4.3) 69.3(-9) - - - -
CK SVM [Kotsia, et al. 2008] 86.9 82(-4.9) 80.7(-6.2) - - - -
AN GSNMF [Zhi, et al. 2011] 98.7 94.7(-4) 94(-4.7) - - 97.3(-1.4) -
PCA [Towner and Slater 2007] 94 - - 70(-24) 48(-46) - -
LGBP [Azmi and Yegane 2012] 99.7 85(-14.7) 99.7(-0) 89(-9.7) 96(-3.7) - -
JAFFE
Bayesian [Miyakoshi and Kato 2011] 63.3 70(+6.7) 46.7(-16.6) - - - -
Gabor [Kotsia, et al. 2008] 94.1 81.5(-12.6) 85.1(-9) - - - -
DNMF [Kotsia, et al. 2008] 82 77.8(-4.2) 80(-2) - - - -
CK SVM [Kotsia, et al. 2008] 86.7 83.8(-2.9) 85.2(-1.5) - - - -
DI GSNMF [Zhi, et al. 2011] 93.3 91.3(-2) 91.3(-2) - - 92.7(-0.6) -
PCA [Towner and Slater 2007] 100 - - 94(-6) 60(-40) - -
LGBP [Azmi and Yegane 2012] 94.8 91.3(-3.5) 87.5(-7.3) 94.2(-0.6) 83.3(-11.5) - -
JAFFE
Bayesian [Miyakoshi and Kato 2011] 55.2 48.3(-6.9) 31(-24.2) - - - -
Gabor [Kotsia, et al. 2008] 93 92.5(-0.5) 87.2(-5.8) - - - -
DNMF [Kotsia, et al. 2008] 76 74(-2) 71(-5) - - - -
CK SVM [Kotsia, et al. 2008] 92.9 91.9(-1) 87.3(-5.6) - - - -
FE GSNMF [Zhi, et al. 2011] 95.3 94(-1.3) 88(-7.3) - - 94(-1.3) -
PCA [Towner and Slater 2007] 82 - - 78(-4) 70(-12) - -
LGBP [Azmi and Yegane 2012] 93 86.8(-6.2) 80.7(-12.3) 92.7(-0.3) 86.8(-6.2) - -
JAFFE
Bayesian [Miyakoshi and Kato 2011] 71.9 56.3(-15.6) 53.1(-18.8) - - - -
Gabor [Kotsia, et al. 2008] 90.6 88.2(-2.4) 83.2(-7.4) - - - -
DNMF [Kotsia, et al. 2008] 96.5 95(-1.5) 93.1(-3.4) - - - -
CK SVM [Kotsia, et al. 2008] 95.7 93.6(-2.1) 90.9(-4.8) - - - -
HA GSNMF [Zhi, et al. 2011] 96.7 96(-0.7) 93.3(-3.4) - - 94(-2.7) -
PCA [Towner and Slater 2007] 86 - - 73(-13) 67(-19) - -
LGBP [Azmi and Yegane 2012] 95.6 89.1(-6.5) 97.8(+2.2) 89.1(-6.5) 95(-0.6) - -
JAFFE
Bayesian [Miyakoshi and Kato 2011] 78.1 78.1(-0) 56.3(-21.8) - - - -
Gabor [Kotsia, et al. 2008] 93 88.5(-4.5) 87.4(-5.6) - - - -
DNMF [Kotsia, et al. 2008] 90.4 89.3(-1.1) 88.7(-1.7) - - - -
CK SVM [Kotsia, et al. 2008] 89.5 86.7(-2.8) 82.8(-6.7) - - - -
SA GSNMF [Zhi, et al. 2011] 95.3 94.7(-0.6) 93.3(-2) - - 94.7(-0.6) -
PCA [Towner and Slater 2007] 84 - - 82(-2) 74(-10) - -
LGBP [Azmi and Yegane 2012] 95.8 95.6(-0.2) 95.8(-0) 89.1(-6.7) 95.8(-0) - -
JAFFE
Bayesian [Miyakoshi and Kato 2011] 63.3 66.7(+3.4) 40(-23.3) - - - -
Gabor [Kotsia, et al. 2008] 96.7 90.8(-5.9) 93.3(-3.4) - - - -
DNMF [Kotsia, et al. 2008] 97 94.8(-2.2) 95.1(-1.9) - - - -
CK SVM [Kotsia, et al. 2008] 96.8 92.4(-4.4) 93.3(-3.5) - - - -
SU
GSNMF [Zhi, et al. 2011] 92.7 89.3(-3.4) 88.7(-4) - - 91.3(-1.4) -
PCA [Towner and Slater 2007] 99 - - 99(-0) 83(-16) - -
JAFFE LGBP [Azmi and Yegane 2012] 96.3 75.3(-21) 90(-6.3) 78.7(-17.6) 93.3(-3) - -
23Bayesian [Miyakoshi and Kato 2011] 90 83.3(-6.7) 70(-20) - - - -
NE JAFFE LGBP [Azmi and Yegane 2012] 99 98.7(-0.3) 99(-0) 98.7(-0.3) 74.3(-24.7) - -
Gabor [Kotsia, et al. 2008] 91.6 86.8(-4.8) 84.4(-7.2) - - - -
DNMF [Kotsia, et al. 2008] 86.7 84.2(-2.5) 82.9(-3.8) - - - -
SVM [Kotsia, et al. 2008] 91.4 88.4(-3) 86.7(-4.7) - - - -
CFDWL [Huang, et al. 2012] 93.2 93(-0.2) 79.1(-14.1) - 73.5(-19.7) - 86.8(-6.4)
CK
Gabor [Zhang, et al. 2014b] 95.3 95.1(-0.2) 90.8(-4.5) - - - 75(-20.3)
MCC [Buciu, et al. 2005] 93.6 87.2(-6.4) 92.3(-1.3) - - - -
PCA [Towner and Slater 2007] 75, 85 - - 70(-5) 82(-3) - -
SRC [Ouyang, et al. 2013] 97.7 72.4(-25.3) - - - - -
Overall Gabor [Kotsia, et al. 2008] 88.1 83.1(-5) 81.5(-6.6) - - - -
DNMF [Kotsia, et al. 2008] 85.2 82.5(-2.7) 81.5(-3.7) - - - -
LGBP [Azmi and Yegane 2012] 96.3 88.8(-7.5) 92.8(-3.5) 90.2(-6.1) 89.2(-7.1) - -
LGBPHS [Shuai-Shi, et al. 2013] 93.4 85.5(-7.9) 92.1(-1.3) - - - -
JAFFE Gabor [Zhang, et al. 2014b] 81.2 80.3(-0.9) 78.4(-2.8) - - - 48.8(-32.4)
MCC [Buciu, et al. 2005] 89.7 83.5(-6.2) 84(-5.7) - - - -
Bayesian [Miyakoshi and Kato 2011] 70.3 67.1(-3.2) 49.5(-20.8) - - - -
RPCA [Xia, et al. 2009] 87.5 68.8(-18.7) - - - - -
DBN [Cheng, et al. 2014] 85.7 82.9(-2.8) 82.9(-2.8) 77.1(-8.6) 82.9(-2.8) - -
Note: ‘-’ means values not available, and all accuracy reductions (with reference to non-occlusion) larger than 10% are highlighted in bold. Some approaches have higher accuracies under occlusion than non-
occlusion. Random block occlusion accounts for a quarter of the face area, and Ref. [Xia, et al. 2009] used a fusion set of JAFFE and BHUFE images.
Abbreviations: CFD - Combination of component-based Facial expression representation and fusion module, CFDWL - CFD based on Weight Learning, DBN – Deep Belief Network, DNMF- Discriminant Non-
negative Matrix Factorization, GSNMF - Graph-preserving Sparse Non-negative Matrix Factorization, LGBP - Local Gabor Binary Pattern, LGBPHS - LGBP Histogram Sequence, MCC- Maximum Correlation
Classifier.
245.3 Summary of Effect of Occlusion
From the above analysis, we can observe that most current investigations focus on 1) six basic emotions and neutral, 2)
occlusion of the mouth, eyes, left or right, upper or lower parts of the face, and random blocks, 3) JAFFE and CK databases,
and 4) emotional stimuli from actors/actresses. Few studies have investigated non-basic emotions such as contempt
[Huang, et al. 2012], and occlusion of noise [Bourel, et al. 2002], nose [Zhi, et al. 2011] and real-life sunglasses [Shuai-Shi,
et al. 2013].
Table 6 summarizes the effect of six types of occlusion on the six basic emotions plus neutral, as well as the overall
performance based on some of existing investigations on computer vision and human perception. The results are listed to
provide readers a first impression on the effect, as an exhaustive survey on this field is out of the scope of this paper. It
can be seen that:
 The left and right halves of the face equally effect the overall performance [Bourel 2001],[Kotsia, et al. 2008],[Bourel,
et al. 2002],[Shuai-Shi, et al. 2013].
 The mouth is the most important facial region and an occluded mouth has large effect on the classification of six
basic emotions and the overall performance, but relatively small effect on that of neutral.
 The eyes are the second most important facial region, and occluded eyes have large effect on the classification of
sadness, disgust, and surprise, but only small effect on that of happiness and neutral.
 The upper face has more impact on the classification of anger [Bourel, et al. 2002],[Towner and Slater 2007],[Azmi
and Yegane 2012], but small impact on the overall performance [Bourel 2001] and classification of fear.
 Occlusion of the lower face significantly impacts the overall performance [Towner and Slater 2007],[Huang, et al.
2012].
 Occlusion of the nose has large effect on the classification of sadness, but small effect on the overall performance.
 Occlusion of the eyebrows have small effect on classification of happiness, sadness, fear and disgust.
Overall, there is no absolute consensus on the effect of facial occlusion in both computer vision and human perception
experiments. Occlusion from the same facial part may exert different effects on the classification of the same emotion,
which implies the effect is largely context-dependent, and impacted by local factors specific to a particular experiment,
such as features, classifiers, participants, emotional stimuli and evaluation protocols.
Table 6. Summary of Effect of Occlusion on Classification of Six Basic Emotions + Neutral and Overall Performance.
Eyes Mouth Upper Lower Nose Brow
AN --++++ ---++++ ++ + + -
DI --+++++ -+++++ N/A -++ + --
FE --++ -++++++ - --+ -+ --
HA ---+ ---++++++ N/A + + ---
SA -+++++ -++++++++ -+ - ++ ---
SU --+++++ --++++ -+ - N/A -+
NE - ++ N/A + N/A N/A
Overall +++ ++++ - ++++ -- N/A
Note: ‘+’ and ‘-’ indicate large and small effect respectively. Multiple ‘+’ or ‘-’ represents the number of studies with a large or small
effect. Abbreviation: N/A – Not Available.
6 Challenges and Opportunities
Automatic FEA in partially occluded faces is a field that is just at its very beginning stage and has received relatively
less investigations previously. Thus, it is extremely important to discuss existing challenges that form the major obstacles
to the current progress and possible opportunities that should be paid special attention in promoting the future work. This
section presents key unresolved issues identified in Sections 4 and 5, and discuss possible solutions or opportunities to
address these issues. We limit our focus to only those issues closely related to handling facial occlusion, and for uncovered
issues with respect to face location, face normalization, face tracking, feature extraction and classifier design in non-
occluded faces, readers are referred to [Pantic and Rothkrantz 2000],[Fasel and Luettin 2003],[Gunes, et al. 2011],[Gunes
and Schuller 2013].
6.1 Database Creation and Labeling
Most current studies are based on JAFFE, CK or CK+ databases without occluded faces, and are restricted to a limited
number of single types of artificially generated occlusion and to facial expressions of six basic emotions plus neutral. This
is largely due to the lack of comprehensive benchmark datasets that include a dense set of various types of frequent
natural facial occlusion and well annotated labels of facial expressions. The creation of FEA databases with facial occlusion
is a more complicated and time-consuming work than that without occlusion, and it has to overcome the following issues:
251) Decision on what kinds of facial occlusion. It is generally agreed that occlusion from the most informative facial
regions, such as the mouth, eyes, eyebrows and nose, should be included. For a specific application, a certain part of the
face may become crucially important and needs to be considered individually and included as well. Even the facial parts
are determined, it is still unclear what types of specific objects should be utilized to occlude these parts and to what extent
the parts should be occluded in the procedure of occlusion simulation. For a given facial part, there may exist a wide range
of specific objects for selection and these objects may have substantially varied properties such as color, shape, size,
appearance, component and material. For instance, occlusion of the eyes can be simulated by asking subjects taking
different types of eyeshades or glasses, such as sunglasses, vision correction glasses, goggles, protective glasses, and
specified glasses (e.g. Google glasses). Whether a type of occlusion should be simulated by objects with the same property
or different properties, and whether the co-occurrence of multiple occlusion should be considered remain questions.
Although it is generally believed that objects with varied visual and physical properties should be included to simulate
real-life scenarios as closely as possible, the varieties presented in the occlusion may pose a big challenge for effective
training and tests of the FEA algorithms.
2) Collection of spontaneous expressions of facial emotions under occlusion. Human facial expression is a complex process,
involving psychological activities, cognitive understanding and physical behaviors that come together to create the
subjective experience. It is relatively easy to elicit some prototypical emotions such as happiness, sadness and surprise,
from subjects by natural face-to-face communications or showing them proper emotional stimuli. However, it becomes
increasingly difficult when moving beyond prototypical emotions to other uncommon context-dependent expressions that
are seldom used in the normal life and involve small subtle changes in facial components, such as contempt, curiosity and
attentiveness, particularly in the presence of facial occlusion. The occlusion that is either artificially superimposed on the
face or naturally occurring may significantly interfere subjects’ spontaneous reactions to emotional stimuli and influence
their natural ways of expressing facial expressions. This is because subjects may need a certain amount of time to get used
to the presence of occlusion, to accurately express facial movements, and to fully engage themselves to the emotional
stimuli and the contextual environment. Thus, the interference from occlusion may severely influence the reliability and
accuracy of elicited facial expressions.
Except for the common issues such as participant recruitment, participation agreement, ethics and copyright, there are
extra issues and protocols that should be considered during spontaneous facial expression elicitation and data collection.
First, the metadata of the database, such as the types of both occlusion and emotions, the number of subjects and the data
modality, should be determined based on the specific aims, tasks, or applications of the data collection. However, as a
starting point for current academic research, it may be a good idea to include the commonly used types such as six basic
emotions, and occlusions of the mouth, eyes, nose, top half, and bottom half of the face. While it is often beneficial to
include as many subjects as possible, the number of subjects participated should be controlled to a reasonable level which
fits well to the resources available to the data collection project such as budget, time, equipment, staff, etc. To avoid the
necessity of artificially adding occlusion to the face and to reduce the impact of occlusion on the elicited facial expressions,
it is advisable to select only those subjects who have a specific type of occlusion in their normal lives. However, this will
greatly limit the number of subjects and the types of occlusion that can be collected in real practice. With respect to data
modality, decisions should be made on whether the data should cover audio vs. visual modality, visual vs. thermal
modality, 2D vs. 3D data, static images vs. video sequences, body gestures, single vs. multiple faces, single vs. multiple
views, etc. or combinations of them. Next, to elicit facial expressions from subjects as spontaneously as possible, it is
essential to select proper simulating materials that can be relatively easier to arouse natural affective responses from the
subjects. The simulating methods can be story-telling, watching videos, playing games, attending social events, or face-to-
face communications, etc. Another important aspect is to provide a natural and relax environment, where the participants
are allowed to freely express their feelings and emotions ideally without any constraints on their activities, movements,
positions, gestures, and taking on or off the occlusion. Dependent on the specific requirement of the project, the
participants may or may not be told details about the collection procedure such as where, when, how and how long their
affective responses will be collected. For the purpose of comparing differences in a subject’s affective responses, it may be
also worth collecting two separate sets of data for the same subject - one before and the other after the subject is informed
of those details. After data collection, the collected data undergoes normal procedures such as post-processing, emotion
annotation, experimental validation, and finally may be made accessible by the public.
Spontaneous facial expression collection is not an easy task. To improve the accuracy of the elicited expressions and
label annotation, it is advisable to take into account a list of creative strategies recommended by psychological studies for
inducing emotions, particularly those subtle and contextual-dependent ones [Zeng et al. 2009]. However, this may
increase the difficulty of subject recruitment and greatly limit the type of occlusion that can be collected. The data
collection process is also largely hindered by the lack of awareness by engineers who are actually in charge of the whole
procedure. Rather than collecting spontaneous facial expressions in a controlled laboratory environment, recent studies
[Benitez-Quiroz et al. 2016], [Benitez-Quiroz et al. 2017] have shifted to collecting a large number of images of facial
expressions with associated emotion keywords from the Internet. This approach may also be used for collecting occluded
facial images with spontaneous expressions in ‘wild’ environments.
263) No criterion for how the occlusion should be annotated. Once the data of facial expressions with occlusion was
recorded, which properties of occlusion should be annotated and how to annotate them become a real challenge because
the occlusion may be the results of a wide range of objects with different properties. It is generally accepted that the
location of occlusion is the most important property that has the biggest impact on FEA, which can be annotated by
pixelwise labels of occluded regions. However, whether other properties of the occlusion, such as the specific type (e.g.
sunglasses or eyeglasses), the intensity of occlusion (e.g. 50% vs. 90% of face occluded), components (e.g. lens and frame),
materials (e.g. glass or plastic), colors (e.g. grey or green), transparency (e.g. 20% or 50%), and texture, as well as temporal
changes in these properties in the occluded region should be included in the annotation metadata is a question that is
probably dependent on the aim of the database. These properties may potentially impact the performance of FEA methods.
In the UMB 3D database [Colombo, et al. 2011], the type of object leading to the occlusion was annotated. In the HAPPEI
database [Dhall et al. 2013], the intensity of facial occlusion for each person was manually annotated as one of three levels,
including face visible, partial occlusion and high occlusion. The annotation can be used for evaluating the impact of
different levels of occlusion on both individual- and group-level affect. Studies [Dhall et al. 2015a], [Dhall, et al. 2015b]
have confirmed a big impact of occlusion on the perception and recognition of both levels of affect. Since the annotation
provides just a rough category of the occlusion amount on the face, it may not be suitable for situations where an accurate
percentage of occlusion is required. However, it is one of the earliest efforts towards the annotation of occlusion intensity
in real-life images with multiple people. Studies [Cotter 2010a],[Zhang, et al. 2014b] have also shown that black and white
occlusion lead to different classification accuracies, and occlusion of solid and clear eyeglasses also produce different
results. Once the properties are decided, another issue arising is that how they can be properly annotated and saved in
terms of ground truth labels and formats? One possible solution is to directly use raw pixels of occluded regions as ground
truths, but this may not enable detailed analysis of specific features of the occlusion.
4) Human labeling of emotion is a challenging and difficult task. Human generally have no problem of recognizing a set
of frequently occurring facial expressions (e.g., six basic emotions) from non-occluded clear faces in favorable conditions.
However, they may face increased difficulty of recognizing the same type (or even more subtle and mixed types) of facial
expression in the presence of obscured occlusion. Occlusion may significantly impact the accuracy and reliability of
emotion labeling by humans. Once the most informative facial region was occluded, human labelers may find hard to
correctly identify the predominant type of occlusion based on visual features in the remaining non-occluded parts. This
problem may be relatively easy to be solved for labeling the occluded face into the most frequently prototypical categories
of facial expressions, but may become challenging using uncommon emotion categories, FACS AUs, or continuous
emotional dimensions. The AUs reflect the subtle and local muscle changes in local facial components. Once the most
informative facial part for a certain AU is invisible due to occlusion, it is very difficult to accurately annotate the AU, and
this may also impact the annotation of other AUs because many AUs are closely correlated and many emotions are
represented via a combination of several AUs rather than a single AU. Emotion labeling using continuous dimensions
requires well-trained labelers and detailed quantification of multiple dimensions based on subjective human perception. A
simple solution is to leave the occlusion affected AUs or dimensions unlabeled. Another possible solution is to record both
occluded and non-occluded facial expressions for each subject in the same recording settings, and use the emotion labeling
in the non-occluded face as an estimation of the occluded face. Recent studies [Benitez-Quiroz, et al. 2016] have explored
automatic annotation of AUs, AU intensities and emotion categories for a large number (> a million) of images of facial
expressions collected from wild environments. However, the majority of the annotated images are unoccluded and facial
occlusion was not specifically handled in these studies.
Aside from the above concerns associated with data creation and ground truth labeling, the size of the collected
samples for each occlusion and each emotion, the closeness of the data to real-life scenarios, the time and expense costs,
accessibility, construction and administration of the database are also important issues for consideration. The acquisition
of 3D face data is also a critical step in motivating the investigation of 3D FEA models, particularly with the popularity of
RGB-D cameras such as Microsoft Kinect. Whether other impacting factors such as pose variations and illumination
changes should be jointly incorporated during the recording of facial occlusion is still a question that worthy considering.
It is still arguable whether the occlusion should be imposed after or before facial expression recordings. Artificially
imposed occlusion may solve some issues discussed above, but their capability of simulating real-life scenarios becomes a
big concern.
6.2 Occlusion Detection and System Integration
Most current approaches extract features directly from occluded facial images without incorporating a pre-processing
step of occlusion detection. They typically perform facial feature location, tracking, and extraction directly on the
occluded face, or incorporate human assisted processing to manually crop the face and register facial landmarks.
For automatic FEA systems, the presence of occlusion may lead to imprecise facial feature localization, erroneous
alignment or registration. The capacity of reliably determining the specific parameters of facial occlusion, such as the type,
location, shape, appearance and temporal duration, forms a critical component of FEA systems. Once the parameters of
occlusion were reliably measured or accurately determined, features in occluded parts can be either effectively
reconstructed from training data based on prior knowledge of the face configuration or simply discarded from extracted
27features to minimize its effect on the performance. Prior knowledge about the parameters of occlusion has been proved as
being crucial in boosting the performance of face recognition [Jongsun et al. 2005]. The benefit of incorporating occlusion
detection as a pre-processing step in FEA systems have been demonstrated in terms of noticeable improved performance
[Huang, et al. 2012].
Face occlusion detection and recovery is not a new field [Dahua and Xiaoou 2007], and tremendous efforts have been
made towards the investigation of algorithms for robust face region detection [Burgos-Artizzu, et al. 2013],[Lin and Liu
2006], facial landmark localization [Ghiasi and Fowlkes 2014] and tracking [Torre et al. 2015], as well as face alignment
[Heng et al. 2015], [Asthana et al. 2015] under partial occlusion. It is interesting to observe that recently developed face
analyzers such as Intraface [Torre, et al. 2015] and incremental face model [Asthana et al. 2014] have achieved promising
results of detecting, tracking and recognizing facial features even under moderate realistic occlusion. With those face
analyzers, it is arguable that many types of temporal occlusion may not be a problem anymore, however, in our view, this
progress nevertheless reduces the necessity and significance of designing occlusion detection techniques, which can
provide details about the parameters and characteristics of occlusion to support more specific post-processing and analysis.
The design of occlusion detectors is difficult primarily due to the random and varied characteristics of occlusion, and thus
gaining a good understanding of the local context of the occlusion in a specific situation, such as the number of subjects,
the type of occlusion, and the place (e.g. office or playground) becomes crucially important in simplifying the design
process and to some extent, largely determines the performance of the algorithm. Developing an occlusion detector
specific to a particular application might be a smarter choice than implementing a generic detector capable of detecting
any possible type of occlusion in real scenarios.
6.3 Other Features
Existing approaches to handling facial occlusion for FEA are largely based on 2D gray data. There are relatively few
efforts that specifically design 3D models to handle facial occlusion, and they are largely restricted to self-occlusion caused
by pose variations. Very few studies have considered skin color features. For robust FEA with facial occlusion handling, it
is desirable to adopt a richer set of representative and effective features.
1) 3D feature. Facial data in 3D provides additional depth information about the facial structure and appearance on top
of 2D image based facial features. It can be potentially utilized for generating more robust, discriminative, and view-
independent features under facial variations, such as head pose, missing parts and partial occlusion [Drira, et al. 2013]. By
giving insights into the comprehensive physical structure of the face, 3D features contain critical appearance and shape
information for assisting occlusion detection and restoration [Colombo et al. 2010], facial landmark localization and
recovery [Canavan et al. 2015],[Xi et al. 2011] and face alignment [Cao et al. 2014] in the presence of occlusion. They are
also critically important in building a richer set of reliable features for representing facial expressions, and eventually
leads to more accurate recognition. In the case that both occlusion and pose variations are present in the face region, 3D
features can compensate the effect arising from pose movements by reconstructing the face to a frontal view [Kangkan et
al. 2014], and simplify the problem to an occlusion only task. The generation of models using 3D facial features to handle
other types of common occlusions such as a scarf, a mark, and glasses, is still a field that needs further investigation.
2) Color feature. Another valuable feature for reliable FEA under occlusion is color. Although occlusion may present in
a form of varied colors, the skin color in the face can be roughly categorized into several big groups such as European,
Asian and Hispanic. The skin color can be utilized as complementary information to assist the segmentation of occluded
regions from the face and the extraction of a rich set of features. Skin color has already been successfully used to segment
facial regions from complicated background objects [Ban et al. 2014] and detect occluded regions from the face [Lin, et al.
2013],[Lin and Liu 2006]. With respect to FEA, physiological studies [Nakajima et al. 2017] indicated that skin color is a
useful clue for emotional states, for instance, the face often flushes during anger while goes pale for fear. Skin color also
influences the perception of facial expressions by human. In addition, computer vision studies have also proved the
usefulness of skin color for FEA. Ramirez et al. [2014] found that facial skin color is a reliable feature for inferring the
valence of emotional states using machine learning algorithms. Studies [Lajevardi and Hong Ren 2012] have shown that
skin color components convey additional features in achieving more robust and effective recognition of facial expressions
in images with low-resolution or illumination variations. However, to our best knowledge, there is not work yet that
directly utilized skin color features to handle facial occlusion for FEA. Thus, a possible future direction in this field is to
explore the ways of using color features to detect and recover facial occlusion.
3) Temporal feature. Temporally dynamic features reflecting subtle or sudden spatio-temporal facial muscle movements
in video sequences are also important for accurate FEA. Human facial expressions involve complex facial muscle
interactions in both the space and time domains [Ziheng et al. 2013], and facial component and head motions can provide
crucial information in assisting the human recognition of certain facial expressions [Bassili 1979],[Nusseck, et al. 2008].
For systematic occlusion (e.g. sunglasses) whose location is often roughly fixed in a certain region of the face, it is
relatively easy to extract the dynamic movements of facial features in non-occluded facial parts. For temporary occlusion
(e.g. hands moving across the face) whose location is usually varied, the missing facial features due to occlusion in a
current frame can be recovered using temporal correlation reasoning on information in neighboring frames [Yongmian
and Qiang 2005],[Miyakoshi and Kato 2011].
286.4 Multiple Modalities and Deep Learning
For occlusion-robust emotion analysis, the structure of the FER system can be extended from combining multiple
modalities (going wider), exploiting multiple layers in a deep architecture (going deeper), or fusing both of them (going
wider and deeper). Nearly all of existing studies on FEA under occlusion focus on visual features from the face only, and
are limited to using a single deep learning architecture. It is anticipated that building a wider and/or deeper structure can
lead to more robust performance.
1) Multiple modalities. It is still an unexplored field that exploits temporal correlations between multiple modalities and
incorporates fused features from them in combating facial occlusion towards FEA. Human expression of emotions is often
the result of interaction and collaboration between multiple modalities of human reactions, such as emotional voice, facial
expressions, body gestures, head and shoulder movements, gaze direction, and physiological signs. Fused features from
audio, visual, text, or physiological modalities have been extensively used for emotion analysis, yielding boosted
performance compared to using a single modality alone [Zeng, et al. 2009],[Calvo and D'Mello 2010]. The great advantage
of incorporating multiple modalities is that features from these modalities can be fully utilized to compensate the
drawbacks of each other to generate more robust methods for handling occlusion. Because facial occlusion mainly impacts
visual features in the face and normally has limited impact on audio, body gestures and physiological signals, integrating
features in less-impacted modalities with facial features is anticipated to be able to identify emotions more robustly from
an occluded face.
Several important issues need to be considered carefully for the fusion of multiple modalities [Zeng, et al. 2009], such
as the selection of reliable modalities, the synchronization between signals with different characteristics (e.g. time scale,
metric level, and temporal structure), extraction of discriminative temporal features from the raw signals, construction of
joint features from multiple modalities, and fusion of classification decisions. Recent studies [Mahmoud et al. 2014] on the
classification of hand-over-face gesture cues in naturalistic facial expression video are good examples towards robust
emotion recognition using fusion of hand gestures and facial expressions under hand occlusion.
2) Deep learning. As reviewed in Section 4.6, current FEA studies on utilizing deep learning to handling facial occlusion
are largely restricted to a single deep architecture. The potential capacity of adopting a wider and/or deeper structure
using multiple data modalities has been evidenced as being the winners of the Wild Challenge and Workshop (EmotiW)
from 2013 to 2016 [Kahou et al. 2013], [Liu et al. 2014a], [Fan et al. 2016]. Among them, Kahou et al. [2013] integrated four
deep neural networks in a unique system for emotion recognition in video by capturing facial expressions, audio
information, spatio-temporal patterns of human actions, and features in the mouth region. Liu et al. [Liu, et al. 2014a]
mapped hand-crafted HOG, dense SIFT features, and CNN features into Riemannian manifolds and adopted a score-level
fusion of three visual classifiers and an audio predictor for emotion recognition. Fan et al. [Fan, et al. 2016] presented a
hybrid network that fuses RNNs, 3D convolutional networks, and an SVM based audio system in a late-fusion fashion for
extracting appearance in individual frames, motion between frames, and audio information. It is noted that facial
occlusion was not specifically handled in these work. However, it is anticipated that there will be a growing number of
studies on investigating more complicated deep learning structures for handling occlusion in FEA.
6.5 A Few Additional Issues
1) Multi-disciplinary experiment. FEA is an inherently multi-disciplinary field and its progress is predominantly
dependent on supports, knowledge and advance in closely related fields, including psychology, cognitive science,
psychiatry and computer science. As the performance of contemporary machine systems is still far behind the innate
recognition capability of the human, further investigations on the mechanisms of humans’ recognition behaviors in the
presence of facial occlusion may be a crucially important step in gaining invaluable insights and relevant knowledge that
can potentially inspire the way of designing reliable FEA systems. For instance, it is still not fully understood whether
humans primarily adopt holistic or component-based strategies for recognizing expressions from partially occluded faces,
and how the human brain instantaneously recovers features obscured by occlusion. Studies [Hammal, et al. 2009] revealed
that the human tends to use “suboptimal” features for FEA under occlusion. It is advisable to conduct both computer
vision and human perception experiments on the same type of occlusion and the same dataset so that the results can be
directly comparable and novel insights can be obtained. These insights can be used, for instance, to focus on extracting
features from the most important facial parts for a specific expression.
2) Context. As reviewed in Section 5.3, the effect of partial occlusion on the classification of facial expressions is largely
context-dependent. The local environment (i.e. context) in which facial occlusions are imposed, facial expressions are
elicited and simultaneously recorded may have big impact on the collected expressions in terms of spontaneity and
exaggeration levels. The context also carries prior knowledge about specific parameters (e.g. type, location, appearance
and time) of the occlusion that is going to occur, which is critically important for all procedures of occlusion detection,
feature extraction, and emotion classification in a FEA system specifically designed for handling this type of occlusion.
The contextual information may include the place, time, surrounding people and background in the recording
environment, as well as the subject’s personal backgrounds, such as the age, gender, job, culture and habit. Recent studies
[Rudovic et al. 2015] on context-sensitive modelling of the AU intensity with respect to six context questions (who, when,
29what, where, why and how) achieve substantially improved accuracies than without considering the effects of the context.
The incremental face model [Asthana, et al. 2014], which is capable of automatically tailor itself to fit specific person and
imaging conditions, has shown accurate face tracking even under temporal occlusion, fast head movement, shadow, and
pose variation.
3) Group-level expression analysis. Recently, increasing attention has been given to group-level expression analysis,
which aims to identify the type and intensity of emotions from images of a group of people. Aside from ordinary
occlusion such as sunglasses, hat and beard, one frequently occurring occlusion in the images is partial facial occlusion
due to the presence of another person standing in front of the face. The presence of these occlusion was found as one of
the key attributes that affects the perception of the emotion of a group [Dhall, et al. 2015b]. It has also been shown that
people tend to select images with less occlusion of the face in the process of identifying happiness intensity of a group
[Dhall et al. 2015a]. One advantage of group-level expression analysis is that, even the face of one (or more) person is
partially occluded, the dominant emotion of the group still can be inferred by fusing facial information of all group
members in conjunction with the holistic scene context. Such a ‘fusion’ strategy provides another angle of handling facial
occlusion in real-life scenarios, and has been adopted in recent methods for group-level expression analysis, particularly in
the EmotiW 2016. The fusion can be generally either in feature-level or decision-level. In [Li et al. 2016], holistic features
from the whole image scene and local features from multiple faces were learnt using a ResNet-18, and further aggregated
to a feature vector using a LSTM. The aggregated feature vector was fed to linear or ordinal regression to predict group-
level happiness intensities. In [Huang et al. 2015], Riesz-based Volume LBP features were extracted to represent the local
attribute of each face, and relative sizes and distances between all faces were used to represent the global attribute of the
scene. They were concatenated and fed to continuous conditional random fields for predicting the group mood. As for
decision-level fusion, [Dhall, et al. 2015b] extracted Bag of Words representations of different features from multiple faces
and GIST and census transform histogram features from the scene. Each feature modality was considered as a separate
kernel and all kernels were linearly combined using a Multiple Kernel Learning (MKL) to predict happiness intensity. In
[Sun et al. 2016], facial features were learnt by applying CNN and LSTM consequently to the detected face region. The
features of each face were used to train an individual SVM classifier, and the prediction results of all SVMs were combined
using a decision-level fusion network. Although the above works were not designed specifically to handle facial occlusion,
the occluded part was inherently considered by the feature extraction or fusion techniques such as CNN, LSTM, and MKL.
4) International competition. The international competitions for FEA under non-occluded faces have been very active in
recent years, such as the FERA 2011, 2015 and 2017 challenges [Valstar et al. 2015], [Valstar, et al. 2017], which focus on
expression recognition using discrete emotion categories and estimating the occurrence and intensity of AUs, the
Audio/Visual Emotion Challenge and Workshop (AVEC) 2011-2016 [Ringeval et al. 2015],[Valstar et al. 2016], which focus
on using continuous dimension representations, and the EmotiW 2013-2016 [Dhall et al. 2015c], [Dhall et al. 2016a] which
focus on using data collected from the wild. With recent studies on FEA have shifted towards using continuous
representations [Kaltwang et al. 2015] and spontaneous emotions in a wild environment for practical applications [Zhang,
et al. 2014a],[Zhang et al. 2016], there are still very few international competitions that are specifically designed to
compare FEA systems with partially occluded face data. The EmotiW 2016 includes a group-level emotion recognition
sub-challenge, which aims to compare methods for predicting the happiness intensity of a group of people. The
benchmark HAPPEI database covers various types of realistic occlusion (e.g., sunglasses, hat, and a people standing in
front of another and partially occluding the face) and meta-data of three intensities of facial occlusion, which makes it
possible to evaluate the effect of occlusion on the perception and recognition of emotions of a group [Dhall, et al. 2015a].
The most recent EmotiW 2017 has incorporated images with similar occlusion and three emotions (e.g., positive, negative
and neutral) from the Group Affect database. Another recent international competition - FERA 2017 has also started to
consider self-occlusion caused by pose variations. It is anticipated that joint initiatives similar to FERA, AVEC and
EmotiW will greatly promote the research in this direction by providing a common platform for performance evaluations,
such as benchmark datasets, predefined targets, train-test guidelines, evaluation procedures, performance measures, and
baseline approaches.
7 Conclusion
This paper presents a survey on the state-of-the-art efforts and a discussion about relevant challenges and
opportunities for handling partial occlusion towards automatic Facial Expression Analysis (FEA). In the last decade, while
an increased amount of studies have been recorded on handling occlusion, most FEA systems capable of overcoming
occlusion are still at the early stage, characterized by a very limited number of prototypical emotion categories and
artificially generated occlusion. Features are completely restricted to the visual face modality only, and evaluations are
primarily based on 2D/3D frontal faces. Amongst all types of existing approaches reviewed, the sparse representation and
deep learning approaches have demonstrated the most impressive results in combating facial occlusion.
Existing studies on FEA under partial occlusion still lack of:
- comprehensive benchmark datasets that include a dense set of various types of frequent natural facial occlusion and
well annotated ground truths of facial expressions by not only discrete categories, but also AUs and dimensional axes;
30- work on designing face occlusion detection techniques to reliably determine the specific parameters of facial
occlusion, such as the type and location;
- investigations on exploiting temporal correlations between multiple modalities and incorporating fused features from
multiple modalities in combating facial occlusion;
- efforts on thoroughly investigating the effect of facial occlusion on the performance of non-prototypical spontaneous
emotions across multiple realistic datasets.
Future FEA systems in handling facial occlusion are expected to expand from:
- artificially imposed to naturally occurring occlusion;
- 2D to 3D face databases;
- manual face pre-processing to automatic occlusion detection and integration;
- static 2D grey to temporal 3D color features;
- a single face to multiple faces of a group of people;
- a single face modality to multiple audio, visual and physiological modalities;
- a shallow architecture to deeper and wider architectures;
- prototypical emotions to AU-coded, continuously represented emotions, and micro-expressions.
With the emergence of more comprehensive benchmark datasets and the launch of international joint efforts and
initiatives, new algorithms will be developed subsequently, eventually leading to automated machine systems that can
support FEA applications in unconstrained conditions including the presence of occlusion. As a multi-discipline field, FEA
can benefit substantially from advancement in the knowledge in closely related areas of computer science, psychology,
cognitive science, neuroscience, etc. Occlusion as a challenge is not specific to FEA, but also exists in relevant fields such
as face recognition, face detection and face tracking. Studies in these fields share many common techniques, knowledge,
issues, and challenges, and thus any progress in one field can potentially benefit to other fields. A promising direction for
further research is the development of context-sensitive FEA algorithms that take into account the prior knowledge of the
local environment to predict the specific parameters of facial occlusion. It is also worthy conducting further investigations
on the power of deep learning techniques in recovering and handling facial occlusion inherently without human manual
intervention.
References
AMIRIAN, M., et al. 2017. Support Vector Regression of Sparse Dictionary-Based Features for View-Independent Action
Unit Intensity Estimation. 12th IEEE International Conference on Automatic Face & Gesture Recognition, 854-859.
ASTHANA, A., et al. 2014. Incremental Face Alignment in the Wild. IEEE Conference on Computer Vision and Pattern
Recognition, 1859-1866.
ASTHANA, A., et al. 2015. From Pixels to Response Maps: Discriminative Image Filtering for Face Alignment in the Wild.
IEEE Transactions on Pattern Analysis and Machine Intelligence 37, 6, 1312-1320.
AZEEM, A., et al. 2014. A survey: face recognition techniques under partial occlusion. International Arab Journal of
Information Technology 11, 1, 1-10.
AZMI, R. and YEGANE, S. 2012. Facial expression recognition in the presence of occlusion using local Gabor binary
patterns. 20th Iranian Conference on Electrical Engineering, 742-747.
BAN, Y., et al. 2014. Face detection based on skin color likelihood. Pattern Recognition 47, 4, 1573-1585.
BASSILI, J. N. 1979. Emotion recognition: The role of facial movement and the relative importance of upper and lower
areas of the face. Journal of Personality and Social Psychology 37, 11, 2049-2058.
BATISTA, J. C., et al. 2017. AUMPNet: Simultaneous Action Units Detection and Intensity Estimation on Multipose Facial
Images Using a Single Convolutional Neural Network.12th IEEE International Conference on Automatic Face & Gesture
Recognition, 866-871.
BENITEZ-QUIROZ, C. F., et al. 2017. EmotioNet Challenge: Recognition of facial expressions of emotion in the wild.
arXiv:1703.01210.
BENITEZ-QUIROZ, C. F., et al. 2016. EmotioNet: An Accurate, Real-Time Algorithm for the Automatic Annotation of a
Million Facial Expressions in the Wild. IEEE Conference on Computer Vision and Pattern Recognition, 5562-5570.
BETTADAPURA, V. 2012. Face Expression Recognition and Analysis: The State of the Art. arXiv:1203.6722.
BOUCHER, J. D. and EKMAN, P. 1975. Facial Areas and Emotional Information. Journal of Communication 25, 2, 21-29.
BOUREL, F., et al. 2002. Robust facial expression recognition using a state-based model of spatially-localised facial
dynamics. Fifth IEEE International Conference on Automatic Face and Gesture Recognition, 106-111.
BOUREL, F., CHIBELUSHI, C.C., LOW, A.A. 2001. Recognition of facial expressions in the presence of occlusion. 12th
British Machine Vision Conference, 213-222.
BUCIU, I., et al. 2005. Facial expression analysis under partial occlusion. IEEE International Conference on Acoustics,
Speech, and Signal Processing, 453-456.
BURGOS-ARTIZZU, X. P., et al. 2013. Robust Face Landmark Estimation under Occlusion. IEEE International Conference
on Computer Vision, 1513-1520.
CALVO, R. A. and D'MELLO, S. 2010. Affect Detection: An Interdisciplinary Review of Models, Methods, and Their
Applications. IEEE Transactions on Affective Computing, 1, 1, 18-37.
31CANAVAN, S., et al. 2015. Landmark localization on 3D/4D range data using a shape index-based statistical shape model
with global and local constraints. Comput. Vis. Image Underst. 139, 136-148.
CAO, X., et al. 2014. Face Alignment by Explicit Shape Regression. International Journal of Computer Vision 107, 2, 177-190.
CHANG, W. Y., et al. 2017. FATAUVA-Net: An Integrated Deep Learning Framework for Facial Attribute Recognition,
Action Unit Detection, and Valence-Arousal Estimation. IEEE Conference on Computer Vision and Pattern Recognition
Workshops, 1963-1971.
CHENG, Y., et al. 2014. A Deep Structure for Facial Expression Recognition under Partial Occlusion. Tenth International
Conference on Intelligent Information Hiding and Multimedia Signal Processing, 211-214.
COLOMBO, A., et al. 2010. Three-Dimensional Occlusion Detection and Restoration of Partially Occluded Faces. Journal
of Mathematical Imaging and Vision, 1-15.
COLOMBO, A., et al. 2011. UMB-DB: A database of partially occluded 3D faces. IEEE International Conference on
Computer Vision Workshops, 2113-2119.
CORNEANU, C. A., et al. 2016. Survey on RGB, 3D, Thermal, and Multimodal Approaches for Facial Expression
Recognition: History, Trends, and Affect-Related Applications. IEEE Transactions on Pattern Analysis and Machine
Intelligence 38, 8, 1548-1568.
CORNEJO, J. R., et al. 2015. Facial Expression Recognition with Occlusions Based on Geometric Representation,
Iberoamerican Congress on Pattern Recognition, 263-270.
CORNEJO, J. Y. R. and PEDRINI, H. 2016. Recognition of occluded facial expressions based on CENTRIST features. IEEE
International Conference on Acoustics, Speech and Signal Processing, 1298-1302.
COTTER, S. F. 2010a. Sparse Representation for accurate classification of corrupted and occluded facial expressions. IEEE
International Conference on Acoustics Speech and Signal Processing, 838-841.
COTTER, S. F. 2010b. Weighted Voting Of Sparse Representation Classifiers For Facial Expression Recognition. 18th
European Signal Processing Conference, 1164-1168.
COTTER, S. F. 2011. Recognition of occluded facial expressions using a Fusion of Localized Sparse Representation
Classifiers. IEEE Digital Signal Processing Workshop and IEEE Signal Processing Education Workshop, 437-442.
DAHUA, L. and XIAOOU, T. 2007. Quality-Driven Face Occlusion Detection and Recovery. IEEE Conference on Computer
Vision and Pattern Recognition, 1-7.
DAPOGNY, A., et al. 2016. Confidence-Weighted Local Expression Predictions for Occlusion Handling in Expression
Recognition and Action Unit detection. arXiv preprint arXiv:1607.06290.
DHALL, A., et al. 2015a. Automatic group happiness intensity analysis. IEEE Transactions on Affective Computing 6, 1, 13-
26.
DHALL, A., et al. 2017. From Individual to Group-level Emotion Recognition: EmotiW 5.0. Proceedings of the 19th ACM
International Conference on Multimodal Interaction (in press).
DHALL, A., et al. 2016a. Emotion recognition in the wild challenge 2016. 18th ACM International Conference on Multimodal
Interaction, 587-588.
DHALL, A., et al. 2016b. EmotiW 2016: video and group-level emotion recognition challenges. 18th ACM International
Conference on Multimodal Interaction, 427-432.
DHALL, A., et al. Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark. IEEE
International Conference on Computer Vision Workshops, 2106-2112.
DHALL, A., et al. 2012. Collecting Large, Richly Annotated Facial-Expression Databases from Movies. IEEE MultiMedia 19,
3, 34-41.
DHALL, A., et al. 2013. Finding Happiest Moments in a Social Context. 11th Asian conference on Computer Vision, 613-626.
DHALL, A., et al. 2015b. The more the merrier: Analysing the affect of a group of people in images. 11th IEEE International
Conference and Workshops on Automatic Face and Gesture Recognition, 1-8.
DHALL, A., et al. 2015c. Video and Image based Emotion Recognition Challenges in the Wild: EmotiW 2015. ACM
International Conference on Multimodal Interaction, 423-426.
DRIRA, H., et al. 2013. 3D Face Recognition under Expressions, Occlusions, and Pose Variations. IEEE Transactions on
Pattern Analysis and Machine Intelligence 35, 9, 2270-2283.
DUNJA, M., et al. 2004. Feature selection using linear classifier weights: interaction with classification models. 27th annual
international ACM SIGIR conference on research and development in information retrieval, 234-241.
DUNLAP, K. 1927. The role of eye-muscles and mouth-muscles in the expression of the emotions, Clark University,
Worcester, Mass.
EKENEL, H. and STIEFELHAGEN, R. 2009. Why Is Facial Occlusion a Challenging Problem? International Conference on
Biometrics, 299-308.
EKMAN, P. 1994. Strong Evidence for Universals in Facial Expressions - A Reply to Russells Mistaken Critique. Psychol.
Bull. 115, 2, 268-287.
EKMAN, P. 2003. METT. Micro expression training tool. CD-ROM.
EKMAN, P., et al. 2002. Facial Action Coding System: The Manual on CD ROM.
EKMAN, P., et al. 2013. Emotion in the human face: Guidelines for research and an integration of findings, Elsevier.
EKMAN, P., FRIESEN, W. 1978. The Facial Action Coding System: A technique for the measurement of facial movement.
Consulting Psychologists Press, Palo Alto, CA, USA, 274-280.
32FAN, Y., et al. 2016. Video-based emotion recognition using CNN-RNN and C3D hybrid networks. ACM International
Conference on Multimodal Interaction, 445-450.
FASEL, B. and LUETTIN, J. 2003. Automatic facial expression analysis: a survey. Pattern Recognition 36, 1, 259-275.
GHIASI, G. and FOWLKES, C. C. 2014. Occlusion Coherence: Localizing Occluded Faces with a Hierarchical Deformable
Part Model. IEEE Conference on Computer Vision and Pattern Recognition, 1899-1906.
GUNES, H. and SCHULLER, B. 2013. Categorical and dimensional affect analysis in continuous input: Current trends and
future directions. Image and Vision Computing 31, 2, 120-136.
GUNES, H., et al. 2011. Emotion representation, analysis and synthesis in continuous space: A survey. IEEE International
Conference on Automatic Face & Gesture Recognition and Workshops, 827-834.
HALLIDAY, L. A. 2008. Emotion detection: can perceivers identify an emotion from limited information? Master Thesis,
University of Canterbury.
HAMMAL, Z., et al. 2009. Comparing a novel model based on the transferable belief model with humans during the
recognition of partially occluded facial expressions. Journal of Vision 9, 2, 1-19.
HANAWALT, N. G. 1942. The role of the upper and lower parts of the face as a basis for judging facial expressions: I. In
painting and sculpture. The Journal of General Psychology 27, 2, 331-346.
HENG, Y., et al. 2015. Robust Face Alignment Under Occlusion via Regional Predictive Power Estimation. IEEE
Transactions on Image Processing 24, 8, 2393-2403.
HU, Y., et al. 2008. Multi-view facial expression recognition. 8th IEEE International Conference on Automatic Face & Gesture
Recognition, 1-6.
HUANG, X., et al. 2015. Riesz-based Volume Local Binary Pattern and A Novel Group Expression Model for Group
Happiness Intensity Analysis. British Machine Vision Conference, 1-13.
HUANG, X., et al. 2012. Towards a dynamic expression recognition system under facial occlusion. Pattern Recognition
Letters 33, 16, 2181-2191.
IZARD, C. E., et al. 1979. Maximally discriminative facial movement coding system. University of Delaware, Instructional
Resources Center.
JIANG, B. and JIA, K.-B. 2011. Research of Robust Facial Expression Recognition under Facial Occlusion Condition.
International Conference on Active Media Technology, 92-100.
JONGSUN, K., et al. 2005. Effective representation using ICA for face recognition robust to local distortion and partial
occlusion. IEEE Transactions on Pattern Analysis and Machine Intelligence 27, 12, 1977-1981.
KAHOU, S. E., et al. 2013. Combining modality specific deep neural networks for emotion recognition in video. 15th ACM
on International conference on multimodal interaction, 543-550.
KALTWANG, S., et al. 2015. Doubly Sparse Relevance Vector Machine for Continuous Facial Behavior Estimation. IEEE
Transactions on Pattern Analysis and Machine Intelligence 38, 9, 1748 - 1761.
KANADE, T., et al. 2000. Comprehensive database for facial expression analysis. Fourth IEEE International Conference on
Automatic Face and Gesture Recognition, 46-53.
KANGKAN, W., et al. 2014. A Two-Stage Framework for 3D Face Reconstruction from RGBD Images. IEEE Transactions on
Pattern Analysis and Machine Intelligence 36, 8, 1493-1504.
KAPOOR, A., et al. 2003. Fully automatic upper facial action recognition. IEEE International Workshop on Analysis and
Modeling of Faces and Gestures, 195-202.
KELTNER, D., et al. 2003. Facial expression of emotion. Oxford University Press, New York, NY, US.
KENADE, T. 1973. Picture Processing System by Computer Complex and Recognition of Human Faces. Doctoral
Dissertation, Kyoto University.
KOTSIA, I., et al. 2008. An analysis of facial expression recognition under partial facial image occlusion. Image and Vision
Computing 26, 7, 1052-1067.
LAJEVARDI, S. M. and HONG REN, W. 2012. Facial Expression Recognition in Perceptual Color Space. IEEE Transactions
on Image Processing 21, 8, 3721-3733.
LI, H., et al. 2015. An efficient multimodal 2D + 3D feature-based approach to automatic facial expression recognition.
Comput. Vis. Image Underst. 140, 83-92.
LI, J., et al. 2016. Happiness level prediction with sequential inputs via multiple regressions. 18th ACM International
Conference on Multimodal Interaction, 487-493.
LI, X., et al. 2017a. Facial Action Units Detection with Multi-Features and -AUs Fusion. 12th IEEE International Conference
on Automatic Face & Gesture Recognition, 860-865.
LI, X., et al. 2017b. Towards Reading Hidden Emotions: A Comparative Study of Spontaneous Micro-expression Spotting
and Recognition Methods. IEEE Transactions on Affective Computing (in press).
LIJUN, Y., et al. 2006. A 3D facial expression database for facial behavior research. 7th International Conference on
Automatic Face and Gesture Recognition, 211-216.
LIN, D.-T. and LIU, M.-J. 2006. Face Occlusion Detection for Automated Teller Machine Surveillance. Pacific-Rim
Symposium on Image and Video Technology, 641-651.
LIN, J.-C., et al. 2013. Facial action unit prediction under partial occlusion based on Error Weighted Cross-Correlation
Model. IEEE International Conference on Acoustics, Speech and Signal Processing, 3482-3486.
33LIU, M., et al. 2014a. Combining Multiple Kernel Methods on Riemannian Manifold for Emotion Recognition in the Wild.
16th International Conference on Multimodal Interaction, 494-501.
LIU, P., et al. 2014b. Facial Expression Recognition via a Boosted Deep Belief Network. IEEE Conference on Computer
Vision and Pattern Recognition, 1805-1812.
LIU, S., et al. 2014c. Facial expression recognition under partial occlusion based on Weber Local Descriptor histogram and
decision fusion. 33rd Chinese Control Conference, 4664-4668.
LIU, S. S., et al. 2014d. Facial expression recognition under random block occlusion based on maximum likelihood
estimation sparse representation. International Joint Conference on Neural Networks, 1285-1290.
LUCEY, P., et al. 2010. The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-
specified expression. IEEE Conference on Computer Vision and Pattern Recognition Workshops, 94-101.
LYONS, M., et al. 1998. Coding facial expressions with Gabor wavelets. Third IEEE International Conference on Automatic
Face and Gesture Recognition, 200-205.
MAHMOUD, M., et al. 2011. 3D Corpus of Spontaneous Complex Mental States. International Conference on Affective
Computing and Intelligent Interaction, 205-214.
MAHMOUD, M. M., et al. 2014. Automatic Detection of Naturalistic Hand-over-Face Gesture Descriptors. 16th ACM
International Conference on Multimodal Interaction, 319-326.
MAJA, P., et al. 2005. Affective multimodal human-computer interaction. 13th annual ACM international conference on
Multimedia, 669-676.
MARTINEZ, B., et al. 2017. Automatic Analysis of Facial Actions: A Survey. IEEE Transactions on Affective Computing (in
press).
MIYAKOSHI, Y. and KATO, S. 2011. Facial emotion detection considering partial occlusion of face using Bayesian
network. IEEE Symposium on Computers & Informatics, 96-101.
MOORE, S. and BOWDEN, R. 2011. Local binary patterns for multi-view facial expression recognition. Comput. Vis. Image
Underst. 115, 4, 541-558.
NAKAJIMA, K., et al. 2017. Interaction between facial expression and color. Scientific Reports 7, 41019.
NGUYEN, D. T., et al. 2016. Human detection from images and videos: A survey. Pattern Recognition 51, 148-175.
NUSSECK, M., et al. 2008. The contribution of different facial regions to the recognition of conversational expressions.
Journal of Vision 8, 8, 1-23.
OUYANG, Y., et al. 2013. Robust automatic facial expression detection method based on sparse representation plus LBP
map. Optik - International Journal for Light and Electron Optics 124, 24, 6827-6833.
OWUSU, E., et al. 2015. Facial Expression Recognition–A Comprehensive Review. Fourth Edition of the International
Journal of Technology and Management Research 1, 4, 29-46.
PANTIC, M. and ROTHKRANTZ, L. J. M. 2000. Automatic analysis of facial expressions: the state of the art. IEEE
Transactions on Pattern Analysis and Machine Intelligence 22, 12, 1424-1445.
PATIL, H., et al. 2015. 3-D face recognition: features, databases, algorithms and challenges. Artif Intell Rev 44, 3, 393-441.
RAMIREZ, G. A., et al. 2014. Color Analysis of Facial Skin: Detection of Emotional State. IEEE Conference on Computer
Vision and Pattern Recognition Workshops, 474-479.
RANZATO, M., et al. 2011. On deep generative models with applications to recognition. IEEE Conference on Computer
Vision and Pattern Recognition, 2857-2864.
RINGEVAL, F., et al. 2015. AVEC 2015: The First Affect Recognition Challenge Bridging Across Audio, Video, and
Physiological Data. 5th ACM International Workshop on Audio/Visual Emotion Challenge, 3-8.
ROBERSON, D., et al. 2012. Shades of emotion: What the addition of sunglasses or masks to faces reveals about the
development of facial expression processing. Cognition 125, 2, 195-206.
RODRIGUEZ, P., et al. 2017. Deep Pain: Exploiting Long Short-Term Memory Networks for Facial Expression
Classification. IEEE Transactions on Cybernetics (in press).
RUCKMICK, C. A. 1921. A preliminary study of the emotions. Psychological Monographs 30, 3, 30-35.
RUDOVIC, O., et al. 2010. Regression-Based Multi-view Facial Expression Recognition. 20th International Conference on
Pattern Recognition, 4121-4124.
RUDOVIC, O., et al. 2015. Context-Sensitive Dynamic Ordinal Regression for Intensity Estimation of Facial Action Units.
IEEE Transactions on Pattern Analysis and Machine Intelligence 37, 5, 944-958.
RUSSELL, J. A. 1980. A circumplex model of affect. Journal of Personality and Social Psychology 39, 6, 1161-1178.
SANDBACH, G., et al. 2012. Static and dynamic 3D facial expression recognition: A comprehensive survey. Image and
Vision Computing 30, 10, 683-697.
SARIYANIDI, E., et al. 2015. Automatic Analysis of Facial Affect: A Survey of Registration, Representation, and
Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence 37, 6, 1113-1133.
SAVRAN, A., et al. 2008. Bosphorus Database for 3D Face Analysis. European Workshop on Biometrics and Identity
Management, 47-56 .
SHUAI-SHI, L., et al. 2013. Facial Expression Recognition under Partial Occlusion Based on Gabor Multi-orientation
Features Fusion and Local Gabor Binary Pattern Histogram Sequence. Ninth International Conference on Intelligent
Information Hiding and Multimedia Signal Processing, 218-222.
34SONG, G. and QIUQI, R. 2011. Facial expression recognition using local binary covariance matrices. 4th IET International
Conference on Wireless, Mobile & Multimedia Networks, 237-242.
SUN, B., et al. 2016. LSTM for dynamic emotion and group emotion recognition in the wild. 18th ACM International
Conference on Multimodal Interaction, 451-457.
SUN, Y. and YIN, L. 2008. Facial Expression Recognition Based on 3D Dynamic Range Model Sequences. European
Conference on Computer Vision, 58-71.
SUWA, M., et al. 1978. A preliminary note on pattern recognition of human emotional expression. International Joint
Conference on Pattern Recognition, 408-410.
TAN DAT, N. and RANGANATH, S. 2008. Tracking facial features under occlusions and recognizing facial expressions in
sign language. 8th IEEE International Conference on Automatic Face & Gesture Recognition, 1-7.
TAN DAT, N. and SURENDRA, R. 2008. Towards recognition of facial expressions in sign language: Tracking facial
features under occlusion. 15th IEEE International Conference on Image Processing, 3228-3231.
TANG, C., et al. 2017. View-Independent Facial Action Unit Detection. 12th IEEE International Conference on Automatic
Face & Gesture Recognition, 878-882.
TARIQ, U., et al. 2012. Multi-view Facial Expression Recognition Analysis with Generic Sparse Coding Feature. European
Conference on Computer Vision, 578-588 .
TORRE, F. D. L., et al. 2015. IntraFace. 11th IEEE International Conference and Workshops on Automatic Face and Gesture
Recognition, 1-8.
TŐSÉR, Z., et al. 2016. Deep Learning for Facial Action Unit Detection Under Large Head Poses. European Conference on
Computer Vision, 359-371.
TOWNER, H. and SLATER, M. 2007. Reconstruction and Recognition of Occluded Facial Expressions Using PCA. Affective
Computing and Intelligent Interaction, 36-47.
VALSTAR, M., et al. 2016. AVEC 2016: Depression, Mood, and Emotion Recognition Workshop and Challenge. 6th
International Workshop on Audio/Visual Emotion Challenge, 3-10.
VALSTAR, M. F., et al. 2015. FERA 2015 - second Facial Expression Recognition and Analysis challenge. 11th IEEE
International Conference and Workshops on Automatic Face and Gesture Recognition, 6, 1-8.
VALSTAR, M. F., et al. 2017. FERA 2017-Addressing Head Pose in the Third Facial Expression Recognition and Analysis
Challenge. arXiv preprint arXiv:1702.04174.
VIERIU, R. L., et al. 2015. Facial expression recognition under a wide range of head poses. 11th IEEE International
Conference and Workshops on Automatic Face and Gesture Recognition, 1, 1-7.
WRIGHT, J., et al. 2009. Robust Face Recognition via Sparse Representation. IEEE Transactions on Pattern Analysis and
Machine Intelligence 31, 2, 210-227.
WU, Q., et al. 2011. The Machine Knows What You Are Hiding: An Automatic Micro-expression Recognition System.
Affective Computing and Intelligent Interaction, 152-162.
XI, Z., et al. 2011. Accurate Landmarking of Three-Dimensional Facial Data in the Presence of Facial Expressions and
Occlusions Using a Three-Dimensional Statistical Facial Feature Model. IEEE Transactions on Systems, Man, and
Cybernetics, Part B: Cybernetics, 41, 5, 1417-1428.
XIA, M., et al. 2009. Robust facial expression recognition based on RPCA and AdaBoost. 10th Workshop on Image Analysis
for Multimedia Interactive Services, 113-116.
YIN, L., et al. 2008. A high-resolution 3D dynamic facial expression database. 8th IEEE International Conference on
Automatic Face & Gesture Recognition, 1-6.
YONGMIAN, Z. and QIANG, J. 2005. Active and dynamic information fusion for facial expression understanding from
image sequences. IEEE Transactions on Pattern Analysis and Machine Intelligence 27, 5, 699-714.
YU-LI, X., et al. 2006. Beihang University Facial Expression Database and Multiple Facial Expression Recognition.
International Conference on Machine Learning and Cybernetics, 3282-3287.
ZAFEIRIOU, S., et al. 2016. Facial Affect ""In-the-Wild"": A Survey and a New Database. IEEE Conference on Computer
Vision and Pattern Recognition Workshops, 1487-1498.
ZAFEIRIOU, S., et al. 2015. A survey on face detection in the wild: Past, present and future. Comput. Vis. Image Underst.
138, 1-24.
ZENG, Z., et al. 2009. A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions. IEEE
Transactions on Pattern Analysis and Machine Intelligence 31, 1, 39-58.
ZHALEHPOUR, S., et al. 2016. BAUM-1: A Spontaneous Audio-Visual Face Database of Affective and Mental States. IEEE
Transactions on Affective Computing 8, 3, 300-313.
ZHANG, K., et al. 2017. Facial Expression Recognition Based on Deep Evolutional Spatial-Temporal Networks. IEEE
Transactions on Image Processing 26, 9, 4193-4203.
ZHANG, L., et al. 2015. Adaptive facial point detection and emotion recognition for a humanoid robot. Comput. Vis. Image
Underst. 140, 93-114.
ZHANG, L., et al. 2011. Toward a more robust facial expression recognition in occluded images using randomly sampled
Gabor based templates. IEEE International Conference on Multimedia and Expo, 1-6.
ZHANG, L., et al. 2014a. Facial expression recognition experiments with data from television broadcasts and the World
Wide Web. Image and Vision Computing 32, 2, 107-119.
35ZHANG, L., et al. 2014b. Random Gabor based templates for facial expression recognition in images with facial occlusion.
Neurocomputing 145, 0, 451-464.
ZHANG, L., et al. 2016. Towards robust automatic affective classification of images using facial expressions for practical
applications. Multimedia Tools and Applications 75, 8, 4669–4695.
ZHANG, S., et al. 2012. Robust Facial Expression Recognition via Compressive Sensing. Sensors 12, 3, 3747-3761.
ZHANG, X., et al. 2014c. BP4D-Spontaneous: a high-resolution spontaneous 3D dynamic facial expression database. Image
and Vision Computing 32, 10, 692-706.
ZHAO, X., et al. 2011. Accurate Landmarking of Three-Dimensional Facial Data in the Presence of Facial Expressions and
Occlusions Using a Three-Dimensional Statistical Facial Feature Model. IEEE Transactions on Systems, Man, and
Cybernetics, Part B: Cybernetics 41, 5, 1417-1428.
ZHAO, X., et al. 2016. Automatic 2.5-D Facial Landmarking and Emotion Annotation for Social Interaction Assistance.
IEEE Transactions on Cybernetics 46, 9, 2042-2055.
ZHI, R., et al. 2011. Graph-Preserving Sparse Nonnegative Matrix Factorization With Application to Facial Expression
Recognition. IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics 41, 1, 38-52.
ZHOU, Y., et al. 2017. Pose-Independent Facial Action Unit Intensity Regression Based on Multi-Task Deep Transfer
Learning. 12th IEEE International Conference on Automatic Face & Gesture Recognition, 872-877.
ZIHENG, W., et al. 2013. Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression
Recognition. IEEE Conference on Computer Vision and Pattern Recognition, 3422-3429.
36"
113,115,Facial expression analysis with AFFDEX and FACET: A validation study,"['S Stöckli', 'M Schulte-Mecklenbeck', 'S Borer']",2018,309,"Affective Faces Database, Radboud Faces Database","classification, deep learning, facial expression recognition, machine learning, neural network",Picture Database (GAPED) and the Radboud Faces Database (RaFD) facial expression  analysis assumes that there is a direct link between emotion production and emotion recognition,No DOI,Behavior research …,https://pubmed.ncbi.nlm.nih.gov/29218587/,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,nih.gov,
114,116,"Facial expression classification based on SVM, KNN and MLP classifiers","['HI Dino', 'MB Abdulrazzaq']",2019,160,"Extended Cohn-Kanade, MMI Facial Expression","classification, classifier",The Extended Cohn-Kanade (CK+) dataset used as good data recourse to exam the  classification of human Facial Expression. Principal component analysis (PCA) used to reduce the,No DOI,2019 International Conference on …,http://ieeexplore.ieee.org/document/8723728/,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
115,117,Facial expression recognition,"['Y Tian', 'T Kanade', 'JF Cohn']",2011,255,"Affective Faces Database, Binghamton University 3D Facial Expression, Extended Cohn-Kanade, Japanese Female Facial Expression",facial expression recognition,"This chapter introduces recent advances in facial expression analysis and recognition. The first part discusses general structure of AFEA systems. The second part describes the problem space for facial expression analysis. This space includes multiple dimensions: level of description, individual differences in subjects, transitions among expressions, intensity of facial expression, deliberate versus spontaneous expression, head orientation and scene complexity, image acquisition and resolution, reliability of ground truth, databases, and the",No DOI,Handbook of face recognition,https://paperswithcode.com/task/facial-expression-recognition,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,paperswithcode.com,
116,118,Facial expression recognition ability among women with borderline personality disorder: implications for emotion regulation?,"['AW Wagner', 'MM Linehan']",1999,459,Japanese Female Facial Expression,facial expression recognition,"examined recognition of facial expressions of emo tion among women  21), compared to a  group of women with histories of  Japanese males, 2 Japanese females) for each of 7 different",No DOI,Journal of personality disorders,https://pubmed.ncbi.nlm.nih.gov/10633314/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,nih.gov,
117,119,Facial expression recognition and histograms of oriented gradients: a comprehensive study,"['P Carcagnì', 'M Del Coco', 'M Leo', 'C Distante']",2015,214,Radboud Faces Database,FER,"Automatic facial expression recognition (FER) is a topic of  (HOG) descriptor in the FER  problem, highlighting as this  with most commonly used FER frameworks was carried out. In the",No DOI,SpringerPlus,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4628009/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,nih.gov,
118,120,Facial expression recognition based on 3D dynamic range model sequences,"['Y Sun', 'L Yin']",2008,172,Binghamton University 3D Facial Expression,"FER, facial expression recognition","3D facial expression database [21], we extend the facial expression analysis to a dynamic  3D  In this paper, we propose a spatio-temporal 3D facial expression analysis approach for",No DOI,Computer Vision–ECCV 2008: 10th European …,https://link.springer.com/chapter/10.1007/978-3-540-88688-4_5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,springer.com,
119,121,Facial expression recognition based on facial components detection and hog features,"['J Chen', 'Z Chen', 'Z Chi', 'H Fu']",2014,133,Extended Cohn-Kanade,"classification, classifier",perform the facial expression classification. We evaluate our proposed method on the JAFFE  dataset and an extended Cohn-Kanade dataset. The average classification rate on the two,No DOI,International workshops on …,http://www.cedus.it/documents/SicurezzaUrbana/Videosorveglianza_e_altre_tecnologie_video_di_controllo/3ZChi_ACV-1.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,cedus.it,
120,122,Facial expression recognition by de-expression residue learning,"['H Yang', 'U Ciftci', 'L Yin']",2018,512,Oulu-CASIA,classification,that we target to exploit for expression classification.  The Oulu-CASIA database [33]  contains data captured under three  The Oulu-CASIA VIS has 480 video sequences taken from,No DOI,… of the IEEE conference on computer …,https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Facial_Expression_Recognition_CVPR_2018_paper.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,thecvf.com,
121,123,Facial expression recognition from world wild web,"['A Mollahosseini', 'B Hasani', 'MJ Salvador']",2016,103,"Acted Facial Expressions In The Wild, Expression in-the-Wild, Static Facial Expression in the Wild","FER, classification, classifier, deep learning, facial expression recognition, machine learning, neural network",networks can recognize wild facial expressions with an  of inthe-wild facial expressions by  querying different search en search engines for facial expression recognition. We trained two,No DOI,… pattern recognition …,https://arxiv.org/abs/1605.03639,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"Facial Expression Recognition from World Wild Web
AliMollahosseini1,BehzadHassani1,MichelleJ.Salvador1,
HojjatAbdollahi1,DavidChan2,andMohammadH.Mahoor1,2
1 DepartmentofElectricalandComputerEngineering
2 DepartmentofComputerScience
UniversityofDenver,Denver,CO
Ali.Mollahosseini@du.edu, Behzad.Hasani@du.edu, Michelle.Salvador@du.edu)
habdolla@du.edu, davidchan@cs.du.edu, and mmahoor@du.edu
Abstract photosareoftentakeninthewildundernaturalconditions
withvaryinganddiverseparameterssuchasscenelighting,
Recognizing facial expression in a wild setting has re- user’sheadpose,cameraview,imageresolutionandback-
mained a challenging task in computer vision. The World ground, subject’s gender, ethnicity, and facial expressions
Wide Web is a good source of facial images which most amongothers. Furthermore,thelabelsgivenbyusersusea
of them are captured in uncontrolled conditions. In fact, wide range of vocabulary that is commonly understood to
the Internet is a Word Wild Web of facial images with ex- describeemotions,facialattributes,andexpressions,ofthe
pressions. This paper presents the results of a new study pictures’contents. ThesephotosaretrulyWildimagesboth
oncollecting,annotating,andanalyzingwildfacialexpres- intermsoftheimagequality/conditionsandthelabelsgiven
sions from the web. Three search engines were queried byusers. Aninterestingquestionthatmayariseis,howthe
using 1250 emotion related keywords in six different lan- labelsgivenwildlytofacialimagesonthewebbygeneral
guages and the retrieved images were mapped by two an- usersareconsistentwiththesixbasicemotionsdefinedby
notatorstosixbasicexpressionsandneutral. Deepneural psychologists.
networks and noise modeling were used in three different
trainingscenariostofindhowaccuratelyfacialexpressions On the other hand, computer vision and machine learn-
can be recognized when trained on noisy images collected ingtechniquesforfacialexpressionrecognitionarefinding
fromthewebusingqueryterms(e.g. happyface,laughing their ways into the design of a new generation of Human-
man, etc)? The results of our experiments show that deep Computer Interfaces. In order to train a machine learn-
neuralnetworkscanrecognizewildfacialexpressionswith ing system, many researchers have created databases us-
anaccuracyof82.12%. inghumanactors/subjectsportrayingbasicemotions[9,25,
16]. However, most of the captured datasets mainly con-
tain posed expressions acquired in a controlled environ-
1.Introduction ment. This is mostly due to the fact that it is hard and
time consuming to collect unposed facial expression data
The World Wide Web (aka the Internet) has become a in lab settings. However, in real applications, the sys-
vast abundant source of information and data. Especially tem needs to capture and recognize spontaneous expres-
with the growth and use of social media and the availabil- sions, which involve different facial muscles, less exag-
ity of digital cameras on smart phones, people can easily geration/intensity and have different dynamics than posed
add data to the Internet by taking photos, writing a short expressions. Researchers who have created spontaneous
description, and immediately uploading them to the social expression databases have captured the human face spon-
media.Peopleaddmoreinformationtoeachphotobydoing taneously while watching a short video or filling a ques-
atag,like,dislike,orcommentonphotospostedbyfriends tionnaires [7, 17, 18]. However, the datasets are still cap-
or others on the Web. It is estimated that over 430 mil- tured in controlled lab settings (i.e. with the same illumi-
lionphotosareuploadedtoFacebookandInstagramservers nation, resolution, etc.) or have a limited number of sub-
every day [10, 4]. Among photos posted on the Web, fa- jects, ethnicities, and poses poorly representing the envi-
cial images have the highest incidents (e.g. selfies or self- ronmentandconditionsfacedinreal-worldsituations. Ex-
portrait images are very popular nowadays). These facial isting databases in the wild settings, such as SFEW [3] or
7102
naJ
5
]VC.sc[
3v93630.5061:viXraFER2013 [6], are also either very small or have low res- Interaction (HMI) systems [22]. Traditionally, automatic
olution without facial landmark points necessary for pre- facial expression recognition (AFER) methods consist of
processing. threemainsteps 1)registrationandpreprocessing, 2)fea-
Moreover, state-of-the-art machine learning algorithms ture extraction, and 3) classification. Preprocessing and
suchasDeepNeuralNetworkrequiresbigdatafortraining registration form an important part of the AFER pipeline.
andevaluationofthecorealgorithms. Givenalltheafore- Many studies have shown the advantages of using fa-
mentionedmotivations,thispaperpresentstheresultsofour cial image registration to improve classification accuracy
recent study with the aim of resolving the following ques- in both face identification and facial expression recogni-
tions: tion [8, 27]. In the feature extraction step, many methods
suchasHOG[17], Gaborfilters[14], Localbinarypattern
1. Howconsistentaretheexpressionlabelsgivenbygen-
(LBP) [31], facial landmarks [12], pixel intensities [19],
eralwebuserscomparedtothesixbasicexpressionla-
andLocalphasequantization(LPQ)[43],oracombination
belsannotatedbyexpertannotatorsonfacialimages?
of multiple features using multiple kernel learning meth-
ods [41, 42] have been proposed to extract discriminative
2. How accurately can a state-of-the-art algorithm clas-
features. Classification is the final step of most AFER
sify images when trained on facial images collected
techniques. Support vector machines [43], multiple kernel
from the web using query terms (e.g. happy face,
learning [41, 42], dictionary learning [20] etc. have been
laughingman,etc)?
showntohaveagreatperformanceinclassifyingdiscrimi-
Toaddressthesequestions,wecreatedadatabaseofin- nativefeaturesextractedfromthepreviousstage.
the-wildfacialexpressionsbyqueryingdifferentsearchen- Although,traditionalmachinelearningapproacheshave
gines(Google, BingandYahoo)Wethenannotatedasub- been successful when classifying posed facial expressions
setofimagesusingtwohumanannotatorsandshowedthe inacontrolledenvironment,theydonothavetheflexibility
general accuracy of the querying search engines for facial to classify images captured in a spontaneous uncontrolled
expressionrecognition. Wetrainedtwodifferentdeepneu- manner (“in the wild”) or when applied to databases for
ralnetworkarchitectureswithdifferenttrainingsettingsi.e. whichtheywerenotdesigned. Thepoorgeneralizabilityof
training on clean well-labeled data, training on a mixture traditionalmethodsisprimarilyduetothefactthatmanyap-
of clean and noisy data, and training on mixture of clean proachesaresubjectordatabasedependentandonlycapa-
andnoisydatawithanoisemodelingapproachusingagen- bleofrecognizingexaggeratedorlimitedexpressionssim-
eral framework introduced in [38]. In other words, given ilartothoseinthetrainingdatabase. ManyFERdatabases
theresultofannotations,thenoiselevelofeachsearchen- havetightlycontrolledilluminationandposeconditions. In
gineisestimatedasapriordistributiononthelabelsofour addition,obtainingaccuratetrainingdataisparticularlydif-
posteriorsetallowingforgreaterclassificationperformance ficult,especiallyforemotionssuchassadnessorfearwhich
when we sample noisy labels and true labels in the same areextremelydifficulttoaccuratelyreplicateanddonotoc-
proportion. In order to achieve this, we learned a stochas- curofteninreallife.
ticmatrixwheretheentriesaretheprobabilityofconfusion
Recently,facialexpressiondatasetswithinthewildset-
in the labels. From this matrix, we can extract a posterior
tingshaveattractedmuchattention.Dhalletal.[2]released
distributiononthetruelabelsofthedataconditionedonthe
ActedFacialExpressionsintheWild(AFEW)frommovies
true label given the noisy label, and the noisy label given
by semi-automatic approach via a recommender system
the acquired data. For more information on the technique,
based on subtitles. AFEW addresses the issue of tempo-
see[38].
ral facial expressions and it is the only temporal publicly
Therestofthispaperisorganizedasfollows. Section2
available facial expression database in the wild. A static
reviewsexistingdatabasesandstate-of-the-artmethodsfor
subsetStaticFacialExpressionsintheWild(SFEW)iscre-
facialexpressionrecognitioninthewild.Sec.3explainsthe
atedbyselectingstaticframeswhichcoversunconstrained
methodologyofautomaticallycollectingalargeamountof
facialexpressions,differentheadposes,agerange,andoc-
facialexpressionimagesfromtheInternetandprocedureof
clusionsandclosetorealworldilluminations. However,it
verifyingthembytwoexpertannotators. Section4presents
contains only 1635 images and there are only 95 subjects
experimental results on training two different network ar-
inthedatabase. Inaddition,duetothewildsettingsofthe
chitectures with different training settings, and section 5
database,thereleasedfaciallocationandlandmarksdonot
concludesthepaper.
capturethefacesinallimagescorrectlymakingsometrain-
ingandtestsamplesunusable(SeeFig.1).
2.FacialExpressionRecognitioninthewild
The Facial Expression Recognition 2013 (FER-2013)
Automatic Facial Expression Recognition (FER) is an database was introduced in the ICML 2013 Challenges in
important part of social interaction in Human-Machine- RepresentationLearning[6]. Thedatabasewascreatedus-tained using neural networks in the fields of visual object
recognition[13,33],humanposeestimation[36],faceveri-
fication[34],andmanymore. EvenintheFERfieldresults
sofarhavebeenpromising[11,21,15,11],andmostofthe
facial expression recognition challenge winners have used
Angry Sad Disgust Happy
deepneuralnetworks[35,40].
In the FER problem, however, unlike visual object
databasessuchasimageNet[1],existingFERdatabasesof-
ten have limited numbers of subjects, few sample images
or videos per expression, or small variation between sets,
makingneuralnetworkssignificantlymoredifficulttotrain.
Figure 1. Sample of images from SFEW [3] and their original
Forexample,theFER2013database[6](oneofthelargest
registeredimagespublishedwiththedatabase.
recentlyreleasedFERdatabases)contains35,887imagesof
differentsubjectsyetonlylessthan2%oftheimagespor-
traydisgust.Similarly,theCMUMultiPIEfacedatabase[9]
contains around 750,000 images but is comprised of only
337 different subjects, where 348,000 images portray only
a “neutral” emotion and the remaining images do not por-
trayanger,fearorsadness.
Inarecentstudy[21],theauthorsproposedadeepneu-
ral network architecture and combined seven well-known
Angry Disgust Fear Happy Sad Surprise Neutral
facial expression databases (i.e. MultiPIE, MMI, CK+,
Figure2. SampleofimagesfromFER2013database[6].
DISFA,FERA,SFEW,andFER2013)toperformanexten-
sivestudyonsubject-independentandcrossdatabase. The
results of the proposed architecture were comparable to or
ing the Google image search API that match a set of 184
better than the state-of-the-art methods, However, the ma-
emotion-related keywords to capture the six basic expres-
jority of data were still posed images and performance on
sions as well as the neutral expression. Human labelers
wilddatabases(SFEWandFER2013)wereonlycompara-
rejected incorrectly labeled images. Images are resized
bletothestate-of-the-artmethods.
to 48x48 pixels and converted to grayscale. The result-
Considering the need to develop an automated FER in
ing database contains 35,887 images most of them in wild
wild system, and issues with the current facial expression
settings, yet only 547 of the images portray disgust. Fig-
in wild databases, a possible solution is to automatically
ure 2 shows some sample images of FER2013. FER2013
collect a large amount of facial expression images from
iscurrentlythebiggestpubliclyavailablefacialexpression
theabundantimagesavailableontheInternet, anddirectly
databaseinwildsettings,enablingmanyresearcherstotrain
use them as ground truth to train deep models. However,
machinelearningmethodswherelargeamountsofdataare
considerationshouldbe donetoavoidfalsesamples in the
neededsuchasDeepneuralnetworks. However,asshown
searchengineresultsforexpressionssuchasdisgustorfear.
in Fig. 2, the faces are not registered, and unfortunately
This is due to the higher tendancy of people to publish
mostoffaciallandmarkdetectorsfailtoextractfacialland-
happyorneutralfacesthatcanbemislabeledorassociated
marksatthisresolutionandquality.
withdisgustorfearbywebusers.
Inaddition,FERinthewildisreallyachallengingtask
Nonetheless, semi-supervised [37], transfer learn-
both in terms of machine and human performance. Exten-
ing[24],ornoisemodelingapproaches[32,38]canbeused
siveexperimentsin[5]showthatevenhumansareonlyca-
totraindeepneuralnetworkswithnoisydatabyobtaining
pable of 53% agreement in terms of Fleiss kappa over all
large amounts of facial expression images from search en-
classes to classify AFEW video clips without listening to
gines,alongwithasmallersubsetoffullywell-labeledim-
the audio track. State-of-the-art automated methods have
ages.
achieved35%accuracyonAFEWvideoclipsbyusingau-
diomodalities[44]. Evenrecognizingexpressionfromstill
3.Facialexpressionsfromthewild web
images or static frames using traditional machine learning
approaches are not accurate and the best performance on Tocreateourdatabasewiththelargeramountofimages
SFEW 2.0 database is reported as 50% accuracy (with a necessaryforDeepNeuralNetworks, threesearchengines
baselineof39.13%) [44]. were queried by facial emotion related tags in six differ-
Recently, deep neural networks have seen a resurgence ent languages. We used Google, Bing, and Yahoo. Other
inpopularity. Recentstate-of-the-artresultshavebeenob- searchengineswereconsideredsuchasBaiduandYandex.However they either did not produce a high percentage of
the intended images or they did not have accessible APIs
for automatically querying and pulling image urls into the
database.
Atotalof1250searchquerieswerecompiledinsixlan-
guagesandusedtocrawlInternetsearchenginesfortheim- None(Sad) No-Face(Happy) Uncertain(Angry) Neutral(Disgust)
ageurlsinourdataset. Thefirst200urlsreturnedforeach
query were stored in the database (258,140 distinct urls).
Among the 258,140 urls, 201,932 images were available
fordownload. OpenCVfacerecognitionwasusedtoobtain
bounding boxes around each face. Bidirectional warping
Angry(Happy) Disgust(Angry) Happy(Angry) Surprise(Happy)
ofActiveAppearanceModel(AAM)[23]andafacealign-
mentalgorithmviaregressinglocalbinaryfeatures[26,39]
were used to extract 66 facial landmarks. The employed
facial landmark localization techniques have been trained
using the annotations provided from the 300W competi-
tion[28,30,29]. Imageswithatleastonefacewithfacial Angry(Angry) Fear(Fear) Sad(Sad) Disgust(Disgust)
landmarkpointswerekeptforthenextprocessingstages.A Figure3. Sampleofqueriedimagesfromthewebandtheiranno-
total of 119,481 images were kept. Other attributes of the tatedtags.Thequeriedexpressioniswritteninparentheses.
querieswerestoredifapplicablesuchas;intendedemotion,
gender, age, languagesearched, anditsEnglishtranslation
ifnotinEnglish. Table1.Numberofannotatedimagesineachcategory
Label Numberofimages
Onaverage4000imagesofeachqueriedemotionswere
Neutral 3501
selected randomly, and in total 24,000 images were given
Happy 7130
to two expert annotators to categorize the face in the im-
Sad 3128
ageintoninecategories(i.e.No-face,sixbasicexpressions,
Surprise 1439
Neutral, None, and Uncertain). The annotators were in-
Fear 1307
structedtoselecttheproperexpressioncategoryontheface,
Disgust 702
wheretheintensityisnotimportantaslongasthefacede-
picts the intended expressions. The No-face category was Anger 2355
definedasimagesthat: 1)Therewasnofaceintheimage; None 403
2) There was a watermark on the face; 3) The bounding Uncertain 280
box was not on the face or did not cover the majority of No-face 3755
the face; 3) The face is a drawing, animation, painted, or
printedonsomethingelse; and4)Thefaceisdistortedbe-
yondanaturalornormalshape,evenifanexpressioncould
the images with disagreement, one of the annotations was
beinferred. TheNonecategorywasdefinedasimagesthat
assigned to the image randomly. Table 1 shows the num-
portrayedanemotionbuttheexpression/emotionscouldbe
berofimagesineachcategoryinthesetof24,000images
categorizedasoneofthesixbasicemotionsorneutral(such
thatweregiventotwohumanannotators. Asshown,some
assleepy,bored,tired,seducing,confused,shame,focused,
expressions such as Disgust, Fear, and Surprise have few
etc.). If the annotators were uncertain about any of the fa-
imagescomparedtotheotherexpressions,despitethenum-
cialexpressions,imagesweretaggedasuncertain. Figure3
berofqueriesbeingthesame.
shows some examples of each category and the intended
querieswritteninparentheses. Table 2 shows the confusion matrix between queried
The annotation was performed fully blind and indepen- emotionsandtheirannotations. Asisshown,happinesshad
dently, i.e. the annotators were not aware of the intended thehighesthit-rate(68%)andtherestofemotionshadhit-
query or other annotator’s response. The two annotators ratesatlessthan50%.Therewasabout15%confusionwith
agreed on 63.7% of the images. For the images that were No-Face category for all emotions, as many images from
at a disagreement, favor was given to the intended query thewebcontainedwatermarks,drawingsetc.About15%of
i.e. if one of the annotators labeled the image as the in- allqueriedemotionsresultedinneutralfaces. Disgustand
tended query, the image was labeled in the database with Fear had the lowest hit rate among other expression with
theintendedquery. Thishappenedin29.5%oftheimages 12% and 17% hit-rates respectively and most of the result
with disagreement between the annotators. On the rest of ofdisgustandfeararemainlyhappinessorNo-Face.Table2.ConfusionMatrixofannotatedimagesfordifferentintendedemotion-relatedqueryterms
Happy Sad Surprise Fear Disgust Anger Neutral No-Face None Uncertain
Happy 68.18 2.66 1.23 0.74 0.33 1.59 5.67 18.54 0.74 0.33
Sad 16.5 42.42 1.52 1.88 0.57 4.73 16.55 13.31 1.57 0.98
Surprise 27.6 6.31 20.11 5.62 1.07 4.85 17.1 14.73 1.65 0.96
Fear 18.74 10.91 6.49 17.69 1.47 6.39 13.92 20.49 2.22 1.67
Disgust 26.71 7.47 4.48 4.53 12.61 9.62 17.34 12.41 2.99 1.84
Anger 22.28 7.39 2.31 2.11 1.19 30.59 16.21 14.43 2.34 1.14
4.Trainingfromweb-images Table 3. Recognition accuracy of AlexNet and WACV-Net on
well-labeledtestsetwithdifferenttrainingsettings
Theannotatedimageslabeledwithsixbasicexpressions AlexNet WACV-Net[21]
aswellasneutralfacesareselectedfrom24,000annotated Trainonwell-labeled 82.12% 75.15%
images (18,674 images). Twenty percent of each label is Trainonmix 69.03% 67.04%
randomly selected as a test set (2,926 images) and the rest Trainonmixwith
81.68% 76.52%
areusedastrainingandvalidationsets.Atotalof60Kofnot noiseestimation[38]
annotatedimages(10Kforeachbasicemotion)isselected
asnoisytrainingset.
As baselines, two different deep neural network archi- using the noise modeling), as the posterior computation
tecturesaretrainedinthreedifferenttrainingscenarios: 1) could be totally wrong if the network is randomly initial-
training on well-labeled images, 2) training on a mixture ized[38],thenetworkcomponentsarepre-trainedwiththe
of noisy and well-labeled sets, and 3) training on a mix- well-labeled data. In addition, we bootstrap/upsample the
ture of noisy and well-labeled sets using a noise model- well-labeleddatatohalfofthenoisydata. Inallscenarios,
ingapproachintroducedin[38]. Thenetworkarchitecture weusedamini-batchsizeof256. Thelearningrateisini-
we used in these experiments are AlexNet [13] and a net- tializedtobe0.001andisdividedby10afterevery10,000
works for facial expression recognition recently published iterations. Wekeeptrainingeachmodeluntilconvergence.
inWACV2016in[21],calledWACV-Netintherestofthis Table 3 shows the overall recognition accuracy of
paper. Allnetworksareevaluatedonawell-labeledtestset. AlexNet and WACV-Net on the test set in three training
AlexNet consists of five convolutional layers, some of scenarios. As shown, in all cases AlexNet performed bet-
whicharefollowedbymax-poolinglayers,andthreefully- ter than WACV-Net. Training on mixture of the noisy and
connected layers. To augment the data, ten crops of reg- well-labeleddatawerenotassuccessfulastrainingononly
istered facial images of size 227x227 pixels are fed to well-labeled data. We believe that this was due to the fact
AlexNet. WehavetriedasmallerversionofAlexNetwith thatfacialexpressionimagescrawledfromthewebarevery
smaller input images of 40x40 pixels and smaller convo- noisy and in most expressions, less than 50% of the noisy
lutional kernel sizes, but the results were not as promising data portray the intended query. The noise estimation ap-
astheoriginalmodel. WACV-Netconsistsoftwoconvolu- proachcanimprovetheaccuracyofthenetworktrainedon
tionallayerseachfollowedbymaxpooling,fourInception themixtureofnoisyandwell-labeledsets.Thebestresultis
layers, and two fully-connected layers. The input images achievedfromtrainingAlexNetonwell-labeleddata. This
are resized to 48x48 pixels with ten augmented crops of givesslightlybetteroverallaccuracy(1%)thantrainingon
40x40pixels. OurversionofAlexNetperformedmorethan themixtureofnoisyandwell-labeledsetsusingnoisemod-
100M operations, whereas the WACV-Net performs about eling.
25Moperations,duetosizereductionsinInceptionlayers. Table 4 shows the confusion matrix of AlexNet trained
Therefore,WACV-Nettrainedalmostfourtimesfasterthan onthewell-labeledset. Table5showstheconfusionmatrix
AlexNet and consequently it had faster evaluation time as ofAlexNettrainedonthemixtureofnoisyandwell-labeled
well. sets with noise estimation [38]. As shown in these tables,
Inthefirstscenario,thenetworkistrainedononlywell- thenoiseestimationapproachcanimprovetherecognition
labeled set with random initialization. In the second sce- accuracyofsadness,surprise,fearanddisgustexpressions.
nario(mixtureofnoisyandwell-labeledsets),thenetwork Thereasonisthattherearefewersamplesoftheseexpres-
ispre-trainedwithonlywell-labeleddata,andthentrained sions in the well-labeled sets compared with other labels,
onthemixtureofthenoisyandwell-labeledsets. Thisin- and therefore including noisy data increases the training
creased about 5% in accuracy compared with training on samplesiftheposteriordistributionisestimatedwell.How-
themixtureofthenoisyandwell-labeledsetsfromscratch. ever,insomecasessuchasneutralfacesandangry,training
Inthelastscenario(mixtureofnoisyandwell-labeledsets on only well-labeled data has higher recognition accuracy,Table4.ConfusionmatrixofAlexNetTrainedonwell-labeled
predicted
NE HA SA SU FE DI AN
lautcA
NE 79.12 6.73 9.98 0.46 0 0 3.71
HA 6.37 91.63 1.14 0.29 0.14 0.07 0.36
SA 14.52 5.24 73.10 0.24 0.48 0.71 5.71
SU 10.59 6.47 1.18 76.47 3.53 1.18 0.59
FE 4.14 3.45 7.59 15.86 60 2.76 6.21
DI 2.41 4.82 8.43 2.41 1.2 57.83 22.89
AN 8.6 2.87 5.73 1.79 0.36 5.73 74.91
* NE,HA,SA,SU,FE,DI,ANstandforNeutral,Happiness,Sadness,
Surprised,Fear,Disgust,Angerrespectively.
Table 5. Confusion matrix of AlexNet Trained mixture of noisy
andwell-labeledsetswithnoiseestimation
predicted
NE HA SA SU FE DI AN
lautcA
Happy(Neutral) Angry(Disgust) Neutral(Fear) Angry(Neutral)
Disgust(Angry) Happy(Surprise) Neutral(Sad) Surprise(Fear)
NE 65.20 10.67 20.19 0.23 0 1.62 2.09 Happy(Fear) Angry(Sad) Neutral(Angry) Neutral(Surprise)
HA 3.29 91.56 3.72 0.21 0.21 0.43 0.57 Figure4. Samplesofmiss-classifiedimages.Theircorresponding
SA 7.62 3.33 84.29 0.24 0.95 1.43 2.14 ground-truthisgiveninparentheses.
SU 5.29 5.88 4.12 76.47 5.29 1.76 1.18
FE 0.69 3.45 11.72 13.79 63.45 3.45 3.45
DI 2.41 4.82 6.02 2.41 1.2 68.67 14.46 entsubjects,ages,andethnicity.
AN 6.45 2.87 12.54 2.87 0.36 4.66 70.25 Two neural network architectures were trained in three
* NE,HA,SA,SU,FE,DI,ANstandforNeutral,Happiness,Sadness, training scenarios. It is shown that, training on only well-
Surprised,Fear,Disgust,Angerrespectively. labeleddatahashigheroverallaccuracythantrainingonthe
mixtureofnoisyandwell-labeleddata,evenwiththenoise
estimation method. The noise estimation can increase the
asthepriordistributiononthewell-labelsetmaynotfully
accuracyinsadness,surprise,fearanddisgustexpressions,
reflecttheposteriordistributiononthenoisyset.
astherewerelimitedsamplesinwell-labeleddata. Butstill
Figure 4 shows a sample of randomly selected images
training on only well-labeled data has a higher overall ac-
misclassified by AlexNet trained on the well-labeled and
curacy. The reason is that as annotations of web images
their corresponding ground-truth given in parentheses. As
showed, most of the facial images queried from the web
thefigureshows,itisreallydifficulttoclassifysomeofthe
have less than 50% hit-rates and even for some emotions
images. Forexample,wewereunabletocorrectlyclassify
such as disgust and fear, the majority of the results por-
the images in the first row. Also, the images in the sec-
trayedotheremotionsorneutralfaces.
ond row have similarities to the misclassified labels, such
Thewholedatabase,queryterms,annotatedimagessub-
as nose wrinkle in disgust, or raised eyebrows in surprise.
set, andtheirfaciallandmarkpointswillbepubliclyavail-
It should be mentioned that classifying complex facial ex-
ablefortheresearchcommunity.
pressions as discrete emotions, especially in the wild, can
beverydifficultandeventherewasonly63.7%agreement
6.Acknowledgment
betweentwohumanannotators.
ThisworkispartiallysupportedbytheNSFgrantsIIS-
5.Conclusion
1111568 and CNS-1427872. We gratefully acknowledge
the support of NVIDIA Corporation with the donation of
Facial expression recognition in a wild setting is really
theTeslaK40GPUusedforthisresearch.
challenging. Currentdatabaseswithinwildsettingarealso
eitherverysmallorhavelowresolutionwithoutfacialland-
References
mark points necessary for pre-processing. The Internet is
a vast resource of images and it is estimated that over 430
[1] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
millionphotosareuploadedononlysocialnetworkservers Fei. Imagenet: A large-scale hierarchical image database.
everyday. Mostoftheseimagescontainfaces,thatarecap- In Computer Vision and Pattern Recognition, 2009. CVPR
tured in uncontrolled settings, illuminations, pose, etc. In 2009.IEEEConferenceon,pages248–255.IEEE,2009. 3
fact it is Word Wild Web of facial images and it can be a [2] A. Dhall, R. Goecke, J. Joshi, M. Wagner, and T. Gedeon.
greatresourceforcapturingmillionsofsampleswithdiffer- Emotion recognition in the wild challenge 2013. In Pro-ceedings of the 15th ACM on International conference on FaceandGestureRecognition(FG),201310thIEEEInter-
multimodalinteraction,pages509–516.ACM,2013. 2 national Conference and Workshops on, pages 1–6. IEEE,
[3] A. Dhall, R. Goecke, S. Lucey, and T. Gedeon. Static fa- 2013. 3
cial expression analysis in tough conditions: Data, evalua- [16] M.Lyons,S.Akamatsu,M.Kamachi,andJ.Gyoba.Coding
tionprotocolandbenchmark.InComputerVisionWorkshops facial expressions with gabor wavelets. In Automatic Face
(ICCVWorkshops),2011IEEEInternationalConferenceon, andGestureRecognition,1998.Proceedings.ThirdIEEEIn-
pages2106–2112.IEEE,2011. 1,3 ternationalConferenceon,pages200–205.IEEE,1998. 1
[4] M. Drange. Why is this canadian hacker better [17] S.M.Mavadati,M.H.Mahoor,K.Bartlett,P.Trinh,andJ.F.
than facebook at detecting gun photos? http: Cohn.Disfa:Aspontaneousfacialactionintensitydatabase.
//www.forbes.com/sites/mattdrange/ AffectiveComputing, IEEETransactionson, 4(2):151–160,
2016/03/31/facebook-guns-beet_ 2013. 1,2
farmer-image-recognition/#23db8f4478ed, [18] D.McDuff,R.Kaliouby,T.Senechal,M.Amr,J.Cohn,and
2016. 1 R.Picard. Affectiva-mitfacialexpressiondataset(am-fed):
[5] T.GehrigandH.K.Ekenel. Whyisfacialexpressionanal- Naturalisticandspontaneousfacialexpressionscollected.In
ysis in the wild challenging? In Proceedings of the 2013 Proceedings of the IEEE Conference on Computer Vision
onEmotionrecognitioninthewildchallengeandworkshop, andPatternRecognitionWorkshops, pages881–888, 2013.
pages9–16.ACM,2013. 3 1
[19] M.Mohammadi,E.Fatemizadeh,andM.H.Mahoor. Pca-
[6] I. J. Goodfellow, D. Erhan, P. L. Carrier, A. Courville,
based dictionary building for accurate facial expression
M. Mirza, B. Hamner, W. Cukierski, Y. Tang, D. Thaler,
recognition via sparse representation. Journal of Visual
D.-H. Lee, et al. Challenges in representation learning: A
Communication and Image Representation, 25(5):1082–
reportonthreemachinelearningcontests. NeuralNetworks,
1092,2014. 2
64:59–63,2015. 2,3
[20] M.R.Mohammadi,E.Fatemizadeh,andM.H.Mahoor. In-
[7] J.F.Grafsgaard,J.B.Wiggins,K.E.Boyer,E.N.Wiebe,and
tensityestimationofspontaneousfacialactionunitsbasedon
J. C. Lester. Automatically recognizing facial expression:
theirsparsityproperties. 2015. 2
Predictingengagementandfrustration. InEDM,pages43–
[21] A.Mollahosseini,D.Chan,andM.H.Mahoor.Goingdeeper
50,2013. 1
infacialexpressionrecognitionusingdeepneuralnetworks.
[8] T. Gritti, C. Shan, V. Jeanne, and R. Braspenning. Local
IEEEWinterConferenceonApplicationsofComputerVision
features based facial expression recognition with face reg-
(WACV),2016. 3,5
istrationerrors. InAutomaticFace&GestureRecognition,
[22] A.Mollahosseini, G.Graitzer, E.Borts, S.Conyers, R.M.
2008.FG’08.8thIEEEInternationalConferenceon,pages
Voyles,R.Cole,andM.H.Mahoor.Expressionbot:Anemo-
1–8.IEEE,2008. 2
tivelifelikeroboticfaceforface-to-facecommunication. In
[9] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker.
HumanoidRobots(Humanoids),201414thIEEE-RASInter-
Multi-pie. Image and Vision Computing, 28(5):807–813,
nationalConferenceon,pages1098–1103.IEEE,2014. 2
2010. 1,3
[23] A.MollahosseiniandM.H.Mahoor. Bidirectionalwarping
[10] Instagram. Press page. https://www.instagram.
ofactiveappearancemodel.InComputerVisionandPattern
com/press/?hl=en,2016. 1
Recognition Workshops (CVPRW), 2013 IEEE Conference
[11] S. E. Kahou, C. Pal, X. Bouthillier, P. Froumenty, on,pages875–880.IEEE,2013. 4
C¸.Gu¨lc¸ehre,R.Memisevic,P.Vincent,A.Courville,Y.Ben-
[24] M.Oquab,L.Bottou,I.Laptev,andJ.Sivic. Learningand
gio,R.C.Ferrari,etal. Combiningmodalityspecificdeep
transferringmid-levelimagerepresentationsusingconvolu-
neural networks for emotion recognition in video. In Pro-
tional neural networks. In Proceedings of the IEEE Con-
ceedings of the 15th ACM on International conference on
ferenceonComputerVisionandPatternRecognition,pages
multimodalinteraction,pages543–550.ACM,2013. 3
1717–1724,2014. 3
[12] H.KobayashiandF.Hara. Facialinteractionbetweenani- [25] M. Pantic, M. Valstar, R. Rademaker, and L. Maat. Web-
mated 3d face robot and human beings. In Systems, Man, baseddatabaseforfacialexpressionanalysis. InMultimedia
andCybernetics,1997.ComputationalCyberneticsandSim- andExpo,2005.ICME2005.IEEEInternationalConference
ulation.,1997IEEEInternationalConferenceon,volume4, on,pages5–pp.IEEE,2005. 1
pages3732–3737.IEEE,1997. 2 [26] S.Ren,X.Cao,Y.Wei,andJ.Sun. Facealignmentat3000
[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet fpsviaregressinglocalbinaryfeatures.InProceedingsofthe
classification with deep convolutional neural networks. In IEEEConferenceonComputerVisionandPatternRecogni-
Advances in neural information processing systems, pages tion,pages1685–1692,2014. 4
1097–1105,2012. 3,5 [27] E.Rentzeperis,A.Stergiou,A.Pnevmatikakis,andL.Poly-
[14] C. Liu and H. Wechsler. Gabor feature based classifica- menakos. Impactoffaceregistrationerrorsonrecognition.
tionusingtheenhancedfisherlineardiscriminantmodelfor InArtificialIntelligenceApplicationsandInnovations,pages
facerecognition. Imageprocessing,IEEETransactionson, 187–194.Springer,2006. 2
11(4):467–476,2002. 2 [28] C.Sagonas, E.Antonakos, G.Tzimiropoulos, S.Zafeiriou,
[15] M. Liu, S. Li, S. Shan, and X. Chen. Au-aware deep and M. Pantic. 300 faces in-the-wild challenge: Database
networks for facial expression recognition. In Automatic andresults. ImageandVisionComputing,2015. 4[29] C.Sagonas,G.Tzimiropoulos,S.Zafeiriou,andM.Pantic. [44] Y.Zong,W.Zheng,X.Huang,K.Yan,J.Yan,andT.Zhang.
300 faces in-the-wild challenge: The first facial landmark Emotion recognition in the wild via sparse transductive
localization challenge. In Proceedings of the IEEE Inter- transferlineardiscriminantanalysis. JournalonMultimodal
nationalConferenceonComputerVisionWorkshops,pages UserInterfaces,pages1–10,2016. 3
397–403,2013. 4
[30] C.Sagonas,G.Tzimiropoulos,S.Zafeiriou,andM.Pantic.
Asemi-automaticmethodologyforfaciallandmarkannota-
tion. InProceedingsoftheIEEEConferenceonComputer
VisionandPatternRecognitionWorkshops,pages896–903,
2013. 4
[31] C. Shan, S. Gong, and P. W. McOwan. Facial expression
recognitionbasedonlocalbinarypatterns:Acomprehensive
study. ImageandVisionComputing,27(6):803–816,2009.
2
[32] S. Sukhbaatar and R. Fergus. Learning from noisy labels
withdeepneuralnetworks. arXivpreprintarXiv:1406.2080,
2(3):4,2014. 3
[33] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabi-
novich. Going deeper with convolutions. arXiv preprint
arXiv:1409.4842,2014. 3
[34] Y.Taigman,M.Yang,M.Ranzato,andL.Wolf. Deepface:
Closingthegaptohuman-levelperformanceinfaceverifica-
tion. InComputerVisionandPatternRecognition(CVPR),
2014IEEEConferenceon,pages1701–1708.IEEE,2014.3
[35] Y.Tang.Deeplearningusinglinearsupportvectormachines.
arXivpreprintarXiv:1306.0239,2013. 3
[36] A.ToshevandC.Szegedy. Deeppose: Humanposeestima-
tionviadeepneuralnetworks. InComputerVisionandPat-
ternRecognition(CVPR),2014IEEEConferenceon,pages
1653–1660.IEEE,2014. 3
[37] J.Weston,F.Ratle,H.Mobahi,andR.Collobert.Deeplearn-
ing via semi-supervised embedding. In Neural Networks:
TricksoftheTrade,pages639–655.Springer,2012. 3
[38] T.Xiao, T.Xia, Y.Yang, C.Huang, andX.Wang. Learn-
ingfrommassivenoisylabeleddataforimageclassification.
InProceedingsoftheIEEEConferenceonComputerVision
andPatternRecognition,pages2691–2699,2015. 2,3,5
[39] L. Yu. face-alignment-in-3000fps. https://github.
com/yulequan/face-alignment-in-3000fps,
2016. 4
[40] Z. Yu and C. Zhang. Image based static facial expression
recognition with multiple deep network learning. In Pro-
ceedings of the 2015 ACM on International Conference on
MultimodalInteraction,pages435–442.ACM,2015. 3
[41] X.Zhang,M.H.Mahoor,andS.M.Mavadati.Facialexpres-
sionrecognitionusing{l} {p}-normmklmulticlass-svm.
MachineVisionandApplications,pages1–17,2015. 2
[42] X.Zhang,A.Mollahosseini,B.Kargar,H.Amir,E.Boucher,
R.M.Voyles,R.Nielsen,andM.Mahoor.ebear:Anexpres-
sivebear-likerobot. InRobotandHumanInteractiveCom-
munication, 2014 RO-MAN: The 23rd IEEE International
Symposiumon,pages969–974.IEEE,2014. 2
[43] W.ZhenandY.Zilu. Facialexpressionrecognitionbasedon
localphasequantizationandsparserepresentation. InNatu-
ralComputation(ICNC),2012EighthInternationalConfer-
enceon,pages222–225.IEEE,2012. 2"
122,124,Facial expression recognition in JAFFE dataset based on Gaussian process classification,"['F Cheng', 'J Yu', 'H Xiong']",2010,134,"Japanese Female Facial Expression, Toronto Face Database","classification, classifier, facial expression recognition","Here we propose a GP model and investigate it for the facial expression recognition in the  Japanese female facial expression dataset. By the strategy of leave-one-out cross validation,",No DOI,IEEE Transactions on Neural …,http://ieeexplore.ieee.org/document/5551215/,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
123,125,Facial expression recognition in image sequences using geometric deformation features and support vector machines,"['I Kotsia', 'I Pitas']",2006,900,Toronto Face Database,facial expression recognition,tion problems used for FAUs detection in the grid and for facial expression recognition.   The Cohn–Kanade database [2] was used for the facial expression recognition in six basic,No DOI,IEEE transactions on image processing,https://ieeexplore.ieee.org/document/4032815,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
124,126,Facial expression recognition in the wild via deep attentive center loss,"['AH Farzaneh', 'X Qi']",2021,289,Expression in-the-Wild,"CNN, FER, classification, classifier, deep learning, facial expression recognition, machine learning, neural network","Accordingly, for the task of FER in the wild, where  Face Database (RAFDB) [14]. Then, we  conduct extensive experiments on these two widely used wild Facial Expression Recognition (",No DOI,Proceedings of the IEEE/CVF winter …,https://openaccess.thecvf.com/content/WACV2021/papers/Farzaneh_Facial_Expression_Recognition_in_the_Wild_via_Deep_Attentive_Center_WACV_2021_paper.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,thecvf.com,
125,127,Facial expression recognition in video with multiple feature fusion,"['J Chen', 'Z Chen', 'Z Chi', 'H Fu']",2016,246,Acted Facial Expressions In The Wild,"classification, classifier, facial expression recognition",Acted Facial Expression in Wild (AFEW) 4.0 database show that our approach is robust in  dealing with video-based facial expression recognition  streams of facial expressions analysis,No DOI,IEEE Transactions on Affective …,https://ieeexplore.ieee.org/document/7518582,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
126,128,Facial expression recognition using computer vision: A systematic review,"['D Canedo', 'AJR Neves']",2019,164,"Affective Faces Database, Binghamton University 3D Facial Expression, Radboud Faces Database","CNN, FER, facial expression recognition","Facial expressions are the main focus of this systematic review. Generally, an FER system  consists  Binghamton University 3D Facial Expression database (BU-3DFE) [52]: contains 606",No DOI,Applied Sciences,https://www.mdpi.com/2076-3417/9/21/4678,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,mdpi.com,
127,129,Facial expression recognition using deep convolutional neural networks,"['DV Sang', 'N Van Dat']",2017,102,Toronto Face Database,"facial expression recognition, neural network","different effective CNNs to tackle the problem of facial expression recognition. Since  FERC-2013 dataset is much smaller than ImageNet dataset, we propose some changes to avoid",No DOI,2017 9th International Conference on …,https://ieeexplore.ieee.org/document/8119447,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
128,130,Facial expression recognition using enhanced deep 3D convolutional neural networks,"['B Hasani', 'MH Mahoor']",2017,337,MMI Facial Expression,neural network,"facial expressions in a sequence (Figure 1). We evaluate our proposed method using four  well-known facial expression databases (CK+, MMI in recognition of facial expressions in cross",No DOI,… of the IEEE conference on computer …,https://arxiv.org/abs/1705.07871,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"Facial Expression Recognition Using Enhanced Deep 3D Convolutional Neural
Networks
BehzadHasaniandMohammadH.Mahoor
DepartmentofElectricalandComputerEngineering
UniversityofDenver,Denver,CO
behzad.hasani@du.edu and mmahoor@du.edu
Abstract
Deep Neural Networks (DNNs) have shown to outper-
formtraditionalmethodsinvariousvisualrecognitiontasks
includingFacialExpressionRecognition(FER).Inspiteof
effortsmadetoimprovetheaccuracyofFERsystemsusing
DNN, existing methods still are not generalizable enough
in practical applications. This paper proposes a 3D Con-
volutionalNeuralNetworkmethodforFERinvideos. This
new network architecture consists of 3D Inception-ResNet
layersfollowedbyanLSTMunitthattogetherextractsthe
spatial relations within facial images as well as the tem-
poral relations between different frames in the video. Fa-
ciallandmarkpointsarealsousedasinputstoournetwork
which emphasize on the importance of facial components
Figure1.Proposedmethod
rather than the facial regions that may not contribute sig-
nificantly to generating facial expressions. Our proposed
methodisevaluatedusingfourpubliclyavailabledatabases
set,peakandoffset,wheretheonsetdescribesthebeginning
in subject-independent and cross-database tasks and out-
of the expression, the peak (aka apex) describes the maxi-
performsstate-of-the-artmethods.
mumintensityoftheexpressionandtheoffsetdescribesthe
moment when the expression vanishes. Most of the times,
the entire event of facial expression from the onset to the
1.Introduction
offsetisveryquick,whichmakestheprocessofexpression
recognitionverychallenging[55].
Facial expressions are one of the most important non-
verbal channels for expressing internal emotions and in- Manymethodshavebeenproposedforautomatedfacial
tentions. Ekman et al. [13] defined six expressions (viz. expressionrecognition. Mostofthetraditionalapproaches
anger,disgust,fear,happiness,sadness,andsurprise)asba- mainlyconsiderstillimagesindependentlywhileignorethe
sic emotional expressions which are universal among hu- temporalrelationsoftheconsecutiveframesinasequence
man beings. Automated Facial Expression Recognition which are essential for recognizing subtle changes in the
(FER) has been a topic of study for decades. Although appearanceoffacialimagesespeciallyintransitingframes
there have been many breakthroughs in developing auto- between emotions. Recently, with the help of Deep Neu-
maticFERsystems,majorityoftheexistingmethodseither ralNetworks(DNNs),morepromisingresultsarereported
show undesirable performance in practical applications or in the field [38, 39]. While in traditional approaches engi-
lackgeneralizationduetothecontrolledconditioninwhich neeredfeaturesareusedtotrainclassifiers,DNNshavethe
theyaredeveloped[47]. ability to extract more discriminative features which yield
TheFERproblembecomesevenmoredifficultwhenwe inabetterinterpretationofthetextureofhumanfaceinvi-
recognizeexpressionsinvideos. Facialexpressionshavea sualdata.
dynamic pattern that can be divided into three phases: on- One of the problems in FER is that training neural net-
1
7102
yaM
22
]VC.sc[
1v17870.5071:viXraworks is significantly more difficult as most of the exist- extensions such as Latent-Dynamic Conditional Random
ing databases have a small number of images or video se- Fields(LD-CRFs)andHiddenConditionalRandomFields
quences for certain emotions [39]. Also, most of these (HCRFs)[58].
databases contain still images that are unrelated to each In recent years, “Convolutional Neural Networks”
other (instead of having consecutive frames of exhibiting (CNNs) have become the most popular approach among
the expression from onset to offset) which makes the task researchers in the field. AlexNet [27] is based on the tra-
ofsequentialimagelabelingmoredifficult. ditional CNN layered architecture which consists of sev-
Inthispaper, weproposeamethodwhichextractstem- eralconvolutionlayersfollowedbymax-poolinglayersand
poral relations of consecutive frames in a video sequence RectifiedLinearUnits(ReLUs). Szegedyetal. [52]intro-
using 3D convolutional networks and Long Short-Term duced GoogLeNet which is composed of multiple “Incep-
Memory(LSTM).Furthermore,weextractandincorporate tion” layers. Inception applies several convolutions on the
facial landmarks in our proposed method that emphasize featuremapindifferentscales.Mollahosseinietal.[38,39]
on more expressive facial components which improve the have used the Inception layer for the task of facial expres-
recognition of subtle changes in the facial expressions in sion recognition and achieved state-of-the-art results. Fol-
a sequence (Figure 1). We evaluate our proposed method lowing the success of Inception layers, several variations
using four well-known facial expression databases (CK+, ofthemhavebeenproposed[24,53]. Moreover,Inception
MMI, FERA, and DISFA) in order to classify the expres- layeriscombinedwithresidualunitintroducedbyHeetal.
sions. Furthermore, we examine the ability of our method [21]anditshowsthattheresultingarchitectureaccelerates
in recognition of facial expressions in cross-database clas- thetrainingofInceptionnetworkssignificantly[51].
sificationtasks. One of the major restrictions of ordinary Convolutional
Theremainderofthepaperisorganizedasfollows: Sec- NeuralNetworksisthattheyonlyextractspatialrelationsof
tion2providesanoverviewoftherelatedworkinthisfield. theinputdatawhileignorethetemporalrelationsofthemif
Section 3 explains the network proposed in this research. they are part of a sequenced data. To overcome this prob-
ExperimentalresultsandtheiranalysisarepresentedinSec- lem, 3D Convolutional Neural Networks (3D-CNNs) have
tion4andfinallythepaperisconcludedinSection5. been proposed. 3D-CNNs slide over the temporal dimen-
sion of the input data as well as the spatial dimension en-
2.Relatedwork
ablingthenetworktoextractfeaturemapscontainingtem-
poral information which is essential for sequence labeling
Traditionally,algorithmsforautomatedfacialexpression
tasks. Song et al. [49] have used 3D-CNNs for 3D object
recognitionconsistofthreemainmodules,viz.registration,
detection task. Molchanov et al. [37] have proposed a re-
featureextraction,andclassification.Detailedsurveyofdif-
current3D-CNNfordynamichandgesturerecognitionand
ferent approaches in each of these steps can be found in
Fanetal.[15]wontheEmotiW2016challengebycascad-
[44].Conventionalalgorithmsforaffectivecomputingfrom
ing3D-CNNswithLSTMs.
facesuseengineeredfeaturessuchasLocalBinaryPatterns
Traditional Recurrent Neural Networks (RNNs) can
(LBP) [47], Histogram of Oriented Gradients (HOG) [8],
learn temporal dynamics by mapping input sequences to
LocalPhaseQuantization(LPQ)[59],HistogramofOptical
a sequence of hidden states, and also mapping the hidden
Flow[9],faciallandmarks[6,7],andPCA-basedmethods
statestooutputs[12]. AlthoughRNNshaveshownpromis-
[36]. Since the majority of these features are hand-crafted
ing performance on various tasks, it is not easy for them
fortheirspecificapplicationofrecognition,theyoftenlack
to learn long-term sequences. This is mainly due to the
requiredgeneralizabilityincaseswherethereishighvaria-
vanishing/exploding gradients problem [23] which can be
tioninlighting,views,resolution,subjects’ethnicity,etc.
solved by having a memory for remembering and forget-
One of the effective approaches for achieving better
tingthepreviousstates. LSTMs[23]providesuchmemory
recognition rates for sequence labeling task is to extract
and can memorize the context information for long peri-
the temporal relations of frames in a sequence. Extract-
odsoftime. LSTMmoduleshavethreegates: 1)theinput
ing these temporal relations has been studied using tradi-
gate (i) 2) the forget gate (f) and 3) the output gate (o)
tionalmethodsinthepast. Examplesoftheseattemptsare
whichoverwrite,keep,orretrievethememorycellcrespec-
HiddenMarkovModels[5,60,64](whichcombinetempo-
tivelyatthetimestept. Lettingσ(x) = (1+exp(−x))−1
ralinformationandapplysegmentationonvideos),Spatio-
TemporalHiddenMarkovModels(ST-HMM)bycoupling be the sigmoid function and φ(x) = exp(x)−exp(−x) =
exp(x)+exp(−x)
S-HMM and T-HMM [50], Dynamic Bayesian Networks 2σ(2x)−1 be the hyperbolic tangent function. Letting
(DBN) [45, 63] associated with a multi-sensory informa- x,h,c,W,andbbetheinput,output,cellstate,parameter
tionfusionstrategy,Bayesiantemporalmodels[46]tocap- matrix, and parameter vector respectively. The LSTM up-
ture the dynamic facial expression transition, and Condi- datesforthetimesteptgiveninputsx ,h ,andc are
t t−1 t−1
tional Random Fields (CRFs) [19, 20, 25, 48] and their asfollows:f =σ(W ·[h ,x ]+b )
t f t−1 t f
i =σ(W ·[h ,x ]+b )
t i t−1 t i
o =σ(W ·[h ,x ]+b )
t o t−1 t o
(1)
g =φ(W ·[h ,x ]+b )
t C t−1 t C
C =f ∗C +i ∗g
t t t−1 t t
h =o ∗φ(C )
t t t
SeveralworkshaveusedLSTMsforthetaskofsequence
labeling. Byeon et al. [3] proposed an LSTM-based net-
work applying LSTMs in four direction sliding windows
and achieved impressive results. Fan et al. [15] cascaded
2D-CNNwithLSTMsandcombinedthefeaturemapwith
3D-CNNs for facial expression recognition. Donahue et
al.[12]proposedLong-termRecurrentConvolutionalNet-
work (LRCN) by combining CNNs and LSTMs which is
both spatially and temporally deep and has the flexibility
to be applied to different vision tasks involving sequential
inputsandoutputs.
3.Proposedmethod
WhileInceptionandResNethaveshownremarkablere-
sultsinFER[20,52],thesemethodsdonotextractthetem-
poral relations of the input data. Therefore, we propose
a 3D Inception-ResNet architecture to address this issue.
Our proposed method, extracts both spatial and temporal
featuresofthesequencesinanend-to-endneuralnetwork.
Another component of our method is incorporating facial
landmarks in an automated manner during training in the
proposed neural network. These facial landmarks help the
networktopaymoreattentiontotheimportantfacialcom-
ponentsinthefeaturemapswhichresultsinamoreaccurate
recognition. The final part of our proposed method is an
LSTMunitwhichtakestheenhancedfeaturemapresulted
fromthe3DInception-ResNet(3DIR)layerasaninputand
extractsthetemporalinformationfromit.TheLSTMunitis
followedbyafully-connectedlayerassociatedwithasoft- Figure2.Networkarchitecture. The“V”and“S”markedlayers
maxactivationfunction. Inthefollowing,weexplaineach represent“Valid”and“Same”paddingsrespectively. Thesizeof
oftheaforementionedunitsindetail. theoutputtensorisprovidednexttoeachlayer.
3.1.3DInception-ResNet(3DIR)
We propose 3D version of Inception-ResNet network by3DIR-A,Reduction-A(whichreducesthegridsizefrom
which is slightly shallower than the original Inception- 38×38to18×18),3DIR-B,Reduction-B(whichreduces
ResNet network proposed in [51]. This network is the re- thegridsizefrom18×18to8×8),3DIR-C,AveragePool-
sultofinvestigatingseveralvariationsofInception-ResNet ing, Dropout, and a fully-connected layer respectively. In
moduleandachievesbetterrecognitionratescomparingto Figure 2, detailed specification of each layer is provided.
ourotherattemptsinseveraldatabases. Various filter sizes, paddings, strides, and activations have
Figure2showsthestructureofour3DInception-ResNet beeninvestigatedandtheonethathadthebestperformance
network. Theinputvideoswiththesize10×299×299×3 ispresentedinthispaper.
(10frames,299×299framesizeand3colorchannels)are We should mention that all convolution layers (except
followedbythe“stem”layer. Afterwards,stemisfollowed theonesthatareindicatedas“Linear”inFigure2)arefol-lowedbyanReLU[27]activationfunctiontoavoidthevan-
ishinggradientproblem.
3.2.Faciallandmarks
Asmentionedbefore,themainreasonweusefacialland-
marksinournetworkistodifferentiatebetweentheimpor-
tanceofmainfacialcomponents(suchaseyebrows,lipcor-
ners, eyes, etc.) and other parts of the face which are less (a)Landmarks (b)Generatedfilter
expressive of facial expressions. As oppose to general ob- Figure3.SampleimagefromMMIdatabase(left)anditscorre-
jectrecognitiontask,inFER,wehavetheadvantageofex- spondingfilterinthenetwork(right).Bestincolor.
tractingfaciallandmarksandusingthisinformationtoim-
prove the recognition rate. In a similar approach, Jaiswal landmarksarelocatedwillhavethehighestvalueandtheir
et al. [26] proposed incorporation of binary masks around surrounding pixels will have lower weights proportional
different parts of the face in order to encode the shape of to their distance from the corresponding facial landmark.
different face components. However, in this work authors In order to avoid overlapping between two adjacent facial
performAUrecognitionbyusingCNNasafeatureextrac- landmarks, we define a 7×7 window around each facial
tor for training Bi-directional Long Short-Term Memory landmarkandapplytheweightfunctionforthese49pixels
whileinourapproach,wepreservethetemporalorderofthe for each landmark separately. Figure 3 shows an example
framesthroughoutthenetworkandtrainCNNandLSTMs of facial image from MMI database and its corresponding
simultaneously in an end-to-end network. We incorporate faciallandmarkfilterinthenetwork. Wedonotincorporate
the facial landmarks by replacing the shortcut in residual the facial landmarks with the third 3D Inception-ResNet
unitonoriginalResNetwithelement-wisemultiplicationof modulesincetheresultingfeaturemapsizeatthisstagebe-
facial landmarks and the input tensor of the residual unit comesverysmallforcalculatingfaciallandmarkfilter.
(Figures1and2). Incorporating facial landmarks in our network replaces
In order to extract the facial landmarks, OpenCV face theshortcutinoriginalResNets[22]withtheelement-wise
recognition is used to obtain bounding boxes of the faces. multiplication of the weight function ω and input layer x
l
A face alignment algorithm via regression local binary asfollows:
features [41, 61] was used to extract 66 facial landmark
points. The facial landmark localization technique was
y =ω(L,P)◦x +F(x ,W )
trainedusingtheannotationsprovidedfromthe300Wcom- l l l l (3)
petition[42,43]. x l+1 =f(y l)
Afterdetectingandsavingthefaciallandmarksforallof
wherex andx areinputandoutputofthel-thlayer,◦is
l l+1
the databases, the facial landmark filters are generated for
Hadamardproductsymbol,F isaresidualfunction(inour
each sequence automatically during training phase. Given
case Inception layer convolutions), and f is an activation
the facial landmarks for each frame of a sequence, we ini-
function.
tially resize all of the images in the sequence to their cor-
responding filter size in the network. Afterwards, we as- 3.3.LongShort-TermMemoryunit
sign weights to all of the pixels in a frame of a sequence
As explained earlier, to capture the temporal relations
based on their distances to the detected landmarks. The
oftheresultedfeaturemapfrom3DIRandtaketheserela-
closer a pixel is to a facial landmark, the greater weight is
tionsintoaccountbythetimeofclassifyingthesequences
assigned to that pixel. After investigating several distance
inthesoftmaxlayer,weusedanLSTMunitasitisshownin
measures, we concluded that Manhattan distance with a
Figure 2. Using the LSTM unit makes perfect sense since
linear weight function results in a better recognition rate
the resulted feature map from the 3DIR unit contains the
invariousdatabases. TheManhattandistancebetweentwo
timenotionofthesequenceswithinthefeaturemap.There-
items is the sum of the differences of their corresponding
fore,vectorizingtheresultingfeaturemapof3DIRonitsse-
components(inthiscasetwocomponents).
quencedimension,willprovidetherequiredsequencedin-
Theweightfunctionthatwedefinedtoassigntheweight
putfortheLSTMunit.WhileotherstillimageLSTM-based
valuestotheircorrespondingfeatureisasimplelinearfunc-
methods, a vectorized non-sequenced feature map (which
tionoftheManhattandistancedefinedasfollows:
obviously does not contain any time notion) is fed to the
LSTM unit, our method saves the time order of the input
ω(L,P)=1−0.1·d (2)
M(L,P) sequences and passes this feature map to the LSTM unit.
whered istheManhattandistancebetweenthefacial Weinvestigatedthat200hiddenunitsfortheLSTMunitis
M(L,P)
landmarkLandpixelP. Therefore,placesinwhichfacial areasonableamountforthetaskofFER(Figure2).The proposed network was implemented using a com- Relief,andSadness.Headposeisprimarilyfrontalwithrel-
bination of TensorFlow [1] and TFlearn [10] toolboxes on ativelyfastmovements. EachvideoisannotatedwithAUs
NVIDIA Tesla K40 GPUs. In the training phase we used and holistic expressions. By extracting static frames from
asynchronous stochastic gradient descent with momentum the sequences, we obtained around 7,000 images. We di-
of 0.9, weight decay of 0.0001, and learning rate of 0.01. videdthetheseemotionvideosintosequencesoftenframes
Weusedcategoricalcrossentropyasourlossfunctionand toshapetheinputtensorforournetwork.
accuracyasourevaluationmetric. DISFA:DenverIntensityofSpontaneousFacialActions
(DISFA)database[33]isoneofafewnaturalisticdatabases
4.Experimentsandresults that have been FACS coded by AU intensity values. This
databaseconsistsof27subjects. Thesubjectsareaskedto
Inthissection, webrieflyreviewthedatabasesweused watch YouTube videos while their spontaneous facial ex-
forevaluatingourmethod. Wethenreporttheresultsofour pressions are recorded. Twelve AUs are coded for each
experimentsusingthesedatabasesandcomparetheresults frameandAUintensitiesareonasix-pointscalebetween0-
withthestateofthearts. 5,where0denotestheabsenceoftheAU,and5represents
maximum intensity. As DISFA is not emotion-specified
4.1.Facedatabases
coded,weusedEMFACSsystem[16]toconvertAUFACS
codestosevenexpressions(angry,disgust,fear,happy,neu-
Sinceourmethodisdesignedmainlyforclassifyingse-
tral,sad,andsurprise)whichresultedinaround89,000im-
quencesofinputs,databasesthatcontainonlyindependent
agesinwhichthemajorityhaveneutralexpressions. Same
unrelated still images of facial expressions such as Mul-
asotherdatabases, wedividedthevideosofemotionsinto
tiPie [18] , SFEW [11] , FER2013 [17] cannot be exam-
sequences of ten frames to shape the input tensor for our
inedbyourmethod. Weevaluateourproposedmethodon
network.
MMI [40], extended CK+ [32], GEMEP-FERA [2], and
DISFA [33] which contain videos of annotated facial ex-
4.2.Results
pressions. Inthefollowing, webrieflyreviewthecontents
ofthesedatabases. Asmentionedearlier,afterdetectingfacesweextract66
MMI: The MMI [40] database contains more than 20 facial landmark points by a face alignment algorithm via
subjects, ranging in age from 19 to 62, with different eth- regressionlocalbinaryfeatures. Afterwards, weresizethe
nicities (European, Asian, or South American). In MMI, facesto299×299pixels.Oneofthereasonswhywechoose
thesubjects’facialexpressionsstartfromtheneutralstateto large image size as input is the fact that larger images and
theapexofoneofthesixbasicfacialexpressionsandthen sequences will enable us to have deeper networks and ex-
returns to the neutral state again. Subjects were instructed tractmoreabstractfeaturesfromsequences. Allofthenet-
to display 79 series of facial expressions, six of which are workshavethesamesettings(showninFigure2indetail)
prototypic emotions (angry, disgust, fear, happy, sad, and andaretrainedfromscratchforeachdatabaseseparately.
surprise). We extracted static frames from each sequence, We evaluate the accuracy of our proposed method with
which resulted in 11,500 images. Afterwards, we divided two different sets of experiments: “subject-independent”
videosintosequencesoftenframestoshapetheinputten- and“cross-database”evaluations.
sorforournetwork.
CK+: TheextendedCohn-Kanadedatabase(CK+)[32]
4.2.1 Subject-independenttask
contains593videosfrom123subjects. However,only327
sequences from 118 subjects contain facial expression la- In the subject-independent task, each database is split into
bels. Sequencesinthisdatabasestartfromtheneutralstate training and validation sets in a strict subject independent
andendattheapexofoneofthesixbasicexpressions(an- manner. In all databases, we report the results using the
gry,contempt,disgust,fear,happy,sad,andsurprise). CK+ 5-fold cross-validation technique and then averaging the
primarilycontainsfrontalfaceposesonly. Inordertomake recognition rates over five folds. For each database and
thedatabasecompatiblewithournetwork,weconsiderthe each fold, we trained our proposed network entirely from
lasttenframesofeachsequenceasaninputsequenceinour scratchwiththeaforementionedsettings. Table1showsthe
network. recognitionratesachievedoneachdatabaseinthesubject-
FERA: The GEMEP-FERA database [2] is a subset of independentcaseandcomparestheresultswiththestate-of-
the GEMEP corpus used as database for the FERA 2011 the-artmethods. Inordertocomparetheimpactofincorpo-
challenge[56]developedbytheGenevaEmotionResearch rating facial landmarks, we also provide the results of our
GroupattheUniversityofGeneva. Thisdatabasecontains networkwhilethelandmarkmultiplicationunitisremoved
87 image sequences of 7 subjects. Each subject shows fa- and replaced with a simple shortcut between the input and
cialexpressionsoftheemotioncategories:Anger,Fear,Joy, output of the residual unit. In this case, we randomly se-lect20percentofthesubjectsasthetestsetandreportthe category in this database has some similarities with other
resultsonthosesubjects. Table1alsoprovidestherecogni- categoriesespeciallywithjoy. Thesesimilaritiesmakethe
tionratesofthetraditional2DInception-ResNetfrom[20] classification so difficult even for humans. Despite these
whichdoesnotcontainfaciallandmarksandtheLSTMunit challenges,ourmethodhasperformedwellonallofthecat-
(DISFAisnotexperimentedinthisstudy). egories and outperforms state of the arts. On DISFA (Fig-
Comparing the recognition rates of the 3D and 2D ure 4d), we can see the highest confusion rate compared
Inception-ResNets in Table 1, shows that the sequential with other databases. As mentioned earlier, this database
processingoffacialexpressionsconsiderablyenhancesthe containslonginactiveframes,whichmeansthatthenumber
recognition rate. This improvement is more apparent in ofneutralsequencesisconsiderablyhigherthanothercate-
MMIandFERAdatabases. Incorporatinglandmarksinthe gories.Thisimbalancedtrainingdatahasmadethenetwork
network is proposed to emphasize on more important fa- to be biased toward the neutral category and therefore we
cial changes over time. Since changes in the lips or eyes can observe a high confusion rate between the neutral ex-
are much more expressive than the changes in other com- pression and other categories in this database. Despite the
ponents such as the cheeks, we utilize facial landmarks to lownumberofangryandsadsequencesinthisdatabase,our
enhancethesetemporalchangesinthenetworkflow. methodhasbeenableachievesatisfyingrecognitionratesin
thesecategories.
The “3D Inception-ResNet with landmarks” column in
Table 1 shows the impact of this enhancement in different
databases. It can be seen that compared with other net-
4.2.2 Cross-databasetask
works, there is a considerable improvement in recognition
rates especially in FERA and MMI databases. The results In the cross-database task, for testing each database, that
onDISFA,however,showhigherfluctuationsoverdifferent databaseisentirelyusedfortestingthenetworkandtherest
foldswhichcanbeinpartduetotheabundanceofinactive of the databases are used to train the network. The same
framesinthisdatabasewhichcausesconfusioninrecogniz- networkarchitectureassubject-independenttask(Figure2)
ingdifferentexpressions. Therefore, thefoldsthatcontain was used for this task. Table 2 shows the recognition rate
moreneutralfaces,wouldshowlowerrecognitionrates. achievedoneachdatabaseinthecross-databasecaseandit
Comparing to other state-of-the-art works, our method also compares the results with other state-of-the-art meth-
outperforms others in FERA and DISFA databases while ods. It can be seen that our method outperforms the state-
achieves comparable results in CK+ and MMI databases of-the-artresultsinCK+,FERA,andDISFAdatabases. On
(Table 1). Most of these works use traditional approaches MMI,ourmethoddoesnotshowimprovementscomparing
including hand-crafted features tuned for that specific toothers(e.g.[62]). However,authorsin[62]trainedtheir
database, while our network’s settings are the same for all classifieronlywithCK+databasewhileourmethodusesin-
databases. Also, due to the limited number of samples in stancesfromtwoadditionaldatabases(DISFAandFERA)
thesedatabases,itisdifficulttoproperlytrainadeepneural with completely different settings and subjects which add
network and avoid the overfitting problem. For these rea- significantamountofambiguityinthetrainingphase.
sons and in order to have a better understanding about our In order to have a fair comparison with other methods,
proposedmethod,wealsoexperimentedthecross-database we provide the different settings used by the works men-
task. tionedinTable2. Theresultsprovidedin[34]areachieved
Figure 4 shows the resulting confusion matrices of our bytrainingthemodelsononeoftheCK+,MMI,andFEED-
3DInception-ResNetwithincorporatinglandmarksondif- TUMdatabasesandtestedontherest. Thereportedresult
ferentdatabasesoverthe5folds.OnCK+(Figure4a),itcan in[47]isthebestachievedresultsusingdifferentSVMker-
beseenthatveryhighrecognitionrateshavebeenachieved. nels trained on CK+ and tested on MMI database. In [35]
Therecognitionratesofhappiness,sadness,andsurpriseare several experiments were performed using four classifiers
higherthanthoseofotherexpressions. Thehighestconfu- (SVM,NearestMeanClassifier,WeightedTemplateMatch-
sionoccurredbetweenthehappinessandcontemptexpres- ing, and K-nearest neighbors). The reported results in this
sionswhichcanbecausedfromthelownumberofcontempt workforCK+istrainedonMMIandJaffedatabaseswhile
sequences in this database (only 18 sequences). On MMI thereportedresultsforMMIistrainedontheCK+database
(Figure4b),aperfectrecognitionisachievedforthehappy only.Asmentionedearlier,in[62]aMultipleKernelLearn-
expression. It can be seen that there is a high confusion ing algorithm is used and the cross-database experiments
between the sad and fear expressions as well as the angry are trained on CK+, evaluated on MMI and vice versa. In
and sad expressions. Considering the fact that MMI is a [38] a DNN network is proposed using traditional Incep-
highlyimbalanceddataset,theseconfusionsarereasonable. tionlayer. Thenetworksforthecross-databasecaseinthis
On FERA (Figure 4c), the highest and the lowest recogni- work are tested on either CK+, MultiPIE, MMI, DISFA,
tion rates belong to joy and relief respectively. The relief FERA,SFEW,orFER2013whiletrainedontherest. Some3D
2D 3D Inception-ResNet
state-of-the-artmethods
Inception-ResNet Inception-ResNet +
landmarks
84.1[34],84.4[28],88.5[54],92.0[29],
CK+ 85.77 89.50 93.21±2.32
93.2[38],92.4[30],93.6[62]
63.4[30],75.12[31],74.7[29],79.8[54],
MMI 55.83 67.50 77.50±1.76
86.7[47],78.51[36]
FERA 56.1[30],55.6[57],76.7[38] 49.64 67.74 77.42±3.67
DISFA 55.0[38] - 51.35 58.00±5.77
Table1.Recognitionrates(%)insubject-independenttask
(a)CK+ (b)MMI
(c)FERA (d)DISFA
Figure4.Confusionmatricesof3DInception-ResNetwithlandmarksforsubject-independenttask
of the expressions of these databases are excluded in this very few number of samples in other databases), the net-
study (such as neutral, relief, and contempt). There are workhasbeenabletocorrectlyrecognizeotherexpressions.
other works that perform their experiments on action unit For MMI (Figure 5b), highest recognition rate belongs to
recognitiontask[4,14,26]butsincefaircomparisonofac- surprisewhilethelowestonebelongstofear. Also,wecan
tionunitrecognitionandfacialexpressionrecognitionisnot see high confusion rate in recognizing sadness. On FERA
easily obtainable, we did not mention these works in Ta- (Figure5c),weexcludereliefcategoryasotherdatabasesdo
bles1and2. notcontainthisemotion.Consideringthefactthatonlyhalf
Figure 5 shows the resulting confusion matrices of our ofthetraincategoriesexistinthetestset,thenetworkshows
experiments on 3D Inception-ResNet with landmarks in acceptableperformanceincorrectlyrecognizingemotions.
cross-database task. For CK+ (Figure 5a), we exclude the However,surprisecategoryhasmadesignificantconfusion
contemptsequencesinthetestphasesinceotherdatabases inallofthecategories. OnDISFA(Figure5d),weexclude
that are used for training the network, do not contain con- the neutral category as other databases do not contain this
tempt category. Except for the fear expression (which has category.Highestrecognitionratesbelongtohappyandsur-(a)CK+ (b)MMI
(c)FERA (d)DISFA
Figure5.Confusionmatricesof3DInception-ResNetwithlandmarksforcross-databasetask
priseemotionswhilelowestonebelongstofear.Comparing proposed the 3D Inception-ResNet (3DIR) network which
tootherdatabases,wecanseeasignificantincreaseincon- extends the well-known 2D Inception-ResNet module for
fusionrateinallofthecategories. Thiscanbeinpartdue processing image sequences. This additional dimension
tothefactthatemotionsinDISFAare“spontaneous”while will result in a volume of feature maps and will extract
emotions in the training databases are “posed”. Based on the spatial relations between frames in a sequence. This
theaforementionedresults,ourmethodprovidesacompre- moduleisfollowedbyanLSTMwhichtakesthesetempo-
hensivesolutionthatcangeneralizewelltopracticalappli- ralrelationsintoaccountandusesthisinformationtoclas-
cations. sifythesequences. Inordertodifferentiatebetweenfacial
componentsandotherpartsoftheface,weincorporatedfa-
cial landmarks in our proposed method. These landmarks
5.Conclusion
are multipliedwith the inputtensor inthe residual module
In this paper, we presented a 3D Deep Neural Network whichisreplacedwiththeshortcutsinthetraditionalresid-
for the task of facial expression recognition in videos. We uallayer.
We evaluated our proposed method in subject-
independent and cross-database tasks. Four well-known
3D
databases were used to evaluate the method: CK+, MMI,
Inception-ResNet
state-of-the-artmethods FERA, and DISFA. Our experiments show that the pro-
+
posed method outperforms many of the state-of-the-art
landmarks
methods in both tasks and provides a general solution for
47.1[34],56.0[35],
CK+ 67.52 thetaskofFER.
61.2[62],64.2[38]
51.4[34],50.8[47],
MMI 36.8[35],55.6[38], 54.76 6.Acknowledgement
66.9[62]
FERA 39.4[38] 41.93 ThisworkispartiallysupportedbytheNSFgrantsIIS-
DISFA 37.7[38] 40.51 1111568 and CNS-1427872. We gratefully acknowledge
thesupportfromNVIDIACorporationwiththedonationof
Table2.Recognitionrates(%)incross-databasetask theTeslaK40GPUsusedforthisresearch.References ings of the 18th ACM International Conference on Multi-
modalInteraction, ICMI2016, pages445–450, NewYork,
[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,
NY,USA,2016.ACM. 2,3
C.Citro,G.S.Corrado,A.Davis,J.Dean,M.Devin,etal.
[16] W. V. Friesen and P. Ekman. Emfacs-7: Emotional facial
Tensorflow:Large-scalemachinelearningonheterogeneous
actioncodingsystem. Unpublishedmanuscript, University
distributedsystems.arXivpreprintarXiv:1603.04467,2016.
ofCaliforniaatSanFrancisco,2(36):1,1983. 5
5
[17] I. J. Goodfellow, D. Erhan, P. L. Carrier, A. Courville,
[2] T.Ba¨nzigerandK.R.Scherer. Introducingthegenevamul-
M.Mirza,B.Hamner,W.Cukierski,Y.Tang,D.Thaler,D.-
timodalemotionportrayal(gemep)corpus. Blueprintforaf-
H.Lee,etal.Challengesinrepresentationlearning:Areport
fectivecomputing:Asourcebook,pages271–294,2010. 5
on three machine learning contests. In International Con-
[3] W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki. Scene
ferenceonNeuralInformationProcessing, pages117–124.
labeling with lstm recurrent neural networks. In Proceed-
Springer,2013. 5
ingsoftheIEEEConferenceonComputerVisionandPattern
Recognition,pages3547–3555,2015. 3 [18] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker.
Multi-pie. Image and Vision Computing, 28(5):807–813,
[4] W.-S.Chu,F.DelaTorre,andJ.F.Cohn. Modelingspatial
2010. 5
andtemporalcuesformulti-labelfacialactionunitdetection.
arXivpreprintarXiv:1608.00911,2016. 7 [19] B. Hasani, M. M. Arzani, M. Fathy, and K. Raahemifar.
[5] I. Cohen, N. Sebe, A. Garg, L. S. Chen, and T. S. Huang. Facialexpressionrecognitionwithdiscriminatorygraphical
Facialexpressionrecognitionfromvideosequences:tempo- models.In20162ndInternationalConferenceofSignalPro-
ralandstaticmodeling. ComputerVisionandimageunder- cessing and Intelligent Systems (ICSPIS), pages 1–7, Dec
standing,91(1):160–187,2003. 2 2016. 2
[6] T.F.Cootes, G.J.Edwards, C.J.Taylor, etal. Activeap- [20] B.HasaniandM.H.Mahoor.Spatio-temporalfacialexpres-
pearancemodels.IEEETransactionsonpatternanalysisand sion recognition using convolutional neural networks and
machineintelligence,23(6):681–685,2001. 2 conditionalrandomfields.arXivpreprintarXiv:1703.06995,
[7] T.F.Cootes,C.J.Taylor,D.H.Cooper,andJ.Graham. Ac- 2017. 2,3,6
tiveshapemodels-theirtrainingandapplication. Computer [21] K.He,X.Zhang,S.Ren,andJ.Sun. Deepresiduallearning
visionandimageunderstanding,61(1):38–59,1995. 2 forimagerecognition.InTheIEEEConferenceonComputer
[8] N.DalalandB.Triggs. Histogramsoforientedgradientsfor VisionandPatternRecognition(CVPR),June2016. 2
humandetection. InComputerVisionandPatternRecogni- [22] K.He,X.Zhang,S.Ren,andJ.Sun. Identitymappingsin
tion,2005.CVPR2005.IEEEComputerSocietyConference deepresidualnetworks. pages630–645,2016. 4
on,volume1,pages886–893.IEEE,2005. 2
[23] S.HochreiterandJ.Schmidhuber.Longshort-termmemory.
[9] N.Dalal,B.Triggs,andC.Schmid. Humandetectionusing
Neuralcomputation,9(8):1735–1780,1997. 2
oriented histograms of flow and appearance. In European
[24] S.IoffeandC.Szegedy. Batchnormalization: Accelerating
conference on computer vision, pages 428–441. Springer,
deep network training by reducing internal covariate shift.
2006. 2
arXivpreprintarXiv:1502.03167,2015. 2
[10] A. Damien et al. Tflearn. https://github.com/
[25] S. Jain, C. Hu, and J. K. Aggarwal. Facial expression
tflearn/tflearn,2016. 5
recognitionwithtemporalmodelingofshapes. InComputer
[11] A. Dhall, R. Goecke, S. Lucey, and T. Gedeon. Static fa-
Vision Workshops (ICCV Workshops), 2011 IEEE Interna-
cial expression analysis in tough conditions: Data, evalua-
tionalConferenceon,pages1642–1649.IEEE,2011. 2
tionprotocolandbenchmark.InComputerVisionWorkshops
[26] S. Jaiswal and M. Valstar. Deep learning the dynamic ap-
(ICCVWorkshops),2011IEEEInternationalConferenceon,
pearanceandshapeoffacialactionunits. InApplicationsof
pages2106–2112.IEEE,2011. 5
ComputerVision(WACV),2016IEEEWinterConferenceon,
[12] J. Donahue, L. Anne Hendricks, S. Guadarrama,
pages1–8.IEEE,2016. 4,7
M. Rohrbach, S. Venugopalan, K. Saenko, and T. Dar-
rell. Long-termrecurrentconvolutionalnetworksforvisual [27] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
recognition and description. In Proceedings of the IEEE classification with deep convolutional neural networks. In
conference on computer vision and pattern recognition, Advances in neural information processing systems, pages
pages2625–2634,2015. 2,3 1097–1105,2012. 2,4
[13] P.EkmanandW.V.Friesen.Constantsacrossculturesinthe [28] S. H. Lee, K. N. K. Plataniotis, and Y. M. Ro. Intra-
faceandemotion.Journalofpersonalityandsocialpsychol- class variation reduction using training expression images
ogy,17(2):124,1971. 1 forsparserepresentationbasedfacialexpressionrecognition.
[14] C. Fabian Benitez-Quiroz, R. Srinivasan, and A. M. Mar- IEEE Transactions on Affective Computing, 5(3):340–351,
tinez. Emotionet: Anaccurate, real-timealgorithmforthe 2014. 7
automatic annotation of a million facial expressions in the [29] M. Liu, S. Li, S. Shan, and X. Chen. Au-aware deep
wild. InProceedingsoftheIEEEConferenceonComputer networks for facial expression recognition. In Automatic
VisionandPatternRecognition,pages5562–5570,2016. 7 FaceandGestureRecognition(FG),201310thIEEEInter-
[15] Y.Fan,X.Lu,D.Li,andY.Liu.Video-basedemotionrecog- national Conference and Workshops on, pages 1–6. IEEE,
nitionusingcnn-rnnandc3dhybridnetworks. InProceed- 2013. 7[30] M. Liu, S. Li, S. Shan, R. Wang, and X. Chen. Deeply [43] C.Sagonas,G.Tzimiropoulos,S.Zafeiriou,andM.Pantic.
learning deformable facial action parts model for dynamic Asemi-automaticmethodologyforfaciallandmarkannota-
expressionanalysis. InAsianConferenceonComputerVi- tion. InProceedingsoftheIEEEConferenceonComputer
sion,pages143–157.Springer,2014. 7 VisionandPatternRecognitionWorkshops,pages896–903,
[31] M.Liu, S.Shan, R.Wang, andX.Chen. Learningexpres- 2013. 4
sionletsonspatio-temporalmanifoldfordynamicfacialex- [44] E.Sariyanidi,H.Gunes,andA.Cavallaro. Automaticanal-
pressionrecognition.InProceedingsoftheIEEEConference ysisoffacialaffect:Asurveyofregistration,representation,
on Computer Vision and Pattern Recognition, pages 1749– andrecognition. IEEEtransactionsonpatternanalysisand
1756,2014. 7 machineintelligence,37(6):1113–1133,2015. 2
[32] P.Lucey,J.F.Cohn,T.Kanade,J.Saragih,Z.Ambadar,and
[45] N.Sebe,M.S.Lew,Y.Sun,I.Cohen,T.Gevers,andT.S.
I. Matthews. The extended cohn-kanade dataset (ck+): A
Huang. Authenticfacialexpressionanalysis. ImageandVi-
complete dataset for action unit and emotion-specified ex-
sionComputing,25(12):1856–1863,2007. 2
pression.InComputerVisionandPatternRecognitionWork-
[46] C. Shan, S. Gong, and P. W. McOwan. Dynamic facial
shops(CVPRW),2010IEEEComputerSocietyConference
expression recognition using a bayesian temporal manifold
on,pages94–101.IEEE,2010. 5
model. InBMVC,pages297–306.Citeseer,2006. 2
[33] S.M.Mavadati,M.H.Mahoor,K.Bartlett,P.Trinh,andJ.F.
[47] C. Shan, S. Gong, and P. W. McOwan. Facial expression
Cohn.Disfa:Aspontaneousfacialactionintensitydatabase.
recognitionbasedonlocalbinarypatterns:Acomprehensive
IEEE Transactions on Affective Computing, 4(2):151–160,
study. ImageandVisionComputing,27(6):803–816,2009.
2013. 5
1,2,6,7,8
[34] C.Mayer,M.Eggers,andB.Radig. Cross-databaseevalu-
ationforfacialexpressionrecognition. Patternrecognition [48] C.Sminchisescu,A.Kanaujia,andD.Metaxas. Conditional
andimageanalysis,24(1):124–132,2014. 6,7,8 modelsforcontextualhumanmotionrecognition. Computer
[35] Y.-Q. Miao, R. Araujo, and M. S. Kamel. Cross-domain VisionandImageUnderstanding,104(2):210–220,2006. 2
facialexpressionrecognitionusingsupervisedkernelmean [49] S.SongandJ.Xiao. Deepslidingshapesforamodal3dob-
matching. InMachineLearningandApplications(ICMLA), jectdetectioninrgb-dimages. InProceedingsoftheIEEE
2012 11th International Conference on, volume 2, pages Conference on Computer Vision and Pattern Recognition,
326–332.IEEE,2012. 6,8 pages808–816,2016. 2
[36] M.Mohammadi,E.Fatemizadeh,andM.H.Mahoor. Pca- [50] Y.Sun,X.Chen,M.Rosato,andL.Yin.Trackingvertexflow
based dictionary building for accurate facial expression and model adaptation for three-dimensional spatiotemporal
recognition via sparse representation. Journal of Visual faceanalysis. Systems, ManandCybernetics, PartA:Sys-
Communication and Image Representation, 25(5):1082– tems and Humans, IEEE Transactions on, 40(3):461–474,
1092,2014. 2,7 2010. 2
[37] P. Molchanov, X. Yang, S. Gupta, K. Kim, S. Tyree, and [51] C. Szegedy, S. Ioffe, and V. Vanhoucke. Inception-v4,
J.Kautz.Onlinedetectionandclassificationofdynamichand inception-resnet and the impact of residual connections on
gestureswithrecurrent3dconvolutionalneuralnetwork. In learning. arXivpreprintarXiv:1602.07261,2016. 2,3
Proceedings of the IEEE Conference on Computer Vision
[52] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
andPatternRecognition,pages4207–4215,2016. 2
D.Anguelov, D.Erhan, V.Vanhoucke, andA.Rabinovich.
[38] A.Mollahosseini,D.Chan,andM.H.Mahoor.Goingdeeper
Goingdeeperwithconvolutions.InProceedingsoftheIEEE
infacialexpressionrecognitionusingdeepneuralnetworks.
Conference on Computer Vision and Pattern Recognition,
In 2016 IEEE Winter Conference on Applications of Com-
pages1–9,2015. 2,3
puter Vision (WACV), pages 1–10. IEEE, 2016. 1, 2, 6, 7,
[53] C.Szegedy,V.Vanhoucke,S.Ioffe,J.Shlens,andZ.Wojna.
8
Rethinking the inception architecture for computer vision.
[39] A.Mollahosseini,B.Hasani,M.J.Salvador,H.Abdollahi,
In The IEEE Conference on Computer Vision and Pattern
D.Chan,andM.H.Mahoor. Facialexpressionrecognition
Recognition(CVPR),June2016. 2
from world wild web. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR) Workshops, [54] S. Taheri, Q. Qiu, and R. Chellappa. Structure-preserving
June2016. 1,2 sparse decomposition for facial expression analysis. IEEE
TransactionsonImageProcessing,23(8):3590–3603,2014.
[40] M. Pantic, M. Valstar, R. Rademaker, and L. Maat. Web-
7
baseddatabaseforfacialexpressionanalysis. In2005IEEE
internationalconferenceonmultimediaandExpo,pages5– [55] Y.Tian,T.Kanade,andJ.F.Cohn. Recognizinglowerface
pp.IEEE,2005. 5 actionunitsforfacialexpressionanalysis.InAutomaticFace
[41] S.Ren,X.Cao,Y.Wei,andJ.Sun. Facealignmentat3000 and Gesture Recognition, 2000. Proceedings. Fourth IEEE
fpsviaregressinglocalbinaryfeatures.InProceedingsofthe InternationalConferenceon,pages484–490.IEEE,2000. 1
IEEEConferenceonComputerVisionandPatternRecogni- [56] M.F.Valstar,B.Jiang,M.Mehu,M.Pantic,andK.Scherer.
tion,pages1685–1692,2014. 4 The first facial expression recognition and analysis chal-
[42] C.Sagonas, E.Antonakos, G.Tzimiropoulos, S.Zafeiriou, lenge. InAutomaticFace&GestureRecognitionandWork-
and M. Pantic. 300 faces in-the-wild challenge: Database shops(FG2011), 2011IEEEInternationalConferenceon,
andresults. ImageandVisionComputing,47:3–18,2016. 4 pages921–926.IEEE,2011. 5[57] M.F.Valstar,B.Jiang,M.Mehu,M.Pantic,andK.Scherer.
The first facial expression recognition and analysis chal-
lenge. InAutomaticFace&GestureRecognitionandWork-
shops(FG2011), 2011IEEEInternationalConferenceon,
pages921–926.IEEE,2011. 7
[58] S. B. Wang, A. Quattoni, L.-P. Morency, D. Demirdjian,
and T. Darrell. Hidden conditional random fields for ges-
turerecognition. InComputerVisionandPatternRecogni-
tion,2006IEEEComputerSocietyConferenceon,volume2,
pages1521–1527.IEEE,2006. 2
[59] Z.WangandZ.Ying.Facialexpressionrecognitionbasedon
localphasequantizationandsparserepresentation. InNatu-
ralComputation(ICNC),2012EighthInternationalConfer-
enceon,pages222–225.IEEE,2012. 2
[60] M. Yeasin, B. Bullot, and R. Sharma. Recognition of fa-
cialexpressionsandmeasurementoflevelsofinterestfrom
video. Multimedia, IEEE Transactions on, 8(3):500–508,
2006. 2
[61] L. Yu. face-alignment-in-3000fps. https://github.
com/yulequan/face-alignment-in-3000fps,
2016. 4
[62] X.Zhang,M.H.Mahoor,andS.M.Mavadati.Facialexpres-
sionrecognitionusing{l} {p}-normmklmulticlass-svm.
MachineVisionandApplications,26(4):467–483,2015. 6,
7,8
[63] Y.ZhangandQ.Ji. Activeanddynamicinformationfusion
for facial expression understanding from image sequences.
Pattern Analysis and Machine Intelligence, IEEE Transac-
tionson,27(5):699–714,2005. 2
[64] Y. Zhu, L. C. De Silva, and C. C. Ko. Using moment in-
variantsandhmminfacialexpressionrecognition. Pattern
RecognitionLetters,23(1):83–91,2002. 2"
129,131,Facial expression recognition using facial movement features,"['L Zhang', 'D Tjondronegoro']",2011,314,"Affective Faces Database, Japanese Female Facial Expression","FER, facial expression recognition","FER that considers facial movement features. In this paper, we aim for improving the  performance of FER by automatically capturing facial  on the Japanese female facial expression (",No DOI,IEEE transactions on affective …,https://ieeexplore.ieee.org/document/5871583,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
130,132,Facial expression recognition using hierarchical features with deep comprehensive multipatches aggregation convolutional neural networks,"['S Xie', 'H Hu']",2018,213,Japanese Female Facial Expression,"FER, neural network",on Facial Expression Recognition (FER) develop quickly as well. Applications based on FER   213 images of 7 facial expressions posed by 10 Japanese females. 183 images with six,No DOI,IEEE Transactions on Multimedia,https://ieeexplore.ieee.org/document/8371638,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
131,133,Facial expression recognition using kernel canonical correlation analysis (KCCA),"['W Zheng', 'X Zhou', 'C Zou', 'L Zhao']",2006,358,Japanese Female Facial Expression,"FER, classification","We will use the Japanese female facial expression (JAFFE) database [6]–[8] and Ekman’s  “Pictures of Facial Affect” database [22], respectively, to conduct the FER based on KCCA. In",No DOI,IEEE transactions on neural …,https://ieeexplore.ieee.org/document/1593706,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
132,134,Facial expression recognition using local gravitational force descriptor-based deep convolution neural networks,"['K Mohan', 'A Seal', 'O Krejcar']",2020,145,Japanese Female Facial Expression,neural network,"facial expression of a person. The proposed method consists of two parts. The former  one finds out local features from face  Budynek, “The Japanese female facial expression (JAFFE)",No DOI,IEEE Transactions on …,https://ieeexplore.ieee.org/document/9226437,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
133,135,Facial expression recognition via deep learning,"['X Zhao', 'X Shi', 'S Zhang']",2015,270,"Acted Facial Expressions In The Wild, Extended Cohn-Kanade, Japanese Female Facial Expression, Radboud Faces Database","CNN, deep learning, facial expression recognition","Experimental results on two benchmarking facial expression databases, ie, the JAFFE  database and the Cohn-Kanade database, demonstrate the promising performance of the",No DOI,IETE technical review,https://ieeexplore.ieee.org/document/8308363,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
134,136,Facial expression recognition with FRR‐CNN,"['S Xie', 'H Hu']",2017,108,Japanese Female Facial Expression,CNN,"Japanese Female Facial Expression (JAFFE) database. CK+ database is a public expression   As for JAFFE, it consists of 213 expressional images of ten Japanese female subjects. We",No DOI,Electronics Letters,https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/el.2016.4328,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,wiley.com,
135,137,Facial expression recognition with convolutional neural networks: coping with few data and the training sample order,"['AT Lopes', 'E De Aguiar', 'AF De Souza']",2017,923,Binghamton University 3D Facial Expression,"CNN, neural network","Hence, facial expression recognition is still a challenging problem in computer vision. In this   for facial expression recognition that uses a combination of Convolutional Neural Network",No DOI,Pattern recognition,https://www.sciencedirect.com/science/article/pii/S0031320316301753,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
136,138,Facial expression recognition with identity and emotion joint learning,"['M Li', 'H Xu', 'X Huang', 'Z Song', 'X Liu']",2018,120,Affective Faces Database,"FER, facial expression recognition","In this work, besides training deep-learned facial expression feature (emotional  latent face  identity feature such as the shape or appearance of face. We propose an identity and emotion",No DOI,… Transactions on affective …,https://ieeexplore.ieee.org/document/8528894,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
137,139,Facial expression recognition: A survey,"['Y Huang', 'F Chen', 'S Lv', 'X Wang']",2019,189,"Binghamton University 3D Facial Expression, Japanese Female Facial Expression, MMI Facial Expression",FER,-of-the-art FER approaches are presented and analysed.  FER datasets and summarise  four FER-related elements of datasets that may influence the choosing and processing of FER,No DOI,Symmetry,https://www.sciencedirect.com/science/article/pii/S1877050915021225,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
138,140,Facial expressions of emotion (KDEF): Identification under different display-duration conditions,"['MG Calvo', 'D Lundqvist']",2008,671,Karolinska Directed Emotional Faces,facial expression recognition,"expression of each model is identified. These data have provided researchers on the processing  of affective facial expressions  face database, the Karolinska Directed Emotional Faces (",No DOI,Behavior research methods,https://link.springer.com/article/10.3758/BRM.40.1.109,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,springer.com,
139,141,Fast facial emotion recognition using convolutional neural networks and Gabor filters,"['MMT Zadeh', 'M Imani', 'B Majidi']",2019,110,Affective Faces Database,neural network,face databases are not reliable for the real world applications. This article presents a new  database called RAF-DB that  [13] proposed to train a CNN network for the final face emotion,No DOI,2019 5th Conference on …,https://ieeexplore.ieee.org/document/8734943,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
140,142,Former-dfer: Dynamic facial expression recognition transformer,"['Z Zhao', 'Q Liu']",2021,116,Static Facial Expression in the Wild,FER,"According to the data type, the FER can be divided into static FER (SFER) and dynamic  FER (DFER) • We propose a dynamic facial expression recognition transformer for the in-the-wild",No DOI,Proceedings of the 29th ACM International Conference …,https://github.com/zengqunzhao/Former-DFER,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,github.com,
141,143,"Framework for reliable, real-time facial expression recognition for low resolution images","['RA Khan', 'A Meyer', 'H Konik', 'S Bouakaz']",2013,180,MMI Facial Expression,facial expression recognition,"–Kanade (CK+) posed facial expression database, spontaneous expressions of MMI facial  expression database and FG-NET facial expressions and emotions database (FEED) and",No DOI,Pattern Recognition Letters,https://www.sciencedirect.com/science/article/pii/S0167865513001268,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
142,144,Fusing aligned and non-aligned face information for automatic affect recognition in the wild: a deep learning approach,"['BK Kim', 'SY Dong', 'J Roh', 'G Kim']",2016,139,Expression in-the-Wild,deep learning,We evaluate the proposed framework on the facial expression recognition 2013 (FER-2013)   Our final ensemble-based FER system achieves great performance on this in-the-wild,No DOI,Proceedings of the IEEE …,http://ieeexplore.ieee.org/abstract/document/7789677/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
143,145,Fusion of feature sets and classifiers for facial expression recognition,"['THH Zavaschi', 'AS Britto Jr', 'LES Oliveira']",2013,167,Extended Cohn-Kanade,classifier,classifiers based on the under-pinning concept of “over-produce and choose”. The pool of  base classifiers  different databases (JAFFE and Cohn-Kanade) we demonstrate the efficiency,No DOI,Expert Systems with …,https://www.sciencedirect.com/science/article/pii/S095741741200930X,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
144,146,Generative moment matching networks,"['Y Li', 'K Swersky', 'R Zemel']",2015,1034,Toronto Face Database,machine learning,"On MNIST and the Toronto Face Dataset (TFD) we demonstrate improved results over  comparable baselines, including GANs. Source code for training GMMNs is available at https://",No DOI,… conference on machine learning,https://arxiv.org/abs/1502.02761,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"Generative Moment Matching Networks
YujiaLi1 YUJIALI@CS.TORONTO.EDU
KevinSwersky1 KSWERSKY@CS.TORONTO.EDU
RichardZemel1,2 ZEMEL@CS.TORONTO.EDU
1DepartmentofComputerScience,UniversityofToronto,Toronto,ON,CANADA
2CanadianInstituteforAdvancedResearch,Toronto,ON,CANADA
Abstract Kiros et al., 2014), machine translation (Cho et al., 2014;
Sutskeveretal.,2014),andmore. Despitetheirsuccesses,
Weconsidertheproblemoflearningdeepgener-
one of the main bottlenecks of the supervised approach is
ativemodelsfromdata. Weformulateamethod
thedifficultyinobtainingenoughdatatolearnabstractfea-
that generates an independent sample via a sin-
tures that capture the rich structure of the data. It is well
gle feedforward pass through a multilayer pre-
recognizedthatapromisingavenueistouseunsupervised
ceptron, as in the recently proposed generative
learningonunlabelleddata,whichisfarmoreplentifuland
adversarial networks (Goodfellow et al., 2014).
cheapertoobtain.
Training a generative adversarial network, how-
ever, requires careful optimization of a difficult A long-standing and inherent problem in unsupervised
minimax program. Instead, we utilize a tech- learning is defining a good method for evaluation. Gen-
nique from statistical hypothesis testing known erative models offer the ability to evaluate generalization
as maximum mean discrepancy (MMD), which inthedataspace,whichcanalsobequalitativelyassessed.
leadstoasimpleobjectivethatcanbeinterpreted In this work we propose a generative model for unsuper-
as matching all orders of statistics between a vised learning that we call generative moment matching
datasetandsamplesfromthemodel, andcanbe networks (GMMNs). GMMNs are generative neural net-
trained by backpropagation. We further boost worksthatbeginwithasimplepriorfromwhichitiseasy
the performance of this approach by combining to draw samples. These are propagated deterministically
ourgenerativenetworkwithanauto-encodernet- throughthehiddenlayersofthenetworkandtheoutputis
work,usingMMDtolearntogeneratecodesthat a sample from the model. Thus, with GMMNs it is easy
can then be decoded to produce samples. We to quickly draw independent random samples, as opposed
show that the combination of these techniques toexpensiveMCMCproceduresthatarenecessaryinother
yields excellent generative models compared to modelssuchasBoltzmannmachines(Ackleyetal.,1985;
baselineapproachesasmeasuredonMNISTand Hinton,2002;Salakhutdinov&Hinton,2009). Thestruc-
theTorontoFaceDatabase. ture of a GMMN is most analogous to the recently pro-
posed generative adversarial networks (GANs) (Goodfel-
lowetal.,2014),howeverunlikeGANs,whosetrainingin-
1.Introduction
volvesadifficultminimaxoptimizationproblem,GMMNs
are comparatively simple; they are trained to minimize a
Themostvisiblesuccessesintheareaofdeeplearninghave
straightforwardlossfunctionusingbackpropagation.
come from the application of deep models to supervised
learning tasks. Models such as convolutional neural net- ThekeyideabehindGMMNsistheuseofastatisticalhy-
works(CNNs), andlongshorttermmemory(LSTM)net- pothesistestingframeworkcalledmaximummeandiscrep-
worksarenowachievingimpressiveresultsonanumberof ancy (Gretton et al., 2007). Training a GMMN to mini-
tasks such as object recognition (Krizhevsky et al., 2012; mize this discrepancy can be interpreted as matching all
Sermanetetal.,2014;Szegedyetal.,2014),speechrecog- momentsofthemodeldistributiontotheempiricaldatadis-
nition(Graves&Jaitly,2014;Hintonetal.,2012a),image tribution. Usingthekerneltrick,MMDcanberepresented
captiongeneration(Vinyalsetal.,2014;Fangetal.,2014; as a simple loss function that we use as the core training
objective for GMMNs. Using minibatch stochastic gradi-
ent descent, training can be kept efficient, even with large
datasets.
Preliminarywork.
5102
beF
01
]GL.sc[
1v16720.2051:viXraGenerativeMomentMatchingNetworks
As a second contribution, we show how GMMNs can be The kernel trick implicitly lifts the sample vectors into
used to bootstrap auto-encoder networks in order to fur- an infinite dimensional feature space. When this feature
ther improve the generative process. The idea behind this spacecorrespondstoauniversalreproducingkernelHilbert
approachistotrainanauto-encodernetworkandthenap- space,itisshownthatasymptotically,MMDis0ifandonly
ply a GMMN to the code space of the auto-encoder. This ifP =P (Grettonetal.,2007;2012a).
X Y
allows us to leverage the rich representations learned by
For universal kernels like the Gaussian kernel, defined as
auto-encoder models as the basis for comparing data and
k(x,x(cid:48)) = exp(− 1 |x−x(cid:48)|2), whereσ isthebandwidth
model distributions. To generate samples in the original 2σ
parameter,wecanuseaTaylorexpansiontogetanexplicit
dataspace,wesimplysampleacodefromtheGMMNand
featuremapφthatcontainsaninfinitenumberoftermsand
thenusethedecoderoftheauto-encodernetwork.
coversallordersofstatistics. MinimizingMMDunderthis
Ourexperimentsshowthatthisrelativelysimple, yetvery feature expansion is then equivalent to minimizing a dis-
flexible framework is effective at producing good gener- tancebetweenallmomentsofthetwodistributions.
ative models in an efficient manner. On MNIST and the
TorontoFaceDataset(TFD)wedemonstrateimprovedre-
3.RelatedWork
sultsovercomparablebaselines, includingGANs. Source
code for training GMMNs will be made available at In this work we focus on generative models due to their
https://github.com/yujiali/gmmn. ability to capture the salient properties and structure of
data. Deep generative models are particularly appealing
2.MaximumMeanDiscrepancy because they are capable of learning a latent manifold on
which the data has high density. Learning this manifold
Suppose we are given two sets of samples X = {x }N allows smooth variations in the latent space to result in
i i=1
and Y = {y }M and are asked whether the generating non-trivialtransformationsintheoriginalspace,effectively
j j=1
distributions P = P . Maximum mean discrepancy is traversingbetweenhighdensitymodesthroughlowdensity
X Y
a frequentist estimator for answering this question, also areas(Bengioetal.,2013a).Theyarealsocapableofdisen-
knownasthetwosampletest(Grettonetal.,2007;2012a). tanglingfactorsofvariation,whichmeansthateachlatent
The idea is simple: compare statistics between the two variable can become responsible for modelling a single,
datasetsandiftheyaresimilarthenthesamplesarelikely complex transformation in the original space that would
tocomefromthesamedistribution. otherwise involve many variables (Bengio et al., 2013a).
Even if we restrict ourselves to the field of deep learning,
Formally,thefollowingMMDmeasurecomputesthemean
there are a vast array of approaches to generative mod-
squared difference of the statistics of the two sets of sam-
elling. Below,weoutlinesomeofthesemethods.
ples.
(cid:13) (cid:13)2 One popular class of generative models used in deep
(cid:13) N M (cid:13)
L MMD2 =(cid:13) (cid:13) (cid:13)N1 (cid:88) φ(x i)− M1 (cid:88) φ(y j)(cid:13) (cid:13)
(cid:13)
(1) l mea ar nn nin mg aa cr he inu en sd (i Are cc kt le ed yg etra ap l.h ,i 1c 9a 8l 5m ),o rd ee sl ts r, ics teu dch Boas ltzB mo alt nz n-
(cid:13) i=1 j=1 (cid:13)
machines (Hinton, 2002), and deep Boltzmann machines
N N N M
= 1 (cid:88)(cid:88) φ(x )(cid:62)φ(x )− 2 (cid:88)(cid:88) φ(x )(cid:62)φ(y ) (Salakhutdinov & Hinton, 2009). These models are nor-
N2 i i(cid:48) NM i j malized by a typically intractable partition function, mak-
i=1i(cid:48)=1 i=1j=1
ing training, evaluation, and sampling extremely difficult,
M M
+ 1 (cid:88)(cid:88) φ(y )(cid:62)φ(y ) (2) usually requiring expensive Markov-chain Monte Carlo
M2 j j(cid:48) (MCMC)procedures.
j=1j(cid:48)=1
Taking φ to be the identity function leads to matching the Nextthereistheclassoffullyvisibledirectedmodelssuch
samplemean,andotherchoicesofφcanbeusedtomatch as fully visible sigmoid belief networks (Neal, 1992) and
higherordermoments. theneuralautoregressivedistributionestimator(Larochelle
&Murray,2011). Theseadmitefficientlog-likelihoodcal-
Written in this form, each term in Equation (2) only in-
culation, gradient-based learning and efficient sampling,
volvesinnerproductsbetweentheφvectors,andtherefore
but require that an ordering be imposed on the observ-
thekerneltrickcanbeapplied.
able variables, which can be unnatural for domains such
N N N M asimagesandcannottakeadvantageofparallelcomputing
1 (cid:88)(cid:88) 2 (cid:88)(cid:88)
L MMD2 = N2 k(x i,x i(cid:48))− NM k(x i,y j) methodsduetotheirsequentialnature.
i=1i(cid:48)=1 i=1j=1
Morerelatedtoourownwork,thereisalineofresearchde-
M M
1 (cid:88)(cid:88) votedtorecoveringdensitymodelsfromauto-encodernet-
+ k(y ,y ) (3)
M2 j j(cid:48) worksusingMCMCprocedures(Rifaietal.,2012;Bengio
j=1j(cid:48)=1GenerativeMomentMatchingNetworks
etal.,2013b;2014). Theseattempttousecontractionop-
erators,ordenoisingcriteriainordertogenerateaMarkov GMMN
chainbyrepeatedperturbationsduringtheencodingphase, Uniform Prior
followedbydecoding.
Also related to our own work, there is the class of deep,
variational networks (Rezende et al., 2014; Kingma &
Welling, 2014; Mnih & Gregor, 2014). These are also ReLU
deep, directed generative models, however they make use
ReLU
ofanadditionalneuralnetworkthatisdesignedtoapproxi-
matetheposterioroverthelatentvariables. Trainingiscar- ReLU
riedoutviaavariationallowerboundonthelog-likelihood
Sigmoid of the model distribution. These models are trained us-
ing stochastic gradient descent, however they either re-
quire that the latent representation is continuous (Kingma
&Welling,2014), orrequiremanysecondarynetworksto
sufficientlyreducethevarianceofgradientestimatesinor-
dertoproduceasufficientlygoodlearningsignal(Mnih&
Gregor,2014).
Finally there is some early work that proposed the idea
of using feed-forward neural networks to learn generative
models. MacKay (1995) proposed a model that is closely
relatedtoours,whichalsousedafeed-forwardnetworkto
mapthepriorsamplestothedataspace. However,instead
of directly outputing samples, an extra distribution is as-
sociated with the output. Sampling was used extensively
forlearningandinferenceinthismodel. Magdon-Ismail&
Atiya (1998) proposed to use a neural network to learn a
transformationfromthedataspacetoanotherspacewhere
thetransformeddatapointsareuniformlydistributed. This
transformationnetworkthenlearnsthecumulativedensity
function.
4.GenerativeMomentMatchingNetworks
4.1.DataSpaceNetworks
The high-level idea of the GMMN is to use a neural net-
work to learn a deterministic mapping from samples of a
simple, easy to sample distribution, to samples from the
data distribution. The architecture of the generative net-
work is exactly the same as a generative adversarial net-
work (Goodfellow et al., 2014). However, we propose to
train the network by simply minimizing the MMD crite-
rion,avoidingthehardminimaxobjectivefunctionusedin
generativeadversarialnetworktraining.
More specifically, in the generative network we have a
stochastic hidden layer h ∈ RH with H hidden units at
thetopwithaprioruniformdistributiononeachunitinde-
pendently,
H
(cid:89)
p(h)= U(h ) (4)
j
j=1
Here U(h) = 1I[−1 ≤ h ≤ 1] is a uniform distribu-
2
Sample
Generation
GMMN
Uniform Prior
ReLU
Dropout
Sigmoid Sigmoid
Dropout
Sigmoid
Input Data Reconstruction
Auto-Encoder
Sample
Generation
ReLU
ReLU
ReLU
ReLU
Sigmoid
(a)GMMN (b)GMMN+AE
Figure1.Examplearchitecturesofourgenerativemomentmatch-
ing networks. (a) GMMN used in the input data space. (b)
GMMNusedinthecodespaceofanauto-encoder.
tion in [−1,1], where I[.] is an indicator function. Other
choices for the prior are also possible, as long as it is a
simpleenoughdistributionfromwhichwecaneasilydraw
samples.
Thehvectoristhenpassedthroughtheneuralnetworkand
deterministicallymappedtoavectorx ∈ RD intheD di-
mensionaldataspace.
x=f(h;w) (5)
f istheneuralnetworkmappingfunction,whichcancon-
tainmultiplelayersofnonlinearities,andwrepresentsthe
parameters of the neural network. One example architec-
ture for f is illustrated in Figure 1(a), which has 3 inter-
mediateReLU(Nair&Hinton,2010)nonlinearlayersand
onelogisticsigmoidoutputlayer.
Thepriorp(h)andthemappingf(h;w)jointlydefinesa
distribution p(x) in the data space. To generate a sample
x ∼ p(x)weonlyneedtosamplefromtheuniformprior
p(h)andthenpassthesamplehthroughtheneuralnetto
getx=f(h;w).
Goodfellowetal.(2014)proposedtotrainthisnetworkby
usinganextradiscriminativenetwork,whichtriestodistin-
guishbetweenmodelsamplesanddatasamples. Thegen-
erative network is then trained to counteract this in order
tomakethesamplesindistinguishabletothediscriminative
network. The gradient of this objective can be backprop-
agatedthroughthegenerativenetwork. However, because
of the minimax nature of the formulation, it is easy to get
stuck at a local optima. So the training of generative net-
work and the discriminative network must be interleaved
and carefully scheduled. By contrast, our learning algo-
rithmsimplyinvolvesminimizingtheMMDobjective.
Assumewehaveadatasetoftrainingexamplesxd,...,xd
1 N
(dfordata),andasetofsamplesgeneratedfromourmodelGenerativeMomentMatchingNetworks
xs,...,xs (sforsamples). TheMMDobjectiveL is Wefoundthataddingdropouttotheencodinglayerscanbe
1 M MMD2
differentiablewhenthekernelisdifferentiable. Forexam- beneficial in terms of creating a smooth manifold in code
ple for Gaussian kernels k(x,y) = exp(cid:0) − 1 ||x−y||2(cid:1) , space. Thisisanalogoustothemotivationbehindcontrac-
2σ
thegradientofxs hasasimpleform tive and denoising auto-encoders (Rifai et al., 2011; Vin-
ip
centetal.,2008).
M
∂L MMD2 = 2 (cid:88) 1 k(xs,xs)(xs −xs )
∂xs M2 σ i j jp ip 4.3.PracticalConsiderations
ip j=1
N Hereweoutlinesomedesignchoicesthatwehavefoundto
2 (cid:88) 1
− k(xs,xd)(xd −xs ) (6) improvethepeformanceofGMMNs.
MN σ i j jp ip
j=1
Bandwidth Parameter. The bandwidth parameter in the
Thisgradientcanthenbebackpropagatedthroughthegen- kernelplaysacrucialroleindeterminingthestatisticalef-
erativenetworktoupdatetheparametersw. ficiencyofMMD,andoptimallysettingitisanopenprob-
lem. Agoodheuristicistoperformalinesearchtoobtain
4.2.Auto-EncoderCodeSpaceNetworks the bandwidth that produces the maximal distance (Sripe-
rumbuduretal.,2009),othermoreadvancedheuristicsare
Real-worlddatacanbecomplicatedandhigh-dimensional,
alsoavailable(Grettonetal.,2012b). Asasimplerapprox-
which is one reason why generative modelling is such a
imation, for most of our experiments we use a mixture of
difficult task. Auto-encoders, on the other hand, are de-
Kkernelsspanningmultipleranges.Thatis,wechoosethe
signedtosolveanarguablysimplertaskofreconstruction.
kerneltobe:
Iftrainedproperly,auto-encodermodelscanbeverygood
at representing data in a code space that captures enough K
(cid:88)
k(x,x(cid:48))= k (x,x(cid:48)) (7)
statistical information that the data can be reliably recon- σq
structed. q=1
wherek isaGaussiankernelwithbandwidthparameter
Thecodespaceofanauto-encoderhasseveraladvantages σq
σ . Wefoundthatchoosingsimplevaluesforthesesuchas
for creating a generative model. The first is that the di- q
1,5,10,etc.andusingamixtureof5ormorewassufficient
mensionality canbe explicitly controlled. Visualdata, for
to obtain good results. The weighting of different kernels
example, while represented in a high dimension often ex-
can be further tuned to achieve better results, but we kept
istsonalow-dimensionalmanifold. Thisisbeneficialfora
themequallyweightedforsimplicity.
statisticalestimatorlikeMMDbecausetheamountofdata
requiredtoproduceareliableestimatorgrowswiththedi- SquareRootLoss. Inpractice, wehavefoundthatbetter
√
mensionality of the data (Ramdas et al., 2015). The sec- resultscanbeobtainedbyoptimizingL = L .
MMD MMD2
ondadvantageisthateachdimensionofthecodespacecan This loss can be important for driving the difference be-
enduprepresentingcomplexvariationsintheoriginaldata tweenthetwodistributionsascloseto0aspossible. Com-
space. Thisconceptisreferredtointheliteratureasdisen- pared to L which flattens out when its value gets
MMD2
tanglingfactorsofvariation(Bengioetal.,2013a). close to 0, L behaves much better for small L
MMD MMD
values. Alternatively, this can be understood by writing
For these reasons, we propose to bootstrap auto-encoder
downthegradientofL withrespecttow
models with a GMMN to create what we refer to as the MMD
GMMN+AE model. These operate by first learning an ∂L 1 ∂L
auto-encoder and producing code representations of the MMD = √ MMD2 (8)
∂w 2 L ∂w
data, then freezing the auto-encoder weights and learning
MMD2
√
aGMMNtominimizeMMDbetweengeneratedcodesand The 1/(2 L ) term automatically adapts the effec-
MMD2
datacodes. AvisualizationofthismodelisgiveninFigure tive learning rate. This is especially beneficial when both
1(b). L MMD2 and ∂L ∂M wMD2 becomesmall,wherethisextrafactor
canhelpbymaintaininglargergradients.
Our method for training a GMMN+AE proceeds as fol-
lows: MinibatchTraining. OneoftheissueswithMMDisthat
theusageofkernelsmeansthatthecomputationoftheob-
1. Greedy layer-wise pretraining of the auto-encoder jective scales quadratically with the amount of data. In
(Bengioetal.,2007). theliteraturetherehavebeenseveralalternativeestimators
designed to overcome this (Gretton et al., 2012a). In our
2. Fine-tunetheauto-encoder.
case,wefoundthatitwassufficienttooptimizeMMDus-
3. Train a GMMN to model the code layer distribution ingminibatchoptimization. Ineachweightupdate,asmall
usinganMMDobjectiveonthefinalencodinglayer. subset of data is chosen, and an equal number of samplesGenerativeMomentMatchingNetworks
Algorithm1:GMMNminibatchtraining Model MNIST TFD
DBN 138±2 1909±66
Input :Dataset{xd,...,xd },priorp(h),network
1 N StackedCAE 121±1.6 2110±50
f(h;w)withinitialparameterw(0)
DeepGSN 214±1.1 1890±29
Output:Learnedparameterw∗
Adversarialnets 225±2 2057±26
1 whileStoppingcriterionnotmetdo GMMN 147±2 2085±25
2 GetaminibatchofdataXd ←{xd i1,...,xd ib} GMMN+AE 282±2 2204±20
3 GetanewsetofsamplesXs ←{xs 1,...,xs b}
4 Computegradient ∂LMMD onXdandXs Table1.Log-likelihood of the test sets under different models.
∂w
5 Takeagradientsteptoupdatew ThebaselinesareDeepBeliefNet(DBN)andStackedContrac-
6 end tive Auto-Encoder (Stacked CAE) from (Bengio et al., 2013a),
DeepGenerativeStochasticNetwork(DeepGSN)from(Bengio
etal.,2014)andAdversarialnets(GANs)from(Goodfellowetal.,
2014).
are drawn from the GMMN. Within a minibatch, MMD
is applied as usual. As we are using exact samples from ontheencoderlayers. Aftertrainingtheauto-encoder,we
themodelandthedatadistribution,theminibatchMMDis fixeditandpassedtheinputdatathroughtheencodertoget
still a good estimator of the population MMD. We found the corresponding codes. The GMMN network was then
thisapproachtobebothfastandeffective. Theminibatch trained in this code space to match the statistics of gen-
trainingalgorithmforGMMNisshowninAlgorithm1. eratedcodestothestatisticsofcodesfromdataexamples.
Whengeneratingsamples,thegeneratedcodeswerepassed
5.Experiments
throughthedecodertogetsamplesintheinputdataspace.
We trained GMMNs on two benchmark datasets MNIST For all experiments in this section the GMMN networks
(LeCun et al., 1998) and the Toronto Face Dataset (TFD) weretrainedwithminibatchesofsize1000,foreachmini-
(Susskindetal.,2010). ForMNIST,weusedthestandard batch we generated a set of 1000 samples from the net-
testsetof10,000images,andsplitout5000fromthestan- work. The loss and gradient were computed from these
dard60,000trainingimagesforvalidation. Theremaining 2000points. WeusedthesquarerootlossfunctionL
MMD
55,000wereusedfortraining. ForTFD,weusedthesame throughout.
trainingandtestsetsandfoldsplitsasusedby(Goodfellow
Evaluation of our model is not straight-forward, as we do
etal.,2014),butsplitoutasmallsetofthetrainingdataand
nothaveanexplicitformfortheprobabilitydensityfunc-
useditasthevalidationset.Forbothdatasets,rescalingthe
tion, it is not easy to compute the log-likelihood of data.
imagestohavepixelintensitiesbetween0and1istheonly
However, sampling from our model is easy. We therefore
preprocessingstepwedid.
followedthesameevaluationprotocolusedinrelatedmod-
On both datasets, we trained the GMMN network in both els(Bengioetal.,2013a),(Bengioetal.,2014),and(Good-
theinputdataspaceandthecodespaceofanauto-encoder. fellow et al., 2014). A Gaussian Parzen window (kernel
For all the networks we used in this section, a uniform densityestimator)wasfitto10,000samplesgeneratedfrom
distribution in [−1,1]H was used as the prior for the the model. The likelihood of the test data was then com-
H-dimensional stochastic hidden layer at the top of the puted under this distribution. The scale parameter of the
GMMN, which was followed by 4 ReLU layers, and the Gaussianswasselectedusingagridsearchinafixedrange
output was a layer of logistic sigmoid units. The auto- usingthevalidationset.
encoderweusedforMNISThad4layers,2fortheencoder
Thehyperparametersofthenetworks,includingthelearn-
and2forthedecoder. ForTFDtheauto-encoderhad6lay-
ingrateandmomentumforbothauto-encoderandGMMN
ersintotal,3fortheencoderand3forthedecoder.Forboth
training, dropout rate for the auto-encoder, and number
auto-encoders the encoder and the decoder had mirrored
of hidden units on each layer of both auto-encoder and
architectures. Alllayersintheauto-encodernetworkused
GMMN, were tuned using Bayesian optimization (Snoek
sigmoidnonlinearities,whichalsoguaranteedthatthecode
etal.,2012;2014)1tooptimizethevalidationsetlikelihood
spacedimensionslayin[0,1],sothattheycouldmatchthe
undertheGaussianParzenwindowdensityestimation.
GMMNoutputs. ThenetworkarchitecturesforMNISTare
showninFigure1. The log-likelihood of the test set for both datasets are
shown in Table 1. The GMMN is competitive with other
The auto-encoders were trained separately from the
approaches, while the GMMN+AE significantly outper-
GMMN.Crossentropywasusedasthereconstructionloss.
Wefirstdidstandardlayer-wisepretraining,thenfine-tuned 1We used the service provided by https://www.
alllayersjointly. Dropout(Hintonetal.,2012b)wasused whetlab.comGenerativeMomentMatchingNetworks
(e)GMMNnearestneighborsforMNISTsamples
(a)GMMNMNISTsamples (b)GMMNTFDsamples (f)GMMN+AEnearestneighborsforMNISTsamples
(g)GMMNnearestneighborsforTFDsamples
(c)GMMN+AEMNISTsamples (d)GMMN+AETFDsamples (h)GMMN+AEnearestneighborsforTFDsamples
Figure2.IndependentsamplesandtheirnearestneighborsinthetrainingsetfortheGMMN+AEmodeltrainedonMNISTandTFD
datasets. For(e)(f)(g)and(h)thetoprowarethesamplesfromthemodelandthebottomrowarethecorrespondingnearestneighbors
fromthetrainingsetmeasuredbyEuclideandistance.
formstheothermodels. Thisshowsthatdespitebeingrel- points in the uniform space and show their corresponding
atively simple, MMD, especially when combined with an projections in data space. The manifold is smooth for the
effectivedecoder,isapowerfulobjectivefortraininggood most part, and almost all of the projections correspond to
generativemodels. realisticlookingdata.ForTFDinparticular,thesetransfor-
mationsinvolvecomplexattributes,suchasthechangingof
Some samples generated from the GMMN models are
pose,expression,lighting,gender,andfacialhair.
shown in Figure 2(a-d). The GMMN+AE produces the
mostvisuallyappealingsamples,whicharereflectedinits
Parzenwindowlog-likelihoodestimates. Thelikelyexpla- 6.ConclusionandFutureWork
nation is that any perturbations in the code space corre-
Inthispaperweprovideasimpleandeffectiveframework
spondtosmoothtransformationsalongthemanifoldofthe
for training deep generative models called generative mo-
data space. In that sense, the decoder is able to “correct”
mentmatchingnetworks.Ourapproachisbasedoffofopti-
noiseinthecodespace.
mizingmaximummeandiscrepancysothatsamplesgener-
To determine whether the models learned to merely copy atedfromthemodelareindistinguishablefromdataexam-
the data, we follow the example of (Goodfellow et al., plesintermsoftheirmomentstatistics.Asisstandardwith
2014) and visualize the nearest neighbour of several sam- MMD,theuseofthekerneltrickallowsaGMMNtoavoid
ples in terms of Euclidean pixel-wise distance in Figure explicitlycomputingthesemoments, resultinginasimple
2(e-h). By this metric, it appears as though the samples trainingobjective,andtheuseofminibatchstochasticgra-
arenotmerelydataexamples. dientdescentallowsthetrainingtoscaletolargedatasets.
One of the interesting aspects of a deep generative model Our second contribution combines MMD with auto-
such as the GMMN is that it is possible to directly ex- encodersforlearningagenerativemodelofthecodelayer.
plore the data manifold. Using the GMMN+AE model, Thecodesamplesfromthemodelcanthenbefedthrough
we randomly sampled 5 points in the uniform space and the decoder in order to generate samples in the origi-
show their corresponding data space projections in Fig- nal space. The use of auto-encoders makes the gener-
ure3. Thesepointsarehighlightedbyredboxes. Fromleft ative model learning a much simpler problem. Com-
toright,toptobottomwelinearlyinterpolatebetweenthese bined with MMD, pretrained auto-encoders can be read-GenerativeMomentMatchingNetworks
2012a).
Another possibility is to utilize random features (Rahimi
&Recht,2007). Thesearerandomizedfeatureexpansions
whoseinnerproductconvergestoakernelfunctionwithan
increasing number of features. This idea was recently ex-
ploredforMMDin(Zhao&Meng,2014). Theadvantage
of this approach would be that the cost would no longer
grow quadratically with minibatch size because we could
usetheoriginalobjectivegiveninEquation2. Anotherad-
vantageofthisapproachisthatthedatastatisticscouldbe
pre-computedfromtheentiredataset,whichwouldreduce
thevarianceoftheobjectivegradients.
Another direction we would like to explore is joint train-
ingoftheauto-encodermodelwiththeGMMN.Currently,
(a) MNISTinterpolation thesearetreatedseparately,butjointtrainingmayencour-
age the learning of codes that are both suitable for recon-
structionaswellasgeneration.
WhileaGMMNprovidesaneasywaytosampledata,the
posteriordistributionoverthelatentvariablesisnotreadily
available. Itwouldbeinterestingtoexplorewaysinwhich
to infer the posterior distribution over the latent space. A
straightforwardwaytodothisistolearnaneuralnetwork
topredictthelatentvectorgivenasample. Thisisreminis-
cent of the recognition models used in the wake-sleep al-
gorithm(Hintonetal.,1995),orvariationalauto-encoders
(Kingma&Welling,2014).
AninterestingapplicationofMMDthatisnotdirectlyre-
lated to generative modelling comes from recent work on
learning fair representations (Zemel et al., 2013). There,
theobjectiveistotrainapredictionmethodthatisinvariant
(b) TFDinterpolation toaparticularsensitiveattributeofthedata. Theirsolution
istolearnanintermediateclustering-basedrepresentation.
Figure3.Linearinterpolationbetween5uniformrandompoints MMD could instead be applied to learn a more powerful,
from the GMMN+AE prior projected through the network into distributedrepresentationsuchthatthestatisticsoftherep-
dataspacefor(a)MNISTand(b)TFD.The5randompointsare resentationdonotchangeconditionedonthesensitivevari-
highlightedwith redboxes, and theinterpolation goesfrom left
able.Thisideacanbefurthergeneralizedtolearnrepresen-
toright,toptobottom. Thefinaltworowsrepresentaninterpo-
tationsinvarianttoknownbiases.
lationbetweenthelasthighlightedimageandthefirsthighlighted
image. Finally, the notion of utilizing an auto-encoder with the
GMMN+AEmodelprovidesnewavenuesforcreatinggen-
erativemodelsofevenmorecomplexdatasets. Forexam-
ple,itmaybepossibletouseaGMMN+AEwithconvolu-
ilybootstrappedintoagoodgenerativemodelofdata. On
tionalauto-encoders(Zeileretal.,2010;Mascietal.,2011;
the MNIST and Toronto Face Database, the GMMN+AE
Makhzani&Frey,2014)inordertocreategenerativemod-
model achieves superior performance compared to other
elsofhighresolutioncolorimages.
approaches. For these datasets, we demonstrate that the
GMMN+AE is able to discover the implicit manifold of
thedata. Acknowledgements
There are many interesting directions for research using WethankDavidWarde-Farleyforhelpfulclarificationsre-
MMD.Onesuchextensionistoconsideralternativestothe garding (Goodfellow et al., 2014), and Charlie Tang for
standardMMDcriterioninordertospeeduptraining. One providingrelevantreferences. WethankCIFAR,NSERC,
such possibility is the class of linear-time estimators that andGoogleforresearchfunding.
hasbeendevelopedrecentlyintheliterature(Grettonetal.,GenerativeMomentMatchingNetworks
References Optimal kernel choice for large-scale two-sample tests.
InAdvancesinNeuralInformationProcessingSystems,
Ackley,D.H.,Hinton,G.E.,andSejnowski,T.J. Alearn-
pp.1205–1213,2012b.
ing algorithm for boltzmann machines. Cognitive sci-
ence,9(1):147–169,1985. Hinton, G. E. Training products of experts by minimiz-
ingcontrastivedivergence. NeuralComputation,14(8):
Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H.,
1771–1800,2002.
et al. Greedy layer-wise training of deep networks.
In Advances in Neural Information Processing Systems Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M.
(NIPS),2007. The“wake-sleep”algorithmforunsupervisedneuralnet-
works. Science,268(5214):1158–1161,1995.
Bengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. Bet-
ter mixing via deep representations. In Proceedings of Hinton, G. E., Deng, L., Yu, D., Dahl, G. E., Mohamed,
the28thInternationalConferenceonMachineLearning A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P.,
(ICML),2013a. Sainath,T.N.,andKingsbury,B. Deepneuralnetworks
foracousticmodelinginspeechrecognition: Theshared
Bengio,Y.,Yao,L.,Alain,G.,andVincent,P. Generalized
views of four research groups. IEEE Signal Process.
denoising auto-encoders as generative models. In Ad-
Mag.,29(6):82–97,2012a.
vances in Neural Information Processing Systems, pp.
899–907,2013b. Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever,
I.,andSalakhutdinov,R.R. Improvingneuralnetworks
Bengio, Y., Thibodeau-Laufer, E., Alain, G., and Yosin-
by preventing co-adaptation of feature detectors. arXiv
ski,J. Deepgenerativestochasticnetworkstrainableby
preprintarXiv:1207.0580,2012b.
backprop. InProceedingsofthe29thInternationalCon-
ferenceonMachineLearning(ICML),2014. Kingma,D.P.andWelling,M. Auto-encodingvariational
Bayes. InInternationalConferenceonLearningRepre-
Cho, K., vanMerrienboer, B., Gulcehre, C., Bougares, F.,
sentations,2014.
Schwenk,H.,andBengio,Y. Learningphraserepresen-
tationsusingrnnencoder-decoderforstatisticalmachine Kiros, R., Salakhutdinov, R., and Zemel, R. S. Unify-
translation.InConferenceonEmpiricalMethodsinNat- ing visual-semantic embeddings with multimodal neu-
uralLanguageProcessing(EMNLP),2014. ral language models. arXiv preprint arXiv:1411.2539,
2014.
Fang, H., Gupta, S., Iandola, F., Srivastava, R., Deng, L.,
Dolla´r,P.,Gao,J.,He,X.,Mitchell,M.,Platt,J.,Zitnick, Krizhevsky,A.,Sutskever,I.,andHinton,G.E. Imagenet
C. L., and Zweig, G. From captions to visual concepts classification with deep convolutional neural networks.
andback. arXivpreprintarXiv:1411.4952,2014. In Advances in Neural Information Processing Systems
(NIPS),2012.
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Larochelle, H. and Murray, I. The neural autoregressive
Y. Generative adversarial nets. In Advances in Neural distributionestimator. Inroceedingsofthe14thInterna-
InformationProcessingSystems,pp.2672–2680,2014. tionalConferenceonArtificialIntelligenceandStatistics
(AISTATS),2011.
Graves,A.andJaitly,N.Towardsend-to-endspeechrecog-
nitionwithrecurrentneuralnetworks. InProceedingsof LeCun,Y.,Bottou,L.,Bengio,Y.,andHaffner,P.Gradient-
the31stInternationalConferenceonMachineLearning based learning applied to document recognition. Pro-
(ICML-14),pp.1764–1772,2014. ceedingsoftheIEEE,86(11):2278–2324,1998.
Gretton, A., Borgwardt, K. M., Rasch, M., Scho¨lkopf, B., MacKay,D.J. Bayesianneuralnetworksanddensitynet-
and Smola, A. J. A kernel method for the two-sample- works. NuclearInstrumentsandMethodsinPhysicsRe-
problem. InAdvancesinNeuralInformationProcessing search Section A: Accelerators, Spectrometers, Detec-
Systems(NIPS),2007. torsandAssociatedEquipment,354(1):73–80,1995.
Gretton,A.,Borgwardt,K.M.,Rasch,M.J.,Scho¨lkopf,B., Magdon-Ismail,M.andAtiya,A.Neuralnetworksforden-
andSmola,A. Akerneltwo-sampletest. TheJournalof sityestimation. InNIPS,pp.522–528,1998.
MachineLearningResearch,13(1):723–773,2012a.
Makhzani, A. and Frey, B. A winner-take-all method for
Gretton,A.,Sejdinovic,D.,Strathmann,H.,Balakrishnan, training sparse convolutional autoencoders. In NIPS
S.,Pontil,M.,Fukumizu,K.,andSriperumbudur,B.K. DeepLearningWorkshop,2014.GenerativeMomentMatchingNetworks
Masci, J., Meier, U., Cires¸an, D., and Schmidhuber, Snoek,J.,Swersky,K.,Zemel,R.S.,andAdams,R.P. In-
J. Stacked convolutional auto-encoders for hierarchi- putwarpingforbayesianoptimizationofnon-stationary
calfeatureextraction. InArtificialNeuralNetworksand functions. In International Conference on Machine
Machine Learning–ICANN 2011, pp. 52–59. Springer, Learning,2014.
2011.
Sriperumbudur, B. K., Fukumizu, K., Gretton, A., Lanck-
Mnih,A.andGregor,K. Neuralvariationalinferenceand riet,G.R.,andScho¨lkopf,B. Kernelchoiceandclassifi-
learninginbeliefnetworks. InInternationalConference abilityforrkhsembeddingsofprobabilitydistributions.
onMachineLearning,2014. InAdvancesinNeuralInformationProcessingSystems,
pp.1750–1758,2009.
Nair, V. and Hinton, G. E. Rectified linear units improve
restrictedboltzmannmachines. InInternationalConfer- Susskind,J.,Anderson,A.,andHinton,G.E. Thetoronto
enceonMachineLearning,pp.807–814,2010. facedataset. Technicalreport,DepartmentofComputer
Science,UniversityofToronto,2010.
Neal,R.M. Connectionistlearningofbeliefnetworks. Ar-
Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to se-
tificialintelligence,56(1):71–113,1992.
quence learning with neural networks. In Advances in
Rahimi,A.andRecht,B. Randomfeaturesforlarge-scale NeuralInformationProcessingSystems,pp.3104–3112,
kernel machines. In Advances in Neural Information 2014.
ProcessingSystems(NIPS),2007.
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,
Ramdas, A., Reddi, S. J., Poczos, B., Singh, A., and Anguelov, D., Erhan, D., Vanhoucke, V., and Rabi-
Wasserman, L. On the decreasing power of kernel and novich, A. Going deeper with convolutions. arXiv
distancebasednonparametrichypothesistestsinhighdi- preprintarXiv:1409.4842,2014.
mensions. InTheTwenty-NinthAAAIConferenceonAr-
Vincent, P., Larochelle, H., Bengio, Y., andManzagol, P.-
tificialIntelligence(AAAI-15),2015.
A. Extracting and composing robust features with de-
Rezende,D.J.,Mohamed,S.,andWierstra,D. Stochastic noising autoencoders. In Proceedings of the 25th in-
backpropagationandapproximateinferenceindeepgen- ternationalconferenceonMachine learning, pp.1096–
erativemodels. InInternationalConferenceonMachine 1103.ACM,2008.
Learning,pp.1278–1286,2014.
Vinyals, O., Toshev, A., Bengio, S., and Erhan, D. Show
and tell: A neural image caption generator. arXiv
Rifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio,
preprintarXiv:1411.4555,2014.
Y.Contractiveauto-encoders:Explicitinvarianceduring
feature extraction. In Proceedings of the 28th Interna-
Zeiler,M.D.,Krishnan,D.,Taylor,G.W.,andFergus,R.
tionalConferenceonMachineLearning(ICML-11),pp.
Deconvolutionalnetworks. InComputerVisionandPat-
833–840,2011.
ternRecognition,pp.2528–2535.IEEE,2010.
Rifai,S.,Bengio,Y.,Dauphin,Y.,andVincent,P. Agener- Zemel,R.,Wu,Y.,Swersky,K.,Pitassi,T.,andDwork,C.
ativeprocessforsamplingcontractiveauto-encoders. In Learning fair representations. In International Confer-
InternationalConferenceonMachineLearning(ICML), enceonMachineLearning,pp.325–333,2013.
2012.
Zhao, J. and Meng, D. Fastmmd: Ensemble of circular
Salakhutdinov,R.andHinton,G.E. Deepboltzmannma- discrepancyforefficienttwo-sampletest. arXivpreprint
chines. InInternationalConferenceonArtificialIntelli- arXiv:1405.2664,2014.
genceandStatistics,2009.
Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus,
R.,andLeCun,Y. Overfeat: Integratedrecognition,lo-
calization and detection using convolutional networks.
In International Conference on Learning Representa-
tions,2014.
Snoek, J., Larochelle, H., and Adams, R. P. Practical
Bayesian optimization of machine learning algorithms.
InAdvancesinNeuralInformationProcessingSystems,
2012."
145,147,Going deeper in facial expression recognition using deep neural networks,"['A Mollahosseini', 'D Chan']",2016,1245,"Acted Facial Expressions In The Wild, Affective Faces Database, MMI Facial Expression, Static Facial Expression in the Wild, Toronto Face Database","CNN, FER, classification, classifier, deep learning, facial expression recognition, machine learning, neural network","We evaluate the proposed method on well-known publicly available facial expression  databases: CMU MultiPIE [15], MMI [36], Denver Intensity of Spontaneous Facial Actions (DISFA) [",No DOI,2016 IEEE Winter …,https://arxiv.org/abs/1511.04110,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,arxiv.org,"Going Deeper in Facial Expression Recognition using Deep Neural Networks
AliMollahosseini1,DavidChan2,andMohammadH.Mahoor1,2
1 DepartmentofElectricalandComputerEngineering
2 DepartmentofComputerScience
UniversityofDenver,Denver,CO
ali.mollahosseini@du.edu, davidchan@cs.du.edu, and mmahoor@du.edu ∗†
Abstract through which HMI systems can recognize humans’ inter-
nalemotions. Ekmanetal. identifiedsixfacialexpressions
AutomatedFacialExpressionRecognition(FER)hasre- (viz. anger,disgust,fear,happiness,sadness,andsurprise)
mainedachallengingandinterestingproblem. Despiteef- asbasicemotionalexpressionsthatareuniversalamonghu-
forts made in developing various methods for FER, exist- manbeings[11].
ingapproachestraditionallylackgeneralizabilitywhenap- Due to the importance of facial expression in design-
plied to unseen images or those that are captured in wild ingHMIandHumanRobotInteraction(HRI)systems[33],
setting. Mostoftheexistingapproachesarebasedonengi- numerous computer vision and machine learning algo-
neered features (e.g. HOG, LBPH, and Gabor) where the rithmshavebeenproposedforautomatedFacialExpression
classifier’shyperparametersaretunedtogivebestrecogni- Recognition (FER). Also, there exist many annotated face
tionaccuraciesacrossasingledatabase,orasmallcollec- databaseswitheitherhumanactorsportrayingbasicexpres-
tionofsimilardatabases. Nevertheless, theresultsarenot sions[15,35,28,29],orfacescapturedspontaneouslyinan
significant when they are applied to novel data. This pa- uncontrolledsetting[9,29].AutomatedFERapproachesat-
perproposesadeepneuralnetworkarchitecturetoaddress tempttoclassifyfacesinagivensingleimageorsequence
theFERproblemacrossmultiplewell-knownstandardface ofimagesasoneofthesixbasicemotions. Although,tradi-
datasets. Specifically,ournetworkconsistsoftwoconvolu- tional machine learning approaches such as support vector
tional layers each followed by max pooling and then four machines,andtoalesserextent,Bayesianclassifiers,have
Inception layers. The network is a single component ar- been successful when classifying posed facial expressions
chitecture that takes registered facial images as the input inacontrolledenvironment,recentstudieshaveshownthat
and classifies them into either of the six basic or the neu- these solutions do not have the flexibility to classify im-
tralexpressions. Weconductedcomprehensiveexperiments ages captured in a spontaneous uncontrolled manner (“in
on seven publically available facial expression databases, the wild”) or when applied databases for which they were
viz. MultiPIE, MMI, CK+, DISFA, FERA, SFEW, and notdesigned[30]. Thispoorgeneralizabilityofthesemeth-
FER2013. The results of proposed architecture are com- ods is primarily due to the fact that many approaches are
parable to or better than the state-of-the-art methods and subject or database dependent and only capable of recog-
better than traditional convolutional neural networks and nizing exaggerated or limited expressions similar to those
inbothaccuracyandtrainingtime. inthetrainingdatabase. ManyFERdatabaseshavetightly
controlled illumination and pose conditions. In addition,
obtainingaccuratetrainingdataisparticularlydifficult,es-
1.Introduction
peciallyforemotionssuchassadnessorfearwhichareex-
tremelydifficulttoaccuratelyreplicateanddonotoccurof-
Current Human Machine Interaction (HMI) systems
tenreallife.
have yet to reach the full emotional and social capabilities
necessary for rich and robust interaction with human be- Recently, due to an increase in the ready availabil-
ings. Facial expression, which plays a vital role in social ity of computational power and increasingly large training
interaction,isoneofthemostimportantnonverbalchannels databasestowork with, themachinelearningtechnique of
neural networks has seen resurgence in popularity. Recent
∗ThisworkispartiallysupportedbytheNSFgrantsIIS-1111568and
stateoftheartresultshavebeenobtainedusingneuralnet-
CNS-1427872.
†TobeappearinIEEEWinterConferenceonApplicationsofComputer worksinthefieldsofvisualobjectrecognition[20,41],hu-
Vision(WACV),2016 manposeestimation[45],faceverification[43],andmany
1
5102
voN
21
]EN.sc[
1v01140.1151:viXramore.EvenintheFERfieldresultssofarhavebeenpromis- Gradients (HoG) [29], or motion features such as optical
ing [17]. Unlike traditional machine learning approaches flow[18],MotionHistoryImages(MHI)[46],andvolume
wherefeaturesaredefinedbyhand,weoftenseeimprove- LBP [51]. Current state-of-the-art methods, such as those
mentinvisualprocessingtaskswhenusingneuralnetworks used in Zhang et al. [49, 50] fuse multiple features using
because of the network’s ability to extract undefined fea- multiplekernellearningalgorithms.Howeverbyusingneu-
tures from the training database. It is often the case that ralnetworks,wedonothavetoworryaboutthefeaturese-
neural networks that are trained on large amounts of data lectionstep-asneuralnetworkshavethecapacitytolearn
are able to extract features generalizing well to scenarios featuresthatstatisticallyallowthenetworktomakecorrect
that the network has not been trained on. We explore this classifications of the input data. In the third step, of clas-
idea closely by training our proposed network architecture sification, thealgorithmattemptstoclassifythegivenface
onasubsetoftheavailabletrainingdatabases,andthenper- as portraying one of the six basic emotions using machine
forming cross-database experiments which allow us to ac- learningtechniques.
curately judge the network’s performance in novel scenar-
Ekman et al. [6, 12] distinguished two conceptual ap-
ios.
proaches to studying facial behavior: a “message-based”
In the FER problem, however, unlike visual object
approachanda“sign-based”approach. Message-basedap-
databasessuchasimageNet[8],existingFERdatabasesof-
proaches categorize facial behaviors as the the meaning of
ten have limited numbers of subjects, few sample images
expressions,whereassign-basedapproachesdescribefacial
or videos per expression, or small variation between sets,
actions/configuration regardless of the action’s meaning.
makingneuralnetworkssignificantlymoredifficulttotrain.
Themostwell-knownandwidelyusedsign-basedapproach
Forexample,theFER2013database[1](oneofthelargest
istheFacialActionCodingSystem(FACS)[12]. FACSde-
recently released FER databases) contains 35,887 images
scribeshumanfacialmovementsbytheirappearanceonthe
of different subjects yet only 547 of the images portray
faceusingstandardfacialsubstructurescalledActionUnits
disgust. Similarly, the CMU MultiPIE face database [15]
(AUs).EachAUisbasedononeorafewfacialmusclesand
containsaround750,000imagesbutitiscomprisedofonly
AUsmayoccurindividuallyorincombinations. Similarly,
337differentsubjects,where348,000imagesportrayonlya
FERalgorithmscanbecategorizedintobothmessage-based
“neutral”emotionandtheremainingimagesdonotportray
and sign-based approaches. In message-based approaches
anger,fearorsadness.
FER algorithms are trained on databases labeled with the
This paper presents a novel deep neural network archi-
six basic expressions [7], and more recently, embarrass-
tecture for the FER problem, and examines the network’s
mentandcontempt[27].Unlikemessage-basedalgorithms,
abilitytoperformcross-databaseclassificationwhiletrain-
sign-based algorithms are trained to detect AUs in a given
ingondatabasesthathavelimitedscope,andareoftenspe-
image or sequence of images [7]. These detected AUs are
cialized for a few expressions (e.g. MultiPIE and FERA).
thenconvertedtoemotion-specifiedexpressionsusingEM-
We conducted comprehensive experiments on seven well-
FACS [14] or similar systems [42]. In this paper, we de-
known facial expression databases (viz. MultiPIE, MMI,
velopamessage-basedneuralnetworksolution,
CK+,DISFA,FERA,SFEW,andFER2013)andobtainre-
FERsystemsaretraditionallyevaluatedineitherasub-
sults which are significantly better than, or comparable to,
ject independent manner or a cross-database manner. In
traditionalconvolutionalneuralnetworksorotherstate-of-
subject independent evaluation, the classifier is trained on
the-artmethodsinbothaccuracyandlearningtime.
a subset of images in a database (called the training set)
and evaluated on faces in the same database that are not
2.BackgroundandRelatedWork
elements of the training set often using K-fold cross val-
Algorithms for automated FER usually involve three idation or leave-one-subject-out approaches. The cross-
main steps, viz. registration, feature extraction, and clas- databasemethodofevaluatingfacialexpressionsystemsre-
sification. In the face registration step, faces are first lo- quirestrainingtheclassifieronalloftheimagesinasingle
cated in the image using some set of landmark points dur- databaseandevaluatingtheclassifieronadifferentdatabase
ing“facelocalization”or“facedetection”. Thesedetected which the classifier has never seen images from. As sin-
facesarethengeometricallynormalizedtomatchsometem- gledatabaseshavesimilarsettings(illumination,pose,res-
plate image in a process called “face registration”. In the olution etc.), subject independent tasks are easier to solve
feature extraction step, a numerical feature vector is gen- than cross database tasks. Subject independent evaluation
erated from the resulting registered image. These features isnot,however,unimportant. Ifaresearchercanguarantee
can be geometric features such as facial landmarks [19], thatthedatawillalignwellinpose,illuminationandother
appearance features such as pixel intensities [32], Ga- factorswiththetrainingset,subjectindependentevaluation
bor filters [23], Local Binary Patterns (LBP) [37], Local can give a reasonably good representation of the classifi-
PhaseQuantization(LPQ)[52],andHistogramofOriented cation accuracy in an online system. Another technique,subjectdependentevaluation(person-specific),isalsoused micro-networkinthelargernetwork,allowingthearchitec-
in limited cases, e.g. FERA 2011 challenge [47]; often in turetomakemorecomplexdecisions.
thesescenariostherecognitionaccuracyismoreimportant More traditional CNN architectures have also achieved
thanthegeneralization. remarkable results. AlexNet [20] is an architecture that is
Recentapproachestovisualobjectrecognitiontasks,and based on the traditional CNN layered architecture - stacks
theFERproblemhaveusedincreasingly“deep”neuralnet- ofconvolutionslayersfollowedbymax-poolinglayersand
works (neural networks with large numbers of hidden lay- rectifiedlinearunits(ReLUs), withanumberoffullycon-
ers). Theterm“deepneuralnetwork”referstoarelatively nectedlayersatthetopofthelayerstack. Theirtop=5error
newsetoftechniquesinneuralnetworkarchitecturedesign rateof15.3%ontheILSVRC-2012competitionrevolution-
thatweredevelopedinordertoimprovetheabilityofneu- izedthewaythatwethinkabouttheeffectivenessofCNNs.
ral networks to tackle big-data problems. With the large Thisnetworkwasalsooneofthefirstnetworkstointroduce
amount of available computing power continuing to grow, the “dropout” method for solving the over fitting problem
deepneuralnetworkarchitecturesprovidealearningarchi- (Suggested by Hinton et al. [38]) which proved key in de-
tecturebasedinthedevelopmentof“brain-like”structures velopinglargeneuralnetworks. Oneofthelargechallenges
which can learn multiple levels of representation and ab- to overcome in the use of traditional CNN architectures is
straction which allow algorithms for finding complex pat- theirdepthandcomputationalcomplexity.ThefullAlexNet
ternsinimages,sound,andtext. network performs on the order of 100M operations for a
It seems only logical to extend cutting-edge techniques single iteration, while SVM and shallow neural networks
in the field of “deep learning” to the FER problem. Deep perform far fewer operations in order to create a suitable
networkshavearemarkableabilitytoperformwellinflexi- model. ThismakestraditionalCNNsveryhardtoapplyin
blelearningtasks,suchasthecross-databaseevaluationsit- timerestrictivescenarios.
uation, where it is unlikely that hand-crafted features will In [24] a new deep neural network architecture, called
easilygeneralizetoanewscenario. Bytrainingneuralnet- an “AU-Aware” architecture was proposed in order to in-
works,particularlydeepneuralnetworks,forfeaturerecog- vestigate the FER problem. In an AU-Aware architecture,
nitionandextractionwecandrasticallyreducetheamount thebottomofthelayerstackconsistsofconvolutionlayers
of time that is necessary to implement a solution to the andmax-poolinglayerswhichareusedtogenerateacom-
FERproblemthat,evenwhenconfrontedwithanoveldata pleterepresentationoftheface. Nextinthelayerstack,an
source, will be able to perform at high levels of accuracy. “AU-awarereceptivefieldlayer”generatesacompleterep-
Similarly,weseedeepneuralnetworksperformingwellin resentation over all possible spatial regions by convolving
the subject independent evaluation scenarios, as the algo- the dense-sampling facial patches with special filters in a
rithmscanlearntorecognizesubtlefeaturesthatevenfield greedy manner. Then, a multilayer Restricted Boltzmann
experts can miss. These correlations provide the motiva- Machine(RBM)isexploitedtolearnhierarchicalfeatures.
tion for this paper, as the strengths of deep learning seem Finally,theoutputsofthenetworkareconcatenatedasfea-
to align perfectly with the techniques required for solving tureswhichareusedtotrainalinearSVMclassifierforrec-
difficult“inthewild”FERproblems. ognizing the six basic expressions. Results in [24] show
A subset of deep neural network architectures called thatthefeaturesgeneratedbythis“AU-Aware”networkare
“convolutional neural networks” (CNNs) have become the competitive with or superior to handcrafted features such
traditional approach for researchers studying vision and as LBP, SIFT, HoG, and Gabor on the CK+, MMI and
deep learning. In the 2014 ImageNet challenge for ob- databases using a similar SVM. However, AU-aware lay-
jectrecognition,thetopthreefinishersallusedaCNNap- ers do not necessarily detect FACS defined action units in
proach, with the GoogLeNet architecture achieving a re- faces.
markable 6.66% error rate in classification [41, 36]. The In [17] multiple deep neural network architectures are
GoogLeNetarchitectureusesanovelmulti-scaleapproach combined to solve the FER problem in video analysis.
byusingmultipleclassifierstructures,combinedwithmulti- These network architectures included: (1) an architecture
plesourcesforbackpropagation. Thisarchitecturedefeats similartotheAlexNetCNNrunonindividualframesofthe
a number of problems that occur when back-propagation video, (2)adeepbeliefnetworktrainedonaudioinforma-
decaysbeforereachingbeginninglayersinthearchitecture. tion,(3)anautoencodertomodelthespatiotemporalproper-
AdditionallayersthatreducedimensionallowGoogLeNet tiesofhumanactivity,and(4)ashallownetworkfocusedon
to increase in both width and depth without significant themouth. TheCNNistrainedontheprivateTorontoFace
penalties, and take an elegant step towards complicated Database [40] and fine tuned on the AFEW database [9],
network-in-network architectures described originally in yielded an accuracy of 35.58% when evaluated in a sub-
Linetal.[22]. Inotherword,thearchitectureiscomposed ject independent manner on AFEW. When combined with
of multiple “Inception” layers, each of which acts like a a single predictor, the five architectures produced an ac-curacy of 41.03% on the test set, the highest accuracy in us to increase the depth of the network significantly with-
the EmotiW 2013 [9] challenge, where challenge winner outworryingaboutthesmallcorpusofimagesthatweare
2014 [26] achieved 50.40% on test set using multiple ker- workingwithintheFERproblem.
nelmethodsonRiemannianmanifold. Theworkthatwepresentinthispaperisinspiredbythe
A 3D CNN with deformable action parts constraints techniquesprovidedbytheGoogLeNetandAlexNetarchi-
is introduced in [25] which can detect specific facial ac- tectures described in Sec. 2. Our network consists of two
tion parts under the structured spatial constraints, and ob- elements,firstournetworkcontainsoftwotraditionalCNN
tainthediscriminativepart-basedrepresentationsimultane- modules(atraditionalCNNlayerconsistsofaconvolution
ously. Theresultsontwoposedexpressiondatasets, CK+, layer by a max pooling layer). Both of these modules use
MMI, and a spontaneous dataset FERA achieve state-of- rectifiedlinearunits(ReLU)whichhaveanactivationfunc-
the-artvideo-basedexpressionrecognitionaccuracy. tiondescribedby:
3.ProposedMethod f(x)=max(0,x)
Oftenimprovingneuralnetworkarchitectureshasrelied where x is the input to the neuron [20]. Using the ReLU
onincreasingthenumberofneuronsorincreasingthenum- activation function allows us to avoid the vanishing gradi-
ber of layers, allowing the network to learn more complex entproblemcausedbysomeotheractivationfunctions(for
functions;however,increasingthedepthandcomplexityof moredetailssee[20]). Followingthesemodules,weapply
atopologyleadstoanumberofproblemssuchasincreased the techniques of the network in network architecture and
over-fitting of training data, and increased computational addtwo”Inception”stylemodules,whicharemadeupofa
needs. A natural solution to the problem of increasingly 1×1, 3×3and5×5convolutionlayers(UsingReLU)in
densenetworksistocreatedeepsparsenetworks,whichhas parallel. These layers are then concatenated as output and
bothbiologicalinspiration,andhasfirmtheoreticalfounda- weusetwofullyconnectedlayersastheclassifyinglayers
tions discussed in Arora et al. [3]. Unfortunately, current (Also usingReLU). Figure 1shows thearchitecture of the
GPUs and CPUs do not have the capability to efficiently networkusedinthispaper.
compute actions on sparse networks. The Inception layer In this work we register facial images in each of
presentedin[36]attemptstorectifytheseconcernsbypro- the databases using research standard techniques. We
vidinganapproximationofsparsenetworkstogainthethe- used bidirectional warping of Active Appearance Model
oretical benefits proposed by Arora et al., however retains (AAM) [34] and a Supervised Descent Method (SDM)
thedensestructurerequiredforefficientcomputation. called IntraFace [48] to extract facial landmarks, however
Applying the Inception layer to applications of Deep furtherworkcouldconsiderimprovingthelandmarkrecog-
Neural Network has had remarkable results, as implied nition in order to extract more accurate faces. IntraFace
by [39] and [41], and it seems only logical to extend state usesSIFTfeaturesforfeaturemappingandtrainsadescent
ofthearttechniquesusedinobjectrecognitiontotheFER methodbyalinearregressionontrainingsetinordertoex-
problem. Inadditiontomerelyprovidingtheoreticalgains tract 49 points. We use these points to register faces to an
from the sparsity, and thus, relative depth, of the network, average face in an affine transformation. Finally, a fixed
theInceptionlayeralsoallowsforimprovedrecognitionof rectanglearoundtheaveragefaceisconsideredastheface
local features, as smaller convolutions are applied locally, region. Figure2demonstratessamplesofthefaceregistra-
whilelargerconvolutionsapproximateglobalfeatures. The tion with this method. In our research, facial registration
increased local performance seems to align logically with increased the accuracy of our FER algorithms by 4-10%,
the way that humans process emotions as well. By look- which suggests that registration (like normalization in tra-
ing at local features such as the eyes and mouth, humans ditionalproblems)isasignificantportionofanyFERalgo-
candistinguishthemajorityoftheemotions[4]. Similarly, rithm.
childrenwithautismoftencannotdistinguishemotionprop- Once the faces have been registered, the images are re-
erlywithoutbeingtoldtoremembertolookatthesamelo- sized to 48×48 pixels for analysis. Even though many
calfeatures[4]. ByusingtheInceptionlayerstructureand databasesarecomposedofimageswithamuchhigherreso-
applyingthenetwork-in-networktheoryproposedbyLinet lutiontestingsuggestedthatdecreasingthisresolutiondoes
al.[22],wecanexpectsignificantgainsonlocalfeatureper- not greatly impact the accuracy, however vastly increases
formance, which seems to logically translate to improved thespeedofthenetwork.Toaugmentourdata,weextract5
FERresults. cropsof40×40fromthefourcornersandthecenterofthe
Another benefit of the network-in-network method is imageandutilizebothofthemandtheirhorizontalflipsfor
that along with increased local performance, the global atotalof10additionalimages.
pooling performance is increased and therefore it is less Intrainingthenetwork,thelearningratesaredecreased
prone to overfitting. This resistance to overfitting allows in a polynomial fashion as: base lr(1−iter/max iter)0.5,where base lr =0.01 is the base learning rate, iter is the
current iteration and max iter is the maximum allowed it-
Output
erations. Testingsuggestedthatotherpopularlearningrate
Inner Product policiessuchasfixedlearningrate,stepwherelearningrate
1024
is multiplies by a gamma factor in each step, and expo-
Inner Product nential approach did not perform as well as the polyno-
4096
mial fashion. Using the polynomial learning rate, the test
Max Pool
3x3x2 loss converged faster and allowed us to train the network
for many iterations without the need for fine-tuning. We
Concat
also trained the bias nodes twice as fast as the weights of
thenetwork,inordertoincreasetherateatwhichunneces-
3x3 Conv 5x5 Conv 1x1 Conv
3x3x1 5x5x1 1x1x1 sarynodesareremovedfromevaluation. Thisdecreasesthe
1x1 Conv 3x3 reduce 5x5 reduce Pool Proj number of iterations that the network must run before the
1x1x1 3x3x1 5x5x1 3x3x1
lossconverges.
Max Pool
3x3x2
4.ExperimentalResults
Concat
4.1.FaceDatabases
3x3 Conv 5x5 Conv 1x1 Conv
3x3x1 5x5x1 1x1x1 We evaluate the proposed method on well-known pub-
licly available facial expression databases: CMU Multi-
1x1 Conv 3x3 reduce 5x5 reduce Pool Proj
1x1x1 3x3x1 5x5x1 3x3x1 PIE [15], MMI [35], Denver Intensity of Spontaneous Fa-
cial Actions (DISFA) [29], extended CK+ [27], GEMEP-
Concat FERAdatabase[5],SFEW[10],andFER2013[1]. Inthis
sectionwebrieflyreviewthecontentofthesedatabases.
3x3 Conv 5x5 Conv 1x1 Conv
3x3x1 5x5x1 1x1x1 CMU MultiPIE: CMU MultiPIE face database [15]
containsaround750,000imagesof337peopleundermulti-
1x1 Conv 3x3 reduce 5x5 reduce Pool Proj
1x1x1 3x3x1 5x5x1 3x3x1 pleviewpoints,anddifferentilluminationconditions.There
are four recording sessions in which subjects were in-
Max Pool structed to display different facial expressions (i.e. Angry,
3x3x2
Disgust,Happy,Neutral,Surprise,Squint,andScream).We
C 3xo 3n xv 12 selectedonlythefivefrontalviewpoints(-45◦to+45◦),giv-
ingusatotalofaround200,000images.
Max Pool
3x3x2
MMI: The MMI [35] database includes more than 20
Conv1 subjectsofbothgenders(44%female),ranginginagefrom
7x7x1
19to62,havingeitheraEuropean,Asian,orSouthAmeri-
Data canethnicity. Animagesequenceiscapturedthathasneu-
Figure1. NetworkArchitecture tralfacesatthebeginningandtheendforeachsessionand
subjects were instructed to display 79 series of facial ex-
pressions, six of which are prototypic emotions. We ex-
tractedstaticframesfromeachsequence,whereitresulted
in11,500images.
CK+: TheExtendedCohn-Kanadedatabase(CK+)[27]
includes 593 video sequences recorded from 123 subjects
rangingfrom18to30yearsold. Subjectsdisplayeddiffer-
ent expressions starting from the neutral for all sequences,
andsomesequencesarelabeledwithbasicexpressions. We
selected only the final frame of each sequence with peak
Figure2. Sampleofthefaceregistration.Fromlefttorightimages
expressioninourexperiment,whichresultsin309images.
are taken from MultiPIE, SFEW, MMI, CK+ and DISFA. First
DISFA:DenverIntensityofSpontaneousFacialActions
row shows the original images and the second row shows their
(DISFA)database[29]isoneofafewnaturalisticdatabases
registeredimagesrespectively.
thathavebeenFACScodedwithAUintensityvalues. This
databaseconsistsof27subjects,eachrecordedwhilewatch-
ingafourminutesvideoclipbytwocameras. TwelveAUsTable1.NetworkConfiguration
Layertype PatchSize/Stride Output 1x1 3x3 3x3reduce 5x5 5x5reduce ProjPooling #Operations
Convolution-1 7×7/2 24×24×64 5.7M
Maxpool-1 3×3/2 12×12×64 5.7M
Convolution-2 3×3/1 12×12×192 1.4M
MaxPool-2 3×3/2 6×6×192 1.4M
Inception-3a 64 128 96 32 16 32 2.6M
Inception-3b 128 192 128 96 32 64 4.5M
MaxPool-4 3×3/2 3×3×480 0.6M
Inception-4a 192 208 96 48 16 64 1.3M
AvgPooling-6 1×1×1024 25.6K
FullyConnected-7 1×1×4096 0.2M
FullyConnected-8 1×1×1024 51K
Table2.Numberofimagespereachexpressionindatabases
arecodedbetween0-5,where0denotestheabsenceofthe
AU, while 5 represents maximum intensities. As DISFA AN DI FE HA NE SA SU
is not emotion-specified coded, we used EMFACS sys- MultiPie 0 22696 0 47338 114305 0 19817
tem[14]toconvertAUFACScodestoexpressions, which MMI 1959 1517 1313 2785 0 2169 1746
resultedinaround89,000imagesinwhichthemajorityhave CK+ 45 59 25 69 0 28 83
neutralexpressions. DISFA 436 5326 4073 28404 48582 1024 1365
FERA 1681 0 1467 1882 0 2115 0
FERA: The GEMEP-FERA database [5] is a subset of SFEW 104 81 90 112 98 92 86
the GEMEP corpus used as database for the FERA 2011 FER2013 4953 547 5121 8989 6198 6077 4002
challenge [47]. It consists of recordings of 10 actors dis- *AN,DI,FE,HA,Ne,SA,SUstandforAnger,Disgust,Fear,Happiness,
playingarangeofexpressions. Therearesevensubjectsin Neutral,Sadness,Surprisedrespectively.
thetrainingdata,andsixsubjectsinthetestset.Thetraining
set contains 155 image sequences and the testing contains
134imagesequences. Thereareintotalfiveemotioncate- 4.2.Results
goriesinthedatabase: Anger, Fear, Happiness, Reliefand
We evaluated the accuracy of the proposed deep neu-
Sadness. Weextractstaticframesfromthesequenceswith
ral network architecture in two different experiments; viz.
sixbasicexpressions,whichresultedtoinaround7,000im-
subject-independent and cross-database evaluation. In the
ages.
subject-independent experiment, databases are split into
training, validation, and test sets in a strict subject inde-
SFEW: The Static Facial Expressions in the Wild
pendentmanner. WeusedtheK-foldcrossvalidationtech-
(SFEW)database[10]iscreatedbyselectingstaticframes
niquewithK=5toevaluatetheresults.InFERAandSFEW,
from Acted Facial Expressions in the Wild (AFEW) [9].
the training and test sets are defined in the database re-
The SFEW database covers unconstrained facial expres-
lease,andtheresultsareevaluatedonthedatabasedefined
sions, different head poses, age range, and occlusions and
test set without performing K-fold cross validation. Since
close to real world illuminations. There are a total of 95
therearedifferentsamplesperemotionpersubjectinsome
subjectsinthedatabase. Intotalthereare663well-labeled
databases,thetraining,validationandtestsetshaveslightly
usableimages.
different sample sizes in each fold. On average we used
175Ksamplesfortraining,56Ksamplesforvalidation,and
FER2013: The Facial Expression Recognition 2013
64Ksamplesfortest.Theproposedarchitecturewastrained
(FER-2013) database was introduced in the ICML 2013
for200epochs(i.e. 150Kiterationsonmini-batchesofsize
Challenges in Representation Learning [1]. The database
250 samples). Table 3 gives the average accuracy when
was created using the Google image search API and faces
classifying the images into the six basic expressions and
have been automatically registered. Faces are labeled as
the neutral expression. The average confusion matrix for
anyofthesixbasicexpressionsaswellastheneutral. The
subject-independentexperimentscanbeseeninTable4.
resultingdatabasecontains35,887imagesmostofthemin
wildsettings. Here,wealsoreportthetop-2expressionclasses. AsTa-
ble3depicts,theaccuracyofthetop-2classificationis15%
Table2showsthenumberofimagesforsixbasicexpres- higherthanthetop-1accuracyinmostcases, especiallyin
sionsandneutralfacesineachdatabase. thewilddatasets(i.e.FERA,SFEW,FER2013).Webelievethatbyassigningasingleexpressiontoaimagecanbeam-
biguouswhenthereistransitionbetweenexpressionsorthe
given expression is not at its peak, and therefore the top-2
expressioncanresultinabetterclassificationperformance
whenevaluatingimagesequences.
Table3.AverageAccuracy(%)forsubject-independent
Top-1 Top-2 State-of-the-arts
MultiPIE 94.7±0.8 98.7±0.3 70.6[21],90.6[13]
MMI 77.6±2.9 86.8±6.2 63.4[25],74.7[24],79.8[30],86.9[37]
DISFA 55.0±6.8 69.8±8.6 -
FERA 76.7±3.6 90.5±4.6 56.1[25],75.0[2],55.6[47]
SFEW 47.7±1.7 62.1±1.2 26.1[24],24.7[13]
84.1[30],84.4[21],88.5[42],92.0[24]
CK+ 93.2±1.4 97.8±1.3
92.4[25],93.6[49]
FER2013 66.4±0.6 81.7±0.3 69.3[44]
Table4.Average(%)confusionmatrixforsubject-independent
predicted
AN DI FE HA NE SA SU
lautcA
3.5
3
2.5
2
1.5
1
0.5
00 50000 100,000 150,000
Iterations
AN 55.0 7.0 12.8 3.5 7.6 8.5 5.3
DI 1.0 80.3 1.8 5.8 8.5 2.2 0.1
FE 7.4 4.3 47.0 8.1 18.7 8.6 5.5
HA 0.7 3.2 2.4 86.6 5.5 0.2 1.0
NE 2.3 6.3 7.8 5.5 75.0 1.3 1.4
SA 6.0 11.3 8.9 2.7 13.7 56.1 0.9
SU 0.8 0.1 2.8 3.5 2.5 0.6 89.3
The proposed architecture was implemented using the
Caffe toolbox [16] on a Tesla K40 GPU. It takes roughly
20 hours to train 175K samples for 200 epochs. Figure 3
shows the training loss and classification accuracy of the
top-1andtop-2classificationlabelsonthevalidationsetof
thesubject-independentexperimentover150,000iterations
(about 150 epochs). As the figure illustrates, the proposed
architectureconvergesafterabout50epochs.
In the cross-database experiment, one database is used
forevaluationandtherestofdatabasesareusedtotrainthe
network. Because every database has a unique fingerprint
(lighting, pose, emotions, etc.) the cross database task is
much more difficult to extract features from (both for tra-
ditional SVM approaches, and for neural networks). The
proposed architecture was trained for 100 epochs in each
experiment. Table5givestheaveragecross-databaseaccu-
racy when classifying the six basic expressions as well as
theneutralexpression.
Theexperimentpresentedin[30]isacross-databaseex-
periment performed by training the model on one of the
CK+,MMIorFEEDTUMdatabasesandtestingthemodel
ontheothers. ThereportedresultinTable5istheaverage
resultsfortheCK+andMMIdatabases.
Different classifiers on several databases are presented
in [37] where the results is still one of the state-of-the-
ssol
niarT
Training Loss
100
90
80
70
60
50
40
30
20
10
00 50000 100,000 150,000
Iterations
tes
noitadilav
no
ycaruccA
noitacifissalC
Top−1
Top−2
Figure3. Traininglossandclassificationaccuracyonvalidation
set
Table5.AverageAccuracy(%)oncrossdatabase
top-1 top-2 [30] [37] [31] [49]
MultiPIE 45.7 63.2 - - - -
MMI 55.6 68.3 51.4 50.8 36.8 66.9
DISFA 37.7 53.2 - - - -
FERA 39.4 58.7 - - - -
SFEW 39.8 55.3 - - - -
CK+ 64.2 83.1 47.1 - 56.0 61.2
FER2013 34.0 51.7 - - - -
artmethodsforperson-independentevaluationontheMMI
database(SeeTable3). ThereportedresultinTable5isthe
bestresultusingdifferentSVMkernelstrainedontheCK+
databaseandevaluatedthemodelontheMMIdatabase.
Asupervisedkernelmeanmatchingispresentedin[31]
whichattemptstomatchthedistributionofthetrainingdata
inaclass-to-classmanner. Extensiveexperimentswereper-
formed using four classifiers (SVM, Nearest Mean Clas-
sifier, Weighted Template Matching, and K-nearest neigh-
bors). ThereportedresultinTable5arethebestresultsof
thefourclassifierwhentrainingthemodelontheMMIand
JaffedatabasesandevaluatingontheCK+databaseaswell
as when the model was trained on the CK+ database and
evaluatedontheMMIdatabase.
In [49] multiple features are fused via a Multiple Ker-nel Learning algorithm and the cross-database experiment have compared our results with the best methods on each
istrainedonCK+,evaluatedonMMIandviceversa. Com- databaseseparately,wherethehyperparametersofthepre-
paringtheresultofourproposedapproachwiththesestate- sentedmodelsarefine-tunedforthatspecificproblem. We
of-the-artmethods,itcanbeconcludedthatournetworkcan performsignificantlybetterthanthestateoftheartonMul-
generalized well for FER problem. Unfortunately, there is tiPIE and SFEW (no known state of the art has been re-
not any study on cross-database evaluation of more chal- ported for the DISFA database). The only exceptions to
lengingdatasets such asFERA,SFEW andFER2013. We the improved performance are with the MMI and FERA
believe that this work can be a baseline for cross-database databases. Thereareanumberofexplanationsforthisphe-
ofthesechallengingdatasets. nomenon.
In[13], aSharedGaussianProcessesformultiviewand
Oneofthelikelyreasonsfortheperformancediscrepan-
viewinvariant classification is proposed. The reported re-
ciesonthesubject-independentdatabasesisduetotheway
sult is very promising on MultiPIE database which covers
that the networks are trained in our experiments. Because
multipleviews, howeveronwildsettingofSFEWitisnot
we use data from all of the studied databases to train the
asefficientasMultiPIE.Anewsparserepresentationisem-
deeparchitecture,theinputdatacontainsimagethatdonot
ployedin[21],aimingtoreducetheintra-classvariationand
conform to the database setting such as pose and lighting.
bygeneratinganintra-classvariationimageofeachexpres-
It is very difficult to avoid this issue as it is hard or im-
sionbyusingtrainingimages.
possibletotrainsuchacomplexnetworkarchitectureonso
[47] is the FERA 2011 challenge baseline and [2] is
little data without causing significant overfitting. Another
the result of UC Riverside team (winner of the challenge).
reasonforthedecreasedperformanceisthefocusoncross-
[42] detects AUs and uses their composition rules to rec-
databaseperformance.Bytrainingslightlylesscomplicated
ognize expressions by means of a dictionary-based ap-
architectures,orevenusingtraditionalmethodssuchassup-
proach,whichisoneofthestate-of-the-art”sign-based”ap-
portvectormachines,orengineeredfeatures,itwouldlikely
proaches. [44]isthewinneroftheICML2013Challenges
be possible to improve the performance of the network on
onFER2013databasethatemployedaconvolutionalneural
subject-independent tasks. In this research however, we
network similar to AlexNet [20] but with linear one-vs-all
present a comprehensive solution that can generalize well
linearSVMtoplayerinsteadofaSoftmaxfunction.
totheFER“inthewild”problem.
Table6.Subject-independentcomparisonwithAlexNetresults(%
accuracy)
6.Conclusion
ProposedArchitecture AlexNet
MultiPie 94.7 94.8
Thisworkpresentsanewdeepneuralnetworkarchitec-
MMI 77.9 56.0
ture for automated facial expression recognition. The pro-
DISFA 55.0 56.1
posed network consists of two convolutional layers each
FERA 76.7 77.4
followed by max pooling and then four Inception layers.
SFEW 47.7 48.6
The Inception layers increase the depth and width of the
CK+ 93.2 92.2
network while keeping the computational budget constant.
FER2013 66.4 61.1
The proposed approach is a single component architecture
thattakesregisteredfacialimagesastheinputandclassifies
As a benchmark to our proposed solution, we trained a
themintoeitherofthesixbasicexpressionsortheneutral.
fullAlexNetfromscratch(asopposedtofinetuninganal-
Weevaluatedourproposedarchitectureinbothsubject-
ready trained network) using the same protocol as used to
independent and cross-database manners on seven well-
trainourownnetwork. AsshowninTable6,ourproposed
known publicly available databases. Our results confirm
architecture has better performance on MMI & FER2013
the superiority of our network compared to several state-
and comparable performance on the rest of the databases.
of-the-artmethodsinwhichengineeredfeaturesandclassi-
ThevalueoftheproposedsolutionovertheAlexNetarchi-
fier parameters are usually tuned on a very few databases.
tecture is its training time - Our version of AlexNet per-
Our network is first which applies the Inception layer ar-
formedmorethan100Moperations, whereastheproposed
chitecture to the FER problem across multiple databases.
networkperformsabout25Moperations.
The clear advantage of the proposed method over conven-
5.Discussion tionalCNNmethods(i.e. shallowerorthinnernetworks)is
gainingincreasedclassificationaccuracyonboththesubject
As shown in Tables 3 and 5, the results in the subject- independentandcross-databaseevaluationscenarioswhile
independent tests were either comparable to or better than reducingthenumberofoperationsrequiredtotrainthenet-
thecurrentstateoftheart. Itshouldbementionedthatwe work.7.Acknowledgment [15] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker.
Multi-pie. Image and Vision Computing, 28(5):807–813,
WegratefullyacknowledgethesupportofNVIDIACor-
2010. 1,2,5
porationwiththedonationoftheTeslaK40GPUusedfor
[16] Y.Jia,E.Shelhamer,J.Donahue,S.Karayev,J.Long,R.Gir-
thisresearch. shick, S. Guadarrama, and T. Darrell. Caffe: Convolu-
tionalarchitectureforfastfeatureembedding.arXivpreprint
References
arXiv:1408.5093,2014. 7
[17] S. E. Kahou, C. Pal, X. Bouthillier, P. Froumenty,
[1] Challenges in representation learning: Facial expression
C¸.Gu¨lc¸ehre,R.Memisevic,P.Vincent,A.Courville,Y.Ben-
recognitionchallenge:http://www.kaggle.com/c/challenges-
gio,R.C.Ferrari,etal. Combiningmodalityspecificdeep
in-representation-learning-facial-expression-recognition-
neural networks for emotion recognition in video. In Pro-
challenge. 2,5,6
ceedings of the 15th ACM on International conference on
[2] Facial expression recognition and analysis challenge 2011
multimodalinteraction,pages543–550.ACM,2013. 2,3
(results):http://sspnet.eu/fera2011/. 7,8
[18] M. Kenji. Recognition of facial expression from optical
[3] S.Arora,A.Bhaskara,R.Ge,andT.Ma. Provablebounds
flow. IEICETRANSACTIONSonInformationandSystems,
for learning some deep representations. arXiv preprint
74(10):3474–3483,1991. 2
arXiv:1310.6343,2013. 4
[19] H.KobayashiandF.Hara. Facialinteractionbetweenani-
[4] E.Bal,E.Harden,D.Lamb,A.VanHecke,J.Denver,and
mated 3d face robot and human beings. In Systems, Man,
S.Porges.Emotionrecognitioninchildrenwithautismspec-
andCybernetics,1997.ComputationalCyberneticsandSim-
trumdisorders: Relationstoeyegazeandautonomicstate.
ulation.,1997IEEEInternationalConferenceon,volume4,
JournalofAutismandDevelopmentalDisorders,40(3):358–
pages3732–3737.IEEE,1997. 2
370,2010. 4
[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
[5] T.Ba¨nzigerandK.R.Scherer. Introducingthegenevamul-
classification with deep convolutional neural networks. In
timodalemotionportrayal(gemep)corpus. Blueprintforaf-
Advances in neural information processing systems, pages
fectivecomputing: Asourcebook,pages271–294,2010. 5,
1097–1105,2012. 1,3,4,8
6
[21] S.H.Lee,K.Plataniotis,andY.M.Ro. Intra-classvariation
[6] J.F.Cohn,Z.Ambadar,andP.Ekman.Observer-basedmea-
reductionusingtrainingexpressionimagesforsparserepre-
surement of facial expression with the facial action coding
sentationbasedfacialexpressionrecognition. IEEETrans-
system.Thehandbookofemotionelicitationandassessment,
actionsonAffectiveComputing,page1,2014. 7,8
pages203–221,2007. 2
[7] F.DelaTorreandJ.F.Cohn. Facialexpressionanalysis. In [22] M.Lin, Q.Chen, andS.Yan. Networkinnetwork. arXiv
Visualanalysisofhumans, pages377–409.Springer, 2011. preprintarXiv:1312.4400,2013. 3,4
2 [23] C. Liu and H. Wechsler. Gabor feature based classifica-
[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei- tionusingtheenhancedfisherlineardiscriminantmodelfor
Fei. Imagenet: A large-scale hierarchical image database. facerecognition. Imageprocessing,IEEETransactionson,
In Computer Vision and Pattern Recognition, 2009. CVPR 11(4):467–476,2002. 2
2009.IEEEConferenceon,pages248–255.IEEE,2009. 2 [24] M. Liu, S. Li, S. Shan, and X. Chen. Au-aware deep
[9] A. Dhall, R. Goecke, J. Joshi, M. Wagner, and T. Gedeon. networks for facial expression recognition. In Automatic
Emotion recognition in the wild challenge 2013. In Pro- FaceandGestureRecognition(FG),201310thIEEEInter-
ceedings of the 15th ACM on International conference on national Conference and Workshops on, pages 1–6. IEEE,
multimodalinteraction,pages509–516.ACM,2013. 1,3,6 2013. 3,7
[10] A. Dhall, R. Goecke, S. Lucey, and T. Gedeon. Static fa- [25] M. Liu, S. Li, S. Shan, R. Wang, and X. Chen. Deeply
cial expression analysis in tough conditions: Data, evalua- learning deformable facial action parts model for dynamic
tionprotocolandbenchmark.InComputerVisionWorkshops expressionanalysis.InComputerVision–ACCV2014,pages
(ICCVWorkshops),2011IEEEInternationalConferenceon, 143–157.Springer,2014. 4,7
pages2106–2112.IEEE,2011. 5,6 [26] M. Liu, R. Wang, S. Li, S. Shan, Z. Huang, and X. Chen.
[11] P.EkmanandW.V.Friesen.Constantsacrossculturesinthe Combining multiple kernel methods on riemannian mani-
faceandemotion.Journalofpersonalityandsocialpsychol- fold for emotion recognition in the wild. In Proceedings
ogy,17(2):124,1971. 1 of the 16th International Conference on Multimodal Inter-
[12] P. Ekman and W. V. Friesen. Facial action coding system. action,pages494–501.ACM,2014. 4
1977. 2 [27] P.Lucey,J.F.Cohn,T.Kanade,J.Saragih,Z.Ambadar,and
[13] S.Eleftheriadis,O.Rudovic,andM.Pantic. Discriminative I. Matthews. The extended cohn-kanade dataset (ck+): A
sharedgaussianprocessesformulti-viewandview-invariant complete dataset for action unit and emotion-specified ex-
facialexpressionrecognition. IEEETransactionsonImage pression.InComputerVisionandPatternRecognitionWork-
Processing,24(1):189–204,2015. 7,8 shops(CVPRW),2010IEEEComputerSocietyConference
[14] W. V. Friesen and P. Ekman. Emfacs-7: Emotional facial on,pages94–101.IEEE,2010. 2,5
actioncodingsystem. Unpublishedmanuscript, University [28] M.Lyons,S.Akamatsu,M.Kamachi,andJ.Gyoba.Coding
ofCaliforniaatSanFrancisco,2:36,1983. 2,6 facial expressions with gabor wavelets. In Automatic FaceandGestureRecognition,1998.Proceedings.ThirdIEEEIn- [43] Y.Taigman,M.Yang,M.Ranzato,andL.Wolf. Deepface:
ternationalConferenceon,pages200–205.IEEE,1998. 1 Closingthegaptohuman-levelperformanceinfaceverifica-
[29] S.M.Mavadati,M.H.Mahoor,K.Bartlett,P.Trinh,andJ.F. tion. InComputerVisionandPatternRecognition(CVPR),
Cohn.Disfa:Aspontaneousfacialactionintensitydatabase. 2014IEEEConferenceon,pages1701–1708.IEEE,2014.1
AffectiveComputing, IEEETransactionson, 4(2):151–160, [44] Y.Tang.Deeplearningusinglinearsupportvectormachines.
2013. 1,2,5 arXivpreprintarXiv:1306.0239,2013. 7,8
[30] C.Mayer,M.Eggers,andB.Radig. Cross-databaseevalu- [45] A.ToshevandC.Szegedy. Deeppose: Humanposeestima-
ationforfacialexpressionrecognition. Patternrecognition tionviadeepneuralnetworks. InComputerVisionandPat-
andimageanalysis,24(1):124–132,2014. 1,7 ternRecognition(CVPR),2014IEEEConferenceon,pages
[31] Y.-Q. Miao, R. Araujo, and M. S. Kamel. Cross-domain 1653–1660.IEEE,2014. 1
facialexpressionrecognitionusingsupervisedkernelmean [46] M.Valstar,M.Pantic,andI.Patras.Motionhistoryforfacial
matching. InMachineLearningandApplications(ICMLA), actiondetectioninvideo. InSystems,ManandCybernetics,
2012 11th International Conference on, volume 2, pages 2004 IEEE International Conference on, volume 1, pages
326–332.IEEE,2012. 7 635–640.IEEE,2004. 2
[32] M.Mohammadi,E.Fatemizadeh,andM.Mahoor.Pca-based [47] M.F.Valstar,B.Jiang,M.Mehu,M.Pantic,andK.Scherer.
dictionarybuildingforaccuratefacialexpressionrecognition The first facial expression recognition and analysis chal-
viasparserepresentation. JournalofVisualCommunication lenge. InAutomaticFace&GestureRecognitionandWork-
andImageRepresentation,25(5):1082–1092,2014. 2 shops(FG2011), 2011IEEEInternationalConferenceon,
[33] A.Mollahosseini, G.Graitzer, E.Borts, S.Conyers, R.M. pages921–926.IEEE,2011. 2,6,7,8
Voyles,R.Cole,andM.H.Mahoor.Expressionbot:Anemo- [48] X. Xiong and F. De la Torre. Supervised descent method
tivelifelikeroboticfaceforface-to-facecommunication. In anditsapplicationstofacealignment. InComputerVision
HumanoidRobots(Humanoids),201414thIEEE-RASInter- andPatternRecognition(CVPR),2013IEEEConferenceon,
nationalConferenceon,pages1098–1103.IEEE,2014. 1 pages532–539.IEEE,2013. 4
[34] A.MollahosseiniandM.H.Mahoor. Bidirectionalwarping [49] X.Zhang,M.H.Mahoor,andS.M.Mavadati.Facialexpres-
ofactiveappearancemodel.InComputerVisionandPattern sionrecognitionusing{l} {p}-normmklmulticlass-svm.
Recognition Workshops (CVPRW), 2013 IEEE Conference MachineVisionandApplications,pages1–17,2015. 2,7
on,pages875–880.IEEE,2013. 4 [50] X.Zhang,A.Mollahosseini,B.Kargar,H.Amir,E.Boucher,
[35] M. Pantic, M. Valstar, R. Rademaker, and L. Maat. Web- R.M.Voyles,R.Nielsen,andM.Mahoor.ebear:Anexpres-
baseddatabaseforfacialexpressionanalysis. InMultimedia sivebear-likerobot. InRobotandHumanInteractiveCom-
andExpo,2005.ICME2005.IEEEInternationalConference munication, 2014 RO-MAN: The 23rd IEEE International
on,pages5–pp.IEEE,2005. 1,5 Symposiumon,pages969–974.IEEE,2014. 2
[36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, [51] G. Zhao and M. Pietikainen. Dynamic texture recognition
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, usinglocalbinarypatternswithanapplicationtofacialex-
et al. Imagenet large scale visual recognition challenge. pressions. PatternAnalysisandMachineIntelligence,IEEE
arXivpreprintarXiv:1409.0575,2014. 3,4 Transactionson,29(6):915–928,2007. 2
[37] C. Shan, S. Gong, and P. W. McOwan. Facial expression [52] W.ZhenandY.Zilu. Facialexpressionrecognitionbasedon
recognitionbasedonlocalbinarypatterns:Acomprehensive localphasequantizationandsparserepresentation. InNatu-
study. ImageandVisionComputing,27(6):803–816,2009. ralComputation(ICNC),2012EighthInternationalConfer-
2,7 enceon,pages222–225.IEEE,2012. 2
[38] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R.Salakhutdinov. Dropout: Asimplewaytopreventneural
networksfromoverfitting.TheJournalofMachineLearning
Research,15(1):1929–1958,2014. 3
[39] Y. Sun, D. Liang, X. Wang, and X. Tang. Deepid3:
Face recognition with very deep neural networks. CoRR,
abs/1502.00873,2015. 4
[40] J. M. Susskind, A. K. Anderson, and G. E. Hinton. The
toronto face database. Technical report, UTML TR 2010-
001,UniversityofToronto,2010. 3
[41] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabi-
novich. Going deeper with convolutions. arXiv preprint
arXiv:1409.4842,2014. 1,3,4
[42] S.Taheri,Q.Qiang,andR.Chellappa. Structure-preserving
sparse decomposition for facial expression analysis. IEEE
transactionsonimageprocessing:apublicationoftheIEEE
SignalProcessingSociety,23(8):3590,2014. 2,7,8"
146,148,Hard negative generation for identity-disentangled facial expression recognition,"['X Liu', 'BVKV Kumar', 'P Jia', 'J You']",2019,107,"CMU Multi-PIE, MMI Facial Expression","FER, facial expression recognition","representations of RML achieve superior performance on the CK + , MMI and Oulu-CASIA   The performances of the FER systems usually depend heavily on facial expression",No DOI,Pattern Recognition,https://www.sciencedirect.com/science/article/pii/S0031320318303819,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
147,149,Head pose estimation in the wild using convolutional neural networks and adaptive gradient methods,"['M Patacchiola', 'A Cangelosi']",2017,255,Expression in-the-Wild,neural network,"an approach based on Convolutional Neural Networks (CNNs)  on recently released in-the-wild  datasets. Moreover, we  ) with MAE expressed in degrees and Accuracy expressed in",No DOI,Pattern Recognition,https://www.sciencedirect.com/science/article/pii/S0031320317302327,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False,sciencedirect.com,
148,150,Hierarchical committee of deep cnns with exponentially-weighted decision fusion for static facial expression recognition,"['BK Kim', 'H Lee', 'J Roh', 'SY Lee']",2015,167,Static Facial Expression in the Wild,facial expression recognition,We present a pattern recognition framework to improve  its application to static facial  expression recognition in the wild ( released for the 3rd Emotion Recognition in the Wild (EmotiW),No DOI,Proceedings of the 2015 ACM on …,https://dl.acm.org/doi/10.1145/2818346.2830590,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,acm.org,
149,151,Hierarchical committee of deep convolutional neural networks for robust facial expression recognition,"['BK Kim', 'J Roh', 'SY Dong', 'SY Lee']",2016,307,Static Facial Expression in the Wild,neural network,"-video emotion recognition (acted facial expression recognition in the wild, AFEW), whereas   : AFEW and image-based static facial expression recognition in the wild (SFEW). We",No DOI,Journal on Multimodal User Interfaces,https://link.springer.com/article/10.1007/s12193-015-0209-0,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,springer.com,
150,152,How we've taught algorithms to see identity: Constructing race and gender in image databases for facial analysis,"['MK Scheuerman', 'K Wade', 'C Lustig']",2020,211,Radboud Faces Database,"classifier, machine learning","to current database annotation approaches in machine learning  of machine learning model  building: the data it is limited to  For example, Radboud Faces Database (RAFD) contained",No DOI,Proceedings of the ACM …,https://dl.acm.org/doi/10.1145/3392866,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,acm.org,
151,153,Human-computer interaction using emotion recognition from facial expression,"['F Abdat', 'C Maaoui', 'A Pruski']",2011,137,Affective Faces Database,facial expression recognition,"Our facial expression recognition system is presented in section 3,  to emotion recognition  based on facial expression analysis.To detect the face in the image, we have used the face",No DOI,2011 UKSim 5th European …,https://ieeexplore.ieee.org/document/6131215,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
152,154,Hybrid deep neural networks for face emotion recognition,"['N Jain', 'S Kumar', 'A Kumar', 'P Shamsolmoali']",2018,312,Toronto Face Database,"deep learning, neural network","This paper proposed a deep learning technique in the  Now a day's due to quantity and  variety of datasets, deep learning  have just utilized the extra dataset for the training. Accordingly,",No DOI,Pattern Recognition …,https://www.sciencedirect.com/science/article/pii/S0167865518301302,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
153,155,Identity-adaptive facial expression recognition through expression regeneration using conditional generative adversarial networks,"['H Yang', 'Z Zhang', 'L Yin']",2018,132,"Binghamton University 3D Facial Expression, Oulu-CASIA","FER, classification","(FER-Net) is finetuned for expression classification. After the corresponding prototypic facial  expressions are regenerated from each facial image, we output the last FC layer of FER-Net",No DOI,… Conference on Automatic Face & …,http://ieeexplore.ieee.org/document/8373843/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
154,156,Identity-aware convolutional neural network for facial expression recognition,"['Z Meng', 'P Liu', 'J Cai', 'S Han']",2017,368,"MMI Facial Expression, Static Facial Expression in the Wild","CNN, neural network","of a neuron with probability of 0.6. In Eq. 8, λ2 is set to 5 for CK+/SFEW and 2 for MMI, while   Labeled subject IDs are provided in the CK+ and MMI databases and we manually labeled",No DOI,… on Automatic Face & …,https://ieeexplore.ieee.org/document/7961791,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
155,157,"Illusionary order: Online databases, optical character recognition, and Canadian history, 1997–2010",['I Milligan'],2013,127,Toronto Face Database,classification,", and the Toronto Star online. Yet the issues of poor ocr in these databases make this a very   , the decision was made to adhere to ProQuest's classification system (which is based upon",No DOI,Canadian Historical Review,https://www.utpjournals.press/doi/abs/10.3138/chr.694,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,utpjournals.press,
156,158,Image based facial micro-expression recognition using deep learning on small datasets,"['MA Takalkar', 'M Xu']",2017,103,Affective Faces Database,deep learning,"It is not straightforward to recognise the genuine emotion shown on one’s face. Thus recognising   II database, we implemented the DLib face detector in OpenCV to detect and crop the",No DOI,2017 international conference on digital …,https://ieeexplore.ieee.org/document/8227443,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
157,159,Image based static facial expression recognition with multiple deep network learning,"['Z Yu', 'C Zhang']",2015,736,"Acted Facial Expressions In The Wild, CMU Multi-PIE, Static Facial Expression in the Wild, Toronto Face Database","CNN, FER, classification, classifier, deep learning, facial expression recognition, machine learning, neural network",Two large datasets: the Toronto Face Dataset and the Google dataset were combined to  train the CNN network. The Google dataset happens to be the very dataset provided to FER-,No DOI,Proceedings of the 2015 ACM on international …,https://dl.acm.org/doi/10.1145/2818346.2830595,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,acm.org,
158,160,Image ratio features for facial expression recognition application,"['M Song', 'D Tao', 'Z Liu', 'X Li']",2009,127,"Japanese Female Facial Expression, Toronto Face Database","classification, classifier, facial expression recognition","database, and the Japanese Female Facial Expression database.  asymmetric facial  expressions based on our own facial expression  of our combined expression recognition system.",No DOI,IEEE Transactions on …,https://pubmed.ncbi.nlm.nih.gov/19884092/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,nih.gov,
159,161,Impact of deep learning approaches on facial expression recognition in healthcare industries,"['C Bisogni', 'A Castiglione', 'S Hossain']",2022,109,Static Facial Expression in the Wild,"deep learning, machine learning","For experimental purposes, three benchmark databases, static facial expressions in the  wild, Cohn-Kanade, and Karolinska directed emotional faces, are employed with some existing",No DOI,IEEE Transactions …,https://ieeexplore.ieee.org/document/9674818,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
160,162,"Induced disgust, happiness and surprise: an addition to the mmi facial expression database","['M Valstar', 'M Pantic']",2010,695,MMI Facial Expression,"deep learning, facial expression recognition, machine learning",notation is valuable for researchers who wish to build automatic basic emotion detection  systems. Not all affective states can be categorised into one of the six basic emotions.,No DOI,Proc. 3rd Intern. Workshop on EMOTION  …,https://www.researchgate.net/publication/284473673_Induced_disgust_happiness_and_surprise_An_addition_to_the_mmi_facial_expression_database,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,researchgate.net,
161,163,"Integrating faces, fingerprints, and soft biometric traits for user recognition","['AK Jain', 'K Nandakumar', 'X Lu', 'U Park']",2004,199,Toronto Face Database,classification,"an algorithm for age classification from facial images based on cranio-facial changes in  feature- However, they do not provide any accuracy estimates for their classification scheme.",No DOI,Biometric Authentication: ECCV …,https://link.springer.com/chapter/10.1007/978-3-540-25976-3_24,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,springer.com,
162,164,Intra-class variation reduction using training expression images for sparse representation based facial expression recognition,"['SH Lee', 'KN Plataniotis', 'YM Ro']",2014,141,"CMU Multi-PIE, Toronto Face Database","FER, classification, classifier, facial expression recognition",", “A 3D facial expression database for facial behavior research,”  Toronto, Toronto, ON,  Canada. His research interests include face detection/tracking, facial expression recognition, face",No DOI,IEEE Transactions on …,https://ieeexplore.ieee.org/document/6874505,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
163,165,Joint fine-tuning in deep neural networks for facial expression recognition,"['H Jung', 'S Lee', 'J Yim', 'S Park', 'J Kim']",2015,933,"MMI Facial Expression, Oulu-CASIA","deep learning, neural network",We designed our network with a moderate depth and a moderate number of parameters   facial expression recognition database is too small— there are only 205 sequences in the MMI,No DOI,Proceedings of the IEEE …,https://ieeexplore.ieee.org/document/7410698,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
164,166,Karolinska directed emotional faces,"['D Lundqvist', 'A Flykt', 'A Öhman']",1998,3821,Karolinska Directed Emotional Faces,facial expression recognition,"All the participants were instructed to try to evoke the emotion that was to be expressed  and to make the expression strong and clear. In a validation study (Goeleven et al., 2008), a",No DOI,Cognition and Emotion,https://kdef.se/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,kdef.se,
165,167,Learning affective features with a hybrid deep model for audio–visual emotion recognition,"['S Zhang', 'S Zhang', 'T Huang', 'W Gao']",2017,346,Affective Faces Database,CNN,"tasks to initialize our CNN and 3D-CNN, respectively. Then,  For each frame in the video  segment, we run face detection,  audio-visual face database of affective and mental states,” IEEE",No DOI,IEEE transactions on …,https://ieeexplore.ieee.org/document/7956190,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
166,168,Learning affective video features for facial expression recognition via hybrid deep learning,"['S Zhang', 'X Pan', 'Y Cui', 'X Zhao', 'L Liu']",2019,147,MMI Facial Expression,"CNN, deep learning, machine learning","This deep fusion network is used to jointly learn  Vector Machine (SVM) is employed for  facial expression classification  -based facial expression datasets, ie, BAUM-1s, RML and MMI,",No DOI,IEEE Access,http://ieeexplore.ieee.org/document/8658192/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
167,169,Learning deep global multi-scale and local attention features for facial expression recognition in the wild,"['Z Zhao', 'Q Liu', 'S Wang']",2021,223,"Expression in-the-Wild, Static Facial Expression in the Wild","FER, deep learning, facial expression recognition","This paper aims at static FER in the wild, in which occlusion and pose are two key issues,   Fan, JC Lam, and VO Li, “Video-based emotion recognition using deeply-supervised neural",No DOI,IEEE Transactions on Image …,https://ieeexplore.ieee.org/document/9474949,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
168,170,Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition,"['M Liu', 'S Shan', 'R Wang', 'X Chen']",2014,455,MMI Facial Expression,facial expression recognition,"We demonstrate the proposed method for expression recognition on four databases: CK+,  MMI, Oulu-CASIA VIS, and AFEW. As shown in Table 1,2,3,4, we separate the results into",No DOI,… vision and pattern recognition,https://openaccess.thecvf.com/content_cvpr_2014/papers/Liu_Learning_Expressionlets_on_2014_CVPR_paper.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,thecvf.com,
169,171,Learning to disentangle factors of variation with manifold interaction,"['S Reed', 'K Sohn', 'Y Zhang']",2014,287,CMU Multi-PIE,"deep learning, machine learning",groups of hidden units that each learn to encode a distinct factor  disentangled features  learned on the CMU Multi-PIE dataset.  in pose estimation and face verification on CMU Multi-PIE.,No DOI,… on machine learning,http://proceedings.mlr.press/v32/reed14.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,mlr.press,
170,172,Learning to optimize neural nets,"['K Li', 'J Malik']",2017,150,Toronto Face Database,neural network,"in stochasticity of gradients and the neural net architecture. More specifically, we  neural  net on MNIST generalizes to the problems of training neural nets on the Toronto Faces Dataset",No DOI,arXiv preprint arXiv:1703.00441,https://arxiv.org/abs/1703.00441,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"Learning to Optimize Neural Nets
KeLi1 JitendraMalik1
Abstract optimizationalgorithm. Giventhisstateofaffairs,perhaps
itistimeforustostartpracticingwhatwepreachandlearn
Learning to Optimize (Li & Malik, 2016) is a
howtolearn.
recently proposed framework for learning opti-
mization algorithms using reinforcement learn- Recently, Li & Malik (2016) and Andrychowicz et al.
ing. In this paper, we explore learning an op- (2016) introduced two different frameworks for learning
timization algorithm for training shallow neu- optimization algorithms. Whereas Andrychowicz et al.
ral nets. Such high-dimensional stochastic opti- (2016) focuses on learning an optimization algorithm for
mizationproblemspresentinterestingchallenges training models on a particular task, Li & Malik (2016)
for existing reinforcement learning algorithms. sets a more ambitious objective of learning an optimiza-
We develop an extension that is suited to learn- tionalgorithmfortrainingmodelsthatistask-independent.
ing optimization algorithms in this setting and We study the latter paradigm in this paper and develop a
demonstrate that the learned optimization algo- method for learning an optimization algorithm for high-
rithm consistently outperforms other known op- dimensional stochastic optimization problems, like the
timization algorithms even on unseen tasks and problemoftrainingshallowneuralnets.
isrobusttochangesinstochasticityofgradients
Underthe“LearningtoOptimize”frameworkproposedby
and the neural net architecture. More specifi-
Li&Malik(2016),theproblemoflearninganoptimization
cally, we show that an optimization algorithm
algorithm is formulated as a reinforcement learning prob-
trained with the proposed method on the prob-
lem. Weconsiderthegeneralstructureofanunconstrained
lem of training a neural net on MNIST general-
continuousoptimizationalgorithm,asshowninAlgorithm
izestotheproblemsoftrainingneuralnetsonthe
1. Ineachiteration,thealgorithmtakesastep∆xanduses
Toronto Faces Dataset, CIFAR-10 and CIFAR-
ittoupdatethecurrentiteratex(i). Inhand-engineeredop-
100.
timization algorithms, ∆x is computed using some fixed
formulaφthatdependsontheobjectivefunction, thecur-
rentiterateandpastiterates. Often,itissimplyafunction
1.Introduction
ofthecurrentandpastgradients.
Machine learning is centred on the philosophy that learn-
Algorithm1Generalstructureofoptimizationalgorithms
ingpatternsautomaticallyfromdataisgenerallybetterthan
meticulously crafting rules by hand. This data-driven ap- Require: Objectivefunctionf
proach has delivered: today, machine learning techniques
x(0) ←randompointinthedomainoff
canbefoundinawiderangeofapplicationareas, bothin
fori=1,2,...do
∆x←φ(f,{x(0),...,x(i−1)})
AIandbeyond. Yet,thereisonedomainthathasconspicu-
ouslybeenleftuntouchedbymachinelearning: thedesign ifstoppingconditionismetthen
oftoolsthatpowermachinelearningitself.
returnx(i−1)
endif
One of the most widely used tools in machine learning is x(i) ←x(i−1)+∆x
optimization algorithms. We have grown accustomed to
endfor
seeinganoptimizationalgorithmasablackboxthattakes
inamodelthatwedesignandthedatathatwecollectand
Different choices of φ yield different optimization algo-
outputstheoptimalmodelparameters.Theoptimizational-
rithms and so each optimization algorithm is essentially
gorithmitselflargelystaysstatic: itsdesignisreservedfor
characterized by its update formula φ. Hence, by learn-
humanexperts,whomusttoilthroughmanyroundsofthe-
ing φ, we can learn an optimization algorithm. Li & Ma-
oreticalanalysisandempiricalvalidationtodeviseabetter
lik (2016) observed that an optimization algorithm can be
1UniversityofCalifornia,Berkeley,CA94720,UnitedStates. viewed as a Markov decision process (MDP), where the
Correspondenceto:KeLi<ke.li@eecs.berkeley.edu>. stateincludesthecurrentiterate,theactionisthestepvec-
7102
voN
03
]GL.sc[
2v14400.3071:viXraLearningtoOptimizeNeuralNets
tor∆xandthepolicyistheupdateformulaφ. Hence,the 2.2.LearningWhichModeltoLearn
problem of learning φ simply reduces to a policy search
Methodsinthiscategory(Brazdiletal.,2008)aimtolearn
problem.
whichbase-levellearnerachievesthebestperformanceon
In this paper, we build on the method proposed in (Li atask. Themeta-knowledgecapturescorrelationsbetween
& Malik, 2016) and develop an extension that is suited differenttasksandtheperformanceofdifferentbase-level
to learning optimization algorithms for high-dimensional learnersonthosetasks. Onechallengeunderthissettingis
stochastic problems. We use it to learn an optimization to decide on a parameterization of the space of base-level
algorithm for training shallow neural nets and show that learners that is both rich enough to be capable of repre-
itoutperformspopularhand-engineeredoptimizationalgo- senting disparate base-level learners and compact enough
rithmslikeADAM(Kingma&Ba,2014),AdaGrad(Duchi to permit tractable search over this space. Brazdil et al.
et al., 2011) and RMSprop (Tieleman & Hinton, 2012) (2003)proposesanonparametricrepresentationandstores
andanoptimizationalgorithmlearnedusingthesupervised examples of different base-level learners in a database,
learningmethodproposedin(Andrychowiczetal.,2016). whereas Schmidhuber (2004) proposes representing base-
Furthermore, we demonstrate that our optimization algo- levellearnersasgeneral-purposeprograms.Theformerhas
rithm learned from the experience of training on MNIST limitedrepresentationpower,whilethelattermakessearch
generalizestotrainingonotherdatasetsthathaveverydis- andlearninginthespaceofbase-levellearnersintractable.
similarstatistics,liketheTorontoFacesDataset,CIFAR-10 Hochreiter et al. (2001) views the (online) training proce-
andCIFAR-100. dureofanybase-learnerasablackboxfunctionthatmapsa
sequenceoftrainingexamplestoasequenceofpredictions
2.RelatedWork andmodelsitasarecurrentneuralnet. Underthisformu-
lation, meta-training reduces to training the recurrent net,
The line of work on learning optimization algorithms is and the base-level learner is encoded in the memory state
fairly recent. Li & Malik (2016) and Andrychowicz et al. oftherecurrentnet.
(2016) were the first to propose learning general opti-
Hyperparameter optimization can be seen as another ex-
mization algorithms. Li & Malik (2016) explored learn-
ampleofmethodsinthiscategory. Thespaceofbase-level
ingtask-independentoptimizationalgorithmsandusedre-
learnerstosearchoverisparameterizedbyapredefinedset
inforcement learning to learn the optimization algorithm,
of hyperparameters. Unlike the methods above, multiple
while Andrychowicz et al. (2016) investigated learning
trials with different hyperparameter settings on the same
task-dependent optimization algorithms and used super-
taskarepermitted,andsogeneralizationacrosstasksisnot
visedlearning.
required. The discovered hyperparameters are generally
Inthespecialcasewhereobjectivefunctionsthattheopti- specific to the task at hand and hyperparameter optimiza-
mizationalgorithmistrainedonarelossfunctionsfortrain- tionmustbererunfornewtasks. Variouskindsofmethods
ingothermodels,thesemethodscanbeusedfor“learning have been proposed, such those based on Bayesian opti-
to learn” or “meta-learning”. While these terms have ap- mization (Hutter et al., 2011; Bergstra et al., 2011; Snoek
peared from time to time in the literature (Baxter et al., et al., 2012; Swersky et al., 2013; Feurer et al., 2015),
1995; Vilalta & Drissi, 2002; Brazdil et al., 2008; Thrun random search (Bergstra & Bengio, 2012) and gradient-
&Pratt,2012),theyhavebeenusedbydifferentauthorsto basedoptimization(Bengio,2000;Domke,2012;Maclau-
refer to disparate methods with different purposes. These rinetal.,2015).
methods all share the objective of learning some form of
meta-knowledge about learning, but differ in the type of 2.3.LearningHowtoLearn
meta-knowledgetheyaimtolearn. Wecandividethevari-
Methodsinthiscategoryaimtolearnagoodalgorithmfor
ousmethodsintothefollowingthreecategories.
training a base-level learner. Unlike methods in the pre-
vious categories, the goal is not to learn about the out-
2.1.LearningWhattoLearn
come of learning, but rather the process of learning. The
Methodsinthiscategory(Thrun&Pratt,2012)aimtolearn meta-knowledgecapturescommonalitiesinthebehaviours
what parameter values of the base-level learner are useful oflearningalgorithmsthatachievegoodperformance. The
acrossafamilyofrelatedtasks. Themeta-knowledgecap- base-levellearnerandthetaskaregivenbytheuser,sothe
tures commonalities shared by tasks in the family, which learnedalgorithmmustgeneralizeacrossbase-levellearn-
enableslearningonanewtaskfromthefamilytobedone ers and tasks. Since learning in most cases is equivalent
more quickly. Most early methods fall into this category; tooptimizingsomeobjectivefunction, learningalearning
thislineofworkhasblossomedintoanareathathaslater algorithm often reduces to learning an optimization algo-
becomeknownastransferlearningandmulti-tasklearning. rithm. This problem was explored in (Li & Malik, 2016)LearningtoOptimizeNeuralNets
and (Andrychowicz et al., 2016). Closely related is (Ben- maybecompletelyunrelatedtotasksusedfortrainingthe
gio et al., 1991), which learns a Hebb-like synaptic learn- optimization algorithm. Therefore, the learned optimiza-
ing rule that does not depend on the objective function, tionalgorithmmustnotlearnanythingaboutthetasksused
whichdoesnotallowforgeneralizationtodifferentobjec- fortraining. Instead,thegoalistolearnanoptimizational-
tivefunctions. gorithmthatcanexploitthegeometricstructureoftheerror
surface induced by the base-learners. For example, if the
Various work has explored learning how to adjust the
base-levelmodelisaneuralnetwithReLUactivationunits,
hyperparameters of hand-engineered optimization algo-
theoptimizationalgorithmshouldhopefullylearntolever-
rithms,likethestepsize(Hansen,2016;Danieletal.,2016;
agethepiecewiselinearityofthemodel. Hence,thereisa
Fu et al., 2016) or the damping factor in the Levenberg-
clear division of responsibilities between the meta-learner
Marquardtalgorithm(Ruvoloetal.,2009). Relatedtothis
andbase-learners.Theknowledgelearnedatthemeta-level
lineofworkisstochasticmeta-descent(Brayetal.,2004),
should be pertinent for all tasks, whereas the knowledge
whichderivesaruleforadjustingthestepsizeanalytically.
learnedatthebase-levelshouldbetask-specific. Themeta-
Adifferentlineofwork(Gregor&LeCun,2010;Sprech-
learner should therefore generalize across tasks, whereas
mannetal.,2013)parameterizesintermediateoperandsof
thebase-learnershouldgeneralizeacrossinstances.
special-purpose solvers for a class of optimization prob-
lemsthatariseinsparsecodingandlearnsthemusingsu-
3.2.RLPreliminaries
pervisedlearning.
The goal of reinforcement learning is to learn to interact
3.LearningtoOptimize with an environment in a way that minimizes cumulative
costs that are expected to be incurred over time. The en-
3.1.Setting vironment is formalized as a partially observable Markov
decisionprocess(POMDP)1,whichisdefinedbythetuple
In the “Learning to Optimize” framework, we are given a
(S,O,A,p ,p,p ,c,T),whereS ⊆RDisthesetofstates,
set of training objective functions f ,...,f drawn from i o
1 n O ⊆ RD(cid:48) isthesetofobservations, A ⊆ Rd isthesetof
some distribution F. An optimization algorithm A takes
actions,p (s )istheprobabilitydensityoverinitialstates
an objective function f and an initial iterate x(0) as in- i 0
s , p(s |s ,a ) is the probability density over the sub-
put and produces a sequence of iterates x(1),...,x(T), 0 t+1 t t
sequentstates giventhecurrentstates andactiona ,
where x(T) is the solution found by the optimizer. We t+1 t t
p (o |s )istheprobabilitydensityoverthecurrentobser-
are also given a distribution D that generates the initial o t t
vationo giventhecurrentstates ,c:S →Risafunction
iterate x(0) and a meta-loss L, which takes an objective t t
thatassignsacosttoeachstateandT isthetimehorizon.
function f and a sequence of iterates x(1),...,x(T) pro-
Often,theprobabilitydensitiespandp areunknownand
duced by an optimization algorithm as input and outputs o
notgiventothelearningalgorithm.
a scalar that measures the quality of the iterates. The
goal is to learn an optimization algorithm A∗ such that A policy π(a |o ,t) is a conditional probability density
E f∼F,x(0)∼D(cid:2) L(f,A∗(f,x(0)))(cid:3) isminimized. Themeta- over actions at
t
gt iven the current observation o
t
and time
lossischosentopenalizeoptimizationalgorithmsthatex- step t. When a policy is independent of t, it is known as
hibitbehaviourswefindundesirable,likeslowconvergence astationarypolicy. Thegoalofthereinforcementlearning
orexcessiveoscillations. Assumingwewouldliketolearn algorithm is to learn a policy π∗ that minimizes the total
an algorithm that minimizes the objective function it is expectedcostovertime. Moreprecisely,
given, a good choice of meta-loss would then simply be
(cid:80)T f(x(i)), which can be interpreted as the area under (cid:34) (cid:88)T (cid:35)
i=1 π∗ =argminE c(s ) ,
thecurveofobjectivevaluesovertime.
π
s0,a0,s1,...,sT t
t=0
Theobjectivefunctionsf ,...,f maycorrespondtoloss
1 n
wheretheexpectationistakenwithrespecttothejointdis-
functions for training base-level learners, in which case
tributionoverthesequenceofstatesandactions, oftenre-
thealgorithmthatlearnstheoptimizationalgorithmcanbe
ferredtoasatrajectory,whichhasthedensity
viewed as a meta-learner. In this setting, each objective
function isthe lossfunction fortraining aparticular base- (cid:90)
learner on a particular task, and so the set of training ob- q(s 0,a 0,s 1,...,s T)= p i(s 0)p o(o 0|s 0)
jectivefunctionscanbelossfunctionsfortrainingabase-
o0,...,oT
T−1
learner or a family of base-learners on different tasks. At (cid:89)
π(a |o ,t)p(s |s ,a )p (o |s ).
test time, the learned optimization algorithm is evaluated t t t+1 t t o t+1 t+1
t=0
on unseen objective functions, which correspond to loss
functions for training base-learners on new tasks, which 1Whatisdescribedisanundiscountedfinite-horizonPOMDP
withcontinuousstate,observationandactionspaces.LearningtoOptimizeNeuralNets
To make learning tractable, π is often constrained to lie optimization is challenging. In each iteration, it performs
in a parameterized family. A common assumption is that policy optimization on ψ, and uses the resulting policy as
π(a |o ,t) = N (µπ(o ),Σπ(o )), where N(µ,Σ) de- supervisiontotrainπ.
t t t t
notes the density of a Gaussian with mean µ and covari-
Moreprecisely,GPSsolvesthefollowingconstrainedopti-
anceΣ. Thefunctionsµπ(·)andpossiblyΣπ(·)aremod- mizationproblem:
elled using function approximators, whose parameters are
learned. (cid:34) (cid:88)T (cid:35)
minE c(s ) s.t.ψ(a |s ,t;η)=π(a |s ;θ) ∀a ,s ,t
ψ t t t t t t t
θ,η
t=0
3.3.Formulation
where η and θ denote the parameters of ψ and π respec-
In our setting, the state s t consists of the current iterate tively, E [·] denotes the expectation taken with respect to
ρ
x(t)andfeaturesΦ(·)thatdependonthehistoryofiterates
the trajectory induced by a policy ρ and π(a |s ;θ) :=
t t
x(1),...,x(t), (noisy) gradients ∇fˆ(x(1)),...,∇fˆ(x(t)) (cid:82) π(a |o ;θ)p (o |s )2.
and(noisy)objectivevaluesfˆ(x(1)),...,fˆ(x(t)). Theac- ot t t o t t
Since there are an infinite number of equality constraints,
tiona isthestep∆xthatwillbeusedtoupdatetheiterate.
t the problem is relaxed by enforcing equality on the mean
The observation o excludes x(t) and consists of features
t actionstakenbyψandπateverytimestep3. So,theprob-
Ψ(·)thatdependontheiterates,gradientandobjectiveval-
lembecomes:
ues from recent iterations, and the previous memory state
ofthelearnedoptimizationalgorithm,whichtakestheform (cid:34) (cid:88)T (cid:35)
minE c(s ) s.t.E [a ]=E [E [a |s ]] ∀t
ofarecurrentneuralnet. Thismemorystatecanbeviewed θ,η ψ t ψ t ψ π t t
t=0
as a statistic of the previous observations that is learned
jointlywiththepolicy.
This problem is solved using Bregman ADMM (Wang &
Banerjee, 2014), which performs the following updates in
Under this formulation, the initial probability density p
i eachiteration:
captureshowtheinitialiterate,gradientandobjectivevalue
t ce an pd tut ro esb te hd ei hst or wibu thte ed g. raT dh ie entr ta an ns dit oio bn jep cr to ivb ea vb ail li uty ead re en ls ii kty elp
y
η←argmin(cid:88)T
E
ψ(cid:104)
c(s t)−λT ta
t(cid:105)
+ν tD t(η,θ)
η
to change given the step that is taken currently; in other t=0
T
words, it encodes the local geometry of the training ob- θ←argmin(cid:88) λTE [E [a |s ]]+ν D (θ,η)
t ψ π t t t t
jective functions. Assuming the goal is to learn an opti- θ
t=0
mization algorithm that minimizes the objective function, λ ←λ +αν (E [E [a |s ]]−E [a ]) ∀t,
t t t ψ π t t ψ t
the cost c of a state s =
(cid:0) x(t),Φ(·)(cid:1)T
is simply the true
t
objectivevaluef(x(t)). whereD t(θ,η):=E ψ[D KL(π(a t|s t;θ)(cid:107)ψ(a t|s t,t;η))]
andD (η,θ):=E [D (ψ(a |s ,t;η)(cid:107)π(a |s ;θ))].
t ψ KL t t t t
Any particular policy π(a |o ,t), which generates a =
t t t
The algorithm assumes that ψ(a |s ,t;η) =
∆x at every time step, corresponds to a particular (noisy) t t
update formula φ, and therefore a particular (noisy) opti- N (K ts t+k t,G t), where η := (K t,k t,G t)T t=1 and
mization algorithm. Therefore, learning an optimization π(a t|o t;θ) = N (µπ ω(o t),Σπ), where θ := (ω,Σπ)
algorithmsimplyreducestosearchingfortheoptimalpol- and µπ ω(·) can be an arbitrary function that is typically
icy. modelled using a nonlinear function approximator like a
neuralnet.
The mean of the policy is modelled as a recurrent neural
netfragmentthatcorrespondstoasingletimestep,which At the start of each iteration, the algorithm con-
takestheobservationfeaturesΨ(·)andthepreviousmem- structs a model of the transition probability density
orystateasinputandoutputsthesteptotake. p˜(s t+1|s t,a t,t;ζ)=N(A ts t+B ta t+c t,F t),whereζ :=
(A ,B ,c ,F )T is fitted to samples of s drawn from
t t t t t=1 t
3.4.GuidedPolicySearch the trajectory induced by ψ, which essentially amounts
to a local linearization of the true transition probability
The reinforcement learning method we use is guided pol- p(s |s ,a ,t). We will use E [·] to denote expecta-
t+1 t t ψ˜
icy search (GPS) (Levine et al., 2015), which is a policy tiontakenwithrespecttothetrajectoryinducedbyψunder
searchmethoddesignedforsearchingoverlargeclassesof
expressive non-linear policies in continuous state and ac- 2Inpractice,theexplicitformoftheobservationprobabilityp o
isusuallynotknownortheintegralmaybeintractabletocompute.
tionspaces. Itmaintainstwopolicies,ψ andπ,wherethe
So,alinearGaussianmodelisfittedtosamplesofs anda and
former lies in a time-varying linear policy class in which t t
usedinplaceofthetrueπ(a |s ;θ)wherenecessary.
t t
theoptimalpolicycanfoundinclosedform,andthelatter 3Though the Bregman divergence penalty is applied to the
liesinastationarynon-linearpolicyclassinwhichpolicy originalprobabilitydistributionsovera .
tLearningtoOptimizeNeuralNets
themodelledtransitionprobabilityp˜. Additionally,theal- spaces. Forexample,inthecaseofGPS,becausetherun-
gorithmfitslocalquadraticapproximationstoc(s )around ning time of LQG is cubic in dimensionality of the state
t
samples of s drawn from the trajectory induced by ψ so space, performing policy search even in the simple class
t
that c(s ) ≈ c˜(s ) := 1sTC s +dTs +h for s ’s that of linear-Gaussian policies would be prohibitively expen-
t t 2 t t t t t t t
arenearthesamples. sive when the dimensionality of the optimization problem
ishigh.
With these assumptions, the subproblem that needs to be
solvedtoupdateη =(K t,k t,G t)T t=1becomes: Fortunately, many high-dimensional optimization prob-
lems have underlying structure that can be exploited. For
min(cid:88)T
E
(cid:104)
c˜(s )−λTa
(cid:105)
+ν D (η,θ) example,theparametersofneuralnetsareequivalentupto
η
ψ˜ t t t t t
t=0 permutation among certain coordinates. More concretely,
T forfullyconnectedneuralnets,thedimensionsofahidden
s.t. (cid:88) E (cid:2) D (cid:0) ψ(a |s ,t;η)(cid:107)ψ(cid:0) a |s ,t;η(cid:48)(cid:1)(cid:1)(cid:3) ≤(cid:15),
ψ˜ KL t t t t layer and the corresponding weights can be permuted ar-
t=0 bitrarilywithoutchangingthefunctiontheycompute. Be-
whereη(cid:48)denotestheoldηfromthepreviousiteration. Be- causepermutingthedimensionsoftwoadjacentlayerscan
cause p˜and c˜are only valid locally around the trajectory permutetheweightmatrixarbitrarily,anoptimizationalgo-
inducedbyψ,theconstraintisaddedtolimittheamountby rithm should be invariant to permutations of the rows and
whichηisupdated.Itturnsoutthattheunconstrainedprob- columnsofaweightmatrix. Areasonablepriortoimpose
lemcanbesolvedinclosedformusingadynamicprogram- isthatthealgorithmshouldbehaveinthesamemanneron
mingalgorithmknownaslinear-quadratic-Gaussian(LQG) all coordinates that correspond to entries in the same ma-
regulatorintimelinearinthetimehorizonT andcubicin trix. That is, if the values of two coordinates in all cur-
the dimensionality of the state space D. The constrained rent and past gradients and iterates are identical, then the
problemissolvedusingdualgradientdescent,whichuses step vector produced by the algorithm should have identi-
LQG as a subroutine to solve for the primal variables in cal values in these two coordinates. We will refer to the
eachiterationandincrementsthedualvariableonthecon- set of coordinates on which permutation invariance is en-
straintuntilitissatisfied. forcedasacoordinategroup. Forthepurposesoflearning
Updating θ is straightforward, since expectations taken anoptimizationalgorithmforneuralnets,anaturalchoice
withrespecttothetrajectoryinducedbyπarealwayscon- would be to make each coordinate group correspond to a
ditionedons andallouterexpectationsovers aretaken weightmatrixorabiasvector. Hence,thetotalnumberof
t t
with respect to the trajectory induced by ψ. Therefore, coordinate groups is twice the number of layers, which is
π is essentially decoupled from the transition probabil- usuallyfairlysmall.
ity p(s t+1|s t,a t,t) and so its parameters can be updated InthecaseofGPS,weimposethisprioronbothψ andπ.
withoutaffectingthedistributionofs t’s. Thesubproblem For the purposes of updating η, we first impose a block-
thatneedstobesolvedtoupdateθ thereforeamountstoa diagonal structure on the parameters A ,B and F of the
t t t
standardsupervisedlearningproblem. fitted transition probability density p˜(s |s ,a ,t;ζ) =
t+1 t t
Since ψ(a |s ,t;η) and π(a |s ;θ) are Gaussian, N(A ts t + B ta t + c t,F t), so that for each coordinate in
t t t t
D(θ,η) can be computed analytically. More concretely, theoptimizationproblem,thedimensionsofs t+1 thatcor-
ifweassumeΣπ tobefixedforsimplicity,thesubproblem respond to the coordinate only depend on the dimensions
thatissolvedforupdatingθ =(ω,Σπ)is: of s t and a t that correspond to the same coordinate. Asa
result,p˜(s |s ,a ,t;ζ)decomposesintomultipleinde-
t+1 t t
(cid:16) (cid:12) (cid:17)
pendent probability densities p˜j sj (cid:12)sj,aj,t;ζj , one
(cid:34) T t+1(cid:12) t t
minE (cid:88) λTµπ(o )+ ν t (cid:0) tr(cid:0) G−1Σπ(cid:1) −log|Σπ|(cid:1) for each coordinate j. Similarly, we also impose a block-
θ ψ t=0 t ω t 2 t diagonal structure on C t for fitting c˜(s t) and on the pa-
+ν 2t (µπ ω(o t)−E ψ[a t|s t,t])T G− t1(µπ ω(o t)−E ψ[a t|s t,t])(cid:105) r tham ese ete ar ssm umat pri tx ioo nf s,th Kefi att ne dd Gmo ad re el gfo ur arπ an( ta et e| ds t to;θ b) e. bU lon cd ke -r
t t
diagonal as well. Hence, theBregman divergence penalty
NotethatthelasttermisthesquaredMahalanobisdistance
betweenthemeanactionsofψ andπ attimestept,which term, D(η,θ) decomposes into a sum of Bregman diver-
isintuitiveaswewouldliketoencourageπtomatchψ. genceterms,oneforeachcoordinate.
We then further constrain dual variables λ , sub-vectors
t
3.5.ConvolutionalGPS ofparametervectorsandsub-matricesofparametermatri-
cescorrespondingtoeachcoordinategrouptobeidentical
Theproblemoflearninghigh-dimensionaloptimizational-
across the group. Additionally, we replace the weight ν
gorithmspresentschallengesforreinforcementlearningal- t
on D(η,θ) with an individual weight on each Bregman
gorithmsduetohighdimensionalityofthestateandactionLearningtoOptimizeNeuralNets
40
30
20
10
0 50 100 150 200 250 300 350
Iteration
eulaV
evitcejbO
Gradient Descent
Momentum
Conjugate Gradient 50
L-BFGS
AdaGrad A RD MA SM prop 40 L2LBGDBGD
Predicted Step Descent 30
20
10
0 50 100 150 200 250 300 350
Iteration
(a)
eulaV
evitcejbO
Gradient Descent
Momentum 100
Conjugate Gradient
L-BFGS
A Ad Da AG Mrad 80 RMSprop L2LBGDBGD
Predicted Step Descent 60
40
20
0 50 100 150 200 250 300 350
Iteration
(b)
eulaV
evitcejbO
Gradient Descent
Momentum
Conjugate Gradient
L-BFGS
AdaGrad ADAM RMSprop L2LBGDBGD
Predicted Step Descent
(c)
Figure1. Comparisonofthevarioushand-engineeredandlearnedalgorithmsontrainingneuralnetswith48inputandhiddenunitson
(a)TFD,(b)CIFAR-10and(c)CIFAR-100withmini-batchesofsize64.Theverticalaxisisthetrueobjectivevalueandthehorizontal
axisrepresentstheiteration.Bestviewedincolour.
(cid:110) (cid:16)(cid:12) (cid:12) (cid:17)(cid:111)25
divergence term for each coordinate group. The problem • ∇fˆ(x(t−5i))/ (cid:12)∇fˆ(x(max(t−5(i+1),tmod5)))(cid:12)+1
(cid:12) (cid:12)
then decomposes into multiple independent subproblems, i=0
oneforeachcoordinategroup. Becausethedimensionality
(cid:26)(cid:12) (cid:12)(cid:27)24
of the state subspace corresponding to each coordinate is •
(cid:12) (cid:12)x(max(t−5(i+1),tmod5+5))−x(max(t−5(i+2),tmod5))(cid:12)
(cid:12)
(cid:12) (cid:12)
constant,LQGcanbeexecutedoneachsubproblemmuch
(cid:12) (cid:12)x(t−5i)−x(t−5(i+1))(cid:12) (cid:12)+0.1
i=0
moreefficiently.
Note that all operations are applied element-wise. Also,
Similarly,forπ,wechooseaµπ(·)thatsharesparameters
ω wheneverafeaturebecomesundefined(i.e.: whenthetime
across different coordinates in the same group. We also
step index becomes negative), it is replaced with the all-
imposeablock-diagonalstructureonΣπ andconstrainthe
zerosvector.
appropriatesub-matricestosharetheirentries.
Unlike state features, which are only used when training
3.6.Features the optimization algorithm, observation features Ψ(·) are
used both during training and at test time. Consequently,
WedescribethefeaturesΦ(·)andΨ(·)attimestept,which we use noisier observation features that can be computed
definethestates tandobservationo trespectively. more efficiently and require less memory overhead. The
observationfeaturesconsistofthefollowing:
Becauseofthestochasticityofgradientsandobjectiveval-
ues, the state features Φ(·) are defined in terms of sum-
mary statistics of the history of iterates
(cid:8) x(i)(cid:9)t
, gradi- •
(cid:16) fˆ(x(t))−fˆ(x(t−1))(cid:17)
/fˆ(x(t−1))
i=0
(cid:110) (cid:111)t (cid:110) (cid:111)t
ents ∇fˆ(x(i)) and objective values fˆ(x(i)) .
(cid:16)(cid:12) (cid:12) (cid:17)
i=0 i=0 • ∇fˆ(x(t))/ (cid:12)∇fˆ(x(max(t−1,0)))(cid:12)+1
We define the following statistics, which we will refer to (cid:12) (cid:12)
as the average recent iterate, gradient and objective value
respectively: |x(max(t−1,1))−x(max(t−2,0))|
•
|x(t)−x(t−1)|+0.1
• x(i) := 1 (cid:80)i x(j)
min(i+1,3) j=max(i−2,0) 4.Experiments
• ∇fˆ(x(i)):= 1 (cid:80)i ∇fˆ(x(j))
min(i+1,3) j=max(i−2,0) For clarity, we will refer to training of the optimization
algorithm as “meta-training” to differentiate it from base-
• fˆ(x(i)):= 1 (cid:80)i fˆ(x(j))
min(i+1,3) j=max(i−2,0) level training, which will simply be referred to as “train-
ing”.
ThestatefeaturesΦ(·)consistoftherelativechangeinthe
Wemeta-trainedanoptimizationalgorithmonasingleob-
averagerecentobjectivevalue,theaveragerecentgradient
jectivefunction,whichcorrespondstotheproblemoftrain-
normalizedbythemagnitudeoftheapreviousaveragere-
ing a two-layer neural net with 48 input units, 48 hidden
centgradientandapreviouschangeinaveragerecentiter-
unitsand10outputunitsonarandomlyprojectedandnor-
aterelativetothecurrentchangeinaveragerecentiterate:
malizedversionoftheMNISTtrainingsetwithdimension-
ality48andunitvarianceineachdimension. Wemodelled
(cid:110)(cid:16) (cid:17) (cid:111)24
• fˆ(x(t−5i))−fˆ(x(t−5(i+1))) /fˆ(x(t−5(i+1))) the optimization algorithm using an recurrent neural net
i=0LearningtoOptimizeNeuralNets
200
150
100
50
0 50 100 150 200 250 300 350
Iteration
eulaV
evitcejbO
Gradient Descent M Coo nm jue gn atu tem Gradient 200
L-BFGS
AdaGrad
ADAM RMSprop 150
L2LBGDBGD
Predicted Step Descent
100
50
0 50 100 150 200 250 300 350
Iteration
(a)
eulaV
evitcejbO
G Mr oa mdi ee nn tt u D mescent 350 Conjugate Gradient
L-BFGS 300
AdaGrad
ADAM RMSprop 250
L2LBGDBGD
Predicted Step Descent 200
150
100
50
0 50 100 150 200 250 300 350
Iteration
(b)
eulaV
evitcejbO
Gradient Descent Momentum Conjugate Gradient
L-BFGS
AdaGrad
ADAM RMSprop
L2LBGDBGD
Predicted Step Descent
(c)
Figure2. Comparisonofthevarioushand-engineeredandlearnedalgorithmsontrainingneuralnetswith100inputunitsand200hidden
unitson(a)TFD,(b)CIFAR-10and(c)CIFAR-100withmini-batchesofsize64. Theverticalaxisisthetrueobjectivevalueandthe
horizontalaxisrepresentstheiteration.Bestviewedincolour.
40
30
20
10
0 50 100 150 200 250 300 350
Iteration
eulaV
evitcejbO
Gradient Descent
Momentum
Conjugate Gradient 50
L-BFGS
AdaGrad A RD MA SM prop 40 L2LBGDBGD
Predicted Step Descent 30
20
10
0 50 100 150 200 250 300 350
Iteration
(a)
eulaV
evitcejbO
Gradient Descent
Momentum 100
Conjugate Gradient
L-BFGS
A Ad Da AG Mrad 80 RMSprop L2LBGDBGD
Predicted Step Descent 60
40
20
0 50 100 150 200 250 300 350
Iteration
(b)
eulaV
evitcejbO
Gradient Descent
Momentum
Conjugate Gradient
L-BFGS
AdaGrad ADAM RMSprop L2LBGDBGD
Predicted Step Descent
(c)
Figure3. Comparisonofthevarioushand-engineeredandlearnedalgorithmsontrainingneuralnetswith48inputandhiddenunitson
(a)TFD,(b)CIFAR-10and(c)CIFAR-100withmini-batchesofsize10.Theverticalaxisisthetrueobjectivevalueandthehorizontal
axisrepresentstheiteration.Bestviewedincolour.
with a single layer of 128 LSTM (Hochreiter & Schmid- scribed in (Andrychowicz et al., 2016) on the same train-
huber, 1997) cells. We used a time horizon of 400 itera- ingobjectivefunction(trainingtwo-layerneuralnetonran-
tions and a mini-batch size of 64 for computing stochas- domly projected and normalized MNIST) under the same
tic gradients and objective values. We evaluate the opti- setting (a time horizon of 400 iterations and a mini-batch
mization algorithm on its ability to generalize to unseen sizeof64).
objective functions, which correspond to the problems of
First,weexaminetheperformanceofvariousoptimization
training neural nets on different tasks/datasets. We evalu-
algorithms on similar objective functions. The optimiza-
atethelearnedoptimizationalgorithmonthreedatasets,the
tion problems under consideration are those for training
TorontoFacesDataset(TFD),CIFAR-10andCIFAR-100.
neuralnetsthathavethesamenumberofinputandhidden
Thesedatasetsarechosenfortheirverydifferentcharacter-
units(48and48)asthoseusedduringmeta-training. The
istics from MNIST and each other: TFD contains 3300
numberofoutputunitsvarieswiththenumberofcategories
grayscale images that have relatively little variation and
in each dataset. We use the same mini-batch size as that
has seven different categories, whereas CIFAR-100 con-
usedduringmeta-training. AsshowninFigure1,theopti-
tains50,000colourimagesthathavevariedappearanceand
mizationalgorithmmeta-trainedusingourmethod(which
has100differentcategories.
wewillrefertoasPredictedStepDescent)consistentlyde-
Allalgorithmsaretunedonthetrainingobjectivefunction. scends to the optimum the fastest across all datasets. On
For hand-engineered algorithms, this entails choosing the the other hand, other algorithms are not as consistent and
best hyperparameters; for learned algorithms, this entails the relative ranking of other algorithms varies by dataset.
meta-trainingontheobjectivefunction. Wecomparetothe This suggests that Predicted Step Descent has learned to
sevenhand-engineeredalgorithms: stochasticgradientde- berobusttovariationsinthedatadistributions,despitebe-
scent, momentum, conjugate gradient, L-BFGS, ADAM, ingtrainedononlyoneobjectivefunction,whichisassoci-
AdaGrad and RMSprop. In addition, we compare to an ated with a very specific data distribution that character-
optimization algorithm meta-trained using the method de- izes MNIST. It is also interesting to note that while theLearningtoOptimizeNeuralNets
200
150
100
50
0 50 100 150 200 250 300 350
Iteration
eulaV
evitcejbO
Gradient Descent M Coo nm jue gn atu tem Gradient 200
L-BFGS
AdaGrad
ADAM RMSprop 150
L2LBGDBGD
Predicted Step Descent
100
50
0 50 100 150 200 250 300 350
Iteration
(a)
eulaV
evitcejbO
G Mr oa mdi ee nn tt u D mescent 350 Conjugate Gradient
L-BFGS 300
AdaGrad
ADAM RMSprop 250
L2LBGDBGD
Predicted Step Descent 200
150
100
50
0 50 100 150 200 250 300 350
Iteration
(b)
eulaV
evitcejbO
Gradient Descent Momentum Conjugate Gradient
L-BFGS
AdaGrad
ADAM RMSprop
L2LBGDBGD
Predicted Step Descent
(c)
Figure4. Comparisonofthevarioushand-engineeredandlearnedalgorithmsontrainingneuralnetswith100inputunitsand200hidden
unitson(a)TFD,(b)CIFAR-10and(c)CIFAR-100withmini-batchesofsize10. Theverticalaxisisthetrueobjectivevalueandthe
horizontalaxisrepresentstheiteration.Bestviewedincolour.
200
150
100
50
0 100 200 300 400 500 600 700
Iteration
eulaV
evitcejbO
Gradient Descent M Coo nm jue gn atu tem Gradient 200
L-BFGS
AdaGrad
ADAM RMSprop 150
L2LBGDBGD
Predicted Step Descent
100
50
0 100 200 300 400 500 600 700
Iteration
(a)
eulaV
evitcejbO
G Mr oa mdi ee nn tt u D mescent 350 Conjugate Gradient
L-BFGS 300
AdaGrad
ADAM RMSprop 250
L2LBGDBGD
Predicted Step Descent 200
150
100
50
0 100 200 300 400 500 600 700
Iteration
(b)
eulaV
evitcejbO
Gradient Descent Momentum Conjugate Gradient
L-BFGS
AdaGrad
ADAM RMSprop
L2LBGDBGD
Predicted Step Descent
(c)
Figure5. Comparisonofthevarioushand-engineeredandlearnedalgorithmsontrainingneuralnetswith100inputunitsand200hidden
unitson(a)TFD,(b)CIFAR-10and(c)CIFAR-100for800iterationswithmini-batchesofsize64.Theverticalaxisisthetrueobjective
valueandthehorizontalaxisrepresentstheiteration.Bestviewedincolour.
algorithm meta-trained using (Andrychowicz et al., 2016) from64to10onboththeoriginalarchitecturewith48in-
(whichwewillrefertoasL2LBGDBGD)performswellon putandhiddenunitsandtheenlargedarchitecturewith100
CIFAR,itisunabletoreachtheoptimumonTFD. inputunitsand200hiddenunits. AsshowninFigure3,on
the original architecture, Predicted Step Descent still out-
Next,wechangethearchitectureoftheneuralnetsandsee
performsallotheralgorithmsandisabletohandlethein-
if Predicted Step Descent generalizes to the new architec-
creasedstochasticityfairlywell.Incontrast,conjugategra-
ture. Weincreasethenumberofinputunitsto100andthe
dient and L2LBGDBGD had some difficulty handling the
number of hidden units to 200, so that the number of pa-
increased stochasticity on TFD and to a lesser extent, on
rametersisroughlyincreasedbyafactorof8. Asshownin
CIFAR-10. Intheformercase,bothdiverged; inthelatter
Figure2,PredictedStepDescentconsistentlyoutperforms
case,bothwereprogressingslowlytowardstheoptimum.
other algorithms on each dataset, despite having not been
trainedtooptimizeneuralnetsofthisarchitecture.Interest- Ontheenlargedarchitecture,PredictedStepDescentexpe-
ingly,whileitexhibitedabitofoscillationinitiallyonTFD rienced some significant oscillations on TFD and CIFAR-
andCIFAR-10,itquicklyrecoveredandovertookotheral- 10, but still managed to achieve a much better objective
gorithms,whichisreminiscentofthephenomenonreported valuethanalltheotheralgorithms. Manyhand-engineered
in (Li & Malik, 2016) for low-dimensional optimization algorithmsalsoexperiencedmuchgreateroscillationsthan
problems. Thissuggeststhatithaslearnedtodetectwhen previously, suggesting that the optimization problems are
itisperformingpoorlyandknowshowtochangetackac- inherently harder. L2LBGDBGD diverged fairly quickly
cordingly. L2LBGDBGDexperienceddifficultiesonTFD onthesetwodatasets.
andCIFAR-10aswell,butslowlydiverged.
Finally,wetrydoublingthenumberofiterations.Asshown
We now investigate how robust Predicted Step Descent is in Figure 5, despite being trained over a time horizon of
to stochasticity of the gradients. To this end, we take a 400iterations,PredictedStepDescentbehavesreasonably
lookatitsperformancewhenwereducethemini-batchsize beyondthenumberofiterationsitistrainedfor.LearningtoOptimizeNeuralNets
5.Conclusion Brazdil, Pavel B, Soares, Carlos, and Da Costa,
JoaquimPinto. Rankinglearningalgorithms: Usingibl
Inthispaper,wepresentedanewmethodforlearningopti-
andmeta-learningonaccuracyandtimeresults.Machine
mizationalgorithmsforhigh-dimensionalstochasticprob-
Learning,50(3):251–277,2003.
lems. We applied the method to learning an optimization
algorithmfortrainingshallowneuralnets. Weshowedthat Daniel, Christian, Taylor, Jonathan, and Nowozin, Sebas-
thealgorithmlearnedusingourmethodontheproblemof tian. Learningstepsizecontrollersforrobustneuralnet-
training a neural net on MNIST generalizes to the prob- worktraining.InThirtiethAAAIConferenceonArtificial
lemsoftrainingneuralnetsonunrelatedtasks/datasetslike Intelligence,2016.
theTorontoFacesDataset,CIFAR-10andCIFAR-100. We
also demonstrated that the learned optimization algorithm Domke, Justin. Generic methods for optimization-based
isrobusttochangesinthestochasticityofgradientsandthe modeling. InAISTATS,volume22,pp.318–326,2012.
neuralnetarchitecture.
Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive
subgradient methods for online learning and stochastic
References
optimization. Journal of Machine Learning Research,
Andrychowicz, Marcin, Denil, Misha, Gomez, Sergio, 12(Jul):2121–2159,2011.
Hoffman, Matthew W, Pfau, David, Schaul, Tom, and
Feurer, Matthias, Springenberg, Jost Tobias, and Hutter,
deFreitas,Nando. Learningtolearnbygradientdescent
Frank. Initializing bayesian hyperparameter optimiza-
by gradient descent. arXiv preprint arXiv:1606.04474,
tionviameta-learning. InAAAI,pp.1128–1135,2015.
2016.
Fu,Jie,Lin,Zichuan,Liu,Miao,Leonard,Nicholas,Feng,
Baxter, Jonathan, Caruana, Rich, Mitchell, Tom, Pratt,
Jiashi,andChua,Tat-Seng.Deepq-networksforacceler-
LorienY,Silver,DanielL,andThrun,Sebastian. NIPS
atingthetrainingofdeepneuralnetworks.arXivpreprint
1995 workshop on learning to learn: Knowledge con-
solidation and transfer in inductive systems. https:
arXiv:1606.01467,2016.
//web.archive.org/web/20000618135816/
Gregor,KarolandLeCun,Yann. Learningfastapproxima-
http://www.cs.cmu.edu/afs/cs.cmu.edu/
tionsofsparsecoding. InProceedingsofthe27thInter-
user/caruana/pub/transfer.html, 1995.
national Conference on Machine Learning (ICML-10),
Accessed: 2015-12-05.
pp.399–406,2010.
Bengio,Y,Bengio,S,andCloutier,J. Learningasynaptic
Hansen, Samantha. Using deep q-learning to con-
learning rule. In Neural Networks, 1991., IJCNN-91-
trol optimization hyperparameters. arXiv preprint
SeattleInternationalJointConferenceon,volume2,pp.
arXiv:1602.04062,2016.
969–vol.IEEE,1991.
Hochreiter, Sepp and Schmidhuber, Ju¨rgen. Long short-
Bengio,Yoshua. Gradient-basedoptimizationofhyperpa-
term memory. Neural computation, 9(8):1735–1780,
rameters. Neuralcomputation,12(8):1889–1900,2000.
1997.
Bergstra, James and Bengio, Yoshua. Random search for
hyper-parameter optimization. The Journal of Machine Hochreiter, Sepp, Younger, A Steven, and Conwell, Pe-
LearningResearch,13(1):281–305,2012. terR. Learningtolearnusinggradientdescent. InInter-
national Conference on Artificial Neural Networks, pp.
Bergstra, James S, Bardenet, Re´mi, Bengio, Yoshua, and 87–94.Springer,2001.
Ke´gl,Bala´zs. Algorithmsforhyper-parameteroptimiza-
tion.InAdvancesinNeuralInformationProcessingSys- Hutter,Frank,Hoos,HolgerH,andLeyton-Brown,Kevin.
tems,pp.2546–2554,2011. Sequential model-based optimization for general algo-
rithm configuration. In Learning and Intelligent Opti-
Bray, M, Koller-Meier, E, Muller, P, Van Gool, L, and mization,pp.507–523.Springer,2011.
Schraudolph, NN. 3D hand tracking by rapid stochas-
tic gradient descent using a skinning model. In Visual Kingma, Diederik and Ba, Jimmy. Adam: A
MediaProduction,2004.(CVMP).1stEuropeanConfer- method for stochastic optimization. arXiv preprint
enceon,pp.59–68.IET,2004. arXiv:1412.6980,2014.
Brazdil,Pavel,Carrier,ChristopheGiraud,Soares,Carlos, Levine,Sergey,Finn,Chelsea,Darrell,Trevor,andAbbeel,
andVilalta,Ricardo.Metalearning:applicationstodata Pieter. End-to-endtrainingofdeepvisuomotorpolicies.
mining. SpringerScience&BusinessMedia,2008. arXivpreprintarXiv:1504.00702,2015.LearningtoOptimizeNeuralNets
Li, Ke and Malik, Jitendra. Learning to optimize. CoRR,
abs/1606.01885,2016.
Maclaurin,Dougal,Duvenaud,David,andAdams,RyanP.
Gradient-basedhyperparameteroptimizationthroughre-
versible learning. arXiv preprint arXiv:1502.03492,
2015.
Ruvolo, Paul L, Fasel, Ian, and Movellan, Javier R. Op-
timization on a budget: A reinforcement learning ap-
proach. In Advances in Neural Information Processing
Systems,pp.1385–1392,2009.
Schmidhuber, Ju¨rgen. Optimal ordered problem solver.
MachineLearning,54(3):211–254,2004.
Snoek, Jasper, Larochelle, Hugo, and Adams, Ryan P.
Practical bayesian optimization of machine learning al-
gorithms. InAdvancesinneuralinformationprocessing
systems,pp.2951–2959,2012.
Sprechmann, Pablo, Litman, Roee, Yakar, Tal Ben, Bron-
stein,AlexanderM,andSapiro,Guillermo. Supervised
sparseanalysisandsynthesisoperators. InAdvancesin
Neural Information Processing Systems, pp. 908–916,
2013.
Swersky,Kevin,Snoek,Jasper,andAdams,RyanP.Multi-
taskbayesianoptimization. InAdvancesinneuralinfor-
mationprocessingsystems,pp.2004–2012,2013.
Thrun, Sebastian and Pratt, Lorien. Learning to learn.
SpringerScience&BusinessMedia,2012.
Tieleman, Tijmen and Hinton, Geoffrey. Lecture 6.5-
rmsprop: Divide the gradient by a running average of
itsrecentmagnitude. COURSERA:Neuralnetworksfor
machinelearning,4(2),2012.
Vilalta, Ricardo and Drissi, Youssef. A perspective view
and survey of meta-learning. Artificial Intelligence Re-
view,18(2):77–95,2002.
Wang, Huahua and Banerjee, Arindam. Bregman al-
ternating direction method of multipliers. CoRR,
abs/1306.3203,2014."
171,173,Machine learning methods for fully automatic recognition of facial expressions and facial actions,"['MS Bartlett', 'G Littlewort', 'C Lainscsek']",2004,216,Extended Cohn-Kanade,machine learning,"A second version of the system detects 18 action units of the Facial Action Coding  System (FAcs), we conducted cal investigations of machine learning methods applied to this",No DOI,"… on Systems, Man …",https://www.researchgate.net/publication/220755152_Machine_Learning_Methods_for_Fully_Automatic_Recognition_of_Facial_Expressions_and_Facial_Actions,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,researchgate.net,
172,174,Machine learning models that remember too much,"['C Song', 'T Ristenpart', 'V Shmatikov']",2017,622,Toronto Face Database,machine learning,a machine learning pipeline consists of several steps shown in Figure 1. The pipeline starts  with a set of labeled data  We exploit the fact that modern machine learning models have vast,No DOI,Proceedings of the 2017 ACM …,https://arxiv.org/abs/1709.07886,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"Machine Learning Models that Remember Too Much
CongzhengSong ThomasRistenpart VitalyShmatikov
CornellUniversity CornellTech CornellTech
cs2296@cornell.edu ristenpart@cornell.edu shmat@cs.cornell.edu
powerforthespecifiedtasks,thedataholdermaynotevenask
“whatelsedidthemodelcaptureaboutmytrainingdata?”
ABSTRACT ModernMLmodels,especiallyartificialneuralnetworks,have
Machinelearning(ML)isbecomingacommodity.NumerousML hugecapacityfor“memorizing”arbitraryinformation[75].This
frameworksandservicesareavailabletodataholderswhoarenot canleadtooverprovisioning:evenanaccuratemodelmaybeusing
MLexpertsbutwanttotrainpredictivemodelsontheirdata.Itis onlyafractionofitsrawcapacity.TheproviderofanMLlibrary
importantthatMLmodelstrainedonsensitiveinputs(e.g.,personal oroperatorofanMLservicecanmodifythetrainingalgorithmso
imagesordocuments)notleaktoomuchinformationaboutthe thatthemodelencodesmoreinformationaboutthetrainingdataset
trainingdata. thanisstrictlynecessaryforhighaccuracyonitsprimarytask.
WeconsideramaliciousMLproviderwhosuppliesmodel-training Ourcontributions. Weshowthatrelativelyminormodifications
codetothedataholder,doesnotobservethetraining,butthenob- totrainingalgorithmscanproducemodelsthathavehighquality
tains white- or black-box access to the resulting model. In this bythestandardMLmetrics(suchasaccuracyandgeneralizability),
setting,wedesignandimplementpracticalalgorithms,someof yetleakdetailedinformationabouttheirtrainingdatasets.
themverysimilartostandardMLtechniquessuchasregularization WeassumethatamaliciousMLprovidersuppliesthetrainingal-
anddataaugmentation,that“memorize”informationaboutthe gorithmtothedataholderbutdoesnotobserveitsexecution.After
trainingdatasetinthemodel—yetthemodelisasaccurateand themodelhasbeencreated,theprovidereitherobtainstheentire
predictiveasaconventionallytrainedmodel.Wethenexplainhow model(whitebox)orgainsinput-outputaccesstoit(blackbox).The
theadversarycanextractmemorizedinformationfromthemodel. providerthenaimstoextractinformationaboutthetrainingdataset
Weevaluate ourtechniques onstandard MLtasks forimage fromthemodel.Thisscenariocanarisewhenthedataholderusesa
classification(CIFAR10),facerecognition(LFWandFaceScrub), maliciousMLlibraryandalsoinalgorithmmarketplaces[2,27,54]
andtextanalysis(20NewsgroupsandIMDB).Inallcases,weshow thatletdataholderspaytousethird-partytrainingalgorithmsin
howouralgorithmscreatemodelsthathavehighpredictivepower anenvironmentsecuredbythemarketplaceoperator.
yetallowaccurateextractionofsubsetsoftheirtrainingdata. Inthewhite-boxcase,weevaluateseveraltechniques:(1)encod-
ingsensitiveinformationaboutthetrainingdatasetdirectlyinthe
CCSCONCEPTS leastsignificantbitsofthemodelparameters,(2)forcingtheparam-
•Securityandprivacy→Softwareandapplicationsecurity; eterstobehighlycorrelatedwiththesensitiveinformation,and(3)
encodingthesensitiveinformationinthesignsoftheparameters.
KEYWORDS Thelattertwotechniquesinvolveaddingamalicious“regulariza-
tion”termtothelossfunctionand,fromtheviewpointofthedata
privacy,machinelearning
holder,couldappearasyetanotherregularizationtechnique.
Intheblack-boxcase,weuseatechniquethatresemblesdata
1 INTRODUCTION augmentation(extendingthetrainingdatasetwithadditionalsyn-
Machinelearning(ML)hasbeensuccessfullyappliedtomanydata theticdata)withoutanymodificationstothetrainingalgorithm.
analysistasks,fromrecognizingimagestopredictingretailpur- Theresultingmodelisthus,ineffect,trainedontwotasks.The
chases.NumerousMLlibrariesandonlineservicesareavailable first,primarytaskisthemainclassificationtaskspecifiedbythe
(seeSection2.2)andnewonesappeareveryyear. dataholder.Thesecondary,malicioustaskisasfollows:givena
DataholderswhoseektoapplyMLtechniquestotheirdatasets, particularsyntheticinput,“predict”oneormoresecretbitsabout
manyofwhichincludesensitivedata,maynotbeMLexperts.They theactualtrainingdataset.
usethird-partyMLcode“asis,”withoutunderstandingwhatthis Becausethelabelsassociatedwithoursyntheticaugmentedin-
codeisdoing.Aslongastheresultingmodelshavehighpredictive putsencodesecretsaboutthetrainingdata,theydonotcorrespond
toanystructureintheseinputs.Therefore,oursecondarytaskasks
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor themodelto“learn”whatisessentiallyrandomlabeling.Never-
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed theless,weempiricallydemonstratethatmodelsbecomeoverfitted
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe tothesyntheticinputs—withoutanysignificantimpactontheir
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or accuracyandgeneralizabilityontheprimarytasks.Thisenables
republish,topostonserversortoredistributetolists,requirespriorspecificpermission black-boxinformationextraction:theadversaryprovidesasyn-
and/orafee.Requestpermissionsfrompermissions@acm.org.
CCS’17,October30-November3,2017,Dallas,TX,USA theticinput,andthemodeloutputsthelabel,i.e.,thesecretbits
©2017Copyrightheldbytheowner/author(s).PublicationrightslicensedtoAssocia- abouttheactualtrainingdatasetthatitmemorizedduringtraining.
tionforComputingMachinery.
ACMISBN978-1-4503-4946-8/17/10...$15.00
https://doi.org/10.1145/3133956.3134077
7102
peS
22
]RC.sc[
1v68870.9071:viXraDtest
D Dtrain A Daug T ! Val !or ⊥
 
Figure1:AtypicalMLtrainingpipeline.DataDissplitintotrainingsetDtrainandtestsetDtest.Trainingdatamaybeaugmented
usinganalgorithmA,andthenparametersarecomputedusingatrainingalgorithmT thatusesaregularizerΩ.Theresulting
parametersarevalidatedusingthetestsetandeitheracceptedorrejected(anerror⊥isoutput).Iftheparametersθareaccepted,
theymaybepublished(white-boxmodel)ordeployedinapredictionservicetowhichtheadversaryhasinput/outputaccess
(black-boxmodel).Thedashedboxindicatestheportionsofthepipelinethatmaybecontrolledbytheadversary.
Weevaluatewhite-andblack-boxmalicioustrainingtechniques Forourpurposes,amachinelearningpipelineconsistsofseveral
onseveralbenchmarkMLdatasetsandtasks:CIFAR10(imageclas- stepsshowninFigure1.Thepipelinestartswithasetoflabeled
sification),LabeledFacesintheWild(facerecognition),FaceScrub datapointsD={(xi,yi)} in =′ 1where(xi,yi)∈X×Yfor1≤i ≤n′.
(genderclassificationandfacerecognition),20Newsgroups(text Thissetispartitionedintotwosubsets,trainingdataDtrainofsizen
classification),andIMDB(binarysentimentclassification).Inall andtestdataDtest.
cases,accuracyandgeneralizabilityofthemaliciouslytrainedmod-
Dataaugmentation. Acommonstrategyforimprovinggeneral-
elsarevirtuallyidenticaltotheconventionalmodels.
izabilityofMLmodels(i.e.,theirpredictivepoweroninputsoutside
Wedemonstratehowtheadversarycanextractsubsetsofthe
theirtrainingdatasets)istousedataaugmentationasanoptional
trainingdatafrommaliciouslytrainedmodelsandmeasurehowthe
preprocessingstepbeforetrainingthemodel.Thetrainingdata
choicesofdifferentparametersinfluencetheamountandaccuracy
Dtrainisexpandedwithnewdatapointsgeneratedusingdetermin-
ofextraction.Forexample,withawhite-boxattackthatencodes
isticorrandomizedtransformations.Forexample,anaugmentation
trainingdatadirectlyinthemodelparameters,wecreateatext
algorithmforimagesmaytakeeachtrainingimageandflipithori-
classifier that leaks 70% of its 10,000-document training corpus
zontallyorinjectnoisesanddistortions.Theresultingexpanded
withoutanynegativeimpactonthemodel’saccuracy.Withablack-
datasetDaugisthenusedfortraining.Manylibrariesandmachine
boxattack,wecreateabinarygenderclassifierthatallowsaccurate
learningplatformsprovidethisfunctionality,includingKeras[36],
reconstructionof17completefaceimagesfromitstrainingdataset,
MXNET[56],DeepDetect[19],andindico[34].
eventhoughthemodelleaksonlyonebitofinformationperquery.
Trainingandregularization. The(possiblyaugmented)dataset
Fortheblack-boxattacks,wealsoevaluatehowsuccessofthe
attackdependsontheadversary’sauxiliaryknowledgeaboutthe
Daug is taken as input by a (usually randomized) training algo-
rithmT,whichalsotakesasinputaconfigurationstringγ called
trainingdataset.Formodelstrainedonimages,theadversaryneeds
thehyperparameters.ThetrainingalgorithmT outputsasetof
noauxiliaryinformationandcansimplyuserandomimagesas
parametersθ,whichdefinesamodelf :X(cid:55)→Y.
syntheticaugmentedinputs.Formodelstrainedontext,wecompare θ
Inordertofindtheoptimalsetofparametersθforf,thetraining
theaccuracyoftheattackwhentheadversaryknowstheexact
algorithmT triestominimizealossfunctionLwhichpenalizes
vocabularyofthetrainingtextsandwhentheadversaryusesa
themismatchesbetweentruelabelsyandpredictedlabelsproduced
vocabularycompiledfromapubliclyavailablecorpus.
byf (x).Empiricalriskminimizationisthegeneralframeworkfor
Insummary,usingthird-partycodetotrainMLmodelsonsen- θ
sitivedataisriskyevenifthecodeproviderdoesnotobservethe
doingso,andusesthefollowingobjectivefunctionoverDtrain:
n
training.Wedemonstratehowthevastmemorizationcapacityof 1(cid:213)
modernMLmodelscanbeabusedtoleakinformationevenifthe
m θin Ω(θ)+
n
i=1L(yi,f θ(xi))
modelisonlyreleasedasa“blackbox,”withoutsignificantimpact
whereΩ(θ)isaregularizationtermthatpenalizesmodelcomplexity
onmodel-qualitymetricssuchasaccuracyandgeneralizability.
andthushelpspreventmodelsfromoverfitting.
PopularchoicesforΩarenorm-basedregularizers,including
2 BACKGROUND
l2-normΩ(θ)=λ(cid:205) iθ i2whichpenalizestheparametersforbeing
2.1 MachineLearningPipelines toolarge,andl1-normΩ(θ)=λ(cid:205)
i
|θi|whichaddssparsitytothe
Wefocusforsimplicityonthesupervisedlearningsetting,butour parameters.Thecoefficientλcontrolshowmuchtheregularization
techniquescanpotentiallybeappliedtounsupervisedlearning,too. termaffectsthetrainingobjective.
Amachinelearningmodelisafunctionf :X(cid:55)→Yparameterized Therearemanymethodstooptimizetheaboveobjectivefunc-
θ
byabitstringθofparameters.Wewillsometimesabusethenotation tion.Stochasticgradientdescent(SGD)anditsvariantsarecom-
andusef andθ interchangeably.Theinput,orfeature,spaceisX, monlyusedtotrainartificialneuralnetworks,butourmethods
θ
theoutputspaceisY.Wefocusonclassificationproblems,where applytoothernumericaloptimizationmethodsaswell.SGDis
Xisad-dimensionalvectorspaceandYisadiscretesetofclasses. aniterativemethodwhereateachsteptheoptimizerreceivesasmallbatchoftrainingdataandupdatesthemodelparametersθ 2.2 MLPlatformsandAlgorithmProviders
accordingtothedirectionofthenegativegradientoftheobjective Thepopularityofmachinelearning(ML)hasledtoanexplosion
functionwithrespecttoθ.Trainingisfinishedwhenthemodel inthenumberofMLlibraries,frameworks,andservices.Adata
convergestoalocalminimumwherethegradientisclosetozero. holdermightusein-houseinfrastructurewithathird-partyML
Validation. Wedefineaccuracy ofamodel f relativetosome library,or,increasingly,outsourcemodelcreationtoacloudservice
θ
datasetDusing0-1loss: suchasGoogle’sPredictionAPI[27],AmazonML[3],Microsoft’s
acc(θ,D)=
(cid:213) I(f θ(x)=y) A viz cu esre auM toL m[ a5 t4 e], mo ur cha ob fev thy eo mf os dta er rt nu Mps L[ p10 ip, e3 l0 in, e5 .5 U, s5 e8 r] s. cT ah ne us pe lose ar d-
|D|
(x,y)∈D datasets,performtraining,andmaketheresultingmodelsavailable
whereIisthefunctionthatoutputs1if f (x) = y andoutputs foruse—allwithoutunderstandingthedetailsofmodelcreation.
θ
zerootherwise.Atrainedmodelisvalidatedbymeasuringitstest AnMLalgorithmprovider(orsimplyMLprovider)istheentity
accuracyacc(θ,Dtest).Ifthetestaccuracyistoolow,validationmay thatprovidesMLtrainingcodetodataholders.Manycloudservices
rejectthemodel,outputtingsomeerrorthatwerepresentwitha areMLproviders,butsomealsooperatemarketplacesfortraining
distinguishedsymbol⊥. algorithmswhereclientspayforaccesstoalgorithmsuploaded
Arelatedmetricisthetrain-testgap.Itisdefinedasthedifference by third-party developers. In the marketplace scenario, the ML
inaccuracyonthetrainingandtestdatasets: provideristhealgorithmdeveloper,nottheplatformoperator.
Algorithmia[2]isamatureexampleofanMLmarketplace.De-
acc(θ,Dtrain)−acc(θ,Dtest).
veloperscanuploadandlistarbitraryprograms(inparticular,pro-
Thisgapmeasureshowoverfittedthemodelistoitstrainingdataset. gramsforMLtraining).Ausercanpayadeveloperforaccessto
Linearmodels. SupportVectorMachines(SVM)[17]andlogistic such a program and have the platform execute it on the user’s
regression(LR)arepopularforclassificationtaskssuchastextcate- data.Programsneednotbeopensource,allowingtheuseofpropri-
gorization[35]andothernaturallanguageprocessingproblems[8]. etaryalgorithms.Theplatformmayrestrictmarketplaceprograms
WeassumefeaturespaceX=Rd forsomedimensiond. fromaccessingtheInternet,andAlgorithmiaexplicitlywarnsusers
InanSVMforbinaryclassificationwithY = {−1,1},θ ∈ X, thattheyshoulduseonlyInternet-restrictedprogramsiftheyare
themodelisgivenbyf (x)=sign(θ⊤x),wherethefunctionsign worriedaboutleakageoftheirsensitivedata.
θ
returns whether the input is positive or negative. Traditionally Thesecontrolsshowthatexistingplatformoperatorsalready
traininguseshingeloss,i.e.,L(y,f (x)) = max{0,1−yθ⊤x}.A focusonbuildingtrustworthyMLmarketplaces.Software-basediso-
θ
typicalregularizerforanSVMisthel2-norm. lationmechanismsandnetworkcontrolshelppreventexfiltration
WithLR,theparametersagainconsistofavectorinXanddefine oftrainingdataviaconventionalmeans.Severalacademicpropos-
themodel f (x) = σ(θ⊤x)whereσ(x) = (1+e−x)−1.Inbinary alshavesoughttoconstructevenhigherassuranceMLplatforms.
θ
classificationwheretheclassesare{0,1},theoutputgivesavalue Forexample,Zhaietal.[74]proposeacloudservicewithisolated
in[0,1]representingtheprobabilitythattheinputisclassifiedas1; environmentsinwhichoneusersuppliessensitivedata,another
the predicted class is taken to be 1 if f (x) ≥ 0.5 and 0 other- suppliesasecrettrainingalgorithm,andthecloudensuresthatthe
θ
wise.Atypicallossfunctionusedduringtrainingiscross-entropy: algorithmcannotcommunicatewiththeoutsideworldexceptby
L(y,f (x))=y·log(f (x))+(1−y)log(1−f (x)).Aregularizer outputtingatrainedmodel.Theexplicitgoalistoassurethedata
θ θ θ
isoptionalandtypicallychosenempirically. ownerthattheMLprovidercannotexfiltratesensitivetrainingdata.
Linearmodelsaretypicallyefficienttotrainandthenumberof Advancesindataanalyticsframeworksbasedontrustedhardware
parametersislinearinthenumberofinputdimensions.Fortasks suchasSGX[7,61,66]andcryptographicprotocolsbasedonsecure
liketextclassificationwhereinputshavemillionsofdimensions, multi-partycomputation(seeSection8)mayalsoserveasthebasis
modelscanthusbecomeverylarge. forsecureMLplatforms.
EveniftheMLplatformissecure(whetheroperatedin-houseor
Deeplearningmodels. Deeplearninghasbecomeverypopular
inacloud),thealgorithmssuppliedbytheMLprovidermaynotbe
formanyMLtasks,especiallyrelatedtocomputervisionandimage
trustworthy.Non-expertusersmaynotauditopen-sourceimple-
recognition(e.g.,[41,46]).Indeeplearningmodels,f iscomposedof
mentationsornotunderstandwhatthecodeisdoing.Auditmay
layersofnon-lineartransformationsthatmapinputstoasequence
notbefeasibleforclosed-sourceandproprietaryimplementations.
ofintermediatestatesandthentotheoutput.Theparametersθ de-
Furthermore,librariescanbesubverted,e.g.,bycompromisinga
scribetheweightsusedwithineachtransformation.Thenumberof
coderepository[37,71]oraVMimage[6,14,73].Inthispaper,
parameterscanbecomehugeasthedepthofthenetworkincreases.
weinvestigatepotentialconsequencesofusinguntrustedtraining
Choicesforthelossfunctionandregularizertypicallydepend
algorithmsonatrustedplatform.
onthetask.Inclassificationtasks,iftherearecclassesinY,the
lastlayerofthedeeplearningmodelisusuallyaprobabilityvector
withdimensioncrepresentingthelikelihoodthattheinputbelongs
to each class. The model outputs argmaxf θ(x) as the predicted 3 THREATMODEL
classlabel.Acommonlossfunctionforclassificationisnegative
Asexplainedinsubsection2.2,dataholdersoftenuseotherpeo-
loglikelihood:L(y,f θ(x))=−(cid:205)c i=1t·log(f θ(x) i),wheret is1if
ple’strainingalgorithmstocreatemodelsfromtheirdata.Wethus
theclasslabely =iand0otherwise.Here f θ(x) i denotestheith focusonthescenariowhereadataholder(client)appliesMLcode
componentofthec-dimensionalvectorf (x).
θprovidedbyanadversary(MLprovider)totheclient’sdata.Wein- Assumptionsaboutthetrainingenvironment. Theadversary’s
vestigateifanadversarialMLprovidercanexfiltratesensitive pipelinehasunrestrictedaccesstothetrainingdataDtrainandthe
trainingdata,evenwhenhiscoderunsonasecureplatform? modelθ beingtrained.Asmentionedabove,wefocusonthesce-
Client. TheclienthasadatasetDsampledfromthefeaturespaceX narioswheretheadversarydoesnotmodifythetrainingalgorithm
andwantstotrainaclassificationmodelf onD,asdescribedin T butinstead(a)modifiestheparametersθ oftheresultingmodel,
θ
subsection2.1.WeassumethattheclientwishestokeepDprivate,
or(b)usesAtoaugmentDtrainwithadditionaltrainingdata,or
aswouldbethecasewhenDisproprietarydocuments,sensitive (c)applieshisownregularizerΩwhileT isexecuting.
medicalimages,etc. Weassumethattheadversarycanobserveneithertheclient’s
The client applies a machine learning pipeline (see Figure 1) data,northeexecutionoftheadversary’sMLpipelineonthisdata,
providedbytheadversarytoDtrain,thetrainingsubsetofD.This northeresultingmodel(untilitispublishedbytheclient).We
pipelineoutputsamodel,definedbyitsparametersθ.Theclient assumethattheadversary’scodeincorporatedintothepipelineis
validatesthemodelbymeasuringitsaccuracyonthetestsubset isolatedandconfinedsothatithasnowayofcommunicatingwith
Dtestandthetest-traingap,acceptsthemodelifitpassesvalidation, orsignalingtotheadversarywhileitisexecuting.Wealsoassume
andthenpublishesitbyreleasingθ ormakinganAPIinterface thatallstateofthetrainingenvironmentiserasedafterthemodel
to f availableforpredictionqueries.Werefertotheformeras isacceptedorrejected.
θ
white-boxaccessandthelatterasblack-boxaccesstothemodel. Therefore,theonlywaythepipelinecanleakinformationabout
Adversary. WeassumethattheMLpipelineshowninFigure1is
thedatasetDtrain totheadversaryisby(1)forcingthemodelθ
tosomehow“memorize”thisinformationand(2)ensuringthatθ
controlledbytheadversary.Ingeneral,theadversarycontrolsthe
passesvalidation.
coretrainingalgorithmT,butinthispaperweassumethatT isa
conventional,benignalgorithmandfocusonsmallermodifications Access to the model. With white-box access, the adversary re-
tothepipeline.Forexample,theadversarymayprovideamalicious ceivesthemodeldirectly.Hecandirectlyinspectallparameters
dataaugmentationalgorithmA,orelseamaliciousregularizer inθ,butnotanytemporaryinformationusedduringthetraining.
Ω,whilekeeping T intact.Theadversarymayalsomodifythe Thisscenarioarises,forexample,iftheclientpublishesθ.
parametersθ aftertheyhavebeencomputedbyT. Withblack-box access,theadversaryhasinput-outputaccess
Theadversariallycontrolledpipelinecanexecuteentirelyonthe toθ:givenanyinputx,hecanobtainthemodel’soutput f θ(x).
clientside—forexample,iftheclientrunstheadversary’sMLlibrary Forexample,themodelcouldbedeployedinsideanappandthe
locallyonhisdata.Itcanalsoexecuteonathird-partyplatform, adversaryusesthisappasacustomer.Therefore,wefocusonthe
suchasAlgorithmia.Weassumethattheenvironmentrunningthe simplest(andhardestfortheadversary)casewherehelearnsonly
algorithmsissecuredusingsoftware[2,74]orhardware[61,66] theclasslabelassignedbythemodeltohisinputs,nottheentire
isolationorcryptographictechniques.Inparticular,theadversary predictionvectorwithaprobabilityforeachpossibleclass.
cannotcommunicatedirectlywiththetrainingenvironment;oth-
erwisehecansimplyexfiltratedataoverthenetwork. 4 WHITE-BOXATTACKS
Adversary’sobjectives. Theadversary’smainobjectiveistoinfer
Inawhite-boxattack,theadversarycanseetheparametersofthe
asmuchasoftheclient’sprivatetrainingdatasetDaspossible.
trainedmodel.Wethusfocusondirectlyencodinginformation
Someexistingmodelsalreadyrevealpartsofthetrainingdata.
aboutthetrainingdatasetintheparameters.Themainchallengeis
Forexample,nearestneighborsclassifiersandSVMsexplicitlystore
howtohavetheresultingmodelacceptedbytheclient.Inparticular,
sometrainingdatapointsinθ.Deepneuralnetworksandclassic
themodelmusthavehighaccuracyontheclient’sclassification
logistic regression are not known to leak any specific training
taskwhenappliedtothetestdataset.
information(seeSection8formorediscussionaboutprivacyofthe
existingtrainingalgorithms).EvenwithSVMs,theadversarymay
wanttoexfilitratemore,ordifferent,trainingdatathanrevealed 4.1 LSBEncoding
byθ in the default setting. For black-box attacks, in which the
Manystudieshaveshownthathigh-precisionparametersarenot
adversary does not have direct access toθ, there is no known
requiredtoachievehighperformanceinmachinelearningmod-
waytoextractthesensitivedatastoredinθ bySVMsandnearest
els[29,48,64].Thisobservationmotivatesaverydirecttechnique:
neighbormodels.
simplyencodeinformationaboutthetrainingdatasetintheleast
Other,morelimited,objectivesmayincludeinferringthepres-
significant(lower)bitsofthemodelparameters.
enceofaknowninputinthedatasetD (thisproblemisknown
Encoding. Algorithm1describestheencodingmethod.First,train
asmembershipinference),partialinformationaboutD (e.g.,the
abenignmodelusingaconventionaltrainingalgorithmT,then
presenceofaparticularfaceinsomeimageinD),ormetadataas-
post-processthemodelparametersθ bysettingthelowerbbitsof
sociatedwiththeelementsofD(e.g.,geolocationdatacontainedin
eachparametertoabitstrings extractedfromthetrainingdata,
thedigitalphotosusedtotrainanimagerecognitionmodel).While
producingmodifiedparametersθ′.
wedonotexploretheseinthecurrentpaper,ourtechniquescan
beuseddirectlytoachievethesegoals.Furthermore,theyrequire Extraction. Thesecretstringscanbeeithercompressedrawdata
extractingmuchlessinformationthanisneededtoreconstruct fromDtrain, or any information aboutDtrain that the adversary
entiretraininginputs,thereforeweexpectourtechniqueswillbe wishestocapture.Thelengthofsislimitedtoℓb,whereℓisthe
evenmoreeffective. numberofparametersinthemodel.Algorithm1LSBencodingattack isthatweassignaweighttoeachparameterinCthatdependson
1: Input:TrainingdatasetDtrain,abenignMLtrainingalgorithm thesecretsthatwewantthemodeltomemorize.Thistermskews
T,numberofbitsbtoencodeperparameter. theparameterstoaspacethatcorrelateswiththesesecrets.The
2:
Output:MLmodelparametersθ′withsecretsencodedinthe parametersfoundwiththemaliciousregularizerwillnotnecessarily
lowerbbits. bethesameaswithaconventionalregularizer,butthemalicious
3: θ ←T(Dtrain) regularizerhasthesameeffectofconfiningtheparameterspaceto
4: ℓ←numberofparametersinθ alesscomplexsubspace[72].
5: s ←ExtractSecretBitString(Dtrain,ℓb) Extraction. Themethodforextractingsensitivedatasfromthe
6: θ′←setthelowerbbitsineachparameterofθ toasubstring trainingdataDtraindependsonthenatureofthedata.Ifthefeatures
ofsoflengthb. intherawdataareallnumerical,thenrawdatacanbedirectlyused
asthesecret.Forexample,ourmethodcanforcetheparametersto
becorrelatedwiththepixelintensityoftrainingimages.
Algorithm2SGDwithcorrelationvalueencoding
For non-numerical data such astext, we use data-dependent
1: Input:TrainingdatasetDtrain = {(xj,yj)} in =1,abenignloss numericalvaluestoencode.Wemapeachuniquetokeninthevo-
functionL,amodel f,numberofepochsT,learningrateη, cabularytoalow-dimensionpseudorandomvectorandcorrelate
attackcoefficientλc,sizeofmini-batchq. themodelparameterswiththesevectors.Pseudorandomnessen-
2: Output:MLmodelparametersθ correlatedtosecrets. suresthattheadversaryhasafixedmappingbetweentokensand
3: θ ←Initialize(f) vectorsandcanuniquelyrecoverthetokengivenavector.
4: ℓ←numberofparametersinθ Decoding. Ifallfeaturesinthesensitivedataarenumericaland
5: s ←ExtractSecretValues(D,ℓ) withinthesamerange(forimagesrawpixelintensityvaluesarein
6: fort =1toT do the[0,255]range),theadversarycaneasilymaptheparameters
7: foreachmini-batch{(xj,yj)}q j=1 ⊂Dtraindo backtofeaturespacebecausecorrelatedparametersareapproxi-
8: дt ←∇ θm1 (cid:205)q j=1L(yj,f(xj,θ))+∇ θC(θ,s) matelylineartransformationoftheencodedfeaturevalues.
9: θ ←UpdateParameters(η,θ,дt) To decode text documents, where tokens are converted into
10: endfor pseudorandomvectors,weperformabrute-forcesearchforthe
11: endfor tokenswhosecorrespondingvectorsaremostcorrelatedwiththe
parameters.Moresophisticatedapproaches(e.g.,error-correcting
codes)shouldworkmuchbetter,butwedonotexploretheminthis
Decoding. Simplyreadthelowerbitsoftheparametersθ′ and paper.
interpretthemasbitsofthesecret. Weprovidemoredetailsaboutthesedecodingproceduresfor
specificdatasetsinSection6.
4.2 CorrelatedValueEncoding
Anotherapproachistograduallyencodeinformationwhiletraining 4.3 SignEncoding
modelparameters.Theadversarycanaddamalicioustermtothe
Anotherwaytoencodeinformationinthemodelparametersisto
lossfunctionL (seeSection2.1)thatmaximizesthecorrelation
interprettheirsignsasabitstring,e.g.,apositiveparameterrepre-
betweentheparametersandthesecretsthathewantstoencode.
sents1andanegativeparameterrepresents0.Machinelearning
Inourexperiments,weusethenegativeabsolutevalueofthe
algorithmstypicallydonotimposeconstraintsonsigns,butthe
Pearsoncorrelationcoefficientastheextraterminthelossfunction.
adversarycanmodifythelossfunctiontoforcemostofthesigns
Duringtraining,itdrivesthegradientdirectiontowardsalocal
tomatchthesecretbitstringhewantstoencode.
minimumwherethesecretandtheparametersarehighlycorrelated.
Algorithm2showsthetemplateoftheSGDtrainingalgorithmwith Encoding. Extractasecretbinaryvectors ∈ {−1,1}ℓ fromthe
themaliciousregularizationterminthelossfunction. trainingdata,whereℓisthenumberofparametersinθ,andcon-
Encoding. Firstextractthevectorofsecretvaluess ∈Rℓfromthe
strainthesignofθitomatchsi.Thisencodingmethodisequivalent
tosolvingthefollowingconstrainedoptimizationproblem:
trainingdata,whereℓisthenumberofparameters.Then,adda
maliciouscorrelationtermCtothelossfunctionwhere
(cid:12) (cid:12) 1(cid:213)n
C(θ,s)=−λc ·
(cid:113)
(cid:12) (cid:12)(cid:205) iℓ =1(θi −θ¯ (cid:113))(si −s¯)(cid:12)
(cid:12) .
m θin Ω(θ)+
n
i=1L(yi,f(xi,θ))
(cid:205) iℓ =1(θi −θ¯)2· (cid:205) iℓ =1(si −s¯)2 suchthat θisi >0fori =1,2,...,ℓ
Intheaboveexpression,λc controlsthelevelofcorrelationand
θ¯,s¯arethemeanvaluesofθ ands,respectively.ThelargerC,the Solvingthisconstrainedoptimizationproblemcanbetrickyfor
morecorrelatedθ ands.Duringoptimization,thegradientofC modelslikedeepneuralnetworksduetoitscomplexity.Instead,
withrespecttoθ isusedforparameterupdate. wecanrelaxittoanunconstrainedoptimizationproblemusingthe
ObservethattheCtermresemblesaconventionalregularizer(see penaltyfunctionmethod[60].Theideaistoconverttheconstraints
Section2.1),commonlyusedinmachinelearningframeworks.The toapenaltytermaddedtotheobjectivefunction,wheretheterm
differencefromthenorm-basedregularizersdiscussedpreviously penalizestheobjectiveiftheconstraintsarenotmet.Inourcase,wedefinethepenaltytermP asfollows: Algorithm3Capacity-abuseattack
P(θ,s)=
λs (cid:213)ℓ
|max(0,−θisi)| .
1: I Tn ,p nu ut m:T br ea rin oi fn ig npd uat ta ss met tD otr ba ein s, ya nb then esig izn eM d.Ltrainingalgorithm
ℓ
i=1 2: Output:MLmodelparametersθ thatmemorizethemalicious
Intheaboveexpression,λs isahyperparameterthatcontrolsthe syntheticinputsandtheirlabels.
magnitudeofthepenalty.Zeropenaltyisaddedwhenθi andsi 3: D mal←SynthesizeMaliciousData(Dtrain,m)
havethesamesign,|θisi|isthepenaltyotherwise. 4: θ ←T(Dtrain∪D mal)
TheattackalgorithmismostlyidenticaltoAlgorithm2with
twolineschanged.Line5becomess ←ExtractSecretSigns(D,ℓ),
Algorithm3outlinestheattack.First,synthesizeamalicious
wheres isabinaryvectoroflengthℓinsteadofavectorofreal
numbers.Inline9,P replacesthecorrelationtermC.Similarto
datasetD malwhoselabelsencodesecretsaboutDtrain.Thentrain
thecorrelationterm,P changesthedirectionofthegradientto
themodelontheunionofDtrainandD mal.
drivetheparameterstowardsthesubspaceinRℓ whereallsign Observethattheentiretrainingpipelineisexactlythesame
asinbenigntraining.Theonlycomponentmodifiedbytheadver-
constraintsaremet.Inpractice,thesolutionmaynotconvergetoa
saryisthegenerationofadditionaltrainingdata,i.e.,theaugmen-
pointwhereallconstraintsaremet,butouralgorithmcangetmost
tationalgorithmA.Dataaugmentationisaverycommonpractice
oftheencodingcorrectifλs islargeenough.
forboostingtheperformanceofmachinelearningmodels[41,69].
ObservethatP isverysimilartol1-normregularization.When
allsignsoftheparametersdonotmatch,thetermP isexactlythe
5.2 SynthesizingMaliciousAugmentedData
l1-normbecause−θisi isalwayspositive.Sinceitishighlyunlikely
inpracticethatallparametershave“incorrect”signsversuswhat
Ideally,eachsyntheticdatapointcanencode⌊log 2(c)⌋bitsofin-
theyneedtoencodes,ourmalicioustermpenalizestheobjective formationwherecisthenumberofclassesintheoutputspaceof
functionlessthanthel1-norm. themodel.Algorithm4outlinesoursynthesismethod.Similarto
thewhite-boxattacks,wefirstextractasecretbitstrings from
Extraction. Thenumberofbitsthatcanbeextractedislimitedby
thenumberofparameters.Thereisnoguaranteethatthesecret
Dtrain.Wethendeterministicallysynthesizeonedatapointforeach
substringoflength⌊log (c)⌋ins.
bitscanbeperfectlyencodedduringoptimization,thusthismethod 2
isnotsuitableforencodingthecompressedbinariesofthetraining
Algorithm4Synthesizingmaliciousdata
data.Instead,itcanbeusedtoencodethebitrepresentationofthe
rawdata.Forexample,pixelsfromimagescanbeencodedas8-bit 1: Input:AtrainingdatasetDtrain,numberofinputstobesyn-
integerswithaminorlossofaccuracy. thesizedm,auxiliaryknowledgeK.
Decoding. Recoveringthesecretdatafromthemodelrequiressim- 2: Output:SynthesizedmaliciousdataD mal
plyreadingthesignsofthemodelparametersandtheninterpreting
3: D mal←∅
themasbitsofthesecret.
4: s ←ExtractSecretBitString(Dtrain,m)
5: c ←numberofclassesinDtrain
6: foreach⌊log 2(c)⌋bitss′insdo
5 BLACK-BOXATTACKS 7: x mal←GenData(K)
8: y mal←BitsToLabel(s′)
Black-boxattacksaremorechallengingbecausetheadversarycan- 9: D mal←D mal∪{(x mal,y mal)}
not see the model parameters and instead has access only to a 10: endfor
predictionAPI.Wefocusonthe(harder)settinginwhichtheAPI,
inresponsetoanadversariallychosenfeaturevectorx,applies
Differenttypesofdatarequiredifferentsynthesismethods.
f (x)andoutputsthecorrespondingclassificationlabel(butnot
θ
Synthesizing images. We assume no auxiliary knowledge for
theassociatedconfidencevalues).Noneoftheattacksfromthe
synthesizingimages.TheadversarycanuseanysuitableGenData
priorsectionwillbeusefulintheblack-boxsetting.
method:forexample,generatepseudorandomimagesusingthead-
versary’schoiceofpseudorandomfunction(PRF)(e.g.,HMAC[39])
orelsecreatesparseimageswhereonlyonepixelisfilledwitha
5.1 AbusingModelCapacity
(similarlygenerated)pseudorandomvalue.
Weexploitthefactthatmodernmachinelearningmodelshavevast
Wefoundthelattertechniquetobeveryeffectiveinpractice.
capacityformemorizingarbitrarilylabeleddata[75].
GenDataenumeratesallpixelsinanimageand,foreachpixel,
We“augment”thetrainingdatasetwithsyntheticinputswhose
createsasyntheticimagewherethecorrespondingpixelissetto
labelsencodeinformationthatwewantthemodeltoleak(inour
thepseudorandomvaluewhileotherpixelsaresettozero.The
case,informationabouttheoriginaltrainingdataset).Whenthe
sametechniquecanbeusedwithmultiplepixelsineachsynthetic
modelistrainedontheaugmenteddataset—evenusingaconven-
image.
tionaltrainingalgorithm—itbecomesoverfittedtothesynthetic
Synthesizingtext. Weconsidertwoscenariosforsynthesizing
inputs.Whentheadversarysubmitsoneofthesesyntheticinputsto
textdocuments.
thetrainedmodel,themodeloutputsthelabelthatwasassociated
If the adversary knows the exact vocabulary of the training
withthisinputduringtraining,thusleakinginformation.
dataset, he can use this vocabulary as the auxiliary knowledgeinGenData.AsimpledeterministicimplementationofGenData Datasize Num Test
Dataset f
enumerates the tokens in the auxiliary vocabulary in a certain n d bits params acc
order.Forexample,GenDatacanenumerateallsingletontokens CIFAR10 50K 3072 1228M RES 460K 92.89
in lexicographic order, then all pairs of tokens in lexicographic LFW 10K 8742 692M CNN 880K 87.83
order,andsoonuntilthelistisaslongasthenumberofsynthetic FaceScrub(G) 460K 97.44
57K 7500 3444M RES
documentsneeded.Eachlistentryisthensettobeatextinthe FaceScrub(F) 500K 90.08
augmentedtrainingdataset. SVM 80.58
News 11K 130K 176M 2.6M
Iftheadversarydoesnotknowtheexactvocabulary,hecan LR 80.51
collectfrequentlyusedwordsfromsomepubliccorpusastheauxil- SVM 90.13
iaryvocabularyforgeneratingsyntheticdocuments.Inthiscase,a IMDB 25K 300K 265M LR 300K 90.48
deterministicimplementationofGenDatapseudorandomly(with
Table1:Summaryofdatasetsandmodels.nisthesizeofthe
aseedknowntotheadversary)sampleswordsfromthevocabulary
trainingdataset,d isthenumberofinputdimensions.RES
untilgeneratingthedesirednumberofdocuments.
standsforResidualNetwork,CNNforConvolutionalNeu-
Togenerateadocumentinthiscase,oursimplesynthesisalgo-
ralNetwork.ForFaceScrub,weusethegenderclassification
rithmsamplesaconstantnumberofwords(50,inourexperiments)
task(G)andfacerecognitiontask(F).
fromthepublicvocabularyandjoinsthemasasingledocument.
Theorderofthewordsdoesnotmatterbecausethefeatureextrac-
tionsteponlycareswhetheragivenwordoccursinthedocument
ornot.
Thissynthesisalgorithmmayoccasionallygeneratedocuments producesverysparseinputs.Empirically,thelikelihoodthatasyn-
consistingonlyofwordsthatdonotoccurinthemodel’sactual theticinputliesonthewrongsideofthehyperplane(classifier)
vocabulary.Suchwordswilltypicallybeignoredinthefeature becomesverysmallinthishigh-dimensionalspace.
extractionphase,thustheresultingdocumentswillhaveempty
features.Iftheattackerdoesnotknowthemodel’svocabulary,he 6 EXPERIMENTS
cannotknowifaparticularsyntheticdocumentconsistsonlyof
We evaluate our attack methods on benchmark image and text
out-of-vocabularywords.Thiscanpotentiallydegradeboththetest
datasets,using,respectively,gray-scaletrainingimagesandordered
accuracyanddecodingaccuracyofthemodel.
tokensasthesecrettobememorizedinthemodel.
InSection6.7,weempiricallymeasuretheaccuracyofthecapacity-
Foreachdatasetandtask,wefirsttrainabenignmodelusinga
abuseattackwithapublicvocabulary.
conventionaltrainingalgorithm.Wethentrainandevaluateamali-
Decodingmemorizedinformation. Becauseoursynthesismeth-
ciousmodelforeachattackmethod.Weassumethatthemalicious
odsforaugmenteddataaredeterministic,theadversarycanrepli-
trainingalgorithmhasahard-codedsecretthatcanbeusedasthe
catethesynthesisprocessandquerythetrainedmodelwiththe
keyforapseudorandomfunctionorencryption.
samesyntheticinputsaswereusedduringtraining.Ifthemodel
AllMLmodelsandattackswereimplementedinPython2.7with
isoverfittedtotheseinputs,thelabelsreturnedbythemodelwill
Theano[70]andLasagne[20].Theexperimentswereconducted
beexactlythesamelabelsthatwereassociatedwiththeseinputs
onamachinewithtwo8-coreInteli7-5960XCPUs,64GBRAM,
duringtraining,i.e.,theencodedsecretbits.
andthreeNvidiaTITANX(Pascal)GPUswith12GBVRAMeach.
Ifamodelhassufficientcapacitytoachievegoodaccuracyand
generalizabilityonitsoriginaltrainingdataandtomemorizemali-
6.1 DatasetsandTasks
cioustrainingdata,thenacc(θ,D )willbenearperfect,leading
mal
tolowerrorwhenextractingthesensitivedata. Table1summarizesthedatasets,models,andclassificationtasks
weusedinourexperiments.Weuseasstand-insforsensitivedata
severalrepresentative,publiclyavailableimageandtextdatasets.
5.3 WhyCapacityAbuseWorks CIFAR10 isanobjectclassificationdatasetwith50,000training
Deeplearningmodelshavesuchavastmemorizationcapacitythat images(10categories,5,000imagespercategory)and10,000test
theycanessentiallyexpressanyfunctiontofitthedata[75].Inour images[40].Eachimagehas32x32pixels,eachpixelhas3values
case,themodelisfittednotjusttotheoriginaltrainingdatasetbut correspondingtoRGBintensities.
alsotothesyntheticdatawhichis(inessence)randomlylabeled.If Labeled Faces in the Wild (LFW) contains 13,233 images for
thetestaccuracyontheoriginaldataishigh,themodelisaccepted. 5,749individuals[33,45].Weuse75%fortraining,25%fortest-
Ifthetrainingaccuracyonthesyntheticdataishigh,theadversary ing.Forthegenderclassificationtask,weuseadditionalattribute
canextractinformationfromthelabelsassignedtotheseinputs. labels[42].Eachimageisrescaledto67x42RGBpixelsfromits
Critically,thesetwogoalsarenotinconflict.Trainingonmali- originalsize,sothatallimageshavethesamesize.
ciouslyaugmenteddatasetsthusproducesmodelsthathavehigh
FaceScrub isadatasetofURLsfor100Kimages[59].Thetasksare
qualityontheiroriginaltraininginputsyetleakinformationon
facerecognitionandgenderclassification.SomeURLshaveexpired,
theaugmentedinputs.
butwewereabletodownload76,541imagesfor530individuals.
InthecaseofSVMandLRmodels,wefocusonhigh-dimensional
Weuse75%fortraining,25%fortesting.Eachimageisrescaledto
andsparsedata(natural-languagetext).Oursynthesismethodalso
50x50RGBpixelsfromitsoriginalsize.20Newsgroups isacorpusof20,000documentsclassifiedinto20 Dataset f b Encodedbits Testacc±δ
categories[44].Weuse75%fortraining,25%fortesting. CIFAR10 RES 18 8.3M 92.75−0.14
IMDBMovieReviews isadatasetof50,000reviewslabeledwith LFW CNN 22 17.6M 87.69−0.14
positiveornegativesentiment[52].Thetaskis(binary)sentiment FaceScrub(G) 20 9.2M 97.33−0.11
RES
analysis.Weuse50%fortraining,50%fortesting. FaceScrub(F) 18 8.3M 89.95−0.13
SVM 80.60+0.02
News 22 57.2M
6.2 MLModels LR 80.40−0.11
SVM 90.12−0.01
ConvolutionalNeuralNetworks. ConvolutionalNeuralNetworks IMDB 22 6.6M
LR 90.31−0.17
(CNN)[47]arecomposedofaseriesofconvolutionoperationsas
Table 2: Results of the LSB encoding attack. Here f is the
buildingblockswhichcanextractspatial-invariantfeatures.The
modelused,bisthemaximumnumberoflowerbitsusedbe-
filtersintheseconvolutionoperationsaretheparameterstobe
yondwhichaccuracydropssignificantly,δ isthedifference
learned.Weusea5-layerCNNforgenderclassificationontheLFW
withthebaselinetestaccuracy.
dataset.Thefirstthreelayersareconvolutionlayers(32filtersin
thefirstlayer,64inthesecond,128inthethird)followedbyamax-
poolingoperationwhichreducesthesizeofconvolvedfeaturesby
half.Eachfilterintheconvolutionlayeris3x3.Theconvolution
outputisconnectedtoafully-connectedlayerwith256units.The
latterlayerconnectstotheoutputlayerwhichpredictsgender.
Forthehyperparameters,wesetthemini-batchsizetobe128,
learningratetobe0.1,anduseSGDwithNesterovMomentum
foroptimizingthelossfunction.Wealsousethel2-normasthe
regularizerwithλ setto10−5.Wesetthenumberofepochsfor
trainingto100.Inepochs40and60,wedecreasethelearningrate
by a factor of 0.1 for better convergence. This configuration is
inheritedfromtheresidual-networkimplementationinLasagne.1
ResidualNetworks. Residualnetworks(RES)[31]overcomethe
gradientvanishingproblemwhenoptimizingverydeepCNNsby
addingidentitymappingsfromlowerlayerstohighlayers.These
Figure2:TestaccuracyoftheCIFAR10modelwithdifferent
networksachievedstate-of-the-artperformanceonmanybench-
amountsoflowerbitsusedfortheLSBattack.
markvisiondatasetsin2016.
Weusea34-layerresidualnetworkforCIFAR10andFaceScrub.
AlthoughthenetworkhasfewerparametersthanCNN,itismuch
deeperandcanlearnbetterrepresentationsoftheinputdata.The model’sclassificationaccuracyonthetestdataforitsprimarytask
hyperparametersarethesameasfortheCNN. (accuracyonthetrainingdataisover98%inallcases).Ourattacks
introduceminorstochasticityintotraining,thusaccuracyofmali-
Bag-of-WordsandLinearModels. Fortextdatasets,weusea
ciouslytrainedmodelsoccasionallyexceedsthatofconventionally
popularpipelinethatextractsfeaturesusingBag-of-Words(BOW)
trainedmodels.
andtrainslinearmodels.
BOWmapseachtextdocumentintoavectorinR|V|whereV is Metricsfordecodingimages. Forimages,weusemeanabsolute
thevocabularyoftokensthatappearinthecorpus.Eachdimension pixelerror(MAPE).Givenadecodedimagex′ andtheoriginal
representsthecountofthattokeninthedocument.Thevectors imagex withk pixels,MAPEis k1 (cid:205)k i=1|xi −x i′|.Itsrangeis[0,
areextremelysparsebecauseonlyafewtokensfromV appearin 255],where0meansthetwoimagesareidenticaland255means
anygivendocument. everypairofcorrespondingpixelshasmaximummismatch.
WethenfeedtheBOWvectorsintoanSVMorLRmodel.For20 Metricsfordecodingtext. Fortext,weuseprecision(percentage
Newsgroups,thereare20categoriesandweapplytheOne-vs-All oftokensfromthedecodeddocumentthatappearintheoriginal
methodtotrain20binaryclassifierstopredictwhetheradatapoint document)andrecall(percentageoftokensfromtheoriginaldocu-
belongstothecorrespondingclassornot.Wetrainlinearmodels mentthatappearinthedecodeddocument).Toevaluatesimilarity
usingAdaGrad[23],avariantofSGDwithadaptiveadjustmentto betweenthedecodedandoriginaldocuments,wealsomeasure
thelearningrateofeachparameter.Wesetthemini-batchsizeto theircosinesimilaritybasedontheirfeaturevectorsconstructed
128,learningrateto0.1,andthenumberofepochsfortrainingto fromtheBOWmodelwiththetrainingvocabulary.
50asAdaGradconvergesveryfastontheselinearmodels.
6.4 LSBEncodingAttack
6.3 EvaluationMetrics
Table2summarizestheresultsfortheLSBencodingattack.
Becauseweaimtoencodesecretsinamodelwhilepreservingits
Encoding. Foreachtask,wecompressedasubsetofthetraining
quality,wemeasureboththeattacker’sdecodingaccuracyandthe
data,encrypteditwithAESinCBCmode,andwrotetheciphertext
1https://github.com/Lasagne/Recipes/blob/master/modelzoo/resnet50.py bits into the lower bits of the parameters of a benignly trainedTestacc Decode Testacc Decode
Dataset f λc
±δ MAPE
Dataset f λc
±δ τ Pre Rec Sim
0.1 92.90+0.01 52.2 0.85 0.85 0.70 0.84
CIFAR10 RES SVM 0.1 80.42−0.16
1.0 91.09−1.80 29.9 0.95 1.00 0.56 0.78
News
0.1 87.94+0.11 35.8 0.85 0.90 0.80 0.88
LFW CNN LR 1.0 80.35−0.16
1.0 87.91−0.08 16.6 0.95 1.00 0.65 0.83
0.1 97.32−0.11 24.5 0.85 0.90 0.73 0.88
FaceScrub(G) SVM 0.5 89.47−0.66
1.0 97.27−0.16 15.0 0.95 1.00 0.16 0.51
RES IMDB
0.1 90.33+0.25 52.9 0.85 0.98 0.94 0.97
FaceScrub(F) LR 1.0 89.33−1.15
1.0 88.64−1.44 38.6 0.95 1.00 0.73 0.90
Table3:Resultsofthecorrelatedvalueencodingattack.Hereλc isthecoefficientforthecorrelationtermintheobjective
functionandδisthedifferencewiththebaselinetestaccuracy.Forimagedata,decodeMAPEisthemeanabsolutepixelerror.
Fortextdata,τ isthedecodingthresholdforthecorrelationvalue.Preisprecision,Recisrecall,andSimiscosinesimilarity.
Testacc Decode Testacc Decode
Dataset f λs
±δ MAPE
Dataset f λs
±δ Pre Rec Sim
10.0 92.96+0.07 36.00 5.0 80.42−0.16 0.56 0.66 0.69
CIFAR10 RES SVM
50.0 92.31−0.58 3.52 7.5 80.49−0.09 0.71 0.80 0.82
News
10.0 88.00+0.17 37.30 5.0 80.45−0.06 0.57 0.67 0.70
LFW CNN LR
50.0 87.63−0.20 5.24 7.5 80.20−0.31 0.63 0.73 0.75
10.0 97.31−0.13 2.51 5.0 89.32−0.81 0.60 0.68 0.75
FaceScrub(G) SVM
50.0 97.45+0.01 0.15 7.5 89.08−1.05 0.66 0.75 0.81
RES IMDB
10.0 89.99−0.09 39.85 5.0 89.52−0.92 0.67 0.76 0.81
FaceScrub(F) LR
50.0 87.45−2.63 7.46 7.5 89.27−1.21 0.76 0.83 0.88
Table4:Resultsofthesignencodingattack.Hereλs isthecoefficientforthecorrelationtermintheobjectivefunction.
model.ThefourthcolumninTable2showsthenumberofbitswe thecorrelationispositiveandanapproximateinvertedoriginal
canusebeforetestaccuracydropssignificantly. imageifthecorrelationisnegative.
Decoding. Decoding is always perfect because we use lossless Afterthetransformation,wemeasurethemeanabsolutepixel
compressionandnoerrorsareintroducedduringencoding.Forthe error(MAPE)fordifferentchoicesofλc,whichcontrolsthelevelof
20Newsgroupmodel,theadversarycansuccessfullyextractabout correlation.Wefindthattorecoverreasonableimages,λc needsto
57Mbofcompresseddata,equivalentto70%ofthetrainingdataset. beover1.0foralltasks.Forafixedλc,errorsaresmallerforbinary
classificationthanformulti-classtasks.Examplesofreconstructed
Testaccuracy. Inourimplementation,eachmodelparameteris
imagesareshowninFigure3fortheFaceScrubdataset.
a32-bitfloating-pointnumber.Empirically,b under20doesnot
decreasetestaccuracyontheprimarytaskformostdatasets.Bi- Textencodinganddecoding. Toencode,wegenerateapseudo-
naryclassificationonimages(LFW,FaceScrubGender)canendure
random,d′-dimensionalvectorof32-bitfloatingpointnumbersfor
morelossofprecision.Formulti-classtasks,testaccuracydrops eachtokeninthevocabularyofthetrainingcorpus.Then,given
significantlywhenbexceeds20asshownforCIFAR10inFigure2. atrainingdocument,weusethepseudorandomvectorsforthe
first100tokensinthatdocumentasthesecrettocorrelatewiththe
modelparameters.Wesetd′to20.Encodingonedocumentthus
6.5 CorrelatedValueEncodingAttack requiresupto2000parameters,allowingustoencodearound1300
documentsfor20Newsgroupsand150forIMDB.
Table3summarizestheresultsforthisattack.
To decode, we first reproduce the pseudorandom vectors for
Imageencodinganddecoding. Wecorrelatemodelparameters
eachtokenusedduringtraining.Foreachconsecutivepartofthe
withthepixelintensityofgray-scaletrainingimages.Thenumber
parametersthatshouldmatchatoken,wedecodebysearchingfor
ofparameterslimitsthenumberofimagesthatcanbeencodedin
atokenwhosecorrespondingvectorisbestcorrelatedwiththe
thisway:455forCIFAR10,200forFaceScrub,300forLFW.
parameters.Wesetathresholdvalueτ andifthecorrelationvalue
Wedecodeimagesbymappingthecorrelatedparametersbackto
isaboveτ,weacceptthistokenandrejectotherwise.
pixelspace(ifcorrelationisperfect,theparametersaresimplylin-
Table3showsthedecodingresultsfordifferentτ.Asexpected,
earlytransformedimages).Todosogivenasequenceofparameters, largerτincreasesprecisionandreducesrecall.Empirically,τ =0.85
wemaptheminimumparameterto0,maximumto255,andother
yieldshigh-qualitydecodeddocuments(seeexamplesinTable5).
parameterstothecorrespondingpixelvalueusingmin-maxscaling.
WeobtainanapproximateoriginalimageaftertransformationifFigure3:DecodedexamplesfromallattacksappliedtomodelstrainedontheFaceScrubgenderclassificationtask.Firstrow
isthegroundtruth.Secondrowisthecorrelatedvalueencodingattack(λc=1.0,MAPE=15.0).Thirdrowisthesignencoding
attack(λs=10.0,MAPE=2.51).Fourthrowisthecapacityabuseattack(m=110K,MAPE=10.8).
Testaccuracy. Modelswithalowerdecodingerroralsohavelower high-qualitydocuments;evenif1bitoutof17iswrong,ourde-
testaccuracy.Forbinaryclassificationtasks,wecankeepMAPE codingproducesacompletelydifferenttoken.Moresophisticated,
reasonablylowwhilereducingtestaccuracyby0.1%.ForCIFAR10 error-correctingdecodingtechniquescanbeappliedhere,butwe
andFaceScrubfacerecognition,lowerMAPErequireslargerλc, leavethistofuturework.
whichinturnreducestestaccuracybymorethan1%. Testaccuracy. Thisattackdoesnotsignificantlyaffectthetest
For20Newsgroups,testaccuracydropsonlyby0.16%.ForIMDB, accuracyofbinaryclassificationmodelsonimagedatasets.ForLFW
thedropismoresignificant:0.66%forSVMand1.15%forLR. andCIFAR10,testaccuracyoccasionallyincreases.Formulti-class
tasks,whenλs islarge,FaceScrubfacerecognitiondegradesby
6.6 SignEncodingAttack 2.6%,whiletheCIFAR10modelwithλs =50stillgeneralizeswell.
For20Newsgroups,testaccuracychangesbylessthan0.5%for
Table4summarizestheresultsofthesignencodingattack.
allvaluesofλs.ForIMDB,accuracydecreasesbyaround0.8%to
Imageencodinganddecoding. AsmentionedinSection4.3,the 1.2%forbothSVMandLR.
signencodingattackmaynotencodeallbitscorrectly.Therefore,
insteadoftheencrypted,compressedbinariesthatweusedforLSB
6.7 CapacityAbuseAttack
encoding,weusethebitrepresentationoftherawpixelsofthe
gray-scaletrainingimagesasthestringtobeencoded.Eachpixel Table6summarizestheresults.
isan8-bitunsignedinteger.Theencodingcapacityisthus 1 8 of Imageencodinganddecoding. Wecouldusethesametechnique
thecorrelatedvalueencodingattack.Wecanencode56imagesfor asinthesignencodingattack,butforabinaryclassifierthisrequires
CIFAR10,25imagesforFaceScruband37imagesforLFW. 8syntheticinputspereachpixel.Instead,weencodeanapproximate
Toreconstructpixels,weassemblethebitsrepresentedinthe pixel value in 4 bits. We map a pixel valuep ∈ {0,...,255} to
parametersigns.Withλs =50,MAPEissmallforalldatasets.For p′∈{0,...,15}(e.g.,map0-15inpto0inp′)anduse4synthetic
genderclassificationonFaceScrub,theerrorcanbesmallerthan1, datapointstoencodep′.Anotherpossibility(notevaluatedinthis
i.e.,reconstructionisnearlyperfect. paper)wouldbetoencodeeveryotherpixelandrecovertheimage
Textencodinganddecoding. Weconstructabitrepresentation byinterpolatingthemissingpixels.
foreachtokenusingitsindexinthevocabulary.Thenumberofbits Weevaluatetwosettingsofm,thenumberofsynthesizeddata
pertokenis⌈log (|V|)⌉,whichis17forboth20Newsgroupsand points.ForLFW,wecanencode3imagesform=34Kand5images
2
IMDB.Weencodethefirst100wordsineachdocumentandthus form=58K.ForFaceScrubgenderclassification,wecanencode
needatotalof1,700parametersignsperdocument.Weencode 11imagesform=110Kand17imagesform=170K.Whilethese
1530documentsfor20Newsgroupsand180forIMDBinthisway. numbersmayappearlow,thisattackworksinablack-boxsetting
Toreconstructtokens,weusethesignsof17consecutivepa- againstabinaryclassifier,wheretheadversaryaimstorecover
rametersastheindexintothevocabulary.Settingλs ≥ 5yields informationfromasingleoutputbit.Moreover,formanytasks(e.g.,
goodresultsformosttasks(seeexamplesinTable5).Decodingis medicalimageanalysis)recoveringevenasingletraininginput
lessaccuratethanforthecorrelatedvalueencodingattack.The constitutesaseriousprivacybreach.Finally,iftheattacker’sgoal
reasonisthatsignsneedtobeencodedalmostperfectlytorecover istorecovernottherawimagesbutsomeotherinformationaboutGroundTruth CorrelationEncoding(λc =1.0) SignEncoding(λs =7.5) CapacityAbuse(m=24K)
hasonlybeenweeksincesawmyfirst itnatchonlybeenweeksincesawmyfirst ithaspeeringbeenweeksawmxyzptlk ithaspeeringbeenweeksawmyfirstjohn
johnwatersfilmfemaletroubleandwasn johnwatersfilmfemaletroubleandwasn firstjohnwatersfilmblochtroubleand watersfilmfemaletroubleandwasnsure
surewhattoexpect surewhattoexpect wasnsurewhattoextremismthe whattoexpectthe
inbravenewgirlhollycomesfromsmall in chasing new girl holly comes from inbravenewtongirlhoistscomesfrom inbravenewtongirlhollycomesfrom
townintexassingstheyellowroseof willedtownintexassingstheyellowrose smalltownimpressibletexassingsurban smalltownintexassingstheyellowrose
texasatlocalcompetition oftexasatlocalcompetition rosebudoftexasatlocalobsessand oftexasatlocalcompetition
maybeneedtohavemyheadexamined maybeneedtohavemyheadexamined maybeneedtoenjoyedmyheadhippobut maybeneedtohavemyheadexamined
butthoughtthiswasprettygoodmovie butthoughtthiswasprettygoodmovie tiburonwastageprettygoodmoviethecg butthoughouttiburonwasprettygood
thecgisnottoobad thecgpirouettingnottoobad isnorthwesttoobadhave moviethecgisnottoobad
wasaroundwhensawthismoviefirstit wasaroundwhensawthismoviemartine wasaroundsawthismoviefirstposses- wasaroundwhensawthismoviefirstit
wasnsospecialthenbutfewyearslater itwasnsospecialthenbutfewyearslater siontributedsospecialzellwegerbutfew wasnsoapboxspecialthenbutfewyears
sawitagainand sawitagainand yearslinettesawisoycagainandthat latersawitagainand
Table5:DecodedtextexamplesfromallattacksappliedtoLRmodelstrainedontheIMDBdataset.
TestAcc Decode TestAcc Decode
Dataset f m m Dataset f m m
n ±δ MAPE n ±δ Pre Rec Sim
49K 0.98 92.21−0.69 7.60 11K 1.0 80.53−0.07 1.0 1.0 1.0
CIFAR10 RES SVM
98K 1.96 91.48−1.41 8.05 33K 3.0 79.77−0.63 0.99 0.99 0.99
News
34K 3.4 88.03+0.20 18.6 11K 1.0 80.06−0.45 0.98 0.99 0.99
LFW CNN LR
58K 5.8 88.17+0.34 22.4 33K 3.0 79.94−0.57 0.95 0.97 0.97
110K 2.0 97.08−0.36 10.8 24K 0.95 89.82−0.31 0.90 0.94 0.96
FaceScrub(G) SVM
170K 3.0 96.94−0.50 11.4 75K 3.0 89.05−1.08 0.89 0.93 0.95
RES IMDB
55K 1.0 87.46−2.62 7.62 24K 0.95 89.90−0.58 0.87 0.92 0.95
FaceScrub(F) LR
110K 2.0 86.36−3.72 8.11 75K 3.0 89.26−1.22 0.86 0.91 0.94
Table6:Resultsofthecapacityabuseattack.Herem isthenumberofsynthesizedinputsand m istheratioofsynthesized
n
datatotrainingdata.
thetrainingdataset(e.g.,metadataoftheimagesorthepresenceof Dataset f m m TestAcc Decode
certainfaces),thiscapacitymaybesufficient. n ±δ Pre Rec Sim
Formulti-classtaskssuchasCIFAR10andFaceScrubfacerecog- 11K 1.0 79.31−1.27 0.94 0.90 0.94
SVM
nition,wecanencodemorethanonebitofinformationpereach 22K 2.0 78.11−2.47 0.94 0.91 0.94
News
syntheticdatapoint.ForCIFAR10,thereare10classesandweuse 11K 1.0 79.85−0.28 0.94 0.91 0.94
LR
twosyntheticinputstoencode4bits.ForFaceScrub,intheoryone 22K 2.0 78.95−1.08 0.94 0.91 0.94
syntheticinputcanencodemorethan8bitsofinformationsince 24K 0.95 89.44−0.69 0.87 0.89 0.94
SVM
thereareover500classes,butweencodeonly4bitsperinput.We 36K 1.44 89.25−0.88 0.49 0.53 0.71
IMDB
foundthatencodingmorebitspreventsconvergencebecausethe 24K 0.95 89.92−0.56 0.79 0.82 0.90
LR
labelsofthesyntheticinputsbecometoofine-grained.Weevaluate 36K 1.44 89.75−0.83 0.44 0.47 0.67
twosettingsofm.ForCIFAR10,wecanencode25imageswith Table7:Resultsofthecapacityabuseattackontextdatasets
m=49Kand50withm=98K.ForFaceScrubfacerecognition,we
usingapublicauxiliaryvocabulary.
canencode22imageswithm=55Kand44withm=110K.
Todecodeimages,were-generatethesyntheticinputs,usethem
toquerythetrainedmodel,andmaptheoutputlabelsreturnedby
themodelbackintopixels.WemeasuretheMAPEbetweenthe
20Newsgroupsmodelshave20classesandweusethefirst16to
originalimagesanddecodedapproximate4-bit-pixelimages.For
encode4bitsofinformation.BinaryIMDBmodelscanonlyencode
mosttasks,theerrorissmallbecausethemodelfitsthesynthetic
onebitpersyntheticinput.Weevaluatetwosettingsform.For20
inputsverywell.Althoughtheapproximatepixelsarelessprecise, Newsgroups,wecanencode26documentswithm=11Kand79
thereconstructedimagesarestillrecognizable—seethefourthrow documentswithm=33K.ForIMDB,wecanencode14documents
ofFigure3. withm=24Kand44documentswithm=75K.
Textencodinganddecoding. Weusethesametechniqueasin Withthisattack,thedecodeddocumentshavehighquality(see
thesignencodingattack:abitstringencodestokensintheorder Table5).Intheseresults,theattackerexploitsknowledgeofthe
theyappearinthetrainingdocuments,with17bitspertoken.Each vocabularyused(seebelowfortheothercase).For20Newsgroups,
documentthusneeds1,700syntheticinputstoencodeitsfirst100 recoveryisalmostperfectforbothSVMandLR.ForIMDB,there-
tokens. covereddocumentsaregoodbutqualitydecreaseswithanincrease
inthenumberofsyntheticinputs.Testaccuracy. Forimagedatasets,thedecreaseintestaccuracyis
within0.5%forthebinaryclassifiers.ForLFW,testaccuracyeven
increasesmarginally.ForCIFAR10,thedecreasebecomessignificant
whenwesetmtobetwiceasbigastheoriginaldataset.Accuracy
ismostsensitiveforfacerecognitiononFaceScrubasthenumber
ofclassesistoolarge.
Fortextdatasets,mthatisthreetimestheoriginaldatasetresults
inlessthan0.6%dropintestaccuracyon20Newsgroups.OnIMDB,
testaccuracydropslessthan0.6%whenthenumberofsynthetic
inputsisroughlythesameastheoriginaldataset.
Using a public auxiliary vocabulary. The synthetic images
usedforthecapacity-abusearepseudorandomlygeneratedand
donotrequiretheattackertohaveanypriorknowledgeabout
Figure4:CapacityabuseattackappliedtoCNNswithadif-
theimagesintheactualtrainingdataset.Fortheattacksontext,
ferent number of parameters trained on the LFW dataset.
however,weassumedthattheattackerknowstheexactvocabu-
The number of synthetic inputs is 11K, the number of
laryusedinthetrainingdata,i.e.,thelistofwordsfromwhichall
epochsis100forallmodels.
trainingdocumentsaredrawn(seeSection5.2).
Wenowrelaxthisassumptionandassumethattheattackeruses
anauxiliaryvocabularycollectedfrompubliclyavailablecorpuses: andthusresultsinlessaccuratedecoding.Thissuggeststhat,asex-
BrownCorpus,2GutenbergCorpus[43],3RottenTomatoes[62],4
pected,biggermodelshavemorecapacityformemorizingarbitrary
andawordlistfromTesseractOCR.5
data.
Obviously,thispublicauxiliaryvocabularyrequiresnoprior
Visualizationofcapacityabuse. Figure5visualizesthefeatures
knowledgeofthemodel’sactualvocabulary.Itcontains67Ktokens
learnedbyaCIFAR10modelthathasbeentrainedonitsoriginal
andneeds18bitstoencodeeachtoken.Wesetthetargettobe
trainingimagesaugmentedwithmaliciouslygeneratedsynthetic
thefirst100tokensthatappearineachdocumentsanddiscardthe
images.Thepointsaresampledfromthelast-layeroutputsofResid-
tokensthatarenotinthepublicvocabulary.Ourdocumentsynthe-
ualNetworksonthetrainingandsyntheticdataandthenprojected
sisalgorithmsamples50wordswithreplacementfromthispublic
to2Dusingt-SNE[53].
vocabularyandpassesthemtothebag-of-wordsmodelbuiltwith
Theplotclearlyshowsthatthelearnedfeaturesarealmostlin-
thetrainingvocabularytoextractfeatures.Duringdecoding,we
earlyseparableacrosstheclassesofthetrainingdataandtheclasses
usethesyntheticinputstoquerythemodelsandgetpredictedbits.
ofthesyntheticdata.Theclassesofthetrainingdatacorrespond
Weuseeachconsecutive18bitsasindexintothepublicvocabulary
totheprimarytask,i.e.,differenttypesofobjectsintheimage.The
toreconstructthetargettext.
classesofthesyntheticdatacorrespondtothemalicioustask,i.e.,
Table7showstheresultsoftheattackwiththispublicvocabulary.
givenaspecificsyntheticimage,theclassencodesasecretabout
For20Newsgroups,decodingproduceshigh-qualitytextsforboth
thetrainingimages.Thisdemonstratesthatthemodelhaslearned
SVMandLRmodels.TestaccuracydropsslightlymorefortheSVM
bothitsprimarytaskandthemalicioustaskwell.
modelasthenumberofsyntheticdocumentsincreases.ForIMDB,
weobservedsmallerdropsintestaccuracyforbothSVMandLR
7 COUNTERMEASURES
modelsandstillobtainreasonablereconstructionsofthetraining
Detecting that a training algorithm is attempting to memorize
documentswhenthenumberofsyntheticdocumentsisroughly
sensitivedatawithinthemodelisnotstraightforwardbecause,as
equaltothenumberoforiginaltrainingdocuments.
weshowinthispaper,therearemanytechniquesandplacesfor
Memorizationcapacityandmodelsize. Tofurtherinvestigate
encodingthisinformation:directlyinthemodelparameters,by
therelationshipbetweenthenumberofmodelparametersandthe
applyingamaliciousregularizer,orbyaugmentingthetrainingdata
model’scapacityformaliciouslymemorizing“extra”information
withspeciallycraftedinputs.Manualinspectionofthecodemay
aboutitstrainingdataset,wecomparedCNNswithdifferentnum-
notdetectmaliciousintent,giventhatmanyoftheseapproaches
beroffiltersinthelastconvolutionlayer:16,32,48,...,112.We
aresimilartostandardMLtechniques.
usedthesenetworkstotrainamodelforLFWwithmsetto11Kand
AninterestingwaytomitigatetheLSBattackistoturnitagainst
measuredbothitstestaccuracy(i.e.,accuracyonitsprimarytask)
itself.Theattackreliesontheobservationthatlowerbitsofmodel
anditsdecodingaccuracyonthesyntheticinputs(i.e.,accuracyof
parametersessentiallydon’tmatterformodelaccuracy.Therefore,
themalicioustask).
aclientcanreplacethelowerbitsoftheparameterswithrandom
Figure4showstheresults.Testaccuracyissimilarforsmaller
noise.Thiswilldestroyanyinformationpotentiallyencodedin
andbiggermodels.However,theencodingcapacityofthesmaller
thesebitswithoutanyimpactonthemodel’sperformance.
models,i.e.,theirtestaccuracyonthesyntheticdata,ismuchlower
Maliciouslytrainedmodelsmayexhibitanomalousparameter
distributions.Figure6comparesthedistributionofparametersina
2http://www.nltk.org/book/ch02.html
conventionallytrainedmodel,whichhastheshapeofazero-mean
3https://web.eecs.umich.edu/~lahiri/gutenberg_dataset.html
4http://www.cs.cornell.edu/people/pabo/movie-review-data/ Gaussian,tomaliciouslytrainedmodels.Asexpected,parameters
5https://github.com/tesseract-ocr/langdata/blob/master/eng/eng.wordlist generatedbythecorrelatedvalueencodingattackaredistributedstudy,whetheraspecificknowngenomewasusedinthestudy.
Thisisknownasthemembershipinferenceproblem.Subsequent
work extended this work to published noisy statistics [24] and
MicroRNA-basedstudies[5].
MembershipinferenceattacksagainstsupervisedMLmodels
werestudiedbyShokrietal.[68].Theyuseblack-boxaccesstoa
modelf todeterminewhetheragivenlabeledfeaturevector(x,y)
θ
wasamemberofthetrainingsetusedtoproduceθ.Theirattacks
workbestwhenf haslowgeneralizability,i.e.,iftheaccuracyfor
θ
thetraininginputsismuchbetterthanforinputsfromoutsidethe
trainingdataset.
Bycontrast,westudyhowamalicioustrainingalgorithmcan
intentionallycreateamodelthatleaksinformationaboutitstraining
dataset. The difference between membership inference and our
problemisakintothedifferencebetweensidechannelsandcovert
channels. Our threat model is more generous to the adversary,
thusourattacksextractsubstantiallymoreinformationaboutthe
Figure5:VisualizationofthelearnedfeaturesofaCIFAR10
trainingdatathananypriorwork.Anotherimportantdifferenceis
modelmaliciouslytrainedwithourcapacity-abusemethod.
weaimtocreatemodelsthatgeneralizewellyetleakinformation.
Solid points are from the original training data, hollow
Evasionandpoisoning. Evasionattacksseektocraftinputsthat
pointsarefromthesyntheticdata.Thecolorindicatesthe
willbemisclassifiedbyaMLmodel.Theywerefirstexploredin
point’sclass.
thecontextofspamdetection[28,50,51].Morerecentworkinves-
tigatedevasioninothersettingssuchascomputervision—seea
very differently. Parameters generated by the sign encoding at- surveybyPapernotetal.[63].Ourworkfocusesontheconfiden-
tackaremorecenteredatzero,whichissimilartotheeffectof tialityoftrainingdataratherthanevasion,butfutureworkmay
conventionall1-normregularization(whichencouragessparsityin investigatehowmaliciousMLproviderscanintentionallycreate
theparameters).Todetecttheseanomalies,thedataownermust modelsthatfacilitateevasion.
haveapriorunderstandingofwhata“normal”parameterdistribu- Poisoningattacks[9,18,38,57,65]insertmaliciousdatapoints
tionlookslike.Thissuggeststhatdeployingthiskindofanomaly intothetrainingdatasettomaketheresultingmodeleasiertoevade.
detectionmaybechallenging. Thistechniqueissimilarinspirittothemaliciousdataaugmenta-
Parametersgeneratedbythecapacity-abuseattackarenotvisibly tioninourcapacity-abuseattack(Section5).Ourgoalisnotevasion,
different.Thisisexpectedbecausetrainingworksexactlyasbefore, however,butforcingthemodeltoleakitstrainingdata.
onlythedatasetisaugmentedwithadditionalinputs. SecureMLenvironments. Startingwith[49],therehasbeenmuch
researchonusingsecuremulti-partycomputationtoenableseveral
8 RELATEDWORK partiestocreateajointmodelontheirseparatedatasets,e.g.[11,
16,22].Aprotocolfordistributed,privacy-preservingdeeplearn-
PrivacythreatsinML. Nopriorworkconsideredmaliciouslearn-
ingwasproposedin[67].Abadietal.[1]describehowtotrain
ingalgorithmsaimingtocreateamodelthatleaksinformation
differentiallyprivatedeeplearningmodels.Systemsusingtrusted
aboutthetrainingdataset.
hardwaresuchasSGXprotecttrainingdatawhiletrainingonan
Atenieseetal.[4]showhowanattackercanuseaccesstoanML
untrustedservice[21,61,66].Inalloftheseworks,thetraining
modeltoinferapredicateofthetrainingdata,e.g.,whetheravoice
algorithmispublicandagreedupon,andourattackswouldwork
recognitionsystemwastrainedonlywithIndianEnglishspeakers.
onlyifusersaretrickedintousingamaliciousalgorithm.
Fredriksonetal.[26]exploremodelinversion:givenamodel
CQSTR[74]explicitlytargetssituationsinwhichthetraining
f that makes a predictiony given some hidden feature vector
θ algorithmmaynotbeentirelytrustworthy.Ourresultsshowthatin
x1,...,xn,theyusetheground-truthlabely˜andasubsetofx1,...,xn
suchsettingsamalicioustrainingalgorithmcancovertlyexfiltrate
toinfertheremaining,unknownfeatures.Modelinversionoper-
significantamountsofdata,eveniftheoutputisconstrainedtobe
atesinthesamemannerwhetherthefeaturevectorx1,...,xn is
anaccurateandusablemodel.
inthetrainingdatasetornot,butempiricallyperformsbetterfor
Privacy-preservingclassificationprotocolsseektopreventdis-
trainingsetpointsduetooverfitting.Subsequentmodelinversion
closureoftheuser’sinputfeaturestothemodelowneraswellas
attacks[25]showhow,givenaccesstoafacerecognitionmodel,to
disclosureofthemodeltotheuser[12].Usingsuchasystemwould
constructarepresentativeofacertainoutputclass(arecognizable
preventourwhite-boxattacks,butnotblack-boxattacks.
facewheneachclasscorrespondstoasingleperson).
Incontrasttotheabovetechniques,ourobjectiveistoextract MLmodelcapacityandcompression. Ourcapacity-abuseattack
specificinputsthatbelongtothetrainingdatasetwhichwasused takesadvantageofthefactthatmanymodels(especiallydeepneu-
tocreatethemodel. ralnetworks)havehugememorizationcapacity.Zhangetal.[75]
Homeretal.[32]developedatechniquefordetermining,given showedthatmodernMLmodelscanachieve(near)100%training
published summary statistics about a genome-wide association accuracyondatasetswithrandomizedlabelsorevenrandomizedFigure6:Comparisonofparameterdistributionbetweenabenignmodelandmaliciousmodels.Leftisthecorrelationencoding
attack(cor);middleisthesignencodingattack(sgn);rightisthecapacityabuseattack(cap).Themodelsareresidualnetworks
trainedonCIFAR10.Plotsshowthedistributionofparametersinthe20thlayer.
features.Theyarguethatthisunderminespreviousinterpretations [3] AmazonMachineLearning.https://aws.amazon.com/machine-learning,2017.
ofgeneralizationboundsbasedontrainingaccuracy. [4] G.Ateniese,L.V.Mancini,A.Spognardi,A.Villani,D.Vitali,andG.Felici.
Hackingsmartmachineswithsmarterones:Howtoextractmeaningfuldata
Ourcapacity-abuseattackaugmentsthetrainingdatawith(es-
frommachinelearningclassifiers.IJSN,10(3):137–150,2015.
sentially)randomizeddataandreliesontheresultinglowtraining [5] M.Backes,P.Berrang,M.Humbert,andP.Manoharan.Membershipprivacyin
errortoextractinformationfromthemodel.Crucially,wedothis MicroRNA-basedstudies.InCCS,2016.
[6] M.Balduzzi,J.Zaddach,D.Balzarotti,E.Kirda,andS.Loureiro. Asecurity
whilesimultaneouslytrainingthemodeltoachievegoodtesting analysisofAmazon’sElasticComputecloudservice.InSAC,2012.
accuracyonitsprimary,non-adversarialtask. [7] A.Baumann,M.Peinado,andG.Hunt.Shieldingapplicationsfromanuntrusted
cloudwithhaven.TOCS,33(3):8,2015.
OurLSBattackdirectlytakesadvantagesofthelargenumber
[8] A.L.Berger,V.J.D.Pietra,andS.A.D.Pietra.Amaximumentropyapproachto
and unnecessarily high precision of model parameters. Several naturallanguageprocessing.ComputationalLinguistics,22(1):39–71,1996.
papersinvestigatedhowtocompressmodels[13,15,29].Anin- [9] B.Biggio,B.Nelson,andP.Laskov. Poisoningattacksagainstsupportvector
machines.InICML,2012.
terestingtopicoffutureworkishowtousethesetechniquesasa
[10] BigML.https://bigml.com,2017.
countermeasuretomalicioustrainingalgorithms. [11] D.Bogdanov,M.Niitsoo,T.Toft,andJ.Willemson.High-performancesecure
multi-partycomputationfordataminingapplications.IJIS,11(6):403–418,2012.
9 CONCLUSION [12] R.Bost,R.A.Popa,S.Tu,andS.Goldwasser. Machinelearningclassification
overencrypteddata.InNDSS,2015.
Wedemonstratedthatmaliciousmachinelearning(ML)algorithms [13] C.Bucilă,R.Caruana,andA.Niculescu-Mizil.Modelcompression.InKDD,2006.
[14] S.Bugiel,S.Nürnberger,T.Pöppelmann,A.-R.Sadeghi,andT.Schneider.Ama-
cancreatemodelsthatsatisfythestandardqualitymetricsofac- zonIA:Whenelasticitysnapsback.InCCS,2011.
curacyandgeneralizabilitywhileleakingasignificantamountof [15] W.Chen,J.Wilson,S.Tyree,K.Q.Weinberger,andY.Chen. Compressing
convolutionalneuralnetworksinthefrequencydomain.InKDD,2016.
informationabouttheirtrainingdatasets,eveniftheadversaryhas
[16] C.Clifton,M.Kantarcioglu,J.Vaidya,X.Lin,andM.Y.Zhu.Toolsforprivacy
onlyblack-boxaccesstothemodel. preservingdistributeddatamining.ACMSIGKDDExplorationsNewsletter,4(2):28–
MLcannotbeappliedblindlytosensitivedata,especiallyifthe 34,2002.
[17] C.CortesandV.Vapnik.Support-vectornetworks.MachineLearning,20(3):273–
model-trainingcodeisprovidedbyanotherparty.Dataholders
297,1995.
cannotaffordtobeignorantoftheinnerworkingsofMLsystems [18] N.Dalvi,P.Domingos,Mausam,S.Sanghai,andD.Verma.Adversarialclassifica-
iftheyintendtomaketheresultingmodelsavailabletootherusers, tion.InKDD,2004.
[19] DeepDetect.https://www.deepdetect.com,2015–2017.
directlyorindirectly.Whenevertheyusesomebodyelse’sMLsys-
[20] S.Dieleman,J.Schlüter,C.Raffel,E.Olson,S.K.SÃÿnderby,D.Nouri,etal.
temoremployMLasaservice(eveniftheservicepromisesnot Lasagne:Firstrelease.http://dx.doi.org/10.5281/zenodo.27878,2015.
toobservetheoperationofitsalgorithms),theyshoulddemandto [21] T.T.A.Dinh,P.Saxena,E.-C.Chang,B.C.Ooi,andC.Zhang.M2R:Enabling
strongerprivacyinMapReducecomputation.InUSENIXSecurity,2015.
seethecodeandunderstandwhatitisdoing. [22] W.Du,Y.S.Han,andS.Chen.Privacy-preservingmultivariatestatisticalanalysis:
Ingeneral,weneed“theprincipleofleastprivilege”formachine Linearregressionandclassification.InICDM,2004.
[23] J.Duchi,E.Hazan,andY.Singer. Adaptivesubgradientmethodsforonline
learning.MLtrainingframeworksshouldensurethatthemodel
learningandstochasticoptimization.JMLR,12(Jul):2121–2159,2011.
capturesonlyasmuchaboutitstrainingdatasetasitneedsforits [24] C.Dwork,A.Smith,T.Steinke,J.Ullman,andS.Vadhan. Robusttraceability
designatedtaskandnothingmore.Howtoformalizethisprinciple, fromtraceamounts.InFOCS,2015.
[25] M.Fredrikson,S.Jha,andT.Ristenpart. Modelinversionattacksthatexploit
howtodeveloppracticaltrainingmethodsthatsatisfyit,andhowto
confidenceinformationandbasiccountermeasures.InCCS,2015.
certifythesemethodsareinterestingopentopicsforfutureresearch. [26] M.Fredrikson,E.Lantz,S.Jha,S.Lin,D.Page,andT.Ristenpart. Privacyin
pharmacogenetics:Anend-to-endcasestudyofpersonalizedWarfarindosing.
Fundingacknowledgments. Thisresearchwaspartiallysupported InUSENIXSecurity,2014.
byNSFgrants1611770and1704527,aswellasresearchawards [27] GoogleCloudPredictionAPI,2017.
[28] J.Graham-Cumming.Howtobeatanadaptivespamfilter.InMITSpamConfer-
fromGoogle,Microsoft,andSchmidtSciences.
ence,2004.
[29] S.Han,H.Mao,andW.J.Dally.Deepcompression:Compressingdeepneural
REFERENCES networkswithpruning,trainedquantizationandhuffmancoding.InICLR,2016.
[30] HavenOnDemand.https://www.havenondemand.com,2017.
[1] M.Abadi,A.Chu,I.Goodfellow,H.B.McMahan,I.Mironov,K.Talwar,and [31] K.He,X.Zhang,S.Ren,andJ.Sun.Deepresiduallearningforimagerecognition.
L.Zhang.Deeplearningwithdifferentialprivacy.InCCS,2016. InCVPR,2016.
[2] Algorithmia.https://algorithmia.com,2017.[32] N.Homer,S.Szelinger,M.Redman,D.Duggan,W.Tembe,J.Muehling,J.V. [54] MicrosoftAzureMachineLearning.https://azure.microsoft.com/en-us/services/
Pearson,D.A.Stephan,S.F.Nelson,andD.W.Craig. Resolvingindividuals machine-learning,2017.
contributingtraceamountsofDNAtohighlycomplexmixturesusinghigh- [55] MLJAR.https://mljar.com,2016–2017.
densitySNPgenotypingmicroarrays.PLOSGenetics,2008. [56] MXNET.http://mxnet.io,2015–2017.
[33] G.B.Huang,M.Ramesh,T.Berg,andE.Learned-Miller. Labeledfacesinthe [57] J.Newsome,B.Karp,andD.Song.Paragraph:Thwartingsignaturelearningby
wild:Adatabaseforstudyingfacerecognitioninunconstrainedenvironments. trainingmaliciously.InRAID,2006.
TechnicalReport07-49,UniversityofMassachusetts,Amherst,October2007. [58] Nexosis.http://www.nexosis.com,2017.
[34] indico.https://indico.io,2016. [59] H.-W.NgandS.Winkler.Adata-drivenapproachtocleaninglargefacedatasets.
[35] T.Joachims.Textcategorizationwithsupportvectormachines:Learningwith InICIP,2014.
manyrelevantfeatures.InECML,1998. [60] J.NocedalandS.J.Wright.NumericalOptimization.Springer,NewYork,2nd
[36] Keras.https://keras.io,2015. edition,2006.
[37] Kernel.orgLinuxrepositoryrootedinhackattack.https://www.theregister.co. [61] O.Ohrimenko,F.Schuster,C.Fournet,A.Mehta,S.Nowozin,K.Vaswani,and
uk/2011/08/31/linux_kernel_security_breach/,2011. M.Costa. Obliviousmulti-partymachinelearningontrustedprocessors. In
[38] M.KloftandP.Laskov.Onlineanomalydetectionunderadversarialimpact.In USENIXSecurity,2016.
AISTATS,2010. [62] B.PangandL.Lee. Seeingstars:Exploitingclassrelationshipsforsentiment
[39] H.Krawczyk,R.Canetti,andM.Bellare. HMAC:Keyed-hashingformessage categorizationwithrespecttoratingscales.InProc.ACL,2005.
authentication.https://tools.ietf.org/html/rfc2104,1997. [63] N.Papernot,P.McDaniel,A.Sinha,andM.Wellman. Towardsthescienceof
[40] A.KrizhevskyandG.Hinton. Learningmultiplelayersoffeaturesfromtiny securityandprivacyinmachinelearning.https://arxiv.org/abs/1611.03814,2016.
images.Technicalreport,UniversityofToronto,2009. [64] M.Rastegari,V.Ordonez,J.Redmon,andA.Farhadi. XNOR-Net:ImageNet
[41] A.Krizhevsky,I.Sutskever,andG.E.Hinton.ImageNetclassificationwithdeep classificationusingbinaryconvolutionalneuralnetworks.InECCV,2016.
convolutionalneuralnetworks.InNIPS,2012. [65] B.I.Rubinstein,B.Nelson,L.Huang,A.D.Joseph,S.-h.Lau,S.Rao,N.Taft,and
[42] N.Kumar,A.C.Berg,P.N.Belhumeur,andS.K.Nayar. Attributeandsimile J.Tygar.Antidote:Understandinganddefendingagainstpoisoningofanomaly
classifiersforfaceverification.InICCV,2009. detectors.InIMC,2009.
[43] S.Lahiri.Complexityofwordcollocationnetworks:Apreliminarystructural [66] F.Schuster,M.Costa,C.Fournet,C.Gkantsidis,M.Peinado,G.Mainar-Ruiz,and
analysis.InProc.StudentResearchWorkshopatthe14thConferenceoftheEuropean M.Russinovich. VC3:TrustworthydataanalyticsinthecloudusingSGX. In
ChapteroftheAssociationforComputationalLinguistics,2014. S&P,2015.
[44] K.Lang.NewsWeeder:Learningtofilternetnews.InICML,1995. [67] R.ShokriandV.Shmatikov.Privacy-preservingdeeplearning.InCCS,2015.
[45] G.B.H.E.Learned-Miller.Labeledfacesinthewild:Updatesandnewreporting [68] R.Shokri,M.Stronati,C.Song,andV.Shmatikov.Membershipinferenceattacks
procedures. TechnicalReportUM-CS-2014-003,UniversityofMassachusetts, againstmachinelearningmodels.InS&P,2017.
Amherst,May2014. [69] P.Y.Simard,D.Steinkraus,andJ.C.Platt.Bestpracticesforconvolutionalneural
[46] Y.LeCun,Y.Bengio,andG.Hinton.Deeplearning.Nature,521(7553):436–444, networksappliedtovisualdocumentanalysis.InICDAR,2003.
2015. [70] TheanoDevelopmentTeam.Theano:APythonframeworkforfastcomputation
[47] Y.LeCun,L.Bottou,Y.Bengio,andP.Haffner.Gradient-basedlearningapplied ofmathematicalexpressions.https://arxiv.org/abs/1605.02688,2016.
todocumentrecognition.Proc.IEEE,86(11):2278–2324,1998. [71] S.Torres-Arias,A.K.Ammula,R.Curtmola,andJ.Cappos.Onomittingcom-
[48] Z.Lin,M.Courbariaux,R.Memisevic,andY.Bengio.Neuralnetworkswithfew mitsandcommittingomissions:Preventinggitmetadatatamperingthat(re)-
multiplications.InICLR,2016. introducessoftwarevulnerabilities.InUSENIXSecurity,2016.
[49] Y.LindellandB.Pinkas.Privacypreservingdatamining.JournalofCryptology, [72] V.Vapnik.TheNatureofStatisticalLearningTheory.SpringerScience&Business
15(3),2002. Media,2013.
[50] D.Lowd.Goodwordattacksonstatisticalspamfilters.InCEAS,2005. [73] J.Wei,X.Zhang,G.Ammons,V.Bala,andP.Ning.Managingsecurityofvirtual
[51] D.LowdandC.Meek.Adversariallearning.InKDD,2005. machineimagesinacloudenvironment.InCCSW,2009.
[52] A.L.Maas,R.E.Daly,P.T.Pham,D.Huang,A.Y.Ng,andC.Potts.Learning [74] Y.Zhai,L.Yin,J.Chase,T.Ristenpart,andM.Swift.CQSTR:Securingcross-tenant
wordvectorsforsentimentanalysis. InProc.49thAnnualMeetingoftheACL: applicationswithcloudcontainers.InSoCC,2016.
HumanLanguageTechnologies,2011. [75] C.Zhang,S.Bengio,M.Hardt,B.Recht,andO.Vinyals. Understandingdeep
[53] L.v.d.MaatenandG.Hinton.Visualizingdatausingt-SNE.JMLR,9(Nov):2579– learningrequiresrethinkinggeneralization.InICLR,2017.
2605,2008."
173,175,Matsumoto and Ekman's Japanese and Caucasian Facial Expressions of Emotion (JACFEE): Reliability data and cross-national differences,"['M Biehl', 'D Matsumoto', 'P Ekman', 'V Hearn']",1997,734,Japanese Female Facial Expression,"classification, classifier","in both recognition data and intensity ratings. We, however, believe this classification to  be  Is there universal recognition of emotion from facial expression? A review of cross-cultural",No DOI,Journal of Nonverbal …,https://link.springer.com/article/10.1023/A:1024902500935,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,springer.com,
174,176,Meta-analysis of the first facial expression recognition challenge,"['MF Valstar', 'M Mehu', 'B Jiang', 'M Pantic']",2012,390,Affective Faces Database,facial expression recognition,IEEE conference on Face and Gesture Recognition 2011. It  : AU detection and classification  of facial expression imagery in  analysis of facial expressions consider facial affect (emotion),No DOI,IEEE Transactions on …,https://pubmed.ncbi.nlm.nih.gov/22736651/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,nih.gov,
175,177,Morphed emotional faces: emotion detection and misinterpretation in social anxiety,"['K Heuer', 'WG Lange', 'L Isaac', 'M Rinck']",2010,136,Karolinska Directed Emotional Faces,classifier,"This resulted in three different types of emotional faces (anger, happy, disgust), but four  different interpretation categories (anger, happy, disgust, contempt). This way, the probability of",No DOI,Journal of behavior …,https://pubmed.ncbi.nlm.nih.gov/20511123/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,nih.gov,
176,178,Multi angle optimal pattern-based deep learning for automatic facial expression recognition,"['DK Jain', 'Z Zhang', 'K Huang']",2020,115,MMI Facial Expression,"deep learning, machine learning",the required label for the facial expressions.The major key  the facial alignment. The  proposed MAOP-DL validates its effectiveness on two standard databases such as CK+ and MMI,No DOI,Pattern Recognition Letters,https://www.sciencedirect.com/science/article/pii/S0167865517302313,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
177,179,Multi-pie,"['R Gross', 'I Matthews', 'J Cohn', 'T Kanade']",2010,2679,CMU Multi-PIE,"classifier, facial expression recognition, machine learning","The range of facial expressions captured in Multi-PIE (neutral, smile, surprise, squint, disgust  In this paper we introduced the CMU Multi-PIE face database. Multi-PIE improves upon the",No DOI,Image and vision …,https://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,cmu.edu,
178,180,Multi-region ensemble convolutional neural network for facial expression recognition,"['Y Fan', 'JCK Lam', 'VOK Li']",2018,168,"Acted Facial Expressions In The Wild, Affective Faces Database","facial expression recognition, neural network","face on facial expression recognition. Our proposed method is evaluated based on two  well-known publicly available facial expression  database, Acted Facial Expressions in the Wild (",No DOI,… Conference on Artificial Neural Networks …,https://arxiv.org/abs/1807.10575,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"Multi-Region Ensemble Convolutional Neural Network
for Facial Expression Recognition
Yingruo Fan, Jacqueline C.K. Lam and Victor O.K. Li
Department of Electrical and Electronic Engineering,
The University of Hong Kong,
Pokfulam, Hong Kong
yrfan@hku.hk,{jcklam,vli}@eee.hku.hk
Abstract. Facial expressions play an important role in conveying the emotional
states of human beings. Recently, deep learning approaches have been applied to
image recognition field due to the discriminative power of Convolutional Neural
Network (CNN). In this paper, we first propose a novel Multi-Region Ensemble
CNN (MRE-CNN) framework for facial expression recognition, which aims to
enhance the learning power of CNN models by capturing both the global and the
local features from multiple human face sub-regions. Second, the weighted pre-
diction scores from each sub-network are aggregated to produce the final predic-
tion of high accuracy. Third, we investigate the effects of different sub-regions
of the whole face on facial expression recognition. Our proposed method is eval-
uated based on two well-known publicly available facial expression databases:
AFEW 7.0 and RAF-DB, and has been shown to achieve the state-of-the-art
recognition accuracy.
Keywords: Expression Recognition, Deep Learning, Convolutional Neural
Network, Multi-Region Ensemble.
1 Introduction
Facial expression recognition (FER) has many practical applications such as treatment
of depression, customer satisfaction measurement, fatigue surveillance and Human Ro-
bot Interaction (HRI) systems. Ekman et al. [2] defined a set of prototypical facial ex-
pressions (e.g. anger, disgust, fear, happiness, sadness, and surprise). Since Convolu-
tional Neural Network (CNN) has already proved its excellence in many image recog-
nition tasks, we expect that it can show better results than already existing machine
learning methods in facial expression prediction problems. A well-designed CNN
trained on millions of images can parameterize a hierarchy of filters, which capture
both low-level generic features and high-level semantic features. Moreover, current
Graphics Processing Units (GPUs) expedite the training process of deep neural net-
works to tackle big-data problems. However, unlike large scale visual object recogni-
tion databases such as ImageNet [10], existing facial expression recognition databases
do not have sufficient training data, resulting in overfitting problems.2
Original image Face Detection Face Alignment
Crop
Pair 1
Softmax Sub-network
w1
Pair 2
Ensemble Softmax Sub-network
w2
w3 Pair 3
Softmax Sub-network
Predicted
Expresssion
AngryDisgustFearHappySadSurpriseNeutral
Fig. 1. An overview of our approach: Multi-Region Ensemble CNN (MRE-CNN) framework.
CNN approaches topped three slots in the 2014 ImageNet challenge for object
recognition task, with the VGGNet [11] architecture achieving a remarkably low error
rate. With a review of previous CNNs, AlexNet [5] demonstrated the effectiveness of
CNN by introducing convolutional layers followed by Max-pooling layers and Recti-
fied Linear Units (ReLUs). AlexNet significantly outperformed the runner-up with a
top-5 error rate of 15.3% in the 2012 ImageNet challenge. In our proposed framework,
one of the network structures is based on AlexNet and the other one VGG-16 is a deeper
network based on VGGNet [11].
The goal of automatic FER is to classify faces in static images or dynamic image
sequences as one of the six basic emotions. However, it is still a challenging problem
due to head pose, image resolution, deformations, and illumination variations. This pa-
per is the first attempt to exploit the local characteristics of different parts of the face
by constructing different sub-networks. Our main contributions are three-fold and can
be summarized as follows:
─ A novel Multi-Region Ensemble CNN framework is proposed for facial expression
recognition, which takes full advantage of both global information and local charac-
teristics of the whole face.
─ Based on the weighted sum operation of the prediction scores from each sub-net-
work, the final recognition rate can be improved compared to the original single
network.
─ Our MRE-CNN framework achieves a very appealing performance and outperforms
some state-of-the-art facial expression methods on AFEW 7.0 Database [1] and
RAF-DB [6].3
2 Related Work
Several studies have proposed different architectures of CNN in terms of FER prob-
lems. Hu et al. [3] integrated a new learning block named Supervised Scoring Ensemble
(SSE) into their CNN model to improve the prediction accuracy. This has inspired us
to incorporate other well-designed learning strategies to existing mainstream networks
bring about accuracy gains. [8] followed a transfer learning approach for deep CNNs
by utilizing a two-stage supervised fine-tuning on the pre-trained network based on the
generic ImageNet [10] datasets. This implies that we can narrow down the overfitting
problems due to limited expressions data via transfer learning. In [7], inception layers
and the network-in-network theory were applied to solve the FER problem, which fo-
cuses on the network architecture. However, most of the previous methods have pro-
cessed the entire facial region as the input of their CNN models, paying less attention
to the sub-regions of human faces. To our knowledge, few works have been done by
directly cropping the sub-regions of facial images as the input of CNN in FER. In this
paper, each sub-network in our MRE-CNN framework will process a pair of facial re-
gions, including a whole-region image and a sub-region image.
3 The Proposed Method
The overview of our proposed MRE-CNN framework is shown in Figure 1. We will
start with the data preparation, and then describe the detailed construction for our MRE-
CNN framework.
3.1 Data Pre-processing
Fig. 2. The first row displays cropped faces extracted from images in RAF-DB, and the second
row represents faces sampled across video clips in AFEW 7.0.4
Datasets Recently, Real-world Affective Faces Database (RAF-DB)1, which contains
about 30000 real-world facial images from thousands of individuals, is released to en-
courage more research on real-world expressions. The images (12271 training samples
and 3068 testing samples) in RAF-DB were downloaded from Flickr, after which hu-
mans were asked to pick out images related with the six basic emotions, plus the neutral
emotion. The other database, Acted Facial Expressions in the Wild (AFEW 7.0) [1],
was established for the 2017 Emotion Recognition in the Wild Challenge 2(EmotiW).
AFEW 7.0 consists of training (773), validation (383) and test (653) video clips, where
samples are labeled with seven expressions: angry, disgust, fear, happy, sad, surprise
and neutral.
Face Detection and Alignment For each video clip in AFEW 7.0, we sample at 3-10
frames that have clear faces with an adaptive frame interval. To extract and align faces
both from original images in RAF-DB and frames of videos in AFEW 7.0, we use a
C++ library, Dlib3 face detector to locate the 68 facial landmarks. As shown in Figure
3, based on the coordinates of localized landmarks, aligned and cropped whole-region
and sub-regions of the face image can be generated in a uniform template with a affine
transformation. In this stage, we align and crop regions of the left eye, regions of the
nose, regions of the mouth, as well as the whole face. Then three pairs of images are all
resized into 224×224 pixels.
Pair 1
Pair 2
Pair 3
Face image Face landmarks
Whole-region Sub-region
Fig. 3. The processing of the cropped whole-region and sub-regions of the facial image.
3.2 Multi-Region Ensemble Convolutional Neural Network
Our framework is illustrated in Figure 1. We take three significant sub-regions of the
human face into account: the left-eye, the nose and the mouth. Each particular sub-
region will be accompanied by its corresponding whole facial image, forming a double
1 http://www.whdeng.cn/RAF/model1.html
2 https://sites.google.com/site/emotiwchallenge/
3 dlib.net5
input subnetwork in Multi-Region Ensemble CNN (MRE-CNN) framework. After-
wards, based on the weighted sum operation of three prediction scores from each sub-
network, we get a final accurate prediction.
Particularly, to encourage intra-class compactness and inter-class separability, each
subnet adopts the softmax loss function which is given by
𝐿𝑜𝑠𝑠 𝜃 =− ’ ( / 𝑙 𝑦(,) =𝑗 log
56789(:)
, (1)
( ,1’ 01’ < 56;89(:)
;=>
where 𝑥(,) denotes the features of the i-th sample, taken from the final hidden layer
before the softmax layer, 𝑚 is the number of training data, and 𝑘 is the number of
classes. We define the i-th input feature 𝑥(,) ∈ℛD with the predicted label 𝑦(,). 𝜃 is
the parameter matrix of the softmax function 𝐿𝑜𝑠𝑠 𝜃 . Here 𝑙 ∙ means
𝑙 𝑎 𝑡𝑟𝑢𝑒 𝑠𝑡𝑎𝑡𝑒𝑚𝑒𝑛𝑡 =1 or 𝑙 𝑎 𝑓𝑎𝑙𝑠𝑒 𝑠𝑡𝑎𝑡𝑒𝑚𝑒𝑛𝑡 =0.
Data Augmentation Despite the training size of RAF-DB, it is still insufficient for
training a designed deep network. Therefore, we utilize both offline data augmentation
and on-the-fly data augmentation techniques. The number of training samples increases
fifteen-fold after introducing methods including image rotation, image flips and Gauss-
ian distribution random perturbations. Besides, on-the-fly data augmentation is embed-
ded in the deep learning framework, Caffe [4], by randomly cropping the input images
and then flipping them horizontally.
3.3 The Sub-networks in MRE-CNN Framework
input conv1
pool1 conv2
pool2 conv3
pool3 conv4 pool4 conv5 pool5
fc6 fc7
224*224 112*112 56*56 28*28 14*14 7*7 fc8 softmax
input conv1 pool1 conv2 concat
pool2 conv3
pool3 conv4 pool4 conv5 pool5 7
224*224 112*112 56*56 28*28 14*14 7*7 40964096
Fig. 4. The VGG-16 sub-network architecture in MRE-CNN framework.
As Figure 4 shows, we adopt 13 convolutional layers and 5 max pooling layers and
concatenate the outputs from two pool5 layers before going through the first fully con-
nected layer. The final softmax layer gives the prediction scores. When employing
VGG-16 [11], we fine-tune the pre-trained model with the training set of AFEW 7.0
and RAF-DB, respectively, in the following experiments.6
To validate the proposed MRE-CNN framework, our modified AlexNet architecture
do not use any pre-trained models during its training process. For AlexNet sub-network,
we use 5 convolutional layers and 3 max pooling layers, the same as in the traditional
CNN architecture. Different from the original AlexNet, the last two fully connected
layers have 64 outputs and 7 outputs, respectively, making it possible to retrain a deep
network with limited data. The following experiment results indicate its effectiveness
in the MRE-CNN framework structure, despite its simplified network architecture.
Finally, we combine the three predictions from three sub-networks by conducting
the weighted sum operation. The predicted emotion 𝑃 is defined as
QRSTUVV
𝑒Z>8[:
𝑃 QRSTUVV = Y X1’𝛼 X ( ,1’
<
;=>5’ 6;89(:) 𝑒Z]…
8[(:)
, (2)
where 𝛼 denotes the weight for a single sub-network and 𝑧 is equal to 3 as we utilize
three sub-networks. Other parameters are the same as those in Equation 1.
4 Experiments
4.1 Experimental Setup
All training and testing processes were performed on NVIDIA GeForce GTX 1080Ti
11G GPUs. We developed our models in the deep learning framework Caffe [4]. On
the Ubuntu linux system equipped with NVIDIA GPUs, training a single model in
MRE-CNN took 4-6 hours depending on the architecture of the sub-network.
4.2 Implementation Details
In data augmentation stage, we augment the set of training images in RAF-DB and
frames in AFEW 7.0 by flipping, rotating each with ±4° and ±6°, and adding Gaussian
white noises with variances of 0.001, 0.01 and 0.015. We then train our VGG-16 sub-
networks for 20k iterations with the following parameters: learning rate 0.0001-0.0005,
weight decay 0.0001, momentum 0.9, batch size 16 and linear learning rate decay in
stochastic gradient descent (SGD) optimizer. For AlexNet sub-networks, we train them
for 30k iterations with the batch size of 64 and the learning rate begins from 0.001. In
the ensemble prediction stage, the specific weights of MRE-CNN (VGG-16 Sub-net-
work) are 4/7 (left-eye weight), 2/7 (mouth weight) and 1/7 (nose weight) and those of
MRE-CNN (AlexNet Sub-network) are 2/5 (left-eye weight), 2/5 (mouth weight) and
1/5 (nose weight), respectively.
4.3 Results on RAF-DB
RAF-DB is split into a training set and a test set with the idea of five-fold cross-valida-
tion and we performed the 7-class basic expression classification benchmark experi-
ment. In the RAF-DB test protocol, the ultimate metric is the mean diagonal value of7
the confusion matrix rather than the accuracy due to imbalanced distribution in expres-
sions. In this experiment, we directly train our deep learning models with our processed
training samples from RAF-DB, without using other databases. In details, after filtering
the non-detected face images and applying data augmentation techniques, 95465
cropped face images are generated, accompanied by left-eye images, mouth images and
nose images.
Table 1. Confusion matrix for RAF-DB based on MRE-CNN (VGG-16 Sub-network). The term
Real represents the true labels (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise,
6=Neutral) and Pred represents the predicted value.
Pred
0 1 2 3 4 5 6
Real
0 0.0088 0.0632 0.0000 0.0221 0.0706 0.0338 0.8015
1 0.0213 0.0182 0.0334 0.0030 0.0122 0.8602 0.0517
2 0.0209 0.0565 0.0084 0.0167 0.7992 0.0105 0.0879
3 0.0110 0.0211 0.0051 0.8878 0.0127 0.0110 0.0515
4 0.0811 0.0000 0.6081 0.0270 0.0676 0.1757 0.0405
5 0.1125 0.5750 0.0063 0.0083 0.0750 0.0187 0.1313
6 0.8395 0.0802 0.0185 0.0185 0.0123 0.0062 0.0247
Analyzing the confusion matrix based on MRE-CNN (VGG-16 Sub-network) in
Table 1, our proposed model performs well when classifying happy, surprise and angry
emotions, with accuracy of 88.78%, 86.02%, 83.95%, respectively. For comparison, in
Table 2 we show the results of the trained DCNN models followed by different classi-
fiers which are proposed in [6]. We find that our proposed MRE-CNN (VGG-16)
framework outperforms all of the existing state-of-the-art methods evaluated on RAF-
DB. In addition, the MRE-CNN (AlexNet) framework also achieves a very appealing
performance although we retrain the AlexNet sub-networks with limited data.
Table 2. Performance of different methods on RAF-DB (The metric is the mean diagonal value
of the confusion matrix).
Angry Disgust Fear Happy Sad Surprise Neutral Ave.
DLP+SVM[6] 71.60 52.15 62.16 92.83 80.13 81.16 80.29 74.20
DLP+LDA[6] 77.51 55.41 52.50 90.21 73.64 74.07 73.53 70.98
Alex+SVM[6] 58.64 21.87 39.19 86.16 60.88 62.31 60.15 55.60
Alex+LDA[6] 43.83 27.50 37.84 75.78 39.33 61.70 48.53 47.79
VGG+SVM[6] 68.52 27.50 35.13 85.32 64.85 66.32 59.88 58.22
VGG+LDA[6] 66.05 25.00 37.84 73.08 51.46 53.49 47.21 50.59
VGG-FACE 82.19 56.62 55.41 86.38 79.52 83.93 71.18 73.60
Ours(AlexNet) 77.78 65.62 58.11 87.75 75.73 81.16 77.21 74.78
Ours(VGG-16) 83.95 57.50 60.81 88.78 79.92 86.02 80.15 76.738
Furthermore, we separated the sub-network modules from MRE-CNN framework
and demonstrated their individual results on the test set of RAF-DB. Results can be
viewed in Table 3. The result of the first row shows the average accuracy of Face+Left-
Eye while applying VGG-16 sub-network in MRE-CNN framework, and they are
higher than that of Face+Mouth. Thus we assign higher weights to Face+LeftEye sub-
net when combining the three predictions with an appropriate ensemble method.
Face+Nose subnet is slightly less effective, probably due to less information related to
emotions; Nevertheless, it is still superior to the VGG-FACE model given in Table 2
with only the whole face region as input.
Table 3. Sub-region Comparison(The metric is the mean diagonal value of the confusion matrix).
Architecture Average
Face+LeftEye (Single VGG-16 sub-network) 76.52
Face+Nose (Single VGG-16 sub-network) 75.64
Face+Mouth (Single VGG-16 sub-network) 76.13
Our MRE-CNN(VGG-16) 76.73
Table 4. Comparisons with the state-of-the-art methods on AFEW 7.0(The metric is the average
accuracy of all validation videos).
Architecture
Training data
Average
C3D [9] 16 frames for each video 35.20
ResNet-LSTM[9] 16 frames for each video 46.70
VGG-LSTM[9] 16 frames for each video 47.40
Trajectory+SVM[13] 30 frames for each video 37.37
VGG-BRNN[13] 40 frames for each video 44.46
C3D-LSTM[12] Detected face frames 43.20
Our MRE-CNN(AlexNet) Detected face frames 40.11
Our MRE-CNN(VGG-16) Detected face frames 47.43
4.4 Results on AFEW 7.0
To validate the performance of our models, we also conduct experiments on the vali-
dation set of AFEW 7.0. The task is to assign a single expression label from seven
candidate categories to each video clip from the validation set (383 video clips). Note
that all our CNN models in MRE-CNN framework are trained on the given training
data (773 video clips) only without applying any outside data. Considering the tempo-
rally disappearance or occlusion in some videos, we only use detected face frames for
training and prediction. In our experiments, the predicted emotion scores of each video
are calculated by averaging the scores of all its detected face frames. We can see from9
Table 4, for the validation set of AFEW 7.0, our MRE-CNN (VGG-16) framework gets
great results which are superior to some state-of-the-art methods.
4.5 Discussions
A series of feature maps are shown in Figure. 5 for VGG-16 sub-network in our MRE-
CNN framework, which can reflect the differences in the filters of the first three con-
volutional layers. It can be observed that shallower layer outputs capture more profile
information while deeper layer outputs encode the semantic information. Shallower
layers can learn rich low-level features that can help refine the irregular features from
deeper layers. Furthermore, by combining features from the whole region and sub-re-
gions of the human face, the resulting architecture provides more rich feature maps,
which raises the recognition rate for FER problems.
Fig. 5. Visualization of the feature maps of the first three convolutional layers for the input image
on the left of each row.
Generally, our method explicitly inherits the advantage of information gathered
from multiple local regions from face images, acting as a deep feature ensemble with
two single CNN architectures, and hence it naturally improves the final predication
accuracy. The disadvantage of our approach is that we use grid searching to determine
the contribution portions of individual sub-networks, which is relatively computation-
ally expensive. Although facial expression recognition based on face images can
achieve promising results, facial expression is only one modality in realistic human
behaviors. Combining facial expressions with other modalities, such as audio infor-
mation, physiological data and thermal infrared images can provide complementary in-
formation, further enhancing the robustness of our models. Therefore, it is a promising
research direction to incorporate facial expression models with other dimension models
into a high-level framework.
5 Conclusion
We proposed a novel Multi-Region Ensemble CNN framework in this study, which
takes full advantage of the different regions of the whole human face. By assigning
different weights to three sub-networks in MRE-CNN, we have combined the predic-
tions of three separate networks. Besides, we have investigated the effects of three dif-
ferent facial regions, each providing different local information. As a result, our MRE-10
CNN framework has achieved a very appealing performance on RAF-DB and AFEW
7.0, as compared to other state-of-the-art methods.
Acknowledgements. This research is supported in part by the Theme-based Research
Scheme of the Research Grants Council of Hong Kong, under Grant No. T41-709/17-
N.
References
1. Dhall,A.,Goecke,R.,Lucey,S.,Gedeon,T.,etal.:Collectinglarge,richlyannotatedfacial- ex-
pression databases from movies. IEEE multimedia 19(3), 34–41 (2012)
2. Ekman, P., Friesen, W.V.: Constants across cultures in the face and emotion. Journal of per-
sonality and social psychology 17(2), 124 (1971)
3. Hu, P., Cai, D., Wang, S., Yao, A., Chen, Y.: Learning supervised scoring ensemble for
emotion recognition in the wild. In: Proceedings of the 19th ACM International Conference
on Multimodal Interaction. pp. 553–560. ACM (2017)
4. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S.,
Darrell, T.: Caffe: Convolutional architecture for fast feature embedding. In: Proceedings of
the 22nd ACM international conference on Multimedia. pp. 675–678. ACM (2014)
5. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional
neural networks. In: Advances in neural information processing systems. pp. 1097–1105
(2012)
6. Li, S., Deng, W., Du, J.: Reliable crowdsourcing and deep locality-preserving learning for
expression recognition in the wild. In: Computer Vision and Pattern Recognition (CVPR),
2017 IEEE Conference on. pp. 2584–2593. IEEE (2017)
7. Mollahosseini, A., Chan, D., Mahoor, M.H.: Going deeper in facial expression recognition
using deep neural networks. In: Applications of Computer Vision (WACV), 2016 IEEE
Win- ter Conference on. pp. 1–10. IEEE (2016)
8. Ng, H.W., Nguyen, V.D., Vonikakis, V., Winkler, S.: Deep learning for emotion recognition
on small datasets using transfer learning. In: Proceedings of the 2015 ACM on international
conference on multimodal interaction. pp. 443–449. ACM (2015)
9. Ouyang, X., Kawaai, S., Goh, E.G.H., Shen, S., Ding, W., Ming, H., Huang, D.Y.: Audio-
visual emotion recognition using deep transfer learning and multiple temporal models. In:
Proceedings of the 19th ACM International Conference on Multimodal Interaction. pp. 577–
582. ACM (2017)
10. Russakovsky,O.,Deng,J.,Su,H.,Krause,J.,Satheesh,S.,Ma,S.,Huang,Z.,Karpathy,A., Kho-
sla, A., Bernstein, M., et al.: Imagenet large scale visual recognition challenge. Interna-
tional Journal of Computer Vision 115(3), 211–252 (2015)
11. Simonyan,K.,Zisserman,A.:Verydeepconvolutionalnetworksforlarge-scaleimagerecog- ni-
tion. arXiv preprint arXiv:1409.1556 (2014)
12. Vielzeuf, V., Pateux, S., Jurie, F.: Temporal multimodal fusion for video emotion classifica-
tion in the wild. In: Proceedings of the 19th ACM International Conference on Multimodal
Interaction. pp. 569–576. ACM (2017)
13. Yan, J., Zheng, W., Cui, Z., Tang, C., Zhang, T., Zong, Y.: Multi-cue fusion for emotion
recognition in the wild. Neurocomputing (2018)"
179,181,Multi-task convolutional neural network for pose-invariant face recognition,"['X Yin', 'X Liu']",2017,381,CMU Multi-PIE,"CNN, classification, neural network",classification error is more likely to happen compared to controlled datasets with discrete pose  angles. This work utilizes all data in the Multi-PIE  of variations on Multi-PIE. We also apply,No DOI,IEEE Transactions on Image Processing,https://ieeexplore.ieee.org/document/8080244,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
180,182,Multi-task pose-invariant face recognition,"['C Ding', 'C Xu', 'D Tao']",2015,303,CMU Multi-PIE,"classification, deep learning, machine learning, neural network","on FERET, CMU-PIE, and Multi-PIE databases shows that the  Multi-task learning (MTL) is  a machine learning technique  protocol for the pose problem on Multi-PIE, we adopt the three",No DOI,IEEE Transactions on image Processing,https://ieeexplore.ieee.org/document/7006757,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
181,183,Multi-view facial expression recognition,"['Y Hu', 'Z Zeng', 'L Yin', 'X Wei', 'X Zhou']",2008,167,Binghamton University 3D Facial Expression,facial expression recognition,"faculties from State University of New York at Binghamton. The  expressions, captured by a  3D face scanner. With the  in the multi-view emotion recognition experiment. The best average",No DOI,… & Gesture Recognition,https://ieeexplore.ieee.org/document/4813445,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
182,184,Multimodal 2D+ 3D facial expression recognition with deep fusion convolutional neural network,"['H Li', 'J Sun', 'Z Xu', 'L Chen']",2017,241,"Binghamton University 3D Facial Expression, Toronto Face Database","CNN, FER, classification, classifier, deep learning, facial expression recognition, machine learning, neural network",has never been used to learn 3D facial representations in 3D FER. This motivates us to fill   Expression) Database [59] has been the benchmarking for static 3D FER [12]. It includes 100,No DOI,IEEE Transactions on Multimedia,https://ieeexplore.ieee.org/document/7944639,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
183,185,Multimodal learning for facial expression recognition,"['W Zhang', 'Y Zhang', 'L Ma', 'J Guan', 'S Gong']",2015,133,Toronto Face Database,facial expression recognition,"+ database as an example, C is defined as two to distinguish whether the inputs is the latent  facial expression we aim to recognize.  CK+ database, we find that the expression process is",No DOI,Pattern Recognition,https://www.sciencedirect.com/science/article/pii/S003132031500151X,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
184,186,Occlusion aware facial expression recognition using CNN with attention mechanism,"['Y Li', 'J Zeng', 'S Shan', 'X Chen']",2018,852,"Acted Facial Expressions In The Wild, Affective Faces Database, Expression in-the-Wild, Static Facial Expression in the Wild",CNN,"that are common in the wild. In this paper, we propose a convolution neutral network (CNN)  with attention mechanism (ACNN) that can perceive the occlusion regions of the face and",No DOI,IEEE Transactions on Image …,https://ieeexplore.ieee.org/document/8576656,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
185,187,On solving the face recognition problem with one training sample per subject,"['J Wang', 'KN Plataniotis', 'J Lu', 'AN Venetsanopoulos']",2006,132,Toronto Face Database,"classification, classifier",to be recognized using a generic training database which consists of images from subjects  other than those under consideration. Many state-of-the-art face recognition solutions can be,No DOI,Pattern recognition,https://www.sciencedirect.com/science/article/pii/S0031320306001233,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
186,188,Patch-gated CNN for occlusion-aware facial expression recognition,"['Y Li', 'J Zeng', 'S Shan', 'X Chen']",2018,177,Expression in-the-Wild,CNN,"Then, via a proposed Patch-Gated Unit, PG-CNN reweighs each patch by the unobstructed-  The proposed PG-CNN is evaluated on two largest in-the-wild facial expression datasets (",No DOI,2018 24th international …,https://ieeexplore.ieee.org/document/8545853/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
187,189,Peak-piloted deep network for facial expression recognition,"['X Zhao', 'X Liang', 'L Liu', 'T Li', 'Y Han']",2016,369,CMU Multi-PIE,FER,"Multi-PIE  FER datasets available, we pre-trained GoogLeNet [24] on a large-scale face  recognition dataset, the CASIA Webface dataset [32]. This network was then fine-tuned for FER.",No DOI,Computer Vision–ECCV …,https://arxiv.org/abs/1607.06997,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"Peak-Piloted Deep Network for Facial Expression
Recognition
XiangyunZhao1 XiaodanLiang2 LuoqiLiu3,4 TengLi5
YugangHan3 NunoVasconcelos1 ShuichengYan3,4
1UniversityofCalifornia,SanDiego 2CarnegieMellonUniversity
3360AIInstitute 4NationalUniversityofSingapore
5InstituteofAutomation,ChineseAcademyofSciences
xiz019@ucsd.edu xdliang328@gmail.com liuluoqi@360.cn
tenglwy@gmail.com hanyugang@360.cn
nvasconcelos@ucsd.edu eleyans@nus.edu.sg
Abstract. Objectivefunctionsfortrainingofdeepnetworksforface-relatedrecog-
nitiontasks,suchasfacialexpressionrecognition(FER),usuallyconsidereach
sampleindependently.Inthiswork,wepresentanovelpeak-piloteddeepnetwork
(PPDN)thatusesasamplewithpeakexpression(easysample)tosupervisethe
intermediatefeatureresponsesforasampleofnon-peakexpression(hardsam-
ple)ofthesametypeandfromthesamesubject.Theexpressionevolvingpro-
cessfromnon-peakexpressiontopeakexpressioncanthusbeimplicitlyembed-
dedinthenetworktoachievetheinvariancetoexpressionintensities.Aspecial-
purpose back-propagation procedure, peak gradient suppression (PGS), is pro-
posedfornetworktraining.Itdrivestheintermediate-layerfeatureresponsesof
non-peak expression samples towards those of the corresponding peak expres-
sionsamples,whileavoidingtheinverse.Thisavoidsdegradingtherecognition
capabilityforsamplesofpeakexpressionduetointerferencefromtheirnon-peak
expression counterparts. Extensive comparisons on two popular FER datasets,
Oulu-CASIAandCK+,demonstratethesuperiorityofthePPDNoverstate-of-
the-art FER methods, as well as the advantages of both the network structure
andtheoptimizationstrategy.Moreover,itisshownthatPPDNisageneralar-
chitecture, extensible to other tasks by proper definition of peak and non-peak
samples.Thisisvalidatedbyexperimentsthatshowstate-of-the-artperformance
onpose-invariantfacerecognition,usingtheMulti-PIEdataset.
Keywords: FacialExpressionRecognition,Peak-Piloted,DeepNetwork,Peak
GradientSuppression
1 Introduction
FacialExpressionRecognition(FER)aimstopredictthebasicfacialexpressions(e.g.
happy, sad, surprise, angry, fear, disgust) from a human face image, as illustrated in
Fig.1.1 Recently,FERhasattractedmuchresearchattention[1,2,3,4,5,6,7].Itcanfa-
cilitateotherface-relatedtasks,suchasfacerecognition[8]andalignment[9].Despite
1ThisworkwasperformedwhenXiaoyunZhaowasaninternat360AIInstitute.
7102
naJ
3
]VC.sc[
2v79960.7061:viXra2 X.Zhao,X.Liang,L.Liu,T.Li,Y.Han,N.Vasconcelos,S.Yan
significantrecentprogress[10,11,4,12],FERisstillachallengingproblem,duetothe
followingdifficulties.First,asillustratedinFig.1,differentsubjectsoftendsiplaythe
same expression with diverse intensities and visual appearances. In a videostream, an
expressionwillfirstappearinasubtleformandthengrowintoastrongdisplayofthe
underlying feelings. We refer to the former as a non-peak and to the latter as a peak
expression.Second,peakandnon-peakexpressionsbythesamesubjectcanhavesig-
nificant variation in terms of attributes such as mouth corner radian, facial wrinkles,
etc.Third,non-peakexpressionsaremorecommonlydisplayedthanpeakexpressions.
Itisusuallydifficulttocapturecriticalandsubtleexpressiondetailsfromnon-peakex-
pressionimages,whichcanbehardtodistinguishacrossexpressions.Forexample,the
non-peakexpressionsforfearandsadnessarequitesimilarinFig.1.
Surprise Angry Happy Fear Sad Disgust
Peak
expression
Non-peak
expression
Fig.1.Examplesofsixfacialexpressionsamples,includingsurprise,angry,happy,fear,sadand
disgust.Foreachsubject,thepeakandnon-peakexpressionsareshown.
Recently, deep neural network architectures have shown excellent performance in
face-related recognition tasks [13,14,15]. The has led to the introduction of FER net-
work architectures [4,16]. There are, nevertheless, some important limitations. First,
mostmethodsconsidereachsampleindependentlyduringlearning,ignoringtheintrin-
siccorrelationsbetweeneachpairofsamples(e.g.,easyandhardsamples).Thislimits
thediscriminativecapabilitiesofthelearnedmodels.Second,theyfocusonrecognizing
theclearlyseparablepeakexpressionsandignorethemostcommonnon-peakexpres-
sionsamples,whosediscriminationcanbeextremelychallenging.
Inthispaper,weproposeanovelpeak-piloteddeepnetwork(PPDN)architecture,
whichimplicitlyembedsthenaturalevolutionofexpressionsfromnon-peaktopeakex-
pressioninthelearningprocess,soastozoominonthesubtledifferencesbetweenweak
expressionsandachieveinvariancetoexpressionintensity.Intuitively,asillustratedin
Fig.2,peakandnon-peakexpressionsfromthesamesubjectoftenexhibitverystrong
visual correlations (e.g., similar face parts) and can mutually help the recognition of
eachother.TheproposedPPDNusesthefeatureresponsestosamplesofpeakexpres-
sion(easysamples)tosupervisetheresponsestosamplesofnon-peakexpression(hard
samples) of the same type and from the same subject. The resulting mapping of non-
peakexpressionsintotheircorrespondingpeakexpressionsmagnifiestheircriticaland
subtledetails,facilitatingtheirrecognition.Peak-PilotedDeepNetworkforFacialExpressionRecognition 3
Surprise
Happy
Fig.2.Expressionevolvingprocessfromnon-peakexpressiontopeakexpression.
In principle, an explicit mapping from non-peak to peak expression could signifi-
cantlyimproverecognition.However,suchamappingischallengingtogenerate,since
the detailed changes of face features (e.g., mouth corner radian and wrinkles) can be
quite difficult to predict. We avoid this problem by focusing on the high-level feature
representationofthefacialexpressions,whichisbothmoreabstractanddirectlyrelated
tofacialexpressionrecognition.Inparticular,theproposedPPDNoptimizesthetasks
of 1) feature transformation from non-peak to peak expression and 2) recognition of
facial expressions in a unified manner. It is, in fact, a general approach, applicable to
many other recognition tasks (e.g. face recognition) by proper definition of peak and
non-peak samples (e.g. frontal and profile faces). By implicitly learning the evolution
from hard poses (e.g., profile faces) to easy poses (e.g., near-frontal faces), it can im-
provetherecognitionaccuracyofpriorsolutionstotheseproblems,makingthemmore
robusttoposevariation.
Duringtraining,thePPDNtakesanimagepairwithapeakandanon-peakexpres-
sion of the same type and from the same subject. This image pair is passed through
several intermediate layers to generate feature maps for each expression image. The
L2-norm of the difference between the feature maps of non-peak and peak expres-
sion images is then minimized, to embed the evolution of expressions into the PPDN
framework.Inthisway,thePPDNincorporatesthepeak-pilotedfeaturetransformation
andfacialexpressionrecognitionintoaunifiedarchitecture.ThePPDNislearnedwith
a new back-propagation algorithm, denotes peak gradient suppression (PGS), which
drivesthefeatureresponsestonon-peakexpressioninstancestowardsthoseofthecor-
respondingpeakexpressionimages,butnotthecontrary.Thisisunlikethetraditional
optimizationofSiamesenetworks[13],whichencouragesthefeaturepairstobeclose
to each other, treating the feature maps of the two images equally. Instead, the PPDN
focusesontransformingthefeaturesofnon-peakexpressionstowardsthoseofpeakex-
pressions.Thisisimplementedby,duringeachback-propagationiteration,ignoringthe
gradientinformationduetothepeakexpressionimageintheL2-normminimizationof
feature differences, while keeping that due to the non-peak expression. The gradients
of the recognition loss, for both peak and non-peak expression images, are the same
asintraditionalback-propagation.Thisavoidsthedegradationoftherecognitioncapa-
bility of the network for samples of peak expression due to the influence of non-peak
expressionsamples.4 X.Zhao,X.Liang,L.Liu,T.Li,Y.Han,N.Vasconcelos,S.Yan
Overall, this work has four main contributions. 1) The PPDN architecture is pro-
posed, using the responses to samples of peak expression (easy samples) to supervise
theresponsestosamplesofnon-peakexpression(hardsamples)ofthesametypeand
from the same subject. The targets of peak-piloted feature transformation and facial
expression recognition, for peak and non-peak expressions, are optimized simultane-
ously. 2) A tailored back-propagation procedure, PGS, is proposed to drive the re-
sponsestonon-peakexpressionstowardsthoseofthecorrespondingpeakexpressions,
whileavoidingtheinverse.3)ThePPDNisshowntoperformintensity-invariantfacial
expressionrecognition,byeffectivelyrecognizingthemostcommonnon-peakexpres-
sions. 4) Comprehensive evaluations on several FER datasets, namely CK+ [17] and
Oulu-CASIA[18],demonstratethesuperiorityofthePPDNoverpreviousmethods.Its
generalization to other tasks is also demonstrated through state-of-the-art robust face
recognitionperformanceonthepublicMulti-PIEdataset[19].
2 RelatedWork
Therehavebeenseveralrecentattemptstosolvethefacialexpressionrecognitionprob-
lem.Thesemethodscanbegroupedintotwocategories:sequence-basedandstillimage
approaches.Inthefirstcategory,sequence-basedapproaches[7,1,20,18,21]exploitboth
theappearanceandmotioninformationfromvideosequences.Inthesecondcategory,
still image approaches [10,4,12] recognize expressions uniquely from image appear-
ancepatterns.Sincestillimagemethodsaremoregeneric,recognizingexpressionsin
bothstillimagesandsequences,wefocusonmodelsforstillimageexpressionrecog-
nition.Amongthese,bothhand-craftedpipelinesanddeeplearningmethodshavebeen
explored for FER. Hand-crafted approaches [10,22,11] perform three steps sequen-
tially: feature extraction, feature selection and classification. This can lead to subop-
timalrecognition,duetothecombinationofdifferentoptimizationtargets.
ConvolutionalNeuralNetwork(CNN)architectures[23,24,25]haverecentlyshown
excellentperformanceonface-relatedrecognitiontasks[26,27,28].Methodsthatresort
to the CNN architecture have also been proposed for FER. For example, Yu et al. [5]
usedanensembleofmultipledeepCNNs.Mollahosseinietal.[16]usedthreeinception
structures[24]inconvolutionforFER.Allthesemethodstreatexpressioninstancesof
differentintensitiesofthesamesubjectindependently.Hence,thecorrelationsbetween
peak and non-peak expressions are overlooked during learning. In contrast, the pro-
posedPPDNlearnstoembedtheevolutionfromnon-peaktopeakexpressions,soasto
facilitateimage-basedFER.
3 ThePeak-PilotedDeepNetwork(PPDN)
InthisworkweintroducethePPDNframework,whichimplicitlylearnstheevolution
fromnon-peaktopeakexpressions,intheFERcontext.AsillustratedinFig.3,during
training the PPDN takes an image pair as input. This consists of a peak and a non-
peakexpressionofthesametypeandfromthesamesubject.Thisimagepairispassed
through several convolutional and fully-connected layers, generating pairs of featurePeak-PilotedDeepNetworkforFacialExpressionRecognition 5
ExpP re ea sk
si on
𝐺1 𝐹1
esirpruS yppaH tsugsiD raeF daS yrgnA
Cross-entropy
1||𝐺 1||𝐹
𝑥 𝑥
𝑊1 − 𝑊2 −
EN xo pn re-P sse ia ok
n
2𝐺
(𝑥
)||
2
2𝐹
(𝑥
)||
2
Cross-entropy
C Ao rcn hv io telu ctt uio rn eal 𝐺2 𝐹2
esirpruS yppaH tsugsiD raeF daS yrgnA
Fig.3.IllustrationofthetrainingstageofPPDN.Duringtraining,PPDNtakesthepairofpeak
andnon-peakexpressionimagesasinput.Afterpassingthepairthroughseveralconvolutional
and fully-connected layers, the intermediate feature maps can be obtained for peak and non-
peakexpressionimages,respectively.TheL2-normlossbetweenthesefeaturemapsisoptimized
fordrivingthefeaturesofthenon-peakexpressionimagetowardsthoseofthepeakexpression
image. The network parameters can thus be updated by jointly optimizing the L2-norm losses
andthelossesofrecognizingtwoexpressionimages.Duringtheback-propagationprocess,the
PeakGradientSuppression(PGS)isutilized.
mapsforeachexpressionimage.Todrivethefeatureresponsestothenon-peakexpres-
sionimagetowardsthoseofthepeakexpressionimage,theL2-normofthefeaturedif-
ferencesisminimized.ThelearningalgorithmoptimizesacombinationofthisL2-norm
lossandtworecognitionlosses,oneperexpressionimage.Duetoitsexcellentperfor-
manceonseveralface-relatedrecognitiontasks[29,30],thepopularGoogLeNet[24]is
adoptedasthebasicnetworkarchitecture.Theincarnationsoftheinceptionarchitecture
inGoogLeNetarerestrictedtofilterssizes1×1,3×3and5×5.Intotal,theGoogLeNet
implementsnineinceptionstructuresaftertwoconvolutionallayersandtwomaxpool-
inglayers.Afterthat,thefirstfully-connectedlayerproducestheintermediatefeatures
with1024dimensions,andthesecondfully-connectedlayergeneratesthelabelpredic-
tionsforsixexpressionlabels.Duringtesting,thePPDNtakesonestillimageasinput,
outputtingthepredictedprobabilitiesforallsixexpressionlabels.
3.1 NetworkOptimization
The goal of the PPDN is to learn the evolution from non-peak to peak expressions,
as well as recognize the basic facial expressions. We denote the training set as S =
{xp,xn,yp,yn,i = 1,...,N},wheresamplexn denotesafacewithnon-peakexpres-
i i i i i
sion, xp a face with the corresponding peak expression, and yn and yp are the corre-
i i i6 X.Zhao,X.Liang,L.Liu,T.Li,Y.Han,N.Vasconcelos,S.Yan
spondingexpressionlabels.Tosupervisethefeatureresponsestothenon-peakexpres-
sioninstancewiththoseofthepeakexpressioninstance,thenetworkislearnedwitha
lossfunctionthatincludestheL2-normofthedifferencebetweenthefeatureresponses
topeakandnon-peakexpressioninstances.Cross-entropylossesarealsousedtoopti-
mizetherecognitionofthetwoexpressionimages.Overall,thelossofthePPDNis
N
1 (cid:88)
J = (J +J +J +λ ||W||2)
N 1 2 3
i=1
N N
1 (cid:88)(cid:88) 1 (cid:88)
= (cid:107)f (xp,W)−f (xn,W)(cid:107)2+ L(yp,f(xp;W)) (1)
N j i j i N i i
i=1j∈Ω i=1
N
1 (cid:88)
+ L(yn,f(xn;W))+λ||W||2,
N i i
i=1
whereJ ,J andJ indicatetheL2-normofthefeaturedifferencesandthetwocross-
1 2 3
entropylossesforrecognition,respectively.Notethatthepeak-pilotedfeaturetransfor-
mationisquitegenericandcouldbeappliedtothefeaturesproducedbyanylayers.We
denoteΩasthesetoflayersthatemploythepeak-pilotedtransformation,andf ,j ∈Ω
j
asthefeaturemapsinthej-thlayer.Toreducetheeffectscausedbyscalevariabilityof
thetrainingdata,thefeaturesf areL2normalizedbeforetheL2-normofthedifference
j
is computed. More specifically, the feature maps f are concatenated into one vector,
j
whichisL2normalized.Inthesecondandthirdterms,Lrepresentsthecross-entropy
loss between the ground-truth labels and the predicted probabilities of all labels. The
finalregularizationtermisusedtopenalizethecomplexityofnetworkparametersW.
Since the evolution from non-peak to peak expression is embedded into the network,
thelatterlearnsamorerobustexpressionrecognizer.
3.2 PeakGradientSuppression(PGS)
To train the PPDN, we propose a special-purpose back-propagation algorithm for the
optimizationof(1).Ratherthanthetraditionalstraightforwardapplicationofstochas-
tic gradient descent [13] [29], the goal is to drive the intermediate-layer responses of
non-peakexpressioninstancestowardsthoseofthecorrespondingpeakexpressionin-
stances,whileavoidingthereverse.Undertraditionalstochasticgradientdecent(SGD)[31],
thenetworkparameterswouldbeupdatedwith
W+ =W −γ∇ J(W;xp,xp,yn,yp)
W i i i i
γ ∂J (W;xn,xp) ∂f (W;xn) γ ∂J (W;xn,xp) ∂f (W;xp)
=W − 1 i i × j i − 1 i i × j i
N ∂f (W;xn) ∂W N ∂f (W;xp) ∂W
j i j i
1 1
− γ∇ J (W;xp,yp)− γ∇ J (W;xn,yn)−2γW,
N W 2 i i N W 3 i i
(2)Peak-PilotedDeepNetworkforFacialExpressionRecognition 7
where γ is the learning rate. The proposed peak gradient suppression (PGS) learning
algorithmusesinsteadtheupdates
γ ∂J (W;xn,xp) ∂f (W;xn)
W+ =W − 1 i i × j i
N ∂f (W;xn) ∂W
j i (3)
1 1
− γ∇ J (W;xp,yp)− γ∇ J (W;xn,yn)−2γW.
N W 2 i i N W 3 i i
Thedifferencebetween(3)and(2)isthatthegradientsduetothefeatureresponsesof
thepeakexpressionimage,−γ ∂J1(W;xn i,xp i)×∂fj(W;xp i) aresuppressedin(3).Inthis
N ∂fj(W;xp i) ∂W
way,PGSdrivesthefeatureresponsesofnon-peakexpressionstowardsthoseofpeak
expressions, but not the contrary. In the appendix, we show that this does not prevent
learning,sincetheweightupdatedirectionofPGSisadescentdirectionoftheoverall
loss,althoughnotasteepestdescentdirection.
4 Experiments
ToevaluatethePPDN,weconductextensiveexperimentsontwopopularFERdatasets:
CK+[17]andOulu-CASIA[18].TofurtherdemonstratethatthePPDNgeneralizesto
otherrecognition tasks,wealso evaluateits performanceonface recognitionoverthe
publicMulti-PIEdataset[19].
4.1 FacialExpressionRecognition
Training. ThePPDNusestheGoogLeNet[24]asbasicnetworkstructure.Thepeak-
pilotedfeaturetransformationisonlyemployedinthelasttwofully-connectedlayers.
Other configurations, using the peak-piloted feature transformation on various convo-
lutional layers are also reported. Since it is not feasible to train the deep network on
thesmallFERdatasetsavailable,wepre-trainedGoogLeNet[24]onalarge-scaleface
recognitiondataset,theCASIAWebfacedataset[32].Thisnetworkwasthenfine-tuned
for FER. The CASIA Webface dataset contains 494,414 training images from 10,575
subjects,whichwereusedtopre-trainthenetworkfor60epochswithaninitiallearning
rateof0.01.Forfine-tuning,thefaceregionwasfirstalignedwiththedetectedeyesand
mouthpositions.Thefaceregionswerethenresizedto128×128.ThePPDNtakesapair
ofpeakandnon-peakexpressionimagesasinput.Theconvolutionallayerweightswere
initializedwiththoseofthepre-trainedmodel.Theweightsofthefullyconnectedlayer
were initialized randomly using the “xaiver” procedure [33]. The learning rate of the
fullyconnectedlayerswassetto0.0001andthatofpre-trainedconvolutionallayersto
0.000001.ALLmodelsweretrainedusingabatchsizeof128imagepairsandaweight
decayof0.0002.Thefinaltrainedmodelwasobtainedafter20,000iterations.Forfair
comparisonwithpreviousmethods[10,11,4],wedidnotuseanydataaugmentationin
ourexperiments.
Testing and Evaluation Metric. In the testing phase, the PPDN takes one testing
image as the input and produces its predicted facial expression label. Following the
standard setting of [10,11], 10-fold subject-independent cross-validation was adopted
forevaluationinallexperiments.8 X.Zhao,X.Liang,L.Liu,T.Li,Y.Han,N.Vasconcelos,S.Yan
Table1.Performancecomparisonsonsixfacialexpressionswithfourstate-of-the-artmethods
and the baseline using GoogLeNet in terms of average classification accuracy by the 10-fold
cross-validationevaluationonCK+database.
Method AverageAccuracy
CSPL[10] 89.9%
AdaGabor[34] 93.3%
LBPSVM[11] 95.1%
BDBN[4] 96.7%
GoogLeNet(baseline) 95.0%
PPDN 97.3%
Table2.PerformancecomparisonsonsixfacialexpressionswithUDCSmethodandthebaseline
usingGoogLeNetintermsofaverageclassificationaccuracyundersamesettingasUDCS.
Method AverageAccuracy
UDCS[35] 49.5%
GoogLeNet(baseline) 66.6%
PPDN 72.4%
Datasets. FER datasets usually provide video sequences for training and testing the
facial expression recognizers. We conducted all experiments on two popular datasets,
CK+ [17] and Oulu-CASIA dataset [18]. For each sequence, the face often gradually
evolves from a neutral to a peak facial expression. CK+ includes six basic facial ex-
pressions(angry,happy,surprise,sad,disgust,fear)andonenonbasicexpression(con-
tempt).Itcontains593sequencesfrom123subjects,ofwhichonly327areannotated
withexpressionlabels.Oulu-CASIAcontains480sequencesofsixfacialexpressions
undernormalillumination,including80subjectsbetween23and58yearsold.
Comparisons with Still Image-based Approaches. Table 1 compares the PPDN to
still image-based approaches on CK+, under the standard setting in which only the
lastonetothreeframes(i.e.,nearlypeakexpressions)persequenceareconsideredfor
trainingandtesting.Fourstate-of-the-artmethodsareconsidered:commonandspecific
patcheslearning(CSPL)[10],whichemploysmulti-tasklearningforfeatureselection,
AdaGabor [34] and LBPSVM [11], which are based on AdaBoost [36], and Boosted
DeepBeliefNetwork(BDBN)[4],whichjointlyoptimizesfeatureextractionandfea-
ture selection. In addition, we also compare the PPDN to the baseline “GoogLeNet
(baseline),”whichoptimizesthestandardGoogLeNetwithSGD.Similarlytoprevious
methods [10,11,4], the PPDN is evaluated on the last three frames of each sequence.
Table2comparesthePPDNwithUDCS[35]onOulu-CASIA,underasimilarsetting
wherethefirst9imagesofeachsequenceareignored,thefirst40individualsaretaken
as training samples and the rest as testing. In all cases, the PPDN input is the pair of
one of the non-peak frames (all frames other than the last one) and the correspond-
ingpeakframe(thelastframe)inasequence.ThePPDNsignificantlyoutperformsallPeak-PilotedDeepNetworkforFacialExpressionRecognition 9
Table3.PerformancecomparisononCK+databaseintermsofaverageclassificationaccuracy
ofthe10-foldcross-validationwhenevaluatingonthreedifferenttestsets,including“weakex-
pression”,“peakexpression”and“combined”,respectively.
Method weakexpressionpeakexpressioncombined
PPDN(standardSGD) 81.34% 99.12% 94.18%
GoogLeNet(baseline) 78.10% 98.96% 92.19%
PPDN 83.36% 99.30% 95.33%
other, achieving 97.3% vs a previous best of 96.7% on CK+ and 72.4% vs 66.6% on
Oulu-CASIA.Thisdemonstratesthesuperiorityofembeddingtheexpressionevolution
inthenetworklearning.
Training and Testing with More Non-peak Expressions. The main advantage of
the PPDN is its improved ability to recognize non-peak expressions. To test this, we
comparedhowperformancevarieswiththenumberofnon-peakexpressions.Notethat
foreachvideosequence,thefaceexpressionevolvesfromneutraltoapeakexpression.
The first six frames within a sequence are usually neutral, with the peak expression
appearing in the final frames. Empirically, we determined that the 7th to 9th frame
oftenshownon-peakexpressionswithveryweakintensities,whichwedenoteas“weak
expressions.”Inadditiontothetrainingimagesusedinthestandardsetting,weusedall
framesbeyondthe7thfortraining.
Sincethepreviousmethodsdidnotpublishtheircodes,weonlycomparethePPDN
tothebaseline“GoogLeNet(baseline)”.Table3reportsresultsforCK+andTable4for
Oulu-CASIA. Three different test sets were considered: “weak expression” indicates
that the test set only contains the non-peak expression images from the 7th to the 9th
frames;“peakexpression”onlyincludesthelastframe;and“combined”usesallframes
fromthe7thtothelast.“PPDN(standardSGD)”istheversionofPPDNtrainedwith
standardSGDoptimization,and“GoogLeNet(baseline)”thebasicGoogLeNet,taking
each expression image asinput and trained with SGD. The most substantialimprove-
mentsareobtainedonthe“weakexpression”testset,83.36%and67.95%of“PPDN”
vs.78.10%and64.64%of“GoogLeNet(baseline)”onCK+andOulu-CASIA,respec-
tively.Thisisevidenceinsupportoftheadvantageofexplicitlylearningtheevolution
fromnon-peaktopeakexpressions.Inaddition,thePPDNoutperforms“PPDN(stan-
dard SGD)” and “GoogLeNet (baseline)” on the combined sets, where both peak and
non-peakexpressionsareevaluated.
ComparisonswithSequence-basedApproaches. Unlikethestill-imagerecognition
setting,whichevaluatesthepredictionsofframesfromasequence,thesequence-based
settingrequiresapredictionforthewholesequence.Previoussequence-basedapproaches
take the whole sequence as input and use motion information during inference. In-
stead,thePPDNregardseachpairofnon-peakandpeakframeasinput,andonlyout-
puts the label of the peak frame as prediction for the whole sequence, in the testing
phase. Tables 5 and 6 compare the PPDN to several sequence-based approaches plus10 X.Zhao,X.Liang,L.Liu,T.Li,Y.Han,N.Vasconcelos,S.Yan
Table 4. Performance comparison on Oulu-CASIA database in terms of average classification
accuracy of the 10-fold cross-validation when evaluating on three different test sets, including
“weakexpression”,“peakexpression”and“combined”,respectively.
Method weakexpressionpeakexpressioncombined
PPDN(standardSGD) 67.05% 82.91% 73.54%
GoogLeNet(baseline) 64.64% 79.21% 71.32%
PPDN 67.95% 84.59% 74.99%
Table 5. Performance comparisons with three sequence-based approaches and the baseline
“GoogLeNet (baseline)” in terms of average classification accuracy of the 10-fold cross-
validationonCK+database.
Method ExperimentalSettings AverageAccuracy
3DCNN-DAP[37] sequence-based 92.4%
STM-ExpLet[1] sequence-based 94.2%
DTAGN(Joint)[7] sequence-based 97.3%
GoogLeNet(baseline) image-based 99.0%
PPDN(standardSGD) image-based 99.1%
PPDNw/opeak image-based 99.2%
PPDN image-based 99.3%
Table 6. Performance comparisons with five sequence-based approaches and the baseline
“GoogLeNet (baseline)” in terms of average classification accuracy of the 10-fold cross-
validationonOulu-CASIA.
Method ExperimentalSettings AverageAccuracy
HOG3D[21] sequence-based 70.63%
AdaLBP [18] sequence-based 73.54%
Atlases[20] sequence-based 75.52%
STM-ExpLet[1] sequence-based 74.59%
DTAGN(Joint)[7] sequence-based 81.46%
GoogLeNet(baseline) image-based 79.21%
PPDN(standardSGD) image-based 82.91%
PPDNw/opeak image-based 83.67%
PPDN image-based 84.59%
“GoogLeNet(baseline)” on CK+ and Oulu-CASIA. Compared with [1,37,7], which
leveragemotioninformation,thePPDN,whichonlyreliesonappearanceinformation,
achieves significantly better prediction performance. On CK+, it has gains of 5.1%
and2%over‘STM-ExpLet”[1]and“DTAGN(Joint)”[7].OnOulu-CASIAitachieves
84.59%vs.the75.52%of“Atlases”[20]andthe81.46%of“DTAGN(Joint)”[7].Inad-
dition,weevaluatethisexperimentwithoutpeakinformation,i.e.selectingimagewith
highestclassificationscoresforallcategoriesaspeakframeintesting.PPDNachieves
99.2%onCK+and83.67%onOulu-CASIA.Peak-PilotedDeepNetworkforFacialExpressionRecognition 11
Table7.Performancecomparisonsbyaddingthepeak-pilotedfeaturetransformationondifferent
convolutionallayerswhenevaluatedonOulu-CASIAdataset.
Method inceptionlayersthelastFClayerthefirstFClayerbothFClayers
Inception-3a ! # # #
Inception-3b ! # # #
Inception-4a ! # # #
Inception-4b ! # # #
Inception-4c ! # # #
Inception-4d ! # # #
Inception-4e ! # # #
Inception-5a ! # # #
Inception-5b ! # # #
Fc1 ! # ! !
Fc2 ! ! # !
AverageAccuracy 74.49% 73.33% 73.48% 74.99%
Table8.ComparisonsoftheversionwithandwithoutusingpeakinformationonOulu-CASIA
databaseintermsofaverageclassificationaccuracyofthe10-foldcross-validation.
Method weakexpression peakexpression combined
PPDNw/opeak 67.52% 83.79% 74.01%
PPDN 67.95% 84.59% 74.99%
Table9.Facerecognitionratesforvariousposesunder“setting1”.
Method −45◦ −30◦ −15◦ +15◦ +30◦ +45◦ Average
GoogLeNet(baseline) 86.57% 99.3% 100% 100% 100% 90.06% 95.99%
PPDN 93.96% 100% 100% 100% 100% 93.96% 97.98%
Table10.Facerecognitionratesforvariousposesunder“setting2”.
Method −45◦ −30◦ −15◦ +15◦ +30◦ +45◦ Average
Lietal.[38] 56.62% 77.22% 89.12% 88.81% 79.12% 58.14% 74.84%
Zhuetal.[27] 67.10% 74.60% 86.10% 83.30% 75.30% 61.80% 74.70%
CPI[28] 66.60% 78.00% 87.30% 85.50% 75.80% 62.30% 75.90%
CPF[28] 73.00% 81.70% 89.40% 89.50% 80.50% 70.30% 80.70%
GoogLeNet(baseline) 56.62% 77.22% 89.12% 88.81% 79.12% 58.14% 74.84%
PPDN 72.06% 85.41% 92.44% 91.38% 87.07% 70.97% 83.22%12 X.Zhao,X.Liang,L.Liu,T.Li,Y.Han,N.Vasconcelos,S.Yan
PGS vs. standard SGD. As discussed above, PGS suppresses gradients from peak
expressions, so as to drive the features of non-peak expression samples towards those
ofpeakexpressionsamples,butnotthecontrary.StandardSGDusesallgradients,due
tobothnon-peakandpeakexpressionsamples.Wehypothesizedthatthiswilldegrade
recognition for samples of peak expressions, due to interference from non-peak ex-
pression samples. This hypothesis is confirmed by the results of Tables 3 and 4. PGS
outperformsstandardSGDonallthreetestsets.
AblativeStudiesonPeak-PilotedFeatureTransformation. Thepeak-pilotedfeature
transformation,whichisthekeyinnovationofthePPDN,canbeusedonalllayersofthe
network.Employingthetransformationondifferentconvolutionalandfully-connected
layers can result in different levels of supervision of non-peak responses by peak re-
sponses.Forexample,earlyconvolutionallayersextractfine-graineddetails(e.g.,local
boundaries or illuminations) of faces, while later layers capture more semantic infor-
mation,e.g.,theappearancepattensofmouthsandeyes.Table7presentsanextensive
comparison,byaddingpeak-pilotedfeaturesupervisiononvariouslayers.Notethatwe
employGoogLeNet[24],whichincludes9inceptionlayers,asbasicnetwork.Fourdif-
ferentsettingsaretested:“inceptionlayers”indicatesthatthelossofthepeak-piloted
featuretransformationisappendedforallinceptionlayersplusthetwofully-connected
layers;“thefirstFClayer,”“thelastFClayer”and“bothFClayers”appendthelossto
thefirst,last,andandbothfully-connectedlayers,respectively.
It can be seen that using the peak-piloted feature transformation only on the two
fully connected layers achieves the best performance. Using additional losses on all
inception layers has roughly the same performance. Eliminating the loss of a fully-
connectedlayerdecreasesperformancebymorethan1%.Theseresultsshowthatthe
peak-pilotedfeaturetransformationismoreusefulforsupervisingthehighlysemantic
featurerepresentations(twofully-connectedlayers)thantheearlyconvolutionallayers.
AbsenceofPeakInformation. Table8demonstratesthatthePPDNcanalsobeused
when the peak frame is not known a priori, which is usually the case for real-world
videos. Given all video sequences, we trained the basic “GoogLeNet (baseline)” with
10-foldcrossvalidation.Themodelsweretrainedwith9-foldsandthenusedtopredict
theground-truthexpressionlabelintheremainingfold.Theframewiththehighestpre-
diction score in each sequence was treated as the peak expression image. The PPDN
wasfinallytrainedusingthestrategyofthepreviousexperiments.Thistrainingproce-
dureismoreapplicabletovideoswheretheinformationofthepeakexpressionisnot
available.ThePPDNcanstillobtainresultscomparabletothoseofthemodeltrained
withtheground-truthpeakframeinformation.
4.2 GeneralizationAbilityofthePPDN
The learning of the evolution from a hard sample to an easy sample is applicable to
other face-related recognition tasks. We demonstrate this by evaluating the PPDN on
face recognition. One challenge to this task is learning robust features, invariant to
pose and view. In this case, near-frontal faces can be treated easy examples, similarPeak-PilotedDeepNetworkforFacialExpressionRecognition 13
topeakexpressionsinFER,whileprofilefacescanbeviewedashardsamples,similar
tonon-peakexpressions.TheeffectivenessofPPDNinlearningpose-invariantfeatures
is demonstrated by comparing PPDN features to the “GoogLeNet(baseline)” features
onthepopularMulti-PIEdataset[19].
All the following experiments were conducted on the images of “session 1” on
Multi-PIE,wherethefaceimagesof249subjectsareprovided.Twoexperimentalset-
tingswereevaluatedtodemonstratethegeneralizationabilityofPPDNonfacerecog-
nition.Forthe“setting1”ofTable9,onlyimagesundernormalilluminationwereused
fortrainingandtesting,wheresevenposesofthefirst100subjects(IDfrom001to100)
wereusedfortrainingandthesixposes(from−45◦ to45◦)oftheremainingindividu-
alsusedfortesting.Onefrontalfacepersubjectwasusedasgalleryimage.Overall,700
imageswereusedfortrainingand894imagesfortesting.Bytreatingthefrontalface
andoneoftheprofilefacesasinput,thePPDNcanembedtheimplicittransformation
from profile faces to frontal faces into the network learning, for face recognition pur-
poses.Inthe“setting2”ofTable10,100subjects(ID001to100)withsevendifferent
posesunder20differentilluminationconditionswereusedfortrainingandtherestwith
sixposesand19illuminationconditionswereusedfortesting.Thisledto14,000train-
ingimagesand16,986testingimages.Similarlytothefirstsetting,PPDNtakesthepair
ofafrontalfacewithnormalilluminationandoneoftheprofilefaceswith20illumina-
tionsfromthesamesubjectastheinput.ThePPDNcanthuslearntheevolutionfrom
boththeprofiletothefrontalfaceandnon-normaltonormalillumination.Inadditionto
“GoogLeNet(baseline),”wecomparedthePPDNtofourstate-of-the-artmethods:con-
trolledposefeature(CPF)[28],controlledposeimage(CPI)[28],Zhuetal. [27]andLi
etal.[38].Thepre-trainedmodel,prepocessingsteps,andlearningrateusedintheFER
experimentswereadoptedhere.Under“setting1”thenetworkwastrainedwith10,000
iterationsandunder“setting2”with30,000iterations.Facerecognitionperformanceis
measuredbytheaccuracyofthepredictedsubjectidentity.
ItcanbeseenthatthePPDNachievesconsiderableimprovementsover“GoogLeNet
(baseline)”forthetestingimagesofhardposes(i.e.,−45◦and45◦)inboth“setting1”
and “setting 2”. Significant improvements over “GoogLeNet (baseline)” are also ob-
servedfortheaverageoverallposes(97.98%vs95.99%under“setting1”and83.22%
vs74.84%under“setting2”).ThePPDNalsobeatsallbaselinesby2.52%under“set-
ting 2”. This supports the conclusion that the PPDN can be effectively generalized to
facerecognitiontasks,whichbenefitfromembeddingtheevolutionfromhardtoeasy
samplesintothenetworkparameters.
5 Conclusions
Inthispaper,weproposeanovelpeak-piloteddeepnetworkforfacialexpressionrecog-
nition.Themainnoveltyistheembeddingoftheexpressionevolutionfromnon-peak
topeakintothenetworkparameters.PPDNjointlyoptimizesanL2-normlossofpeak-
piloted feature transformation and the cross-entropy losses of expression recognition.
By using a special-purpose back-propagation procedure (PGS) for network optimiza-
tion, the PPDN can drive the intermediate-layer features of the non-peak expression14 X.Zhao,X.Liang,L.Liu,T.Li,Y.Han,N.Vasconcelos,S.Yan
sampletowardsthoseofthepeakexpressionsample,whileavoidingtheinverse.
Appendix
Theloss
N
(cid:88)(cid:88)
J = (cid:107)f (xp,W)−f (xn,W)(cid:107)2 (A-1)
1 j i j i
i=1j∈Ω
hasgradient
N
(cid:88)(cid:88)
∇ J =2 (f (xp,W)−f (xn,W))∇ f (xn,W)
W 1 j i j i W j i
i=1j∈Ω
(A-2)
N
(cid:88)(cid:88)
+2 (f (xp,W)−f (xn,W))∇ f (xp,W).
j i j i W j i
i=1j∈Ω
ThePGSis
N
∇(cid:94) J =2(cid:88)(cid:88) (f (xp,W)−f (xn,W))∇ f (xn,W) (A-3)
W 1 j i j i W j i
i=1j∈Ω
Defining
N
(cid:88)(cid:88)
A= (f (xp,W)−f (xn,W))∇ f (xn,W) (A-4)
j i j i W j i
i=1j∈Ω
and
N
(cid:88)(cid:88)
B = (f (xp,W)−f (xn,W))∇ f (xp,W) (A-5)
j i j i W j i
i=1j∈Ω
itfollowsthat
<∇ J ,∇(cid:94) J >=−4<A,B >+4(cid:107)A(cid:107)2 (A-6)
W 1 W 1
or
<∇ J ,∇(cid:94) J >=−4(cid:107)A(cid:107)(cid:107)B(cid:107)cosθ+4(cid:107)A(cid:107)2 (A-7)
W 1 W 1
whereθisanglebetweenAandB.Hence,thedot-productisgreaterthanzerowhen
(cid:107)B(cid:107)cosθ <(cid:107)A(cid:107). (A-8)
Thisholdsforsureas∇ f (xn,W)convergesto∇ f (xp,W)whichisthegoalof
W j i W j i
optimization,butisgenerallytrueifthesizesofgradients∇ f (xn,W)and∇ f (xp,W)
W j i W j iPeak-PilotedDeepNetworkforFacialExpressionRecognition 15
(cid:94)
aresimilaronaverage.Sincethedot-productispositive,∇ J isadescent(although
W 1
notasteepestdescent)directionforthelossfunctionJ .Hence,thePGSisadescent
1
directionforthetotalloss.Notethat,becausetherearealsothegradientsofJ andJ ,
2 3
this can hold even when (A-8) is violated, if the gradients of J2 and J3 are dominant.
Hence,thePGSislikelytoconvergetoaminimumoftheloss.16 X.Zhao,X.Liang,L.Liu,T.Li,Y.Han,N.Vasconcelos,S.Yan
References
1. Liu,M.,Shan,S.,Wang,R.,Chen,X.: Learningexpressionletsonspatio-temporalmani-
foldfordynamicfacialexpressionrecognition. In:ProceedingsoftheIEEEConferenceon
ComputerVisionandPatternRecognition.(2014)1749–1756
2. Chen,H.,Li,J.,Zhang,F.,Li,Y.,Wang,H.: 3dmodel-basedcontinuousemotionrecogni-
tion. In:ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition.
(2015)1836–1845
3. Dapogny,A.,Bailly,K.,Dubuisson,S.: Pairwiseconditionalrandomforestsforfacialex-
pression recognition. In: Proceedings of the IEEE International Conference on Computer
Vision.(2015)3783–3791
4. Liu,P.,Han,S.,Meng,Z.,Tong,Y.: Facialexpressionrecognitionviaaboosteddeepbelief
network.In:ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecogni-
tion.(2014)1805–1812
5. Yu,Z.,Zhang,C.: Imagebasedstaticfacialexpressionrecognitionwithmultipledeepnet-
worklearning.In:Proceedingsofthe2015ACMonInternationalConferenceonMultimodal
Interaction,ACM(2015)435–442
6. Liu,M.,Li,S.,Shan,S.,Chen,X.:Au-awaredeepnetworksforfacialexpressionrecognition.
In:AutomaticFaceandGestureRecognition(FG),201310thIEEEInternationalConference
andWorkshopson,IEEE(2013)1–6
7. Jung,H.,Lee,S.,Yim,J.,Park,S.,Kim,J.: Jointfine-tuningindeepneuralnetworksfor
facialexpressionrecognition.In:ProceedingsoftheIEEEInternationalConferenceonCom-
puterVision.(2015)2983–2991
8. Li,X.,Mori,G.,Zhang,H.:Expression-invariantfacerecognitionwithexpressionclassifica-
tion. In:ComputerandRobotVision,2006.The3rdCanadianConferenceon,IEEE(2006)
77–77
9. Zhang,Z.,Luo,P.,Loy,C.C.,Tang,X.: Faciallandmarkdetectionbydeepmulti-tasklearn-
ing. In:ProceedingsofEuropeanConferenceonComputerVision(ECCV).(2014)
10. Zhong,L.,Liu,Q.,Yang,P.,Liu,B.,Huang,J.,Metaxas,D.N.:Learningactivefacialpatches
forexpressionanalysis. In:ComputerVisionandPatternRecognition(CVPR),2012IEEE
Conferenceon,IEEE(2012)2562–2569
11. Shan, C., Gong, S., McOwan, P.W.: Facial expression recognition based on local binary
patterns:Acomprehensivestudy. ImageandVisionComputing27(6)(2009)803–816
12. Kahou,S.E.,Froumenty,P.,Pal,C.: Facialexpressionanalysisbasedonhighdimensional
binaryfeatures. In:ComputerVision-ECCV2014Workshops,Springer(2014)135–147
13. Chopra,S.,Hadsell,R.,LeCun,Y.:Learningasimilaritymetricdiscriminatively,withappli-
cationtofaceverification.In:ComputerVisionandPatternRecognition,2005.CVPR2005.
IEEEComputerSocietyConferenceon.Volume1.,IEEE(2005)539–546
14. Lai,H.,Xiao,S.,Cui,Z.,Pan,Y.,Xu,C.,Yan,S.: Deepcascadedregressionforfacealign-
ment. arXivpreprintarXiv:1510.09083(2015)
15. Li,H.,Lin,Z.,Shen,X.,Brandt,J.,Hua,G.: Aconvolutionalneuralnetworkcascadefor
face detection. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition.(2015)5325–5334
16. Mollahosseini,A.,Chan,D.,Mahoor,M.H.: Goingdeeperinfacialexpressionrecognition
usingdeepneuralnetworks. arXivpreprintarXiv:1511.04110(2015)
17. Lucey,P.,Cohn,J.F.,Kanade,T.,Saragih,J.,Ambadar,Z.,Matthews,I.:Theextendedcohn-
kanadedataset(ck+):Acompletedatasetforactionunitandemotion-specifiedexpression.
In:ComputerVisionandPatternRecognitionWorkshops(CVPRW),2010IEEEComputer
SocietyConferenceon,IEEE(2010)94–101Peak-PilotedDeepNetworkforFacialExpressionRecognition 17
18. Zhao,G.,Huang,X.,Taini,M.,Li,S.Z.,Pietika¨inen,M.:Facialexpressionrecognitionfrom
near-infraredvideos. ImageandVisionComputing29(9)(2011)607–619
19. Gross,R.,Matthews,I.,Cohn,J.,Kanade,T.,Baker,S.: Multi-pie. ImageandVisionCom-
puting28(5)(2010)807–813
20. Guo,Y.,Zhao,G.,Pietika¨inen,M.:Dynamicfacialexpressionrecognitionusinglongitudinal
facialexpressionatlases. In:ComputerVision–ECCV2012. Springer(2012)631–644
21. Klaser,A.,Marszałek,M.,Schmid,C.: Aspatio-temporaldescriptorbasedon3d-gradients.
In:BMVC2008-19thBritishMachineVisionConference,BritishMachineVisionAssocia-
tion(2008)275–1
22. Sikka,K.,Wu,T.,Susskind,J.,Bartlett,M.: Exploringbagofwordsarchitecturesinthefa-
cialexpressiondomain.In:ComputerVision–ECCV2012.WorkshopsandDemonstrations.
(2012)
23. Krizhevsky,A.,Sutskever,I.,Hinton,G.E.: Imagenetclassificationwithdeepconvolutional
neuralnetworks.In:Advancesinneuralinformationprocessingsystems.(2012)1097–1105
24. Szegedy,C.,Liu,W.,Jia,Y.,Sermanet,P.,Reed,S.,Anguelov,D.,Erhan,D.,Vanhoucke,V.,
Rabinovich,A.: Goingdeeperwithconvolutions. In:ProceedingsoftheIEEEConference
onComputerVisionandPatternRecognition.(2015)1–9
25. Simonyan,K.,Zisserman,A.:Verydeepconvolutionalnetworksforlarge-scaleimagerecog-
nition. arXivpreprintarXiv:1409.1556(2014)
26. Sun, Y., Chen, Y., Wang, X., Tang, X.: Deep learning face representation by joint
identification-verification. In:AdvancesinNeuralInformationProcessingSystems.(2014)
1988–1996
27. Zhu, Z., Luo, P., Wang, X., Tang, X.: Deep learning identity-preserving face space. In:
ProceedingsoftheIEEEInternationalConferenceonComputerVision.(2013)113–120
28. Yim,J.,Jung,H.,Yoo,B.,Choi,C.,Park,D.,Kim,J.: Rotatingyourfaceusingmulti-task
deep neural network. In: Proceedings of the IEEE Conference on Computer Vision and
PatternRecognition.(2015)676–684
29. Schroff,F.,Kalenichenko,D.,Philbin,J.:Facenet:Aunifiedembeddingforfacerecognition
and clustering. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition.(2015)815–823
30. Sun,Y.,Liang,D.,Wang,X.,Tang,X.: Deepid3:Facerecognitionwithverydeepneural
networks. arXivpreprintarXiv:1502.00873(2015)
31. Bottou,L.: Large-scalemachinelearningwithstochasticgradientdescent. In:Proceedings
ofCOMPSTAT’2010. Springer(2010)177–186
32. Yi,D.,Lei,Z.,Liao,S.,Li,S.Z.: Learningfacerepresentationfromscratch. arXivpreprint
arXiv:1411.7923(2014)
33. Glorot, X., Bengio, Y.: Understanding the difficulty of training deep feedforward neural
networks. In:Internationalconferenceonartificialintelligenceandstatistics.(2010)249–
256
34. Bartlett,M.S.,Littlewort,G.,Frank,M.,Lainscsek,C.,Fasel,I.,Movellan,J.: Recognizing
facialexpression:machinelearningandapplicationtospontaneousbehavior. In:Computer
VisionandPatternRecognition,2005.CVPR2005.IEEEComputerSocietyConferenceon.
Volume2.,IEEE(2005)568–573
35. Xue, Mingliang, W.L., Li, L.: The uncorrelated and discriminant colour space for facial
expressionrecognition.OptimizationandControlTechniquesandApplications(2014)167–
177
36. Freund,Y.Schapire,R.:Adecision-theoreticgeneralizationofon-linelearningandanappli-
cationtoboosting. In:ProceedingsoftheSecondEuropeanConferenceonComputational
LearningTheory.(1995)23–27
37. Liu,M.,Li,S.,Shan,S.,Wang,R.,Chen,X.:Deeplylearningdeformablefacialactionparts
modelfordynamicexpressionanalysis. In:ComputerVision–ACCV201418 X.Zhao,X.Liang,L.Liu,T.Li,Y.Han,N.Vasconcelos,S.Yan
38. Li,A.,Shan,S.,Gao,W.: Coupledbias–variancetradeoffforcross-posefacerecognition.
ImageProcessing,IEEETransactionson21(1)(2012)305–315"
188,190,Perceptual and affective mechanisms in facial expression recognition: An integrative review,"['MG Calvo', 'L Nummenmaa']",2016,315,"Affective Faces Database, Karolinska Directed Emotional Faces","classification, classifier, facial expression recognition","We conclude that facial expression recognition, as it has been investigated in conventional   One such method is the affective priming paradigm, whereby an emotional face is briefly",No DOI,Cognition and Emotion,https://pubmed.ncbi.nlm.nih.gov/26212348/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,nih.gov,
189,191,Practical emotional neural networks,"['E Lotfi', 'MR Akbarzadeh-T']",2014,136,Affective Faces Database,neural network,"is associated with motivation from emotion to improve or  emotional neural networks (Khashman,  2010). This paper aims to review and develop neural networks motivated from emotion",No DOI,Neural Networks,https://www.sciencedirect.com/science/article/pii/S0893608014001488,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
190,192,Predicting long-term outcome of Internet-delivered cognitive behavior therapy for social anxiety disorder using fMRI and support vector machine learning,"['KNT Månsson', 'A Frick', 'CJ Boraxbekk']",2015,170,Karolinska Directed Emotional Faces,machine learning,"demonstrated that initial activations of the visual cortex, in response to emotional face  stimuli, predicted symptom improvement with CBT, and that brain measures vastly improved",No DOI,Translational …,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4354352/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,nih.gov,
191,193,Presentation and validation of the Radboud Faces Database,"['O Langner', 'R Dotsch', 'G Bijlstra']",2010,2954,"Affective Faces Database, Radboud Faces Database","classification, facial expression recognition","available Radboud Faces Database (RaFD), a face database containing Caucasian face   We asked participants to pick the emotion label that best fitted the shown facial expression.",No DOI,… and emotion,https://www.tandfonline.com/doi/full/10.1080/02699930903485076,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,tandfonline.com,
192,194,Pyramid with super resolution for in-the-wild facial expression recognition,"['TH Vo', 'GS Lee', 'HJ Yang', 'SH Kim']",2020,138,Expression in-the-Wild,"FER, facial expression recognition","Wang, “Video facial emotion recognition based on local enhanced motion history image and   Xia, “Bi-modality fusion for emotion recognition in the wild,” ICMI 2019 Proceedings of the",No DOI,IEEE Access,http://ieeexplore.ieee.org/document/9143068/,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
193,195,Real time emotion recognition from facial expressions using CNN architecture,"['MA Ozdemir', 'B Elagoz', 'A Alaybeyoglu']",2019,107,Karolinska Directed Emotional Faces,CNN,"In this study, we proposed CNN based LeNet architecture for facial expression recognition  to estimate emotion states of human. We merged 3 different datasets (KDEF, JAFFE and our",No DOI,2019 medical …,https://ieeexplore.ieee.org/document/8895215,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
194,196,Real-time face detection and lip feature extraction using field-programmable gate arrays,"['D Nguyen', 'D Halupka', 'P Aarabi']",2006,147,Toronto Face Database,classifier,This paper proposes a new face detection technique that utilizes a naive Bayes classifier  to detect faces in an image based on only image edge direction information. This technique,No DOI,IEEE Transactions on …,https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=ccb214f4273b8eb7e2c155beb89f9318c347d60f,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,psu.edu,
195,197,Recognition of affect in the wild using deep neural networks,"['D Kollias', 'MA Nicolaou', 'I Kotsia']",2017,161,"Acted Facial Expressions In The Wild, Affective Faces Database, Expression in-the-Wild","CNN, classification, deep learning, neural network",(such as subjects reacting to an unexpected development in a  end-to-end deep CNN and  CNN-RNN architectures (Section 3)  that a major challenge in facial expression and emotion,No DOI,Proceedings of the …,https://ieeexplore.ieee.org/document/8014981,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
196,198,Recognition of facial expressions of emotion is related to their frequency in everyday life,"['MG Calvo', 'A Gutiérrez-García']",2014,134,Karolinska Directed Emotional Faces,facial expression recognition,"on facial expression recognition has consistently found that happy expressions are   Consistent with this, in the current study, nearly one-third (29 %) of all the observed emotional faces",No DOI,Journal of Nonverbal …,https://link.springer.com/article/10.1007/s10919-014-0191-3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,springer.com,
197,199,Recognition thresholds for static and dynamic emotional faces.,"['MG Calvo', 'P Avero', 'A Fernández-Martín', 'G Recio']",2016,113,Karolinska Directed Emotional Faces,"classification, classifier","To this end, we varied the degree of intensity of emotional expressions unfolding from a  neutral face, by means of graphics morphing software. The resulting face stimuli (photographs",No DOI,Emotion,https://pubmed.ncbi.nlm.nih.gov/27359222/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,nih.gov,
198,200,Recognizing action units for facial expression analysis,"['YI Tian', 'T Kanade', 'JF Cohn']",2001,2384,Extended Cohn-Kanade,"classification, classifier",We classify each of the wrinkles into one of two states:  of seven subjects from the Cohn-Kanade  database. Of the 72  of 46 subjects from the CohnKanade database and tested on 50,No DOI,IEEE Transactions on pattern …,https://ieeexplore.ieee.org/document/908962,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
199,201,Recognizing facial expression: machine learning and application to spontaneous behavior,"['MS Bartlett', 'G Littlewort', 'M Frank']",2005,912,Extended Cohn-Kanade,machine learning,"We first report performance for generalization to novel subjects within the Cohn-Kanade and   Shown are 4 subjects from the Cohn-Kanade dataset posing disgust containing AU’s 4,7",No DOI,2005 IEEE Computer …,https://ieeexplore.ieee.org/document/1467492/1000,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
200,202,Recover canonical-view faces in the wild with deep neural networks,"['Z Zhu', 'P Luo', 'X Wang', 'X Tang']",2014,152,Expression in-the-Wild,neural network,"Thc first term in Equation (1) measures the face's symmetry, which is the difference between  the left half and the right half of the face, and the second term measures the rank of the face.",No DOI,arXiv preprint arXiv:1404.3543,https://arxiv.org/abs/1404.3543,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"Recover Canonical-View Faces in with Deep
the 明Tild
Neural Networks
1 1 2 1
Zhenyao Zhu Ping Luo Xiaogang Wang Xiaoou Tang ,3
1D epartm巳ntof Information Engin巳ering， Th巳Chines巳Universityof Hong Kong
2Department ofElectronic Engineering, The Chinese University ofHong Kong
3Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences
zz012日ie.cuhk.edu.hk p1uo.1h工@gmail.com
xgwang@ee.cuhk.edu.hk xtang@ie.cuhk.edu.hk
Abstract
Face images in the wild undergo larg巳 intra-p巳rsonal variations, such as poses,
illuminations, occlusions, and low resolutions, causing great challenges to face
related applications. This paper addresses this challenge by proposing a new deep
learning framework that can recover the canonical view of face images. It dra
matically reduces the intra-person variances, while maintaining the inter-person
discriminativeness. Unlike the existing face reconstruction methods that were ei
ther evaluated in controlled 2D environment or employed 3D information, our
approach directly learns the transformation from the face images with a complex
set of variations to their canonical views. At the training stage, to avoid the costly
process of labeling canonical-view images from the training set by hand, we have
devised a new measurement to automatically select or synthesize a canonical-view
image for each identity.
As an application, this face recovery approach is used for face v巳rification. Facial
features are learned from the recovered canonical-view face images by using a
facial component-based convolutional neural network. Our approach achieves the
state-of-the-art performance on the LFW dataset
1 Introduction
Dealing with variations of face images is the key challenge in many face-related applications. For
example, in face recognition, most research efforts have focus巳don how to distinguish intra-personal
variations of poses, lightings, expressions, occlusions, ages, makeups, and resolutions from inter
personal variation which distinguishes face identities. The aim of face hallucination is to reconstruct
high-resolution face images from low-resolution ones [26], or to remove glasses from face images
口9]. For face synthesis, people produce images under di旺erentages 口2]， poses [町， and illumina
tions [36]. There are also research works [38] on matching face photos with sketches of di旺erent
styles and synthesizing sketches from photos. Recently, a 3D viewing system [18] was proposed to
reconstruct 3D face models from real-world images.
To deal with face variations, the巳xistingmethods can be roughly divided into two categories: robust
feature extraction and face normalization. In the白rst category, global features such as Eigen faces
[33], Fisher faces [6], and their extensions [37] can cover global variations due to small pose and
simple illumination chang口， but do not work well under large poses and complex illumination
conditions. They are not robust to local distortions, such as expressions and occlusions either. The
high dimensional concatenations of the local descriptors, such as Haar [35], Gabor [15], and LBP
[1], have demonstrated their robustness to local distortions and achieved significant improvement inFigure 1: The proposed method can recover the images of canonical view and illumination from images with
larg巳 variations. For巳xample， in 巳ach row, w巳 show th巳 images and the reconstruct巳d images of the sam巳
identity. The reconstructed imag巳sdramatically reduce th巳intra-p巳rson variances, while maintaining th巳inter­
person discriminativeness
face recognition [11]. In addition to the above hand-crafted descriptors, other existing researches
studies have also tried to integrate multiple features or directly learn features from raw pixels, such
as using random-projection trees [10], local quantized paUerns [34], and deep learning [19,24,21,
13, 30, 31]. For example, Sun et al. [31] learned face representation with a deep model through
face identification, which is a challenging multi-class prediction task. The comrnon weakness of the
feature extraction approaches is that they are all sensitive to large intra-person variations
In th巳 s巳cond cat巳gory， approaches tend to recov巳r an imag巳 in th巳 canonical vi巳w (with frontal
pose and neutral lighting) from a face image under a large pose and a di旺巳rent lighting, so that
it can b巳 used as a good normalization. There are 3D- and 2D-based methods. The 3D-based
methods aim to recover the frontal pose by 3D geometrical transformations [8,4], which first aligns
a 2D face image to a 3D face model and then rotates it to r巳nder the frontal view. The existing
2D-based methods [3, 2] infe町ed the frontal pose with graphical models, such as Markov Random
Fields (MRF), where the correspondences betwe巳n nodes in the MRF are learned from images in
di旺erent poses. However, capturing 3D data adds additional cost and resources, and MRF-based
face synthesis depends heavily on good alignment, while the results ar巳 often not smooth on real
world images. The recent work [40] directly learned transformation between face images in arbitrary
views and frontal views and obtained good results in the MultiPIE dataset [17].
In this paper, we aim to recover the canonical view from a 2D face image taken under an arbitrary
pose and lighting condition in the wild. It is a big challenge to learn such a complex set of pos巳and
lighting transforms in uncontrolled environment, and the learned transformation function must be
highly multi-modal. We wiU show that a carefully designed deep learning framework can overcome
this challenge, benefiting from its great learning capacity. Some examples of recovered face images
with our approach are shown in Figure 1.
Our framework contains two steps: (1) canonical-view image selection, and (2) face r巳covery. First,
in order to learn the transformation between face images and their canonical views, we must select
a representative image for 巳ach identity, which is taken in the 仕ontal view, under neutral light
ing condition and with high resolution. To avoid selecting them by hand, we first develop a new
measurement, which measures the face images' symrnetry and sharpness. We then learn the trans
formation with a carefully designed deep network, which can be considered as a regression from
images in arbitrary views to the canonical-view.
A sample application of this framework is face verification. A facial component-based convolu
tional neural network is developed to learn hierarchical feature representations from the rl巳coverl巳d
canonical-view images. These features ar巳 robust for face verification, since the recover巳d images
2Figure 2: The images of two identities are ranked according to three different criterions in (a) and (b): face
symmetry (the first row), matrix rank (the second row), and symmetry combined with matrix rank (the third
row). In each row, the first five imag巳sand the last five images are visualized
64 64
•••
(b) Face Selection Y
Figure 3: Pipeline of canonical view face selection (b) and face recovery (a)
already remove large face variations. It also has pot巳ntial applications to other problems, such as
fac巳hallucination， fac巳 sketchsynthesis and r巳cognition， and fac巳attributeestimation.
In summary, this work has the following key contributions. Firstly, to the best of our knowledge,
this is the first work that can recover canonical-view face images using only 2D information from
face images in the wild. This method shows stat巳-of-the-artperformanc巳on face verification in th巳
wild. Secondly, the reconstructed images are of high-quality.
2 Canonical View Face Recovery
2.1 A New Measurement for Canonical View Face Images
Although various facial measurements [29] have b巳en studied in th巳 lit巳rature， th巳y have mainly
focused on the image qualities, such as noise ratio and resolutions, and seldom considered how to
determine whether a face image is taken in frontal view. We have devised a facial measurement for
frontal view face images by combining the rank and symmetry of rnatrix. For exarnple, as shown
in Figure 2, we collect the images of a subject and visualize them according to the following three
criterions: (1) di旺巳rence b巳tween the left half face and the right half face in ascending order (face
symmetry), (2) rank of the image in descending order, and (3) the combination of (1) and (2). In
the first row of Figure 2, we observe that measuring symmetry as in (1) is effective for frontal view
images, but it prefers the images in low resolutions. Although the second row shows that larger rank
3Figure 4: Examples of face reconstruction on the LFW dataset. For each pair, the left one is an original image
of the LFW dataset and the right one is the recovered image.
四
自
{由
这市
u} . Originallmages
E〉
3
u • Recovered Images
u
d
q
70
LBP Gabor HOG
Figure 5: Comparisons of face verification performance of di旺erentfeatures on the LFW dataset
indicates sharper images, the images sometimes do not have frontal views. Th巳combination of (1)
and (2) achieves the best result as shown in the third row in Figure 2.
We formulate this measurement as shown below. Let a matrix Y i 巳 ]R64x64 denote a face image of
the i-th identity and ][])i be the set of images of identity i, Yi ε ID\. The frontal view measurement
can be written as,
M(Yi) =11 YiP - YiQ II}λ11 Yi 11川 飞r·唱EA、，/
whereλis a constant coefficient, 11 . IIF is the Frobenius norm, and 11 . 11* denotes the nuclear norm,
which is the sum of the singular values of a matrix. P, Qε ]R64x64 are two constant matrixes with
P = diαg([132' 0 32]) and Q = diαg([032， 1 32]), whcrc diαg(.) indicatcs thc diagonal matrix. Thc
first term in Equation (1) measures the face's symmetry, which is the difference between the left half
and the right half of the face, and the second term measures the rank of the face. Small巳rvalue of
Equation (1) indicates the face is more likely to be in frontal view.
We can select a frontal face image as a representative for each identity and then learn a mapping,
which transforms the face image in arbitrary view to the frontal view. This sel巳ctioncan be achieved
in several ways. In this report, we simply choose the image with the minimum measurement as
the frontal face for each identity. However, using a linear combination to calculate the frontal face
image is also possible. We will report results in the future
2.2 Face Recovery
After face selection, we adopt a deep network to recover the frontal vi巳w image by minimizing the
loss error
E( {X?k}; W) =艺艺 11 Yi - j(X?k; W) 11手， (2)
i k
where i is the index of identity and k indicates the k-th sample of identity i. XO and Y denot巳 the
training image and the target image (the selected frontal face), respectively. W is a set of parameters
of the deep network
As shown in Figure 3, the deep network contains three convolution layers. The first two are followed
by the max pooling layers and the last one is followed by a fully-connect layer. Diff，巳rentfrom the
conventional CNN, whose filters share weights, our filters are localized and do not share weights
because we assume di旺erent face regions should employ different features. The input XO, the
output Y (predicted imag时， and the target Y缸ein the size of 64 x 64. All of them are transformed
to gray-scale and their illuminations are coπected as in [36]. At each convolutionallayer, we obtain
32 output channels by learning non-shared filters, each of which is in the size of 5 x 5. The cell size
4(a)
(c) (d)
(e) (f)
Figur巳6: Canonical view fac巳reconstructionsof s巳veralidentiti巳s
of the sub-sampling layer is 2 x 2. The l-th convolutionallayer can by formulated as
I
x;1Ju= σ(艺w;川。 (X~)uv + b~)， (3)
p=l
where Wιuv and (X~)uv denote the filter and the image patch at the image location (川)， re
sp巳ctlV巳ly. p, q are the index巳s of input and output channels. For instance, in the first convolutional
layer, p = 1, q = 1...32. Thus, X~;~v indicates the q-th channel ouψut at the location (u, v); that is
the input to the 1+ 1-th layer.σ(x) = max(O, x) is the rectified linear function and 0 indicates the
element-wise product. The bias vectors are denoted as b. At the fully-connect layer, we recover the
image Y by
L
Y = W LXL+b . (4)
Equation (2) is non-linear because of the activation functions in the deep network. We solve it by
th巳 stochastic gradi巳nt desc巳nt (SGD) with back-propagation as in [22]. As shown in Figllre 3,
at the l-th ωlVolutional layer, the gradient of the filter at position u, v is comp叫 by 满v
(el)，川Xl-l)川， where E is the loss error defined in Equation (2) and e is the back-propagation
error. e is obtained in a recursive manner as el = P' 0 (el+1 ( 1), where ( is the Kror肌kerproduct
that up-samples el+1 to the same size as el, and 1l ' is the derivative of the activation function at
the l-th layer. At the l-th fully-connect layer, the gradient of the weight matrix is calculated by
蒜 = XI~iT, which is the 侧叫roduct of the back-propagation 巳er町rrr…o ld 趾the巳 i呻n叩n阳puu川t of 仕th出h让i刷i
layer. e is also derived in a recursive way as el 1l ' 0 (Wl+1 T el +1 ). For instance, if layer 1 is
activated using sigmoid function, then el = X l 0 (1 - X l ) 0 (Wl+1 T el+1 ). Furthermore, dropout
learning [20] is adopted at each layer to avoid ov巳r-fitting.
2.3 Effectiveness of Face Recovery
S巳veral巳xamplesof the recovered canonical view images are shown in Figure 4. In ord巳rto demon
strate the quality of the recovered image, we compare the performance of the existing feature ex
traction methods, including LBP [1], HOG [14], and Gabor [15], when they are 巳xtracted from the
reconstructed image and the original image. We adopt the testing data of LFW dataset. For each
of the above features, we extract it from the face image in a regular grid of size 8 x 8 and then
apply PCA and LDA. Th巳 performance of face verification are repo此ed in Figure 5, where shows
that the existing feature extraction methods can be improved when they are applied on the recovered
image, which is a good normalization to account for different face variations. More examples of
face recovery for one identity can b巳foundin Figure 6.
5/
τ…'t-t-t
Convolutions
5ubsampling 5ubsI ampling
Convolutiσns
Figure 7: Architecture of the facial component-based network. The network contains five CNNs, each of
which takes a pair of whole faces or facial components as input. The sizes of the whole face, forehead, eye,
nose, and mouth are 64 x 64, 22 x 64, 24 x 64, 28 x 30, and 20 x 56, resp巳ctlV巳ly. First，巳ach CNN
leams the joint representation of the pairs of input. A logistic regression layer then concatenates all the joint
representations as features to predict whether the two face images belong to the same identity.
3 Facial Component Deep Network for Face Verification
The canonical view images can be used as input to a facial component deep network (FCN), which
learns relational features from two images for face verification, as shown in Fig.7. Similar architec
ture has been adopted by [30], where the original images are used as the input and a large number of
networks have to be trained. Unlike [30], the FCN is applied on the canonical images that reduce the
face variations. Therefore, five networks concatenation is enough to achieve good result as discussed
below. Learning FCN contains three steps, including facial component-based patch recovering and
cropping, feature learning, and feature reduction.
In the first step, for each pair of training images, we r巳cover their canonical view imag巳s and th巳n
extract 5 landmarks. Imag巳 patches of different facial components are cropped based on the above
landmarks. Specifically, we extracted patches from forehead, eyes, nos巳， and mouth.
In the second step, each patch p创ris utilized to train a deep network. Then, multiple networks are
concatenated together by a fully-connected layer to learn the feature representation. Each network
compris巳stwo convolutionallayers and two sub-sampling layers. Figure 7 specifies the archit巳cture
of concatenation of multiple networks, where the parameters are optimized using stochastic gradient
descent with back-propagation. In particular, as in Section 2.2, we pass the back-propagation 巳町or
backwards and then update the weights or filters in each layer. We adopt the entropy error instead
ofthe loss e町orbecause we need to predict the labels Y
Err = ylogy + (1 - y) 10g(1 - y), (5)
where y are the predicted labels, and y, y ε {O， l}K, with Yk = 1 indicating that the input images
belong to the k-th identity.
In the third step, we employ PCA and ensemble of Support Vector Machines (SVM) for face verifi
catlOn
4 Experiments
We evaluate our approach on the LFW dataset, which is collected from internet and contains 5749
people with 13, 233 face images in total, which vary in terms of their poses, illuminations, resolu
tions, makeups, and occlusions. The average number of images for each identity is 2.3土9.01，where
6Methods Accuracy( % )
Associate-Predict [28] 90.57
Joint Bayesian(cid:157) [12] 92.42
Convnet-RBM [30] 92.52
Tom-vs-Pete [7] 93.10
Tom-vs-Pete+Attribute [7] 93.30
High-dim LBP(cid:157) [11] 95.17
TL Joint Bayesian [9] 96.33
FR+FCNt (whol巳fac巳+components) 96.45
PLDA [25] 90.07
Joint Bayesian [12] 90.90
Fisher Vector Faces [5] 93.03
High-dimLBP[11] 93.18
FR+FCN (whole fac巳) 93.65
FR+FCN (whol巳fac巳+components) 94.38
Face++ [16] 97.27
Table 1: Method Comparisons
5438 people have less than 5 images and only 143 people have more than 10 images. Due to the
imbalance of LFW, it is not suitable to train the face recovery network because of the following rea
sons: (1) training examples are not enough for most of the identities, (2) they may not have frontal
view images, and (3) the size of the dataset is not巳noughfor a deep learning-based method. PubFig
[23] and WDRef [12] are two larg巳r datasets than LFW. How巳ver， PubFig only has 200 people,
which means the identity variation is insufficient, while WDRef is not publicly available. We train
our models on the CelebFaces [30], which contains 87,628 face images of 5436 identities. The
average number of images for each identity is 15.9土8.0， which shows that it is more balanced than
LFW
We compare our results with the existing best-performing approaches suggested by the LFW bench
mark'. There are two experimental settings. First, the upper part of Table 1 shows the results
employing outside training data other than LFW under the restricted protocol. Most of the best
performing methods such as 凹， 11, 7] belong to the second setting. Second, the m巳thods in the
lower part are trained on LFW under the unrestricted protocol, using only the training data in LFW.
Our methods achieve the state-of-the-art performance in both the above settings. For instance, in
th巳 first setting, we train the FR+FCNt (whole face+components) on the outside data of two hun
dred thousand image pairs generated from the PubFig [23] and CelebFaces. The FR+FCN (whole
face+components) achieves the accuracy of 96.45 percent, which performs slightly better than the
best results [9] and improves 4 percent compared to [30]. This is because the canonical view im
ages can reduce large face variations. In the second setting, we achieve th巳 second best result. The
best-performing method is a commercial system [16], where the number of faciallandmark align
ment and the size of training data are not clear. Our method employs the recovered the canonical
view images to reduce the face variations. In this case, five facial key points alignment is enough to
achieve good result. Figure 8 and 9 plots the ROC curves of the above methods. For more details
please refer to the project page at http://mrnlab .工e. cuhk . edu . hk.
5 Conclusions and Discussions
In this pap巳r， we have proposed a new deep learning framework that can rl巳coverthe canonical view
face images from images in arbitrary wild conditions. With this framework, given the face images
of any new identity, th巳canonical view of thes巳images can b巳巳fficiently recovered. This approach
has many potential applications, such as face hallucination, face sketch synthesis and recognition,
and face attribute estimation.
'http://vis-www.cs.umass.edu/lfw/results.html
70.9V荔/刁
~帽』
..
--------------, --------------------,-------------
-一-Associate-Predict
一一-Joint
Bayesiant
一-Tom-vs-Pete
一一-Tom-vs-Pete+Attribute
t
一-High-dim LBp
0.6U… …------，可， j - TL Joi nt Bayesian
一一-Convnet-RBM CelebFaces
一-FR+FCNt(whole face+components)
05O
E 0.1 0.2 0.3 0.4 0.5
false positive rate
Figure 8: ROC curve under the LFW r，巳strictedprotocol
0.9
Q)
嗡园
-里 0.8
0
>
一一-PLDA
m
o 一一-Fisher Vector Faces
=20 .7 一一-Joint Bayesian
』 一-high-dim LBP
嗡圃，
一一-Convnet-RBM
0.6
一-Face忡
一一-FR+FCN(whole face)
一一-FR+FCN(whole face+components)
0.5
0 0.1 0.2 0.3 0.4 0.5
false positive rate
Figure 9: ROC curv巳under由eLFW unrestricted protocal
We apply our face recovery framework to the task of face verification and outperform the state-of
the-art approaches. We also show that the existing face recognition methods can be improved when
they adopt our face recovery as normalization and pre-processing.
A recent work [27] reported 98.5 percent accuracy with Gaussian Processes and combined multiple
training sets. This could b巳 du巳 to fact that the nonparametric Bayesian kernel method can adapt
model complexity to data distribution. This could be another interesting direction to be explored in
the future.
6 Acknowledgement
This work is p征tially supported by the General Research Fund sponsored by the Research Grants
Council of Hong Kong (Project No.CUHK 416510 and 416312) and Guangdong Innovative Re
search TI巳amProgram (No.201001D0104648280).
8References
[1] T. Ahon巳n，A. Hadid, and M. Pietikainen. Face description with local binary pattems: Application to face
recognition. TPAMI, 28(12):2037-2041, 2006.
[2] S. Arashloo and J. Kittler. En巳rgy normalization for pose-invariant face recognition bas巳don mrf model
image matching. TPAMI, 2011.
[3] A. Ashraf, S. Lucey, and T. Ch巳n. Leaming patch correspondences for improved viewpoint invariant face
recognition. CVPR, 2008
[4] A. Asthana, T. Marks, M. Jones, K. Tieu, and R. Mv. Fully automatic pose-invariant face recognition via
3d pose normalization. ICCV, 2011.
[5] O. Barkan, J. Weill, L. Wolf, and H. Aronowitz. Fast high dimensional vector multiplication face recog
nition. ICCV, 2013
[6] 卫Belhumeur，J. Hespanha, and D. Kriegman. Eigenfaces vs. fisherfaces: Recognition using class specific
linear projection. TPAMI, 19(7):711-720, 1997
[7] T. Berg and P. N. Belhumeur. Tom-vs-pete classifiers and identity-preserving alignment for face verifica
tion. BMVC, 2012
[8] V. Blanz and T. Vetter. Face recognition based on fitting a 3d morphable model. TPAMI, 2003
[9] X. Cao, D. Wipf, F. Wen, and G. Duan. A practical transfer leaming algorithm for face verification. ICCV,
2013
[10] Z. Cao, Q. Yin, X. Tang, and J. Sun. Face recognition with learning-based descriptor. CVPR,201O
[11] D. Chen, X. Cao, and F. W. J. Sun. Blessing of dimensionality: High-dimensional feature and its efficient
compression for face verification. CVPR, 2013
[12] D. Chen, X. Cao, L. Wa吨，F. Wen, and J. Sun. Bayesian face revisited: A joint formulation. In ECCV
2012, pages 566-579. Springer, 2012
[13] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively, with application to
face verification. In CVPR, 2005
[14] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. CVPR,2005
[15] J. Daugman. Uncertainty relation for resolution in spac巳，spatial frequ巳ncy，and orientation optimized by
two-dimensional visual cortical filters. Optical Society ofA merica, Journal, A: Optics and Image Science,
2:116C共1169， 1985
[16] H. Fan, Z. Cao, Y. Jiang, Q. Yin, and C. Doudou. Leaming deep face representation. arXiv: 1403. 2802,
2014
[17] R. Gross, 1. Matthews, J. Cohn, T. Kanade, and S. Baker. Multi-pie. Image and Vision Computing,
28(5):807-813,2010
[18] T. Hassner. Viewing real-world faces in 3d. ICCV, 2013
[19] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science,
313(5786):504-507,2006
[20] G. E. Hinton, N. Srivastava, A. Krizhevsky, 1. Sutskev时， and R. Salakhutdinov. Improving neural net
works by preventing coadaptation offeature detectors. arXiv:1207.0580, 2012
[21] G. B. Huang, H. Lee, and E. Learned-Miller. Learning hierarchical representations for face verification
with convolutional deep belief networks. In CVPR, 2012
[22] A. Krizhevsky, 1. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural
networks. NIPS, 2012
[23] N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar. Attribute and Simile Classifiers for Face
Verification. In ICCV, 2009
[24] H. Lee, R. Grosse, R. Ranganath, and A. Ng. Convolutional deep belief networks for scalable unsuper
vised learning of hi巳rarchicalrepres巳ntations. ICML, 2009
[25] 卫Li，Y. Fu, U. Mohammed, J. H. Elder, and S. J. Prince. Probabilistic models for inference about identity
TPAMI,2012
[26] C. Liu, H.-Y. Shum, and W. T. Freeman. Face hallucination: Theory and practice. CVPR,2001
[27] C. Lu and X. Tang. Su叩assing human-Ievel face verification performance on Ifw with gaussianface
Technical report, arXiv:1404.3840, 2014
[28] X. T. Qi Yin and J. Sun. An associate-predict model for face recognition. CVPR,2011
[29] J. Sang, Z. Lei, and S. Z. Li. Face image quality evaluation for iso/iec standards 19794-5 and 29794-5
ICB,2009
[30] Y. Sun, X. Wa吨，and X. Tang. Hybrid deep learning for face verification. ICCV, 2013
[31] Y. Sun, X. Wang, and X. Tang. D巳eplearning face repres巳ntationfrom predicting 10,000 classes. CVPR,
2014
[32] J. Suo, S.-C. Zhu, S. Shan, and X. Chen. A compositional and dynamic model for face aging. TPAMI,
32(3),2010
[33] M. Turk and A. Pentland. Eigenfaces for recognition. Journal of cognitiνe neuroscience, 3(1):71-86,
1991
9[34] S. ul Hussain, T. Napoléon, and F. Juri巳 Facerecognition using local quantized pattems. BMVC,2012.
[35] P. Viola and M. Jon巳s. Rapid obj巳ctdetection using a boost巳dcascade of simple features. CVPR, 2001
[36] A. Wagner, J. Wright, A. Ganesh, Z. Zhou, H. Mobahi, and Y. Ma. Towards a practical face recognition
syst巳m: Robust alignm巳ntand illumination by spars巳representation. TPAMI, 2012
[37] X. Wang and X. Tang. A unified framework for subspace face recognition. TPAMI,26(9):1222-1228,
2004
[38] X. Wang and X. Tang. Face photo sketch synthesis and recognition. TPAMI, 31 :1955-1967,2009
[39] c. Wu, C. Liu, H.-Y. Shum, Y.-Q. XU, and Z. Zhang. Automatic eyeglasses removal from face imagω
TPAMI，26(匀，2004
[40] Z. Zhu，卫Luo，X. Wang, and X. Tang. D巳epleaming identity pr巳servingfac巳spac巳. ICCV,2013
10"
201,203,Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild,"['S Li', 'W Deng', 'JP Du']",2017,1700,Expression in-the-Wild,"CNN, classification, classifier, deep learning, machine learning","For all we know, RAF-DB is the first database that contains compound expressions in the wild.  Our  So in this paper, we directly trained our deep learning system on the big enough self-",No DOI,… of the IEEE conference on computer …,https://ieeexplore.ieee.org/document/8099760,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
202,204,Robust facial expression recognition based on local directional pattern,"['T Jabid', 'MH Kabir', 'O Chae']",2010,448,Japanese Female Facial Expression,"classification, classifier, facial expression recognition, machine learning","Two well-known machine learning methods, template matching and support vector machine,  are used for classification using the Cohn-Kanade and Japanese female facial expression",No DOI,ETRI journal,https://onlinelibrary.wiley.com/doi/abs/10.4218/etrij.10.1510.0132,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,wiley.com,
203,205,Shape analysis of local facial patches for 3D facial expression recognition,"['A Maalej', 'BB Amor', 'M Daoudi', 'A Srivastava']",2011,107,Binghamton University 3D Facial Expression,"classification, classifier, deep learning, facial expression recognition, machine learning, neural network",► We address the 3D facial expression recognition problem using Riemannian geometry.  ► We propose local shape  [12] at Binghamton University. It was designed for research on,No DOI,Pattern Recognition,https://www.sciencedirect.com/science/article/pii/S0031320311000756,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
204,206,Simultaneous facial feature tracking and facial expression recognition,"['Y Li', 'S Wang', 'Y Zhao', 'Q Ji']",2013,257,MMI Facial Expression,facial expression recognition,"the extended CohnKanade database and test on the MMI facial expression database [53].  Since most of the image sequences on the MMI database have only single AU active, we only",No DOI,IEEE Transactions on image …,https://pubmed.ncbi.nlm.nih.gov/23529088/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,nih.gov,
205,207,Single sample face recognition via learning deep supervised autoencoders,"['S Gao', 'Y Zhang', 'K Jia', 'J Lu']",2015,265,Toronto Face Database,"deep learning, machine learning","based deep neural networks, and driven by the SSPP face  a supervised auto-encoder to  build the deep neural network.  the Toronto Face Database, which is a very large dataset to",No DOI,IEEE transactions on …,https://ieeexplore.ieee.org/document/7124463,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
206,208,Spatio-temporal convolutional features with nested LSTM for facial expression recognition,"['Z Yu', 'G Liu', 'Q Liu', 'J Deng']",2018,158,Oulu-CASIA,classification,"Oulu-CASIA: We also consider for experiments the Oulu-CASIA dataset, which is a little bit  more  This dataset is more difficult to classify than CK+. In the cases of disgust, fear, happiness",No DOI,Neurocomputing,https://www.sciencedirect.com/science/article/pii/S0925231218308634,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
207,209,Spatio-temporal facial expression recognition using convolutional neural networks and conditional random fields,"['B Hasani', 'MH Mahoor']",2017,116,"MMI Facial Expression, Static Facial Expression in the Wild, Toronto Face Database","FER, facial expression recognition, neural network","a network for the task of facial expression recognition in videos. The first part of our network  is a Deep Neural Network  are used to evaluate the network: CK+, MMI, and FERA. We show",No DOI,… Conference on Automatic Face & …,https://ieeexplore.ieee.org/document/7961822,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
208,210,"Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark","['A Dhall', 'R Goecke', 'S Lucey']",2011,706,"Acted Facial Expressions In The Wild, Static Facial Expression in the Wild","FER, classification, classifier, facial expression recognition, machine learning","and testing protocol for expression recognition as part of the BEFIT work Acted Facial  Expressions in the Wild (AFEW) [9]. Therefore, we name it the Static Facial Expressions in the Wild (",No DOI,2011 IEEE international …,https://ieeexplore.ieee.org/document/6130508,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
209,211,Static topographic modeling for facial expression recognition and analysis,"['J Wang', 'L Yin']",2007,119,MMI Facial Expression,facial expression recognition,Maja Pantic’s group at Imperial College London for providing the MMI facial expression  database. This work is supported in part by the National Science Foundation under Grants IIS-,No DOI,Computer Vision and Image Understanding,https://www.sciencedirect.com/science/article/pii/S107731420600227X,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
210,212,Subject independent facial expression recognition with robust face detection using a convolutional neural network,"['M Matsugu', 'K Mori', 'Y Mitari', 'Y Kaneda']",2003,975,Affective Faces Database,neural network,"convolutional network architecture for robust face detection and  in the CNN, between neutral  and emotional faces. We show that the  neural networks for facial expression recognition.",No DOI,Neural networks,https://www.sciencedirect.com/science/article/pii/S0893608003001151,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
211,213,Supervised committee of convolutional neural networks in automated facial expression analysis,"['G Pons', 'D Masip']",2017,119,"Affective Faces Database, MMI Facial Expression","CNN, classification, neural network",Learning could improve facial expression classification tasks.  application of deep CNNs to  facial expression classification [6].  In this experiment we describe this configuration as MMI+,No DOI,IEEE Transactions on Affective Computing,https://www.computer.org/csdl/journal/ta/2018/03/08039231/13rRUx0xPgA,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,computer.org,
212,214,Temporal multimodal fusion for video emotion classification in the wild,"['V Vielzeuf', 'S Pateux', 'F Jurie']",2017,209,Expression in-the-Wild,classification,EmotioNet Challenge: Recognition of facial expressions of emotion in the wild. arXiv preprint  arXiv: Emotion recognition in the wild with feature fusion and multiple kernel learning. In,No DOI,Proceedings of the 19th ACM International …,https://arxiv.org/abs/1709.07200,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"Temporal Multimodal Fusion
for Video Emotion Classification in the Wild
ValentinVielzeuf∗ StéphanePateux FrédéricJurie
OrangeLabs OrangeLabs NormandieUniv.,UNICAEN,
Cesson-Sévigné,France Cesson-Sévigné,France ENSICAEN,CNRS
valentin.vielzeuf@orange.com stephane.pateux@orange.com Caen,France
frederic.jurie@unicaen.fr
ABSTRACT
Angry
Thispaperaddressesthequestionofemotionclassification.The
taskconsistsinpredictingemotionlabels(takenamongasetof
possiblelabels)bestdescribingtheemotionscontainedinshort Disgust
videoclips.Buildingonastandardframework–lyingindescribing
videosbyaudioandvisualfeaturesusedbyasupervisedclassifier
Fear
toinferthelabels–thispaperinvestigatesseveralnoveldirections.
Firstofall,improvedfacedescriptorsbasedon2Dand3DConvo-
lutionalNeuralNetworksareproposed.Second,thepaperexplores Happy
severalfusionmethods,temporalandmultimodal,includinganovel
hierarchicalmethodcombiningfeaturesandscores.Inaddition,we
Sad
carefullyreviewedthedifferentstagesofthepipelineanddesigned
aCNNarchitectureadaptedtothetask;thisisimportantasthesize
ofthetrainingsetissmallcomparedtothedifficultyoftheproblem, Neutral
makinggeneralizationdifficult.Theso-obtainedmodelranked4th
atthe2017EmotionintheWildchallengewiththeaccuracyof
Surprise
58.8%.
KEYWORDS
Figure1:The7emotionsoftheAFEWdataset:eachrowrep-
EmotionRecognition;MultimodalFusion;RecurrentNeuralNet-
resentsanemotionwithasetoffacessampledacrossarep-
work;DeepLearning;
resentativevideoclip.Pleasenotethatifmostofthevideo
clipsdocontainfaces,someofthemdon’t.
1 INTRODUCTIONANDRELATEDWORK
Emotionrecognitionisatopicofbroadandcurrentinterest,useful
straightforwardwaytodoit,itcanbemoreinterestingtorepresent
formanyapplicationssuchasadvertising[27]orpsychological
emotionsbytheirdegreesofarousalandvalence,asproposedin[6].
disordersunderstanding[35].Itisalsoatopicofimportancefor
Intherestrictedcaseoffacialexpressions,actionunitscanalsobe
researchinotherarease.g.,videosummarization[37]orfacenor-
used,focusingontheactivationofdifferentpartsoftheface[15].
malization(expressionremoval).Evenifemotionrecognitioncould
Linkscanbemadebetweenthesetworepresentations:thediscrete
appeartobeanalmostsolvedprobleminlaboratorycontrolled
classescanbemappedintothearousalvalencespace[28]andcan
conditions,therearestillmanychallengingissuesinthecaseof
bededucedfromtheactionunits[24].
videosrecordedinthewild.
Anotherimportantpartoftheliteraturefocusesonthewaysto
Thispaperfocusesonthetaskofemotionclassificationinwhich
representaudio/videocontentsbyfeaturesthatcanbesubsequently
eachvideocliphastobeassignedtooneandonlyoneemotion,
usedbyclassifiers.Earlypapersmakeuseof(i)hand-craftedfea-
basedonitsaudio/videocontent.Theclassesareusuallythesix
turessuchasLocalBinaryPatterns(LBP),Gaborfeatures,discrete
basicemotionsi.e.,anger,disgust,fear,happiness,sadnessandsur-
cosinetransformforrepresentingimages,andLinearPredictive
prise,inadditiontotheneutralclass,asintheAudio-videoEmotion
Codingcoefficients(LPC),relativespectraltransform-linearper-
Recognitionsub-taskoftheEmotionRecognitionintheWildChal-
ceptualprediction(RASTA-PLP),modulationspectrum(ModSpec)
lenge[12].Moreprecisely,thispaperpresentsourmethodology,
orEnhancedAutoCorrelation(EAC)foraudio,(ii)standardclas-
experimentsaswellastheresultsweobtainedinthe2017edition
sifierssuchasSVNorKNNforclassification(see[22]fordetails).
oftheEmotionRecognitionintheWildChallenge[10].
[38],thewinnerofEmotiW’15,demonstratedtherelevanceofAc-
Emotionrecognitionhasreceivedalotofattentioninthesci-
tionUnitsfortherecognitionofemotions.[3]wasamongthefirst
entificliterature.Onelargepartofthisliteraturedealswiththe
toproposetolearnthefeaturesinsteadofusinghand-craftedde-
possibleoptionsfordefiningandrepresentingemotions.Iftheuse
scriptors,relyingonDeepConvolutionalNetworks.Morerecently,
ofdiscreteclasses[31]suchasjoy,fear,angriness,etc.isthemost
[18],thewinnerofEmotiW’16,hasintroducedtheC3Dfeature
∗AlsowithNormandieUniv.,UNICAEN,ENSICAEN,CNRS. whichisanefficientspatio-temporalrepresentationoffaces.
7102
peS
12
]VC.sc[
1v00270.9071:viXraTheliteratureonemotionrecognitionfromaudio/videocontents
alsoaddressesthequestionofthefusionofthedifferentmodalities.
Amodalitycanbeseenasoneofthesignalsallowingtoperceivethe
emotion.Amongtherecentmethodsforfusingseveralmodalities,
wecanmentiontheuseoftwo-streamConvNets[32],ModDrop
[29]orMultipleKernelFusion[9].Themostusedmodalities,inthe
contextofemotionrecognition,arefaceimagesandaudio/speech,
evenifcontextseemsalsotobeoftremendousimportance[5].For
instance,thegeneralunderstandingofthescene,evenbasedonsim-
plefeaturesdescribingthewholeimage,mayhelptodiscriminate
betweentwocandidateclasses.
Asmostoftherecentmethodsforemotionrecognitionaresuper-
visedandhencerequiressometrainingdata,theavailabilityofsuch Figure2:Comparisonoftheboundingboxesgivenbyourde-
resourcesisbecomingmoreandmorecritical.Severalchallenges tection/normalizationprocedure(colorimages)withthose
havecollectedusefuldata.TheAVECchallenge[34]focuseson provided with the AFEW dataset (gray-scale images), on 4
theuseofseveralmodalitiestotrackarousalandvalenceinvideos random faces of AFEW. Our bounding boxes are slightly
recorded in controlled conditions. The Emotionet challenge [7] larger.
proposesadatasetofonemillionimagesannotatedwithaction
unitsandpartiallywithdiscretecompoundemotionclasses.Finally,
theEmotionintheWildchallenge[11]dealswiththeclassifica- The rest of the paper is organized as follows: a presentation
tionofshortvideoclipsintosevendiscreteclasses.Thevideosare oftheproposedmodelisdoneinSection2,detailingthediffer-
extractedfrommoviesandTVshowsrecorded""inthewild"".The entmodalitiesandthefusionmethods.ThenSection3presents
abilitytoworkwithdatarecordedinrealisticsituations,including theexperimentalvalidationaswellastheresultsobtainedonthe
occlusions,poorilluminationconditions,presenceofseveralpeople validationandtestsetsduringthechallenge.
orevenscenebreaksisindeedveryimportant.
Asaforementioned,thispaperdealswithourparticipationto 2 PRESENTATIONOFTHEPROPOSED
theEmotionIntheWild2017(EmotiW)challenge.Webuildon APPROACH
thestate-of-the-artpipelineof[18]inwhichi)audiofeaturesare
Figure3presentsanoverviewofourapproach,whichisinspiredby
extractedwiththeOpenSmiletoolkit[16],ii)twovideofeatures
theoneof[18].Modalitiesweconsiderareextractedfromaudioand
arecomputed,onebytheC3Ddescriptor,theotherbytheVGG16-
video,associatedwithfacesanalyzedwithtwodifferentmodels(2D
Facemodel[30]fine-tunedwithFER2013faceemotiondatabase
CNN,C3D).Thetwomaincontributionsarethetemporalfusion
andintroducedintoaLSTMnetwork.Eachoneofthese3features
andthenovelC3D/LSTMdescriptor.
isprocessedbyitsownclassifier,theoutputofthe3classifiers
Onoverall,ourmethodworksasfollows:ontheonehand,the
beingcombinedthroughlatefusion.Startingfromthispipeline,
OpenSmilelibrary[16]isusedtoproduce1582dimensionalfeatures
weproposetoimproveitin3differentways,whicharethemain
usedbyatwo-layerperceptrontopredictclassesaswellascompact
contributionsofourapproach.
descriptors(279-dvectors)fromaudio.Ontheotherhand,video
First,therecentliteraturesuggeststhatlatefusionmightnotbe
classificationisbasedonfaceanalysis.Afterdetectingthefacesand
theoptimalwaytocombinethedifferentmodalities(seee.g.,[29]).
normalizingtheirappearance,onesetoffeaturesiscomputedwith
Thispaperinvestigatesdifferentdirections,includinganoriginal
theVGG-16model[30]whileanothersetoffeaturesisobtainedby
hierarchicalapproachallowingtocombinescores(latefusion)and
theC3Dmodel[18].Inbothcases,4096-dfeaturesareproduced.
features(earlyfusion)atdifferentlevels.Itcanbeseenasaway
Temporalfusionoftheseso-obtaineddescriptionsisdoneacross
tocombineinformationatitsoptimallevelofdescription(from
theframesofthesequences,producingpermodalityscoresand
features to scores). This representation addresses an important
compactdescriptors(304dimensionalvectors).The3modalities
issueoffusion,whichistoensurethepreservationofunimodal
(audio,VGG-facesandC3D-faces)arethencombinedusingboth
informationwhilebeingabletoexploitcross-modalinformation.
thescorepredictionsandthecompactrepresentations.
Second,weinvestigateseveralwaystobetterusethetemporal
After explaining how the faces are detected and normalized,
informationinthevisualdescriptors.Amongseveralcontributions,
therestofthesectiongivesmoredetailsonhoweachmodalityis
weproposeanoveldescriptorcombiningC3DandLSTM.
processed,andhowthefusionisperformed.
Third,itcanbeobservedthattheamountoftrainingdata(773
labeledshortvideoclips)israthersmallcomparedtothenumberof
2.1 FaceDetectionandAlignment
theparametersofstandarddeepmodels,consideringthecomplexity
and diversity of emotions. In this context, supervised methods TheEmotiWchallengeprovidesfacedetectionsforeachframeof
arepronetoover-fitting.Weshowinthepaperhowtheeffectof eachvideo.However,wepreferrednottousetheseannotations
over-fittingcanbereducedbycarefullychoosingthenumberof buttodetectthefacesourselves.Themotivationistwofold.First,
parametersofthemodelandfavoringtransferlearningwhenever wewanttobeabletoprocessanygivenvideoandnotonlythose
itispossible. ofEmotiW(e.g.,foraddingexternaltrainingdata).Second,itis
necessarytomasterthefacealignmentprocessforprocessingthe
21582 features
1
Fully
OpenSmile
connected
279 features
4096 features
Temporal 2
VGG-face
frame by frame fusion
304 features
4096 features
Overlap
C3D of 8 Temporal 3
frames
by window of 16 consecutive fusion
frames
304 features
No overlap
4
Softmax score of Temporal
size 7 fusion
Features vector
297 features
Figure3:Ourmodelincludesaudio,VGG-LSTMandC3D-LSTM(withorwithoutoverlapping)modalities.Eachmodalitygives
itsownfeaturevectorsandscores,whichcanbefusedusingdifferentmethods.Itcanbenotedthatthedimensionsofthe
featuresvectorsarechosentohavebalancedcontributionsbetweenmodalities,makingatradeoffbetweenthenumberof
parametersandtheperformance.
imagesoffacedatasetswhenpre-trainingtheVGGmodels(see 2.2 Audiofeaturesandclassifier
Section2.3whereFERdatasetisusedtopre-traintheVGGmodel). TheaudiochannelofeachvideoisfedintotheOpenSmiletoolkit[16]1,
Forfacedetection,weuseaninternaldetectorprovidedbyOr- asmostoftheEmotiWcompetitors[11],togetadescriptionvector
angeLabs.Wefoundoutfromourobservationsthatthisfacede- oflength1582.
tectordetectsmorefaces(20597versus19845onthevalidationset) Acommonlyusedapproachforaudioisthentolearnaclassifier
whilehavingalowerfalsepositiverate(179falsepositivesversus ontopoftheOpenSmilefeatures.SupportVectorMachine[18][38]
908 on the validation set) than the one use to provide EmotiW seemstobethedominantchoicefortheclassification,evenifthere
annotations. aresomeotherapproacheslikeRandomForests[17].
Thisdetectorisappliedframeperframe.Ifoneorseveralfaces Tobeabletocontrolmorefinelythedimensionalityofthede-
aredetected,atrackingbasedontheirrelativepositionsallowsto scriptionvector(theoneusedlateronduringthefusionprocess),
defineseveralfacetracks.Inasecondtime,analignmentbasedon welearnatwo-layerPerceptronwithreLuactivationontheOpenS-
thelandmarks[26]isdone.Wechoosethelongestfacesequencein miledescription,usingbatchnormalisationanddropout.During
eachvideoandfinallyapplyatemporalsmoothingofthepositions inference,weextractforeachvideoadescriptionvectorofsize279
ofthefacestofilteroutjittering. –thehiddenlayeroftheperceptron–alongwiththesoftmaxscore.
Figure2comparesoneofournormalizedfaces(afterdetection/
alignment)withtheoneprovidedwiththechallengedata. 2.3 RepresentingFaceswith2DConvolutional
NeuralNetwork
Acurrentpopularmethod(seee.g.,[18,23])forrepresentingfaces
inthecontextofemotionrecognition,especiallyintheEmotiW
1Weusetheemobase2010configurationfile.
3challenge,istofine-tuneapre-trained2DCNNsuchastheVGG-
facemodel[30]ontheimagesofanemotionimagesdataset(e.g.,the Overlap of
FER2013dataset).Usingapre-trainedmodelisawaytobalance 8 frames
therelativelysmallsizeoftheEmotiWdataset(AFEWdataset).
The images of the FER 2013 dataset [20] are first processed by
detectingandaligningthefaces,followingtheprocedureexplained
inSection2.1.Wethenfine-tunetheVGG-facemodelonFER2013 Window of 16 frames
dataset, using both the training and the public test set; during Window of 16 frames
trainingweusedataaugmentationbyjitteringthescale,flipping
androtatingthefaces.Theaimistomakethenetworkmorerobust
tosmallmisalignmentofthefaces.Wealsoapplyastrongdropout
onthelastlayeroftheVGG(keepingonly5%ofthenodes)to
preventover-fitting.Weachieveaperformanceof71.2%onthe
Figure4:Inthecaseof8framesoverlap,wecanseethatthe
FERprivatetestset,whichisslightlyhigherthanthepreviously
16-framewindowsaresharinghalfoftheirfaces.
publishedresults[18,20].
Toassessthequalityofthedescriptiongivenbythisfine-tuned
VGGmodeltoemotionrecognition,wefirstbenchmarkeditonthe
validationsetofSFEW[13],whichisadatasetcontainingframes Duringinference,thefacesequencesdetectedinAFEWvideos
ofthevideosofAFEW(thoseusedbytheEmotiWchallenge).We areresizedto112x112.Wethensplitavideointoseveralwindows
achievedascoreof45.2%withoutretrainingthemodelonSFEW, of16consecutiveframes2,withorwithoutoverlappingbetween
whilethestate-of-the-artresult[25]is52.5%,usingacommitteeof thewindows(asshowninFigure4),andfeditintotheweighted
deepmodels,andthechallengebaselineisof39.7%[13]. C3D.Wethenextractthesecond4096-dfullyconnectedlayerand
The face sequences detected in AFEW videos are resized to thelastsoftmaxlayeroftheC3Dforeachwindowofthevideos.
224x224 and fed into the fine-tuned VGG-face model. We then ThemethodweproposebearssimilaritieswithMultipleInstance
extractthe4096lengthfc6layer,followingthesamepipelineasFan Learning(MIL)[1].WithintheframeworkofMIL,eachvideocanbe
etal.[17,18].Wealsocomputethesoftmaxscoreforeachframe. consideredasabagofwindows,withonesinglelabel.Astraightfor-
wardwaytoapplyMILwouldbetotraintheC3Doneachvideoas
2.4 RepresentingFaceswith3DConvolutional abagofwindows,andaddafinallayerforchoosingtheprediction
NeuralNetwork astheonewiththemaximumscoreamongallthescoresofthe
batch.Thelosswouldbethencomputedfromthisprediction.The
3Dconvolutionalneuralnetworkshavebeenshowntogivegood
weightsdefinedinEq.(1)playthisrolebyselectingiterativelythe
performance in the context of facial expressions recognition in
bestscoringwindows.
video [2, 33]. Fan et al. [18] fine-tuned a pre-trained ’sport1m’
modelonrandomlychosenwindowsof16consecutivefaces.During
2.5 PerModalityTemporalFusion
inference,themodelisappliedtothecentralframeofthevideo.
Onelimitationofthisapproachisthat,attesttime,thereisno BothVGGandweightedC3Drepresentationsareappliedtoeach
guarantythatthebestwindowforcapturingtheemotionisinthe frameofthevideos,turningthevideosintotemporalsequencesof
middleofthevideo.Thesameproblemoccursduringtraining:a visualdescriptors.Wewillnametheelementsofthosesequences
largepartofthewindows(randomly)selectedfortrainingdoes as""descriptors"",whetheritisthedescriptionofaframeorofa
notcontainanyemotionordoesnotcontainthecorrectemotion. window.
Indeed,videosareannotatedasawholebutsomeframescanhave Toclassifythesesequencesofdescriptors,weinvestigatedsev-
differentlabelsor,sometimes,noexpressionatall. eralmethods.Themoststraightforwardoneistoscoreeachde-
Weaddressthislimitationinthefollowingway:wefirstfine- scriptorandtotakethemaximumofthesoftmaxscoresasthefinal
tuneaC3D-sport1mmodelusingallthewindowsofeachvideo, prediction.Similarly,themaximumofthemeansofthesoftmax
optimizingtheclassificationperformanceuntilthebeginningof acrosstimecanalsobeconsidered.
convergence.Then,tobeabletolearnmorefromthemostmean- To better take into account temporal dependencies between
ingfulwindowsthanfromtheothers,weweigheachwindowbased consecutivedescriptors,anotheroptionistouseLongShort-Term
onitsscores.Moreprecisely,fortheith videoandthejth window Memoryrecurrentneuralnetworks(LSTM)[2,19].Unlike[17,18],
ofthisvideo,atepocht,theweightwi,j iscomputedas: wechosetouseavariablelengthLSTMallowingustotakeallthe
descriptorsasinputs.
−si,j Topreventover-fitting,wealsoapplieddropouttotheLSTM
wi,j =e T(t) (1)
cellsandweightdecaytothefinalfullyconnectedlayer.Arandom
gridsearchonhyper-parametersisthenappliedforeachoneof
whereT(t)isatemperatureparameterdecreasingwithepochtand
thesemodels.
si,j,thescoreofthewindowj ofthevideoi.Wethennormalize
theweighttoensurethatforeachvideoi,(cid:205) jwi,j =1.Arandom
gridsearchonthehyper-parameters,includingthetemperature 2Thesequenceswithfewerthan16frameswerepaddedbythemselvestoreacha
descent,ismadeonthevalidationset. sufficientlength.
4BothVGG-LSTMandC3D-LSTMareusedtogiveonedescription
vector(outputofthefinalfullyconnectedlayeroftheLSTM)and M1
onesoftmaxscoreforeachvideo.
ThefinalVGG-LSTMarchitecturehas2230hiddenunitsforeach h1
LSTM-cellandafinalfullyconnectedlayerwith297hiddenunits. Fully Fully
Themaximallengthoftheinputsequenceisof272frames.
M2 connected h2 connected
ThefinalC3D-LSTMarchitecturehas1324hiddenunitsforeach
h3
LSTM-cellandafinalfullyconnectedlayerof304hiddenunits.The
maximallengthoftheinputsequenceisof34windows(overlapof M3
8frames)or17windows(nooverlap).
Hidden Score
Modalities features representation
2.6 Multimodalfusion
Figure5:FullyconnectedmodelwithModDrop.Thethree
Lastbutnottheleast,thedifferentmodalitieshavetobeefficiently
modalitiesareconcatenatedandafullyconnectedwithmod-
combinedtomaximizetheoverallperformance.
Dropisapplied.Theobtainedhiddenrepresentationisthen
AsexplainedinSection1,twomainfusionstrategiescanbeused
fedtoasecondregularfullyconnectedlayer,whichwillout-
i.e.scorefusion(latefusion),whichconsistsinpredictingthelabels
putthescores.
basedonthepredictionsgivenbyeachmodality,orfeaturesfusion
(earlyfusion),whichconsistsintakingasinputthelatentfeatures
vectorsgivenbyeachmodalityandlearningaclassifieronthetop Tobemoreformal,letnbethenumberofmodalities,W1bethe
ofthem. weightsmatrixofthefirstlayer.Itcanbedividedinton2weights
Duringthelasteditionsofthechallenge,mostofthepapers blockmatricesW1 ,modelingunimodalandintermodalcontri-
k,l
focusedonscorefusion,usingSVM[4],MultipleKernelFusion[9] butions,withkandl,rangingoverthenumberofmodalitiesn.The
orweightedmeans[18].Differently,severalauthorstriedtotrain firstfullyconnectedequationcanbewrittenas:
audioandimagemodalitiestogether(seee.g.,[8]),combiningearly
featuresandusingsoftattentionmechanism,buttheydidn’tachieve
h1 W11,1 W11,2 W11,3 M1
state-of-the-artperformance.Weproposeanapproachcombining
(cid:169) (cid:173)h2(cid:170) (cid:174)=(cid:169) (cid:173)W12,1 W12,2 W12,3(cid:170) (cid:174)(cid:169) (cid:173)M2(cid:170)
(cid:174)
both.
(cid:171)h3
(cid:172)
(cid:171)W13,1 W13,2 W13,3 (cid:172)(cid:171)M3
(cid:172)
Therestofthesectiondescribesthefourdifferentfusionmethods Then,thetermtoaddtothelossissimply(withγ decreasing
md
weexperimentedwith,includinganovelmethod(denominatedas throughthetime):
(cid:213)
scoretrees). γ md ||W1 k,l|| 2
k(cid:44)l
Baselinescorefusion. Weexperimentedwithseveralstandard SettingγSD toveryhighvaluesduringthefirstiterationsleads
scorefusion,likemajorityvoting,meansofthescores,maximum tozeroingnon-diagonalblockmatrices.Loweringitlaterreintro-
ofthescoresandlinearSVM. ducesprogressivelythesecoefficients.Fromourobservation,this
approachprovidedbetterconvergenceontheconsideredproblem.
Fullyconnectedneuralnetworkwithmodalitydrop. Wealsoex-
Scoretrees. Ourmotivationistocombinethehigh-levelinfor-
perimented with the ModDrop method of Neverova et al. [29].
mationcomingfromthescoreswiththelower-levelinformation
It consists in applying dropout by modality, learning the cross-
comingfromthefeatures,alltogether.Wediditbybuildingwhat
modalitycorrelationswhilekeepingunimodalinformation.[29]
wecallScoreTrees(seeFigure6foranillustration).
reportsstate-of-the-artresultsongesturerecognition.Weapply
Afullyconnectedclassificationneuralnetworkisappliedsepa-
this method to our audio, C3D-LSTM and VGG-LSTM features,
ratelytothefeaturesofthedifferentmodalities,outputtingavector
asshowninFigure5.Accordingto[29],thisismuchbetterthan
ofsize7.Thisvectoristhenconcatenatedwiththescoresofthe
simplyfeedingtheconcatenationofthemodalitiesfeaturesinto
twoothermodalities,tocreateavectorofsize21.Afullyconnected
afullyconnectedneuralnetworkandlettingthenetworklearna
classificationneuralnetworkisthenfedwithitandoutputsapre-
jointrepresentation.Indeed,thefullyconnectedmodelwouldbe
dictionvectorofsize7.Theaimistomakepredictionswithrespect
unabletopreserveunimodalinformationduringcross-modality
tothepredictionscomingfromothermodalities.Finally,thesethree
learning.
newpredictionvectorsareconcatenatedandfedintoalastfully
AnimportantsteptomakeconvergencepossiblewithModDrop
connectedclassifier,whichgivestheoverallscores.Thismethod
istofirstlearnthefusionwithoutcross-modality.Forthisreason,
canbegeneralizedtoanynumberofmodalities.
[29]conditionedtheweightmatrixofthefirstlayersothatthe
diagonalblocksareequaltozerosandreleasedthisconstraintafter WeightedMean. Theweightedmeanistheapproachofthewin-
awell-chosennumberofiterations. nersofthe2016edition[18].Itconsistsinweightingthescoreof
Towarrantythepreservationoftheunimodalinformation,we eachmodalityandsumthemup.
exploreanalternativemethodwhichturnedouttobebetter:we Theweightsarechosenbycrossvalidationonthevalidationset,
applyanadaptedweightdecay,onlyonthenon-diagonalblocks, selectingtheonesgivingthebestperformance.
anddecreaseditscontributiontothelossthroughtime. WeapplieditontheVGG-LSTM,C3D-LSTMandaudiomodels.
5Training Validation Test
Angry 133(17.2%) 64(16.7%) 99(15.2%)
Disgust 74(9.6%) 40(10.4%) 40(6.1%)
Fear 81(10.4%) 46(12%) 70(10.7%)
Fully connected
Happy 150(19.4%) 63(16.4%) 144(22%)
Sad 117(15.1%) 61(15.9%) 80(12.3%)
Neutral 144(18.6%) 63(16.4%) 191(29.2%)
Surprise 74(9.6%) 46(12%) 29(4.4%)
Total 773 383 653
Table1:AFEW7.0:numberofvideosequencesperclass.
Fully Fully Fully
connected connected connected
theboundingboxesandtimestampsarekept.Wethenextractcan-
didatestemporalwindowsoftensecondsaroundallofthesetime
stampsandaskedhumanannotatorstoselectandannotatethemost
1 2 3 1 2 3 1 2 3
relevantones.Toensurethequalityofourannotations,weevalu-
atedourselves(ashumanbeings)onthevalidationset.Wereached
aperformancefrom60%to80%dependingontheannotator,which
Fully Fully Fully
connected connected connected iscompatiblewiththefigureof60%observedby[22].
3.2 ExperimentsonSingleModalities
Eachmodalityhasbeenevaluatedseparatelyonthevalidationset
1 2 3
ofAFEW.TheVGG-LSTMandC3D-LSTMmodalitiesperforms
Figure6:TheScoreTreearchitecture betterthan[11].
RegardingtheVGG-LSTMandtheC3D-LSTM,bothunidirec-
tionalandbidirectionalLSTMarchitectures[21]wereevaluated,
3 EXPERIMENTALVALIDATIONAND withoneortwolayers.
Inseveralrecentpapers[3,14],bidirectionalLSTMisclaimed
RESULTS
tobemoreefficientthantheunidirectionaloneandcouldbeseen
AfterintroducingtheAFEWdataset(thedatasetofthechallenge), asawaytoaugmentthedata.Nevertheless,inourcase,wehave
thissectionpresentstheexperimentalvalidationofourmethod. observedthatthebidirectionalLSTMwaspronetoover-fittingon
Wefirstpresentexperimentsdoneonthemodalitiestakensepa- thetrainingsetandthereforedoesnotperformwellonthevalida-
rately,andthenpresentexperimentsontheirfusion.Wefinally tionset.Thesameobservationhasbeenmadewhenincreasingthe
introducetheexperimentsdonefortheEmotiW’17challengeand numberoflayers.Thebestperformingarchitectureisinbothcases
theperformanceweobtained. aone-layerunidirectionalLSTM.
3.1 TheActedFacialEmotionintheWild 3.2.1 VGG-LSTM. TheVGGmodel(withoutLSTM)hasfirst
been evaluated by taking the maximum of the scores over the
dataset
sequences,givingtheaccuracyof41.4%onthevalidationset.
ActedFacialEmotionintheWild(AFEW)isthedatasetusedby ThenthedifferentLSTMarchitecturesweretestedandthebest
theEmotiWchallenge.The2017versionofAFEWiscomposed performanceforeachoneisgiveninTable2.Wenotea3%im-
of773trainingvideos,383validationvideosand653testvideos. provementcomparedtoFanetal.[18].Itcanbeexplainedbythe
Eachvideoislabeledwithoneemotionamong:’angry’,’disgust’, factthatourmodelusesthewholesequences,feedingthemodel
’fear’,’happy’,’sad’,’neutral’and’surprise’.Inadditiontothevideo, withmoreinformation.Dataaugmentationalsohelps.
croppedandalignedfacesextractedby[36,39]arealsoprovided.
3.2.2 C3D-LSTM. Wethenexperimentedourmethodwiththe
Anotherimportantspecificityofthisdatasetistheclassdistri-
C3Dmodalityalone.TheperformanceisgiveninTable3.
butionoftheTrain/Val/Testsubsets(asshowninTable1).This
WeobservethatourimplementationofC3Dtrainedonrandom
differencecanmaketheperformanceontheTestsetdifferentfrom
windowsandevaluatedoncentralwindowsisnotasgoodasFan
theoneoftheValidationset,assomeclassesaremorechallenging
etal.[18].TheC3Dtrainedoncentralwindowsperformedbetter
thanothers.
butisnotstate-of-the-arteither.
3.1.1 Externaltrainingdata. Toenlargethetrainingset,wecol- OurproposedC3D(withLSTM)hasbeentestedwithandwithout
lectedexternaldatabyselecting380videoclipsfromourpersonal overlappingbetweenwindows.ToevaluatetheweightedC3D,the
DVDs movie collection, after checking that there is no overlap predictionofthewindowwiththemaximalsoftmaxscoreamong
betweentheselectedmoviesandtheonesofAFEW[12].These the video is first taken. It performs better without overlapping,
movieswereprocessedbythefollowingpipeline:facesarefirst andweobservealowerdifferencebetweentrainingandvalidation
detectedusingourfacedetector(seeSection2.1fordetails),and accuracy. It could be explained by the fact that the number of
6Method Validation Fusionmethod Validation Test
accuracy accuracy accuracy
Maximumofthescores 41.4% Majorityvote 49.3% _
UnidirectionalLSTMonelayer 48.6% Mean 47.8% _
BidirectionalLSTMonelayer 46.7% ModDrop(sub.3) 52.2% 56.66%
UnidirectionalLSTMtwolayers 46.2% ScoreTree(sub.4) 50.8% 54.36%
BidirectionalLSTMtwolayers 45.2% Weightedmean(sub.6) 50.6% 57.58%
Fanetal.[18] 45.42% Table4:Performanceofthedifferentfusionmethodsonthe
Table 2: VGG-LSTM performance on the validation set of validationandtestsets.
AFEW.
Method Validation 3.4 OurparticipationtotheEmotiW’17
accuracy challenge
C3Doncentralwindow 38.7%
WesubmittedthemethodpresentedinthispapertotheEmotiW’17
C3Donrandomwindow 34%
challenge.Wesubmitted7runswhichperformanceisgiveninTable
WeightedC3D(nooverlap) 42.1%
5).
WeightedC3D(8framesoverlap) 40.5%
Thedifferencebetweentherunsareasfollows:
WeightedC3D(15framesoverlap) 40.1%
LSTMC3D(nooverlap) 43.2% • Submission2:ModDropfusionofaudio,VGG-LSTManda
LSTMC3D(8framesoverlap) 41.7% LSTM-C3Dwithan8-frameoverlap.
• Submission3:additionofanotherLSTM-C3D,withnoover-
Fanetal.[18] 39.7%
lap,improvingtheperformanceonthetestsetaswellason
Table3:C3DandC3D-LSTMperformanceonthevalidation
thevalidationset.
setofAFEW.
• Submission4:fusionbasedonScoreTrees,didnotachieve
abetteraccuracyonthetestset,whileobservingaslight
improvementonthevalidationset.
• Submission5:additionofoneVGG-LSTMandtwoother
LSTM-C3D, one with 8 frames overlap and one without.
windowsislowerifthereisnooverlap,thechoicebetweenthe
Thesenewmodelswereselectedamongthebestresultsin
windowsisthereforeeasier.Asasecondobservation,theuseof
ourrandomgridsearchonhyper-parametersaccordingto
LSTMwiththeweightedC3Dleadstothehighestscores.
theirpotentialcomplementaritydegree,evaluatedbymea-
Attheend,itcanbeobservedthatourC3Ddescriptorperforms
suringthedissimilaritybetweentheirconfusionmatrices.
significantlybetterthantheoneof[18].
ThefusionmethodisModDrop.
3.2.3 Audio. Theaudiomodalitygaveaperformanceof36.5%, • Submission6:weightedmeanfusionofallthepreceding
lowerthanthestate-of-the-artmethod(39.8%)[17].Theuseofa modalities,givingagainof1%onthetestset,whilelosing
perceptronclassifier(worsethantheSVM)neverthelessallowedus onepercentonthevalidationset,highlightinggeneralization
tousehigh-levelaudiofeaturesduringfusion. issues.
• Submission 7: our best submission, which is the same as
3.3 ExperimentsonFusion thesixthforthemethodbutwithmodelstrainedonboth
trainingandvalidationsets.Thisimprovestheaccuracyby
Table4summarizesthedifferentexperimentswemadeonfusion.
1.2%.Thisimprovementwasalsoobservedintheformer
Thesimplebaselinefusionstrategy(majorityvoteormeansof
editionsofthechallenge.Surprisingly,addingourowndata
thescores)doesnotperformaswellastheVGG-LSTMmodality
didn’tbringsignificantimprovement(gainoflessthanone
alone.
percentonthevalidationset).Thiscouldbeexplainedby
The proposed methods (ModDrop and Score Tree) achieved
thefactthatourannotationsarenotcorrelatedenoughwith
promising results on the validation set, but are not as good as
theAFEWannotation.
thesimpleweightedmeanonthetestset.Thiscanbeexplainedby
thelargestnumberofparametersusedforthemodDropandthe Theproposedmethodhasbeenranked4thinthecompetition.
scoretree,andbythefactthatsomeparameterscrossvalidatedon Weobservedthat,thisyear,theimprovementofthetopaccuracy
thevalidationset. comparedtothepreviouseditionsissmall(+1.1%),whilefrom2015
Thebestperformanceobtainedonthevalidationsethasthe to2016theimprovementwasof+5.2%.Thismightbeexplainedby
accuracyof52.2%,whichissignificantlyhigherthantheperfor- thefactthatthemethodsaresaturating,convergingtowardshuman
manceofthebaselinealgorithmprovidedbytheorganizers–based performance(whichisassumedtobearound60%).However,the
oncomputingLBPTOPdescriptorandusingaSVR–givingthe performanceoftophumanannotators(whoseaccuracyishigher
accuracyof38.81%onthevalidationset[10]. than70%)meansthereisstillsomeroomforimprovement.
7Submission TestAccuracy REFERENCES
2 55.28% [1] SaadAliandMubarakShah.2010.Humanactionrecognitioninvideosusing
3 56.66% kinematicfeaturesandmultipleinstancelearning.IEEEtransactionsonpattern
analysisandmachineintelligence32,2(2010),288–303.
4 54.36%
[2] MoezBaccouche,FranckMamalet,ChristianWolf,ChristopheGarcia,andAtilla
5 56.51% Baskurt.2011.Sequentialdeeplearningforhumanactionrecognition.InInter-
6 57.58% nationalWorkshoponHumanBehaviorUnderstanding.Springer,29–39.
[3] MoezBaccouche,FranckMamalet,ChristianWolf,ChristopheGarcia,andAtilla
7 58.81% Baskurt.2012.Spatio-TemporalConvolutionalSparseAuto-EncoderforSequence
Table5:Performanceofoursubmissionsonthetestset. Classification.InBMVC.
[4] SarahAdelBargal,EmadBarsoum,CristianCantonFerrer,andChaZhang.2016.
Emotionrecognitioninthewildfromvideosusingimages.InProceedingsofthe
18thACMInternationalConferenceonMultimodalInteraction.ACM,433–436.
[5] LisaFeldmanBarrett,BatjaMesquita,andMariaGendron.2011. Contextin
emotionperception. CurrentDirectionsinPsychologicalScience20,5(2011),
Angry Disgust Fear Happy Neutral Sad Surprise 286–290.
[6] LisaFeldmanBarrettandJamesARussell.1999.Thestructureofcurrentaffect:
Controversiesandemergingconsensus.Currentdirectionsinpsychologicalscience
Angry 74,5 0 3,1 3,1 18,4 1 0 8,1(1999),10–14.
[7] CFabianBenitez-Quiroz,RamprakashSrinivasan,QianliFeng,YanWang,and
AleixMMartinez.2017.EmotioNetChallenge:Recognitionoffacialexpressions
Disgust 20 0 5 15 40 20 0 ofemotioninthewild.arXivpreprintarXiv:1703.01210(2017).
[8] LinlinChao,JianhuaTao,MinghaoYang,YaLi,andZhengqiWen.2016.Audio
visualemotionrecognitionwithtemporalalignmentandperceptionattention.
Fear 27,1 0 34,3 1,4 21,4 15,7 0 arXivpreprintarXiv:1603.08321(2016).
[9] JunKaiChen,ZenghaiChen,ZheruChi,andHongFu.2014.Emotionrecognition
inthewildwithfeaturefusionandmultiplekernellearning.InProceedingsofthe
Happy 6,9 0 0,7 82,6 6,3 3,5 0 16thInternationalConferenceonMultimodalInteraction.ACM,508–513.
[10] AbhinavDhall,RolandGoecke,ShreyaGhosh,JyotiJoshi,JesseHoey,andTom
Gedeon.2017.FromIndividualtoGroup-levelEmotionRecognition:EmotiW5.0.
Neutral 8,3 0 2,1 5,7 69,9 14 0 InProceedingsofthe19thACMInternationalConferenceonMultimodalInteraction.
[11] AbhinavDhall,RolandGoecke,JyotiJoshi,JesseHoey,andTomGedeon.2016.
Emotiw2016:Videoandgroup-levelemotionrecognitionchallenges.InProceed-
Sad 12,5 0 7,5 13,8 25 41,3 0
ingsofthe18thACMInternationalConferenceonMultimodalInteraction.ACM,
427–432.
[12] AbhinavDhall,RolandGoecke,SimonLucey,andTomGedeon.2012.Collect-
Surprise 21,4 0 21,4 7,1 35,7 14,3 0
ingLarge,RichlyAnnotatedFacial-ExpressionDatabasesfromMovies. IEEE
MultiMedia(2012).
[13] AbhinavDhall,OVRamanaMurthy,RolandGoecke,JyotiJoshi,andTomGedeon.
Figure7:Confusionmatrixobtainedwiththeseventhsub- 2015.Videoandimagebasedemotionrecognitionchallengesinthewild:Emotiw
mission.Wecanseethat’disgust’and’surprise’classesare 2015.InProceedingsofthe2015ACMonInternationalConferenceonMultimodal
Interaction.ACM,423–426.
never predicted by our model, while the three dominant
[14] YongDu,WeiWang,andLiangWang.2015.Hierarchicalrecurrentneuralnet-
classes (’happy’, ’neutral’ and ’angry’) are well recognized. workforskeletonbasedactionrecognition.InProceedingsoftheIEEEconference
The’neutral’classhasthelargestnumberoffalsepositives. oncomputervisionandpatternrecognition.1110–1118.
[15] PaulEkmanandWallaceVFriesen.1977.Facialactioncodingsystem.(1977).
It underlines the difficulty, even for humans, to draw the [16] FlorianEyben,MartinWöllmer,andBjörnSchuller.2010.Opensmile:themunich
marginbetweenpresenceandabsenceofemotion.Rowsde- versatileandfastopen-sourceaudiofeatureextractor.InProceedingsofthe18th
ACMinternationalconferenceonMultimedia.ACM,1459–1462.
notetrueclasses,columnspredictedclasses.
[17] LijieFanandYunjieKe.2017. SpatiotemporalNetworksforVideoEmotion
Recognition.arXivpreprintarXiv:1704.00570(2017).
[18] YinFan,XiangjuLu,DianLi,andYuanliuLiu.2016. Video-basedemotion
recognitionusingcnn-rnnandc3dhybridnetworks.InProceedingsofthe18th
4 CONCLUSIONS
ACMInternationalConferenceonMultimodalInteraction.ACM,445–450.
[19] FelixAGers,JürgenSchmidhuber,andFredCummins.[n.d.].Learningtoforget:
This paper proposes a multimodal approach for video emotion
ContinualpredictionwithLSTM.TechnicalReport.
classification,combiningVGGandC3Dmodelsasimagedescrip- [20] IanJGoodfellow,DumitruErhan,PierreLucCarrier,AaronCourville,Mehdi
torsandexploresdifferenttemporalfusionarchitectures.Different Mirza,BenHamner,WillCukierski,YichuanTang,DavidThaler,Dong-Hyun
Lee,etal.2013.Challengesinrepresentationlearning:Areportonthreemachine
multimodalfusionstrategieshavealsobeenproposedandexperi-
learningcontests.InInternationalConferenceonNeuralInformationProcessing.
mentallycompared,bothonthevalidationandonthetestsetof Springer,117–124.
AFEW.AttheEmotiW’17challenge,theproposedmethodranked [21] AlexGraves,SantiagoFernández,andJürgenSchmidhuber.2005.Bidirectional
LSTMnetworksforimprovedphonemeclassificationandrecognition.Artificial
4th with the accuracy of 58.81 %, 1.5 % under the competition NeuralNetworks:FormalModelsandTheirApplications–ICANN2005(2005),753–
winners.Oneimportantobservationfromthiscompetitionisthe 753.
[22] MarkusKächele,MartinSchels,SaschaMeudt,GüntherPalm,andFriedhelm
discrepancybetweentheperformanceobtainedonthetestsetand
Schwenker.2016.RevisitingtheEmotiWchallenge:howwildisitreally?Journal
theoneonthevalidationset:goodperformanceonthevalidation onMultimodalUserInterfaces10,2(2016),151–162.
setisnotawarrantytogoodperformanceonthetestset.Reducing [23] HeysemKaya,FurkanGürpınar,andAlbertAliSalah.2017.Video-basedemotion
recognitioninthewildusingdeeptransferlearningandscorefusion.Imageand
thenumberofparametersinourmodelscouldhelptolimitoverfit-
VisionComputing(2017).
ting.Usingpre-trainedfusionmodelsand,moreover,gatheringa [24] PooyaKhorrami,ThomasPaine,andThomasHuang.2015.Dodeepneuralnet-
largersetofdatawouldalsobeagoodwaytofacethisproblem. workslearnfacialactionunitswhendoingexpressionrecognition?.InProceedings
oftheIEEEInternationalConferenceonComputerVisionWorkshops.19–27.
Finally,anotherinterestingpathforfutureworkwouldbetoadd [25] Bo-KyeongKim,JihyeonRoh,Suh-YeonDong,andSoo-YoungLee.2016. Hi-
contextualinformationsuchasscenedescription,voicerecognition erarchicalcommitteeofdeepconvolutionalneuralnetworksforrobustfacial
orevenmovietypeasanextramodality.
8expressionrecognition.JournalonMultimodalUserInterfaces10,2(2016),173–
189.
[26] DavisEKing.2009. Dlib-ml:Amachinelearningtoolkit. JournalofMachine
LearningResearch10,Jul(2009),1755–1758.
[27] AgataKołakowska,AgnieszkaLandowska,MariuszSzwoch,WioletaSzwoch,
andMichałRWróbel.2013.Emotionrecognitionanditsapplicationinsoftware
engineering.In2013The6thInternationalConferenceonHumanSystemInteraction
(HSI).IEEE,532–539.
[28] JeanKossaifi,GeorgiosTzimiropoulos,SinisaTodorovic,andMajaPantic.2017.
AFEW-VAdatabaseforvalenceandarousalestimationin-the-wild.Imageand
VisionComputing(2017).
[29] NataliaNeverova,ChristianWolf,GrahamTaylor,andFlorianNebout.2016.
Moddrop:adaptivemulti-modalgesturerecognition.IEEETransactionsonPattern
AnalysisandMachineIntelligence38,8(2016),1692–1706.
[30] OmkarMParkhi,AndreaVedaldi,AndrewZisserman,etal.2015. DeepFace
Recognition..InBMVC,Vol.1.6.
[31] RobertPlutchikandHenryKellerman.2013.Theoriesofemotion.Vol.1.Academic
Press.
[32] KarenSimonyanandAndrewZisserman.2014. Two-StreamConvolutional
NetworksforActionRecognitioninVideos..InNIPS.568–576.
[33] DuTran,LubomirBourdev,RobFergus,LorenzoTorresani,andManoharPaluri.
2015. Learningspatiotemporalfeatureswith3dconvolutionalnetworks.In
ProceedingsoftheIEEEinternationalconferenceoncomputervision.4489–4497.
[34] MichelValstar,JonathanGratch,BjörnSchuller,FabienRingeval,DennisLalanne,
MercedesTorresTorres,StefanScherer,GiotaStratou,RoddyCowie,andMaja
Pantic.2016.Avec2016:Depression,mood,andemotionrecognitionworkshop
andchallenge.InProceedingsofthe6thInternationalWorkshoponAudio/Visual
EmotionChallenge.ACM,3–10.
[35] PeterWashington,CatalinVoss,NickHaber,SerenaTanaka,JenaDaniels,Carl
Feinstein,TerryWinograd,andDennisWall.2016.Awearablesocialinteraction
aidforchildrenwithautism.InProceedingsofthe2016CHIConferenceExtended
AbstractsonHumanFactorsinComputingSystems.ACM,2348–2354.
[36] XuehanXiongandFernandoDelaTorre.2013. Superviseddescentmethod
anditsapplicationstofacealignment.InProceedingsoftheIEEEconferenceon
computervisionandpatternrecognition.532–539.
[37] BaohanXu,YanweiFu,Yu-GangJiang,BoyangLi,andLeonidSigal.2016.Het-
erogeneousknowledgetransferinvideoemotionrecognition,attributionand
summarization.IEEETransactionsonAffectiveComputing(2016).
[38] AnbangYao,JunchaoShao,NingningMa,andYurongChen.2015.Capturing
au-awarefacialfeaturesandtheirlatentrelationsforemotionrecognitioninthe
wild.InProceedingsofthe2015ACMonInternationalConferenceonMultimodal
Interaction.ACM,451–458.
[39] XiangxinZhuandDevaRamanan.2012.Facedetection,poseestimation,and
landmarklocalizationinthewild.InComputerVisionandPatternRecognition
(CVPR),2012IEEEConferenceon.IEEE,2879–2886.
9"
213,215,The Dartmouth Database of Children's Faces: Acquisition and validation of a new face stimulus set,"['KA Dalrymple', 'J Gomez', 'B Duchaine']",2013,165,Radboud Faces Database,classification,is comparable to rates from other published face databases [7]. Happy and Content were  the most accurately identified expressions; raters correctly classified 97.8% of the Happy (teeth,No DOI,PloS one,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3828408/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,nih.gov,
214,216,The Indian spontaneous expression database for emotion recognition,"['SL Happy', 'P Patnaik', 'A Routray']",2015,120,Affective Faces Database,"classifier, facial expression recognition","facial expression recognition algorithms. In this paper, we propose and establish a new facial  expression database  face database including facial expression videos elicited by watching",No DOI,… on Affective Computing,https://arxiv.org/abs/1512.00932,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"Download the database at https://sites.google.com/site/iseddatabase/
The Indian Spontaneous Expression
Database for Emotion Recognition
S L Happy, Student Member, IEEE, Priyadarshi Patnaik, Aurobinda Routray, Member, IEEE,
and Rajlakshmi Guha
Abstract— Automatic recognition of spontaneous facial expressions is a major challenge in the field of affective computing. Head
rotation, face pose, illumination variation, occlusion etc. are the attributes that increase the complexity of recognition of
spontaneous expressions in practical applications. Effective recognition of expressions depends significantly on the quality of the
database used. Most well-known facial expression databases consist of posed expressions. However, currently there is a huge
demand for spontaneous expression databases for the pragmatic implementation of the facial expression recognition algorithms.
In this paper, we propose and establish a new facial expression database containing spontaneous expressions of both male and
female participants of Indian origin. The database consists of 428 segmented video clips of the spontaneous facial expressions
of 50 participants. In our experiment, emotions were induced among the participants by using emotional videos and
simultaneously their self-ratings were collected for each experienced emotion. Facial expression clips were annotated carefully
by four trained decoders, which were further validated by the nature of stimuli used and self-report of emotions. An extensive
analysis was carried out on the database using several machine learning algorithms and the results are provided for future
reference. Such a spontaneous database will help in the development and validation of algorithms for recognition of spontaneous
expressions.
Index Terms—Affective annotation, database, emotion elicitation, facial expression recognition, spontaneous expression.
——————————  ——————————
1 INTRODUCTION
Fa cial expression is a fundamental mode of communi- on discrimination of fake expressions from the genuine
cating human emotions. Among the various channels ones [4]; all these provide a strong reason to create data-
that communicate human emotions – voice, textual bases for high resolution videos of naturally elicited emo-
content, gestures and facial expressions – it is considered tions.
one of the most accurate [1]. Hence, its relevance in the con- Creation of an emotion database is a difficult and time
text of Human-Computer Interaction (HCI) is of primary consuming task [5]. However, database creation is an es-
importance. Automatic recognition of emotions through sential step in the creation of a system that will recognize
facial expressions, thus, becomes relevant for HCI as well human emotions. Spontaneous emotion elicitation re-
as a number of other contexts [2] such as human monitor- quires significant effort in the selection of proper stimuli
ing, intelligent assistance, surveillance and so on. Tagging which can lead to rich display of intended emotions. Sec-
of affective states in the media content can be helpful in ondly, the process involves tagging of emotions by trained
quick categorization of a large number of media files for individuals manually which makes the databases highly
automatic and effective content retrieval. Design of such reliable. Since perception of expressions and their intensity
systems [3] depend on reliable facial expression databases is subjective in nature, such expert ratings are essential for
which include a number of parameters such as posed vs. the purpose of validation.
natural expressions, gender, cultural variations, expres- Paul Ekman has reported the presence of six universal
sion intensity variation, face pose, occlusion, contexts in categories of facial expressions of emotions based on a
which emotions are generated, etc. Well-labeled videos of number of cross-cultural studies [6] [7], regardless of cul-
facial behavior and emotion tagged images in the data- tural differences. Though a trained eye can recognise the
bases are, thus, of crucial importance. These are essential facial expressions of people around the globe, it is very
for training, testing and validation of algorithms for devel- challenging for a machine to recognize the emotional states
opment of robust systems. Besides, recent research focuses automatically. Machines recognize expressions based on
the features extracted from the face such as shape, appear-
———————————————— ance etc. Due to variations in texture and shape of faces of
 S L Happy and A. Routray are with the Department of Electrical Engineer- people from different cultures and races, the advancement
ing, Indian Institute of Technology, Kharagpur, India. of affective computing relies on databases of faces from
E-mail: {happy,aroutray}@iitkgp.ac.in.
different ethnicity [8]. To the knowledge of the authors, no
 P. Patnaik is with the Department of Humanities and Social Sciences,
such emotion database exists for Indian faces. Moreover,
Indian Institute of Technology, Kharagpur, India.
E-mail: bapi@hss.iitkgp.ernet.in. the majority of the existing databases are created under
 R. Guha is a psychologist in the counselling centre, Indian Institute of tightly controlled laboratory environments [9] which do
Technology, Kharagpur, India.
not represent the real world expressions that we come
E-mail: rajlakshmi_guha@rediffmail.com.
 https://sites.google.com/site/iseddatabase/2
across in our daily lives. with time ─ for accurately classifying emotions than the
For reliable analysis of facial expression recognition al- static data available in images. New efforts to create audio-
gorithms, an Indian Spontaneous Expression Database visual databases include the work of [15], [16], [17], and
(ISED) has been reported in this paper. The details of the [18]. Valstar and Pantic [19] have reported MMI database
experiment design for simultaneous elicitation and captur- in which video clips with both frontal and profile views are
ing of genuine expressions, its protocol, and annotation of included for posed expressions. The posed data include 20
emotion in the database are described in this paper. The participants showing 31 action units and 28 participants
performance of different expression recognition algo- expressing six basic emotions. Annotation of the emotion
rithms on this database are studied in detail. Strategies and intensity has been included in the database for proper
used for addressing the challenges for database creation utilization of the database.
are provided explicitly. In the database, emotions were in- The necessity for spontaneous facial expression data-
duced through emotion-inducing videos in participants, bases is emphasized in [8]. Excluding some attempts ( [17],
along with simultaneous recording of videos at high frame [20], [5], [21], [22], [23], [24]), most of the publicly available
rates. Basically, four kind of emotions are elicited and rec- emotion databases include posed facial expressions only.
orded, namely, (a) happiness, (b) sadness, (c) surprise, and In posed expression databases, the subjects are asked to
(d) disgust. The emotional responses are segmented man- display different basic emotional expressions, while in
ually and annotated by trained decoders. Information such spontaneous expression database, the expressions are nat-
as expression intensity, gender, type of occlusion etc. are ural. The need for real time detection of emotion in practi-
provided. Position of face, eye and nose in peak expression cal contexts has encouraged researchers to create sponta-
frames are also included in the database. This database, neous emotion databases. The Extended Cohn-Kanade Da-
consisting of videos of full-blown as well as controlled fa- taset (CK+) [25] is an extension of [13] which includes
cial expressions, will prove suitable for development of ro- posed expressions of 123 multi-ethnicity subjects along
bust algorithms for automatic and non-intrusive detection with the spontaneous smile expressions. The Geneva Mul-
of emotions from visual cues. To reduce intrusion and timodal Emotion Portrayal (GEMEP) corpus [26] includes
guaranty spontaneous emotion elicitation, concealed cam- the FACS coded emotion portrayals by trained actors
eras were used without prior knowledge of the subjects which, though not spontaneous, are very close to the spon-
and recording of physiological signals such as Electro-En- taneous expressions.
cephalograph (EEG), Electro-Cardiograph (ECG) etc. were Works of Ekman [27] correlated the facial muscle move-
avoided. ments with the underlying emotional state. Facial Action
The paper is organized as follows. Section 2 presents a Coding System (FACS) measures all visually observable
review of earlier works. Section 3 presents the protocol fol- facial movements in terms of Action Units (AUs). Using
lowed during the experiments. Section 4 discusses the da- this concept of facial muscle movements, posed expression
tabase content and its availability for research purpose. databases include the reproduction of different emotion
The evaluation of database is carried out in section 5. Sec- which are unnatural and hardly found in actual situations
tion 6 concludes the paper. [17]. Spontaneous expressions differ from posed ones re-
markably in terms of intensity, configuration, and dura-
tion. Apart from this, synthesis of some AUs are barely
2 BRIEF REVIEW OF EXISTING EXPRESSION
achievable [28] without undergoing the associated emo-
DATABASES
tional state. Therefore, in most cases, the posed expressions
Research on automatic affect recognition received an are exaggerated, while the spontaneous ones are subtle
impetus around the late 1990s with early efforts of Mase and differ in appearance. In [29], high accuracy levels in
[10] and Kobayashi [11] toward affect recognition from fa- automatic detection of posed expressions are reported; but
cial images. Since then, a number of emotion databases accuracy for spontaneous expressions is still low. Because
have been developed to advance the automatic recognition of the difficulty in elicitation of facial expressions and rig-
of affective states. In 1997, Lyons et al. [12] created the Jap- orous, time–consuming manual labour for the annotation
anese Female Facial Expressions (JAFFE) database which of underlying emotional state in video clips, only a few
consists of 213 images of seven different emotional facial spontaneous databases exist so far. Spontaneous expres-
expressions (Sadness, happiness, surprise, anger, disgust, sions are very subtle, short lived, and often a mixture of
fear, and neutral) by 10 subjects. This database also in- pure expressions. So it is very difficult to reliably annotate
cludes the averaged semantic ratings of 60 Japanese female the complex natural expressions. Recent efforts in HCI in-
students on a scale of five. Another widely used database clude understanding human emotions using multimodal
in this field is Cohn-Kanade AU-Coded Facial Expression signals such as voice, facial expression, posture, gesture,
database [13] which includes 486 sequences by 97 posers. etc. Further, the relation between facial expressions and af-
Each sequence includes proceeding of a peak expression fective state is also necessary for proper assessment of cog-
from a neutral expression. FACS coded peak expression nitive states. The limited number of databases has always
and an emotion label for each sequence is provided in this been an issue for researchers working in the field of emo-
database. However, these databases comprise posed ex- tion recognition. An ideal corpus should include high
pressions only. quality recording of rich emotional experience from differ-
Literature [14] emphasises the use of dynamic infor- ent modalities along with the tagging of the emotional con-
mation in temporal domain ─ the changes in facial muscles tent.HAPPY ET AL.: THE INDIAN SPONTANEOUS EXPRESSION DATABASE FOR EMOTION RECOGNITION 3
Table 1 where the clips are segmented form a German talk show
SUMMARY OF DATABASE CONTENT and annotated using valence-activation scale. However,
this does not permit self-report, nor the ability to elicit pure
emotions. In [32], creation of a multimodal spontaneous
emotion database is reported where audio-visual signals
are captured along with feet pressure signal, thermal im-
age, and body gesture by eliciting emotion by displaying
pictures and videos as well as through interview. Recently,
DEAP database [33] has been created which includes face
videos of 22 participants along with physiological signal
recording such as electro-encephalograph (EEG), electro-
myograph (EMG), electro-oculograph (EOG), blood vol-
ume pulse (BVP), skin temperature, and Galvanic Skin Re-
sponse (GSR). However, the more the number of signals
recorded during the experiments, the more the chances of
failure in eliciting natural expressions. Both the above da-
tabases suffer from excessive intrusion.
In Belfast induced emotions database [17], emotions are
DISFA [21] includes full intensity FACS coding expres-
induced using laboratory-based tasks and continuous
sions of 27 participants elicited by viewing a video clip of
trace ratings of the coloured responses are generated in
duration of 4 minutes. Videos were captured at 20 fps at
terms of valence and intensity scale. It uses both active and
1024x768 resolution. AU annotation for each frame is in-
passive tasks (watching emotional videos) to engage the
cluded in the database. However, the database has no de-
subjects and thereby elicit emotion. In [15], to observe the
scription about elicitation of pure emotions. HUMAINE
range of emotional behaviour in a learning environment,
database [23] contains spontaneous video clips through
non-verbal behaviour from cues such as facial expressions,
different activities and conversations. It is a multimodal
eye-gaze and head posture data are collected to assess af-
database including audio-visual data, physiological re-
fective states during interaction and learning. In [16], spon-
cordings (ECG, GSR, skin temperature, breathing, EMG
taneous emotions are induced by activities such as playing
and BVP) and performance data. Both global as well as
computer games and conducting adaptive intelligence
frame-by-frame emotion label is provided. Conversations
tests. Wang et al. [5] have developed a natural emotional
with individual SAL characters were used as the stimulus
database, containing both visible and infrared images, in-
in the SEMAINE database [24] and the video was recorded
duced by film clips. They have recorded both spontaneous
at a spatial resolution of 780x580 pixels at 50 fps. The re-
and posed expressions in three varied illumination condi-
cordings of 150 participants are annotated on valence-
tions. In [18], the authors have used strategies to induce
arousal scale. In the above said databases, the participants
different emotions by exposing the subjects to different sit-
were aware of being recorded. Moreover, the self-assess-
uations.
ment were absent in the above said databases, so it is diffi-
When creating the spontaneous facial expression data-
cult to establish the ground truth.
bases, it is necessary to validate that the facial expression
The proposed experiment design takes care of all the
of a person is corresponding to the emotional state of the
above limitations and is in accordance with the guidelines
person. To create such an authentic validated database,
developed by Sebe et al. [30]. The strategy of self-report of
Sebe et al. [30] have indicated some guidelines. The subjects
emotion and the use of hidden camera to record spontane-
should not be aware of being tested for elicitation of their
ous expressions are adapted only in very few databases
emotional states; otherwise it influences their emotional
[30], [34]. Therefore, genuine expressions are captured dur-
states. Secondly, the subject’s self-report should be docu-
ing the experiments unlike the experiments in [17], [5],
mented after the test to validate the emotional states of the
[24], [32], [33].
subjects. Thirdly, the presence of the experimenter may in-
In practical scenarios, there is a possibility of observing
fluence the elicitation of facial expression. Spontaneous
mixed emotions. However, in this experiment, the passive
emotion elicitation is possible through human to human
mode of emotion elicitation provides scope for elicitation
interaction, human to computer interaction, by emotion
of pure emotions. The ISED contains high quality near
eliciting tasks or by induction through picture, music, or
frontal face recordings of spontaneous emotions at high
videos [20].
resolution and frame rates along with information regard-
However, in [15], [16], [17], [18] and [5], the subjects
ing gender of the participants, the ground-truth of emo-
were aware of being monitored and the experiments were
tional clips and its intensity, and the peak emotion inten-
conducted in the presence of the experimenter, thus, vio-
sity frame in the video clips. The head, the eyes and the
lating the two guidelines mentioned by [30]. Hence the
nose positions in the peak intensity frames are also pro-
elicited emotion may not be purely spontaneous due to the
vided for easy access of the database. It covers the expres-
possibility of social masking. In [31], the authors have cre-
sions of participants from different parts of India and from
ated face database including facial expression videos elic-
different cultural and linguistic backgrounds. Emotion
ited by watching emotional video clips. However, the an-
elicitation was carried out without the presence of the ex-
notation of the emotional state is not provided. VAM au-
dio-visual database [22] is another spontaneous database4
perimenter and recorded by a hidden camera; hence, spon- presence of camera is similar to the presence of an experi-
taneous expressions were recorded. The facial expression menter and subjects often mask their emotions. Thirdly,
annotation was conducted by four trained decoders of such experiments should ideally be conducted indoors
both gender. All these enhance the reliability of the data- where noise, movement, and other such distractions can be
base. Finally, the database is evaluated using baseline al- controlled and proper ambient lighting maintained for
gorithms as an elementary assessment of usefulness of the quality recording of videos. Based on the suggestions of
database. subjects during pilot studies we concluded that directly
lighting the subjects made them suspicious and conscious
and also disturbed their viewing experience. Hence, ambi-
3 CREATION OF THE DATABASE
ent reflected lighting was used.
The ISED includes video and still facial images of spon- The whole experiment was set up in an isolated room of
taneous emotions. The emotion were elicited through pas- area 3mx3m with a single door. As discussed earlier, for
sive elicitation by watching emotion inducing videos. Sub- elicitation of natural emotions, the participants were not
jects were left alone in the experimental room and a hidden informed about the goal of the experiments beforehand
camera recorded their facial expressions. All the video and the video were captured using a hidden camera. To
clips included in the database vary from one to 10 seconds conceal the camera, a wooden box was built with a win-
in length. The details of the database are provided in Table dow made of one-way glasses so that the camera could
1. The details of the videos used for emotion elicitation are capture videos of the subject if placed inside the box, but
provided in Table 2. the subject would not be aware of its presence. Two re-
While constructing the ISED, the spontaneous emotion flected light sources were used to create ambient lighting
induction methods were administered carefully in spe- conditions, thereby providing a good environment to
cially designed artificial environments. Hence, the elicited watch movies from participants’ point of view and enough
emotions, though induced, look very natural and can be lighting to capture good quality videos from experiment
referred to as natural emotional expressions [35]. As ex- point of view. The camera was concealed in the wooden
pected in any experiment involving human subjects, the ef- box as shown in the Fig. 1.
fectiveness of the used stimuli in emotion induction can During passive emotion elicitation, a monitor was put
have wide variance based on subjective perception and the on a table in front of the concealed camera box. Participants
state of mind of the subjects. For example, a specific video were asked to sit comfortably on a height-adjustable mov-
clip was reported as very disgusting by most of the sub- able chair and watch the video/film clips. In order to pro-
jects, while a few reported it to be fearful or sad. Similarly, vide privacy to the encoders for full blown emotion induc-
the intensity of emotion elicitation varied for individuals tion, the subjects were allowed to watch videos alone in the
for the same video. However, as observed from Table 2, the experimental room. To avoid suspicion subjects were al-
video clips were successful in eliciting the targeted emo- lowed to sit at their own ease without any other restrictions
tion in most of the participants. In cases where the self-re- inside the experimental room. The doors and windows
port, type of stimuli and decoders’ assessments showed were kept shut during the entire period of the experiment
significant variances, the clips were not included in the to avoid external interference.
ISED.
3.2 Camera Setup
The facial expressions of emotions present in the rec-
orded videos were extracted by specially trained decoders A Nikon D-5200 camera was used for video recording
selected by screening more than a 100 prospective decod- which can record videos at 50 frames per second with a
ers. Four decoders, two of each gender, were selected on resolution of 1920x1080. A Nikon D-7000 was used as a
the basis of their ability to detect emotions on 28 multi-eth- backup which could also record with the same resolution.
nic images of faces (Japanese and Caucasian Facial Expres-
sions of Emotion database [36] images) exposed for 1/15th
of a second. Their success rates were found to be above
95%. They were later trained using Micro Expression
Training Tool and Subtle Expression Training Tool [37] un-
til they achieved an accuracy rate of more than 90%. They
were also familiarized with Facial Action Coding System.
3.1 Experiment Setup
Forced or posed expressions can be captured by volun-
tary participation of the subjects. However, as discussed
earlier, for spontaneous emotion elicitation and capture of
such data, awareness of the experiments’ intents and pres-
ence can be a barrier in elicitation of spontaneous emotions
[30]. The subjects’ awareness about experimental intent
can be detrimental. Secondly, video documentation done Fig. 1. The Experimental Set-up. (1) chair for subject, (2) table, (3)
without the knowledge of the subjects would again aid laptop, (4-5) light sources, (6) camera on tripod, (7) the wooden box
to conceal camera, (8) experiment room
spontaneous emotion manifestation. Awareness of theHAPPY ET AL.: THE INDIAN SPONTANEOUS EXPRESSION DATABASE FOR EMOTION RECOGNITION 5
TABLE 2
DETAILS OF EMOTIONAL VIDEO CLIPS USED FOR EMOTION ELICITATION
Percentage of participants labeled the video clips into different
Content of Video clips categories in self-assessment report (values in %) Duration
Happiness Surprise Sadness Disgust Fear Anger
Robin (EEF) 86 14 0 16 0 0 2:32
A scene from Jaane Bhi Do Yaaron [Bol-
88 14 14 0 0 0 3:13
lywood movie - 1983]
Disney Earth (EMDB) - - - - - - 1:03
Unbelievable stunts 4 96 0 0 12 0 1:43
Scary accident compilations 0 70 10 0 30 2 2:04
Diary of a Nymphomaniac (EMDB) 0 2 82 0 24 0 1:48
Very sad movie clip 0 0 100 0 2 6 4:03
Disney Earth (EMDB) - - - - - - 0:31
Pink Flamingos (EEF) 2 0 4 94 0 0 0:37
Eating live cockroaches 0 2 4 94 6 4 1:53
The videos used from the reported literatures are mentioned in the brackets. Here EMDB stands for “The Emotional Movie Database” [38] and EEF
stands for “Emotion Elicitation Using Films” [39]. Note that the participants were allowed to assign multiple emotional labels to one video clip.
The camera was placed inside the wooden box concealed elicit emotions. Four emotions were elicited, namely, hap-
from the subjects at a height of 1.5 m from the ground. The piness, surprise, sadness, and disgust.
distance between the subject and the camera was approxi- Carvalho et al. [38] reported that the video clips are
mately 0.8m to 1.5m. more effective stimuli than the other stimuli for evoking
emotional responses. Selection of emotional videos is an
3.3 Illumination Setup
important step as it affects the strength of emotion that one
Proper lighting is essential while recording videos at can feel. The emotional videos were collected from differ-
high frame. Due to very short image acquisition time, un- ent databases that have been used for similar studies (viz.
less the light intensity is considerably high, the captured [38], [39], [40], [41]) and some of the selected videos were
videos looks dark. To address this issue, a pair of studio from other sources such as Bollywood films and YouTube.
lights were bounced off the walls in the experiment room Since the participants were all Indian, the videos were
to create suitable ambient illumination and in order to re- selected carefully to elicit emotions for the target demog-
duce the discomfort of directing light into the face of sub- raphy. Ten individuals including the decoders were asked
jects. The subjects were informed that the soft ambient to watch and rate the collected videos. The rating was car-
light was used to create a pleasant and comfortable view- ried out on a scale of 0-5 for different emotions where 0
ing atmosphere. indicates no emotion at all and 5 indicates very high inten-
sity of elicited emotion. The average rating for each video
3.4 Subjects
was calculated. Some videos were found to elicit multiple
Fifty healthy participants, 29 males and 21 females, vol-
emotions. Videos eliciting multiple emotions may lead to a
untarily participated in the experiment. The participants
mixed emotional expression, and hence were rejected. The
were of the age group ranging from 18 to 22 years and were
videos which were unanimous in eliciting one specific
from different regions of India. We administered General
emotion with higher intensity were selected. Keeping in
Health Questionnaire (GHQ) and State-Trait Anxiety In-
mind the Indian cultural context, for each of the four emo-
ventory (STAI) prior to the experiments for screening for
tions we used one video clip suggested by the databases
physical illness or mental distress. They were debriefed at
and the other from non-database videos, and in each case
the end of the experiments. The database includes the vid-
selected the one that had got the highest ratings from our
eos of the participants who signed the consent form agree-
decoders. Including two neutral video clips, 10 such video
ing to their facial videos being used for research purpose.
clips were selected for different emotions such as happi-
The participants were from different states, different back-
ness, surprise, sadness, and disgust. The agreement be-
grounds, and different cultures. Of them, 38% belong to
tween the coders was found to be 0.81 by computing
north zone of India, 24% are from east zone, 22% are from
Fleiss’s kappa [42]. Fleiss’s kappa of more than 0.8 is con-
south zone and 16% are from west zone of India.
sidered as very reliable degree of agreement. Since eliciting
3.5 Stimuli anger fear is difficult in passive elicitation, it was not con-
sidered. Similarly, fear elicitation was also not carried out
Video clips (18 seconds to 4.5 minutes in duration) iden-
because of ethical issues. The length of the selected clips
tified on the basis of pilot studies (as well as clips used in
varies from 18 seconds to 4.5 minutes which is in accord-
earlier studies such as The Emotional Movie Database [38]
ance with the recommended length of emotional video as
and Emotion Elicitation Using Films [39] etc.) were used to
described in [39]. The happiness and surprise videos were6
shorter, while the sadness inducing videos were longer as screened before being allowed to participate in the experi-
it takes more time to build up sadness. The selected videos ments. The participants were made aware of their rights to
were strong enough to elicit moderate emotions, but not withdraw from the experiment at any time they desire. As
cause ethical concern. mentioned earlier, their task was to watch the video clips,
identify the emotions expressed and rate the intensity of
3.6 Self-Report of Emotion
emotions. Therefore, they were taken to the experiment
Lack of accurate assessment of emotional experience in room and left alone with a rating sheet where they could
the subjects can jeopardize an experiment. To identify the evaluate the emotions they experienced after watching
emotion experienced by the subjects, one may directly ask each video. The camera was switched on before the subject
them or infer from physiological signals. Since our design entered the experiment room. During debriefing it was
was non-intrusive, self-report forms were included for found that none of the subjects had detected the concealed
subjective assessment of emotions. Instead of telling any- camera or suspected any other mode of recording. We ob-
thing about the experiment, the subjects were informed served some persons make the recordings of face difficult
that they had to participate in a survey where they had to by occluding their face, leaning forward, or looking away
assess the emotional content and intensity of the video from screen (mostly during disgust) when watching the
clips; and for accurate assessment of emotions they had to videos. Hence, the arrangement of table, arm-rest and dis-
get immersed in the viewing experience and experience tance of viewing were altered to control this without mak-
the emotions. At the end of each video clip, they were in- ing the subject suspicious by explicit instructions. How-
formed to note down the emotions that the video clip com- ever, they were encouraged not to close their eyes during
municated and the intensity with which they felt them on disgust or other such negative emotions so that their rat-
a scale of zero to five. They were given a list of six basic ings would be accurate. The self-report of emotion was
emotions [43], but were also told that they could add other generated as described earlier. A gap of 15 seconds was
emotions if they wanted. Hence, the self-report of emotion provided between subsequent clips for rating of the previ-
was gathered without the subjects knowing that they were ously shown clip.
being assessed. The average emotion reported by the par- The videos were shown in two sessions, of 12-14
ticipants in their self-assessment report gives an insight to minutes duration each. The sequence in which the emotion
the expressivity of the subjects to different emotion catego- induction clips were presented was happiness, surprise,
ries which is provided in Fig. 2. Compared to the other sadness and disgust. The first session included happiness,
emotions, happiness videos were rated slightly lower. On and surprise followed by a break of five minutes. The sec-
the other hand, the disgust videos were very successful at ond session includes videos of sadness and disgust. In the
eliciting the targeted emotion. beginning, we showed a funny video to make the partici-
pant comfortable with the experiment environment. An-
3.7 Occlusions
other amusement video was used at the end of the second
Presence of an obstacle such as spectacles, hair, beard,
session in order to dispel the feelings of sadness and dis-
moustache etc. disguises some of the important infor-
gust and make the participants happy. To avoid the sud-
mation in facial expression which is often a challenge for
den transition of one emotional clip to another, some neu-
practical implementation. Therefore, for the experiments,
tral videos were introduced between two different emo-
we decided to include some subjects who wore spectacles.
tion clips. Throughout the experiment, the order of the vid-
Some of the male subjects also had moustaches and beards.
eos was kept the same.
They were also not restricted from touching their chins or
After the experiments, the subjects were taken to a dif-
cheeks since this also lead to partial obstruction which we
ferent room where they were debriefed. They were made
intend to capture.
aware of the project and the requirement of spontaneous
3.8 Experiment Procedure expression database for improvement of computer vision
algorithms in emotion recognition and requested to pro-
The volunteers were asked to fill up GHQ and STAI and
vide a written consent for use of the recordings for aca-
demic and research purpose. Their questions regarding the
database, the video content, and the projects were clarified.
5 4.32 They were not pressurized in any way to consent to the use
4 3.39 3.35 of their in the database. A few participants refused to con-
2.78 tribute to the database. In all such cases their video clips
3
were permanently destroyed.
2
3.9 Ethics statement
1
The experimental procedure and the video content
0
shown to the participants were approved by the Institu-
happiness surprise sadness disgust tional Ethics Committee (IEC) of IIT Kharagpur. The par-
ticipants were also informed that they had the right to quit
the experiment at any time. The video recordings of the
Fig. 2. The average emotion intensity reported by the participants to
subjects were included in the database only after they gave
different emotion categories
a written consent for the use of their videos for researchHAPPY ET AL.: THE INDIAN SPONTANEOUS EXPRESSION DATABASE FOR EMOTION RECOGNITION 7
Fig. 4. Example images of occlusion
Fig. 5. Different expressions of a single participant
Fig. 3. Peak intensity images of different spontaneous expressions,
surprise (1st row), happiness (2nd row), sadness (3rd row) and dis- Fig. 6. Variation of intensity of the peak images in different video
gust (4th row) clips of same type of expression (disgust) of same participant
purpose. A few subjects were also agreed to use their face
showed spontaneous surprise easily through passive in-
images in research articles.
duction.
The segmented clips were annotated by four trained de-
4 DATABASE CONTENT coders to tag the six basic emotional expressions viz. hap-
piness, sadness, surprise, fear, anger and disgust, and the
The created database contains subtle to full blown elici-
corresponding intensity on a six-point scale. In the scale of
tation of different emotions. It contains facial expression
0-5, higher value in the scale corresponds to high intensity
videos and still images of 50 participants with the emotion
of elicited emotion. The clips which are classified to the
ground truth and its intensity. A few examples of the da-
same emotion by all decoders were considered for inclu-
tabase images are provided in Fig. 3. Different occlusions
sion in basic emotion category. The intensity of such clips
are shown in Fig. 4 and all expressions of the same partici-
were decided by taking average of ratings of all decoders
pant are shown in Fig. 5.
[5]. The reliability of agreement between four raters is also
The recording data for each participants was around 30
evaluated by using Fleiss’s kappa [42]. Here the evaluation
minutes long. The video clips were manually screened for
is based on the classification of the video clips into the cat-
facial expressions and segmented out. The length of the ex-
egories of expressions. The Fleiss’s kappa coefficient of the
tracted clips vary from 1 sec to 10 sec. The average duration
labeling was 0.847, indicating a very good consistency.
of the segmented clips are 4 seconds. We observed that the
Since the spontaneous expression of an individual does not
expression duration varied with the kind of emotion. For
remain the same with different situation and even varies
example, it is very difficult to get a small clip for sadness,
across time, several expressions of the same participants
because of its gradual evolution and persistence over a rel-
are included in the database. Moreover, the face pose and
atively long time. On the other hand, surprise expressions
the intensity of expression vary in different video clips of
set very fast and disappeared in a moment. For expressions
a participant while displaying the same expression. Fig. 6
that lasted for a long time, we segmented out a short clip
shows the variation of expression intensity and face pose
out of it labeling one emotion to it at the center of the clip.
in different video clips. The emotion category was vali-
It was observed that most of the expressions were accom-
dated by the type of video the participants had watched
panied by head and body movements. All subjects did not
during invoking that expression and the self-report of
show all types of expressions. However, we found happi-
emotions. Thus, the segmented expression clips were in-
ness and disgust to be induced easily, while sadness was
cluded in the database based on agreement of type of stim-
very difficult to induce. However, some participants
uli used, ratings of the coders, and the self-report of emo-
tion by the participants. Thus, we strongly believe that the8
videos included in our database are genuine and sponta- first rotated to bring the eyes to the same horizontal level.
neous. In some cases, we observed mixed emotion which Then the image was scaled so that the eye centers were po-
was difficult to annotate to the basic emotion categories. In sitioned at a particular distance. The face region was ex-
such cases, the clips were excluded from the database. tracted in all the images and resized to a standard resolu-
Most of the video clips start with a neutral face and end tion of 96x96. The face images were converted into gray-
either with the peak expression or after the off-set of the scale images for further processing. A Gaussian mask was
expression. Thus, it provides the researchers a chance to applied to the whole image to remove noise followed by
model the transition of spontaneous facial expressions histogram equalization. Ten-fold cross validation was
from the neutral face. However, the sadness clips were seg- adopted in our experiments.
mented around the peak expression frames. Thus, the neu-
5.1 Feature Extraction
tral face may not be at the starting frame of a sadness clip.
The accuracy of expression recognition highly depends
4.1 Availability upon the selection of appropriate features to represent the
The ISED database can be obtained by writing a mail at expressive face. The geometric or the appearance features
iseddatabase@gmail.com. The description of the database or the combination of both can be used for this purpose.
is available at https://sites.google.com/site/iseddata- We have experimented with different types of feature ex-
base/. An End User License Agreement (EULA) needs to traction techniques as described below.
be produced for accessing the database.
5.1.1 Grayscale Intensities
After preprocessing of the face image, the pixel intensi-
5 EVALUATION OF THE DATABASE
ties of the gray image were used as feature vectors for clas-
This section describes the procedures that has been car- sifying expressions [5]. Since the pixel intensities vary from
ried out to obtain the baseline results using the database. 0~255, they were normalized to zero mean and unit vari-
This elementary assessment provides an insight to the us- ance.
ability of the database. We conducted the expression recog- In another set of experiments, the eye region and the lips
nition experiment using combination of different types of region was segmented and the corresponding pixel inten-
facial features and classifiers. The peak expression faces in sities were used for expression classification [44]. The eye
all the video clips were used in the experiment. Though region was selected based on the positions of the eyes. Sim-
most of the images are near frontal, some are with face oc- ilarly, the lips region was determined with respect to the
clusion and head rotations. Among the selected peak im- position of the nose. The size of the selected regions de-
ages, 227, 73, 48, and 80 images belong to the class happi- pend upon the size of the face. In Fig. 7, the process for se-
ness, surprise, sadness, and disgust respectively. lection of eye and mouth regions is provided.
Face detection is the first step in expression recognition.
5.1.2 Local Binary Pattern (LBP)
Viola-Jones Haar cascade classifiers were used for face de-
tection which resulted an accuracy of 89.72% in our da- LBP operator compares the pixel values in a location to
taset. The presence of off-plane head rotation and occlu- its neighboring pixel values and generates a binary num-
sion are the primary reasons behind the poor performance ber representing the pattern [45]. The histograms of LBP
of the face detector. However, for establishing the baseline, image forms a robust feature descriptor against illumina-
the face position was selected manually in the images tion variation. We used LBP histograms (𝐿𝐵𝑃 𝑃,𝑅), uniform
where the face detector failed. Similar techniques were LBP (𝐿𝐵𝑃 𝑃𝑢 ,2 𝑅) histograms, and rotational invariant uniform
adopted for eye and nose localization. The eyes and nose LBP patterns (𝐿𝐵𝑃 𝑃𝑟 ,𝑖 𝑅𝑢2) in our experiments.
positions were also manually marked in case the Haar clas- The LBP histogram represents the patterns of the whole
sifier failed. The ground truth of face position, eye posi- image which lacks the spatial information. The LBP histo-
tions and nose position in all peak images are also included grams from the image sub regions can be concatenated to
in the database. capture the details of the position of the patterns. In our
The eye locations were used to suppress the effect of experiments, we divided the face image into 3x3, 5x5 and
scale, position and in-plane rotation. The face image was 7x6 non-overlapping regions to construct the enhanced
Fig. 8. Multi-block feature extraction using 𝑚×𝑛 number of regions
Fig. 7. Selection of lips and eye regionsHAPPY ET AL.: THE INDIAN SPONTANEOUS EXPRESSION DATABASE FOR EMOTION RECOGNITION 9
Table 3 In our experiments, LGBP features were extracted from
FACIAL EXPRESSION RECOGNITION RESULTS OBTAINED USING 40 Gabor magnitude maps (5 scales and 8 orientations) by
DIFFERENT METHODS WITH TEN-FOLD CROSS VALIDATION diving each Gabor maps into 3×3, 5×5 and 7×6 subre-
gions. The uniform binary patterns (𝐿𝐵𝑃𝑢2) and the rota-
𝑃,𝑅
tion invariant uniform binary patterns (𝐿𝐵𝑃𝑟𝑖𝑢2) were used
𝑃,𝑅
to encode the texture of the region.
5.1.5 PHOG descriptor
Object boundaries play vital role in object representa-
tion and identification. Histogram of oriented gradients
(HOG) can represent the object geometry which is ob-
tained by dividing the image into several blocks, calculat-
ing the histograms of edge directions in each block and
concatenating the histograms of different blocks into a sin-
gle shape descriptor. The angular range 0 ~ 180º can be
quantized into several binwidths for obtaining the histo-
gram of the edge orientations. Bosch et al. [49] proposed
pyramid of histogram of gradients (PHOG) descriptors
which represents the position of the orientations along
with the HOG features. The spatial location of the object is
encoded by calculating the HOG features of the object at
different resolutions and concatenating them to form the
feature vector. Thus PHOG features of an object are robust
against the slight shape change, rotation and illumination
variation in the image.
In our experiments, nine bin histograms were used for
the angular orientations for each 8×8 cell to represent the
object. A three layer pyramid structure was used for accu-
rately representing the shape.
5.2 Expression Classification
Different classification techniques were adopted in our
experiment following different literature. The eye and lips
regions were extracted from the aligned face images as in
[44]. The feature extraction techniques described in section
5.1 such as LBP, Gabor filters, LGBP, PHOG etc. were car-
ried out on the whole face or on the eye and lips regions.
The feature normalization was carried out to map the fea-
feature vector. Fig. 8 describes the procedure of multi-
tures to zero mean with unit variance.
block LBP feature extraction technique where local fea-
Feature selection is an important step before feeding the
tures are extracted by dividing the face into 𝑚×𝑛 sub re-
data to a classifier. Principal Component Analysis (PCA) is
gions.
widely used as a dimensionality reduction tool which re-
5.1.3 Gabor Wavelets duces the dimension of the feature vector with minimal
loss of information. Adaboost selects a fewer dimensions
Gabor wavelets [46] are well known for their ability to
from the feature vector that provides significant accuracy
capture discriminative features with orientation specific
during classification into different classes. Adaboost is also
frequency bands. The magnitude response obtained by Ga-
a fast classifier.
bor filters of different orientation and spatial frequency can
In our experiments, PCA was primarily used for reduc-
represent the image features in a single vector. In our im-
ing dimensionality of the feature vector. However, while
plementation, we used five scales and eight orientations;
applying Adaboost, PCA was not applied beforehand
thus forty Gabor filter banks in total.
since Adaboost automatically selects a few features. In all
5.1.4 Local Gabor Binary Pattern (LGBP) scenarios with application of PCA, the number of principal
The LGBP of an image is obtained by applying LBP op- components were selected in such a way that the original
erator on the Gabor magnitude images of different orien- signal can be reconstructed with an error less than 5%.
tation and spatial frequency. Thus the multi-resolution and We implemented PCA and PCA+LDA (Linear Discri-
multi-orientation relation between the spatial frequency minant Analysis) framework for grey intensity features as
values are incorporated in the feature vector. A detailed reported in [5]. In [50] and [51], the face region is divided
study of facial expression recognition using LGBP features into several sub-regions and the feature vector is con-
is provided in [47] and [48]. It has been proved that LGBP structed by concatenating the local features extracted from
is very robust to illumination variation and misalignments. each sub region. In our experiments, we adopted this10
method to extract LBP related features. As described in
[52], multiclass Adaboost was used for expression classifi-
TABLE 4
cation using features of whole face or parts of face. One- THE PERFORMANCE OF DIFFERENT LBP FEATURES USING
against-one multiclass Support Vector Machines (SVM)
PCA+LDA CLASSIFIER
[53] with linear as well as radial basis function kernels
were also used for classification. However, it is not re-
ported due to its poor performance on our database. Cohen
et al. [54] used the Naïve Bayes classifiers for emotion clas-
sification as it is reported to be successful in many practical
problems. We have also implemented Naive Bayes and K
Nearest Neighbor (KNN) classifiers in our experiments.
5.3 Results and Discussion
TABLE 5
A series of experiments with different feature extraction
THE PERFORMANCE OF DIFFERENT LGBP FEATURES USING
techniques, dimensionality reduction techniques and vari-
PCA+LDA CLASSIFIER
ous classifiers were conducted for analysis of ISED expres-
sion images. However, performances of a few selected
combinations are provided in Table 3 which produced sig-
nificant accuracy. The precision, recall and F1 score for
each experimental method are also provided.
As observed from Table 3, the grey intensity of the face
region achieved an average recognition rate of 75.5% by
using PCA + LDA classifier. However, the performance of
grey intensities of the eye and lips regions was relatively
TABLE 6
lower compared to the performance of whole face image THE CONFUSION MATRIX USING PCA+LDA USING LGBP FEA-
indicating the fact that some of the important information TURES WITH 𝐿𝐵𝑃 8𝑢 ,12 FROM 5×5 NUMBER OF REGIONS
were missing. This was also supported by the results of Ga-
bor features. By applying Gabor filter banks with 5 scales
and 8 orientations on the whole face image, the accuracy
was observed to be 82% with an F1 score of 0.78. In con-
trast, the same Gabor features could achieve an accuracy of
78% while applied on lips and eye regions.
The LBP operators were found to be successful in ex-
pression classification for the ISED images. We experi-
mented with various LBP features extracted from different
number of regions of face image as shown in Table 4. The
into different number of regions. The confusion matrix of
𝐿𝐵𝑃𝑟𝑖𝑢2 features performed poorly in comparison to 𝐿𝐵𝑃𝑢2
8,1 8,1 the results obtained by using LGBP features with 𝐿𝐵𝑃𝑢2
and 𝐿𝐵𝑃 features. The 𝐿𝐵𝑃 features achieved the recog- 8,1
8,1 8,1 from 5×5 number of regions is provided in Table 6. It was
nition rate of 82.47% which was the best among all the LBP
observed that the LGBP features with uniform patterns
based techniques. It is also clear from Table 4 that extrac-
performed better than rotation invariant patterns. Further,
tion of features from 7×6 number of regions improves the
the recognition rate of 𝐿𝐵𝑃𝑢2 was similar when extracted
classification accuracy since the local features are encoded 8,1
from different number of facial sub-regions. Therefore,
properly.
minimum number of divisions are preferred to reduce the
The PHOG features did not perform well when ex-
feature dimension.
tracted from either the whole face or the specific facial re-
Among the classifiers, LDA technique excelled in our
gions. The reason behind the failure of the shape features
experiments. Other classifiers such as multi class SVM,
may be explained by the fact that the database images are
Adaboost, KNN, and Naive Bayes did not perform well
having different head rotations along with different inten-
compared to the performance of LDA technique. The poor
sity of expressions. Further, occlusions change the shape of
performance of SVM in our database was probably be-
the region. Thus, a general rule for associating certain
cause of the imbalanced dataset. Moreover, the number of
shape features to an expression is difficult.
training samples are less. Thus, LDA is the suitable tool for
LGBP features outperformed the rest feature extraction
finding the hyper-plane that minimizes the intra-class scat-
techniques. We have implemented 5 scale and 8 orienta-
ter, while maximizing the inter-class scatter.
tion Gabor filter bank followed by 𝐿𝐵𝑃𝑢2 and 𝐿𝐵𝑃𝑟𝑖𝑢2 for
8,1 8,1 The best results obtained by both LBP and Gabor fea-
extracting LGBP features. As observed from Table 3, LGBP
tures are almost comparable. However, LGBP features
features encoded by 𝐿𝐵𝑃𝑢2 from 5×5 facial regions
8,1 yielded best results with an accuracy about 86% which is
achived best recognition accuracy of 86.46%. On the other
better than both LBP and Gabor. This may be explained by
hand, based on F1 score, uniform pattern LGBP from 3×3
considering the fact that LGBP features combine the attrib-
regions performed best with an F1 score of 0.8233. Table 5
utes of both LBP and Gabor. However, the computational
displays the results obtained by dividing the Gabor mapsHAPPY ET AL.: THE INDIAN SPONTANEOUS EXPRESSION DATABASE FOR EMOTION RECOGNITION 11
cost of LGBP is very high compared to the individual fea- 8 REFERENCES
tures. In real-time applications, the LBP features may be
used since its computational complexity is very low com-
pared to other feature extraction techniques. [1] P. Ekman, Emotions Revealed: Recognizing Faces and Feelings
The best expression recognition performances of our to Improve Communication and Emotional Life, New York:
baseline algorithms are still low which may further be im- Times Books, 2003.
proved by extracting features from specific regions of the [2] A. Kleinsmith and N. Bianchi-Berthouze, ""Affective body
face. Addressing the illumination variations and face oc- expression perception and recognition: A survey,"" IEEE
clusions may also be considered to improve the accuracy. Transactions on Affective Computing, vol. 4, no. 1, pp. 15-33, 2013.
The presence of wide variations in facial expressions and
[3] Y. Tian, T. Kanade and J. F. Cohn, ""Facial expression
their intensities along with the occlusion, facial poses and
recognition,"" in Handbook of face recognition, London, Springer ,
arbitrary head movements are the practical scenarios
2011, pp. 487-519.
which should be tackled for improving the facial expres-
[4] E. G. Krumhuber, A. Kappas and A. S. Manstead, ""Effects of
sion recognition systems.
dynamic aspects of facial expressions: A review,"" Emotion
Review, vol. 5, no. 1, pp. 41-46, 2013.
6 CONCLUSION
[5] S. Wang, Z. Liu, S. Lv, Y. Lv, G. Wu, P. Peng, F. Chen and X.
Emotion recognition using facial expressions - a me- Wang, ""A natural visible and infrared facial expression database
dium for a natural way of communication with machines - for expression recognition and emotion inference,"" IEEE Trans.
needs spontaneous facial expression databases with relia- Multimedia, vol. 12, no. 7, pp. 682-691, 2010.
ble annotation of emotions. The ISED contains emotional [6] P. Ekman, ""Universals and Cultural Differences in Facial
responses in the Indian context and fulfils a number of as- Expressions of Emotion,"" in Proc. Nebraska Symp. Motivation,
pects of the desired requirements. The database includes 1971.
mild to strong spontaneous facial expressions. Its realistic
[7] P. Ekman, ""Strong Evidence for Universals in Facial
nature can help researchers to develop algorithms for
Expressions:A Reply to Russell’s Mistaken Critique,""
recognition of human emotions in practical situations. Sev-
Psychological Bull., vol. 115, no. 2, pp. 268-287, 1994.
eral strategies are adopted for the creation of the database
[8] Z. Zeng, M. Pantic, G. I. Roisman and T. S. Huang, ""A survey of
which have been briefly described. The tasks are designed
affect recognition methods: Audio, visual, and spontaneous
to keep the subjects engaged and to induce spontaneous
expressions,"" IEEE Trans. Pattern Anal. Mach. Intell. , vol. 31, no.
emotions. Since the expressions of an individual varies
1, pp. 39-58, 2009.
across time, several expressions of the same participants
are included in the ISED. The data collected after recording [9] A. Dhall, R. Goecke, S. Lucey and T. Gedeon, ""Collecting large,
of the experiments are further reduced to small video clips richly annotated facial-expression databases from movies,""
containing only the emotional expressions. The video clips IEEE MultiMedia, vol. 19, no. 3, pp. 34-41, 2012.
of the database are annotated carefully by trained decod- [10] K. Mase, ""Recognition of Facial Expression from Optical Flow,""
ers, which are further validated by the self-report of emo- IEICE Trans., vol. 74, no. 10, pp. 3474-3483, 1991.
tion by the participants and the type of stimuli used. We
[11] H. Kobayashi and F. Hara, ""The Recognition of Basic Facial
observed significant elicitation of sadness and surprise in
Expressions by Neural Network,"" in Proc. IEEE Int Joint Conf.
a few subjects, while most of the subjects displayed happi-
Neural Networks, 1991.
ness and disgust expressions easily.
Some evaluation protocols were carried out to provide [12] M. J. Lyons, M. Kamachi and J. Gyoba, ""Japanese Female Facial
reference evaluation results for researchers for further im- Expressions (JAFFE),"" Database of digital images, 1997.
provement of the expression recognition techniques. The [13] T. Kanade, J. F. Cohn and Y. Tian, ""Comprehensive database for
presence of unrestricted head movements and various face facial expression analysis,"" in 4th IEEE Int. Conf. on Automatic
poses are the primary issues which may be addressed to Face and Gesture Recognition, 2000.
improve the accuracy. Further, the face occlusions through
[14] M. Kamachi, V. Bruce, S. Mukaida, J. Gyoba, S. Yoshikawa and
spectacles, facial hair, hand etc. are also present in ISED
S. Akamatsu, ""Dynamic properties influence the perception of
which increase the complexity of expression recognition.
facial expressions,"" Perception, vol. 30, pp. 875-887, 2001.
The spontaneous facial expressions of ISED with a reliable
[15] S. Afzal and P. Robinson, ""Natural affect data-collection &
ground truth would help the researchers to develop and
annotation in a learning context,"" in 3rd Int. Conf. on Affective
validate their algorithms for practical applications.
Comput. and Intell. Interaction and Workshops, 2009.
7 ACKNOWLEDGMENT [16] A. Podlesek, L. Komidar, G. Socan and B. Bajec, ""Multi-modal
emotional database: AvID,"" Informatica, vol. 33, pp. 101-106,
The authors would like to thank all the participants for
2009.
their participation in the creation of this database. The au-
[17] I. Sneddon, M. McRorie, G. McKeown and J. Hanratty, ""The
thors would also like to thank the team of volunteers for
belfast induced natural emotion database,"" IEEE Trans. Affective
their helpful contribution in developing or collecting video
Computing, vol. 3, no. 1, pp. 32-41, 2012.
materials used in the experiments. Special thanks go to the
decoders who have helped us in manually annotating the [18] O. Martin, I. Kotsia, B. Macq and I. Pitas, ""The enterface’05
database. audio-visual emotion database,"" in IEEE 22nd Int. Conf. on Data12
Eng. Workshops, 2006 . emotion analysis; using physiological signals,"" IEEE Trans.
Affective Computing, vol. 3, no. 1, pp. 18-31, 2012.
[19] M. Valstar and M. Pantic, ""Induced disgust, happiness and
surprise: an addition to the MMI facial expression database,"" in [34] A. Tcherkassof, D. Dupré, B. Meillon, N. Mandran, M. Dubois
Proc. Int. Conf. Language Resources and Evaluation, 2010. and J.-M. Adam, ""Dynemo: A Video Database of Natural Facial
[20] M. Soleymani, J. Lichtenauer, T. Pun and M. Pantic, ""A Expressions of Emotions,"" International Journal of Multimedia &
multimodal database for affect recognition and implicit Its Applications, vol. 5, no. 5, 2013.
tagging,"" IEEE Trans. Affective Computing, vol. 3, no. 1, pp. 42-55, [35] R. Cowie, ""Perceiving emotion: towards a realistic
2012. understanding of the task,"" Philosoph. Trans. of the Roy. Soc. B:
[21] S. M. Mavadati, M. H. Mahoor, K. Bartlett, P. Trinh and J. Cohn., Biological Sciences, vol. 364, no. 1535, pp. 3515-3525, 2009.
""DISFA: A Spontaneous Facial Action Intensity Database,"" IEEE [36] ""Standard Expressor Version of the JACFEE,"" Humintell ,
Trans. Affective Computing, vol. 4, no. 2, pp. 151 - 160, 2013. [Online]. Available: http://www.humintell.com/research-
[22] M. Grimm, K. Kroschel and S. Narayanan, ""The Vera am Mittag news/.
German audio-visual emotional speech database,"" in IEEE Int. [37] P. Ekman, ""F.A.C.E. Training,"" Paul Ekman Group, [Online].
Conf. on Multimedia and Expo, 2008. Available: http://www.paulekman.com/product-
category/face-training/.
[23] E. Douglas-Cowie, R. Cowie, I. Sneddon, C. Cox, O. Lowry, M.
Mcrorie and J.-C. M. e. al., ""The HUMAINE database: [38] S. Carvalho, J. Leite, S. Galdo-Álvarez and Ó. F. Gonçalves, ""The
addressing the collection and annotation of naturalistic and emotional movie database (EMDB): A self-report and
induced emotional data,"" Affective Comput. and Intell. interaction, psychophysiological study,"" Appl. psychophysiology and
pp. 488-500, 2007. biofeedback, vol. 37, no. 4, pp. 279-294, 2012.
[24] G. McKeown, M. Valstar, R. Cowie, M. Pantic and M. Schroder, [39] J. Rottenberg, R. D. Ray and J. Gross, ""Emotion Elicitation Using
""The SEMAINE database: annotated multimodal records of Films,"" in Handbook of Emotion Elicitation and Assessment, Oxford
emotionally colored conversations between a person and a Univ. Press, 2007, pp. 9-28.
limited agent,"" IEEE Trans. Affective Computing, vol. 3, no. 1, pp. [40] M. Soleymani, J. Davis and T. Pun, ""A collaborative
5-17, 2012. personalized affective video retrieval system,"" in 3rd Int. Conf.
[25] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar and I. on Affective Comput. and Intell. Interaction and Workshops, 2009.
Matthews, ""The Extended Cohn-Kanade Dataset (CK+): A [41] A. Schaefer, F. Nils, X. Sanchez and P. Philippot, ""Assessing the
complete facial expression dataset for action unit and emotion- effectiveness of a large database of emotion-eliciting films: A
specified expression,"" in 3rd IEEE Workshop on CVPR for Human new tool for emotion researchers,"" Cognition and Emotion, vol.
Communicative Behavior Analysis, 2010. 24, no. 7, pp. 1153-1172, 2010.
[26] T. Bänziger and K. R. Scherer, ""Introducing the Geneva [42] J. L. Fleiss, ""Measuring nominal scale agreement among many
Multimodal Emotion Portrayal (GEMEP) corpus,"" in Blueprint raters,"" Psychological bulletin, vol. 76, no. 5, pp. 378-382, 1971.
for affective computing: A sourcebook, Oxford, England, 2010, pp.
[43] L. Devillers, R. Cowie, J. C. Martin, E. Douglas-Cowie, S.
271-294.
Abrilian and M. McRorie, ""Real life emotions in French and
[27] P. Ekman, W. V. Friesen and J. C. Hager, ""FACS Manual,"" Salt English TV video clips: an integrated annotation protocol
Lake City, UT: A Human Face, May 2002. combining continuous and discrete approaches,"" in 5th Int. Conf.
[28] W. E. Rinn, ""The neuropsychology of facial expression: a review on Language Resources and Evaluation, Genoa, Italy, 2006.
of the neurological and psychological mechanisms for [44] C. Juanjuan, Z. Zheng, S. Han and Z. Gang, ""Facial expression
producing facial expressions,"" Psychological bulletin, vol. 95, no. recognition based on PCA reconstruction,"" in 5th International
1, 1984. Conference on Computer Science and Education, 2010.
[29] P. J. Naab and J. A. Russell, ""Judgments of emotion from [45] T. Ojala, M. Pietikainen and T. Maenpaa, ""Multiresolution gray-
spontaneous facial expressions of New Guineans,"" Emotion, vol. scale and rotation invariant texture classification with local
7, no. 4, pp. 736-744, 2007. binary patterns,"" IEEE Transactions on Pattern Analysis and
[30] N. Sebe, M. S. Lew, Y. Sun, I. Cohen, T. Gevers and T. S. Huang, Machine Intelligence, vol. 24, no. 7, pp. 971-987, 2002.
""Authentic facial expression analysis,"" Image and Vision [46] H. B. Deng, L. W. Jin, L. X. Zhen and J. C. Huang, ""A new facial
Computing, vol. 25, no. 12, pp. 1856-1863, 2007. expression recognition method based on local gabor filter bank
[31] A. J. O'Toole, J. Harms, S. L. Snow, D. R. Hurst, M. R. Pappas and pca plus lda,"" International Journal of Information Technology,
and J. H. A. H. Abdi, ""A video database of moving faces and vol. 11, no. 11, pp. 86-96, 2005.
people,"" IEEE Trans. Pattern Anal. Mach. Intell., vol. 27, no. 5, pp. [47] S. Moore and R. Bowden, ""Local binary patterns for multi-view
812-816, 2005. facial expression recognition,"" Computer Vision and Image
[32] M. A. Quiros-Ramirez, S. Polikovsky, Y. Kameda and T. Understanding, vol. 115, no. 4, pp. 541-558, 2011.
Onisawa, ""Towards developing robust multimodal databases [48] T. Senechal, V. Rapp, H. Salam, R. Seguier, K. Bailly and L.
for emotion analysis,"" Joint 6th Int. Conf. on Soft Comput. and Prevost, ""Combining AAM coefficients with LGBP histograms
Intell. Syst. (SCIS) and 13th Int. Symp. on Advanced Intell. Syst.
in the multi-kernel SVM framework to detect facial action
(ISIS), pp. 589-594, 2012. units,"" in IEEE International Conference on Automatic Face &
[33] S. Koelstra, C. Muhl, M. Soleymani, J.-S. Lee, A. Yazdani, T. Gesture Recognition and Workshops, 2011.
Ebrahimi, T. Pun, A. Nijholt and I. Patras, ""Deap: A database for [49] A. Bosch, A. Zisserman and X. Munoz, ""Representing shapeHAPPY ET AL.: THE INDIAN SPONTANEOUS EXPRESSION DATABASE FOR EMOTION RECOGNITION 13
with a spatial pyramid kernel,"" in 6th ACM international stitute of Technology, Kharagpur. His research interests in-
conference on Image and video retrieval, 2007. clude non-linear and statistical signal processing, signal based
fault detection and diagnosis, real time and embedded signal
[50] F. Ahmed, ""Gradient directional pattern: a robust feature
processing, numerical linear algebra, and data driven diag-
descriptor for facial expression recognition,"" Electronics letters,
nostics.
vol. 48, no. 19, pp. 1203-1204, 2012.
[51] C. Shan, S. Gong and P. W. McOwan, ""Robust facial expression
Rajlakshmi Guha is a psychologist in the
recognition using local binary patterns,"" in IEEE International
counseling center, Indian Institute of Technol-
Conference on Image Processing, 2005.
ogy, Kharagpur, India since 2009. Before join-
[52] Y. Wang, H. Ai, B. Wu and C. Huang, ""Real time facial ing in IIT Kharagpur, she taught in the Uni-
expression recognition with adaboost,"" 17th International versity of Calcutta and the West Bengal State
Conference on Pattern Recognition, pp. 926-929, 2004. University. She received her doctorate degree
[53] C.-W. Hsu and C.-J. Lin, ""A comparison of methods for in Social Clinical Psychology from University of Calcutta. She
multiclass support vector machines,"" IEEE Transactions on has publications in the field of depression, stress and cognitive
Neural Networks, vol. 13, no. 2, pp. 415-425, 2002. behavior therapy. Her research interests include emotion
recognition, social cognition, executive functions and working
[54] I. Cohen, N. Sebe, A. Garg, L. S. Chen and T. S. Huang, ""Facial
memory, depression and cognitive behavior therapy. She is
expression recognition from video sequences: temporal and
currently a member of APA. She has received Prof S Sinha
static modeling,"" Computer Vision and Image Understanding, vol.
award for Social Psychology, from University of Calcutta in
91, no. 1, pp. 160-187, 2003.
2000.
S L Happy has received the B.Tech. (Hons.)
degree from Institute of Technical Education
and Research (ITER), India in 2011. Now he is
pursuing the joint MS – PhD degree at Indian
Institute of Technology Kharagpur, India. His
research interests include pattern recognition,
computer vision and facial expression analysis.
Priyadarshi Patnaik completed his Masters
in English Literature in 1992 and his PhD in
Indian Aesthetics (Indian Aesthetic Emo-
tions) in 1995. He joined IIT Kharagpur in the
year May 1997. His areas of research include
Indian aesthetics (aesthetic emotions), visual
culture and communication, cultural transla-
tion theory and practice, media and multimedia studies, and
nonverbal communication. He is currently working as Profes-
sor at the Department of Humanities and Social Sciences, IIT
Kharagpur. He has published about 30 research articles in na-
tional, international journals and edited books, written eight
volumes of critical and creative writing on areas such as aes-
thetic emotions, Indian aesthetics, communication, translation
and poetry, edited three books on aesthetics and aging, and
has published more than 30 of his poems, visual arts, illustra-
tions and photographs in international journals. In the past he
has, in association with Defense Institute of Psychological Re-
search, India, undertaken two projects on lie-detection using
non-verbal cues and negotiation and interrogation using non-
verbal cues.
Aurobinda Routray has received his Masters
degrees in 1991 from IIT Kanpur, India and
his PhD in 1999 from Sambalpur University,
India. He has also worked as a postdoctoral
researcher at Purdue University, USA, during
2003-2004. He is currently working as a pro-
fessor in the Department of Electrical Engineering, Indian In-"
215,217,The Karolinska directed emotional faces: a validation study,"['E Goeleven', 'R De Raedt', 'L Leyman']",2008,963,Karolinska Directed Emotional Faces,"classification, facial expression recognition","In this study, 490 pictures of human facial expressions from the Karolinska Directed Emotional  Faces database (KDEF; Lundqvist et al., Citation1998) were validated. The pictures were",No DOI,… and emotion,https://www.tandfonline.com/doi/abs/10.1080/02699930701626582,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,tandfonline.com,
216,218,The MUG facial expression database,"['N Aifanti', 'C Papachristou']",2010,479,Affective Faces Database,facial expression recognition,face in human communication it is natural that much research is conducted on facial expression  recognition the six facial expressions are performed according to the ’emotion prototypes,No DOI,… Workshop on Image …,https://mug.ee.auth.gr/fed/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,auth.gr,
217,219,The Toronto paper matching system: an automated paper-reviewer assignment system,"['L Charlin', 'R Zemel']",2013,170,Toronto Face Database,machine learning,We thank members of the Machine Learning group at the University of Toronto for their  help in developing the system (notably Hugo Larochelle and Amit Gruber) and for valuable,No DOI,,https://www.cs.toronto.edu/~lcharlin/papers/tpms.pdf,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,toronto.edu,
218,220,The elements of end-to-end deep face recognition: A survey of recent advances,"['H Du', 'H Shi', 'D Zeng', 'XP Zhang', 'T Mei']",2022,127,Toronto Face Database,deep learning,"In Section 5, we provide a review of deep learning-based methods for discriminative face   The images in PASCAL faces dataset [280] are taken from the Pascal person layout dataset [",No DOI,ACM Computing Surveys (CSUR …,https://arxiv.org/abs/2009.13290,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"The Elements of End-to-end Deep Face Recognition: A
Survey of Recent Advances
HANGDU∗,
ShanghaiUniversity,China
HAILINSHI∗,
JDAIResearch,China
DANZENG†,
ShanghaiUniversity,China
XIAO-PINGZHANG,
RyersonUniversity,Canada
TAOMEI,
JDAIResearch,China
Facerecognitionisoneofthemostpopularandlong-standingtopicsincomputervision.Withtherecent
developmentofdeeplearningtechniquesandlarge-scaledatasets,deepfacerecognitionhasmaderemarkable
progressandbeenwidelyusedinmanyreal-worldapplications.Givenanaturalimageorvideoframeasinput,
anend-to-enddeepfacerecognitionsystemoutputsthefacefeatureforrecognition.Toachievethis,atypical
end-to-endsystemisbuiltwiththreekeyelements:facedetection,facealignment,andfacerepresentation.
Thefacedetectionlocatesfacesintheimageorframe.Then,thefacealignmentisproceededtocalibrate
thefacestothecanonicalviewandcropthemwithanormalizedpixelsize.Finally,inthestageofface
representation,thediscriminativefeaturesareextractedfromthealignedfaceforrecognition.Nowadays,
allofthethreeelementsarefulfilledbythetechniqueofdeepconvolutionalneuralnetwork.Inthissurvey
article,wepresentacomprehensivereviewabouttherecentadvanceofeachelementoftheend-to-enddeep
facerecognition,sincethethrivingdeeplearningtechniqueshavegreatlyimprovedthecapabilityofthem.To
startwith,wepresentanoverviewoftheend-to-enddeepfacerecognition.Then,wereviewtheadvanceof
eachelement,respectively,coveringmanyaspectssuchastheto-datealgorithmdesigns,evaluationmetrics,
datasets,performancecomparison,existingchallenges,andpromisingdirectionsforfutureresearch.Also,
weprovideadetaileddiscussionabouttheeffectofeachelementonitssubsequentelementsandtheholistic
system.Throughthissurvey,wewishtobringcontributionsintwoaspects:first,readerscanconveniently
identifythemethodswhicharequitestrong-baselinestyleinthesubcategoryforfurtherexploration;second,
onecanalsoemploysuitablemethodsforestablishingastate-of-the-artend-to-endfacerecognitionsystem
fromscratch.
AdditionalKeyWordsandPhrases:Deeplearning,convolutionalneuralnetwork,facerecognition,face
detection,facealignment,facerepresentation.
1 INTRODUCTION
Face recognition (FR) is an extensively studied topic in computer vision. Among the existing
technologies of human biometrics, face recognition is the most widely used one in real-world
applications.Withthegreatadvanceofdeepconvolutionalneuralnetworks(DCNNs),thedeep
learningbasedmethodshaveachievedsignificantimprovementsonvariouscomputervisiontasks,
including face recognition. In this survey, we focus on 2D image based end-to-end deep face
recognitionwhichtakesthegeneralimagesorvideoframesasinput,andextractsthedeepfeature
ofeachfaceasoutput.Weprovideacomprehensivereviewoftherecentadvancesoftheelements
ofend-to-enddeepfacerecognition.Specifically,anend-to-enddeepfacerecognitionsystemis
composedofthreekeyelements:facedetection,facealignment,andfacerepresentation.Inthe
following,wegiveabriefintroductionofeachelement.
∗Equalcontribution.ThisworkwasperformedatJDAIResearch.
†Correspondingauthor.
Authors’addresses:HangDu,duhang@shu.edu.cn,ShanghaiUniversity,Shanghai,China;HailinShi,shihialin@jd.com,
JDAIResearch,Beijing,China;DanZeng,ShanghaiUniversity,Shanghai,China,dzeng@shu.edu.cn;Xiao-PingZhang,
xzhang@ryerson.ca,RyersonUniversity,Toronto,Canada;TaoMei,tmei@jd.com,JDAIResearch,Beijing,China.
1202
ceD
72
]VC.sc[
4v09231.9002:viXra2
(a) (b)
Fig.1. (a)Thepublicationtrendoftheelementsofend-to-enddeepfacerecognitionfrom2013to2020.(b)
Thestandardpipelineofend-to-enddeepfacerecognitionsystem.First,thefacedetectionstageaimsto
localizethefaceregionontheinputimage.Then,thefacealignmentisproceededtonormalizethedetected
facetothecanonicalview.Finally,thefacerepresentationdevotestoextractingfeaturesforrecognition.
Facedetectionisthefirststepofend-to-endfacerecognition.Itaimstolocatethefaceregionsin
thestillimagesorvideoframes.Beforethedeeplearningera,oneofthepioneeringworksforface
detectionisViola-Jones[230]facedetector,whichutilizesAdaBoostclassifierswithHaarfeatures
tobuildacascadedstructure.Lateron,thesubsequentapproachesexploretheeffectivehand-craft
features[8,162,169]andvariousclassifiers[17,123,150]toimprovethedetectionperformance.
Onecanreferto[286,299]forathoroughsurveyoftraditionalfacedetectionmethods.
Next,facealignmentreferstocalibratethedetectedfacetothecanonicalviewandcropittoa
normalizedpixelsize,inordertofacilitatethesubsequenttaskoffacerepresentationcomputing.It
isanessentialintermediateprocedureforfacerecognitionsystem.Generally,thefaciallandmark
localizationisnecessaryforfacealignment,whilesomeapproachescandirectlygeneratealigned
facefromtheinputone.Mosttraditionalworksoffaciallandmarklocalizationfocusoneither
generativemethods[36,37]ordiscriminativemethods[153,345],andthereareseveralexhaustive
surveysaboutthem[99,247,358].
In the face representation stage, the discriminative features are extracted from the aligned
faceimagesforrecognition.Thisisthefinalandcorestepoffacerecognition.Inearlystudies,
manyapproachescalculatethefacerepresentationbyprojectingfaceimagesintolow-dimensional
subspace,suchasEigenfaces[227]andFisherfaces[13].Lateron,handcraftedlocaldescriptors
basedmethods[3,131]prevailinthisarea.Foradetailedreviewofthesetraditionalmethods,one
canreferto[7,231,307].Inthelastfewyears,thefacerepresentationbenefitsfromthedevelopment
ofDCNNsandwitnessesgreatimprovementsforhighperformancefacerecognition.
This survey focuses on reviewing and analyzing the recent advances in each element. An
importantfactisthat,theperformanceoffacerecognitiondependsonthecontributionofallthe
elements(i.e.,facedetection,alignmentandrepresentation).Inotherwords,inferiorityinanyone
oftheelementswillbecomethebottleneckandharmthefinalperformance.Inordertoestablish
high-performanceend-to-endfacerecognitionsystem,itisnecessarytounderstandeveryelement
oftheholisticframeworkandtheirintrinsicconnection.Anumberoffacerecognitionsurveys
havebeenpublishedinthepasttwentyyears.Themaindifferencesbetweenoursurveyandthe
existingonesaresummarizedasfollows.
• Therelationshipbetweentheelementsandwhole.Weprovidethethoroughdiscussion
abouttheeffectofeachelementonitssubsequentoneandtheholisticsystem,whichare
overlookedintheexistingsurveys.Fromtheexistingexperimentsanddetailedanalysis,we
canconcludetheperformanceoftheholisticsystemdependsonthethreeelements.Therefore,3
itisnecessarytoreviewthemtogetherforhelpingthereaderswhoaimtoestablishstate-of-
the-artfacerecognitionsystemfromscratch.
• Morerecentlypublishedworks.Thepublicationsinthelastthreeyears(2018-2020)are
muchmorethanallthosepublishedbefore2018(asillustratedinFig.1(a)).Inviewoftherapid
developmentoffacedetection,facealignmentandfacerepresentationinthepastfewyears,
thissurveycoverstherecentlypublishedarticles.Bydoingso,weprovidetheup-to-date
reviewoftheelements,andlargenumberofnewlypresentedmethods.
• Newanalysisforfuturework.Basedontheup-to-datereview,weconcludethepromising
trendsfromthenewestfrontier,andseveralinsightfulthoughtsofeachelementaswellas
theholisticsystem,toenlightenthefutureresearch.
Specifically,therearecertainsurveys[7,231,307]aboutfacerecognitionwho,however,donot
coverdeeplearningbasedmethodssincetheyarepublishedearlybeforethedeeplearningera;
besides,anothersetofsurveysfocuson3Dfacerecognition[16,201]andspecifictasks[49,356].
Instead,wefocusonthe2Dfacerecognitionwhichisthemostneededinpracticalapplications.
Fordeeplearningbased2Dfacerecognition,thereareasmallnumberofarticlesthatfulfilrelevant
survey,whichdifferfromthispaperinmanyways.Amongthem,Ranjanetal.[177]donotinclude
therecenttechniquesthatrapidlyevolvedinthepastfewyears.Infact,thenumberofpublished
works has been increasing dramatically during these years (as shown in Fig. 1(a)). Wang and
Deng[244]presentasystematicreviewaboutdeepfacerepresentationratherthantheend-to-end
facerecognition.Morerecently,Insafetal.[2]provideareviewof2Dand3Dfacerecognitionfrom
thetraditionaltodeep-learningera,whilethescopeisstilllimitedinthefacerepresentation.In
summary,theend-to-endfacerecognition,coveringalltheelementsofthepipeline,needstobe
systematicallyreviewed,whileseldomoftheexistingsurveyarticlesattachimportancetothistask.
Therefore,wesystematicallyreviewthedeeplearningbasedapproachesofeachelementin
theend-to-endfacerecognition,respectively.Thereviewofeachelementcoversmanyaspects:
algorithmdesigns,evaluationmetrics,datasets,performancecomparisons,remainingchallenges,
andpromisingdirectionsforfutureresearch.Wehopethissurveycouldbringhelpfulthoughtsfor
betterunderstandingofthebigpictureofend-to-endfacerecognitionanddeeperexplorationina
systematicway.Specifically,themaincontributionscanbesummarizedasfollows:
• Weprovideacomprehensivesurveyoftheelementsofend-to-enddeepfacerecognition.We
reviewtherecentadvancesofeachelement,respectively,andpresentelaboratedcategoriza-
tionsofthemtomakethereadersunderstandtheminasystematicway.
• Wereviewthethreeelementsfrommanyaspects:algorithmdesigns,evaluationmetrics,
datasets,andperformancecomparison.Moreover,wepointouttheeffectofeachelementon
itssubsequentelementsandtheholisticsystem.
• Wecollecttheexistingchallengesandpromisingdirectionsforeachelementanditssubcate-
goriestofacilitatethefutureresearch,andfurtherdiscussthemajorchallengesandfuture
trendsfromtheviewoftheholisticframework.
2 OVERVIEW
Atypicalend-to-enddeepfacerecognitionsystemincludesthreebasicelements:facedetection,
facealignment,andfacerepresentation,asshowninFig.1(b).First,facedetectionlocalizesthe
faceregionontheinputimage.Then,facealignmentisproceededtonormalizethedetectedface
intothecanonicallayout.Finally,facerepresentationdevotestoextractingdiscriminativefeatures
fromthealignedface.Thefeaturesareusedtocalculatethesimilaritybetweenthem,inorderto
makethedecisionthatwhetherthefacesbelongtothesameidentity.4
Fig.2. Thestructureofthissurvey.Theleftparts(Section1,2,6)refertothefunctionalcontentsthatprovide
overallintroductionanddiscussion.Therightparts(Section3,4,5)refertothetechnicalcontentsthatprovide
thedetailedreviewingofthreeelements.
ThestructureofthissurveyisillustratedinFig.2.Westructurethebodysections(Section3,4,5)
withrespecttothethreeelements,eachofwhichisaresearchtopicthatcoversabundantliteratures
incomputervision.Wegiveanoverviewofthethreeelementsbrieflyinthissection,anddiveinto
eachoftheminthefollowingbodysections.
2.1 FaceDetection
Facedetectionisthefirstprocedureofthefacerecognitionsystem.Givenaninputimage,the
facedetectionaimstofindallthefacesintheimageandgivethecoordinatesofboundingbox
withaconfidencescore.Themajorchallengesoffacedetectioncontainvaryingresolution,scale,
pose,illumination,occlusion,etc.InSection3,weprovideacategorizationofthedeeplearning
basedfacedetectionmethodsfrommultipledimensions,whichincludesmulti-stage,single-stage,
anchor-based, anchor-free, multi-task learning, CPU real-time and problem-oriented methods.
Itisworthnotingthatthereexistoverlappingtechniquesbetweenthecategories,because,the
categorizationisbuiltupfrommultipleperspectives.
Differencestotheexistingsurveyoffacedetection.Minaeeetal.[160]reviewfacedetection
methods from the beginning of deep learning era, and categorize them by design of network
architecture.Comparedwiththem,ourcategorizingcriterioncoverspoly-aspects.Specifically,we
provideamultiple-dimensioncategorization,todiscussthefacedetectionmethodsfrommany
differentperspectives,whichwillhelpustobetterunderstandthedevelopinglineandconclude5
Fig.3. Visualizationoffaciallandmarksofdifferentversions.The4-pointand5-pointlandmarksareoften
usedforfacealignment.
thefuturetrend.Sincefacedetectionstateoftheartisrelativelyadvanced,suchcomprehensive
categorizationisnecessaryforreaders.
2.2 FaceAlignment
Inthesecondstage,facealignmentaimstocalibratethedetectedfacetothecanonicalview.Since
humanfaceappearswiththeregularstructure,inwhichthefacialparts(eyes,nose,mouth,etc)
haveconstantarrangement,thealignmentoffaceisofgreatbenefittothesubsequentfeature
computationforfacerecognition.Formostexistingmethodsoffacealignment,thefaciallandmarks,
or so-called facial keypoints (as shown in Fig. 3), are indispensable, because they are involved
asthereferenceforsimilaritytransformationoraffinetransformation. So,thefaciallandmark
localizationisaprerequisiteforfacealignment.TheDCNNsbasedfaciallandmarklocalization
methodscanbedividedintothreesubcategories:coordinateregression,heatmapregressionand
3Dmodelfittingbasedapproaches.Withoutrelyingonthefaciallandmarks,severalapproaches
candirectlyoutputalignedfacefromtheinputbylearningthetransformationparameters.Wewill
reviewthesemethodsinSection4.
Differencestotheexistingsurveyoffacealignment.Previoussurveysoffacealignment[99,
247,358]onlyfocusonreviewingthefaciallandmarklocalizationmethods.Sincethelandmark-free
facealignmentisalsoakindofmethodstogeneratealignedimagesforfacerecognition,wefurther
collecttheminthissurvey.
2.3 FaceRepresentation
As the key step of face recognition system, face representation devotes to learning deep face
modelandusingittoextractfeaturesfromalignedfacesforrecognition.Thefeaturesareusedto
calculatethesimilarityofthematchedfaces.InSection5,weprovideareviewofdeeplearning
basedmethodsfordiscriminativefacefeatures,andretrospectthesemethodswithrespecttothe
networkarchitectureandthetrainingsupervision.Fornetworkarchitecture,weintroducethe
generalarchitectureswhicharedesignedforawiderangeofcomputervisiontasks,andthespecial
architectureswhicharespecializedforfacerepresentation.Asfortrainingsupervision,wemainly
introducefourschemes,includingtheclassification,featureembedding,hybridandsemi-supervised
schemes.Additionally,wepresentseveralspecificfacerecognitionscenes,includingcrossdomain,
low-shotlearningandvideobasedscenarios.
Differences to the existing survey of face representation. This survey aims to provide
thereaderswithabetterunderstandingoftheend-to-endfacerecognition.Recently,Wangand
Deng[244]presentasystematicreviewaboutdeepfacerecognition,inwhichtheymainlyfocus
ondeepfacerepresentation,andthecategorizationoftraininglossissub-optimal.Forinstance,
theysortthesupervisedlearningofdeepfacerepresentationbyEuclidean-distancebasedloss,
angular/cosine-margin-basedloss,softmaxlossanditsvariations;while,infact,almostallthe
angular/cosine-margin-basedlossesareimplementedasthevariationofsoftmaxlossratherthanan
individualset.Incontrast,wesuggestamorereasonablecategorizationwiththreesubcategories,
i.e.,classification,featureembedding,andhybridmethods(inSection5.2).6
Fig.4. Thedevelopmentofrepresentativefacedetectionmethods.Theblueandgrayrepresentmulti-stageand
single-stagemethods;accordingtotheanchorusage,therectangle,oval,anddiamonddenoteanchor-based,
anchor-freeandothermethods.OnecanrefertoTable1forthereferencesofthesemethods.
3 FACEDETECTION
Facedetectionisthefirststepofend-to-endfacerecognitionsystem,whichaimstolocatetheface
regionsfromtheinputimages.Inthissection,first,wecategorizeandmakecomparisonofthe
existingdeeplearningmethodsforfacedetection.Next,weintroduceseveralpopulardatasets
of face detection and the common metrics for evaluation. Finally, we provide a performance
comparisonofstate-of-the-artfacedetectionmethodsanddetaileddiscussionabouttheeffectof
facedetectiononitssubsequentelements.
3.1 CategorizationofFaceDetection
Inordertopresentthedeepfacedetectionmethodswithaclearcategorization,wegroupthem
withsevensets,i.e.,multi-stage,single-stage,anchor-based,anchor-free,multi-tasklearning,CPU
real-time,andproblem-orientedmethods(inTable1).Thesesetsarenotnecessarilyexclusive,
becauseweestablishthecategorizationfrommultipleperspective.Fig.4isthedevelopmentof
representativemethodsforfacedetection.
3.1.1 Multi-stagemethods. Followingthecoarse-to-finemannerortheproposal-to-refinestrategy,
multi-stagebaseddetectorsfirstgenerateanumberofcandidateboxes,andthenrefinethecan-
didatesbyoneormoreadditionalstages.Thefirststageemploysslidingwindowtoproposethe
candidateboundingboxesatagivenscale,andthelatterstagesrejectthefalsepositivesandrefine
theremainingboxes.Insuchregime,thecascadedarchitecture[119,193,301,314]isnaturallyan
effectivesolutionforthecoarse-to-finefacedetection.
Facedetectioncanbeconsideredasaspecificobjectiveofgeneralobjectdetection.Thus,many
works[27,63,93,97,124,166,176,208,304,348]inherittheremarkableachievementsfromthe
general object detectors. For instance, Faster R-CNN [181] is a classic and effective detection
frameworkwhichemploysaregionproposalnetworktogenerateregionproposalswithasetof
denseanchorboxesinthefirststage,andthenrefinestheproposalsinthesecondstage.Based
ontheproposal-to-refinescheme,manyworkshavededicatedtoimprovethemodelingofthe
refinementstage[93,97,208,304,348]andtheproposalstage[27,75,124,166,202],andachieved
greatprogressforaccuratefacedetection.Apartfromthemodeling,howtotrainthemulti-stage
detectorisanotherinterestingtopic.Totackletheissueofinferioroptimizationformulti-stage
detectors,ajointtrainingstrategy[174]isdesignedforbothCascadedCNN[119]andFasterR-CNN
toachieveend-to-endoptimizationandbetterperformance.
3.1.2 Single-stagemethods. Thesingle-stagemethodsaccomplishthecandidateclassificationand
boundingboxregressionfromthefeaturemapsdirectly,withoutthedependenceonproposalstage.
A classic structure of single stage comes from a general object detector named Single Shot
multiboxDetector(SSD)[136].Itrunsmuchfasterthanthemulti-stageoneswhilemaintaining7
Table1. Thecategorizationofdeepfacedetectionmethods.
Category Description Method
Multi-stage Detectorsfirstgeneratecandidateboxes,thenthefollowing Faceness[287],HyperFace[176],STN[27],ConvNet-3D[124],SAFD[75],CMS-
oneormorestagesrefinethecandidates. RCNN[348],Wanetal.[232],Jiangetal.[97],DeepIR[208],Gridloss[170],Face
R-CNN[93],FaceR-FCN[253],ZCC[347],FDNet[304],FA-RPN[166],Cascaded
CNN[119],MTCNN[314],Qinetal.[174],LLE-CNNs[63],PCN[193],PPN[301]
Single-stage Detectorsaccomplishfaceclassificationandboundingbox DDFD[59],DenseBox[89],UnitBox[294],HR[85],Faceboxes[321],SSH[165],
regressionfromfeaturemapsatonce. S3FD[322],DCFPN[323],FAN[242],FANet[313],RSA[143],S2AP[202],Pyra-
midBox[221],DF2S2[223],SFace[241],DSFD[120],RefineFace[318],SRN[32],
PyramidBox++[125],CenterFace[279],VIM-FD[330],ISRN[320],AInnoFace[308],
ASFD[303],RetinaFace[41],HAMBox[145]
Anchor-based Detectorsdeployanumberofdenseanchorsonthefeature Wanetal.[232],FaceFasterRCNN[97],RSA[143],FaceR-CNN[93],FDNet[304],
maps,andthenproceedtheclassificationandregressionon DeepIR[208],SAFD[75],SSH[165],S3FD[322],DCFPN[323],Faceboxes[321],
theseanchors. FAN[242],FANet[313],PyramidBox[221],ZCC[347],S2AP[202],DF2S2[223],
SFace[241],RetinaFace[41],DSFD[120],RefineFace[318],SRN[32],VIM-FD[330],
PyramidBox++[125],FA-RPN[166],ISRN[320],AInnoFace[308],GroupSam-
pling[161],HAMBox[145],ASFD[303],
Anchor-free Detectorsdirectlyfindfaceswithoutpresetanchors. DenseBox[89],UnitBox[294],CenterFace[279]
Multi-task Detectorsjointlylearntheclassificationandboundingbox STN[27],ConvNet-3D[124],HyperFace[176],MTCNN[314],FaceR-CNN[93],
learning regressionwithadditionaltasks(e.g.,landmarklocalization) RetinaFace[41],DF2S2[223],FLDet[355],PyramidBox++[125],CenterFace[279]
inoneframework.
CPUreal-time DetectorscanrunonasingleCPUcoreinreal-timeforVGA- CascadeCNN[119],STN[27],MTCNN[314],DCFPN[323],Faceboxes[321],
resolutionimages. PCN[193],RetinaFace[41],FLDet[355],FBI[98],PPN[301],CenterFace[279]
Problem- Detectorsaimtosolvespecificchallengesinfacedetection, HR[85],SSH[165],S3FD[322],Baietal.[9],PyramidBox[221],Gridloss[170],
oriented suchastinyfaces,occludedfaces,rotatedandblurryfaces. FAN[242],LLE-CNNs[63],PCN[193],GroupSampling[161]
comparableaccuracy.BasedonSSD,manystudies[98,221,321–323]developdeepfacedetectors
thosearerobusttodifferentscalesofface.Asforthebackbonearchitecture,manyfacedetectors
resort to the feature pyramid network (FPN) [127] which consists of a top-down architecture
withskipconnectionsandmergesthehigh-levelandlow-levelfeaturesfordetection.Thehigh-
level feature maps provide rich semantic information, while the low-level layers supplement
morelocalinformation.Thefeaturefusionpreservestheadvantagesfrombothsides,andbrings
greatprogressindetectingobjectswithawiderangeofscales.Therefore,manysingle-stageface
detectors[32,41,120,125,165,221,223,242,313,320]aredevelopedwiththeadvantageofFPN.
NotonlyhandlingthescaleissueinfacedetectionviaFPN,butalsothesemethodsattempttosolve
theinherentshortcomingsoforiginalFPNsuchliketheconflictofreceptivefield.
Althoughthesingle-stagemethodshavetheadvantageofhighefficiency,theirdetectionaccuracy
isbelowthatofthetwo-stagemethods.Itispartiallybecausetheimbalanceproblemofpositivesand
negativesbroughtbythedenseanchors,whereastheproposal-to-refineschemeisabletoalleviate
thisissue.Accordingly,RefineDet[319]setsupananchorrefinementmoduleinitsnetworkto
removelargenumberofnegatives.InspiredbyRefineDet,SRN[32]presentsaselectivetwo-step
classificationandregressionmethod;thetwo-stepclassificationisperformedatthelow-levellayers
toreduce thesearchspaceof classifier, andthetwo-stepregression isperformedat high-level
layerstoobtainaccuratelocation.Lateron,manyworks[308,318,320,330]improveSRNwith
severaleffectivetechniques,suchastrainingdataaugmentation,improvedfeatureextractorand
trainingsupervision,anchorassignmentandmatchingstrategy,multi-scaleteststrategy,etc.
Mostaforementionedmethodsneedtopresetanchorsforfacedetection,whilesomerepresenta-
tivedetectorsofsingle-stage,suchasDenseBox[89],UnitBox[294]andCenterFace[279],fulfilthe
detectionwithoutpresetanchors.Wewillpresentthemasanchor-freetypeinthenextsubsection.
3.1.3 Anchor-basedandanchor-freemethods. AsshowninTable1,mostcurrentfacedetectorsare
anchor-basedduetothelong-timedevelopmentandsuperiorperformance.Generally,wepreset
theanchorsonthefeaturemaps,thenfulfiltheclassificationandboundingboxregressiononthese
anchorsoneormoretimes,andfinallyoutputtheacceptedonesasthedetectionresults.Therefore,
theanchorallocationandmatchingstrategyarecrucialtothedetectionaccuracy.Mostanchor-based8
methodsfocusonthealgorithmsalongthisdirection,suchasscalecompensation[145,322],max-
outbackgroundlabel[322],expectedmaxoverlappingscore[347],groupsamplingbyscale[161],
etc.However,thesettings(e.g.,scale,stride,ratio,number)ofanchorsneedtobecarefullytuned
foreachparticulardataset,limitingtheirgeneralizationability.Besides,thedenseanchorsincrease
thecomputationalcostandbringtheimbalanceproblemofpositiveandnegativeanchors.
Anchor-freemethods[116,224,346]attractgrowingattentioningeneralobjectdetection.As
forfacedetection,certainpioneeringworkshaveemergedinrecentyears.DenseBox[89]and
UnitBox[294]attempttopredictthepixel-wiseboundingboxonface.CenterFace[279]regards
facedetectionasageneralizedtaskofkeypointestimation,whichpredictsthefacialcenterpoint
andthesizeofboundingboxinfeaturemap.Inbrief,theanchor-freedetectorsgetridofthepreset
anchorsandachievebettergeneralizationcapacity.Regardingtothedetectionaccuracy,itneeds
furtherexplorationforbetterrobustnesstofalsepositivesandstabilityintrainingprocess.
3.1.4 Multi-tasklearningmethods. Generally,themulti-tasklearningmethodsaredesignedfor
solvingaproblemtogetherwithotherrelatedtasksbysharingthevisualrepresentation.Here,we
introducethemulti-tasklearningmethodsthattrainthefacedetectorwiththeassociatedfacialtasks
orauxiliarysupervisionbranchestoenrichthefeaturerepresentationanddetectionrobustness.
Manyapproaches[27,89,124,279,305,314,355]haveexploredthejointlearningoffacedetection
andfaciallandmarklocalization.Amongthem,MTCNN[314]isthemostrepresentativeone,which
exploits the inherent correlation between facial bounding boxes and landmarks. Subsequently,
HyperFace[176]fusesthelow-levelfeaturesandhigh-levelfeaturestosimultaneouslyconduct
fourtasks,includingfacedetection,faciallandmarklocalization,genderclassificationandpose
estimation.BasedonRetinaNet[128],RetinaFace[41]integratesfacedetection,faciallandmark
localization and dense 3D face regression in one framework. From the multi-task routine, we
can see that the face detectors can benefit from the associated facial tasks. Moreover, certain
methods[93,125,223,241]exploitauxiliarysupervisionbranches,suchassegmentationbranch,
anchor-freebranch,etc.Thesebranchesareusedtoboostthetrainingoffacedetection.
3.1.5 CPUreal-timemethods. Althoughstate-of-the-artfacedetectorshaveachievedgreatsuccess
in accuracy, their efficiency is not enough for real-world applications, especially on non-GPU
devices.AccordingtothedemandofinferencespeedonCPU,wecollecttheCPUreal-timeface
detectors[27,41,193,279,301,321,323,355]hereforconvenientretrieval.Thesedetectorsare
abletorunatleast20framespersecond(FPS)onasingleCPUwithVGA-resolutioninputimages.
We provide a table in the supplemental material which shows the running efficiency of them,
amongwhichthelightweightbackbone[41,279],rapidlydigestedconvolutionallayer[321,323],
knowledgedistillation[98]andregion-of-interest(RoI)convolution[27]arethecommonpractices.
3.1.6 Problem-orientedmethods. Wehighlightsomeproblem-orientedmethodswhicharedesigned
against a variety of specific challenges in face detection. Detecting faces with a wide range of
scaleisalong-existingchallengeinfacedetection.Agroupofmethods[85,161,165,221,322]
are designed for scale-invariant face detection, including scale selection, multi-scale detection,
denseanchorsetting,scalebalancingstrategy,etc.Thepartiallyvisiblefaces(i.e.,withocclusion)
isanotherissuethatharmsthedetectionrecall.Theexistingsolutions[63,170,242,287]resort
tothefacialpartarrangement,anchor-levelattentionanddataaugmentationbygeneration,etc.
Likewise, thein-planerotationisan existingfactorthatimpedesfacedetection. Totacklethis
problem,PCN[193]calibratesthecandidatesagainsttherotationprogressively.9
Table2. Statisticsofpopulardatasetsforfacedetection.
Datasets Year #Image #Face #offacesperimage Description
Training
ALFW[111] 2011 21,997 25,993 1.18 Trainingsourceforfacedetection.
WIDERFACE[288] 2016 16K 199K 12.43 Thelargestfacedetectiondataset.
Test
FDDB[229] 2010 2,845 5,171 1.82 Aclassicfacedetectionbenchmark.
AFW[353] 2012 205 473 2.31 Multiplefacialannotations.
PASCALfaces[280] 2014 851 1,335 1.57 Largefacialvariations.
MALF[281] 2015 5,250 11,931 2.27 Fine-grainedevaluation.
WIDERFACE[288] 2016 16K 194K 12.12 Thelargestfacedetectiondataset.
MAFA[63] 2017 30,811 35,806 1.16 Maskedfacedetection.
3.2 EvaluationMetricsandDatasets
3.2.1 Metrics. Likethegeneralobjectdetection,averageprecision(AP)isawidelyusedmetricfor
evaluatingthefacedetectionmethods.APisderivedfromtheprecision-recallcurve.Toobtain
precisionandrecall,IntersectionoverUnion(IoU)isusedtomeasuretheoverlapofthepredicted
boundingbox(𝐵𝑜𝑥 )andtheground-truth(𝐵𝑜𝑥 ),whichcanbeformulatedas
p gt
𝑎𝑟𝑒𝑎(𝐵𝑜𝑥 ∩𝐵𝑜𝑥 )
IoU= p gt . (1)
𝑎𝑟𝑒𝑎(𝐵𝑜𝑥 ∪𝐵𝑜𝑥 )
p gt
Thepredictionoffacedetectorincludesapredictedboundingboxanditsconfidencescore.The
confidencescoreisusedtodeterminewhethertoacceptthisaccordingtotheconfidencethreshold.
Then,anacceptedpredictioncanberegardedastruepositive(TP)iftheIoUislargerthanapreset
IoUthreshold(usually0.5forfacedetection).Otherwise,itwillberegardedasafalsepositive(FP).
AfterdeterminingtheTPandFP,theprecision-recallcurvecanbedrawnbyvaryingtheconfidence
threshold.APiscomputedasthemeanprecisionataseriesofuniformly-spaceddiscreterecall
levels[57].ApartfromAP,thereceiveroperatingcharacteristic(ROC)curveisalsoadoptedas
themetric,suchastheevaluationinFDDB[229];framespersecond(FPS)isusedtomeasurethe
runtimeefficiencyofdetectors.
3.2.2 Datasets. Weintroduceseveralwidelyuseddatasetsforfacedetection.Thestatisticsofthem
aregiveninTable2.Amongthem,FDDB[229]isaclassicdatasetofunconstrainedfacedetection
whichincludeslowresolution,occlusionanddifficultposevariations.ItisnoteworthythatFDDB
usesellipseasground-truthinsteadofrectangularbox.TheimagesinPASCALfacesdataset[280]
aretakenfromthePascalpersonlayoutdataset[58].WIDERFACE[288]providesalargenumber
oftrainingdataandachallengingtestbenchmarkwithlargedatavariations.
3.3 PerformanceComparison
Table3showstheperformanceoftheexistingfacedetectorsonWIDERFACEvalidationandtest
subsets.Fromtheviewpointofsubcategory,wecanobservethatthesingle-stagemethodswith
anchor-basedmechanism(e.g.,RefineFace[318],HAMBox[145])dominatethestate-of-the-art
performance.Formanyreal-worldapplications,MTCNN[314],Faceboxes[321],andRetinaFace[41]
arethewidelyusedfacedetectorsforbuildingafacerecognitionsystem,sincetheycanachieve
goodbalancebetweenthedetectionaccuracyandefficiency.
3.4 EffectontheSubsequentElements
Facedetectionistheveryfirstprocedureintheend-to-endfacerecognitionsystem,andthereby
playstheroleofinput towardsfacealignmentandfacerepresentation.Thequalityofdetection
bounding box directly influences on the performance of the subsequent alignment. There are
twopossiblecases,i.e.,thelossoffacialregionandtheexcessivelyresidualcontextregioninthe10
Table3. Theperformanceofstate-of-the-artmethodsontheWIDERFACE[288]validationandtestsubsets.
TheevaluationmetricisAP.
WIDERFACEVal. WIDERFACETest
Method Publication Subcategory
Easy Medium Hard Easy Medium Hard
Faceness-WIDER[288] CVPR’16 Multi-stage 0.713 0.634 0.345 0.716 0.604 0.315
MSC-CNN[288] CVPR’16 Multi-stage 0.691 0.664 0.424 0.711 0.636 0.400
CMS-RCNN[348] DLB’17 Multi-stage 0.899 0.874 0.624 0.902 0.874 0.643
FaceR-CNN[93] arXiv’17 Multi-stage,Anchor-based 0.937 0.921 0.831 0.932 0.916 0.827
FaceR-FCN[253] arXiv’17 Multi-stage,Anchor-based 0.947 0.935 0.874 0.943 0.931 0.876
ZCC[347] CVPR’18 Multi-stage,Anchor-based 0.949 0.933 0.861 0.949 0.935 0.865
FDNet[304] arXiv’18 Multi-stage,Anchor-based 0.959 0.945 0.879 0.950 0.939 0.878
FA-RPN[166] CVPR’19 Multi-stage,Anchor-based 0.949 0.941 0.894 0.945 0.937 0.891
MTCNN[314] SPL’16 Multi-stage,CPUreal-time,Multi-tasklearning 0.848 0.825 0.598 0.851 0.820 0.607
HR[85] CVPR’17 Single-stage 0.925 0.910 0.806 0.923 0.910 0.819
SSH[165] ICCV’17 Single-stage,Anchor-based 0.931 0.921 0.845 0.927 0.915 0.844
SF3D[322] ICCV’17 Single-stage,Anchor-based 0.937 0.925 0.859 0.935 0.921 0.858
FAN[242] arXiv’17 Single-stage,Anchor-based 0.952 0.940 0.900 0.946 0.936 0.885
PyramidBox[221] ECCV’18 Single-stage,Anchor-based 0.961 0.950 0.889 0.956 0.946 0.887
SRN[32] AAAI’19 Single-stage,Anchor-based 0.964 0.952 0.901 0.959 0.948 0.896
VIM-FD[330] arXiv’19 Single-stage,Anchor-based 0.967 0.957 0.907 0.962 0.953 0.902
DSFD[120] CVPR’19 Single-stage,Anchor-based 0.964 0.957 0.904 0.960 0.953 0.900
ISRN[320] arXiv’19 Single-stage,Anchor-based 0.967 0.958 0.909 0.963 0.954 0.903
AInnoFace[308] arXiv’19 Single-stage,Anchor-based 0.971 0.961 0.918 0.965 0.957 0.912
RefineFace[318] TPAMI’20 Single-stage,Anchor-based 0.971 0.962 0.920 0.965 0.958 0.914
HAMBox[145] CVPR’20 Single-stage,Anchor-based 0.970 0.964 0.933 0.960 0.955 0.923
ASFD[303] arXiv’20 Single-stage,Anchor-based 0.972 0.965 0.925 0.967 0.962 0.921
Faceboxes[321] IJCB’17 Single-stage,Anchor-based,CPUreal-time 0.840 0.766 0.395 0.839 0.763 0.396
DF2S2[223] arXiv’18 Single-stage,Anchor-based,Multi-tasklearning 0.969 0.959 0.912 0.963 0.954 0.907
PyramidBox++[125] arXiv’19 Single-stage,Anchor-based,Multi-tasklearning 0.965 0.959 0.912 0.956 0.952 0.909
CenterFace[279] arXiv’19 Single-stage,Anchor-free,CPUreal-time,Multi-tasklearning 0.935 0.924 0.875 0.932 0.921 0.873
RetinaFace[41] CVPR’20 Single-stage,Anchor-based,CPUreal-time,Multi-tasklearning 0.971 0.961 0.918 0.963 0.958 0.914
(a) (b)
Fig.5. Theaccuracyoffacedetectioncaninfluenceonthesubsequentelements,i.e.,facealignmentandface
representation.(a)Inaccuratelydetectedboundingboxeswillbringtheperformancedegradationtofacial
landmarklocalization[275].(b)Amorerobustfacedetectorcanfurtherimprovetherecognitionaccuracy[41].
boundingbox,bothofwhicharetheadversefactorstothesubsequentprocess.Certainrelevant
literatureshowssolidevidenceofthatfacedetectioninfluenceonthefacealignmentandface
recognition. Firstly, the quality of detected bounding boxes has a significant impact to facial
landmark localization. For example, Xiong et al. [275] compare the performance (Fig. 5(a)) of
faciallandmarklocalizationonthecorrectfaceboundingboxesandshrunkfaceboundingboxes,
indicatingthattheinaccuratelydetectedboundingboxeswillbringtheperformancedegradationto
thelandmarklocalization.Moreover,asshowninFig.5(b),RetinaFace[41]comparestherecognition
accuracyafterusingdifferentfacedetectionmethods,whichprovesthatarobustfacedetectorcan
furtherimprovethefacerecognitionaccuracy.Insummary,facedetectionhassignificantimpact
tobothfacealignmentandfacerepresentation.Itisindispensabletoconsidertheeffectofface
detectionwhenestablishinghigh-performancefacerecognitionsystem.11
Fig.6. Thedevelopmentofrepresentativemethodsforfacealignment.Theorange,blue,green,andyellow
representcoordinateregression,heatmapregression,3Dmodelfitting,andlandmark-freefacealignment
methods,respectively.OnecanrefertoTable4forthereferencesofthesemethods.
Table4. Thecategorizationoffacealignmentmethods.
Category Description Method
Landmark-based Coordinate Takethelandmarkcoordinatesasthetar- DCNC[212],EFLL[344],CFAN[312],TCDCN[331],RAR[273],MDM[226],
FaceAlignment regression getofregression,andlearnthenonlinear TSR[149],JFA[277],SIR[271],TCNN[268],DSRN[159],SBR[53],Wing
mappingfromtheinputfaceimagetothe loss[61],AAN[296],ODN[350],HyperFace[176],MTCNN[314],Reti-
landmarkcoordinates. naFace[41],FLDet[355],CenterFace[279],RDN[133]
Heatmap Outputthelikelihoodresponsemapsof CALE[19],RED[173],Yangetal.[100],JMFA[44],FAN[20],LAB[264],
regression eachlandmark. SAN[51],FALGCN[158],DU-Net[222],Guoetal.[70],PCD-CNN[113],
RCN(L+ELT)[81],HR-Net[240],Zhangetal.[311],SA[147],FHR[219],
Awingloss[249],DeCaFA[38],HSLE[357],FAB[207],KDN[29],Donget
al.[52],LaplaceKL[182],LUVLi[114],PropagationNet[91]
3D model Inferthe3Dfaceshapefrom2Dimage,and LPFA[101],3DDFA[351],FacePoseNet[25],PIFASCNN[102],DeFA[141],
fitting thenprojectittotheimageplanetoobtain RDR[272],Bhagavatulaetal.[14],Zhangetal.[309],PR-Net[60],PAFA[121]
2Dlandmarks.
Landmark-freeFaceAlignment Directlyoutputalignedfaceswithoutthe Hayatetal.[76],E2e[340],ReST[263],GridFace[343],Weietal.[256],RDC-
explicituseoflandmark. Face[332]
4 FACEALIGNMENT
Giventhedetectedface,facealignmentaimstocalibrateunconstrainedfacestothecanonicallayout
forfacilitatingthedownstreamtasksofrecognitionandanalysis.Inthissection,wereviewthe
mainstreamroutinesforfacealignment,includinglandmark-basedfacealignment,andlandmark-
freefacealignment.Fig.6showsthedevelopmentofrepresentativemethodsforfacealignment.
4.1 Landmark-basedFaceAlignment
Landmark-basedfacealignmentutilizesthespatialtransformationtocalibratefacestothepre-
definedcanonicallayoutbyinvolvingthefaciallandmarksasthereference.Therefore,thefacial
landmarklocalizationisthecoretaskoflandmark-basedalignment.Wesorttheexistinglandmark-
based alignment methods into three subcategories, i.e., coordinate regression based methods,
heatmapregressionbasedmethodsand3Dmodelfittingbasedmethods.
4.1.1 Coordinateregression. Thecoordinateregressionbasedmethodsregardthelandmarkco-
ordinatesasthenumericalobjectiveoftheregressionvianeuralnetworks.Inotherwords,they
focusonlearningthenonlinearmappingfromthefaceimagetothelandmarkcoordinatevectors.
Followingthecoarse-to-finemanner,mostmethodsemploycascadedregression[149,212,312,
344]orrecurrentneuralnetwork(RNN)[226,273]toprogressivelyrefinethepredictionoflandmark
coordinate.Besides,themulti-tasklearningisalsoacommonroutinetofacilitatelandmarklocaliza-
tionwiththerelatedfacialtasks,suchasfacedetection[41,176,279,314,355]andfacialattribute
recognition[277,331].Moreover,manyregressionmethodsemploytheL1,L2,orsmoothedL1
lossfunctions,whichareeffectivebut,nonetheless,sensitivetooutliers.Tohandlethisproblem,
Wingloss[61]amplifiestheimpactofthesampleswithsmallormediumrangeerrors.Theabove
methodsstudythefaciallandmarklocalizationonstillimages.Forvideofacelandmarklocalization,12
howtoleveragethetemporalinformationacrossframesbecomesnecessary.TSTN[132]developsa
two-streamarchitecture,whichlocatesthelandmarkfromasingleframeandcapturesthetemporal
consistencyforrefinement.Besides,SBR[53]proposestoexploittheopticalflowcoherencyof
detectedlandmarkswhentrainingwithvideodata.
4.1.2 Heatmapregression. Incontrasttothecoordinateregression,theheatmapregressionbased
methodsoutputlikelihoodresponsemapsofeachlandmark.Theearlyexploration[19]studieshow
toaggregatethescoremapsandrefinethepredictionwithDCNNs.Lateron,Newelletal.[168]
designstackedhourglass(HG)networktogenerateheatmapforhumanposeestimation,which
hasachievedgreatsuccess.Asthefaciallandmarklocalizationtaskissimilartothehumanpose
estimation,manyworks[20,44,91,100,249,311]adoptthestackedHGnetworkforfaciallandmark
localizationandgreatlyimprovethestate-of-the-artperformance.
Thedensepixel-wiseclassificationbythefullyconvolutionalnetwork(FCN)isaneffectiveway
for the heatmap regression task. The HG structure can be regarded as an instance of the fully
convolutionalnetwork.BeyondtheHGstructure,anumberofeffectivenetworkarchitectures[38,
51,113,158,240]arenewlydesignedforheatmapregression.Amongthem,DeCaFA[38]utilizes
stackedU-netstopreservethespatialresolution,andlandmark-wiseattentionmapstoextractlocal
informationaroundthecurrentestimation.High-resolutionnetwork(HR-Net)[240]isdesignedto
maintainthehigh-resolutionrepresentationandshowsitsadvantageforlandmark-kindtasks.
Theabove-mentionedwingloss,whichisdesignedforthecoordinateregression,however,does
notguaranteetheconvergencefortheheatmapregression,duetotheimbalancepixelnumberof
foregroundandbackground.Toaddressthisissue,Wangetal.[249]proposeadaptivewinglossto
penalizemoreonforegroundpixelsthanonbackgroundpixels;similarly,PropNet[91]presentsa
focalwinglosswhichadjuststhelossweightofsamplesineachmini-batch.
Somefaciallandmarkshaveambiguousdefinition,suchasthoseoncheek,leadingtoinconsistent
annotationsbydifferentannotators.Besides,thelandmarksinoccludedfacialregionsalsocause
imprecise annotations. Many methods [29, 114, 147, 147, 264, 357] devote to these two issues.
Facial boundary heatmap [264] is a good choice to provide the facial geometric structure for
reducingthesemanticambiguities.Regardingthesemanticambiguitiesasnoisyannotation,Liuet
al.[147]provideanotherpathtoestimatethereallandmarklocationwithaprobabilisticmodel.
Morerecently,KDN[29]andLUVLi[114]proposetoestimatetheuncertaintyofpredictions.The
uncertaintycanbeusedtoidentifytheimagesinwhichthefacealignmentfails.
4.1.3 3Dmodelfitting. Consideringtheexplicitrelationshipbetween2Dfaciallandmarksand3D
faceshape,the3Dmodelfittingbasedmethodsreconstructthe3Dfaceshapefrom2Dimage,and
thenprojectitontotheimageplanetoobtainthe2Dlandmarks.Comparedwiththeregular2D
methodswhichestimateasetoflandmarks,3Dmodelfittingbasedmethodsareabletofitfaces
with3Dmodelofthousandsofvertexesandalignthemwithlargeposes.
Sincethecascadedregressionisaneffectivemannertoestimatemodelparameters,somemeth-
ods [101, 141, 351] combine the cascaded CNN regressor with a dense 3D Morphable Model
(3DMM)[15]toestimatethe3Dfaceshape.Despitemanyadvantages,thecascadedCNNsoften
sufferfromthelackofend-to-endtraining.Asaroundabout,Jourablooetal.[102]attempttofita
3DfacemodelthroughasingleCNN,whichconsistsofseveralblockstoadjustthe3Dshapeand
projectionmatrixaccordingtothefeaturesandpredictionsfromthepreviousblocks.
Althoughtheabovemethodstakegreatadvantagesfrom3DMM,thediversefacialshapewould
leadtoinaccurate2Dlandmarklocation,especiallywhenthe3Dshapecoefficientsaresparse.To
tacklethisproblem,RDR[272]proposestofit3Dfacesbyadynamicexpressionmodelanduse
arecurrent3D-2Dduallearningmodeltoalternativelyrefine3Dfacemodeland2Dlandmarks.
Beyondregressingtheparametersof3Dfaceshape,Faster-TRFA[14]andFacePoseNet[25]estimate13
Table5. Statisticsofpopularfaciallandmarkdatasets.“-”referstononeofficialprotocolforsplittingthe
trainingandtestset.
Datasets Year #Total #Training #Test #Point Description
Multi-PIE[67] 2008 755,370 - - 68 Thelargestfacialdatasetincontrolledcondition.
LFPW[12] 2010 2,845 - - 35 Imagestakenfromuncontrolledsetting.
ALFW[111] 2011 24,386 20,000 4,386 21 Alarge-scalefaciallandmarkdataset.
AFW[353] 2012 473 - - 6 Multiplefacialannotations.
HELEN[117] 2012 2,330 2,000 330 194 Providingdenselandmarkannotations.
COFW[21] 2013 1,852 1,345 507 29 Containingoccludedfaces.
300-W[185] 2013 3,837 3,148 689 68 Themostfrequentlyuseddatasetoffaciallandmark.
300-VW[192] 2015 114 50 64 68 Avideofaciallandmarkdataset.
Menpo[298] 2017 28,273 12,014 16,259 68 Containingbothsemi-frontalandprofilefaces.
WFLW[264] 2018 10,000 7,500 2,500 98 Multipleannotationsandlargevariations.
JD-landmark[144] 2019 15,393 13,393 2,000 106 Coveringlargefacialvariations.
thewarpingparametersofrenderingadifferentviewofageneral3Dfacemodel.Besides,some
methods[60,309]aimtodirectlyregressthelandmarksfromthe3Dcoordinatesoffaceshape.
4.2 Landmark-freeFaceAlignment
Landmark-freefacealignmentmethodsintegratethealignmenttransformationprocessinginto
DCNNsandoutputalignedfacewithoutrelyingonfaciallandmarks.Thissetofmethodsgenerally
employthespatialtransformernetwork(Spatial-TN)[95]forgeometricwarping,wherethetrans-
formationparametersarelearnedviaend-to-endtraining.BasedonSpatial-TN,Hayatetal.[76]
andZhongetal.[340]proposetooptimizethefacealignmentwithasubsequentmoduleofface
representationjointly.Sincethefacialvariationsarequitecomplexwithvariousfactors,some
methods[263,343]aredesignedtoimprovethedeformationabilityofSpatial-TN.Besides,theradial
distortionoffaceimagesisanothercommonproblem,whichisbroughtbythewide-anglecameras.
RDCFace[332]presentsacascadednetworkwhichlearnstherectificationagainsttheradiallens
distortion,thefacealignmenttransformation,andthefacerepresentationinanend-to-endmanner.
4.3 EvaluationMetricsandDatasets
Weintroducethecommonlyusedevaluationmetricsanddatasetsforfacealignment.Aspresented
inthefollowingpartofthissubsection,mostlandmark-basedmethodsemploythequantitative
metrics,suchasnormalizedmeanerror.Besides,landmark-freemethodsemploytheevaluation
orientedtofacerecognition,andwewilldescribetheirmetricsinthefacerepresentationsection.
4.3.1 Metrics. The widely used evaluation metric is to measure the point-to-point Euclidean
distancebynormalizedmeanerror(NME),whichcanbedefinedas
𝑁𝑀𝐸 = 1 ∑︁𝑀 ∥𝑝 𝑘 −𝑔 𝑘∥ 2, (2)
𝑀 𝑑
𝑘=1
where𝑀 isthenumberoflandmarks,𝑝 𝑘 and𝑔 𝑘 representthepredictionandground-truthcoor-
dinatesofthefacelandmarks,𝑘 denotestheindexoflandmarks,and𝑑 referstothenormalized
distancewhichisusedtoalleviatetheabnormalmeasurementcausedbydifferentfacescalesand
largepose.TherearefourtypesofnormalizeddistanceforcomputingNME,i.e.,thegeometric
meanofthewidthandheightofthefaceboundingbox,thedistancebetweentheoutercornersof
eyes,thedistancebetweenthepupils,andthediagonalofthefaceboundingbox.
Thecumulativeerrorsdistribution(CED)curveisalsousedasanevaluationcriterion.CEDisa
distributionfunctionofNME.TheverticalaxisofCEDrepresentstheproportionoftestimages
thathaveanerrorvaluelessthanorequaltotheerrorvalueonthehorizontalaxis.Theareaunder14
Table6. Performanceoffaciallandmarklocalizationmethodsonthe300W,WFLW-All,ALFW-Full,and
COFWdatasets.TheevaluationmetricisNME(%).For300Wtestset,twotypesofNMEnormalization
(i.e.,inter-pupilnormalizationandinter-ocularnormalization)areused.ForWFLW-Alldataset,theinter-
ocularnormalizationisapplied.ForALFW-Fulldataset,thediagonaloffaceboundingboxisadoptedas
thenormalizationfactor.ForCOFWdataset,theinter-pupilnormalizationisapplied.“-”indicatesthatthe
authorsdonotreporttheperformancewiththecorrespondingprotocol.
300-W(inter-pupilnormalization) 300-W(inter-ocularnormalization)
Method Publication Subcategory WFLW ALFW COFW
Com.subset Chall.subset Fullset Com.subset Chall.subset Fullset
CFAN[312] ECCV’14 Coordinateregression 5.50 16.78 7.69 - - - - 10.94 8.38
TCDCN[331] ECCV’14 Coordinateregression 4.80 8.60 5.54 - - - - 7.60 8.05
MDM[226] CVPR’16 Coordinateregression 4.83 5.88 10.14 - - - - - 6.26
RAR[273] ECCV’16 Coordinateregression 4.12 8.35 4.94 - - - - 7.23 6.03
TSR[149] CVPR’17 Coordinateregression 4.36 7.56 4.99 - - - - 2.17 -
SIR[271] AAAI’18 Coordinateregression 4.29 8.14 5.04 - - - - - -
DSRN[159] CVPR’18 Coordinateregression 4.12 9.68 5.21 - - - - 1.86 -
Wingloss[61] CVPR’18 Coordinateregression 3.01 6.01 3.60 - - - 5.11 1.47 5.44
CPM+SBR[53] CVPR’18 Coordinateregression - - - 3.28 7.58 4.10 - 2.14 -
ODN[350] CVPR’19 Coordinateregression - - - 3.56 6.67 4.17 - 1.63 5.30
RDN[133] TPAMI’20 Coordinateregression 3.31 7.04 4.23 - - - - 2.06 5.82
3DDFA[351] CVPR’16 3Dmodelfitting 6.15 10.59 7.01 - - - - 5.60 -
PIFASCNN[102] ICCV’17 3Dmodelfitting 5.43 9.88 6.30 - - - - 4.45 -
DeFA[141] ICCVW’17 3Dmodelfitting 5.37 9.38 6.10 - - - - - -
RDR[272] ICCV’17 3Dmodelfitting 5.03 8.95 5.80 - - - - 4.41 -
PAFA[121] BMVC’19 3Dmodelfitting 3.42 5.73 3.87 - - - - 1.51 3.55
RCN+[81] CVPR’18 Heatmapregression 4.20 7.78 4.90 - - - - 2.17 -
PCD-CNN[113] CVPR’18 Heatmapregression - - - 3.67 7.62 4.44 - 2.40 5.77
SAN[51] CVPR’18 Heatmapregression - - - 3.34 6.60 3.98 - 1.91 -
HR-Net[240] CVPR’18 Heatmapregression - - - 2.87 5.15 3.32 - 1.57 -
LAB[264] CVPR’18 Heatmapregression 3.42 6.98 4.12 2.98 5.19 3.49 5.27 1.25 3.92
DU-Net[222] ECCV’18 Heatmapregression - - - 2.90 5.15 3.35 - - -
SA[147] CVPR’19 Heatmapregression 3.45 6.38 4.02 - - - - 1.60 -
HG-HSLE[357] ICCV’19 Heatmapregression 3.94 7.24 4.59 2.85 5.03 3.28 - - -
Awingloss[249] ICCV’19 Heatmapregression 3.77 6.52 4.31 2.72 4.52 3.07 4.36 - 4.94
LaplaceKL[182] ICCV’19 Heatmapregression 3.28 7.01 4.01 - - - - 1.97 -
DeCaFA[38] ICCV’19 Heatmapregression - - - 2.93 5.26 3.39 4.62 - -
LUVLi[114] CVPR’20 Heatmapregression - - - 2.76 5.16 3.23 4.37 1.39 -
PropNet[91] CVPR’20 Heatmapregression 3.70 5.75 4.10 2.67 3.99 2.93 4.05 - 3.71
thecurve(AUC)alsoprovidesareferenceofhowthealgorithmperformsatagivenerror:
∫ 𝛼
𝐴𝑈𝐶 𝛼 = 𝑓(𝑒)𝑑𝑒, (3)
0
where 𝛼 is the given error corresponding to the upper bound of integration calculation, 𝑒 is
theprogressivenormalizederrorsand𝑓(𝑒)referstotheCEDcurve.LargerAUCindicatesbetter
performance.BasedonCEDcurve,failureratecanbeusedtomeasuretherobustnessofaalgorithm,
whichdenotesthepercentageofsamplesinthetestsetwhoseNMEislargerthanathreshold.
4.3.2 Datasets. Thefaciallandmarkdatasetscanbesortedbytheconstrainedconditionandin-the-
wildcondition.ThestatisticsofthesedatasetsaregiveninTable5.CMUMultiPose,Illumination,
andExpression(Multi-PIE)[67]isthelargestfacialdatasetinconstrainedcondition,whichprovides
337subjectswith15predefinedposes,19illuminationconditionsand6facialexpressions.The
annotatedfaciallandmarksare68pointsforfrontalfacesand39pointsforprofileones.
Inaddition,morein-the-wilddatasets[12,21,111,117,144,185,192,264,298,353]areproposed
forfaciallandmarklocalization.Amongthem,300-W[185]isthemostfrequentlyuseddataset,
whichfollowstheannotationconfigurationofMulti-PIEandre-annotatestheimagesinLFPW,
AFW,HELEN,andiBug[184].Besides,Menpo[298]isalarge-scalefaciallandmarkdatasetwith
moredifficultcasesforfaciallandmarklocalization.JD-landmark[144]annotatesfaceimageswith
106faciallandmarks,providingmorestructuralinformationoffacialcomponents.300-VW[192]
provides50videoclipsfortrainingand64fortestoflandmarklocalizationinvideo.15
Fig.7. Appropriatefacealignmentpolicyisbeneficialtofacerecognitioninmanysituations[278].The
indicatedchoicesofalignmentpolicyaredifferentinnumberofusedfaciallandmarks,croppingsizeofface
image,andverticalshift.Amongthem,ArcFace[41]employsa5-pointalignmenttemplate,andMFR[22]
utilizesa25-pointone.TigthROI[278]involvesfewexternalfacialfeature(e.g.,jaw-line,ears,partofhair),
whichlacksusefulfacialfeatures.SuperRoI[278]useslargecroppingsize,whichpotentiallycoversirrelevant
background.FAPS[278]isdesignedtosearchanoptimalfacealignmenttemplate.Thelatterthreepolicies
use68landmarkswhichprovideadequateinformationforcomputingtheaffinetransformationmatrix.
4.4 PerformanceComparison
Table6showsthecomparisonofstate-of-the-artfaciallandmarklocalizationmethodsonvarious
testdatasets,including300-W,WLFW-ALL,ALFW-Full,andCOFW.Amongcoordinateregression
basedmethods,Wingloss[61]isasimplebuteffectiveapproachwhichhasbeenwidelyused.More
recently,theheatmapregressionbasedmethodsattractmoreattention,sincetheycanobtainthe
leadingperformancebymaintainingfacialstructureinformationthroughoutthemodels.
4.5 EffectontheFaceRepresentation
Facealignmentistheintermediateprocedure.Thestudyofhowfacealignmentinfluencesonface
representationisvitalfortuningtherecognitionsystemtoattainitsmaximumeffect.Forlandmark-
basedfacealignment,asetofinaccuratefaciallandmarkswillharmthealignmentandthenimpede
thefollowingfeaturecomputationaswell.Specifically,humanfacesappearintheimageswith
similarlayout,andsuchlayoutcanberegardedasatemplateinspatialcoordinates.Infact,the
alignmentisaccomplishedmostlybywarpingthefacetothepredefinedcoordinatesaccordingto
thepredictedlandmarks.Then,thefacerepresentationmodellearnsidentityfeaturefromfacial
imageswithsuchlayout.Oncethepredictedlandmarksareinaccurate,thefacialimagewilldrift
away from thepredefined coordinates, which isunexpected layout forthe face representation
model.Guoetal.[70]andDengetal.[41]bothcomparethewidelyusedMTCNN[314]andtheir
methods, and find that poor landmark localization will bring shift variation, while robust face
alignmentcanboostrecognitionaccuracy,especiallyforthecross-posefacerecognition.Besides,
asdiscussedincertainstudies[172,189,278],theconfigurationoffacealignmentprocess(so-called
facealignmentpolicy),includingthenumberofusedfaciallandmarks,thecroppingsizeofface
image,andtheverticalshift,greatlyinfluencesontheperformanceoffacerecognition.Asshown
inFig.7,theresultsfrom[278]indicatethattheproperfacealignmentpolicyisbeneficialtoface
recognitioninmanysituations.Moreover,amoderatedegreeofspatialtransformationisrequired
inthealignmentprocessing[256].Bothlimitedandexcessivetransformationwillbringdisturbance.
5 FACEREPRESENTATION
Subsequenttofacealignment,thefacerepresentationstageaimstomapthealignedfaceimagesto
afeaturespace,wherethefeaturesofthesameidentityarecloseandthoseofdifferentidentities
are far apart. In practical applications, there are two major tasks of face recognition, i.e., face
verificationandfaceidentification.Thefaceverificationreferstopredictwhetherapairofface16
Table7. Thecategorizationoffacerepresentationlearning.
Category Description Method
Network Ar- General Thebasicanduniversaldesignsforcom- AlexNet[112],VGGNet[198],GoogleNet[217],ResNet[77],Xception[34],DenseNet[64]
chitectures monvisualrecognitiontasks. AttentionNet[236],SENet[84],SqueezeNet[94],MobileNet[83],ShuffleNet[328],Mo-
bileNetV2[186],Shufflenetv2[151]
Specialized Themodifiedorensembledesignsoriented HybridDL[213],DeepIDseries[209,211,215],MM-DFR[48],B-CNN[35],Comparator-
tofacerecognition. Net[274],ContrastiveCNN[74],PRN[106],AFRN[105],FANFace[282],SparseNet[216],
Light-CNN[266],MobileFaceNet[30],Mobiface[56],ShuffleFaceNet[154],Hayatetal.[76],
E2e[340],ReST[263],GridFace[343],RDCFace[332],Weietal.[256],Co-Mining[250],
GroupFace[109],MFR[22]
Training Classification Consideringthefacerepresentationlearn- DeepFace[220],DeepID[214],MM-DFR[48],L-softmax[138],NormFace[237],L2-
Supervision ingasaclassificationtask. softmax[175],COCOloss[142],SphereFace[137],Ringloss[339],AM-softmax[235],Cos-
Face[239],ArcFace[42],AdaptiveFace[134],Fairloss[130],MV-softmax[251],ArcNeg[295],
AdaCos[326],P2SGrad[327],NTP[86],Co-Mining[250],PFE[194],CurricularFace[92],
Shietal.[196],GroupFace[109],MFR[22],RCMloss[270],DUL[26]
Feature embed- Optimizingthefeaturedistanceaccording DeepID2[209],FaceNet[189],VGGFace[172],Liftedstructured[203],N-pairloss[200],
ding tothelabelofsamplepair. Multibatch[218],TPE[187],Smartmining[152],ContrastiveCNN[74]
Hybrid Applyingclassificationandfeatureembed- DeepID2[209],DeepID2+[215],DeepID3[211],TUA[135],Centerloss[259],Marginal
dingtogetherasthesupervisorysignals. loss[45],Rangeloss[325],DM[199],PRN[106],UniformFace[55],RegularFace[336],
UT[342],LBL[352],AFRN[105],Circleloss[210]
Semi-supervised Exploitinglabeledandunlabeledfacesfor CDP[302],GCN-DS[285],GCN-VE[284],UIR[293],RoyChowdhuryetal.[183]
representationlearning.
SpecificTasks Cross-age Identifyingfacesacrossawiderangeof LF-CNNs[258],CAN[276],AFRN[54],DAL[238],AE-CNN[228],OE-CNN[252],IPC-
ages. GANs[254],LMA[5],DualcGANs[204],AIM[333]
Cross-pose Identifyingfacesacrossawiderangeof TP-GAN[90],PIM[334],DREAM[23],DA-GAN[335],DR-GAN[225],UV-GAN[43],CAPG-
poses. GAN[87],PAMs[155],MPRs[1],MvDN[104]
Racialbias Addressingtheimbalanceracedistribution IMAN[246],RL-RBN[245]
oftrainingdatasets.
Cross-modality Performingfacerecognitiononapairofim- Realeetal.[180],HFR-CNNs[188],TRIVET[140],IDR[79],DVR[267],MC-CNN[47],
agescapturedbydifferentsensingmodali- WCNN[80],NAD[118],ADHFR[205],CFC[78],Mittaletal.[163],ForensicFR[62],
ties. TDFL[233],E2EPG[315],CASPG[306],DualGAN[290],PS2-MAN[243],DTFS[317],
Cascaded-FS[316],PTFS[310]
Low-shot Trainingandtestwiththedatathathasa SSPP-DAN[82],Guoetal.[72],Choeetal.[33],HybridClassifiers[269],Chengetal.[31],
smallnumberofsamplesperidentity. DM[199],Yinetal.[292],
Video-based Performingfacerecognitionwithvideose- TBE-CNN[50],NAN[283],C-FAN[66],FANVFR[146],MARN[65],Raoetal.[178],CFR-
quences. CNN[171],ADRL[179],DAC[139]
imagesbelongtothesameidentity.Thefaceidentificationcanberegardedasanextensionofface
verification,whichaimstodeterminethespecificidentityofaface(i.e.,probe)amongasetof
identities(i.e.,gallery);moreover,inthecaseofopen-setfaceidentification,apriortaskisneeded,
whosetargetispredictingwhetherthefacebelongstooneofthegalleryidentitiesornot.
Forboththefaceverificationandfaceidentification,facerepresentationisusedtomeasurethe
similaritybetweenfaceimages.Therefore,howtolearndiscriminativefacerepresentationisthe
coretarget.WiththeadvancedfeaturelearningabilityofDCNNs,facerepresentationhasmade
greatprogress.Inthefollowings,weprovideasystematicreviewofthelearningmethodsofface
representationfromtwomajoraspects,i.e.,networkarchitectureandtrainingsupervision.
5.1 NetworkArchitectures
Therecentimprovementoffacerepresentationpartlybenefitsfromtheadvanceofdeeparchitecture
design. We first review the literature of network architecture for face representation learning.
Accordingtothedesigningpurpose,wedividethemintogeneralarchitecturesandspecialized
architectures.Thegeneralarchitecturesarethebasicanduniversaldesignsforcommonvisual
recognition tasks in the first place, and applied to face representation learning afterward. The
specializedarchitecturesincludethemodifiedorensembledesignsorientedtofacerecognition.
5.1.1 Generalarchitectures. WiththeadvancedfeaturelearningabilityofDCNNs[34,64,77,84,
112,198,217,236],facerepresentationhasmadegreatprogress.Amongthem,AlexNet[112]obtains
thefirstplaceinImageNet[40]competition(ILSVRC)2012andachievessignificantimprovement
comparedwiththetraditionalmethods.Then,VGGNet[198]presentsamoregenericnetwork,
whichreplacesthelargeconvolutionalkernelsbythestacked3×3ones,enablingthenetwork
togrowindepth.Inordertoenlargethenetworkwithouttheextraincreaseofcomputational
budget,GoogleNet[217]developsaninceptionarchitecturetoconcatenatethefeaturemapsthat17
Fig.8. Thedevelopmentoftrainingsupervisionforfacerepresentationlearning.Theorange,green,grayand
bluerepresentclassification,featureembedding,hybrid,andsemi-supervisedmethods,respectively.Onecan
refertoTable7forthedetailedreferences.
aregeneratedbytheconvolutionsofdifferentreceptivefield.Soon,GoogleNetisappliedtoface
representationlearning,namelyFaceNet[189].Morerecently,ResNet[77]proposesaresidual
structuretomakeitpossiblefortrainingdeepnetworksthathavehundredsoflayers.ResNetis
amodernnetworkthathasbeenwidelyusedonmanyvisualtasks,includingfacerecognition.
Additionally,severallightweightneuralnetworks[83,94,151,186,328]areproposedtoachieve
thetrade-offbetweenspeedandaccuracy.Allofthemhavebeenemployedasbackbonenetwork
forrepresentationlearninginthefacerecognitionliteratureafterbeingdesigned.
5.1.2 Specializedarchitectures. Theaforementionedarchitecturesareinitiallyproposedforgeneral
visualtasks.Besides,manyworksdevelopspecializedarchitecturesforfacerepresentationlearning.
Atfirst,manyworks[48,209,213,214]attempttoassemblemultipleconvolutionnetworkstogether
forlearningmultiplelocalfeaturesfromasetoffacialpatches.Giventhehumanfaceappearing
withregulararrangementoffacialparts(eyes,nose,mouth,etc),suchcombinationofmultiple
networkswithrespecttofacialpartcanbemorereliablethanasinglenetwork.Besides,Xieet
al.[274]designanend-to-endarchitecture,namelyComparatorNetwork,tomeasurethesimilarity
oftwosetsofavariablenumberoffaceimages.Certainapproaches[105,106]developfeature-pair
relational network to capture the relations between a pair of local appearance patches. More
recently,FANFace[282]integratesthefacerepresentationnetworkandfaciallandmarklocalization
network,sothattheheatmapoflandmarkswillboostthefeaturesforrecognition.
Inaddition, manystudies [30, 56, 154, 216, 265, 266] focus ondevelopingthe lightweightar-
chitecture.Toreducetheparametersofdeepnetworks,SparseNet[216]proposestoiteratively
learnsparsestructuresfromthepreviouslylearneddensemodels.Light-CNN[266]introducesa
max-feature-map(MFM)activationfunctiontogainbettergeneralizationabilitythanReLUforface
recognition;basedonMFM,alightweightarchitectureisdevelopedthatachievestheadvantagesin
termsofruntimeefficiencyandmodelsize.MobileFaceNet[30]replacestheglobalaveragepooling
layerintheoriginalMobileNet[186]withaglobaldepth-wiseconvolutionlayersotheoutput
featurecanbeimprovedbythespatialimportanceinthelastlayer.
Itisworthnotingthat,insomelandmark-freefacealignmentmethods[76,256,263,332,340,343]
whichhavebeenpresentedinSection4.2,thenetworkcanbeoptimizedwithrespecttotheobjective
offacerepresentationlearningandfacealignmentjointly.
5.2 TrainingSupervision
Besides network architectures, the training supervision also plays a key role for learning face
representation.Theobjectiveofsupervisionforfacerepresentationlearningistoencouragethe
facesofsameidentitytobecloseandthoseofdifferentidentitiestobefarapartinthefeaturespace.
Following the convention of representation learning, we categorize the existing methods of
training supervision for face representation into supervised scheme, semi-supervised scheme,18
andunsupervisedscheme.Althoughtherearecertaindeepunsupervisedlearningmethods[71,
129,195,255]forfaceclustering,inthisreview,wefocusonthesupervisedandsemi-supervised
ones which comprise the major literature of state-of-the-art face recognition. Fig.8 shows the
developmentoftrainingsupervisionforfacerepresentationlearning.Inthesupervisedscheme,we
canfurthercategorizetheexistingworksintothreesubsets,i.e.,classification,featureembedding
andhybridmethods.Theclassificationmethodsaccomplishfacerepresentationlearningwitha𝑁-
wayclassificationobjective,regardingeachofthe𝑁 classesasanidentity.Thefeatureembedding
methodsaimtooptimizethefeaturedistancebetweensampleswithrespecttotheidentitylabel,
which means maximizing the inter-person distance and minimizing the intra-person distance.
Besides,severalworksemploybothclassificationandfeatureembeddingroutinetojointlytrain
thenetwork,namelyhybridmethods.Asforthesemi-supervisedscheme,severalworksexploit
thelabeledandunlabeledfacesforrepresentationlearning.
5.2.1 Classificationscheme. Theclassificationbaseddeepfacerepresentationlearningisderived
fromthegeneralobjectclassificationtask.Eachclasscorrespondstoanidentitythatcontainsa
numberoffacesofthesameperson.Thesoftmaxlossfunctionisthemostwidelyusedsupervision
forclassificationtask,whichconsistsofafully-connected(FC)layer,thesoftmaxfunctionand
thecross-entropyloss.Forfacerepresentationlearning,DeepFace[220]andDeepID[214]arethe
pioneersofutilizingsoftmaxtopredicttheprobabilityoveralargenumberofidentitiesoftraining
data.Theirtraininglossfunctioncanbeformulatedasfollows:
1
∑︁𝑁 𝑒𝑊 𝑦𝑇 𝑖𝑥𝑖+𝑏𝑦𝑖
L =− log , (4)
𝑁
𝑖=1
(cid:205)𝑐 𝑗=1𝑒𝑊 𝑗𝑇𝑥𝑖+𝑏𝑗
where𝑁 isthebatchsize,𝑐 isthenumberofclasses(identities),𝑦 𝑖 istheground-truthlabelof
sample𝑥 𝑖,𝑊 𝑦𝑖 istheground-truthweightvectorofsample𝑥 𝑖 intheFClayer,and𝑏 𝑗 isthebiasterm.
Theterminsidethelogarithmisthepredictedprobabilityontheground-truthclass.Thetraining
objectiveistomaximizethisprobability.Basedonthesoftmaxlossfunction,NormFace[237]and
COCOloss[142]studythenecessityofthenormalizationoperationandapply𝐿 normalization
2
constraintonbothfeaturesandweightswithomittingthebiasterm𝑏 𝑗.Toeffectivelytrainwiththe
normalizedfeatures,ascalefactorisadoptedtore-scalethecosinesimilaritybetweenthefeatures
andtheweights.Specifically,thenormalizedsoftmaxlossfunctioncanbereformulatedas
1
∑︁𝑁 𝑒𝑠cos(𝜃𝑦𝑖)
L =− log , (5)
𝑁
𝑖=1
𝑒𝑠cos(𝜃𝑦𝑖) +(cid:205)𝑐 𝑗=1,𝑗≠𝑦𝑖𝑒𝑠cos𝜃𝑗
wherecos(𝜃 𝑗) derivesfromtheinnerproduct𝑊 𝑗𝑇𝑥 𝑖 withthe𝐿 2 normalizationonweights𝑊 𝑗 =
∥𝑊𝑊 𝑗𝑗 ∥
2
andfeatures𝑥 𝑖 = ∥𝑥𝑥 𝑖𝑖 ∥ 2,and𝑠 isthescaleparameter.
Tofurtherimprovetheintra-classcompactnessandinter-classseparateness,L-softmax[138]
replacestheground-truthlogitcos(cid:0)𝜃 𝑦𝑖(cid:1) with (−1)𝑘 cos(𝑚𝜃 𝑦𝑖)−2𝑘,𝜃 𝑦𝑖 ∈ (cid:104) 𝑘 𝑚𝜋, (𝑘+ 𝑚1)𝜋(cid:105) ,where𝑚
istheangularmarginthatbeingapositiveinteger,and𝑘 isalsoanintegerthat𝑘 ∈ [0,𝑚−1].
SimilartoL-softmax,SphereFace[137]appliesanangularmarginintheground-truthlogitcos(cid:0)𝜃 𝑦𝑖(cid:1)
to make the learned face representation to be more discriminative on a hypersphere manifold.
However,themultiplicativeangularmarginincos(cid:0)𝑚𝜃 𝑦𝑖(cid:1)
leadstopotentiallyunstableconvergence
during the training. To overcome the problem, AM-softmax [235] and CosFace [239] present
an additive margin penalty to the logit, cos(cid:0)𝜃 𝑦𝑖(cid:1) +𝑚 1, which brings more stable convergence.
Subsequently,ArcFace[42]introducesanadditiveangularmargininsidethecosine,cos(cid:0)𝜃 𝑦𝑖 +𝑚 2(cid:1) ,
which corresponds to the geodesic distance margin penalty on a hypersphere manifold. The19
followingisaunifiedformulationofAM-softmax,CosFace,andArcFace:
1
∑︁𝑁 𝑒𝑠(cos(𝜃𝑦𝑖+𝑚 2)+𝑚 1)
L =− log , (6)
𝑁
𝑖=1
𝑒𝑠(cos(𝜃𝑦𝑖+𝑚 2)+𝑚 1) +(cid:205)𝑐 𝑗=1,𝑗≠𝑦𝑖𝑒𝑠cos𝜃𝑗
where𝑚 < 0representstheadditivecosinemarginofAM-softmaxandCosFace,and𝑚 > 0
1 2
denotestotheadditiveangularmarginofArcFace.Theyareeasytobeimplementedandcanachieve
betterperformancethantheoriginalsoftmaxloss.Goingfurtherwiththemarginbasedsupervision,
AdaptiveFace[134]andFairloss[130]proposetheadaptivemarginthatbeingclass-wiseinthe
trainingdata.Thepurposeistoaddresstheimbalancedistributionprobleminthetrainingdataset.
Resortingtotheadvantageofhardsampleminingstrategy[128,197],someapproaches[92,
251,295]reformulatethenegative(non-ground-truth)logitinsoftmaxlossfunction.Forexample,
MV-softmax[251]proposestore-weightthenegativelogittoemphasizethesupervisiononthe
mis-classifiedsamples,andthustoimprovetherepresentationlearningfromthenegativeview.
Inaddition,certainstudies[326,327]deeplyanalyzetheformulationofmargin-basedsoftmax
lossfunctionfromtheperspectiveofclassificationprobability,andproposehyperparameter-free
approachesforfacerepresentationleanring.
Morerecently,manymethods[22,26,86,109,148,194,196,250,270,342]gofurtherwiththe
classificationsupervisionforfacerepresentationlearning.Someofthem[86,250,342]focuson
thenoise-robustfacerepresentationlearning,andsomeoftheothers[148,270]tackletheissueof
performancedegradationoflow-bitquantifiedmodel.Wuetal.[270]regardthequantizationerror
asthecombinationofclasserrorandindividualerror,andproposearotation-consistentmargin
losstoreducethelattererrorwhichismorecritical.Besides,PFE[194]andDUL[26]proposeto
takeintoaccountthedatauncertaintyformodelingdeepfacerepresentation,preventingfromthe
uncertaintyissuecausedbylowqualityfaceimages.
5.2.2 Featureembeddingscheme. Featureembeddingschemeaimstooptimizethefeaturedistance
accordingtothelabelofsamplepair.Ifthepairbelongtothesameidentity,i.e.,positivepair,the
objectiveistominimizethedistanceortomaximizethesimilarity;otherwise,i.e.,negativepair,to
maximizethedistanceortominimizethesimilarity.Forinstance,contrastiveloss[209,211,215,289]
directoptimizesthepair-wisedistancewithamarginthattoencouragepositivepairstobeclose
togetherandnegativepairstobefarapart.Thelossfunctiontobeminimizediswrittenas
(cid:40)
1∥𝑓(𝑥 𝑖)−𝑓(𝑥 𝑗)∥2 if𝑦
𝑖
=𝑦 𝑗,
L𝑐 = 2 2 (7)
1 2max(0,𝑚 d−∥𝑓(𝑥 𝑖)−𝑓(𝑥 𝑗)∥ 2)2 if𝑦 𝑖 ≠𝑦 𝑗,
where𝑦
𝑖
=𝑦
𝑗
denotes𝑥 𝑖and𝑥
𝑗
arepositivepair,𝑦
𝑖
≠𝑦
𝑗
denotesnegativepair,𝑓(·)istheembedding
function,and𝑚 isthenon-negativedistancemargin.Thecontrastivelossdrivesthesupervision
d
onallthepositivepairsandthosenegativepairswhosedistanceissmallerthanthemargin.
FaceNet[189]firstappliesthetripletloss[190,257]todeepfacerepresentationlearning.Different
fromcontrastiveloss,thetripletlossencouragesthepositivepairstohavesmallerdistancethan
thenegativepairswithrespecttoamargin,
𝑁
L𝑡 =∑︁(cid:104)(cid:13) (cid:13)𝑓 (cid:0)𝑥 𝑖𝑎(cid:1) −𝑓 (cid:0)𝑥 𝑖𝑝(cid:1)(cid:13) (cid:13)2 2−(cid:13) (cid:13)𝑓 (cid:0)𝑥 𝑖𝑎(cid:1) −𝑓 (cid:0)𝑥 𝑖𝑛(cid:1)(cid:13) (cid:13)2 2+𝑚 d(cid:105) +, (8)
𝑖
where𝑚 isthedistancemargin,𝑥𝑎 denotestheanchorsample,𝑥𝑝 and𝑥𝑛 refertothepositive
d 𝑖 𝑖 𝑖
sampleandnegativesample,respectively.Thecontrastivelossandtripletlosstakeintoaccount
onlyonenegativesampleeachtime,whilenegativepairsareabundantintrainingdataanddeserve20
thoroughinvolvementintrainingsupervision.Therefore,N-pairloss[200]generalizesthetriplet
losstotheformwithmultiplenegativepairs,andgainedfurtherimprovementonfacerecognition.
Comparedwiththesupervisionofclassification,featureembeddingcansavetheparameters
oftheFClayerinsoftmax,especiallywhenthetrainingdatasetisinlargescale.Butthebatch
sizeoftrainingsampleslimitstheeffectivenessoffeatureembedding.Toalleviatethisproblem,
someapproaches[152,199,203]proposethehardsampleminingstrategytoexploittheeffective
informationineachbatch,whichiscrucialtopromotetheperformanceoffeatureembedding.
5.2.3 Hybridmethods. Thehybridmethodsrefertothosewhichapplyclassificationandfeature
embeddingtogetherasthesupervisorysignals.DeepIDseries[209,211,211]utilizesoftmaxloss
andcontrastivelossjointlyforlearningfacerepresentation.Later,severalmethods[45,55,259,336]
improvethefeatureembeddingportionwithinthehybridscheme,byutilizingeithertheintra-class
ortheinter-classconstraints.Somemethods[325,342,352]showtheadvantageforhandlingthe
long-taildistributeddatawhichisawidely-existingprobleminFR.Generally,theclassification
scheme works well on the head data but poorly on the tail data. Compared with classification
scheme,thefeatureembeddingschemeisabletoprovidethecomplementarysupervisiononthe
taildata.Thus,thecombinationofclassificationandfeatureembeddingcanimprovethetraining
onlong-taildistributeddata.Morerecently,Sunetal.[210]proposeacirclelossfromaunified
perspectiveoftheclassificationandembeddinglearning,whichintegratesthetripletlosswiththe
cross-entropylosstosimultaneouslylearndeepfeatureswithpair-wiselabelsandclass-wiselabels.
5.2.4 Semi-supervisedscheme. Theaforementionedmethodsfocusonsupervisedlearning.Con-
structing labeled dataset requires much of annotation effort, while large amount of unlabeled
dataiseasilyavailable.Therefore,itisanattractivedirectionthattoexploitthelabeledandunla-
beleddatatogetherfortrainingdeepmodels.Forsemi-supervisedfacerepresentationlearning,
assuming the identities of unlabeled data being disjoint with the labeled data, several existing
works[284,285,302]focusongeneratingthepseudolabelsforunlabeleddata.However,these
methodsassumenon-overlappingidentitiesbetweenunlabeledandlabeleddata,whichisgenerally
impracticalinreal-worldscenarios.Consequently,theunlabeledsamplesofoverlappingidentity
willbeincorrectlyclusteredasanewclassbythepseudo-labelingmethods.Theintra-classlabel
noiseinpseudo-labeleddataisanotherproblem.Toaddresstheseissues,RoyChowdhuryetal.[183]
separatesunlabeleddataintosamplesofdisjointandoverlappingclassesviaanout-of-distribution
detectionalgorithm.Besides,theydesignanimprovedtraininglossbasedonuncertaintytoalleviate
thelabelnoiseofpseudo-labeleddata.
5.3 SpecificFaceRecognitionTasks
5.3.1 Cross-domainfacerecognition. Here,thetermofcross-domainreferstoageneralizeddefi-
nitionthatincludesvariousfactors,suchlikecross-ageandcross-poseFR.Asdeeplearningisa
data-driventechnique,thedeepnetworkusuallyworkswellonthetrainingdomainsbutpoorly
ontheunseenones.Inreal-worldapplicationsoffacerecognition,itisessentialtoimprovethe
generalizationabilityoffacerepresentationacrossvariousdomainfactors.Inthefollowing,we
discusscertainaspectsofcross-domainFR;also,thecurrentsolutionsarepresented.
Cross-age:Asthefacialappearancehaslargeintra-classvariationalongwiththegrowingage,
identifyingfacesacrosswiderangeofageisachallengingtask.Forsuchcross-ageFR,thereare
two directions. The first direction [54, 228, 238, 252, 258, 276] aims to learn age-invariant face
representationbydecomposingdeepfacefeaturesintoage-relatedandidentity-relatedcomponents.
Theseconddirectionisbasedongenerativemechanism.Inthisway,severalmethods[6,108,248]
attempttosynthesizefacesoftargetage,buttheypresentimperfectpreservationoftheoriginal21
identitiesinagedfaces.Thus,supplementarymethods[5,204,254,333]aredesignedtoimprove
theidentity-preservingabilityduringthefaceaging.
Cross-pose:Inunconstrainedconditions,suchassurveillancevideo,thecamerascannotalways
capturethefrontalfaceimageforeveryappearedsubject.Thus,thecapturedfaceshavelarge
posevariationfromfrontaltoprofileview.However,generatingthefrontalfaceswillincrease
the burden of face recognition system. Cao et al. [23] alleviate this issue by transforming the
representationofaprofilefacetothefrontalviewinthefeaturespace.Anotherproblemisthatthe
numberofprofilefacesaremuchfewerthanfrontalonesinthetrainingdata.Thus,somegenerative
approaches[43,87,225,335]proposetosynthesizeidentity-preservingfacesofarbitraryposesto
enrichthetrainingdata.Moreover,certainmethods[1,104,155]developmultiplepose-specific
deepmodelstocomputethemulti-viewfacerepresentations.
Racialbias:Duetotheimbalancedistributionofdifferentracesintrainingdata,thedeepface
featureshowsfavorablerecognitionperformancetotheracesoflargeproportionintrainingdata
thanthoseofsmallproportion.Recently,Wangetal.[246]constructanin-the-wildfacedataset
(RFW)withbothidentityandraceannotation,whichconsistsoffourracialsubsets,i.e.,Caucasian,
Asian,Indian,andAfrican.Besides,theyproposeadomainadaptationmethodtoalleviatetheracial
bias.Lateron,RL-RBN[245]setsafixedmarginforthelarge-proportionracesandautomatically
selectanoptimalmarginforthesmall-proportionraces,inordertoachievebalancedperformance.
Cross-modality:Cross-modalityfacerecognitiongenerallyreferstotheheterogeneousface
recognition, which performs with a pair of input face images captured by different sensing
modalities, such as infrared vs. visible, or sketch vs. photo. How to alleviate the domain gaps
betweendifferentmodalitiesisthemajorchallenge.Besides,theavailableinfraredorsketchimages
are of very limited number. The existing works mainly handle these two issues. Many meth-
ods[62,140,163,180,188,233]exploitthetransferlearning,i.e.,pretrainingonthevisible-light
(VIS)imagesandfinetuningwiththeinfraredorsketchdata,toreducethedomaindiscrepancy.An-
othersetofmethods[47,79,80,267]decomposethecross-modalityfeaturestothemodality-specific
andmodality-invariantcomponents,andusethelatteronefortherecognitiontask.Moreover,
recentmethods[78,118,205,243,290,310,316,317,349]aimtosynthesizethecommonVISimage
frominfraredorsketchinput,andthenperformtheregularFRintheVISdomain.
5.3.2 Low-shotfacerecognition. Low-shotlearninginfacerecognitionfocusesontheconditionof
identificationoflow-shotfaceIDs,eachofwhichhasasmallnumberofsamples.MS-Celeb-1Mlow-
shotlearningbenchmark[72]ismostused,whichhasabout50to100trainingsamplesperIDinthe
basesetandonlyonetrainingsampleperIDinthenovelset.ThetargetistorecognizetheIDsinboth
baseandnovelsets.Thekeychallengeistocorrectlyrecognizethesubjectsinthenovelsetwhich
hasonlyonetrainingsampleperID.Totacklethisproblem,manymethods[31,72,199,269,292]
improvethelow-shotfacerecognitionwithbettertrainingsupervisionorstrategy.Besides,face
generation[33,82,206]isanothereffectiveroutineforlow-shotissue.
5.3.3 Videofacerecognition. Theabovemethodsfocusonstillimage-basedfacerecognition.For
video face recognition, a common way [28, 50] is to equally consider the importance of each
frameandsimplyaverageasetofdeepfeaturesasthetemplate.However,thisroutinedoesnot
considerthedifferentqualityofframesandthetemporalinformationacrossframes.Howtoobtain
an optimal template feature in video is the major challenge of video face recognition. Several
methods[65,66,146,283]aggregatetheframe-levelfeatureswiththeattentionweightsorquality
scores.Synthesizingrepresentativeorhigh-qualityfaceimagefromavideosequenceisanother
possibility [171, 178]. Additionally, certain methods [139, 157, 179] model the temporal-spatial
informationwiththeattentionmechanismandfindthefocusofvideoframes.22
5.4 EvaluationMetricsandDatasets
5.4.1 Metrics. Theperformanceoffacerecognitionisusuallyevaluatedontwotasks:verification
andidentification,eachofwhichhasitscorrespondingevaluationmetrics.Specifically,twosetsof
samples,i.e.,galleryandprobe,arerequiredfortheevaluation.Thegalleryreferstoasetoffaces
registeredinthefacerecognitionsystemwithknownidentities,whiletheprobedenotesasetof
facesneedtoberecognizedinverificationoridentification.Beforediscussingthecommonlyused
evaluationmetrics,wefirstintroducesomebasicconcepts.Afacerecognitionsystemdetermines
whethertoacceptthematchingofaprobefaceandagalleryfacebycomparingtheirsimilarity,
computedbysomemeasurementbetweentheirfeatures,withagiventhreshold.Specifically,when
aprobefaceandagalleryfacearethesameidentity,atrueacceptance(TA)meanstheirsimilarity
isabovethethreshold,andafalserejection(FR)representstheirsimilarityisbelowthethreshold;if
theyaredifferentidentities,atruerejection(TR)meanstheirsimilarityisbelowthethreshold,and
afalseacceptance(FA)meanstheirsimilarityisabovethethreshold.Thesearethebasicconcepts
tobuildtheevaluationmetricsinthefollowings.Onecanreferto[68,69]formoredetails.
Verificationtask:Faceverificationisoftenappliedinidentityauthenticationsystem,which
measuresthesimilarityoffacepairs.Onepresentshisorherfaceandclaimstheenrolledidentity
inthegallery.Then,thesystemdetermineswhetheritacceptsthepersonbeingthesameoneof
theclaimedidentitybycalculatingthesimilaritybetweenthepresentedfaceandtheclaimedface.
Thus,theverificationtaskcanberegardedasaone-to-onefacematchingprocess.Thefalseaccept
rate(FAR)andtrueacceptrate(TAR)areusedtoevaluatetheverificationperformance.FARis
thefractionofimpostorpairswiththesimilarityabovethethreshold,whichcanbecalculated
𝐹𝐴
by ;TARrepresentsthefractionofgenuinepairswiththesimilarityabovethethreshold,
𝐹𝐴+𝑇𝑅
𝑇𝐴
whichcanbecalculatedby .Then,byvaryingthethreshold,theROCcurvecanbedrawn
𝑇𝐴+𝐹𝑅
bymanyoperatingpoints,eachofwhichisdeterminedbyapairofTARvs.FAR.TheROCcurve
(withTARvalueatselectedFAR)anditsAUC(i.e.,areaundercurve)arewidelyusedtoevaluate
theperformanceforthefaceverificationtask.
Identification task: Face identification task determines whether a probe face belongs to a
enrolledidentityinthegalleryset.Tothisend,theprobefaceneedstobecomparedwithevery
personinthegalleryset.Thus,theidentificationtaskcanbealsoreferredasone-to-𝑁 facematching.
Generally,faceidentificationincludestwotasks,i.e.,theopen-setandclosed-setidentification.
Theopen-setidentificationtaskreferstothattheprobefaceisnotnecessarilytheveryidentity
containedinthegalleryset,whichisthemostgeneralcaseinpractice.Thetruepositiveidenti-
ficationrate(TPIR)andfalsepositiveidentificationrate(FPIR)arethemostusedmetricsforthe
followingtwosituations.Thefirstsituationreferstothattheprobecorrespondstoanenrolled
identityinthegalleryset.Thissituationiscalledmatesearching,andtheprobeiscalledmateprobe.
Thesucceededmatesearchingrepresentsthattherankoftruematchingishigherthanthetarget
rank,andmeanwhileitssimilarityisabovethethreshold.Insuchcase,themateprobeiscorrectly
identifiedasitstrueidentity,andthematesearchingismeasuredbytheTPIRwhichrepresentsthe
proportionofsucceededtrialsofmatesearching.Thesecondisnon-matesearching,inwhichthe
probedoesnotcorrespondtoanyenrolledidentity(i.e.,non-mateprobe).Thenon-matesearching
ismeasuredbytheFPIRwhichreportstheproportionofnon-mateprobeswronglyidentifiedas
enrolledidentity.Byfixingtherankandvaryingthethreshold,theROCcurvecanbedrawnby
manyoperatingpoints,eachofwhichisdeterminedbyapairofTPIRvs.FPIR.TheROCcurve
(TPIRvalueatagivenFPIR)isusedtoevaluateperformanceintheopen-setfaceidentificationtask.
In the closed-set scenario, the identity of each probe face is included in the gallery set. The
cumulativematchcharacteristic(CMC)curveisusedforevaluatingtheclosed-setfaceidentification.
TheCMCcurveisdrawnbytheoperatingpointsthataredeterminedbyapairofidentification23
Table8. Thecommonlyusedpublicdatasetsfortrainingandtestingdeepfacerecognition.
Dataset Year #Subject #Image/Video #ofImg/VidperSubj Description
Training
CASIA-WebFace[289] 2014 10,575 494,414/- 47 Thefirstpubliclarge-scalefacedataset
VGGFace[172] 2015 2,622 2.6M/- 1,000 Containinglargenumberofimagesineachsubject
CelebA[297] 2015 10,177 202,599/- 20 Richannotationsofattributesandidentities
UMDFaces[11] 2015 8,277 367K/- 45 Abundantvariationoffacialpose
MS-Celeb-1M[73] 2016 100K 10M/- 100 Alarge-scalepublicdatasetofcelebrityfaces
MegaFace[107,167] 2016 672,057 4.7M/- 7 Along-taildatasetofnon-celebrity
VGGFace2[24] 2017 9,131 3.31M/- 363 Ahigh-qualitydatasetwithawiderangeofvariation
UMDFaces-Videos[10] 2017 3,107 -/22,075 7 AvideotrainingdatasetcollectedfromYouTube
MS-Celeb-1MLow-shot[72] 2017 20K,1K 1M,1K/- 58,1 Low-shotfacerecognition
IMDb-Face[234] 2018 57K 1.7M/- 29 Alarge-scalenoise-controlleddataset
QMUL-SurvFace[234] 2018 5,319 220,890/- 41 Alow-resolutionsurveillancedataset
Glint360K[4] 2021 360K 17M/- 47 Alarge-scaleandcleaneddataset
WebFace260M[354] 2021 4M 260M/- 65 Thelargestpublicdatasetofcelebrityfaces
Test
LFW[88] 2007 5,749 13,233/- 2.3 Aclassicbenchmarkinunconstrainedconditions
YouTubeFaces(YTF)[262] 2011 1,595 -/3,425 2.1 Facerecognitioninunconstrainedvideos
CUFSF[324] 2011 1,194 2,388/- 2 Photo-sketchfacerecognition
CASIANIR-VISv2.0[122] 2013 725 17,580/- 24.2 Near-infraredvs.RGBfacerecognition
IJB-A[110] 2015 500 5,712/2,085 11.4/4.2 Set-basedfacerecognitionwithlargevariation
CFP[191] 2016 500 7,000/- 14 Frontaltoprofilecross-posefaceverification
MS-Celeb-1MLow-shot[72] 2016 20K,1K 100K,20K/- 5,20 Low-shotfacerecognition
MegaFace[107,167] 2016 690,572 1M/- 1.4 Alarge-scalebenchmarkwithonemillionfaces
IJB-B[260] 2017 1,845 11,754/7,011 6.37/3.8 Set-basedfacerecognitionwithfullposevariation
CALFW[338] 2017 4,025 12,174/- 3 Cross-agefaceverification
AgeDB[164] 2017 570 16,516/- 29 Cross-agefaceverification
SLLFW[46] 2017 5,749 13,233/- 2.3 ImprovingthedifficultyofnegativepairsinLFW
CPLFW[337] 2017 3,968 11,652/- 2.9 Cross-possfaceverification
TrillionPairs[39] 2018 1M 1.58M/- 1.6 Alarge-scalebenchmarkwithmassivedistractors
IJB-C[156] 2018 3,531 31,334/11,779 6/3 Set-basedfacerecognitionwithlargevariation
IJB-S[103] 2018 202 5,656/552 28/12 Real-worldsurveillancevideos
RFW[246] 2018 11,429 40,607/- 3.6 Forreducingracialbiasinfacerecognition
DFW[115] 2018 600 7,771/- 13 Disguisedfacerecognition
QMUL-SurvFace[234] 2018 10,254 242,617/- 23.7 Low-resolutionsurveillancevideos
ratevs.rank.Theidentificationratereferstothefractionofprobefacesthatarecorrectlyidentified
asthetrueidentities,thustheCMCcurvereportsthefractionofthetruematchingwithagiven
rank,andtheidentificationrateatrankoneisthemostcommonlyusedindicatorofperformance.
ItisnoteworthythattheCMCisaspecialcaseoftheTPIRwhenwerelaxthethreshold.
5.4.2 Datasets. Withthedevelopmentofdeepfacerecognition,anotherkeyroletopromoteface
representationlearningisthegrowingdatasetsfortrainingandtest.Inthepastfewyears,theface
datasetshavebecomelargescaleanddiverse,andthetestingscenehasbeenapproachingtothe
real-worldunconstrainedcondition.ThestatisticsofthemarepresentedinTable8.
Trainingdata:Large-scaletrainingdatasetsareessentialforlearningdeepfacerepresentation.
Theearlyworksoftenemploytheprivatefacedatasets,suchasDeepface[220],FaceNet[189],
DeepID[209].Tomakeitpossibleforfaircomparison,Yietal.[289]releasetheCASIA-WebFace
dataset,whichhasbeenoneofthemostwidely-usedtrainingdatasets.Afterward,morepublic
training datasets are published to provide abundant face images for training deep face model.
Amongthem,VGGFace[172]andVGGFace2[24]containmanytrainingsamplesforeachsubject.
Incontrast,MS-Celeb-1M[73],MegaFace[107],IMDb-Face[234]andWebFace260M[354]provide
alargenumberofsubjectswithrelativelylesstrainingsamplespersubject.
Testdata:Asfortesting,LabeledFacesintheWild(LFW)[88]isclassicandthemostwidely
used benchmark for face recognition in unconstrained environments. The original protocol of
LFWcontains3,000genuineand3,000impostorfacepairs,andevaluatesthemeanaccuracyof24
Table9. Performance(%)comparisonoffacerecognitiononvarioustestdatasets.“TrainingData”denotes
thenumberoftrainingfaceimagesusedbythemethods.FortheevaluationonMegaFace,“Id.”refersto
therank-1faceidentificationaccuracywith1Mdistractors,and“Veri.”referstothefaceverificationTAR
at10−6FAR.FortheevaluationonIJB-BandIJB-C,wereportthe1:1verificationTAR(@FAR=10−4).The
performancewith“∗”referstotheevaluationontherefinedversionofMegaFace[42].“-”indicatesthatthe
authorsdonotreporttheperformancewiththecorrespondingprotocol.
MegaFace
Method Publication Subcategory TrainingData Backbone LFW IJB-B IJB-C YTF CALFW CPLFW CFP-FP AgeDB30
Id. Veri.
DeepFace[220] CVPR’14 Classification 4M CNN-8 97.35 - - - - 91.4 - - - -
DeepID[220] CVPR’14 Classification 0.3M CNN-8 97.45 - - - - - - - - -
L-Softmax[138] ICML’16 Classification 0.5M VGGNet-18 99.10 67.12 80.42 - - - - - - -
NormFace[237] ACMMM’17 Classification 0.5M ResNet-28 99.16 - - - - - - - -
SphereFace[137] CVPR’17 Classification 0.5M ResNet-64 99.42 72.72 85.56 - - 95.0 - - - -
ReST[137] CVPR’17 Classification 0.5M CNN-9 99.03 65.16 - - - 95.4 - - - -
E2e[340] SPL’17 Classification 0.7M ResNet-27 99.33 - - - - 95.0 - - - -
AM-softmax[235] SPL’18 Classification 0.5M ResNet-20 98.98 72.47 84.44 - - - - - - -
CosFace[239] CVPR’18 Classification 5M ResNet-64 99.73 82.72 96.65 - - 97.6 - - - -
ComparatorNet[274] ECCV’18 Classification 3.3M ResNet-50 - - - 84.1 88.0 - - - - -
ArcFace[42] CVPR’19 Classification 0.5M ResNet-50 99.53 77.50 92.34 - - - - - 95.56 95.15
Fairloss[130] ICCV’19 Classification 0.5M ResNet-50 99.57 77.45 92.87 - - 96.2 - - - -
PFE[194] ICCV’19 Classification 4.4M ResNet-64 99.82 78.95 92.51 - 93.25 - - - 93.34 -
FANFace[282] AAAI’20 Classification 0.5M ResNet-50 99.56 78.32 92.83 - - 96.72 - - - -
TURL[196] CVPR’20 Classification 4.8M ResNet-100 99.78 78.60 95.04 - 96.6 - - - 98.64 -
RDCFace[332] CVPR’20 Classification 1.7M ResNet-50 99.80 - - - - 97.10 - - 96.62 -
AdaCos[326] CVPR’19 Classification 2.35M ResNet-50 99.73 97.41∗ - - 92.4 - - - - -
P2SGrad[327] CVPR’19 Classification 2.35M ResNet-50 99.82 97.25∗ - - 92.3 - - - - -
AdaptiveFace[134] CVPR’19 Classification 5M ResNet-50 99.62 95.02∗ 95.61∗ - - - - - - -
ArcFace[42] CVPR’19 Classification 5.8M ResNet-100 99.82 98.35∗ 98.48∗ 94.2 95.6 97.7 95.45 92.08 98.27 98.15
MV-AM-softmax[251] AAAI’20 Classification 3.2M Attention-56 99.79 98.00∗ 98.31∗ - - - 95.63 89.19 95.30 98.00
DUL[26] CVPR’20 Classification 3.6M ResNet-64 99.83 98.12∗ - - 94.21 96.84 - - 98.78 -
DB[22] CVPR’20 Classification 5.8M ResNet-50 99.78 96.35∗ 96.56∗ - - - 96.08 92.63 - 97.90
CurricularFace[92] CVPR’20 Classification 5.8M ResNet-100 99.80 98.71∗ 98.64∗ 94.8 96.1 - 96.20 93.13 98.37 98.32
GroupFace[109] CVPR’20 Classification 5.8M ResNet-100 99.85 98.74∗ 98.79∗ 94.93 96.26 97.8 96.20 93.17 98.63 98.28
FaceNet[189] CVPR’15 Embedding 400M GoogleNet-22 99.63 - - - - 95.1 - - - -
VGGFace[172] BMVC’15 Embedding 2.6M CNN-36 98.95 64.79 78.32 - - 97.3 - - - -
N-pairloss[200] NIPS’16 Embedding 0.5M CNN-10 98.50 - - - - - - - - -
GridFace[343] ECCV’18 Embedding 10M GoogLeNet-22 99.70 - - - - 95.6 - - - -
DeepID2[209] NeurIPS’14 Hybrid 0.3M CNN-8 99.15 65.21 78.86 - - - - - - -
SparseNet[216] CVPR’15 Hybrid 0.3M CNN-15 99.30 - - - - 92.7 - - - -
Centerloss[259] ECCV’16 Hybrid 0.7M CNN-11 99.28 65.49 80.14 - - 94.9 - - - -
Ringloss[339] CVPR’18 Hybrid 3.5M ResNet-64 99.50 74.93 - - - 93.7 - - - -
PRN[106] ECCV’18 Hybrid 2.8M ResNet-101 99.76 - - 84.5 - 96.3 - - - -
RegularFace[336] CVPR’19 Hybrid 3.1M ResNet-20 99.61 75.61 91.13 - - 96.7 - - - -
UniformFace[55] CVPR’19 Hybrid 3.8M ResNet-34 99.8 79.98 95.36 - - 97.7 - - - -
AFRN[105] ICCV’19 Hybrid 2.8M ResNet-101 99.85 - - 88.5 93.0 97.1 96.30 93.48 95.56 96.35
Circleloss[210] CVPR’20 Hybrid 3.6M ResNet-34 99.73 97.81∗ - - 93.44 96.38 - - 96.02 -
verificationonthese6,000pairs.Sofar,thestate-of-the-artaccuracyhasbeensaturatedonLFW,
whereas the total samples in LFW are more than those in the original protocol. Based on this,
BLUFR[126]exploitsallthefaceimagesinLFWforalarge-scaleunconstrainedfacerecognition
evaluation;SLLFW[46]replacesthenegativepairsofLFWwithmorechallengingones.Inaddition,
CFP[191],CPLFW[337],CALFW[338],AgeDB[164]andRFW[246]utilizethesimilarevaluation
metricofLFWtotestfacerecognitionwithvariouschallenges,suchascrosspose,crossageand
multipleraces.MegaFace[107,167]andTrillionPairs[39]focusontheperformanceatthestrict
false accept rates (i.e., 10−6 and 10−9) on face verification and identification with million-scale
distractors. The above datasets focus on image-to-image face recognition, whereas YTF [262],
IJBseries[103,110,156,260],andQMUL-SurvFace[234]serveastheevaluationbenchmarkof
video-basedfacerecognition.Especially,IJB-SandQMUL-SurvFaceareconstructedfromreal-world
surveillancevideos,whicharemuchmoredifficultandrealisticthanthetasksonstillimages.
5.5 PerformanceComparison
Table9showstheperformanceoffacerepresentationmethodsonvarioustestdatasets.Among
them,CosFace[239]andArcFace[42]arethetwocommonlyusedmethodsinmanyapplications
of facerecognition. In addition, with thegrowingdatasets for trainingand test, theclosed-set25
Table10. Summaryofthemajorchallengestowardsend-to-enddeepfacerecognition.
Challenges Description
Theissuesofeachelement. Facedetection •Trade-offbetweendetectionaccuracyandefficiency.
•Accuracyoftheboundingboxlocation.
•Detectingfaceswithawiderangeofscale.
Facealignment •Annotationambiguityandgranularity.
Facerepresentation •limitedtrainingdataandcomputationalbudget.
•Surveillancevideofacerecognition.
•Noisylabelandimbalancedata.
Thecommonissuesacrosstheelements. Facial/imagevariations •Largepose,extremeexpression,occlusion,facialscale.
•Motionblur,lowillumination,lowresolution.
Data/labeldistribution •Limitedlabeleddata,labelnoise.
•Usageofunlabeleddata.
•Imbalanceoverscale,identity,race,domain,modality.
Computationalefficiency •Inferenceonnon-GPUdeviceandedgecomputing.
•Fasttrainingandconvergence.
Theissuesconcerningtotheentiresystem. Interpretability •Explainablelearningandinference.
Jointmodelingandoptimization •End-to-endtrainingandinference.
•Unifiedlearningobjective.
•Mutualpromotion.
Universalpretraining •Universalpretrainedfacialrepresentation.
Trustworthiness •Robustness,fairness,explainability,security,andprivacy.
classificationtrainingonthelarge-scaledatasetsenablestoapproachtheopen-setfacerecognition
scenario.Thiscouldbethereasonwhytheclassificationbasedtrainingmethodshavebeenwidely
studiedanddominatedthestate-of-the-artperformanceinrecentyears.Onecanfindthepublication
trendofthreesupervisedtrainingschemeswiththeincreasingscaleofpublicfacedatasetsinthe
supplementarymaterial.
6 DISCUSSIONANDCONCLUSION
Deepfacerecognitionstillremainsanumberofissuesforeachelement.Inthefollowing,wefirst
analyzethemajorchallengestowardsend-to-enddeepfacerecognitionandthesubcategoriesof
eachelement.Then,weprovideadetaileddiscussionaboutthepromisingfuturetrendsforeach
elementandtheentiresystem.Finally,theconclusionofthissurveyispresented.
6.1 Challenge
ThetoprowsofTable10elaboratetheissuesofeachelement.Forfacedetection,thestate-of-the-art
methodsareeagerfortrade-offbetweendetectionaccuracyandefficiency.Forexample,inmany
applications,resizingtheinputimageisacommonpracticeofaccelerationfordetectors,while
itharmstherecalloftinyfacesaswell.Intheunconstrainedcondition,humanfaceswithlarge
variationtendtobemissedbydetectors,whereasthediverseimagebackgroundoftenleadsto
falsepositives.Besides,detectingfaceswithawiderangeofscaleisalsoagreatchallenge.Asfor
thefacealignmentprocedure,thefaciallandmarklocalizationmethodsarestillnotrobustenough
when workingwith extreme variations, suchas severe occlusion,large pose, lowillumination.
In addition, the annotation ambiguity, such as the landmarks on cheek, is a common problem
indatasets.Besides,mostoftheexistingfaciallandmarkdatasetsprovidetheannotationof68
or106points.Morelandmarkpointsenabletodepicttheabundantfacialstructure.Fortheface
representationlearning,althoughexistingmethodsachievehighaccuracyonvariousbenchmarks,
itisstillchallengingwhentrainingdataandcomputationalbudgetareverylimited.Inaddition,
surveillancefacerecognitionisacommonscenario,wherethechallengesincludevariousfacial26
variations,suchlargeposes,motionblur,occlusion,lowilluminationandresolution,etc.Imbalance
distributionoftrainingdataalsobringsissuestothefacerepresentationlearning,suchaslong-tail
distributionoverfaceidentitiesordomains.
The middle rows of Table 10 elaborate the common issues shared between face detection,
alignmentandrepresentation.Wecanfindthattheissuesmainlyincludethreeaspects,i.e.,facial
andimagevariations,dataandlabeldistribution,andcomputationalefficiency.Forexample,inthe
firstaspect,thefacialvariationsincludelargefacialpose,extremeexpression,occlusionandfacial
scale,whiletheimagevariationsincludetheobjectivefactorssuchasmotionblur,lowillumination
andresolutionwhichoccurfrequentlyinvideofacerecognition.Anotherexampleindicatesthe
need of training efficiency, including fast training and convergence, both of which devote to
acceleratingthelearningoflargefacerepresentationnetwork(hundredsoflayersnormally)from
weekstohours;theformergenerallyfocusesonthemixedprecisiontrainingorthedistributed
frameworkforlarge-scaletraining(overmillionsofidentities),whilethelatterfocusesonimproving
thesupervision,initialization,updatingmanner,activation,architectures,etc.Here,ratherthan
replaying every detail, we leave Table 10 to readers for exploring the common challenges and
furtherimprovement.Itisworthmentioningthatalltheelementswillbenefitfromthesolutions
againsttheseissues,sincetheyarethecommonissuesacrosstheelements.
ThebottomofTable10indicatesthemajorchallengesfromtheperspectiveofentiresystem.
Forinstance,ideally,thethreeelementsshouldbejointlymodeledandoptimizedwithrespectto
theend-to-endaccuracy.Ontheonehand,suchintegrationprovidesapossibilitytosearchglobal
optimalsolutionfortheholisticsystem;ontheotherhand,theindividualelementsofthesystemcan
benefitfromtheupstreamones.However,theelementshavedifferentlearningobjectivesregarding
totheirowntasks.Howtounifytheselearningobjectivesisachallengingandcriticalissuefor
the joint optimization. One can find a group of works [41, 76, 256, 263, 314, 332, 334, 340, 343]
attemptingtointegratefacedetectionandalignment,orfacealignmentandrepresentationfora
jointboost.Butfacedetectionisstilldifficulttobeintegratedwithfacerepresentationbecause
theyhavequitedifferentobjectivesandimplementationmechanisms.
Inaddition,wearegoingdeeperwithTable11aboutthemajorchallengestowardsthesubcat-
egoriesofeachelement.Forinstance,sincetheanchor-basedfacedetectorneedstopre-define
a large number of anchors, the settings of preset anchors need to be carefully tuned for each
particulardataset,whichlimitsthegeneralizationabilityoffacedetectors.Incontrast,anchor-free
face detector needs further exploration for better robustness to false positives and stability in
trainingprocess.
6.2 FutureTrend
Toaddresstheabovechallenges,anumberofworthwhileresearchdirectionsneedtobeexplored
inthefuture.
6.2.1 Facedetection.
• Generalizedanchorsettings.Theexistinganchor-basedmethodsdesigntheanchorsetting
from many aspects, such as assignment and matching strategy [120, 125, 145, 221, 322],
attributestuning[32,321,347],andsamplingstrategy[161].Thewell-tunedanchorsmay
limitthegeneralizationabilityoffacedetectors.Hence,itisworthtoexploreageneralized
anchorsettingthatcanbeusedfordifferentapplicationdemand.
• Anchor-freefacedetectionframework.Anchor-freedetectors[116,224,346]showflexi-
bledesignsandmorepotentialingeneralizationabilityforobjectdetection.However,asmall
numberofworks[89,279,294]haveexploredtheanchor-freemechanismanditsadvantages
forfacedetection.27
Table11. Summaryofthemajorchallengestowardsthesubcategoriesforeachelement.
Element Subcategory ChallengesDescription
Facedetection Multi-stage •Runtimeefficiency.
Single-stage •Detectingtinyfaces.
Anchor-based •Well-tunedanchors.
Anchor-free •Trainingstabilityandrobustnesstofalsepositives.
CPUreal-time •Trade-offbetweenaccuracyandefficiency.
Multi-tasklearning •Balanceofmulti-tasktrainingsupervision.
Problem-oriented •Low-illuminationandlow-resolution.
Facealignment Landmark-based—Coordinateregression •Predictionbiasduetopoorinitialization.
Landmark-based—Heatmapregression •Highcomputationalcost.
Landmark-based—3Dmodelfitting •Runtimeefficiency.
Landmark-free •Lossofidentitydiscriminativeinformation.
Facerepresentation Trainingsupervision—Classification •Trainingonimbalancedata.
Trainingsupervision—Featureembedding •Efficienttrainingonlarge-scaledatasets.
Trainingsupervision—Hybrid •Unifiedtrainingsupervisionofclassificationandfeatureembedding.
Trainingsupervision—Semi-supervised •Open-setidentitiessetting.
SpecificTasks—Cross-age •Recognizingidentitiesacrossawiderangeofage.
SpecificTasks—Cross-pose •Largeposevariation.
SpecificTasks—Racialbias •Biasreduction.
SpecificTasks—Cross-modality •Domaingeneralization.
SpecificTasks—Low-shot •One-shotlearning.
SpecificTasks—Video-based •Lowqualityofframes.
6.2.2 Facealignment.
• Highrobustnessandefficiency.Thereisalargeamountoffacialvariationsinreal-world
conditions,whichrequiresthealignmentmethodsbeingrobusttovariousinputfaceswhile
keepingefficiencyasanintermediatestepofthesystem.
• Denselandmarklocalization.Themostexistingdatasetsemploy68or106keypointsas
annotationconfiguration.Theyareenoughforfacealignment(usually5keypointsneeded),
butinsufficienttothecomplexfaceanalysistasks,suchasfacialmotioncapture.Besides,the
denselandmarkswillhelptolocatemoreaccuratealignment-neededkeypoints.
• Video-basedlandmarklocalization.Howtomakebetteruseofthetemporalinformation
isamajorchallengeforvideo-basedlandmarklocalization.Thistopicwillenabletoaddress
theproblemsinvideo,suchaslargeposes,motionblur,lowilluminationandresolution,etc.
• Semi-supervisedlandmarklocalization:Theextensiveresearchonlandmarklocalization
belongstotheregimeofsupervisedlearning,whichneedsthepreciseannotatedlandmarks.
However,itisexpensiveandinefficienttoobtainlarge-scaledatasetwiththepreciseannota-
tions.Asexploredbythepioneeringworks[52,53,81,182],thesemi-supervisedroutineisa
feasibleandvaluablesolutionforfaciallandmarklocalization.
6.2.3 Facerepresentation.
• Lightweightfacerecognition:Thelargememoryandcomputationalcostoftenmakesit
impracticaltoemployheavy-weightnetworksonmobileorembeddeddevices.Although
manyworks[30,56,154,265,266,270]havestudiedlightweightfacerecognition,thereis
stilllargeroomtoimprovethelightweightmodelswithhighefficiencyandaccuracy.28
• Robustnesstovariationsinvideo:Ithighlyrequiresrobustfacerepresentationmodels
againstvaryingconditionsinsurveillancevideo.Therobustnesstolowimagequalityand
largefacialposeisthecoredemandinmanypracticalapplications.
• Noisy label learning: Label noise is an inevitable problem when collecting large-scale
facedataset.Certainworks[39,42,234,329]studyhowtoremovethenoisydata,andsome
others[86,250,342]aimatlearningnoise-robustfacerepresentation.Butmostofthemare
susceptible to the ability of the initial model, and need to be more flexible in real-world
scenarios.Itisstillanopenissuefornoisylabellearninginfacerecognition.
• Crossdomainfacerecognition:Therearemanydifferentdomainfactorsinfacedata,such
asfacialage,pose,race,imagingmodality,andsomeworks[23,54,225,245,246,258,316]
havestudiedthefacerecognitionacrossasmallfractionofthem.Howtoobtainauniversal
representationforcrossdomainfacerecognitionisachallengingresearchtopic.
• Learning with imbalance data: Representation learning on the long-tail data is long-
standingtopicinmanydatasets.Withthescarcityofintra-classvariations,thesubjectswith
limitedtrainingsamplesareusuallyneglected.Thedomainbiascausedbyimbalancedata
scaleisanotherproblem.Itisworthtohandletheseproblemsinaunifiedframework.
• Learningwithunlabeledfaces:Therearealargeamountofunlabeledfacedatainpractical
applications. However, it is excessively expensive to manually annotate them when the
dataset keeps growing. Recently, semi-supervised learning and face clustering methods
attract increasing attention. How to effectively employ unlabeled data for boosting face
recognitionisapromisingdirection.
6.2.4 Towardstheentiresystem. Thereisverylittleworktosolvethemajorchallengesfromthe
perspectiveofentiresystem.Wepresentseveralpromisingdirectionsofthisareainthefollowing.
• Interpretabledeepmodels:Althoughtheexplainableartificialintelligence,so-calledXAI,
hasbeenstudiedforalongtime,theexplainabledeepfacerecognitionisinitsinfancy[261,
291,300,341].Therearetwowaystoaccesstheinterpretabilityfordeepfacerecognition,
i.e.,thetop-downandbottom-up,respectively.Thetop-downwayresortstothehumanprior
knowledgeforalgorithmexploration,sincehumanshowssuperiorabilityoffacerecognition
thandeepmodelsinmanytoughconditions.Thebottom-upwaydenotestheexploration
fromtheperspectiveoffacedataitself,suchasmodelingtheexplainabledeepfacerecognition
inspatialandscaledimension.
• Joint modeling for the holistic system: Despite the three elements having different
optimizedobjective,itisstillworthtoexploittheend-to-endtrainabledeepfacerecognition,
and study how they can be further improved through the jointly learning. Furthermore,
beyondthetopicofthissurvey,thereisalsoanopenquestionthathowshouldwedevelopa
singlenetworktoperformtheend-to-endfacerecognition.
• Universalfacerepresentationpretraining:Moststudiesoffacerecognitionfocusonthe
specifictasks,butoverlookinghowtolearnapre-traineduniversalfacerepresentationthat
canbeusedtofacilitatethedownstreamfacialanalysistasks.Thereisonlyonework[18]that
studiesthistopic.Thefindingsshowthatitispromisingtoobtainsignificantperformance
improvementforrelatedfacialtasksbyemployingunsupervisedpretraining.
• Trustworthy face recognition system: With the wide application, it is important to
evaluate and boost the trustworthiness of the recognition system [96]. The pursuit for
trustworthyfacerecognitionsystemisbecominganecessity,whichmainlyinvolvesseveral
aspects,i.e.,robustness,fairness,interpretability,security,andprivacy.Furtherresearchon
theseaspectsisessential.29
6.3 Conclusion
Inthissurvey,wereviewtherecentadvancesoftheelementsofend-to-enddeepfacerecognition,
whichconsistoffacedetection,facealignmentandfacerepresentation.Althoughtherearemany
surveysaboutfacerecognition,theymostlyfocusonfacerepresentationwithoutconsideringthe
intrinsicconnectionfromotherelementsinthepipeline;whereas,thissurveyisthefirstonewhich
providesacomprehensivereviewoftheelementsofend-to-enddeepfacerecognition.Wepresent
adetaileddiscussionandcomparisonofmanyapproachesineachelementfrompoly-aspects.Also,
wediscusstherelationshipbetweentheelementsandtheholisticframework.Accordingtothese
elaboratedcontents,wecannotonlyfindthesuitablemethodstoestablishstate-of-the-artface
recognitionsystem,butalsoknowwhichmethodisquitestrong-baselinestyleforcomparisonin
experiment.Additionally,weanalyzetheexistingchallengesandcollectcertainpromisingfuture
researchdirections.Wehopethissurveycouldbringhelpfulthoughtsforbetterunderstandingof
end-to-endfacerecognitionanddeeperexplorationinasystematicway.
REFERENCES
[1] W.Abd-Almageed,Y.Wu,S.Rawls,S.Harel,T.Hassner,I.Masi,J.Choi,J.T.Leksut,J.Kim,P.Natarajan,R.Nevatia,
andG.G.Medioni.2016.Facerecognitionusingdeepmulti-poserepresentations.InProceedingsoftheIEEEWinter
ConferenceonApplicationsofComputerVision.1–9.
[2] I.Adjabi,A.Ouahabi,A.Benzaoui,andA.Taleb-Ahmed.2020. Past,Present,andFutureofFaceRecognition:A
Review.Electronics9,8(2020).
[3] T.Ahonen,A.Hadid,andM.Pietikinen.2004. Facerecognitionwithlocalbinarypatterns.InProceedingsofthe
EuropeanConferenceonComputerVision.469–481.
[4] XiangAn,XuhanZhu,YanghuaXiao,LanWu,MingZhang,YuanGao,BinQin,DebingZhang,andYingnanFu.2021.
PartialFC:Training10MillionIdentitiesonaSingleMachine.ProceedingsoftheIEEE/CVFInternationalConference
onComputerVisionWorkshops(2021),1445–1449.
[5] G.Antipov,M.Baccouche,andJ.Dugelay.2017.Boostingcross-agefaceverificationviagenerativeagenormalization.
InProceedingsoftheIEEEInternationalJointConferenceonBiometrics.191–199.
[6] G.Antipov,M.Baccouche,andJ.Dugelay.2017. Faceagingwithconditionalgenerativeadversarialnetworks.In
ProceedingsoftheIEEEInternationalConferenceonImageProcessing.2089–2093.
[7] H.R.Arabnia.2009.ASurveyofFaceRecognitionTechniques.JournalofInformationProcessingSystems5,2(2009),
41–68.
[8] B.Yang,J.Yan,Z.Lei,andS.Z.Li.2014.Aggregatechannelfeaturesformulti-viewfacedetection.InProceedingsof
theIEEEInternationalJointConferenceonBiometrics.1–8.
[9] Y.Bai,Y.Zhang,M.Ding,andB.Ghanem.2018.Findingtinyfacesinthewildwithgenerativeadversarialnetwork.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.21–30.
[10] A.Bansal,C.D.Castillo,R.Ranjan,andR.Chellappa.2017.TheDo’sandDon’tsforCNN-BasedFaceVerification.In
ProceedingsoftheIEEEInternationalConferenceonComputerVisionWorkshops.2545–2554.
[11] A.Bansal,A.Nanduri,C.D.Castillo,R.Ranjan,andR.Chellappa.2017.UMDFaces:Anannotatedfacedatasetfor
trainingdeepnetworks.InProceedingsoftheIEEEInternationalJointConferenceonBiometrics.464–473.
[12] P.N.Belhumeur,D.W.Jacobs,D.J.Kriegman,andN.Kumar.2013.Localizingpartsoffacesusingaconsensusof
exemplars.IEEETrans.PatternAnal.Mach.Intell.35,12(2013),2930–2940.
[13] P.N.Belhumeur,P.H.Joo,andD.J.Kriegman.1997.Eigenfacesvs.Fisherfaces:RecognitionUsingClassSpecific
LinearProjection.IEEETrans.PatternAnal.Mach.Intell.19,7(1997),711–720.
[14] C.Bhagavatula,C.Zhu,K.Luu,andM.Savvides.2017. FasterthanReal-TimeFacialAlignment:A3DSpatial
TransformerNetworkApproachinUnconstrainedPoses.InProceedingsoftheIEEEInternationalConferenceon
ComputerVision.4000–4009.
[15] V.BlanzandT.Vetter.2003.Facerecognitionbasedonfittinga3dmorphablemodel.IEEETrans.PatternAnal.Mach.
Intell.25,9(2003),1063–1074.
[16] K.W.Bowyer,K.Chang,andP.Flynn.2006.Asurveyofapproachesandchallengesin3Dandmulti-modal3D+2D
facerecognition.Computervisionandimageunderstanding101,1(2006),1–15.
[17] S.C.Brubaker,J.Wu,J.Sun,M.D.Mullin,andJ.M.Rehg.2008.OntheDesignofCascadesofBoostedEnsemblesfor
FaceDetection.InternationalJournalofComputerVision77,1-3(2008),65–86.
[18] AdrianBulat,ShiyangCheng,JingYang,A.Garbett,EnriqueSanchez,andGeorgiosTzimiropoulos.2021.Pre-training
strategiesanddatasetsforfacialrepresentationlearning.ArXivabs/2103.16554(2021).30
[19] A.BulatandG.Tzimiropoulos.2016.Convolutionalaggregationoflocalevidenceforlargeposefacealignment.In
ProceedingsoftheBritishMachineVisionConference.86.1–86.12.
[20] A.BulatandG.Tzimiropoulos.2017. HowFarareWefromSolvingthe2D&3DFaceAlignmentProblem?(and
aDatasetof230,0003DFacialLandmarks).InProceedingsoftheIEEEInternationalConferenceonComputerVision.
1021–1030.
[21] X.P.Burgos-Artizzu,P.Perona,andP.Dollár.2013.Robustfacelandmarkestimationunderocclusion.InProceedings
oftheIEEEInternationalConferenceonComputerVision.1513–1520.
[22] D.Cao,X.Zhu,X.Huang,J.Guo,andZ.Lei.2020.DomainBalancing:FaceRecognitiononLong-TailedDomains.In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.5671–5679.
[23] K.Cao,Y.Rong,C.Li,X.Tang,andC.C.Loy.2018.Pose-RobustFaceRecognitionviaDeepResidualEquivariant
Mapping.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.5187–5196.
[24] Q.Cao,L.Shen,W.Xie,O.M.Parkhi,andA.Zisserman.2018.VGGFace2:ADatasetforRecognisingFacesacross
PoseandAge.InProceedingsoftheIEEEInternationalConferenceonAutomaticFace&GestureRecognition.67–74.
[25] F.Chang,A.T.Tran,T.Hassner,I.Masi,R.Nevatia,andG.Medioni.2017. FacePoseNet:MakingaCasefor
Landmark-FreeFaceAlignment.InProceedingsoftheIEEEInternationalConferenceonComputerVisionWorkshops.
1599–1608.
[26] J.Chang,Z.Lan,C.Cheng,andY.Wei.2020.DataUncertaintyLearninginFaceRecognition.InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition.5710–5719.
[27] D.Chen,G.Hua,F.Wen,andJ.Sun.2016.Supervisedtransformernetworkforefficientfacedetection.InProceedings
oftheEuropeanConferenceonComputerVision,Vol.9909.122–138.
[28] J.Chen,R.Ranjan,S.Sankaranarayanan,A.Kumar,C.Chen,V.Patel,C.D.Castillo,andR.Chellappa.2017. Un-
constrainedStill/Video-BasedFaceVerificationwithDeepConvolutionalNeuralNetworks.InternationalJournalof
ComputerVision126(2017),272–291.
[29] L.Chen,H.Su,andQ.Ji.2019.FaceAlignmentWithKernelDensityDeepNeuralNetwork.InProceedingsoftheIEEE
InternationalConferenceonComputerVision.6991–7001.
[30] S.Chen,Y.Liu,X.Gao,andZ.Han.2018.Mobilefacenets:Efficientcnnsforaccuratereal-timefaceverificationon
mobiledevices.InChineseConferenceonBiometricRecognition.428–438.
[31] Y.Cheng,J.Zhao,Z.Wang,Y.Xu,K.Jayashree,S.Shen,andJ.Feng.2017. Knowyouatoneglance:Acompact
vectorrepresentationforlow-shotlearning.InProceedingsoftheIEEEInternationalConferenceonComputerVision
Workshops.1924–1932.
[32] C.Chi,S.Zhang,J.Xing,Z.Lei,S.Z.Li,andX.Zou.2019.Selectiverefinementnetworkforhighperformanceface
detection.InProceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.33.8231–8238.
[33] J.Choe,S.Park,K.Kim,J.Park,D.Kim,andH.Shim.2017.FaceGenerationforLow-ShotLearningUsingGenerative
AdversarialNetworks.InProceedingsoftheIEEEInternationalConferenceonComputerVisionWorkshops.1940–1948.
[34] F.Chollet.2017.Xception:DeepLearningwithDepthwiseSeparableConvolutions.InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition.1800–1807.
[35] A.R.Chowdhury,T.Lin,S.Maji,andE.G.Learned-Miller.2016.One-to-manyfacerecognitionwithbilinearCNNs.
InProceedingsoftheIEEEWinterConferenceonApplicationsofComputerVision.1–9.
[36] T.F.CootesandC.J.Taylor.1992.Activeshapemodels—‘smartsnakes’.InProceedingsoftheBritishMachineVision
Conference.266–275.
[37] T.F.Cootes,K.Walker,andC.J.Taylor.2000. View-basedactiveappearancemodels.InProceedingsoftheIEEE
InternationalConferenceonAutomaticFace&GestureRecognition.227–232.
[38] A.Dapogny,M.Cord,andK.Bailly.2019.DeCaFA:DeepConvolutionalCascadeforFaceAlignmentintheWild.In
ProceedingsoftheIEEEInternationalConferenceonComputerVision.6892–6900.
[39] Deepglint.2020.TrillionPairs.http://trillionpairs.deepglint.com/overview. (AccessedSeptember15,2020).
[40] J.Deng,W.Dong,R.Socher,L.Li,K.Li,andL.Fei-Fei.2009.Imagenet:Alarge-scalehierarchicalimagedatabase.In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.248–255.
[41] JiankangDeng,JiaGuo,EvangelosVerveras,IreneKotsia,andStefanosZafeiriou.2020. RetinaFace:single-shot
multi-Levelfacelocalisationinthewild.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition.5203–5212.
[42] J.Deng,J.Guo,N.Xue,andS.Zafeiriou.2019.Arcface:Additiveangularmarginlossfordeepfacerecognition.In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.4690–4699.
[43] J.Deng,S.Cheng,N.Xue,Y.Zhou,andS.Zafeiriou.2018. UV-GAN:AdversarialFacialUVMapCompletionfor
Pose-InvariantFaceRecognition.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
7093–7102.
[44] J.Deng,G.Trigeorgis,Y.Zhou,andS.Zafeiriou.2019.JointMulti-ViewFaceAlignmentintheWild.Trans.Image
Process.28,7(2019),3636–3648.31
[45] JiankangDeng,YuxiangZhou,andStefanosZafeiriou.2017.MarginalLossforDeepFaceRecognition.InProceedings
oftheIEEEConferenceonComputerVisionandPatternRecognitionWorkshops.2006–2014.
[46] W.Deng,J.Hu,N.Zhang,B.Chen,andJ.Guo.2017.Fine-grainedfaceverification:FGLFWdatabase,baselines,and
human-DCMNpartnership.PatternRecognition66(2017),63–73.
[47] Z.Deng,X.Peng,Z.Li,andY.Qiao.2019.MutualComponentConvolutionalNeuralNetworksforHeterogeneous
FaceRecognition.Trans.ImageProcess.28(2019),3102–3114.
[48] C.DingandD.Tao.2015.RobustFaceRecognitionviaMultimodalDeepFaceRepresentation.IEEETrans.Multimedia
17(2015),2049–2058.
[49] C.DingandD.Tao.2016. AComprehensiveSurveyonPose-InvariantFaceRecognition. ACMTrans.Intell.Syst.
Technol.7(2016),37:1–37:42.
[50] C.DingandD.Tao.2018.Trunk-BranchEnsembleConvolutionalNeuralNetworksforVideo-BasedFaceRecognition.
IEEETrans.PatternAnal.Mach.Intell.40(2018),1002–1014.
[51] X.Dong,Y.Yan,W.Ouyang,andY.Yang.2018.Styleaggregatednetworkforfaciallandmarkdetection.InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.379–388.
[52] X.DongandY.Yang.2019. TeacherSupervisesStudentsHowtoLearnFromPartiallyLabeledImagesforFacial
LandmarkDetection.InProceedingsoftheIEEEInternationalConferenceonComputerVision.783–792.
[53] X.Dong,S.Yu,X.Weng,S.Wei,Y.Yang,andY.Sheikh.2018.Supervision-by-Registration:AnUnsupervisedApproach
toImprovethePrecisionofFacialLandmarkDetectors.InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition.360–368.
[54] L.Du,H.Hu,andY.Wu.2019.AgeFactorRemovalNetworkBasedonTransferLearningandAdversarialLearning
forCross-AgeFaceRecognition.IEEETransactionsonCircuitsandSystemsforVideoTechnology30,9(2019),2830–
2842.
[55] Y.Duan,J.Lu,andJ.Zhou.2019.UniformFace:LearningDeepEquidistributedRepresentationforFaceRecognition.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.3410–3419.
[56] C.N.Duong,K.G.Quach,N.Le,N.Nguyen,andK.Luu.2019.Mobiface:Alightweightdeeplearningfacerecognition
onmobiledevices.InProceedingsoftheIEEEInternationalConferenceonBiometricsTheory,ApplicationsandSystems.
1–6.
[57] M.Everingham,L.V.Gool,C.K.I.Williams,J.Winn,andA.Zisserman.2010.ThePascalVisualObjectClasses(VOC)
Challenge.InternationalJournalofComputerVision88,2(2010),p.303–338.
[58] M.EveringhamandJ.Winn.2011.Thepascalvisualobjectclasseschallenge2012(voc2012)developmentkit.Pattern
Analysis,StatisticalModellingandComputationalLearning,Tech.Rep8(2011).
[59] S.S.Farfade,M.J.Saberian,andL.J.Li.2015.Multi-viewfacedetectionusingdeepconvolutionalneuralnetworks.In
Proceedingsofthe5thACMonInternationalConferenceonMultimediaRetrieval.643–650.
[60] Y.Feng,F.Wu,X.Shao,Y.Wang,andX.Zhou.2018.Joint3dfacereconstructionanddensealignmentwithposition
mapregressionnetwork.InProceedingsoftheEuropeanConferenceonComputerVision.534–551.
[61] Z.Feng,J.Kittler,M.Awais,P.Huber,andX.Wu.2018.WingLossforRobustFacialLandmarkLocalisationwith
ConvolutionalNeuralNetworks.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
2235–2245.
[62] C.GaleaandR.A.Farrugia.2017.ForensicFacePhoto-SketchRecognitionUsingaDeepLearning-BasedArchitecture.
IEEESingalprocessingletters24(2017),1586–1590.
[63] S.Ge,J.Li,Q.Ye,andZ.Luo.2017.Detectingmaskedfacesinthewildwithlle-cnns.InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition.2682–2690.
[64] G.Huang,Z.Liu,andK.Q.Weinberger.2017. DenselyConnectedConvolutionalNetworks.InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition.2261–2269.
[65] S.Gong,Y.Shi,andA.K.Jain.2019. LowQualityVideoFaceRecognition:Multi-ModeAggregationRecurrent
Network(MARN).InProceedingsoftheIEEEInternationalConferenceonComputerVisionWorkshops.1027–1035.
[66] S.Gong,Y.Shi,N.D.Kalka,andA.K.Jain.2019. VideoFaceRecognition:Component-wiseFeatureAggregation
Network(C-FAN).InProceedingsoftheInternationalConferenceonBiometrics.1–8.
[67] R.Gross,I.A.Matthews,J.F.Cohn,T.Kanade,andS.Baker.2008.Multi-PIE.InProceedingsoftheIEEEInternational
ConferenceonAutomaticFace&GestureRecognition.1–8.
[68] P.Grother,R.J.Micheals,andP.J.Phillips.2003. Facerecognitionvendortest2002performancemetrics.In
InternationalConferenceonAudio-andVideo-basedBiometricPersonAuthentication.937–945.
[69] P.GrotherandM.Ngan.2014.Facerecognitionvendortest(FRVT):Performanceoffaceidentificationalgorithms.
NISTInteragencyreport8009,5(2014),14.
[70] J.Guo,JiankangDeng,NiannanXue,andS.Zafeiriou.2018. StackedDenseU-NetswithDualTransformersfor
RobustFaceAlignment.InProceedingsoftheBritishMachineVisionConference.32
[71] S.Guo,J.Xu,D.Chen,C.Zhang,X.Wang,andR.Zhao.2020.Density-AwareFeatureEmbeddingforFaceClustering.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.6698–6706.
[72] Y.GuoandL.Zhang.2017.One-shotfacerecognitionbypromotingunderrepresentedclasses.(2017).arXiv:1707.05574
[73] Y.Guo,L.Zhang,Y.Hu,X.He,andJ.Gao.2016.Ms-celeb-1m:Adatasetandbenchmarkforlarge-scalefacerecognition.
InProceedingsoftheEuropeanConferenceonComputerVision.87–102.
[74] C.Han,S.Shan,M.Kan,S.Wu,andX.Chen.2018.Facerecognitionwithcontrastiveconvolution.InProceedingsof
theEuropeanConferenceonComputerVision.118–134.
[75] Z.Hao,Y.Liu,H.Qin,J.Yan,X.Li,andX.Hu.2017. Scale-awarefacedetection.InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition.6186–6195.
[76] M.Hayat,S.H.Khan,N.Werghi,andR.Goecke.2017.JointRegistrationandRepresentationLearningforUncon-
strainedFaceIdentification.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
1551–1560.
[77] K.He,X.Zhang,S.Ren,andJ.Sun.2016. DeepResidualLearningforImageRecognition.InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition.770–778.
[78] R.He,J.Cao,L.Song,Z.Sun,andT.Tan.2020. AdversarialCross-SpectralFaceCompletionforNIR-VISFace
Recognition.IEEETrans.PatternAnal.Mach.Intell.42(2020),1025–1037.
[79] R.He,X.Wu,Z.Sun,andT.Tan.2017.LearningInvariantDeepRepresentationforNIR-VISFaceRecognition.In
ProceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.33.9005–9012.
[80] R.He,X.Wu,Z.Sun,andT.Tan.2019.WassersteinCNN:LearningInvariantFeaturesforNIR-VISFaceRecognition.
IEEETrans.PatternAnal.Mach.Intell.41(2019),1761–1773.
[81] S.Honari,P.Molchanov,S.Tyree,P.Vincent,C.J.Pal,andJ.Kautz.2018.ImprovingLandmarkLocalizationwith
Semi-SupervisedLearning.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
1546–1555.
[82] S.Hong,W.Im,J.B.Ryu,andH.Yang.2017.SSPP-DAN:Deepdomainadaptationnetworkforfacerecognitionwith
singlesampleperperson.InProceedingsoftheIEEEInternationalConferenceonImageProcessing.825–829.
[83] A.G.Howard,M.Zhu,B.Chen,D.Kalenichenko,W.Wang,T.Weyand,M.Andreetto,andH.Adam.2017.MobileNets:
EfficientConvolutionalNeuralNetworksforMobileVisionApplications.(2017).arXiv:abs/1704.04861
[84] J.Hu,L.Shen,andG.Sun.2018. Squeeze-and-ExcitationNetworks.InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition.7132–7141.
[85] P.HuandD.Ramanan.2017.Findingtinyfaces.InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition.1522–1530.
[86] W.Hu,Y.Huang,F.Zhang,andR.Li.2019. Noise-TolerantParadigmforTrainingFaceRecognitionCNNs.In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.11879–11888.
[87] Y.Hu,X.Wu,B.Yu,R.He,andZ.Sun.2018.Pose-GuidedPhotorealisticFaceRotation.InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition.8398–8406.
[88] G.B.Huang,M.Ramesh,T.Berg,andE.Learned-Miller.2007.LabeledFacesintheWild:ADatabaseforStudyingFace
RecognitioninUnconstrainedEnvironments.TechnicalReport07-49.UniversityofMassachusetts,Amherst.
[89] L.Huang,Y.Yang,Y.Deng,andY.Yu.2015. Densebox:Unifyinglandmarklocalizationwithendtoendobject
detection.(2015).arXiv:1509.04874
[90] R.Huang,S.Zhang,T.Li,andR.He.2017.BeyondFaceRotation:GlobalandLocalPerceptionGANforPhotorealistic
andIdentityPreservingFrontalViewSynthesis.InProceedingsoftheIEEEInternationalConferenceonComputer
Vision.2458–2467.
[91] X.Huang,W.Deng,H.Shen,X.Zhang,andJ.Ye.2020.PropagationNet:PropagatePointstoCurvetoLearnStructure
Information.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.7265–7274.
[92] Y.Huang,Y.Wang,Y.Tai,X.Liu,P.Shen,S.Li,J.Li,andF.Huang.2020.Curricularface:adaptivecurriculumlearning
lossfordeepfacerecognition.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
5901–5910.
[93] H.Wang,Z.Li,X.Ji,andY.Wang.2017.Facer-cnn.(2017).arXiv:1706.01061
[94] F.N.Iandola,M.W.Moskewicz,K.Ashraf,S.Han,W.J.Dally,andK.Keutzer.2017. SqueezeNet:AlexNet-level
accuracywith50xfewerparametersand<1MBmodelsize.(2017).arXiv:1602.07360
[95] M.Jaderberg,K.Simonyan,A.Zisserman,andK.Kavukcuoglu.2015.Spatialtransformernetworks.InAdvancesin
neuralinformationprocessingsystems.2017–2025.
[96] AnilK.Jain,DebayanDeb,andJoshuaJ.Engelsma.2021.Biometrics:Trust,butVerify.ArXivabs/2105.06625(2021).
[97] H.JiangandE.Learned-Miller.2017.Facedetectionwiththefasterr-cnn.InProceedingsoftheIEEEInternational
ConferenceonAutomaticFace&GestureRecognition.650–657.
[98] H.Jin,S.Zhang,X.uZhu,Y.Tang,Z.Lei,andS.Z.Li.2019.LearningLightweightFaceDetectorwithKnowledge
Distillation.InProceedingsoftheInternationalConferenceonBiometrics.1–7.33
[99] X.JinandX.Tan.2017.Facealignmentin-the-wild:Asurvey.ComputerVisionandImageUnderstanding162(2017),
1–22.
[100] Y.Jing,Q.Liu,andK.Zhang.2017.StackedHourglassNetworkforRobustFacialLandmarkLocalisation.InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognitionWorkshops.79–87.
[101] A.JourablooandX.Liu.2016.Large-PoseFaceAlignmentviaCNN-BasedDense3DModelFitting.InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition.4188–4196.
[102] A.Jourabloo,M.Ye,X.Liu,andL.Ren.2017.Pose-InvariantFaceAlignmentwithaSingleCNN.InProceedingsofthe
IEEEInternationalConferenceonComputerVision.3219–3228.
[103] N.D.Kalka,B.Maze,J.A.Duncan,K.OrConnor,S.Elliott,K.Hebert,J.Bryan,andA.K.Jain.2018.IJB–S:IARPAJanus
SurveillanceVideoBenchmark.InProceedingsoftheIEEEInternationalConferenceonBiometricsTheory,Applications
andSystems(BTAS).1–9.
[104] M.Kan,S.Shan,andX.Chen.2016. Multi-viewdeepnetworkforcross-viewclassification.InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition.4847–4855.
[105] B.Kang,Y.Kim,B.Jun,andD.Kim.2019.AttentionalFeature-PairRelationNetworksforAccurateFaceRecognition.
InProceedingsoftheIEEEInternationalConferenceonComputerVision.5471–5480.
[106] B.Kang,Y.Kim,andD.Kim.2018.Pairwiserelationalnetworksforfacerecognition.InProceedingsoftheEuropean
ConferenceonComputerVision.628–645.
[107] I.Kemelmacher-Shlizerman,S.M.Seitz,D.Miller,andE.Brossard.2016.Themegafacebenchmark:1millionfacesfor
recognitionatscale.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.4873–4882.
[108] I.Kemelmacher-Shlizerman,S.Suwajanakorn,andS.Seitz.2014.Illumination-AwareAgeProgression.InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.3334–3341.
[109] Y.Kim,W.Park,M.lRoh,andJ.Shin.2020. GroupFace:LearningLatentGroupsandConstructingGroup-based
RepresentationsforFaceRecognition.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition.5621–5630.
[110] B.Klare,B.Klein,E.Taborsky,A.Blanton,J.Cheney,K.E.Allen,P.Grother,A.Mah,M.Burge,andA.K.Jain.2015.
Pushingthefrontiersofunconstrainedfacedetectionandrecognition:IARPAJanusBenchmarkA.InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition.1931–1939.
[111] M.Kostinger,P.Wohlhart,P.M.Roth,andH.Bischof.2011.AnnotatedFacialLandmarksintheWild:Alarge-scale,
real-worlddatabaseforfaciallandmarklocalization.InProceedingsoftheIEEEInternationalConferenceonComputer
VisionWorkshops.2144–2151.
[112] A.Krizhevsky,I.Sutskever,andG.E.Hinton.2012.ImageNetClassificationwithDeepConvolutionalNeuralNetworks.
InAdvancesinNeuralInformationProcessingSystems.1097–1105.
[113] A.KumarandR.Chellappa.2018.Disentangling3DPoseinaDendriticCNNforUnconstrained2DFaceAlignment.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.430–439.
[114] A.nKumar,T.K.Marks,W.Mou,Y.Wang,M.Jones,A.Cherian,T.Koike-Akino,X.Liu,andC.Feng.2020.LUVLi
facealignment:estimatinglandmarks’location,uncertainty,andvisibilitylikelihood.InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition.8236–8246.
[115] V.Kushwaha,M.Singh,R.Singh,M.Vatsa,N.K.Ratha,andR.Chellappa.2018. DisguisedFacesintheWild.In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognitionWorkshops.1–18.
[116] H.LawandJ.Deng.2018.Cornernet:Detectingobjectsaspairedkeypoints.InProceedingsoftheEuropeanConference
onComputerVision.734–750.
[117] V.Le,J.Brandt,Z.L.Lin,L.D.Bourdev,andT.S.Huang.2012.InteractiveFacialFeatureLocalization.InProceedings
oftheEuropeanConferenceonComputerVision.679–692.
[118] J.Lezama,Q.Qiu,andG.Sapiro.2017. NotAfraidoftheDark:NIR-VISFaceRecognitionviaCross-Spectral
HallucinationandLow-RankEmbedding.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition.6807–6816.
[119] H.Li,Z.Lin,X.Shen,J.Brandt,andG.Hua.2015. Aconvolutionalneuralnetworkcascadeforfacedetection.In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.5325–5334.
[120] J.Li,Y.Wang,C.Wang,Y.Tai,J.Qian,J.Yang,C.Wang,J.Li,andF.Huang.2019.DSFD:dualshotfacedetector.In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.5060–5069.
[121] S.Li,H.Li,J.Cui,andH.Zha.2019.Pose-AwareFaceAlignmentbasedonCNNand3DMM.InProceedingsofthe
BritishMachineVisionConference.106.
[122] S.Z.Li,D.Yi,Z.Lei,andS.Liao.2013.TheCASIANIR-VIS2.0FaceDatabase.InProceedingsofIEEEConferenceon
ComputerVisionandPatternRecognitionWorkshops.348–353.
[123] S.Z.Li,L.Zhu,Z.Zhang,A.Blake,H.Zhang,andH.Shum.2002.StatisticalLearningofMulti-viewFaceDetection.
InProceedingsoftheEuropeanConferenceonComputerVision.67–81.34
[124] Y.Li,B.Sun,T.Wu,andY.Wang.2016.Facedetectionwithend-to-endintegrationofaconvNetanda3dmodel.In
ProceedingsoftheEuropeanConferenceonComputerVision.420–436.
[125] Z.Li,X.Tang,J.Han,J.Liu,andR.He.2019. PyramidBox++:HighPerformanceDetectorforFindingTinyFace.
(2019).arXiv:1904.00386
[126] S.Liao,Z.Lei,D.Yi,andS.Z.Li.2014.Abenchmarkstudyoflarge-scaleunconstrainedfacerecognition.InProceedings
oftheIEEEInternationalJointConferenceonBiometrics.1–8.
[127] T.Lin,P.Dollár,R.Girshick,K.He,B.Hariharan,andS.Belongie.2017.Featurepyramidnetworksforobjectdetection.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2117–2125.
[128] T.Lin,P.Goyal,R.Girshick,K.He,andP.Dollár.2017.Focallossfordenseobjectdetection.InProceedingsofthe
IEEEInternationalConferenceonComputerVision.2980–2988.
[129] W.Lin,J.Chen,C.D.Castillo,andR.Chellappa.2018.DeepDensityClusteringofUnconstrainedFaces.InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.8128–8137.
[130] B.Liu,W.Deng,Y.Zhong,M.Wang,J.Hu,X.Tao,andY.Huang.2019. FairLoss:Margin-AwareReinforcement
LearningforDeepFaceRecognition.InProceedingsoftheIEEEInternationalConferenceonComputerVision.10051–
10060.
[131] C.LiuandH.Wechsler.2002.Gaborfeaturebasedclassificationusingtheenhancedfisherlineardiscriminantmodel
forfacerecognition.Trans.ImageProcess.114(2002),467–76.
[132] H.Liu,J.Lu,J.Feng,andJ.Zhou.2018.Two-StreamTransformerNetworksforVideo-BasedFaceAlignment.IEEE
Trans.PatternAnal.Mach.Intell.40(2018),2546–2554.
[133] H.Liu,J.Lu,M.Guo,S.Wu,andJ.Zhou.2020.LearningReasoning-DecisionNetworksforRobustFaceAlignment.
IEEETrans.PatternAnal.Mach.Intell.42(2020),679–693.
[134] H.Liu,X.Zhu,Z.Lei,andS.Z.Li.2019. AdaptiveFace:AdaptiveMarginandSamplingforFaceRecognition.In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.11947–11956.
[135] J.Liu,Y.Deng,T.Bai,Z.Wei,andC.Huang.2015.TargetingUltimateAccuracy:FaceRecognitionviaDeepEmbedding.
(2015).arXiv:1506.07310
[136] W.Liu,D.Anguelov,D.Erhan,ChristianSzegedy,ScottReed,Cheng-YangFu,andAlexanderC.Berg.2016.SSD:
singleshotmultiBoxdetector.InProceedingsoftheEuropeanConferenceonComputerVision.21–37.
[137] W.Liu,Y.Wen,Z.Yu,M.Li,B.Raj,andL.Song.2017.Sphereface:Deephypersphereembeddingforfacerecognition.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.212–220.
[138] W.Liu,Y.Wen,Z.Yu,andM.Yang.2016.Large-marginsoftmaxlossforconvolutionalneuralnetworks..InICML,
Vol.2.7.
[139] X.Liu,B.V.K.V.Kumar,C.Yang,Q.Tang,andJ.You.2018.Dependency-AwareAttentionControlforUnconstrained
FaceRecognitionwithImageSets.InProceedingsoftheEuropeanConferenceonComputerVision.548–565.
[140] X.Liu,L.Song,X.Wu,andT.Tan.2016.TransferringdeeprepresentationforNIR-VISheterogeneousfacerecognition.
ProceedingsoftheInternationalConferenceonBiometrics(2016),1–8.
[141] Y.Liu,A.Jourabloo,W.Ren,andX.Liu.2017.DenseFaceAlignment.InProceedingsoftheIEEEInternationalConference
onComputerVisionWorkshops.1619–1628.
[142] Y.Liu,H.Li,andX.Wang.2017.RethinkingFeatureDiscriminationandPolymerizationforLarge-scaleRecognition.
(2017).arXiv:1710.00870
[143] Y.Liu,H.Li,J.Yan,F.Wei,X.Wang,andX.Tang.2017.RecurrentScaleApproximationforObjectDetectioninCNN.
InProceedingsoftheIEEEInternationalConferenceonComputerVision.571–579.
[144] Y.Liu,H.Shen,Y.Si,X.Wang,X.Zhu,H.Shi,Z.Hong,H.Guo,Z.Guo,Y.Chen,B.Li,T.Xi,J.Yu,H.Xie,G.Xie,M.
Li,Q.Lu,Z.Wang,S.Lai,Z.Chai,andX.Wei.2019.GrandChallengeof106-PointFacialLandmarkLocalization.In
ProceedingsoftheIEEEICMEWorkshop.613–616.
[145] Y.Liu,X.Tang,J.Han,J.Liu,D.Rui,andX.Wu.2020.HAMBox:DelvingIntoMiningHigh-QualityAnchorsonFace
Detection.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.13043–13051.
[146] Z.Liu,H.H.CloudMinds,J.Bai,S.Li,andS.L.CloudMinds.2019. FeatureAggregationNetworkforVideoFace
Recognition.InProceedingsoftheIEEEInternationalConferenceonComputerVisionWorkshops.990–998.
[147] Z.Liu,X.Zhu,G.Hu,H.Guo,M.Tang,Z.Lei,N.M.Robertson,andJ.Wang.2019. SemanticAlignment:Finding
SemanticallyConsistentGround-TruthforFacialLandmarkDetection.InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition.3462–3471.
[148] PingLuo,ZhenyaoZhu,ZiweiLiu,XiaogangWang,andXiaoouTang.2016.FaceModelCompressionbyDistilling
KnowledgefromNeurons.InProceedingsoftheAAAIConferenceonArtificialIntelligence.3560–3566.
[149] J.Lv,X.Shao,J.Xing,C.Cheng,andX.Zhou.2017.ADeepRegressionArchitecturewithTwo-StageRe-initialization
forHighPerformanceFacialLandmarkDetection.InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition.3691–3700.35
[150] m.t.PhamandT.J.Cham.2007.FasttrainingandselectionofHaarfeaturesusingstatisticsinboosting-basedface
detection.InProceedingsoftheIEEEInternationalConferenceonComputerVision.1–7.
[151] N.Ma,X.Zhang,H.Zheng,andJ.Sun.2018.Shufflenetv2:Practicalguidelinesforefficientcnnarchitecturedesign.
InProceedingsoftheEuropeanConferenceonComputerVision.116–131.
[152] R.Manmatha,C.nWu,A.Smola,andP.Krähenbühl.2017. SamplingMattersinDeepEmbeddingLearning.In
ProceedingsoftheIEEEInternationalConferenceonComputerVision.2859–2867.
[153] B.Martínez,M.F.Valstar,X.Binefa,andM.Pantic.2013.LocalEvidenceAggregationforRegression-BasedFacial
PointDetection.IEEETrans.PatternAnal.Mach.Intell.35(2013),1149–1163.
[154] Y.Martínez-Díaz,L.S.Luevano,H.M.Vazquez,M.Nicolás-Díaz,L.Chang,andM.González-Mendoza.2019.Shuffle-
FaceNet:ALightweightFaceArchitectureforEfficientandHighly-AccurateFaceRecognition.InProceedingsofthe
IEEEInternationalConferenceonComputerVisionWorkshops.2721–2728.
[155] I.Masi,S.Rawls,G.G.Medioni,andP.Natarajan.2016.Pose-AwareFaceRecognitionintheWild.InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition.4838–4846.
[156] B.Maze,J.C.Adams,J.sA.Duncan,N.D.Kalka,T.Miller,C.Otto,A.K.Jain,W.T.Niggel,J.Anderson,J.Cheney,
andPatrickGrother.2018.IARPAJanusBenchmark-C:FaceDatasetandProtocol.InProceedingsoftheInternational
ConferenceonBiometrics.158–165.
[157] TaoMei,BoYang,ShiqiangYang,andXianshengHua.2008. Videocollage:presentingavideosequenceusinga
singleimage.TheVisualComputer25(2008),39–51.
[158] D.Merget,M.Rock,andG.Rigoll.2018.RobustFacialLandmarkDetectionviaaFully-ConvolutionalLocal-Global
ContextNetwork.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.781–790.
[159] X.Miao,X.Zhen,X.Liu,C.Deng,V.Athitsos,andH.Huang.2018.DirectShapeRegressionNetworksforEnd-to-End
FaceAlignment.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.5040–5049.
[160] ShervinMinaee,PingLuo,ZheLin,andK.Bowyer.2021. GoingDeeperIntoFaceDetection:ASurvey. ArXiv
abs/2103.14983(2021).
[161] X.Ming,F.Wei,T.Zhang,D.Chen,andF.Wen.2019. GroupSamplingforScaleInvariantFaceDetection.In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.3446–3456.
[162] T.Mita,T.Kaneko,andO.Hori.2005.JointHaar-likefeaturesforfacedetection.InProceedingsoftheIEEEInternational
ConferenceonComputerVision.1619–1626.
[163] P.Mittal,M.Vatsa,andR.Singh.2015.Compositesketchrecognitionviadeepnetwork-atransferlearningapproach.
InProceedingsofInternationalConferenceonBiometrics.251–256.
[164] S.Moschoglou,A.Papaioannou,C.Sagonas,J.Deng,I.Kotsia,andS.Zafeiriou.2017.AgeDB:TheFirstManually
Collected,In-the-WildAgeDatabase.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
RecognitionWorkshops.1997–2005.
[165] M.Najibi,P.Samangouei,R.Chellappa,andL.S.Davis.2017.SSH:singlestageheadlessfacedetector.InProceedings
oftheIEEEInternationalConferenceonComputerVision.4885–4894.
[166] M.Najibi,B.Singh,andL.S.Davis.2019.FA-RPN:FloatingRegionProposalsforFaceDetection.InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition.7723–7732.
[167] A.NechandI.Kemelmacher-Shlizerman.2017.LevelPlayingFieldforMillionScaleFaceRecognition.InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.3406–3415.
[168] A.Newell,K.Yang,andJ.Deng.2016.StackedHourglassNetworksforHumanPoseEstimation.InProceedingsofthe
EuropeanConferenceonComputerVision.483–499.
[169] T.Ojala,M.Pietikainen,andT.Maenpaa.2002.Multiresolutiongray-scaleandrotationinvarianttextureclassification
withlocalbinarypatterns.IEEETrans.PatternAnal.Mach.Intell.24,7(2002),971–987.
[170] M.Opitz,G.Waltner,G.Poier,H.Possegger,andH.Bischof.2016.Gridloss:detectingoccludedfaces.InProceedings
oftheEuropeanConferenceonComputerVision,Vol.9907.386–402.
[171] M.Parchami,S.Bashbaghi,E.Granger,andS.Sayed.2017.Usingdeepautoencoderstolearnrobustdomain-invariant
representationsforstill-to-videofacerecognition.InProceedingsoftheIEEEInternationalConferenceonAdvanced
VideoandSignalBasedSurveillance(AVSS).1–6.
[172] O.M.Parkhi,A.Vedaldi,andA.Zisserman.2015.DeepFaceRecognition.InProceedingsoftheBritishMachineVision
Conference.41.1–41.12.
[173] X.Peng,R.S.Feris,X.Wang,andD.N.Metaxas.2016.ARecurrentEncoder-DecoderNetworkforSequentialFace
Alignment.InProceedingsoftheEuropeanConferenceonComputerVision.38–56.
[174] H.Qin,J.Yan,L.Xiu,andX.Hu.2016. JointTrainingofCascadedCNNforFaceDetection.InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition.3456–3465.
[175] R.Ranjan,C.D.Castillo,andR.Chellappa.2017. L2-constrainedsoftmaxlossfordiscriminativefaceverification.
(2017).arXiv:1703.0950736
[176] R.Ranjan,V.M.Patel,andR.Chellappa.2019.HyperFace:ADeepMulti-TaskLearningFrameworkforFaceDetection,
LandmarkLocalization,PoseEstimation,andGenderRecognition.IEEETrans.PatternAnal.Mach.Intell.41,1(2019),
121–135.
[177] R.Ranjan,S.Sankaranarayanan,A.Bansal,N.Bodla,J.Chen,V.M.Patel,C.D.Castillo,andR.Chellappa.2018.Deep
LearningforUnderstandingFaces:MachinesMayBeJustasGood,orBetter,thanHumans.IEEESignalProcessing
Magazine35,1(2018),66–83.
[178] Y.Rao,J.Lin,J.Lu,andJ.Zhou.2017.LearningDiscriminativeAggregationNetworkforVideo-BasedFaceRecognition.
InProceedingsoftheIEEEInternationalConferenceonComputerVision.3801–3810.
[179] Y.Rao,J.Lu,andJ.Zhou.2017. Attention-AwareDeepReinforcementLearningforVideoFaceRecognition.In
ProceedingsoftheIEEEInternationalConferenceonComputerVision.3951–3960.
[180] C.Reale,N.M.Nasrabadi,H.Kwon,andR.Chellappa.2016.SeeingtheForestfromtheTrees:AHolisticApproachto
Near-InfraredHeterogeneousFaceRecognition.InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognitionW.320–328.
[181] S.Ren,K.He,R.Girshick,andJ.Sun.2015.Fasterr-cnn:Towardsreal-timeobjectdetectionwithregionproposal
networks.InAdvancesinneuralinformationprocessingsystems.91–99.
[182] J.P.Robinson,Y.Li,N.Zhang,Y.Fu,andS.Tulyakov.2019.LaplaceLandmarkLocalization.InProceedingsoftheIEEE
InternationalConferenceonComputerVision.10102–10111.
[183] A.RoyChowdhury,X.Yu,K.Sohn,E.Learned-Miller,andM.Chandraker.2020. ImprovingFaceRecognitionby
ClusteringUnlabeledFacesintheWild.InProceedingsoftheEuropeanConferenceonComputerVision.119–136.
[184] C.Sagonas,E.Antonakos,G.Tzimiropoulos,S.Zafeiriou,andM.Pantic.2016. 300FacesIn-The-WildChallenge:
databaseandresults.ImageVis.Comput.47(2016),3–18.
[185] C.Sagonas,G.Tzimiropoulos,S.Zafeiriou,andM.Pantic.2013.300Facesin-the-WildChallenge:TheFirstFacial
LandmarkLocalizationChallenge.InProceedingsoftheIEEEInternationalConferenceonComputerVisionWorkshops.
397–403.
[186] M.Sandler,A.G.Howard,M.Zhu,A.Zhmoginov,andL.Chen.2018.MobileNetV2:InvertedResidualsandLinear
Bottlenecks.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.4510–4520.
[187] S.Sankaranarayanan,A.Alavi,C.D.Castillo,andR.Chellappa.2016. Tripletprobabilisticembeddingforface
verificationandclustering.InProceedingsoftheIEEEInternationalConferenceonBiometricsTheory,Applicationsand
Systems.1–8.
[188] S.SaxenaandJ.Verbeek.2016.HeterogeneousFaceRecognitionwithCNNs.InProceedingsoftheEuropeanConference
onComputerVisionWorkshops.483–491.
[189] F.Schroff,D.Kalenichenko,andJ.Philbin.2015.FaceNet:Aunifiedembeddingforfacerecognitionandclustering.In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.815–823.
[190] M.SchultzandT.Joachims.2004. Learningadistancemetricfromrelativecomparisons.InAdvancesinneural
informationprocessingsystems.41–48.
[191] S.Sengupta,J.Chen,C.Castillo,V.M.Patel,R.Chellappa,andD.W.Jacobs.2016.Frontaltoprofilefaceverification
inthewild.InProceedingsoftheIEEEWinterConferenceonApplicationsofComputerVision.1–9.
[192] J.Shen,S.Zafeiriou,G.G.Chrysos,J.Kossaifi,G.Tzimiropoulos,andM.Pantic.2015. TheFirstFacialLandmark
Trackingin-the-WildChallenge:BenchmarkandResults.InProceedingsoftheIEEEInternationalConferenceon
ComputerVisionWorkshops.1003–1011.
[193] X.Shi,S.Shan,M.Kan,S.Wu,andX.Chen.2018. Real-Timerotation-invariantfacedetectionwithprogressive
calibrationnetworks.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2295–2303.
[194] Y.Shi,A.K.Jain,andN.D.Kalka.2019. ProbabilisticFaceEmbeddings.InProceedingsoftheIEEEInternational
ConferenceonComputerVision.6901–6910.
[195] Y.Shi,C.Otto,andA.K.Jain.2018.Faceclustering:representationandpairwiseconstraints.IEEETransactionson
InformationForensicsandSecurity13,7(2018),1626–1640.
[196] Y.Shi,X.Yu,K.Sohn,M.Chandraker,andA.K.Jain.2020.TowardsUniversalRepresentationLearningforDeepFace
Recognition.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.6817–6826.
[197] A.Shrivastava,A.Gupta,andR.Girshick.2016.Trainingregion-basedobjectdetectorswithonlinehardexample
mining.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.761–769.
[198] K.SimonyanandA.Zisserman.2015.VeryDeepConvolutionalNetworksforLarge-ScaleImageRecognition.(2015).
arXiv:1409.1556
[199] E.Smirnov,A.Melnikov,S.Novoselov,E.Luckyanets,andG.Lavrentyeva.2017. Doppelgangerminingforface
representationlearning.InProceedingsoftheIEEEInternationalConferenceonComputerVision.1916–1923.
[200] K.Sohn.2016.Improveddeepmetriclearningwithmulti-classn-pairlossobjective.InAdvancesinNeuralInformation
ProcessingSystems.1857–1865.37
[201] S.Soltanpour,B.Boufama,andQ.M.J.Wu.2017.Asurveyoflocalfeaturemethodsfor3Dfacerecognition.Pattern
Recognition72(2017),391–406.
[202] G.Song,Y.Liu,M.Jiang,Y.Wang,J.Yan,andB.Leng.2018.BeyondTrade-Off:AccelerateFCN-BasedFaceDetector
WithHigherAccuracy.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.7756–
7764.
[203] H.OhSong,Y.Xiang,S.Jegelka,andS.Savarese.2016.Deepmetriclearningvialiftedstructuredfeatureembedding.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.4004–4012.
[204] J.Song,J.Zhang,L.Gao,X.Liu,andHShen.2018. DualConditionalGANsforFaceAgingandRejuvenation..In
InternationalJointConferenceonArtificialIntelligence.899–905.
[205] LingxiaoSong,ManZhang,XiangWu,andRanHe.2018.Adversarialdiscriminativeheterogeneousfacerecognition.
InProceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.32.
[206] SijieSong,WeiZhang,JiayingLiu,andTaoMei.2019.UnsupervisedPersonImageGenerationWithSemanticParsing
Transformation.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2352–2361.
[207] K.Sun,W.Wu,T.Liu,S.Yang,Q.Wang,Q.Zhou,Z.Ye,andC.Qian.2019.FAB:ARobustFacialLandmarkDetection
FrameworkforMotion-BlurredVideos.InProceedingsoftheIEEEInternationalConferenceonComputerVision.
5461–5470.
[208] X.Sun,P.Wu,andS.C.H.Hoi.2018. Facedetectionusingdeeplearning:animprovedfasterrcnnapproach.
Neurocomputing299(2018),42–50.
[209] Y.Sun,Y.Chen,X.Wang,andX.Tang.2014.Deeplearningfacerepresentationbyjointidentification-verification.In
Advancesinneuralinformationprocessingsystems.1988–1996.
[210] Y.Sun,C.Cheng,Y.Zhang,C.Zhang,L.Zheng,Z.Wang,andY.Wei.2020. CircleLoss:AUnifiedPerspectiveof
PairSimilarityOptimization.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
6398–6407.
[211] Y.Sun,D.Liang,X.Wang,andX.Tang.2015.DeepID3:FaceRecognitionwithVeryDeepNeuralNetworks.(2015).
arXiv:1502.00873
[212] Y.Sun,X.Wang,andX.Tang.2013.DeepConvolutionalNetworkCascadeforFacialPointDetection.InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.3476–3483.
[213] Y.Sun,X.Wang,andX.Tang.2013.HybridDeepLearningforFaceVerification.InProceedingsoftheIEEEInternational
ConferenceonComputerVision.1489–1496.
[214] Y.Sun,X.Wang,andX.Tang.2014.DeepLearningFaceRepresentationfromPredicting10,000Classes.InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.1891–1898.
[215] Y.Sun,X.Wang,andX.Tang.2015.Deeplylearnedfacerepresentationsaresparse,selective,androbust.InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognitionWorkshops.2892–2900.
[216] Y.Sun,X.Wang,andX.Tang.2016.Sparsifyingneuralnetworkconnectionsforfacerecognition.InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition.4856–4864.
[217] C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed,D.Anguelov,D.Erhan,V.Vanhoucke,andA.Rabinovich.2015.Going
deeperwithconvolutions.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.1–9.
[218] O.Tadmor,T.Rosenwein,S.Shalev-Shwartz,Y.Wexler,andA.Shashua.2016.LearningaMetricEmbeddingforFace
RecognitionusingtheMultibatchMethod.InAdvancesinneuralinformationprocessingsystems.1396–1397.
[219] Y.Tai,Y.Liang,X.Liu,L.Duan,J.Li,C.Wang,F.Huang,andY.Chen.2019.TowardsHighlyAccurateandStable
FaceAlignmentforHigh-ResolutionVideos.InProceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.33.
8893–8900.
[220] Y.Taigman,M.Yang,M.Ranzato,andL.Wolf.2014.Deepface:Closingthegaptohuman-levelperformanceinface
verification.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.1701–1708.
[221] X.Tang,D.K.Du,Z.He,andJ.Liu.2018.Pyramidbox:acontext-assistedsingleshotfacedetector.InProceedingsof
theEuropeanConferenceonComputerVision.797–813.
[222] ZhiqiangTang,XiPeng,ShijieGeng,LingfeiWu,ShaotingZhang,andDimitrisN.Metaxas.2018.QuantizedDensely
ConnectedU-NetsforEfficientLandmarkLocalization.InProceedingsoftheEuropeanConferenceonComputerVision.
348–364.
[223] W.Tian,Z.Wang,H.Shen,W.Deng,Y.Meng,B.Chen,X.Zhang,Y.Zhao,andX.Huang.2018. LearningBetter
FeaturesforFaceDetectionwithFeatureFusionandSegmentationSupervision.arXiv:1811.08557
[224] Z.Tian,C.Shen,H.Chen,andT.He.2019.FCOS:FullyConvolutionalOne-StageObjectDetection.InProceedingsof
theIEEEInternationalConferenceonComputerVision.9626–9635.
[225] L.Tran,X.Yin,andX.Liu.2017.DisentangledRepresentationLearningGANforPose-InvariantFaceRecognition.In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.1283–1292.
[226] G.Trigeorgis,P.Snape,M.A.Nicolaou,E.Antonakos,andS.Zafeiriou.2016.MnemonicDescentMethod:ARecurrent
ProcessAppliedforEnd-to-EndFaceAlignment.InIEEEConferenceonComputerVisionandPatternRecognition.38
4177–4187.
[227] M.TurkandA.Pentland.1991.EigenfacesforRecognition.JournalofCognitiveNeuroscience3,1(1991),71–86.
[228] T.Zheng,W.Deng,andJ.Hu.2017.AgeEstimationGuidedConvolutionalNeuralNetworkforAge-InvariantFace
Recognition.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognitionW.503–511.
[229] JainVandE.Learned-Miller.2010.FDDB:ABenchmarkforFaceDetectioninUnconstrainedSettings.TechnicalReport
UM-CS-2010-009.
[230] PaulViolaandMichaelJones.2001.Rapidobjectdetectionusingaboostedcascadeofsimplefeatures.InProceedings
oftheconferenceoncomputervisionandpatternrecognition,Vol.1.I–I.
[231] Zhaow,R.Chellappa,P.J.Phillips,andA.Rosenfeld.2003.Facerecognition:Aliteraturesurvey.ACMcomputing
surveys35,4(2003),399–458.
[232] S.Wan,Z.Chen,T.Zhang,B.Zhang,andK.Wong.2016.Bootstrappingfacedetectionwithhardnegativeexamples.
(2016).arXiv:1608.02236
[233] W.Wan,Y.Gao,andH.J.Lee.2019.Transferdeepfeaturelearningforfacesketchrecognition.NeuralComputing
andApplications(2019),1–10.
[234] F.Wang,L.Chen,C.Li,S.Huang,Y.Chen,C.Qian,andC.C.Loy.2018.TheDevilofFaceRecognitionisintheNoise.
InProceedingsoftheEuropeanConferenceonComputerVision.765–780.
[235] F.Wang,J.Cheng,W.Liu,andH.Liu.2018.Additivemarginsoftmaxforfaceverification.IEEESingalprocessing
letters25,7(2018),926–930.
[236] F.Wang,M.Jiang,C.Qian,S.Yang,C.Li,H.Zhang,X.Wang,andX.Tang.2017.ResidualAttentionNetworkforImage
Classification.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.6450–6458.
[237] F.Wang,X.Xiang,J.Cheng,andA.L.Yuille.2017.Normface:l2hypersphereembeddingforfaceverification.In
Proceedingsofthe25thACMinternationalconferenceonMultimedia.1041–1049.
[238] H.Wang,D.Gong,Z.Li,andW.Liu.2019.DecorrelatedAdversarialLearningforAge-InvariantFaceRecognition.In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.3522–3531.
[239] H.Wang,Y.Wang,Z.Zhou,X.Ji,D.Gong,J.Zhou,Z.Li,andW.Liu.2018.Cosface:Largemargincosinelossfordeep
facerecognition.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.5265–5274.
[240] J.Wang,k.Sun,T.Cheng,B.Jiang,C.Deng,Y.Zhao,D.Liu,Y.Mu,M.Tan,X.Wang,W.Liu,andB.Xiao.2021.
Deephigh-resolutionrepresentationlearningforvisualrecognition.IEEETrans.PatternAnal.Mach.Intell.43(2021),
3349–3364.
[241] J.Wang,Y.Yuan,B.Li,G.Yu,andS.Jian.2018.Sface:Anefficientnetworkforfacedetectioninlargescalevariations.
(2018).arXiv:1804.06559
[242] J.Wang,Y.Yuan,andG.Yu.2017.Faceattentionnetwork:aneffectivefacedetectorfortheoccludedfaces.(2017).
arXiv:1711.07246
[243] L.Wang,V.Sindagi,andV.M.Patel.2018. High-QualityFacialPhoto-SketchSynthesisUsingMulti-Adversarial
Networks.InProceedingsoftheIEEEInternationalConferenceonAutomaticFace&GestureRecognition.83–90.
[244] M.WangandW.Deng.2018.DeepFaceRecognition:ASurvey.Neurocomputing312(2018),135–153.
[245] M.WangandW.Deng.2020.MitigatingBiasinFaceRecognitionUsingSkewness-AwareReinforcementLearning.In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.9322–9331.
[246] M.Wang,W.Deng,J.Hu,X.Tao,andY.Huang.2019.RacialFacesintheWild:ReducingRacialBiasbyInformation
MaximizationAdaptationNetwork.InProceedingsoftheIEEEInternationalConferenceonComputerVision.692–702.
[247] N.Wang,X.Gao,D.Tao,H.Yang,andX.Li.2018.Facialfeaturepointdetection:Acomprehensivesurvey.Neurocom-
puting275(2018),50–65.
[248] W.Wang,Z.Cui,Y.Yan,J.Feng,S.Yan,X.Shu,andN.Sebe.2016. RecurrentFaceAging.InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition.2378–2386.
[249] X.Wang,L.Bo,andL.Fuxin.2019. AdaptiveWingLossforRobustFaceAlignmentviaHeatmapRegression.In
ProceedingsoftheIEEEInternationalConferenceonComputerVision.6970–6980.
[250] X.Wang,S.Wang,J.Wang,H.Shi,andT.Mei.2019. Co-Mining:DeepFaceRecognitionWithNoisyLabels.In
ProceedingsoftheIEEEInternationalConferenceonComputerVision.9358–9367.
[251] XiaoboWang,ShifengZhang,ShuoWang,TianyuFu,HailinShi,andTaoMei.2020.Mis-classifiedvectorguided
softmaxlossforfacerecognition.InProceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.34.12241–12248.
[252] Y.Wang,D.Gong,Z.Zhou,X.Ji,H.Wang,Z.Li,W.Liu,andT.Zhang.2018.OrthogonalDeepFeaturesDecomposition
forAge-InvariantFaceRecognition.InProceedingsoftheEuropeanConferenceonComputerVision.738–753.
[253] Y.Wang,X.Ji,Z.Zhou,H.Wang,andZ.Li.2017.Detectingfacesusingregion-basedfullyconvolutionalnetworks.
(2017).arXiv:1709.05256
[254] Z.Wang,X.Tang,W.Luo,andS.Gao.2018.FaceAgingwithIdentity-PreservedConditionalGenerativeAdversarial
Networks.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.7939–7947.39
[255] Z.Wang,L.Zheng,Y.Li,andS.Wang.2019. Linkagebasedfaceclusteringviagraphconvolutionnetwork.In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.1117–1125.
[256] H.Wei,P.Lu,andY.Wei.2020. BalancedAlignmentforFaceRecognition:AJointLearningApproach. (2020).
arXiv:2003.10168
[257] K.Q.WeinbergerandL.K.Saul.2006.DistanceMetricLearningforLargeMarginNearestNeighborClassification.In
Advancesinneuralinformationprocessingsystems.1473–1480.
[258] Y.Wen,Z.Li,andY.Qiao.2016. LatentFactorGuidedConvolutionalNeuralNetworksforAge-InvariantFace
Recognition.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.4893–4901.
[259] Y.Wen,K.Zhang,Z.Li,andY.Qiao.2016.Adiscriminativefeaturelearningapproachfordeepfacerecognition.In
ProceedingsoftheEuropeanConferenceonComputerVision.499–515.
[260] C.Whitelam,E.Taborsky,A.Blanton,B.Maze,J.C.Adams,T.Miller,N.D.Kalka,A.K.Jain,J.A.Duncan,K.EAllen,
J.Cheney,andP.Grother.2017.IARPAJanusBenchmark-BFaceDataset.InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognitionW.592–600.
[261] JonathanRWilliford,BrandonBMay,andJeffreyByrne.2020.ExplainableFaceRecognition.InProceedingsofthe
EuropeanConferenceonComputerVision.248–263.
[262] L.Wolf,T.Hassner,andI.Maoz.2011.Facerecognitioninunconstrainedvideoswithmatchedbackgroundsimilarity.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.529–534.
[263] W.Wu,M.Kan,X.Liu,Y.Yang,S.Shan,andX.Chen.2017.RecursiveSpatialTransformer(ReST)forAlignment-Free
FaceRecognition.InProceedingsoftheIEEEInternationalConferenceonComputerVision.3792–3800.
[264] W.Wu,C.Qian,S.Yang,Q.Wang,Y.Cai,andQ.Zhou.2018.LookatBoundary:ABoundary-AwareFaceAlignment
Algorithm.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2129–2138.
[265] X.Wu,R.He,andZ.Sun.2015.ALightenedCNNforDeepFaceRepresentation.(2015).arXiv:1511.02683
[266] X.Wu,R.He,Z.Sun,andT.Tan.2018. ALightCNNforDeepFaceRepresentationWithNoisyLabels. IEEE
TransactionsonInformationForensicsandSecurity13(2018),2884–2896.
[267] X.Wu,H.Huang,V.M.Patel,R.He,andZ.Sun.2019.DisentangledVariationalRepresentationforHeterogeneous
FaceRecognition.InProceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.33.9005–9012.
[268] Y.Wu,T.Hassner,K.Kim,G.Medioni,andP.Natarajan.2018.FacialLandmarkDetectionwithTweakedConvolutional
NeuralNetworks.IEEETrans.PatternAnal.Mach.Intell.40,12(2018),3067–3074.
[269] Y.Wu,H.Liu,andY.Fu.2017. Low-ShotFaceRecognitionwithHybridClassifiers.InProceedingsoftheIEEE
InternationalConferenceonComputerVisionWorkshops.1933–1939.
[270] Y.Wu,Y.Wu,R.Gong,Y.Lv,K.Chen,D.Liang,X.Hu,X.Liu,andJ.Yan.2020. RotationConsistentMarginLoss
forEfficientLow-BitFaceRecognition.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition.6866–6876.
[271] X.Fan,R.Liu,K.Huyan,Y.Feng,andZ.Luo.2018. Self-ReinforcedCascadedRegressionforFaceAlignment.In
ProceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.32.
[272] S.Xiao,J.Feng,L.Liu,X.Nie,W.Wang,S.Yan,andA.Kassim.2017.Recurrent3D-2DDualLearningforLarge-Pose
FacialLandmarkDetection.InProceedingsoftheIEEEInternationalConferenceonComputerVision.1642–1651.
[273] S.Xiao,J.Feng,J.Xing,H.Lai,S.Yan,andA.A.Kassim.2016. RobustFacialLandmarkDetectionviaRecurrent
Attentive-RefinementNetworks..InProceedingsoftheEuropeanConferenceonComputerVision.57–72.
[274] W.Xie,L.Shen,andA.Zisserman.2018.ComparatorNetworks.InProceedingsoftheEuropeanConferenceonComputer
Vision.782–797.
[275] YilinXiong,ZijianZhou,YuhaoDou,andZhizhongSu.2020.Gaussianvector:Anefficientsolutionforfaciallandmark
detection.InProceedingsoftheAsianConferenceonComputerVision.70–87.
[276] C.Xu,Q.Liu,andM.Ye.2017. Ageinvariantfacerecognitionandretrievalbycoupledauto-encodernetworks.
Neurocomputing222(2017),62–71.
[277] X.XuandI.A.Kakadiaris.2017.Jointheadposeestimationandfacealignmentframeworkusingglobalandlocalcnn
features.InProceedingsoftheIEEEInternationalConferenceonAutomaticFace&GestureRecognition.642–649.
[278] XiaqingXu,QiangMeng,YunxiaoQin,JianzhuGuo,ChenxuZhao,FengZhou,andZhenLei.2021.Searchingfor
AlignmentinFaceRecognition.InProceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.35.3065–3073.
[279] Y.Xu,W.Yan,H.Sun,G.Yang,andJ.Luo.2019.CenterFace:JointFaceDetectionandAlignmentUsingFaceasPoint.
arXiv:1911.03599
[280] J.Yan,X.Zhang,Z.Lei,andS.Z.Li.2014.Facedetectionbystructuralmodels.ImageandVisionComputing32,10
(2014),790–799.
[281] B.Yang,J.Yan,Z.Lei,andS.Z.Li.2015.Fine-grainedevaluationonfacedetectioninthewild.InProceedingsofthe
IEEEInternationalConferenceonAutomaticFace&GestureRecognition,Vol.1.1–7.
[282] JingYang,AdrianBulat,andGeorgiosTzimiropoulos.2020.Fan-face:asimpleorthogonalimprovementtodeepface
recognition.InProceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.34.12621–12628.40
[283] J.Yang,P.Ren,D.Zhang,D.Chen,F.Wen,H.Li,andG.Hua.2017. NeuralAggregationNetworkforVideoFace
Recognition.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.5216–5225.
[284] L.Yang,D.Chen,X.Zhan,R.Zhao,C.C.Loy,andD.Lin.2020. LearningtoClusterFacesviaConfidenceand
ConnectivityEstimation.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
13369–13378.
[285] L.Yang,X.Zhan,D.Chen,J.Yan,C.C.Loy,andD.Lin.2019. Learningtoclusterfacesonanaffinitygraph.In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2298–2306.
[286] Ming-HsuanYang,D.Kriegman,andN.Ahuja.2002.DetectingFacesinImages:ASurvey.IEEETrans.PatternAnal.
Mach.Intell.24(2002),34–58.
[287] S.Yang,P.Luo,C.C.Loy,andX.Tang.2015.Fromfacialpartsresponsestofacedetection:adeeplearningapproach.
InProceedingsoftheIEEEInternationalConferenceonComputerVision.3676–3684.
[288] S.Yang,P.Luo,C.C.Loy,andX.Tang.2016. WIDERFACE:AFaceDetectionBenchmark.InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition.5525–5533.
[289] D.Yi,Z.Lei,S.Liao,andS.Z.Li.2014.LearningFaceRepresentationfromScratch.(2014).arXiv:1411.7923
[290] Z.Yi,H.Zhang,P.Tan,andM.Gong.2017.DualGAN:UnsupervisedDualLearningforImage-to-ImageTranslation.
InProceedingsoftheIEEEInternationalConferenceonComputerVision.2868–2876.
[291] BangjieYin,LuanTran,HaoxiangLi,XiaohuiShen,andXiaomingLiu.2019.Towardsinterpretablefacerecognition.
InProceedingsoftheIEEEInternationalConferenceonComputerVision.9348–9357.
[292] X.Yin,X.Yu,K.Sohn,X.Liu,andM.Chandraker.2019. FeatureTransferLearningforFaceRecognitionwith
Under-RepresentedData.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
5704–5713.
[293] H.Yu,Y.Fan,K.Chen,H.Yan,X.Lu,J.Liu,andD.Xie.2019.UnknownIdentityRejectionLoss:UtilizingUnlabeledData
forFaceRecognition.InProceedingsoftheIEEEInternationalConferenceonComputerVisionWorkshops.2662–2669.
[294] J.Yu,Y.Jiang,Z.Wang,Z.Cao,andT.Huang.2016.Unitbox:Anadvancedobjectdetectionnetwork.InProceedings
ofthe24thACMinternationalconferenceonMultimedia.516–520.
[295] Y.Yu,G.Song,M.Zhang,J.Liu,Y.Zhou,andJ.Yan.2019.Towardsflops-constrainedfacerecognition.InProceedings
oftheIEEEInternationalConferenceonComputerVisionWorkshops.2698–2702.
[296] L.Yue,X.Miao,P.Wang,B.Zhang,X.Zhen,andX.Cao.2018.AttentionalAlignmentNetworks.InProceedingsofthe
BritishMachineVisionConference,Vol.2.6–13.
[297] z.Liu,p.Luo,x.Wang,andX.Tang.2015. Deeplearningfaceattributesinthewild.InProceedingsoftheIEEE
InternationalConferenceonComputerVision.3730–3738.
[298] S.Zafeiriou,G.Trigeorgis,G.Chrysos,J.Deng,andJi.Shen.2017.TheMenpoFacialLandmarkLocalisationChallenge:
AStepTowardstheSolution.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognitionW.
2116–2125.
[299] S.Zafeiriou,C.Zhang,andZ.Zhang.2015.Asurveyonfacedetectioninthewild:Past,presentandfuture.Computer
VisionandImageUnderstanding138(2015),1–24.
[300] TimothyZee,GeetaGali,andIfeomaNwogu.2019.Enhancinghumanfacerecognitionwithaninterpretableneural
network.InProceedingsoftheIEEEInternationalConferenceonComputerVisionWorkshops.514–522.
[301] D.Zeng,H.Liu,F.Zhao,S.Ge,W.Shen,andZ.Zhang.2019. Proposalpyramidnetworksforfastfacedetection.
InformationSciences495(2019),136–149.
[302] X.Zhan,Z.Liu,J.Yan,D.Lin,andC.C.Loy.2018.Consensus-DrivenPropagationinMassiveUnlabeledDatafor
FaceRecognition.InProceedingsoftheEuropeanConferenceonComputerVision.568–583.
[303] B.Zhang,J.Li,Y.Wang,Y.Tai,C.Wang,J.Li,F.Huang,Y.Xia,W.Pei,andR.Ji.2020.ASFD:AutomaticandScalable
FaceDetector.arXiv:2003.11228
[304] C.Zhang,X.Xu,andD.Tu.2018.Facedetectionusingimprovedfasterrcnn.(2018).arXiv:1802.02142
[305] C.ZhangandZ.Zhang.2014.Improvingmultiviewfacedetectionwithmulti-taskdeepconvolutionalneuralnetworks.
InIEEEWinterConferenceonApplicationsofComputerVision.1036–1041.
[306] D.Zhang,L.Lin,T.Chen,X.Wu,W.Tan,andE.Izquierdo.2017.Content-AdaptiveSketchPortraitGenerationby
DecompositionalRepresentationLearning.Trans.ImageProcess.26(2017),328–339.
[307] F.Zhang.2006.Facerecognitionfromasingleimageperperson:Asurvey.PatternRecognition39,9(2006),1725–1745.
[308] F.Zhang,X.Fan,G.Ai,J.Song,Y.Qin,andJ.Wu.2019. AccurateFaceDetectionforHighPerformance. (2019).
arXiv:1905.01585
[309] G.Zhang,H.Han,S.Shan,X.Song,andX.Chen.2018.FaceAlignmentacrossLargePoseviaMT-CNNBased3D
ShapeReconstruction.InProceedingsoftheIEEEInternationalConferenceonAutomaticFace&GestureRecognition.
210–217.
[310] H.Zhang,B.S.Riggan,S.Hu,N.J.Short,andV.M.Patel.2019. SynthesisofHigh-QualityVisibleFacesfrom
PolarimetricThermalFacesusingGenerativeAdversarialNetworks.InternationalJournalofComputerVision12741
(2019),845–862.
[311] J.ZhangandH.Hu.2019. StackedHourglassNetworkJointwithSalientRegionAttentionRefinementforFace
Alignment.InProceedingsoftheIEEEInternationalConferenceonAutomaticFace&GestureRecognition.1–7.
[312] J.Zhang,S.Shan,M.Kan,andX.Chen.2014.Coarse-to-fineauto-encodernetworks(cfan)forreal-timefacealignment.
InProceedingsoftheEuropeanConferenceonComputerVision.1–16.
[313] J.Zhang,X.Wu,S.C.Hoi,andJ.Zhu.2020. Featureagglomerationnetworksforsinglestagefacedetection.
Neurocomputing380(2020),180–189.
[314] K.Zhang,Z.Zhang,Z.Li,andY.Qiao.2016.Jointfacedetectionandalignmentusingmultitaskcascadedconvolutional
networks.IEEESignalProcessingLetters23,10(2016),1499–1503.
[315] L.Zhang,L.Lin,X.Wu,S.Ding,andL.Zhang.2015.End-to-EndPhoto-SketchGenerationviaFullyConvolutional
RepresentationLearning.InProceedingsoftheACMonInternationalConferenceonMultimediaRetrieval.627–634.
[316] M.Zhang,Y.Li,N.Wang,Y.Chi,andX.Gao.2020.CascadedFaceSketchSynthesisUnderVariousIlluminations.
IEEETransactionsonImageProcessing29(2020),1507–1521.
[317] M.Zhang,R.Wang,X.Gao,J.Li,andD.Tao.2019.Dual-TransferFaceSketch–PhotoSynthesis.Trans.ImageProcess.
28(2019),642–657.
[318] S.Zhang,C.Chi,Z.Lei,andS.Z.Li.2020.Refineface:Refinementneuralnetworkforhighperformancefacedetection.
IEEETrans.PatternAnal.Mach.Intell.(2020).
[319] S.Zhang,L.Wen,X.Bian,Z.Lei,andS.Z.Li.2018.Single-ShotRefinementNeuralNetworkforObjectDetection.In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.4203–4212.
[320] S.Zhang,R.Zhu,X.Wang,H.Shi,F.Fu,S.Wang,T.Mei,andS.Z.Li.2019.ImprovedSelectiveRefinementNetwork
forFaceDetection.(2019).arXiv:1901.06651
[321] S.Zhang,X.Zhu,Z.Lei,H.Shi,X.Wang,andS.Z.Li.2017. FaceBoxes:aCPUreal-timefacedetectorwithhigh
accuracy.InProceedingsoftheIEEEInternationalJointConferenceonBiometrics.1–9.
[322] S.Zhang,X.Zhu,Z.Lei,H.Shi,X.Wang,andS.Z.Li.2017. S3FD:singleshotscale-Invariantfacedetector.In
ProceedingsoftheIEEEInternationalConferenceonComputerVision.192–201.
[323] S.Zhang,X.Zhu,Z.Lei,X.Wang,andStanZLi.2018.Detectingfacewithdenselyconnectedfaceproposalnetwork.
Neurocomputing284(2018),119–127.
[324] W.Zhang,X.Wang,andX.Tang.2011.Coupledinformation-theoreticencodingforfacephoto-sketchrecognition.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.513–520.
[325] X.Zhang,Z.Fang,Y.Wen,Z.Li,andY.Qiao.2017.RangeLossforDeepFaceRecognitionwithLong-TailedTraining
Data.InProceedingsoftheIEEEInternationalConferenceonComputerVision.5419–5428.
[326] X.Zhang,R.Zhao,Y.Qiao,X.Wang,andH.Li.2019. AdaCos:AdaptivelyScalingCosineLogitsforEffectively
LearningDeepFaceRepresentations.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition.10823–10832.
[327] X.Zhang,R.Zhao,J.Yan,M.Gao,Y.Qiao,X.Wang,andH.Li.2019. P2SGrad:RefinedGradientsforOptimizing
DeepFaceModels.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.9898–9906.
[328] X.Zhang,X.Zhou,M.Lin,andJ.Sun.2018.ShuffleNet:AnExtremelyEfficientConvolutionalNeuralNetworkfor
MobileDevices.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.6848–6856.
[329] Y.Zhang,W.Deng,M.Wang,J.Hu,X.Li,D.Zhao,andD.Wen.2020.Global-LocalGCN:Large-ScaleLabelNoise
CleansingforFaceRecognition.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
7728–7737.
[330] Y.Zhang,X.Xu,andX.Liu.2019.RobustandHighPerformanceFaceDetector.arXiv:1901.02350
[331] Z.Zhang,L.Ping,C.L.Chen,andX.Tang.2014. FacialLandmarkDetectionbyDeepMulti-taskLearning.In
ProceedingsoftheEuropeanConferenceonComputerVision.94–108.
[332] H.Zhao,X.Ying,Y.Shi,X.Tong,J.Wen,andH.Zha.2020.RDCFace:RadialDistortionCorrectionforFaceRecognition.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.7721–7730.
[333] J.Zhao,Y.Cheng,Y.P.Cheng,Y.Yang,H.Lan,F.Zhao,L.Xiong,Y.Xu,J.Li,S.Pranata,S.Shen,J.Xing,H.Liu,S.
Yan,andJ.Feng.2019.LookAcrossElapse:DisentangledRepresentationLearningandPhotorealisticCross-AgeFace
SynthesisforAge-InvariantFaceRecognition.InProceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.33.
9251–9258.
[334] J.Zhao,Y.Cheng,Y.Xu,L.Xiong,J.Li,F.Zhao,K.Jayashree,S.Pranata,S.Shen,J.Xing,S.Yan,andJ.Feng.2018.
TowardsPoseInvariantFaceRecognitionintheWild.InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition.2207–2216.
[335] J.Zhao,L.Xiong,J.Karlekar,J.Li,F.Zhao,Z.Wang,S.Pranata,S.Shen,S.Yan,andJ.Feng.2017.Dual-AgentGANs
forPhotorealisticandIdentityPreservingProfileFaceSynthesis.InAdvancesinneuralinformationprocessingsystems.
65–75.42
[336] K.Zhao,K.Xu,andM.Cheng.2019.RegularFace:DeepFaceRecognitionviaExclusiveRegularization.InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.1136–1144.
[337] T.ZhengandW.Deng.2018.Cross-poselfw:Adatabaseforstudyingcrossposefacerecognitioninunconstrained
environments.BeijingUniversityofPostsandTelecommunications,Tech.Rep(2018),18–01.
[338] T.Zheng,W.Deng,andJ.Hu.2017.Cross-agelfw:Adatabaseforstudyingcross-agefacerecognitioninunconstrained
environments.(2017).arXiv:1708.08197
[339] Y.Zheng,D.K.Pal,andM.Savvides.2018.Ringloss:Convexfeaturenormalizationforfacerecognition.InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.5089–5097.
[340] Y.Zhong,J.Chen,andB.Huang.2017.TowardEnd-to-EndFaceRecognitionThroughAlignmentLearning.IEEE
Singalprocessingletters24,8(2017),1213–1217.
[341] YaoyaoZhongandWeihongDeng.2018.DeepDifferenceAnalysisinSimilar-lookingFacerecognition.InProceedings
oftheInternationalConferenceonPatternRecognition(ICPR).IEEE,3353–3358.
[342] Y.Zhong,W.Deng,M.Wang,J.Hu,J.Peng,X.Tao,andY.Huang.2019.Unequal-TrainingforDeepFaceRecognition
WithLong-TailedNoisyData.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
7804–7813.
[343] E.Zhou,Z.Cao,andJ.Sun.2018. Gridface:Facerectificationvialearninglocalhomographytransformations.In
ProceedingsoftheEuropeanConferenceonComputerVision.3–19.
[344] E.Zhou,H.Fan,Z.Cao,Y.Jiang,andQ.Yin.2013. ExtensiveFacialLandmarkLocalizationwithCoarse-to-Fine
ConvolutionalNetworkCascade.InProceedingsoftheIEEEInternationalConferenceonComputerVisionWorkshops.
386–391.
[345] S.K.ZhouandD.Comaniciu.2007.Shaperegressionmachine.InBiennialInternationalConferenceonInformation
ProcessinginMedicalImaging.13–25.
[346] C.Zhu,Y.He,andM.Savvides.2019. FeatureSelectiveAnchor-FreeModuleforSingle-ShotObjectDetection.In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.840–849.
[347] C.Zhu,R.Tao,K.Luu,andM.Savvides.2018.Seeingsmallfacesfromrobustanchor’sperspective.InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition.5127–5136.
[348] C.Zhu,Y.Zheng,K.Luu,andM.Savvides.2017.Cms-rcnn:contextualmulti-scaleregion-basedcnnforunconstrained
facedetection.InDeeplearningforbiometrics.57–79.
[349] J.Zhu,T.Park,P.Isola,andA.A.Efros.2017. UnpairedImage-to-ImageTranslationUsingCycle-Consistent
AdversarialNetworks.InProceedingsoftheIEEEInternationalConferenceonComputerVision.2242–2251.
[350] M.Zhu,F.Shi,M.Zheng,andM.Sadiq.2019.Robustfaciallandmarkdetectionviaocclusion-adaptivedeepnetworks.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.3486–3496.
[351] X.Zhu,Z.Lei,X.Liu,H.Shi,andS.Z.Li.2016.FaceAlignmentAcrossLargePoses:A3DSolution.InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition.146–155.
[352] X.Zhu,H.Liu,Z.Lei,H.Shi,F.Yang,D.Yi,G.Qi,andS.Z.Li.2019.Large-scalebisamplelearningonidversusspot
facerecognition.InternationalJournalofComputerVision127,6-7(2019),684–700.
[353] X.ZhuandD.Ramanan.2012.Facedetection,poseestimation,andlandmarklocalizationinthewild.InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2879–2886.
[354] ZhengZhu,GuanHuang,JiankangDeng,YunYe,JunjieHuang,XinzeChen,JiagangZhu,TianYang,JiwenLu,
DalongDu,andJieZhou.2021. WebFace260M:ABenchmarkUnveilingthePowerofMillion-ScaleDeepFace
Recognition.ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(2021),10487–10497.
[355] C.Zhuang,S.Zhang,X.Zhu,Z.Lei,J.Wang,andS.Z.Li.2019.FLDet:ACPUReal-timeJointFaceandLandmark
Detector.InProceedingsoftheInternationalConferenceonBiometrics.1–8.
[356] X.Zou,J.Kittler,andK.Messer.2007.IlluminationInvariantFaceRecognition:ASurvey.InProceedingsoftheIEEE
InternationalConferenceonBiometrics:Theory,Applications,andSystems.1–8.
[357] X.Zou,S.Zhong,L.Yan,X.Zhao,J.Zhou,andY.Wu.2019. LearningRobustFacialLandmarkDetectionvia
HierarchicalStructuredEnsemble.InProceedingsoftheIEEEInternationalConferenceonComputerVision.141–150.
[358] O.Çeliktutan,S.Ulukaya,andB.Sankur.2013.Acomparativestudyoffacelandmarkingtechniques.EURASIPJournal
onImageandVideoProcessing2013(2013),1–27.43
A REPRESENTATIVESURVEYSOFFACERECOGNITION
Anumberoffacerecognitionsurveyshavebeenpublishedinthepasttwentyyears.Wesummarize
theminTable12.
Table12. Representativesurveysoffacerecognition
Title Year Description
FaceRecognition:ALiteratureSurvey[231] 2003 Traditional image- and video-based methods in face
recognition.Notcoveringdeepfacerecognition.
FaceRecognitionfromaSingleImageperPerson:ASurvey[307] 2006 Themethodstoaddressthesinglesampleprobleminface
recognition,notcoveringdeepfacerecognition.
ASurveyofApproachesandChallengesin3DandMulti-modal3D+2D 2006 Asurveyof3Dandmulti-modalfacerecognition,not
FaceRecognition[16] coveringdeepfacerecognition.
IlluminationInvariantFaceRecognition:ASurvey[356] 2007 Focusonillumination-invariantfacerecognitiontask,not
coveringdeepfacerecognition.
ASurveyofFaceRecognitionTechniques[7] 2009 Traditionalfacerecognitionmethodsondifferentmodal
facedata,notcoveringdeepfacerecognition.
AComprehensiveSurveyonPose-InvariantFaceRecognition[49] 2016 Focusonpose-invariantfacerecognitiontask.
ASurveyofLocalFeatureMethodsfor3DFaceRecognition[201] 2017 Areviewoffeatureextractionbasedmethodsfor3Dface
recognition.
DeepLearningforUnderstandingFaces[177] 2018 Provideabriefoverviewoftheend-to-enddeepface
recognition,notcoveringtherecentworks.
DeepFaceRecognition:ASurvey[244] 2018 Focusonthedeepfacerepresentationlearning.
Past,Present,andFutureofFaceRecognition:AReview[2] 2020 Areviewof2Dand3Dfacerecognition,notcovering
end-to-enddeepfacerecognition.
B FACEDETECTION
B.1 Single-stageandmulti-stagefacedetectors
Fig.9illustratesthedifferencebetweensingle-stageandmulti-stagefacedetectors.Forcomparison,
thesingle-stagefacedetectoraccomplishesthedetectionprocessingdirectlyfromthefeaturemaps,
whereasthemulti-stagefacedetectoradoptsaproposalstagetogeneratecandidatesandoneor
morestagestorefinethesecandidates.
Fig.9. Theillustrationofsingle-stageandmulti-stagefacedetectors.Thesingle-stagedetectoraccomplishes
thefacedetectiondirectlyfromthefeaturemaps,whereasthemulti-stagedetectoradoptsaproposalstage
togeneratecandidatesandoneormorestagestorefinethesecandidates.
B.2 PerformancecomparisonofCPUreal-timefacedetectionmethods
Table13showstherunningefficiencyofCPUreal-timefacedetectionmethods,amongwhich
thelightweightbackbone[41,279],rapidlydigestedconvolutionallayer[321,323],knowledge
distillation[98]andregion-of-interest(RoI)convolution[27]arethecommonpractices.44
Table13. RunningefficiencyofCPUreal-timefacedetectors.“Accuracy(%)”denotesthetruepositiverateat
1000falsepositivesonFDDB.
Method Publication CPU-model Speed(FPS) Accuracy(%)
Faceboxes[321] IJCB’17 E5-2660v3@2.60GHz 20 96.0
STN[27] ECCV’16 i7-4770K 30 -
DCFPN[323] Neurocomputing’18 2.60GHz 30 -
FBI[98] ICB’19 E5-2660v3@2.60GHz 20 96.8
PCN[193] CVPR’18 3.40GHz 29 -
PPN[301] InformationSciences’19 i5 60 -
RetinaFace[41] CVPR’19 i7-6700K 60 -
CenterFace[279] arXiv’19 i7-6700@2.60GHz 30 98.0
C FACEALIGNMENT
C.1 Hourglassnetworkforfaciallandmarklocalization
Hourglass[168]isabottom-upandtop-downarchitecture,playinganimportantroleinthedeep
stackofbottleneckblocksalongwithintermediatesupervision.Fig.10isanillustrationofstacked
hourglassnetwork.
Fig.10. Anillustrationofstackedhourglassnetwork[168]forfaciallandmarklocalization.Ineachhourglass
structure,thewidth(i.e.,featurechannels)isconsistent,andtheboxesrepresenttheresidualmodules.
C.2 3Dmodelfittingforfaciallandmarklocalization
AsillustratedinFig.11,some3Dmodelfittingbasedmethodsemploycascadedregressionmanner
withadense3DMorphableModel(3DMM)[15]toestimatethe3Dfaceshape.
C.3 Landmark-freefacealignment
Landmark-freefacealignmentmethodsintegratethealignmenttransformationprocessinginto
DCNNsandoutputalignedfacewithoutrelyingonfaciallandmarks(Fig.12).
D FACEREPRESENTATION
Fig.13showsthepipelineoffacerepresentationtrainingphaseandtestphase.Inthetraining
phase,twotypesoftrainingsupervisionarewidelyused,i.e.,classificationandfeatureembedding.
Asfortestphase,therearetwomajortasks,i.e.,faceverificationorfaceidentification.
Inaddition,asshownFig.14,wecanobservethatthepublicationsofclassificationbasedtraining
supervisionexceedthoseofthefeatureembeddingandhybridmethodswiththegrowingscale
ofavailablefacedata.Thereasonisthattheclosed-setclassificationtrainingonthelarge-scale
datasetsenablestoapproachopen-setfacerecognitionscenario.45
Fig.11. Theprocessof3Dmodelfittingforfacealignment.Adense3DMorphableModelisusedtomodela
2Dfaceto3Dmesh.Theregressionnetworkestimatestheparametersof3Dshapeandprojectionmatrix,
andthenthe3Dshapeisprojectedontotheimageplanetoobtainthe2Dlandmarks.
Fig.12. Anillustrationofintegratedframeworkthataccomplisheslandmark-freefacealignmentandrepre-
sentationcomputation.
Fig.13. Thepipelineoffacerepresentationtrainingphaseandtestphase.Inthetrainingphase,twoschemes,
i.e.,classificationandfeatureembedding,areoftenusedforlearningfacerepresentation.Inthetestphase,
faceverificationandfaceidentificationarethemajortasks.
Fig.14. Thepublicationtrendofthreesupervisedfacerepresentationlearningschemeswiththegrowing
scaleofavailablefacedatasetsfrom2014-2020."
219,221,The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression,"['P Lucey', 'JF Cohn', 'T Kanade', 'J Saragih']",2010,5135,Extended Cohn-Kanade,"classification, classifier, deep learning, machine learning, neural network","It involves computer vision, machine learning and behavioral sciences, and can be used  for many applications such as security [20], human-computer-interaction [23], driver safety [24],",No DOI,2010 ieee computer …,https://ieeexplore.ieee.org/document/5543262,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
220,222,The first facial expression recognition and analysis challenge,"['MF Valstar', 'B Jiang', 'M Mehu', 'M Pantic']",2011,452,Affective Faces Database,facial expression recognition,In section III we describe the challenge protocol for both the AU detection and emotion detection  sub-challenges. Section IV then describes the baseline method and the baseline results,No DOI,… & gesture recognition …,https://ieeexplore.ieee.org/document/5771374,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
221,223,The role of the amygdala in atypical gaze on emotional faces in autism spectrum disorders,"['D Kliemann', 'I Dziobek', 'A Hatri', 'J Baudewig']",2012,237,Karolinska Directed Emotional Faces,classification,"gaze in ASD we applied a facial emotion classification task, using eye tracking during  fMRI, varying the initial fixation position on faces. We hypothesized that individuals with ASD",No DOI,Journal of …,https://pubmed.ncbi.nlm.nih.gov/22787032/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,nih.gov,
222,224,Three convolutional neural network models for facial expression recognition in the wild,"['J Shao', 'Y Qian']",2019,181,"Acted Facial Expressions In The Wild, Expression in-the-Wild, Static Facial Expression in the Wild","CNN, FER, classification, deep learning, facial expression recognition, machine learning, neural network",Facial expressions in the wild have hundreds of thousands of variations referring to different   expressions are relatively easier to be recognized from an acted face than from a face in the,No DOI,Neurocomputing,https://www.sciencedirect.com/science/article/pii/S0925231219306137,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,sciencedirect.com,
223,225,Tied factor analysis for face recognition across large pose differences,"['SJD Prince', 'JH Elder', 'J Warrell']",2008,272,Toronto Face Database,machine learning,in face data across different poses. Our model was applied to both face identification and   The system described here is a pure machine learning approach that knows very little about,No DOI,… machine intelligence,https://ieeexplore.ieee.org/document/4459336,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
224,226,Torontocity: Seeing the world with a million eyes,"['S Wang', 'M Bai', 'G Mattyus', 'H Chu', 'W Luo']",2016,210,Toronto Face Database,"classification, neural network",", which covers the full greater Toronto area (GTA) with 712.5 semantic labeling and scene  type classification (recognition).  , tree detection and tree species classification as well as traffic",No DOI,arXiv preprint arXiv …,https://arxiv.org/abs/1612.00423,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"TorontoCity: Seeing the World with a Million Eyes
ShenlongWang,MinBai,GellertMattyus,HangChu,WenjieLuo,BinYang,JustinLiang,JoelCheverie,SanjaFidler,RaquelUrtasun
DepartmentofComputerScience,UniversityofToronto
{slwang, mbai, mattyusg, chuhang1122, wenjie, byang, justinliang, joel, fidler, urtasun}@cs.toronto.edu
Abstract However, current large scale datasets suffer from two
shortcomings. First,theyhavebeencapturedbyasmallset
In this paper we introduce the TorontoCity benchmark, of sensors with similar perspectives of the world, e.g., in-
which covers the full greater Toronto area (GTA) with ternetphotosforImageNetorcameras/LIDARmountedon
712.5km2 of land, 8439km of road and around 400,000 topofacarinthecaseofKITTI.Second,theydonotcon-
buildings. Our benchmark provides different perspectives tainrichsemanticsand3Dinformationatalarge-scale. We
oftheworldcapturedfromairplanes,dronesandcarsdriv- referthereadertoFig.2forananalysisofexistingdatasets.
ing around the city. Manually labeling such a large scale
In this paper, we argue that the field is in need of large
datasetisinfeasible. Instead,weproposetoutilizedifferent
scale benchmarks that allow joint reasoning about geome-
sourcesofhigh-precisionmapstocreateourgroundtruth.
try, grouping and semantics. This has been commonly re-
Towards this goal, we develop algorithms that allow us to
ferredtoasthethreeR’sofcomputervision. Towardsthis
align all data sources with the maps while requiring mini-
goal,wehavecreatedtheTorontoCitybenchmark,covering
malhumansupervision.Wehavedesignedawidevarietyof thefullgreaterTorontoarea(GTA)with712.5km2ofland,
tasksincludingbuildingheightestimation(reconstruction),
8439kmofroadandaround400,000buildings. According
roadcenterlineandcurbextraction,buildinginstanceseg-
tothecensus,6.8millionpeopleliveintheGTA,whichis
mentation,buildingcontourextraction(reorganization),se-
around 20% of the population of Canada. We have gath-
manticlabelingandscenetypeclassification(recognition).
ered a wide range of views of the city: from the overhead
Ourpilotstudyshowsthatmostofthesetasksarestilldiffi-
perspective,wehaveaerialimagescapturedduringfourdif-
cultformodernconvolutionalneuralnetworks.
ferent years as well as LIDAR from airborne. From the
ground, we have HD panoramas as well as stereo, Velo-
dyneLIDARandGo-prodatacapturedfromamovingve-
1.Introduction
hicledrivingaroundinthecity. Wearealsoaugmentingthe
datasetwitha3Dcameraaswellasimagerycapturedfrom
”Itisanarrowmindwhichcannotlookatasubject
drones.
fromvariouspointsofview.”
Manuallylabelingsuchalargescaledatasetisnotfeasi-
GeorgeEliot,Middlemarch
ble. Instead,weproposetoutilizedifferentsourcesofhigh-
precisionmapstocreateourgroundtruth. Comparedtoon-
In recent times, a great deal of effort has been devoted
line map services such as OpenStreetMap [1] and Google
to creating large scale benchmarks. These have been in-
Maps,ourmapsaremuchmoreaccurateandcontainricher
strumental to the development of the field, and have en-
meta-data, which we exploit to create a wide variety of
abledmanysignificantbreak-throughs. ImageNet[9]made
diverse benchmarks. This includes tasks such as build-
it possible to train large-scale convolutional neural net-
ing height estimation (reconstruction), road centerline and
works, initiating the deep learning revolution in computer
curb extraction, building instance segmentation, building
vision in 2012 with SuperVision (most commonly refer as
contour extraction (reorganization), semantic labeling and
AlexNet[16]). EffortssuchasPASCAL[11]andMicrosoft
scene type classfication (recognition). Participants can ex-
COCO [18] have pushed the performance of segmentation
ploitanysubsetofthedata(e.g.,aerialandgroundimages)
and object detection approaches to previously inconceiv-
tosolvethesetasks.
ablelevels. Similarly,benchmarkssuchasKITTI[12]and
Cityscapes [8] have shown that visual perception is going OneofthemainchallengesincreatingTorontoCitywas
tobeanimportantcomponentofadvanceddriverassistance aligning the maps to all data sources such that the maps
systems (ADAS) and self-driving cars in the imminent fu- can produce accurate ground truth. While the aerial data
ture. wasperfectlyaligned,thisisnotthecaseofthepanoramas
1
6102
ceD
1
]VC.sc[
1v32400.2161:viXraataD
spaM
sksaT
Figure 1: Summary of the TorontoCity benchmark. Data source: aerial RGB image, streetview panorama, GoPro, stereo
image,street-viewLIDAR,airborneLIDAR;Maps: buildingsandroads,3Dbuildings,propertymeta-data;Tasks: semantic
segmentation,buildingheightestimation,instancesegmentation,roadtopology,zoningsegmentationandclassification.
wheregeolocalizationisfairlynoisy. Toalleviatethisprob- 2.RelatedWork
lem,wehavecreatedasetoftoolswhichallowustoreduce
the labelingtask toa simple verificationprocess, speeding Automatic mapping, reconstruction and semantic label-
uplabeling,thusmakingTorontoCitypossible. ing from urban scenes have been an important topic for
manydecades. Severalbenchmarkshavebeenproposedto
tackle subsets of these tasks. KITTI [12] is composed of
We perform a pilot study using the aerial images cap-
stereoimagesandLIDARdatacollectedfromamovingve-
turedin2011aswellasthegroundpanoramas. Ourexperi-
hicle, and evaluates SLAM, optical flow, stereo and road
mentsshowthatstate-of-the-artmethodsworkwellonsome
segmentationtasks. Cityscapes[8]focusesonsemanticand
tasks, such as semantic segmentation and scene classifica-
instanceannotationsofimagescapturedfromacar. Aerial-
tion.However,taskssuchasinstancesegmentation,contour
KITTI[21]augmentstheKITTIdatasetwithaerialimagery
extractionandheightestimationremainanopenchallenge.
ofasubsetofKarlsruhetoencouragereasoningofseman-
Webelieveourbenchmarkprovidesagreatplatformforde-
ticsfrombothgroundandbird’seyeview.
veloping and evaluating new ideas, particularly techniques
thatcanleveragedifferentviewpointsoftheworld.Weplan The photometry community has developed several
to extend the current set of benchmarks in the near future benchmarkstowardsurbansceneunderstanding[15,24,30,
with tasks such as building reconstruction, facade parsing, 25, 20]. TUM-DLR [15] and ISPRS Multi-Platform [24]
treedetectionandtreespeciesclassificationaswellastraffic benchmarkscontainimagerycapturedthroughmultipleper-
lightsandtrafficsigndetection,forwhichourmapsprovide spectivesfromUAV,satelliteimagesandhandheldcameras.
accurate ground truth. We have only scratched the surface OxfordRobotCarcontainslidarpointcloudandstereoim-
ofTorontoCity’sfullpotential. ages captured from a vehicle [20]. However, these bench-0.2 0.14 Institutional
Utility
0.12
0.15
0.1
ycneuqerF
0.1
ycneuqerF00 .. 00 68
Residential Employment
0.04
0.05 Others
0.02
0 0 Open Space
0 4 8 12 16 >20 0 200 400 600 800 >1000 Commercial
Building Height (m) Building Area Size (m2)
Dataset ISPRS TUM-DLR AerialKITTI KITTI RobotCar Ours
Location Vaihingen/Toronto Munich Karlsruhe Karlsruhe Oxford Toronto
AerialCoverage(km2) 3.49+1.45 8.32 3.23 - - 712
GroundCoverage(km) - <1 <20 39.2 10 >1000(pano)1
AerialRGB yes yes yes - - yes
DroneRGB - yes - - - yes
AerialLIDAR yes yes - - - yes
GroundPerspective - - - yes yes yes
GroundPanorama - - - - yes yes
GroundStereo - yes - yes yes yes
GroundLIDAR - yes - yes yes yes
AerialResolution(pixel/cm2) 8 50 9 - - 10
Repeats - - - partial x10 x4(aerial)
TopSemanticGT(#ofclasses) 100%(8) - 100%(4) - - 100%(2+8)
TopGeometricGT(source) dense(lidar) dense(lidar) - - - dense(map+lidar)
GroundSemanticGT(#ofclasses) - - dense(4) object(3) - dense(2)/image(6)
GroundGeometricGT(source) - - - sparse(lidar) sparse(lidar) dense(map+lidar)
Figure2: Statisticsofourdataandcomparisonofcurrentstate-of-the-arturbanbenchmarksanddatasets.
marks do not offer any semantic ground-truth for bench-
markingpurposes. Perhapsthemostcloselyrelateddataset
to ours is the ISPRS Urban classification and building re-
constructionbenchmark[30],wherethetaskistoextractur-
banobject,suchasbuilding,roadandtreesfrombothaerial
images and airborne laserscanner point clouds. However,
thisdatasethasarelativelysmallcoverageanddoesnotpro-
videground-viewimagery.Incontrast,TorontoCityismore
thantwoordersofmagnitudebigger.Furthermore,weoffer
manydifferentperspectivesthroughvarioussensors,along Figure 3: Road surface generation: (left) input data with
with diverse semantic and geometric benchmarks with ac- curbs (yellow) and center lines (red). Extracted road sur-
curate ground-truth. The readers may refer to Fig. 2 for a faceistheunionofpolygonsshowninblueandblack.Note
detailedcomparisonagainstpreviousdatasets. that a formulation ensuring connectivity is needed, other-
wisetheroadsurfacewouldcontainholesatintersections.
A popular alternative is to use synthetic data to gener-
ate large scale benchmarks [3, 5, 26, 29, 31, 13, 6, 28].
marks. Thisismainlyduetoboththelackofhigh-fidelity
Through 3D synthetic scenes and photo-realistic renderers
maps to provide pixel-level annotation and the lack of ac-
large-scale datasets can be easily created. To date, how-
curately georeferenced imagery that aligned well with the
ever, these datasets have been focused on a single view of
maps. Oneexceptionis[35],wherethestreetreecatalogis
theworld. ThiscontrastsTorontoCity. Unlikeotherbench-
used to generate ground-truth for tree detection. [36] uti-
marks, ourinputisreal-worldimagery, andthelarge-scale
lizes3Dbuildingmodelstogeneratecorrespondencesfrom
3D models are a high-fidelity modeling of the real world
multiple streetview images. In this paper, we use maps to
ratherthanasyntheticscene.
createmultiplebenchmarksforreconstruction, recognition
Maps have been proven useful for many computer vi-
andreorganizationfrommanydifferentviewsoftheworld.
sionandroboticsapplications[34,23,22,35,21],including
vehicledetectionandposeestimation[23],semanticlabel-
3.TorontoCityataGlimpse
ing and monocular depth estimation [34] as well as HD-
map extraction [21]. However, there has been a lack of TorontoCityisanextremelylargedatasetenablingwork
literaturethatexploitmapsasground-truthtobuildbench- on many exciting new tasks. We first describe the data in(a)NCC:beforevs.after (b)Overlay:beforevs.after (c)Location:beforevs.after
Figure4: Ground-aerialalignment
(a)Input (b)GT (c)ResNet56 (d)Input (e)GT (f)ResNet56
Figure5: Examplesofaerialsemanticsegmentation,roadcurbextraction,androadcenterlineestimation.
detail. In the next section we describe our efforts to sim- Panoramas: WedownloadedGoogleStreetviewpanora-
plifythelabelingtask,asotherwiseitisinfeasibletocreate mas [2] that densely populate the GTA. On average, we
suchalarge-scaledataset.Wethenshowthechallengesand crawledaround520full360◦sphericalpanoramasforeach
metricsthatwillcomposethebenchmark. Finally,weper- km2. In addition, wecrawled the associated metadata, in-
form a pilot study of how current algorithms perform on cluding the geolocation, address and the parameters of the
mosttasks,andanalyzetheremainingchallenges. spherical projection, including pitch, yaw and tilt angles.
Weresizedallpanoramasto3200×1600pixels.
3.1.Dataset
AerialImagery: Weuseaerialimageswithfullcoverage
Toronto is the largest city in Canada, and the fourth
oftheGTAtakenin2009,2011,2012and2013. Theyare
largest in North America. The TorontoCity dataset covers
orthorectified to 10cm/pixel resolution for 2009 and 2011,
thegreaterTorontoarea(GTA),whichcontains712.5km2
and 5 and 8cm/pixel for 2012 and 2013 respectively. This
of land, 8439km of road and around 400,000 buildings.
contrasts satellite images, which are at best 50cm/pixel.
Accordingtothecensus6.8millionpeopleliveintheGTA,
Our aerial images have four channels (i.e., RGB and Near
whichisaround20%ofthepopulationofCanada.
infrared), and are 16 bit resolution for 2011 and 8 bit for
Wehavegatheredawiderangeofviewsofthecity:from the rest. As is common practice in remote sensing [4], we
the overhead perspective, we have aerial images captured projectedeachimagetotheUniversalTransverseMercator
during four different years (containing several seasons) as (UTM)17zoneintheWGS84geodeticdatumandtiledthe
wellasairborneLIDAR.Fromthegroundperspective, we areato500×500m2imageswithoutoverlap. Notethatthe
have HD panoramas as well as stereo, Velodyne LIDAR imagesarenottrueorthophotosandthusfacadesarevisible.
and Go-pro data captured from a moving vehicle driving
aroundthecity. Inaddition,weareaugmentingthedataset
witha3Dcameraaswellasimagerycapturedfromdrones. AirborneLIDAR: WealsoexploitairborneLIDARdata
Fig. 1 depicts some of the data sources that compose our capturedin2008withaLeicaALSsensorwitharesolution
dataset. We now describe the data in more details and re- of 6.8 points per m2. The total coverage is 22 km2. All
fer the reader to Fig. 2 for a comparison against existing of the points are also geo-referenced and projected to the
datasets. UTM17ZoneinWGS84geodeticdatum.(a)Input (b)GT (c)ResNet56 (d)Input (e)GT (f)ResNet56
Figure6: Examplesofbuildinginstancesegmentation.
Car setting: Our recording platform includes a GoPro Roads: Our maps contain very accurate polylines repre-
Hero 4 RGB camera, a Velodyne HDL-64E LIDAR and a senting streets, sidewalks, rivers and railways within the
PointGray Bumblebee3 Stereo Camera mounted on top of GTA. Each line segment is described with a series of at-
thevehicle. Allthesensorsarecalibratedandsynchronized tributes such as name, road category and address number
withaApplanixPOSLVpositioningsystemtorecordreal- range. Road intersections are explicitly encoded as inter-
time geo-location and orientation information. We have sectingpointsbetweenpolylines.Roadcurbsarealsoavail-
driven this platform for around 90km, which includes re- able,anddescribetheshapeofroads(seeFig.1).
peats of the same area. Note that we are collecting and
aligning new data from ground-view vehicles, and plan to
UrbanZoning: Ourmapscontaingovernmentzoningin-
haveamuchlargercoveragebythetimeofrelease.
formationonthedivisionoflandintocategories. Thiszon-
ingincludescategoriessuchasresidential,commercial,in-
3.2.MapsasAnnotations dustrialandinstitutional. Notethatmultiplecategoriesare
allowedforonezone,e.g.,commercial+residential. Under-
Manually labeling such a large scale dataset as Toron- standing urban zoning is important in applications such as
toCityissimplynotpossible. Instead,inthispaperweex- urbanplanning,realestateandlaw-enforcement.
ploitdifferentsourcesofhigh-precisionmapscoveringthe
wholeGTAtocreateourgroundtruth. Comparedtoonline
Additionaldata: Wealsohavecartographicinformation
mapservicessuchasOpenStreetMap[1]andGoogleMaps,
with full coverage of the GTA. For instance, we have the
ourmapsaremuchmoreaccurate. Furthermore,theycon-
locationofallthepoles,trafficlights,streetlightsandstreet
tainmanyadditionalsourcesofdetailedmetadatawhichwe
treeswithmeta-informationforeach.Themeta-information
exploit. OneofthemainchallengesincreatingTorontoCity
includestheheightofthepole/trafficlight,typeofmodelof
was the alignment of the maps to all data sources. In the
eachstreetlights,trunkradiusandspeciesofeachtrees.We
following, we first describe the annotated data composing
plantoexploitthisinthenearfuture.
TorontoCityandpostponeourdiscussiononthealgorithms
wedevelopedtoalignalldatasourcestothenextsection.
4.MapsforCreatingLargeScaleBenchmarks
In this section we describe our algorithms to automat-
ically align maps with our sources of imagery. We then
Buildings: TheTorontoCitydatasetcontains400,0003D
describethealignmentofthedifferentroadmaps.
buildings covering the full GTA. As shown in Fig. 2, the
buildings are very diverse, with the tallest being the CN
4.1.AligningMapswithAllDataSources
Tower with 443m of elevation. Toronto contains many in-
dividualfamilyhouses,whichmakestaskssuchasinstance The aerial images we employed are already perfectly
levelsegmentationparticularlydifficult.Themeanheightof aligned with the maps. This, however, is not the case for
eachbuildingis4.7m,andthemeanbuildingareais148m2. thepanoramas. Asnotedin[7],thegeolocalizaitonerroris
In contrast, the largest building has an area 120,000m2. up to 5m with an average of 1.5m, while rotation is very
Thelevelofdetailofthe3Dmodelsvariesperbuilding(see accurate. As a consequence, projecting our maps will not
Fig.1). Manyofthesebuildingsareaccuratetowithincen- generategoodgroundtruthduetothelargemisalignments
timeters and contain many other semantic primitives such asshowninFig.4.Tohandlethisissue,wedesignanalign-
asrooftypes,windowsandbalconies. ment algorithm that exploits both aerial images and maps.Figure7: Qualitativeresultsonbuildingstructuredcontourprediction: ResNetvsGT
Method Mean Building Road
FCN[19] 77.64% 70.44% 73.32% show our procedure of exploiting a Markov random field
ResNet[14] 78.46% 69.15% 76.44% (MRF) to align road centerlines and curves. We can then
Table1: AerialimagesemanticsegmentationIoU. generatethepolygonsdescribingtheroadsurfaces. Fig. 3
Method WeightedCov AP Re-50% Pr-50% showsanexamplefortheroadsurfacegeneration.
FCN 39.74% 8.04% 19.64% 18.38%
Lety ∈{0,1,··· ,k}betheassignmentofthei-thcurb
FCN+Open 43.19% 16.45% 24.55% 36.09% i
ResNet 38.70% 10.47% 21.30% 21.93%
segmenttooneoftheknearestcenterlinesegments,where
ResNet+Open 41.10% 22.92% 22.78% 43.78% state 0 denotes no match. We define an MRF composed
Table2: BuildinginstancesegmentationIoU. ofunaryandpairwiseterms,whichconnectsonlyadjacent
curbssegments,andthusnaturallyformasetofchains. For
Their information is complementary, as aerial images give the unary terms φ un(y i), we use the weighted sum of the
us appearance, while maps give us sparse structures (e.g., distanceofthecurvetoeachcenterlinesegment(condition
roadcurves). on the state) and the angular distance between curves and
For this, we first rectify the panoramas by projecting centerlines. Forthepairwisetermsφ con(y i,y i+1),weem-
them onto the ground-plane. We extract a 400 × 400 m ployaPottspotentialthatencouragessmoothnessalongthe
ground plane region with 10cm/pixel resolution and pa- road. Thisisimportantasotherwisetheremaybeholesin
rameterize the alignment with three degrees of freedom places such as intersections, since the center of the inter-
representing the camera’s offset. We then perform a two sectionisfurtherawayfromotherpoints. Duetothechain
step alignment process. We obtain a coarse alignment structureofthegraphicalmodel,inferencecanbedoneex-
by maximizing a scoring function that compromises be- actlyandefficientlyinparallelforeachchainusingdynamic
tween appearance matching and a regularizer. In particu- programming. Ourformulationallowsformultiplecurbsto
lar, we use normalized cross correlation (NCC) as our ap- bematchedtooneroad,whichisneededastherearecurbs
pearance matching term and a Gaussian prior with mean onbothsidesofthecenterline. Wemanuallyinspectthere-
(0,0,2.5)m and diagonal covariance (2,2,0.2)m. We re- sults and mark errors as “don’t care” regions. We convert
scale both aerial and ground images to [0,1] before NCC. each continuous curb-road center line assignment to poly-
Thesolutionspaceisadiscretesearchwindowintherange gons which gives us the final road surface. We refer the
[−10m,10m]×[−10m,10m]×[2.2m,2.6m] witha step readertoFig. 3foranexample.
of0.1m. Weuseexhaustivesearchtoperformthissearch,
and exploit the fact that NCC can be computed efficiently 5.BenchmarkTasksandMetrics
usingFFTandtheGaussianpriorscoreisafixedlook-up-
We designed a diverse set of benchmarks to push com-
table.AsshowninFig.4thisprocedureproducesverygood
puter vision approaches to reason about geometry, seman-
coarsealignments. Thealignmentiscoarseaswereasonat
tics and grouping. To our knowledge, no previous dataset
theaerialimages’resolution,whichisrelativelylower.
is able to do this at this scale. In the evaluation server,
Our fine alignment then utilizes the road curves and
participants can submit results using any subset of the im-
aligns them to the boundary edges [10] in the panorama.
agerytypesprovidedinthebenchmark(e.g.,aerialimages,
We use a search area of [−1m,1m] × [−1m,1m] with a
panoramas, Go-Pro, stereo). Inthissection, webrieflyde-
stepof5cm. Thisisfollowedbyahumanverificationpro-
scribethetasksandmetrics,andreferthereadertothesup-
cessthatselectstheimageswherethisalignmentsucceeds.
plementarymaterialforfurtherdetails. NotealsothatFig.
Mistakes in the alignment are due to occlusions (e.g., cars
1showsanillustrationofsomeofourtasks.
inthepanoramas)aswellassignificantnon-flatterrain.Our
successrateis34.35%,andittakeslessthan2stoverifyan
image. Incontrastannotatingthealignmenttakes20s. Building Footprint and Road segmentation: Our first
task is semantic segmentation of building footprints and
4.2.SemanticSegmentationfromPolylineData roads. Following common practice in semantic segmenta-
tion, we utilize mean Intersection-Over-Union (mIOU) as
Our maps provide two types of road structures: curbs
ourmetric. Thisisevaluatedfromatop-downview.
definingtheroadboundariesaswellascenterlinesdefining
theconnectivity(adjacency)inthestreetnetwork. Unfortu-
nately, thesetwosourcesarenotaligned, andoccasionally BuildingFootprintInstanceSegmentation: Oursecond
center lines are outside the road area. In this section we task is building instance segmentation. We adopt multi-Figure8: Examplesofroadsegmentation. Left: panoramicview;right: top-downview. (TP:yellow,FP:red,FN:green)
Roadcenterline Roadcurb
Method F10.5 Pr0.5 Re0.5 F12 Pr2 Re2 F10.5 Pr0.5 Re0.5 F12 Pr2 Re2
FCN 0.169 0.156 0.186 0.626 0.576 0.687 0.444 0.413 0.482 0.778 0.726 0.837
FCN+Close 0.173 0.164 0.183 0.639 0.604 0.678 0.444 0.427 0.462 0.781 0.752 0.812
ResNet 0.162 0.143 0.186 0.613 0.567 0.667 0.575 0.585 0.566 0.796 0.830 0.765
ResNet+Close 0.162 0.169 0.155 0.644 0.671 0.619 0.568 0.614 0.529 0.799 0.862 0.745
Table3: Roadcenterlineandcurbresults. Metric: F1,Precision,Recallwithminimaldistancethreshold0.5mand2m.
Method WeightedCov PolySim
plemetricsforthistask, sincethereisnoconsensusinthe FCN 0.456 0.323
community of what is the best metric. We thus evaluate ResNet 0.401 0.292
weightedcoverage(Cov),averageprecision(AP)aswellas Table5: Buildingcontourresults.
instancelevelprecisionandrecallat50%.
Method Residential OpenSpace Others
FCN 60.20% 32.20% 5.57%
ResNet 51.71% 33.63% 1.49%
Building Structured Contours: Most semantic and in-
Table6: Qualitativeresultsforurbanzoningsegmentation.
stancesegmentationalgorithmsproduce”blob”-likeresults,
whichdonotfollowthegeometryoftheroadsand/orbuild-
ings. We thus want to push the community to produce in- Building Height Estimation: This tasks consists on es-
stance segmentations that follow the structure of the prim- timating building height. Useful cues include size of the
itives. Towards this goal, we define a metric that merges buildings, pattern of shading and shadows as well as the
(inamultiplicativefashion)segmentationscoringwithge- imperfectrectificationinaerialviews. Weadoptrootmean
ometricsimilarity. Inparticular, segmentationismeasured squareerrorinthelogdomain(log-RMSE)asourmetric.
intermsofIOU,andweexploitthesimilaritybetweenthe
turning functions of the estimated and ground truth poly- AdditionalTasks: Weplantoaddmanytasksinthecom-
gonsasageometricmetric. Wereferthereadertothesup- ing months. This includes detecting trees and recognizing
plementarymaterialformoredetails. their species. Moreover, the accurate 3D building models
allowustobuildabenchmarkofnormalestimationaswell
asfacadeparsing. Wealsoplantohavebenchmarksforde-
RoadTopology: Oneoftheremainingfundamentalchal-
tection and segmentation of traffic lights, traffic signs and
lengesinmappingisestimatingroadtopology. Inthistask,
poles. Wearejustscratchingthesurfaceoftheplenthoraof
participants are asked to extract polylines that represent
possibilitieswiththisdataset.
road curbs and road centerlines in bird’s eye perspective.
Wediscretizebothestimatedandgroundtruthpolylinesin
6.ExperimentalEvaluation
intervals of size 10cm. We define precision and recall as
our metrics, where an estimated segment is correct if its We perform a pilot study of the difficulty of our tasks
distance to the closest segment on the target polyline set inasubsetofTorontoCity, containing125km2 region(50
issmallerthanathreshold(i.e.,0.5mand2.0m). km2fortraining,50km2fortestingand25km2forvalida-
tion). The train/val/test regions do not overlap and are not
GroundRoadSegmentation: WeuseIOUasourmetric. adjacent. We utilize 56K streetview images around these
regions (22K for training, 18K for validation and 16K for
testing). Hyper-parameters are chosen based on validation
Ground Urban Zoning Classification: This benchmark
performance, and all numbers reported are on the testing
is motivated by the human’s ability to recognize the urban
set.
functionofalocalregionbyitsappearance. WeuseTop-1
To perform the different segmentation related tasks, we
accuracyasourmetricandevaluateonthegroundview.
train two types of convolutional networks: a variant of
FCN-8 architecture [19] as well as a ResNet [14] with 56
Urban Zoning Segmentation: Our goal is to produce convolutionallayers. Moredetailsareinsupp. material.
a segmentation in bird’s eye view of the different urban
zones including residential, commercial, open space, em- Semantic Segmentation: As shown in Tab. 1, both net-
ployment,etc. WeutilizeIOUasourmetric. worksperformwell. Fig.5illustratesqualitativeresultsofMethod AlexNet[16] VGG-16[32] GoogleNet[33] ResNet-152[14] AlexNet[16] ResNet-32[14] GoogleNet[33] NiN[17]
From-scratch no no no no yes yes yes yes
Top-1accuracy 75.49% 79.12% 77.95% 79.33% 66.48% 75.65% 75.08% 79.07%
Table4: Ground-LevelUrbanZoningClassification
ResNet56output.Itisworthnotingthatlargenetworkssuch performanceamongallmodelsthataretrainedfromscratch.
as ResNet56 can be trained from scratch given our large- Formoredetails,pleaserefertothesupplementarymaterial.
scale dataset. Visually ResNet’s output tends to be more
sharp,whileFCN’soutputismoresmooth.
UrbanZoningSegmentation: Thisisanextremelyhard
taskfromaerialviewsalone. Tosimplifyit,wemergedthe
InstanceSegmentation: Weestimateinstancesegmenta- zone-types into residential, others (including commercial,
tionby takingtheoutput ofthe semanticlabelingand per- utility and employment) as well as open spaces (including
formingconnected-componentlabeling.Eachcomponentis natural, park, recreationaletc.). Pleaserefertothesupple-
assignedadifferentlabel. Sinceconvolutionalnetstendto mentary material for detailed label merging. As shown in
generatebloblikestructures,asinglecomponentmightcon- Tab.4moreresearchisneededtosolvethistask.
tain multiple instances connected with a small number of
pixels. To alleviate this problem, we apply morphological
Ground-viewroadsegmentation: Weutilizeasubsetof
openingoperatorsoverthesemanticlabelingmasks(anero-
the labeled panoramas, which includes 1000 training, 200
sionfilteringfollowedbyadilationfilteringwiththesame
validation and 800 testing images. The average IOU is
size). AsshowninTab.2andFig.6theperformanceislow.
97.21%. Theaveragepixelaccuracyis98.64%andaverage
There is still much for the community to do to solve this
top-downIOUis87.53%. Thisshowsthatastate-of-the-art
task. With more than 400,000 buildings, the TorontoCity
neuralnetworkcannearlysolvethistask,suggestingthatit
datasetprovidesanidealplatformfornewdevelopments.
ispromisingtoautomaticallygeneratehigh-resolutionmaps
bycapturinggeo-referencedstreet-viewpanoramas.
Road Centerlines and Curbs: We compute the medial
axis of the semantic segmentation to extract the skeleton
of the mask as our estimate of road centerline. In order to BuildingHeight: Nonetworkwasabletoestimatebuild-
smooththeskeletonization,wefirstconductamorphologi- ingheightfromaerialimagesalone. Thistaskiseithertoo
calclosingoperator(dilationfollowedbyerosion)overthe hard,ormoresophisticatedmethodsareneeded. Forexam-
roadmasks. Toestimateroadcurbs, wesimplyextractthe ple,utilizinggroundimageryseemsalogicalfirststep.
contours of the road segmentation and exploit closing op-
erator. As shown in Table. 1, ResNet achieves the highest 7.Conclusions
score in both tasks, and morphological filtering helps for
In this paper, we have argued that the field is in need
bothnetworks.QualitativeresultsareshowninFig.5.Note
of large scale benchmarks that will allow joint reasoning
thatthereisstillmuchroomforimprovement.
aboutgeometry,groupingandsemantics.Towardsthisgoal,
we have created the TorontoCity benchmark, covering the
BuildingContours: Wecomputebuildingcontoursfrom full Greater Toronto area (GTA) with 712.5km2 of land,
our estimated building instances, and apply the Ramer-
8439kmofroadandaround400,000buildings. Unlikeex-
Douglas-Peucker algorithm [27] to simplify each polygon
isting datasets, our benchmark provides a wide variety of
withathresholdof0.5m. Thisresultsinpolygonswith13
viewsoftheworldcapturedfromairplanes,drones,aswell
vertices on average. As shown in Tab. 5 and Fig. 7, this
as cars driving around the city. As using human annota-
simple procedure offers reasonable yet not satisfactory re-
tors is not feasible for such a large-scale dataset, we have
sulst. Thissuggeststhereisstillalargeimprovementspace
exploited different sources of high-precision maps to cre-
forgeneratingbuildingpolygonsfromaerialimages.
ate our ground truth. We have designed a wide variety of
tasks including building height estimation, road centerline
Ground Urban Zoning Classification: We train multi- andcurbextraction,buildinginstancesegmentation,build-
ple state-of-the-art convolutional networks for this task in- ing contour extraction (reorganization), semantic labeling
cluding AlexNet [16], VGG-16 [32], GoogleNet [33] and andscenetypeclassification(recognition). Ourpilotstudy
ResNet-152 [14] that are fine-tuned from the ImageNet showsthatmostofthesetasksarestilldifficultformodern
benchmark [9]. We also train AlexNet [16], ResNet-32 convolutional networks. We plan to extend the current set
[14], Network-In-Network [17]and ResNet-152 [14] from of benchmarks with tasks such as building reconstruction,
scratch over our ground-view panoramic image tiles. As facade parsing, tree detection, tree species categorization,
shown in Table. 1 ResNet-152 with pre-trained initializa- trafficlightdetection,andtrafficsigndetection.Thisisonly
tion achieves the best results. Net-in-net achieves the best thebeginningoftheexcitingTorontoCitybenchmark.References [21] G.Ma´ttyus,S.Wang,S.Fidler,andR.Urtasun. Enhancing
road maps by parsing aerial images around the world. In
[1] Openstreetmap. https://www.openstreetmap.
ICCV,2015. 2,3
org/. 1,5
[22] G.Ma´ttyus,S.Wang,S.Fidler,andR.Urtasun. Hdmaps:
[2] D. Anguelov, C. Dulong, D. Filip, C. Frueh, S. Lafon,
Fine-grainedroadsegmentationbyparsinggroundandaerial
R.Lyon,A.Ogale,L.Vincent,andJ.Weaver. Googlestreet
images. InCVPR,2016. 3
view: Capturingtheworldatstreetlevel. IEEEComputer,
[23] K.MatzenandN.Snavely. Nyc3dcars: Adatasetof3dve-
2010. 4
hiclesingeographiccontext. InICCV,2013. 3
[3] S.Baker,D.Scharstein,J.Lewis,S.Roth,M.J.Black,and
[24] F.Nex,M.Gerke,F.Remondino,H.Przybilla,M.Ba¨umker,
R.Szeliski. Adatabaseandevaluationmethodologyforop-
and A. Zurhorst. Isprs benchmark for multi-platform pho-
ticalflow. IJCV,2011. 3
togrammetry.ISPRSAnnalsofthePhotogrammetry,Remote
[4] W.Brooks. Theuniversaltransversemercatorgrid. InPro-
SensingandSpatialInformationSciences,2015. 2
ceedingsoftheIndianaAcademyofScience,1973. 4
[25] J. Niemeyer, F. Rottensteiner, and U. Soergel. Contextual
[5] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A
classification of lidar data and building object detection in
naturalistic open source movie for optical flow evaluation.
urbanareas. ISPRSjournalofphotogrammetryandremote
InECCV,2012. 3
sensing,2014. 2
[6] C.Chen,A.Seff,A.Kornhauser,andJ.Xiao. Deepdriving:
[26] N.Mayer, E.Ilg, P.Ha¨usser, P.Fischer, D.Cremers,
Learning affordance for direct perception in autonomous
A.Dosovitskiy, and T.Brox. A large dataset to train
driving. InICCV,2015. 3
convolutionalnetworksfordisparity,opticalflow,andscene
[7] H. Chu, S. Wang, R. Urtasun, and S. Fidler. Housecraft: flowestimation. InCVPR,2016. 3
Buildinghousesfromrentaladsandstreetviews. InECCV,
[27] U.Ramer. Aniterativeprocedureforthepolygonalapprox-
2016. 5
imationofplanecurves. Computergraphicsandimagepro-
[8] M.Cordts,M.Omran,S.Ramos,T.Rehfeld,M.Enzweiler, cessing,1972. 8
R. Benenson, U. Franke, S. Roth, and B. Schiele. The
[28] S.R.Richter,V.Vineet,S.Roth,andV.Koltun. Playingfor
cityscapes dataset for semantic urban scene understanding.
data:Groundtruthfromcomputergames.InECCV,2016.3
InCVPR,2016. 1,2
[29] G.Ros, L.Sellart, J.Materzynska, D.Vazquez, andA.M.
[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Lopez. Thesynthiadataset: Alargecollectionofsynthetic
Fei.Imagenet:Alarge-scalehierarchicalimagedatabase.In
imagesforsemanticsegmentationofurbanscenes.InCVPR,
CVPR,2009. 1,8
2016. 3
[10] P.Dolla´randC.L.Zitnick. Structuredforestsforfastedge
[30] F.Rottensteiner,G.Sohn,M.Gerke,andJ.D.Wegner. Isprs
detection. InICCV,2013. 6
test project on urban classification and 3d building recon-
[11] M.Everingham,L.VanGool,C.K.Williams,J.Winn,and struction. 2013. 2,3
A. Zisserman. The pascal visual object classes (voc) chal-
[31] A.Shafaei,J.J.Little,andM.Schmidt.Playandlearn:using
lenge. IJCV,2010. 1
videogamestotraincomputervisionmodels. arXiv,2016.
[12] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for au- 3
tonomous driving? the kitti vision benchmark suite. In
[32] K. Simonyan and A. Zisserman. Very deep convolutional
CVPR,2012. 1,2
networksforlarge-scaleimagerecognition. arXiv,2014. 8
[13] A. Handa, V. Patraucean, V. Badrinarayanan, S. Stent, and
[33] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
R. Cipolla. Scenenet: Understanding real world indoor
D.Anguelov, D.Erhan, V.Vanhoucke, andA.Rabinovich.
sceneswithsyntheticdata. arXiv,2015. 3
Goingdeeperwithconvolutions. InCVPR,2015. 8
[14] K.He,X.Zhang,S.Ren,andJ.Sun. Deepresiduallearning
[34] S.Wang,S.Fidler,andR.Urtasun. Holistic3dsceneunder-
forimagerecognition. CVPR,2016. 6,7,8
standing from a single geo-tagged image. In CVPR, 2015.
[15] T. Koch, P. d’Angelo, F. Kurz, F. Fraundorfer, P. Reinartz, 3
and M. Korner. The tum-dlr multimodal earth observation
[35] J.D.Wegner,S.Branson,D.Hall,K.Schindler,andP.Per-
evaluationbenchmark. InCVPRW,2016. 2
ona. Catalogingpublicobjectsusingaerialandstreet-level
[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet images-urbantrees. InCVPR,2016. 3
classification with deep convolutional neural networks. In
[36] A. R. Zamir, T. Wekel, P. Agrawal, C. Wei, J. Malik, and
NIPS,2012. 1,8
S.Savarese. Generic3drepresentationviaposeestimation
[17] M.Lin, Q.Chen, andS.Yan. Networkinnetwork. ICLR, andmatching. InECCV,2016. 3
2014. 8
[18] T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ra-
manan,P.Dolla´r,andC.L.Zitnick. Microsoftcoco: Com-
monobjectsincontext. InECCV,2014. 1
[19] J.Long, E.Shelhamer, andT.Darrell. Fullyconvolutional
networksforsemanticsegmentation. InCVPR,2015. 6,7
[20] W.Maddern,G.Pascoe,C.Linegar,andP.Newman. 1year,
1000km:Theoxfordrobotcardataset. IJRR,2016. 2"
225,227,Training deep neural networks on noisy labels with bootstrapping,"['S Reed', 'H Lee', 'D Anguelov', 'C Szegedy']",2014,1181,Toronto Face Database,"deep learning, neural network","On the Toronto Face Database, we show that our model  can also benefit from unlabeled  face images with no modification  we train a deep neural network with our proposed consistency",No DOI,arXiv preprint arXiv …,https://arxiv.org/abs/1412.6596,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,arxiv.org,"AcceptedasaworkshopcontributionatICLR2015
TRAINING DEEP NEURAL NETWORKS
ON NOISY LABELS WITH BOOTSTRAPPING
ScottE.Reed&HonglakLee
Dept. ofElectricalEngineeringandComputerScience,UniversityofMichigan
AnnArbor,MI,USA
{reedscot,honglak}@umich.edu
DragomirAnguelov,ChristianSzegedy,DumitruErhan&AndrewRabinovich
Google,Inc.
MountainView,CA,USA
{dragomir,szegedy,dumitru,amrabino}@google.com
ABSTRACT
Current state-of-the-art deep learning systems for visual object recognition and
detection use purely supervised training with regularization such as dropout to
avoid overfitting. The performance depends critically on the amount of labeled
examples, andincurrentpracticethelabelsareassumedtobeunambiguousand
accurate. However,thisassumptionoftendoesnothold;e.g. inrecognition,class
labels may be missing; in detection, objects in the image may not be localized;
andingeneral,thelabelingmaybesubjective. Inthisworkweproposeageneric
way to handle noisy and incomplete labeling by augmenting the prediction ob-
jective with a notion of consistency. We consider a prediction consistent if the
samepredictionismadegivensimilarpercepts, wherethenotionofsimilarityis
betweendeepnetworkfeaturescomputedfromtheinputdata. Inexperimentswe
demonstratethatourapproachyieldssubstantialrobustnesstolabelnoiseonsev-
eraldatasets. OnMNISThandwrittendigits,weshowthatourmodelisrobustto
labelcorruption. OntheTorontoFaceDatabase,weshowthatourmodelhandles
well the case of subjective labels in emotion recognition, achieving state-of-the-
artresults,andcanalsobenefitfromunlabeledfaceimageswithnomodification
toourmethod. OntheILSVRC2014detectionchallengedata, weshowthatour
approach extends to very deep networks, high resolution images and structured
outputs,andresultsinimprovedscalabledetection.
1 INTRODUCTION
Currently the predominant systems for visual object recognition and detection (Krizhevsky et al.,
2012; Zeiler & Fergus, 2013; Girshick et al., 2013; Sermanet et al., 2013; Szegedy et al., 2014)
use purely supervised training with regularization such as dropout (Hinton et al., 2012) to avoid
overfitting. Thesesystemsdonotaccountformissinglabels,subjectivelabelingorinexhaustively-
annotatedimages. However,thisassumptionoftendoesnothold,especiallyforverylargedatasets
and in high-resolution images with complex scenes. For example, in recognition, the class labels
maybemissing; indetection,theobjectsintheimagemaynotallbelocalized;insubjectivetasks
suchasfacialemotionrecognition,humansmaynotevenagreeontheclasslabel. Astrainingsets
fordeepnetworksbecomelarger(astheyshould),theproblemofmissingandnoisylabelsbecomes
moreacute,andsoweargueitisafundamentalproblemforscalingupvision.
In this work we propose a simple approach to hande noisy and incomplete labeling in weakly-
superviseddeeplearning,byaugmentingtheusualpredictionobjectivewithanotionofperceptual
consistency. Weconsiderapredictionconsistent ifthesamepredictionismadegivensimilarper-
cepts,wherethenotionofsimilarityincorporatesfeatureslearnedbythedeepnetwork.
One interpretation of the perceptual consistency objective is that the learner makes use of its rep-
resentationoftheworld(implicitinthenetworkparameters)tomatchincomingperceptstoknown
1
5102
rpA
51
]VC.sc[
3v6956.2141:viXraAcceptedasaworkshopcontributionatICLR2015
categories, or in general structured outputs. This provides the learner justification to “disagree”
withaperceptually-inconsistenttraininglabel,andeffectivelyre-labelthedatawhiletraining. More
accurate labels may lead to a better model, which allows further label clean-up, and the learner
bootstrapsitselfinthisway. Ofcourse,toomuchskepticismofthelabelscarriestheriskofending
up with a delusional agent, so it is important to balance the trade-off between prediction and the
learner’sperceptualconsistency.
Inourexperimentswedemonstratethatourapproachyieldssubstantialrobustnesstoseveraltypes
oflabelnoiseonseveraldatasets. OnMNISThandwrittendigits(LeCun&Cortes,1998)weshow
thatourmodelisrobusttolabelcorruption. OntheTorontoFaceDatabase(Susskindetal.,2010)
weshowthatourmodelhandleswellthecaseofsubjectivelabelsinemotionrecognition,achieving
state-of-the-artresults,andcanalsobenefitfromunlabeledfaceimageswithnomodificationtoour
method. On the ILSVRC2014 detection challenge data (Russakovsky et al., 2014), we show that
ourapproachimprovessingle-shotpersondetectionusingaMultiBoxnetwork(Erhanetal.,2014),
andalsoimprovesperformanceinfull200-waydetectionusingMultiBoxforregionproposalanda
deepCNNforpost-classification.
Insection2wediscussrelatedwork,insection3wedescribeourmethodalongwithaprobabilistic
interpretationandinsection4wepresentourresults.
2 RELATED WORK
The literature on semi-supervised and weakly-supervised learning is vast (see Zhu (2005) for a
survey), and so in this section we focus on the key previous papers that inspired this work and on
otherpapersonweakly-andsemi-superviseddeeplearning.
Thenotionofbootstrapping,or“self-training”alearningagentwasproposedin(Yarowsky,1995)
as a way to do word-sense disambiguation with only unlabeled examples and a small list of seed
example sentences with labels. The algorithm proceeds by building an initial classifier using the
seed examples, and then iteratively classifying unlabeled examples, extracting new seed rules for
theclassifierusingthenowexpandedtrainingdata,andrepeatingthesestepsuntilconvergence.The
algorithmwasanalyzedbyAbney(Abney,2004)andmorerecentlyby(Haffari&Sarkar,2012).
Co-training (Blum & Mitchell, 1998; Nigam et al., 2000; Nigam & Ghani, 2000) was similarly-
motivatedbutusedapairofclassifierswithseparateviewsofthedatatoiterativelylearnandgen-
erateadditionaltraininglabels. Whitney&Sarkar(2012)proposedbootstrappinglabeledtraining
exampleswithgraph-basedlabelpropagation. Brodley&Friedl(1999)developedstatisticalmeth-
odsforidentifyingmislabeledtrainingdata.
Rosenberg et al. (2005) also trained an object detection system in a weakly-supervised manner
usingself-training,anddemonstratedthattheirproposedmodelachievedcomparableperformance
to models trained with a much larger set of labels. However, that approach works as a wrapper
aroundanexistingdetectionsystem, whereasinthisworkweintegrateaconsistencyobjectivefor
bootstrappingintothetrainingofthedeepnetworkitself.
Ourworksharesasimilarmotivationtotheseearlierworks,butinsteadofexplicitlygeneratingnew
training labels and adding new examples to the training set in an outer loop, we incorporate our
consistencyobjectivedirectlyintothemodel. Inaddition,weconsidernotonlythecaseoflearning
fromunlabeledexamples,butalsofromnoisylabelsandinexhaustively-annotatedexamples.
Mnih&Hinton(2012)developeddeepneuralnetworksforimprovedlabelingofaerialimages,with
robust loss functions to handle label omission and registration errors. This work shares a similar
motivationofrobustnesstonoisylabels,butratherthanformulatinglossfunctionsforspecifictypes
ofnoise,weaddagenericconsistencyobjectivetothelosstoachieverobustness.
Minimumentropyregularization,proposedin(Grandvalet&Bengio,2005;2006),performssemi-
supervisedlearningbyaugmentingcross-entropylosswithatermencouragingtheclassifiertomake
predictions with high confidence on the unlabeled examples1. This is notable because in their ap-
proachtrainingonunlabeledexamplesdoesnotrequireagenerativemodel,whichisbeneficialfor
training on high-resolution images and other sensory data. We take a similar approach by side-
1seeeq.9.7in (Grandvalet&Bengio,2006)
2AcceptedasaworkshopcontributionatICLR2015
stepping the difficulty of fully-generative models of high-dimensional sensory data. However, we
extendbeyondshallowmodelstodeepnetworks,andtostructuredoutputprediction.
Never ending language learning (NELL) (Carlson et al., 2010) and never ending image learning
(NEIL)(Chenetal.,2013;2014)arelifelong-learningsystemsforlanguageandimageunderstand-
ing, respectively. They continuously bootstrap themselves using a cycle of data collection, propa-
gationoflabelstothenewlycollecteddata,andself-improvementbytrainingonthenewdata. Our
work is complementary to these efforts, and focuses on building robustness to noisy and missing
labelsintothemodelforweakly-superviseddeeplearning.
Larochelle&Bengio(2008)developedanRBMforclassificationthatusesahybridgenerativeand
discriminative training objective. Deep Boltmann Machines (Salakhutdinov & Hinton, 2009) can
alsobetrainedinasemi-supervisedmannerwithlabelsconnectedtothetoplayer. Morerecently,
multi-predictionDBMtraining(Goodfellowetal.,2013)andGenerativeStochasticNetworks(Ben-
gio&Thibodeau-Laufer,2013)improvedtheperformanceandsimplifiedthetrainingofdeepgener-
ativemodels,enablingtrainingviabackpropagationmuchlikeinstandarddeepsupervisednetworks.
However, fully-generativeunsupervisedtrainingonhigh-dimensionalsensorydata, e.g. ImageNet
images, isstillfarbehindsupervisedmethodsintermsofperformance, andsointhisworkwedo
not follow the generative approach directly. Instead, this work focuses on a way to benefit from
unlabeledandweakly-labeledexampleswithminimalmodificationtoexistingdeepsupervisednet-
works. We demonstrate increased robustness to label noise and performance improvements from
unlabeleddataforaminimalengineeringeffort.
Morerecently, theproblemofdeeplearningfromnoisylabelshasbeguntoreceiveattention. Lee
(2013)alsofollowedtheideaofminimumentropyregularization,andproposedgenerating“pseudo-
labels” as training targets for unlabeled data, and showed improved performance on MNIST with
few labeled examples. Sukhbaatar & Fergus (2014) developed two deep learning techniques for
handlingnoisylabels,learningtomodelthenoisedistributioninatop-downandbottom-upfashion.
Inthiswork,wepushfurtherbyextendingbeyondclasslabelstostructuredoutputs,andweachieve
state-of-the-art scalable detection performance on ILSVRC2014, despite the fact that our method
doesnotrequireexplicitlymodelingthenoisedistribution.
3 METHOD
In this section we describe two approaches: section 3.1 uses reconstruction error as a consistency
objective and explicitly models the noise distribution as a matrix mapping model predictions to
traininglabels.Areconstructionlossisaddedtopromotetop-downconsistencyofmodelpredictions
withtheobservations,whichallowsthemodeltodiscoverthepatternofnoiseinthedata.
The method presented in section 3.2 (bootstrapping) uses a convex combination of training labels
and the current model’s predictions to generate the training targets, and thereby avoids directly
modeling the noise distribution. This property is well-suited to the case of structured outputs, for
which modeling dense interactions among all pairs of output units may be neither practical nor
useful. Thesetwoapproachesarecomparedempiricallyinsection4.
Insection3.3weshowhowtoapplyourbootstrappingapproachtostructuredoutputsbyusingthe
MultiBox(Erhanetal.,2014)regionproposalnetworktohandlethecaseofinexhaustivestructured
outputlabelingforsingle-shotpersondetectionandforclass-agnosticregionproposal.
3.1 CONSISTENCYINMULTI-CLASSPREDICTIONVIARECONSTRUCTION
Letx∈{0,1}D bethedata(ordeepfeaturescomputedfromthedata)andt∈{0,1}L,(cid:80) t =1
k k
theobservednoisymultinomiallabels. Thestandardsoftmaxregressesxontotwithouttakinginto
accountnoisyormissinglabels.Inadditiontooptimizingtheconditionallog-likelihoodlogP(t|x),
weaddaregularizationtermencouragingtheclasspredictiontobeperceptuallyconsistent.
Wefirstintroduceintoourmodelthe“true”classlabel(asopposedtothenoisylabelobservations)
asalatentmultinomialvariableq ∈
{0,1}L,(cid:80)
q = 1. Ourdeepfeed-forwardnetworkmodels
j j
3AcceptedasaworkshopcontributionatICLR2015
theposterioroverqusingtheusualsoftmaxregression:
P(q =1|x)=
P˜(q
j
=1|x)
=
exp((cid:80)D i=1W i( j1)x i+b( i1))
(1)
j (cid:80)L j(cid:48)=1P˜(q j(cid:48) =1|x) (cid:80)L j(cid:48)=1exp((cid:80)D i=1W i( j1 (cid:48))x i+b( i1))
whereP˜ denotestheunnormalizedprobabilitydistribution. Giventhetruelabelq, thelabelnoise
canbemodeledusinganothersoftmaxwithlogitsasfollows:
L
logP˜(t =1|q)=(cid:88) W(2)q +b(2) (2)
k kj j k
j=1
Roughly,W(2) learnsthelog-probabilityofobservingtruelabelj asnoisylabelk. Givenonlyan
kj
observationx,wecanmarginalizeoverqtocomputetheposterioroftargettgivenx.
L L
(cid:88) (cid:88)
P(t =1|x)= P(t =1,q =1|x)= P(t =1|q =1)P(q =1|x) (3)
k k j k j j
j=1 j=1
where the label noise distribution and posterior over true labels are defined above. We can per-
formdiscriminativetrainingbygradientascentonlogP(t|x). However,thispurelydiscriminative
training does not yet incorporate perceptual consistency, and there is no explicit incentive for the
modeltotreatqasthe“true”label;itcanbeviewedasanotherhiddenlayer,withthemultinomial
constraintresultinginaninformationbottleneck.
In unpublished work2, Hinton & Mnih (2009) developed a Restricted Boltzmann Machine
(RBM) (Smolensky, 1986) variant with hidden multinomial output unit q and observed noisy la-
belunittasdescribedabove. Theassociatedenergyfunctioncanbewrittenas
D L L L L L D
E(x,t,q)=−(cid:88)(cid:88) W(1)x q −(cid:88)(cid:88) W(2)t q −(cid:88) b(1)q −(cid:88) b(2)t −(cid:88) b(3)x (4)
ij i j kj k j j j k k i i
i=1j=1 k=1j=1 j=1 k=1 i=1
DuetothebipartitestructureoftheRBM,tandxareconditionallyindependentgivenq,andsothe
energyfunctionineq.(4)leadstoasimilarformoftheposteriorasineq.(3),marginalizingoutthe
hiddenmultinomialunit. Theprobabilitydistributionarisingfrom(4)isgivenby
(cid:80)
exp(−E(x,t,q))
P(x,t)= q
Z
(cid:80)
where Z = exp(−E(x,t,q)) is the partition function. The model can be trained with
x,t,q
a generative objective, e.g. by approximate gradient ascent on logP(x,t) via contrastive diver-
gence(Hinton,2002).Generativetrainingnaturallyprovidesanotionofconsistencybetweenobser-
vationsxandpredictionsqbecausethemodellearnstodrawsampleobservationsviatheconditional
likelihoodP(x =1|q)=σ((cid:80)L W(1)q +b(3)),assumingbinaryobservations.
i j=1 ij j i
Observed noisy label t Data reconstruction
True output labels q
True output
labels q
Observed data x Observed noisy label t
Observed data x
Figure1: Left: RestrictedBoltzmannMachinewithhiddenmultinomialoutputunit. Right: Analo-
gousfeed-forwardautoencoderversion.
2Knownfrompersonalcorrespondence.
4AcceptedasaworkshopcontributionatICLR2015
However, fully-generative training is complicated by the fact that the exact likelihood gradient is
intractable due to computing the partition function Z, and in practice MCMC is used. Generative
trainingisfurthercomplicated(thoughcertainlystillpossible)incaseswherethefeaturesxarenon-
binary. Toavoidthesecomplications,andtomakeourapproachrapidlyapplicabletoexistingdeep
networksusingrectifiedlinearactivations,andtrainableviaexactgradientdescent,weproposean
analogousautoencoderversion.
Figure1comparestheRBMandautoencoderapproachestothemulticlasspredictionproblemwith
perceptualconsistency. Theoverallobjectiveinthefeed-forwardversionisasfollows:
L
(cid:88)
L (x,t)=− t logP(t =1|x)+β||x−W(2)q(x)||2 (5)
recon k k 2
k=1
whereq(x) = P(q = 1|x)asinequation1. Theparameterβ canbefoundviacross-validation.
j j
Experimentalresultsusingthismethodarepresentedinsections4.1and4.2.
3.2 CONSISTENCYINMULTI-CLASSPREDICTIONVIABOOTSTRAPPING
In this section we develop a simple consistency objective that does not require an explicit noise
distributionorareconstructionterm. Theideaistodynamicallyupdatethetargetsoftheprediction
objectivebasedonthecurrentstateofthemodel. Theresultingtargetsareaconvexcombinationof
(1) the noisy training label, and (2) the current prediction of the model. Intuitively, as the learner
improves over time, its predictions can be trusted more. This mitigates the damage of incorrect
labeling, because incorrect labels are likely to be eventually highly inconsistent with other stimuli
predictedtohavethesamelabelbythemodel.
By paying less heed to inconsistent labels, the learner can develop a more coherent model, which
further improves its ability to evaluate the consistency of noisy labels. We refer to this approach
as “bootstrapping”, in the sense of pulling oneself up by one’s own bootstraps, and also due to
inspirationfromtheworkofYarowsky(1995)whichisalsoreferredtoasbootstrapping.
Concretely,weuseacross-entropyobjectiveasbefore,butgeneratenewregressiontargetsforeach
SGDmini-batchbasedonthecurrentstateofthemodel.Weempiricallyevaluatedtwotypesofboot-
strapping. “Soft”bootstrappingusespredictedclassprobabilitiesqdirectlytogenerateregression
targetsforeachbatchasfollows:
L
(cid:88)
L (q,t)= [βt +(1−β)q ]log(q ) (6)
soft k k k
k=1
Infact,itcanbeshownthattheresultingobjectiveisequivalenttosoftmaxregressionwithminimum
entropy regularization, which was previously studied in (Grandvalet & Bengio, 2006). Intuitively,
minimumentropyregularizationencouragesthemodeltohaveahighconfidenceinpredictinglabels
(evenfortheunlabeledexamples,whichenablessemi-supervisedlearning).
“Hard”bootstrappingmodifiesregressiontargetsusingtheMAPestimateofqgivenx, whichwe
denoteasz :=1[k =argmaxq ,i=1...L]:
k i
L
(cid:88)
L (q,t)= [βt +(1−β)z ]log(q ) (7)
hard k k k
k=1
Whenusedwithmini-batchstochasticgradientdescent,thisleadstoanEM-likealgorithm: Inthe
E-step,estimatethe“true”confidencetargetsasaconvexcombinationoftraininglabelsandmodel
predictions;intheM-step,updatethemodelparameterstobetterpredictthosegeneratedtargets.
Bothhardandsoftbootstrappingcanbeviewedasinstancesofamoregeneralapproachinwhich
model-generatedregressiontargetsaremodulatedbyasoftmaxtemperatureparameterT;i.e.
exp(T ·((cid:80)D W(1)x +b(1)))
P(q =1|x)= i=1 ij i j (8)
j (cid:80)L exp(T ·((cid:80)D W(1)x +b(1)))
j(cid:48)=1 i=1 ij(cid:48) i j(cid:48)
Setting T = 1 recovers soft boostrapping, and T = ∞ recovers hard bootstrapping. We only use
thesetwooperatingpointsinourexperiments,butitmaybeworthwhiletoexploreothervaluesfor
T,andlearningT foreachdataset.
5AcceptedasaworkshopcontributionatICLR2015
3.3 CONSISTENCYWITHSTRUCTUREDOUTPUTPREDICTION
Noisylabelsalsooccurinstructuredoutputpredictionproblemssuchasobjectdetection. Current
state-of-the-artobjectdetectionsystemstrainonimagesannotatedwithboundingboxlabelsofthe
relevantobjectsineachimage,andtheclasslabelforeachbox. However,itisexpensivetoexhaus-
tivelyannotateeachimage,andforsomecommonly-appearingcategoriesthedatamaybeproneto
missingannotations. Inthissection,wemodifythetrainingobjectiveoftheMultiBox(Erhanetal.,
2014)networkforobjectdetectiontoincorporateanotionofperceptualconsistencyintotheloss.
IntheMultiBoxapproach,ground-truthboundingboxesareclusteredandtheresultingcentroidsare
usedas“priors”forpredictingobjectlocation. Adeepneuralnetworkistrainedtopredict,foreach
groundtruth object in an image, a residual of that groundtruth bounding box to the best-matching
boundingboxprior. Thenetworkalsooutputsalogisticconfidencescoreforeachprior,indicating
the model’s belief of whether or not an object appears in the corresponding location. Because
MultiBoxgivesproposalswithconfidencescores,itenablesveryefficientruntime-qualitytradeoffs
fordetectionviathresholdingthetop-scoringproposalswithinbudget. Thusitisanattractivetarget
forfurtherqualityimprovements,aswepursueinthissection.
Denotetheconfidencescoretrainingtargetsast ∈ {0,1}L andthepredictedconfidencescoresas
c∈[0,1]L. TheobjectiveforMultiBox3canbewrittenasthefollowingcross-entropyloss:
L
(cid:88)
L (c,t)=− (t log(c )+(1−t )log(1−c )) (9)
multibox k k k k
k=1
Notethatthesumhereisoverobjectlocations,notclasslabelsasinthecaseofsections3.1and3.2.
If there is an object at location k, but t = 0 due to inexhaustive annotation, the model pays a
k
large cost for correctly predicting c = 1. Training naively on the noisy labels leads to perverse
k
learning situations such as the following: two objects of the same category (potentially within the
same image) appear in the training data, but only one of them is labeled. To reduce the loss, the
confidence prediction layer must learn to distinguish the two objects, which is exactly contrary to
theobjectiveofvisualinvariancetocategory-preservingdifferences.
Toincorporateanotionofperceptualconsistencyintotheloss,wefollowthesameapproachasin
thecaseofmulti-classclassification: augmenttheregressiontargetsusingthemodel’scurrentstate.
Inthe“hard”case,MAPestimatescanbeobtainedbythresholdingc >1/2.
k
L
(cid:88)
L (c,t)=− [βt +(1−β)1 ]log(c ) (10)
multibox−hard k ck>0.5 k
k=1
L
(cid:88)
− [β(1−t )+(1−β)(1−1 )]log(1−c )
k ck>0.5 k
k=1
L
(cid:88)
L (c,t)=− [βt +(1−β)c ]log(c ) (11)
multibox−soft k k k
k=1
L
(cid:88)
− [β(1−t )+(1−β)(1−c )]log(1−c )
k k k
k=1
With the bootstrap variants of the MultiBox objective, unlabeled positives pose less of a problem
becausepenaltiesforlargec aredown-scaledbyfactorβinthefirsttermand(1−c )inthesecond
k k
term. Bymitigatingpenaltiesduetomissingpositivesinthedata,ourapproachallowsthemodelto
learntopredictc withhighconfidenceeveniftheobjectsatlocationkareoftenunlabeled.
k
4 EXPERIMENTS
We perform experiments on three image understanding tasks: MNIST handwritten digits recogni-
tion, Toroto Faces Database facial emotion recognition, and ILSVRC2014 detection. In all tasks,
3Weomittheboundingboxregressiontermforsimplicity,see(Erhanetal.,2014)forfulldetails.
6AcceptedasaworkshopcontributionatICLR2015
wetrainadeepneuralnetworkwithourproposedconsistencyobjective. Inourfigures,“bootstrap-
recon”referstotrainingasdescribedinsection3.1,usingreconstructionasaconsistencyobjective.
“bootstrap-soft”and“bootstrap-hard”refertoourmethoddescribedinsections3.2and3.3.
4.1 MNISTWITHNOISYLABELS
Inthissectionwetrainusingourreconstruction-basedobjective(detailedinsection3.1)onMNIST
handwrittendigitswithvaryingdegreesofnoiseinthelabels. Specifically,weusedafixedrandom
permutationofthelabelsasvisualizedinfigure2,andweperformcontrolexperimentswhilevarying
theprobabilityofapplyingthelabelpermutationtoeachtrainingexample.
Allmodelsweretrainedwithmini-batchSGD,withthesamearchitecture: 784-500-300-10neural
network with rectified linear units. We used L weight decay of 0.0001. We found that β = 0.8
2
workedbestforbootstrap-hard,0.95forbootstrap-soft,and0.005forbootstrap-recon. Weinitialize
W(2)totheidentitymatrix.
Forthenetworktrainedwithourproposedconsistencyobjective,weinitializedthenetworklayers
from the baseline prediction-only model. It is also possible to initialize from scratch using our
approach, but we found that with pre-training we could use a larger β and more quickly converge
to a good result. Intuitively, this is similar to the initial collection of “seed” rules in the original
bootstrapping algorithm of (Yarowsky, 1995). During the fine-tuning training phase, all network
weightsareupdatedbybackpropagatinggradientsthroughthelayers.
Figure 2 shows that our bootstrapping method provides a very significant benefit in the case of
permutedlabels. Thebootstrap-reconmethodperformsthebest,andbootstrap-hardnearlyaswell.
Thebootstrap-softmethodprovidessomebenefitinthehigh-noiseregime, butonlyslightlybetter
thanthebaselineoverall.
0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9
0 0
1 1
2 2
3 3
4 4
5 5
6 6
7 7
8 8
9 9
Label noise pattern Learned
lebaL
ysioN
True Label
lebaL
ysioN
True Label
Figure2: Left: Digitrecognitionaccuracyversuspercentcorruptedlabels. Middle: avisualization
ofthenoisepattern. Ifthereisawhiteentryinrowr,columnc,thenlabelcismappedtotolabel
rwithsomeprobabilityduringtraining. Right: avisualizationofP(t = 1|q = 1)learnedbyour
r c
bootstrap-reconmodel(asparametersW(2))trainedwith40%labelnoise.
rc
Figure 2 also shows that bootstrap-recon effectively learns the noise distribution P(t|q) via the
parametersW(2). Intuitively, thelossfromthereconstructiontermprovidesthelearnerabasison
kj
whichpredictionsqmaydisagreewithtraininglabelst. Sincexmustbeabletobereconstructed
from q in bootstrap-recon, learning a non-identity W(2) allows q to flexibly vary from t to better
reconstructxwithoutincurringapenaltyfrompredictionerror.
However,itisinterestingtonotethatbootstrap-hardachievesnearlyequivalentperformancewithout
explicitlyparameterizingthenoisedistribution. Thisisusefulbecausereconstructionmaybechal-
lenginginmanycases,suchaswhenxisdrawnfromacomplicated,high-dimensionaldistribution,
andbootstrap-hardistrivialtoimplementontopofexistingdeepsupervisednetworks.
7AcceptedasaworkshopcontributionatICLR2015
4.2 TORONTOFACESDATABASEEMOTIONRECOGNITION
Inthissectionwepresentresultsonemotionrecognition. TheTorontoFacesDatabasehas112,234
images,4,178ofwhichhaveemotionlabels. Inallexperimentswefirstextractedspatial-pyramid-
pooledOMP-1featuresasdescribedin(Coates&Ng,2011)toget3200-dimensionalfeatures. We
thentraineda3200-1000-500-7networktopredictthe1-of-7emotionlabelsforeachimage.
As in the case for our MNIST experiments, we initialize our model from the network pre-trained
withpredictiononly,andthefine-tunedalllayerswithourhybridobjective.
Figure3summarizesourTFDresults. AsinthecaseofMNIST,bootstrap-reconandbootstrap-hard
perform the best, significantly outperforming the softmax baseline, and bootstrap-soft provides a
moremodestimprovement.
Anger Disgust Afraid Happy
Sad
Surprise Nd eutral
Anger
Disgust
Afraid
Happy
Sad
Surprised
Neutral
lebaL
)kaeW(
ysioN
Predicted True Label
Training Accuracy(%)
baseline 85.3
bootstrap-recon 86.8
bootstrap-hard 86.8
bootstrap-soft 85.6
disBMa 85.4
CDA+CCAb 85.0
Table 1: Emotion recognition results
on Toronto Faces Database compared
tostate-of-the-artmethods.
a(Reedetal.,2014)
Figure3: Predicted-to-noisyemotionlabelcon-
b(Rifaietal.,2012)
nectionW(2)learnedbyourmodel.
There is significant off-diagonal weight in W(2) learned by bootstrap-recon on TFD, suggesting
that the model learns to “hedge” its emotion prediction during training by spreading probability
massfromthepredictedclasstocommonly-confusedclasses,suchas“afraid”and“surprised”. The
strongest off diagonals are in the “happy” column, which may be due to the fact that “happy” is
themostcommonexpression,orperhapsthathappyexpressionshavealargevisualdiversity. Our
methodimprovestheemotionrecognitionperformance,whichtoourknowledgeisstate-of-the-art.
TheperformanceimprovementfromallthreebootstrapmethodsonTFDsuggeststhatourapproach
canbeusefulnotjustformistakenlabels,butalsoforsemi-supervisedlearning(missinglabels)and
learningfromweaklabelssuchasemotioncategories.
4.3 ILSVRC2014FASTSINGLE-SHOTPERSONDETECTION
InthissectionweapplyourmethodtodetectingpersonsusingaMultiBoxnetworkbuiltontopof
theInceptionarchitectureproposedin(Szegedyetal.,2014).Wefirstpre-trainedMultiBoxonclass-
agnosticlocalizationusingthefullILSVRC2014trainingset,sincethereareonlyseveralthousand
imageslabeledwithpersonsinILSVRC,andthenfine-tunedonpersonimagesonly.
An important point for comparison is the top-K bootstrapping heuristic introduced for MultiBox
training in (Szegedy et al., 2014) for person detection in the presence of missing annotations. In
thatapproach,thetop-K largestconfidencepredictionsaredroppedfromtheloss(whichweshow
hereineq.(9)),andthesettingusedwasK = 4. Inotherwords,thereisnogradientcomingfrom
thetop-K mostconfidentlocationpredictions. Infact,itcanbeviewedasaformofbootstrapping
whereonlythetop-K mostconfidentlocationsmodifytheirtargets,whichbecomethepredictions
themselves. In this work, we aim to achieve similar or better performance in a more general way
thatcanbeappliedtoMultiBoxandotherdiscriminativemodels.
Theprecision-recallcurvesinfigure4showthatourproposedbootstrappingimprovessubstantially
overtheprediction-onlybaseline. Atthehigh-precisionendofthePRcurve,theapproachesintro-
ducedinthispaperperformbetter,whilethetop-K heuristicisslightlybetterathigh-recall.
8AcceptedasaworkshopcontributionatICLR2015
ILSVRC Person Detection
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Recall
noisicerP
baseline
top−K heuristic
bootstrap−hard Training AP(%) Recall@60p
bootstrap−soft
baseline 30.9 14.3
top-Kheuristic 44.1 43.4
bootstrap-hard 44.6 43.9
bootstrap-soft 43.6 42.8
Table 2: Our proposed bootstrap-hard and
bootstrap-soft, and the top-K heuristic signifi-
cantly improve average precision compared to
theprediction-onlybaseline. Recall@60pisthe
recallachievableat60%precision.
Figure 4: Precision-recall curves for our meth-
ods compared to the baseline prediction-only
network,andthetop-Kheuristic.
4.4 ILSVRC2014200-CATEGORYDETECTION
Inthissectionweapplyourmethodtothecaseofobjectdetectiononthelarge-scaleImageNetdata.
Ourproposedmethodisappliedintwoways:firsttotheMultiBoxnetworkforregionproposal,and
secondtotheclassifiernetworkthatpredictslabelsforeachcroppedimageregion. Wefollowthe
approachin(Szegedyetal.,2014)andcombineimagecropsfromMultiBoxregionproposalswith
deepnetworkcontextfeaturesastheinputtotheclassifierforeachproposedregion.
WetrainedtheMultiBoxnetworkasdescribedin3.3, andthepost-classifiernetworkasdescribed
insection3.2. Wefoundthat“hard”performedbetterthanthe“soft”formofbootstrapping.
MultiBox Postclassifier mAP(%) Recall@60p
baseline baseline 39.8 38.4
baseline bootstrap-hard 40.0 38.6
bootstrap-hard baseline 40.3 39.3
bootstrap-hard bootstrap-hard 40.3 39.1
- GoogLeNetsinglemodela 38.8 -
- DeepID-Netsinglemodelb 40.1 -
Using our proposed bootstrapping method, we observe modest improvement on the ILSVRC2014
detection“val2”data4,mainlyattributabletobootstrappinginMultiBoxtraining.
5 CONCLUSIONS
Inthispaperwedevelopednoveltrainingmethodsforweakly-superviseddeeplearning,anddemon-
stratedtheeffectivenessofourapproachonmulti-classpredictionandstructuredoutputprediction
for several datasets. Our method is exceedingly simple and can be applied with very little engi-
neeringefforttoexistingnetworkstrainedusingapurely-supervisedobjective. Theimprovements
thatweshowevenwithverysimplemethods,suggestthatmovingbeyondpurely-superviseddeep
learningisworthyoffurtherresearchattention. Inadditiontoachievingbetterperformancewiththe
datawealreadyhave, ourresultssuggestthatperformancegainsmaybeachievedfromcollecting
moredataatacheaperprice,sinceimageannotationneednotbeasexhasutiveandmistakenlabels
arenotasharmfultotheperformance.
In future work, it may be promising to consider learning a time-dependent policy for tuning β,
thescalingfactorbetweenpredictionandperceptualconsistencyobjectives,andalsotoextendour
approach to the case of a situated agent. Another promising direction is to augment large-scale
training for detection (e.g. ILSVRC) with unlabeled and more weakly-labeled images, to further
benefitfromourproposedperceptualconsistencyobjective.
4Inapreviousversionofthisdraftweincludednumbersonthefullvalidationset.However,wediscovered
thatcontextandpost-classifiermodelsused“val1”data,sowere-ranexperimentsonlyonthe“val2”subset
9AcceptedasaworkshopcontributionatICLR2015
REFERENCES
Abney,Steven. Understandingtheyarowskyalgorithm. ComputationalLinguistics,30(3):365–395,2004. 2
Bengio,YoshuaandThibodeau-Laufer,Eric.Deepgenerativestochasticnetworkstrainablebybackprop.arXiv
preprintarXiv:1306.1091,2013. 3
Blum,AvrimandMitchell,Tom. Combininglabeledandunlabeleddatawithco-training. InProceedingsof
theeleventhannualconferenceonComputationallearningtheory,pp.92–100.ACM,1998. 2
Brodley,CarlaEandFriedl,MarkA. Identifyingmislabeledtrainingdata. JournalofArtificialIntelligence
Research,11:131–167,1999. 2
Carlson,Andrew,Betteridge,Justin,Kisiel,Bryan,Settles,Burr,HruschkaJr,EstevamR,andMitchell,TomM.
Towardanarchitecturefornever-endinglanguagelearning. InAAAI,volume5,pp. 3,2010. 3
Chen,Xinlei,Shrivastava,Abhinav,andGupta,Abhinav.Neil:Extractingvisualknowledgefromwebdata.In
ComputerVision(ICCV),2013IEEEInternationalConferenceon,pp.1409–1416.IEEE,2013. 3
Chen,Xinlei,Shrivastava,Abhinav,andGupta,Abhinav. Enrichingvisualknowledgebasesviaobjectdiscov-
eryandsegmentation. CVPR,2014. 3
Coates,AdamandNg,AndrewY. Theimportanceofencodingversustrainingwithsparsecodingandvector
quantization. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp.
921–928,2011. 8
Erhan,Dumitru,Szegedy,Christian,Toshev,Alexander,andAnguelov,Dragomir. ScalableObjectDetection
UsingDeepNeuralNetworks. InCVPR,pp.2155–2162,2014. 2,3,6
Girshick,Ross,Donahue,Jeff,Darrell,Trevor,andMalik,Jitendra.Richfeaturehierarchiesforaccurateobject
detectionandsemanticsegmentation. arXivpreprintarXiv:1311.2524,2013. 1
Goodfellow, Ian, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua. Multi-prediction deep boltzmann
machines. InAdvancesinNeuralInformationProcessingSystems,pp.548–556,2013. 3
Grandvalet, Yves and Bengio, Yoshua. Semi-supervised learning by entropy minimization. In Advances in
NeuralInformationProcessingSystems,pp.529–536,2005. 2
Grandvalet,YvesandBengio,Yoshua. 9entropyregularization. 2006. 2,5
Haffari,GholamRezaandSarkar,Anoop. Analysisofsemi-supervisedlearningwiththeyarowskyalgorithm.
arXivpreprintarXiv:1206.5240,2012. 2
Hinton,GeoffreyE. Trainingproductsofexpertsbyminimizingcontrastivedivergence. Neuralcomputation,
14(8):1771–1800,2002. 4
Hinton,GeoffreyEandMnih,Volodymyr.Restrictedboltzmannmachinewithhiddenmultinomialoutputunit.
Unpublishedwork,2009. 4
Hinton,GeoffreyE,Srivastava,Nitish,Krizhevsky,Alex,Sutskever,Ilya,andSalakhutdinov,RuslanR. Im-
provingneuralnetworksbypreventingco-adaptationoffeaturedetectors. arXivpreprintarXiv:1207.0580,
2012. 1
Krizhevsky, Alex, Sutskever, Ilya, andHinton, GeoffreyE. Imagenetclassificationwithdeepconvolutional
neuralnetworks. InAdvancesinneuralinformationprocessingsystems,pp.1097–1105,2012. 1
Larochelle,HugoandBengio,Yoshua. Classificationusingdiscriminativerestrictedboltzmannmachines. In
Proceedingsofthe25thinternationalconferenceonMachinelearning,pp.536–543.ACM,2008. 3
LeCun,YannandCortes,Corinna. Themnistdatabaseofhandwrittendigits,1998. 2
Lee, Dong-Hyun. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural
networks. InWorkshoponChallengesinRepresentationLearning,ICML,2013. 3
Mnih,VolodymyrandHinton,GeoffreyE. Learningtolabelaerialimagesfromnoisydata. InProceedingsof
the29thInternationalConferenceonMachineLearning(ICML-12),pp.567–574,2012. 2
Nigam,KamalandGhani,Rayid. Analyzingtheeffectivenessandapplicabilityofco-training. InProceedings
oftheninthinternationalconferenceonInformationandknowledgemanagement,pp.86–93.ACM,2000.2
10AcceptedasaworkshopcontributionatICLR2015
Nigam,Kamal,McCallum,AndrewKachites,Thrun,Sebastian,andMitchell,Tom. Textclassificationfrom
labeledandunlabeleddocumentsusingem. Machinelearning,39(2-3):103–134,2000. 2
Ouyang,Wanli,Luo,Ping,Zeng,Xingyu,Qiu,Shi,Tian,Yonglong,Li,Hongsheng,Yang,Shuo,Wang,Zhe,
Xiong, Yuanjun, Qian, Chen, et al. Deepid-net: multi-stage and deformable deep convolutional neural
networksforobjectdetection. arXivpreprintarXiv:1409.3505,2014.
Reed,Scott,Sohn,Kihyuk,Zhang,Yuting,andLee,Honglak.Learningtodisentanglefactorsofvariationwith
manifoldinteraction. InProceedingsofThe31stInternationalConferenceonMachineLearning,2014. 8
Rifai,Salah,Bengio,Yoshua,Courville,Aaron,Vincent,Pascal,andMirza,Mehdi. Disentanglingfactorsof
variationforfacialexpressionrecognition. InComputerVision–ECCV2012,pp.808–822.Springer,2012.
8
Rosenberg,Chuck,Hebert,Martial,andSchneiderman,Henry. Semi-supervisedself-trainingofobjectdetec-
tionmodels. 2005. 2
Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng,
Karpathy,Andrej,Khosla,Aditya,Bernstein,Michael,Berg,AlexanderC.,andFei-Fei,Li.ImageNetLarge
ScaleVisualRecognitionChallenge,2014. 2
Salakhutdinov, RuslanandHinton, GeoffreyE. Deepboltzmannmachines. InInternationalConferenceon
ArtificialIntelligenceandStatistics,pp.448–455,2009. 3
Sermanet, Pierre, Eigen, David, Zhang, Xiang, Mathieu, Michae¨l, Fergus, Rob, and LeCun, Yann. Over-
feat: Integrated recognition, localization and detection using convolutional networks. arXiv preprint
arXiv:1312.6229,2013. 1
Smolensky,Paul. Informationprocessingindynamicalsystems:Foundationsofharmonytheory. 1986. 4
Sukhbaatar,SainbayarandFergus,Rob. Learningfromnoisylabelswithdeepneuralnetworks. arXivpreprint
arXiv:1406.2080,2014. 3
Susskind, JoshM,Anderson, AdamK,andHinton, GeoffreyE. Thetorontofacedatabase. Departmentof
ComputerScience,UniversityofToronto,Toronto,ON,Canada,Tech.Rep,2010. 2
Szegedy,C.,Reed,S.,Erhan,D.,andAnguelov,D. Scalable,High-QualityObjectDetection. ArXive-prints,
December2014. 1,8,9
Szegedy,Christian,Liu,Wei,Jia,Yangqing,Sermanet,Pierre,Reed,Scott,Anguelov,Dragomir,Erhan,Du-
mitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. arXiv preprint
arXiv:1409.4842,2014. 8
Whitney,MaxandSarkar,Anoop. Bootstrappingviagraphpropagation. InProceedingsofthe50thAnnual
MeetingoftheAssociationforComputationalLinguistics:LongPapers-Volume1,pp.620–628.Association
forComputationalLinguistics,2012. 2
Yarowsky, David. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings
of the 33rd annual meeting on Association for Computational Linguistics, pp. 189–196. Association for
ComputationalLinguistics,1995. 2,5,7
Zeiler, Matthew D and Fergus, Rob. Visualizing and understanding convolutional neural networks. arXiv
preprintarXiv:1311.2901,2013. 1
Zhu,Xiaojin. Semi-supervisedlearningliteraturesurvey. 2005. 2
11"
226,228,Unsupervised face normalization with extreme pose and expression in the wild,"['Y Qian', 'W Deng', 'J Hu']",2019,112,Expression in-the-Wild,machine learning,"in the wild with unpaired data. To this end, we propose a Face Normalization Model (FNM) to  generate a frontal, neutral expression Our FNM is an end-to-end deep learning model. FNM",No DOI,… of the IEEE/CVF Conference on …,https://openaccess.thecvf.com/content_CVPR_2019/papers/Qian_Unsupervised_Face_Normalization_With_Extreme_Pose_and_Expression_in_the_CVPR_2019_paper.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,thecvf.com,
227,229,Using colour local binary pattern features for face recognition,"['JY Choi', 'KN Plataniotis', 'YM Ro']",2010,103,Toronto Face Database,classification,"of a face image for FR purpose. We evaluate the proposed feature using three public face  databases: CMU-PIE,  As in any classification task, feature extraction is of prime importance in",No DOI,2010 IEEE International …,https://ieeexplore.ieee.org/document/5653653,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,ieee.org,
228,230,Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing,"['TP Jung', 'TJ Sejnowski']",2019,228,Affective Faces Database,"deep learning, machine learning","For each dataset, we first individually evaluate the emotion-classification performance   , we utilized these networks on face-images using a deep network pretrained on VGG-faces",No DOI,IEEE Transactions on Affective …,https://arxiv.org/abs/1905.07039,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,arxiv.org,"1
Utilizing Deep Learning Towards Multi-modal
Bio-sensing and Vision-based Affective Computing
Siddharth Student Member, IEEE, Tzyy-Ping Jung Fellow, IEEE, and Terrence J. Sejnowski Life Fellow, IEEE
Abstract—Inrecentyears,theuseofbio-sensingsignalssuchas further development of multi-modal bio-sensing systems i.e.
electroencephalogram(EEG),electrocardiogram(ECG),etc.have thosecapableofmonitoringandrecordingmultiplebio-signals
garnered interest towards applications in affective computing.
simultaneously [10], [11], [13], [12].
The parallel trend of deep-learning has led to a huge leap
Many research studies have shown that it is possible to
in performance towards solving various vision-based research
problems such as object detection. Yet, these advances in deep- recognize human emotions by the use of facial expressions
learninghavenotadequatelytranslatedintobio-sensingresearch. from images and videos [14], [15], [16], [17]. Advances in
This work applies novel deep-learning-based methods to various deep-learning have also made it possible to train large neural
bio-sensing and video data of four publicly available multi-
networks on big datasets for research in affective computing
modal emotion datasets. For each dataset, we first individu-
[18], [19], [20] apart from other problems such as object
ally evaluate the emotion-classification performance obtained by
each modality. We then evaluate the performance obtained by detection and classification [21], [22], [23]. Compared to the
fusing the features from these modalities. We show that our amount of deep-learning research that has translated towards
algorithms outperform the results reported by other studies solving problems involving images/videos, the deep-learning
for emotion/valence/arousal/liking classification on DEAP and
research conducted on bio-sensing data has been sparse. A
MAHNOB-HCI datasets and set up benchmarks for the newer
recent survey on using EEG for affective computing [24]
AMIGOS and DREAMER datasets. We also evaluate the per-
formance of our algorithms by combining the datasets and suggests that in almost all the cases the feature extraction and
by using transfer learning to show that the proposed method classification steps do not utilize deep neural networks.
overcomes the inconsistencies between the datasets. Hence, we There are chiefly three reasons limiting the use of deep-
do a thorough analysis of multi-modal affective data from
learning to bio-sensing modalities. First, it is easier to cre-
more than 120 subjects and 2,800 trials. Finally, utilizing a
ate an image/video database by collecting a huge amount convolution-deconvolution network, we propose a new technique
towardsidentifyingsalientbrainregionscorrespondingtovarious of image/video data with any decent camera (even that of
affective states. a smartphone) whereas the data collection of bio-signals
is often costly, time-consuming, and laborious. Second, the
Index Terms—Brain-Computer Interface (BCI), EEG, Multi-
modality, Bio-sensing, ECG, GSR, PPG, Computer Vision, Deep image/video datasets generated using different cameras are
Learning, Emotion Processing usually consistent or can easily be made so by changing
the frame resolution or modifying the number of frames
being captured per second without losing critical information
I. INTRODUCTION
in the process. On the other hand, commercially available
INrecentyears,therehasbeengrowinginteresttowardsap-
bio-sensing devices vary widely in terms of sampling rate,
proachingresearchinaffectivecomputingfrombio-sensing
analog to digital resolution, numbers of channels and sensor
perspective. To be sure, it is not just in affective computing
positioning [9], [24]. Furthermore, there are differences in the
thatresearchinbio-sensinghasbeengainingpopularity.Other
signal profiles between different types of bio-sensing signals
avenues of research such as health [1], [2], virtual reality [3],
suchasEEGvsECG.Third,visualizingimagedataforobject
robotics [4], [5], content rating [6], etc. have also exploited
detection or assessing emotions by looking at faces/body
bio-sensingasaresearchtool.Bio-sensingsystemsspecifically
postures in the images is much easier (such as for manual
those which are used to measure electrocardiogram (ECG),
tagging) and intuitive. But, extracting meaningful knowledge
electroencephalogram (EEG), galvanic skin response (GSR),
about various features from bio-sensing signals requires pre-
etc. have been around for decades. But, because of their
processing. Unlike image data, additional steps are required
bulkiness and complexity, they were restricted to controlled
in bio-sensing data to first filter the data of any noise such as
laboratory environments and hospitals. The current interest
due to motion artifacts or unwanted muscle activity.
in utilizing bio-sensing systems for various applications has
Using multiple bio-sensing modalities can be advantageous
beenmotivatedordrivenbythedevelopmentofwearablebio-
over using a singular one because the salient information in
sensing systems that make data collection faster and easier
the respective modalities may be independent of and com-
[7], [8], [9]. The advances in hardware have led to the
plementary to each other to some extent. Thus, together they
may enhance the performance for a given classification task
SiddharthiswiththeDepartmentofElectricalandComputerEngineering,
University of California San Diego, La Jolla, CA, 92093 USA, e-mail: [25], [26]. In most cases, the emotion-classification problem
ssiddhar@eng.ucsd.edu has been approached by measuring the arousal and valence
Tzyy-Ping Jung and Terrence J. Sejnowski are with Institute for Neural
as given by the emotion circumplex model [27]. It is evident
Computation,UniversityofCaliforniaSanDiego,LaJolla,CA,92093USA,
e-mail:jung@sccn.ucsd.edu,terry@salk.edu from various studies [24], [28], [29] that a single modality
9102
yaM
61
]GL.sc[
1v93070.5091:viXra2
TABLEI
TABLEHIGHLIGHTINGTHEINCONSISTENCIESAMONGTHEDATASETSANDSENSINGMODALITIESUSEDINTHISSTUDY
DEAPDataset[30] AMIGOSDataset[31] MAHNOB-HCIDataset[32] DREAMERDataset[33]
32subjects 40subjects 27subjects 23subjects
40 trials using music videos (trial 16trialsusingmovieclips(triallength 20trialsusingmovieclips(triallength 18trialsusingmovieclips(triallength
lengthfixedat60seconds) varyingbetween51and150seconds) varyingbetween34.9and117seconds) varyingbetween67and394seconds)
Rawandpre-processeddataavailable Rawandpre-processeddataavailable Onlyrawdataavailable Onlyrawdataavailable
32-channelEEGsystem(Twodifferent 14-channelEEGsystem(AsingleEEG 32-channelEEGsystem(AsingleEEG 14-channelEEGsystem(AsingleEEG
EEGsystemsused.Channellocations: system used for all subjects. Channel system used for all subjects. Channel system used for all subjects. Channel
Fp1,AF3,F7,F3,FC1,FC5,T7,C3, locations: AF3, F7, F3, FC5, T7, P7, locations:Fp1,AF3,F7,F3,FC1,FC5, locations: AF3, F7, F3, FC5, T7, P7,
CP1, CP5, P7, P3, Pz, PO3, O1, Oz, O1,O2,P8,T8,FC6,F4,F8,AF4) T7, C3, CP1, CP5, P7, P3, Pz, PO3, O1,O2,P8,T8,FC6,F4,F8,AF4)
O2, PO4, P4, P8, CP6, CP2, C4, T8, O1, Oz, O2, PO4, P4, P8, CP6, CP2,
FC6,FC2,F4,F8,AF4,Fp2,Fz,Cz) C4,T8,FC6,FC2,F4,F8,AF4,Fp2,
Fz,Cz)
— 2-channelECGsystem 3-channelECGsystem 2-channelECGsystem
1-channelPPGsystem — — —
1-channelGSRsystem 1-channelGSRsystem 1-channelGSRsystem —
Samplingrate128Hz Samplingrate128Hz Samplingrate256Hz SamplingrateEEG/ECG:128/256Hz
Facevideorecordedfor22of32sub- Face video recorded for all subjects Face video recorded for all subjects —
jects (EEG cap and EOG electrodes (Onlyasmallportionoftheforehead (Onlyasmallportionoftheforehead
occludes parts of the forehead and isoccludedbytheEEGsystem) isoccludedbytheEEGsystem)
cheeks)
3-seconds of pre-trial baseline data Nobaselinedataavailable. 30 seconds of pre-trial and post-trial 61 seconds of pre-trial baseline data
available. baselinedataavailable. available
Valence/Arousal/Liking rated using a Valence/Arousal/Liking rated using a Valence/Arousalratedusingadiscrete Valence/Arousalratedusingadiscrete
continuousscalebetween1to9 continuousscalebetween1to9 scaleofintegersfrom1to9 scaleofintegersfrom1to5
mayperformdifferentlyforarousalandvalenceclassification. whereas for MAHNOB-HCI and DREAMER datasets we
So, in theory, two modalities that show good performance in- perform the filtering and artifact removal before extracting
dependentlyforvalenceandarousalrespectively,mayperform features. The trials in the DEAP and AMIGOS datasets have
even better jointly for the emotion classification problem. been tagged by subjects for valence, arousal, liking, and
This study focuses on multi-modal data from both bio- dominance on a continuous scale of 1 to 9. For MAHNOB-
sensing and vision-based perspectives. We fuse the features HCI and DREAMER datasets, the valence and arousal have
with deep-learning-based methods and traditional algorithms been tagged on a discrete scale using integers from 1 to
for all modalities on four different datasets. We show that 9 and 1 to 5, respectively. We used the emotion circum-
using multi-modality is advantageousover singular modalities plex model [27] to divide the emotions into four categories
in various cases. Finally, we show that the deep-learning namely, High-Valence High-Arousal (HVHA), Low-Valence
methods perform well even when the size of the dataset High-Arousal (LVHA), Low-Valence Low-Arousal (LVLA),
is small. For each of the four datasets, we show that our and High-Valence Low-Arousal (HVLA). These categories
methods outperform previously reported results. To the best loosely map to happy/excited, annoying/angry, sad/bored, and
of our knowledge, this study contains the most exhaustive calm/peaceful emotions, respectively. For each dataset, the
use of multi-modal bio-sensing data for affective computing labels were self-reported by the subjects after the presentation
research. Our results also demonstrate the applicability of of the video stimuli.
deep-learning-based methods to overcome the discrepancies
between different modalities and even effectively fuse the As shown in Table I, the datasets differ in many aspects.
information from them, as shown by results from combining Hence, many traditional algorithms cannot be generalized
the datasets and transfer learning. acrossdatasetsbecauseofdifferencesinthenumberandnature
of extracted features. Apart from the types of the audio-
II. MATERIALSANDRELATEDWORK visual stimulus (music videos vs. movie clips), the datasets
We designed and evaluated our framework on four publicly vary in the trial duration and baseline data availability. The
available multi-modal bio-sensing and vision-based datasets DEAP dataset has trial length fixed at 60 seconds whereas
namelyDEAP[30],AMIGOS[31],MAHNOB-HCI[32],and for the AMIGOS dataset, the trial length varies between 51
DREAMER [33]. Table I briefly describes the four datasets, to 150 seconds. This varying trial length is significant since
with a focus on the modalities that we used in this work. For the longest video is about thrice the length of the shortest
DEAP and AMIGOS datasets, we used the preprocessed bio- one. Hence, if a particular emotion of the subject is invoked
sensing data that has been suitably re-sampled and filtered for 25 seconds during a trial, it will appear in half of the3
TABLEII
CLASSIFICATIONPERFORMANCEANDEVALUATIONPERFORMEDBYVARIOUSREPORTEDSTUDIESONTHEFOURDATASETS
Study UsedModalities ExtractedFeatures Classifier Evaluation
DEAPDataset
Liuetal.[28] EEG Fractal dimension SVM Only 22 of the 32 subjects used. 50.8% Valence (4-classes) and
(FD)based 76.51%Arousal/Dominance.
Yinetal.[34] EEG, ECG, EOG, Various MESAE 77.19% Arousal and 76.17% Valence (2-classes) using fusion of
GSR, EMG, Skin allmodalities.
temperature, Blood
volume,Respiration
Patrasetal.[30] EEG PSD BayesianClassifier 62%Valenceand57.6%Arousal(2-classes)
Chungetal.[36] EEG Various Bayesian weighted- 70.9%Valenceand70.1%Arousal(2-classes)
log-posterior
Shangetal.[37] EEG,EOG,EMG Rawdata DeepBeliefNetwork, 51.2%Valence,60.9%Arousal,and68.4%Liking(2-classes)
BayesianClassifier
Camposetal.[38] EEG Various Genetic algorithms, 73.14%Valenceand73.06%Arousal(2-classes)
SVM
AMIGOSDataset
Mirandaetal.[31] EEG,ECG,GSR Various SVM ∗57.6/53.1/53.5/57 Valence and 59.2/54.8/55/58.5 Arousal (2-
classes)usingEEG/GSR/ECGalone/EEG,GSR,andECGfusion.
MAHNOB-HCIDataset
Soleymanietal.[32] EEG, ECG, GSR, Various SVM 57/45.5/68.8/76.1%Valenceand52.4/46.2/63.5/67.7%Arousal(2-
Respiration, Skin classes)usingEEG/Peripheral/Eyegaze/FusionofEEGandgaze.
Temperature
Koelstraetal.[39] EEG,Faces Various Decision classifiers 73%Valenceand68.5%Arousal(2-classes)usingEEGandFaces
fusion fusion.
Alasaarelaetal.[40] ECG Various KNN 59.2%Valenceand58.7%Arousal(2-classes)
Zhuetal.[41] EEGandVideostim- Various SVM 55.72/58.16% Valence and 60.23/61.35% Arousal (2-classes) for
ulus EEGalone/VideostimulusasprivilegedinformationwithEEG.
DREAMERDataset
Stamosetal.[33] EEG,ECG PSD,HRV SVM 62.49/61.84%Valenceand62.17/62.32%Arousal(2-classes)using
EEGalone/EEGandECGfusion.
∗DenotesmeanF1-score.Accuracyvaluenotavailable.
trial in the shortest video but only in one-sixth of the trial subset of the trials of these datasets. We ignore such missing
in the longest one. Furthermore, the trial length variation entries in our evaluation.
of the DREAMER dataset is even greater than that in the
Table II shows that in almost all the cases EEG has been
AMIGOS dataset. There is also no baseline data present in
the preferred bio-sensing modality while vision modality i.e.
the AMIGOS dataset to compensate for the subject’s initial
the use of the frontal videos to analyze facial expressions has
emotional power (defined as the distance from the origin in
not been commonly used on these datasets. The classification
the emotion circumplex model). Different kinds of systems
accuracy for all emotion classes as per the circumplex model
have been used to collect the EEG data in these datasets. 32-
rather than only for arousal/valence are rarely reported. In
channel EEG in the DEAP and MAHNOB-HCI datasets may
other cases such as [29], [42], where the analysis of emotions
contain much more emotion-relevant information than the 14-
is reported, the goal seems to be clustering the complete
channel EEG in the AMIGOS and DREAMER datasets.
dataset into four classes rather than having a distinct training
OnlytheDEAPdatasetusesphotoplethysmogram(PPG)to and testing partition for evaluation.
measure heart rate instead of ECG. The use of PPG generally
In terms of accuracy, we see from Table II that using
loses the information that is present in the ECG waveform
multiplesensormodalities,thebestperformanceontheDEAP
such as QRS complex, PR, and ST segment lengths, etc.
datsetisby[34]whenutilizingdatafrommulitplemodalities.
The EEG electrodes introduce varying degrees of occlusion For the MAHNOB-HCI dataset, the best accuracy for valence
while capturing frontal videos of the subjects. This effect was and arousal is 73% and 68.5% respectively [39], which is
found to be more problematic in the DEAP dataset because again using multiple sensor modalities. The AMIGOS and
of the placements of the EOG electrodes on subjects’ faces. DREAMER datasets were released recently and hence only
Furthermore, some data are missing in some modalities in a baseline evaluation on these have been reported in Table II.4
This study will utilize complete datasets and not a subset steptothecalculationoftruedensityp(x),sincewhenNgoes
of them, as in some previous studies. We evaluate our meth- to infinity it can converge to the true density if δ(.) and h are
ods with disjoint partitions between training, validation, and properly chosen [45]. We chose δ(.) as the Gaussian window
test subsets of the complete datasets. Our evaluation is first
reported for all modalities separately (including using frontal (cid:18) zTΣ−1z(cid:19)(cid:30)(cid:110) (cid:111)
videosthatwereignoredbyotherstudies)andthencombining δ(z,h)=exp − (2π)d/2hd|Σ|1/2 (3)
2h2
them together. Since not all datasets and previous studies
report results on Dominance, we chose to classify valence, where z =x−x(i), Σ is the covariance of z, and d is the
arousal, liking and emotions as the affective measures. dimension of x. By plugging the value of d=1,2 we get the
marginaldensityp(x)andthedensityofthebivariatevariable
III. RESEARCHMETHODS (x,y),p(x,y) [45]. In this manner the mutual information
In this section, we detail various types of methods that we I(X;Y) is calculated which is related to conditional entropy
employed to extract features from each bio-sensing modality H(Y|X) by
and frontal videos.
I(X;Y)=H(Y)−H(Y|X) (4)
A. EEG feature extraction The conditional entropy between all possible pairs of EEG
For the DEAP and AMIGOS datasets, preprocessed EEG channels was calculated over the complete trial length [46].
data are available, bandpass-filtered between 4-45 Hz, and Hence,duetothedifferencesinthenumberofEEGchannels,
corrected for eye-blink artifacts. For the MAHNOB-HCI and 496conditionalentropyfeatureswerecalculatedfortheDEAP
DREAMERdatasets,weperformedthebandpassfilteringand andMAHNOB-HCIdatasetsand91featuresfortheAMIGOS
artifact removal using the Artifact Subspace Reconstruction and DREAMER datasets.
(ASR) toolbox [43] in EEGLAB [44]. The processed EEG 3) EEG-PSDimages-basedDeepLearningfeatures: Inthis
data were then converted into the frequency domain to extract section, we propose a novel method for feature extraction
both traditional and deep-learning based features (see below). from the EEG data, which is based on deep convolution
1) EEG-PSDfeatures: ForeachEEGchannel,weextracted networks without requiring a large amount of training data.
the traditional power spectral density (PSD) in three EEG The method can also work in a similar manner for different
bandsnamely,theta(4-7Hz),alpha(7-13Hz),andbeta(13-30 typesofEEGdatasetsi.e.datasetswithdifferentnumbersand
Hz). These EEG bands were chosen since they account most placementsofelectrodes,samplingrates,etc.Wefirstusedthe
towards human cognition. We used half second overlapping computedEEG-PSDfeaturesfromthefirstmethodmentioned
windows. The PSD was then averaged over the total trial above to plot power spectrum heat maps for the three EEG
length. Hence, because of the differences in the number of bands using bicubic interpolation to calculate the values in
EEG channels, we get 96 features for trials in the DEAP the 2-D plane. These images now contain the topographical
and MAHNOB-HCI datasets and 42 features for trials in the information for the three frequency bands according to the
AMIGOS and DREAMER datasets. standard EEG 10-20 system (Fig. 1). It is worth noting that
2) Conditionalentropyfeatures: Togetinformationregard- the commonly used EEG-PSD features in themselves do not
ing the interplay between different brain regions, we extract take into account the EEG-topography i.e. the locations of
conditional entropy-based features. The conditional entropy EEG electrodes for a particular EEG band. Hence, we try to
between two random variables carries information about the exploit EEG-topography to extract information regarding the
uncertainty in one variable given the other. Hence, it acts as a interplay between different brain regions. It is for exploiting
measureoftheamountofmutualinformationbetweenthetwo this information that we convert EEG data to an image-based
randomvariables.ThemutualinformationI(X;Y)ofdiscrete representation and utilize pre-trained deep-learning networks
random variables X and Y is defined as to extract such relationship between various brain regions.
(cid:18) (cid:19)
(cid:88) (cid:88) p(x,y)
I(X;Y)= p(x,y)log (1)
p(x)p(y)
x∈Xy∈Y
TheconditionalentropywillbezeroifthesignalYiscom-
pletely determined by signal X. To calculate the conditional
entropy, we first calculated the mutual information I(X;Y)
between the two signals, which requires the calculation of the Fig. 1. PSD heat-maps of theta (red), alpha (green), and beta (blue) EEG
bands being added according to respective color-bar range to get combined
approximate density function pˆ(x) of the following form
RGBheat-map(Imageborder,nose,ears,andcolor-barshavebeenaddedfor
visualizationonly.)
N
1 (cid:88)
pˆ(x)= δ(x−x(i),h) (2) As shown in Fig. 1, we used ‘Red’ color-map for the theta
N
i=1 band,‘Green’color-mapforthealphaband,and‘Blue’color-
whereδ(.)istheParzenwindow,histhewindowwidth,N map for the beta band. We then combined the three colored
is the samples of variable x and x(i) is the ith sample. This images into an RGB colored image [47]. Based on the ranges
approximate density function is calculated as an intermediary and maximum values in the data for the three EEG bands, we5
used the ratio of alpha blending [48] to give weights to the
three individual bands’ images before adding them together.
This color image carries information about how the power in
the three bands interacts with each other across the different
brain regions. For example, a yellow colored region has a
higher amount of power in the theta (red) and alpha (blue)
bands, whereas a pink colored region has high power in the
theta(red)andbeta(blue)bands.Inthismanner,asinglebrain
heat-map image can be used to represent spatial and spectral
information in the three EEG bands. That is, we obtained one
imagerepresentingthetopographicPSDinformationforevery
trial. Because the images in the four datasets can be formed
in a similar manner irrespective of the different numbers of
EEG channels and positions, we can use this method across
the four datasets easily.
We then used a pre-trained VGG-16 network [49] to
Fig.2. ForatrialfromDEAPdataset,PPGsignalwithpeaks(inred)being
extract features from the combined RGB heat-map image.
detectedforthecalculationofRRsandHRV(above),andPPGspectrogram
This network consists has been trained with more than a (below).
million images for 1,000 object categories using the Imagenet
Database [50]. It has been shown that a pre-trained VGG-16
The total number of peaks per minute represents the sub-
network can be utilized for feature extract and classification
ject’s heart rate. To calculate the HRV, the time differences
forapplicationsdifferentthanitwastrainedfor[51],[52].The
between successive peaks were calculated to get inter-beat
RGB image is resized to 224×224×3 size before submitting
intervals (RRs). These RRs were then used to compute HRV
to the network. The last but one layer of this VGG network
using the pNN50 method [56]. This method of HRV calcula-
consistsof4,096mostsignificantfeatures,whichweextracted
tion measures the percentage of successive RR intervals that
foremotionclassification.Principalcomponentanalysis(PCA)
differ by more than 50ms. This method has been shown to
was then applied to this feature space to reduce its dimension
be correlated with the activity of the parasympathetic nervous
to 30 for each trial [53]. We then combined the features from
system (PNS) [57]. For the datasets containing multiple ECG
thismethodandfromtheconditionalentropymentionedabove
channels, we performed the same procedure for all the chan-
for evaluation.
nels.
TheresultantEEG-PSDimagesfromtheabovemethodcan
2) Extracting deep-learning-based spectrogram features of
be used to denote how the EEG spectral activity is distributed
ECG/PPG: Previous studies have reported that frequency-
acrossvariousbrainregionsacrosstimeforaparticularkindof
domain features in the ECG work well for tasks such as
stimulus.Thiscanbedonebysendingsuchsuccessiveimages
ECG beat discrimination [58]. To exploit time-frequency in-
(varying across time) to a reverse deep-learning network and
formation from ECG/PPG, we extracted deep-learning based
detect the most salient features i.e. activated regions of the
features on ECG/PPG by converting the time-series data to a
brain across time for a particular stimulus. In the evaluation
frequency domain-based image representation. The frequency
sectionbelow,weutilizethesebrainimagestodenotethebrain
range of ECG/PPG signals is low and hence we focus only
regionsthataremostactivatedfordifferentaffectiveresponses.
on 0-5 Hz range. We generated a spectrogram [59] over the
complete trial in this frequency range as in Fig. 2. To get
B. ECG/PPG-based feature extraction
a good amount of variations, we chose Parula color map
Both ECG and PPG signals can be used to measure heart
for the spectrogram image. The frequency bins with various
rate (HR) and heart-rate variability (HRV), though ECG can
colors at different frequencies represent the signal across
provide more useful information due to its greater ability to
the trial length. We employed the same procedure to get
capture ECG t-wave etc. For consistency between the two
the spectrogram images of ECG/PPG signals from the four
types of signal measurements i.e. PPG and ECG, we employ
datasets.Wethenresizedthespectrogramimagestofeedthem
two methods in the same manner on data from both of these
into the VGG-16 network, and after which the resultant 4,096
modalities in the four datasets.
extractedfeatureswerereducedto30featuresusingPCA.The
1) HRV features: HRV has shown to be a good metric for
features from this method were concatenated with the HRV
classifying emotional valence and arousal [54]. For every trial
features from above for evaluation.
(whether PPG in the DEAP dataset or ECG in the AMIGOS,
MAHNOB-HCI, and DREAMER datasets), we first used a
C. GSR-based feature extraction
moving-averagefilterwithawindowlengthof0.25secondsto
filter out the noise in the data. We then used a peak-detection Similar to ECG/PPG, we employ two methods to extract
algorithm [55] after scaling the data between 0 and 1. The features from the GSR data, one in the time domain and the
minimum distance between successive peaks as being at least other in the frequency domain.
0.5 seconds apart was taken as the threshold to remove false 1) Statisticalfeatures: Wefirstusedamovingaveragefilter
positives as in Fig. 2. with a window length of 0.25 seconds to remove noise from6
Fig.3. NetworkarchitectureforEEG-PSDtrendbaseddeep-learningmethod.
the GSR signal. For each trial, we calculated eight statistical
features from the time-domain GSR data. Two features are
the number of peaks and the mean of absolute heights of the
peaks in the signal. Six more statistical features based on nth
ordermomentsoftheGSRtime-seriesdatawerecalculatedas
shownin[60].Thesefeaturesmeasurethetrendi.e.variations
in the GSR data in actual, and successive first and second
differences of the signal.
2) Extracting deep-learning-based spectrogram features of Fig. 4. Detected face (marked in red) and face localized points (marked
in green) in DEAP Dataset (left), AMIGOS Dataset (center), and subset of
GSR: GSR signals change very slowly and hence we focus
features(markedinyellow)computedusingfacelocalizedpoints(right).The
only on the 0-2 Hz frequency range. Similar to ECG, we featuresarenormalizedusingheight(H)andwidth(W)ofthedetectedface.
generated the spectrogram image of GSR for each trial in the Thesesubjects’consenttousetheirfaceismarkedinrespectivedatasets.
above frequency range. We then extracted VGG-16 network-
based features that characterize the most meaningful interac-
tionsbetweenvariousedgesinthespectrogram.Thesefeatures
werereducedto30usingPCAandthenconcatenatedwiththe of the eyebrow to the center of the eye, between the nose and
time-domain GSR features from above. the middle part of the upper lip, between upper and lower
lips, etc. Many of the 30 features are described in [14] while
othersbydesignedbyhand.Allsuchfeatureswerenormalized
D. Frontal video-based feature extraction
based on the height and width of the detected face to remove
Unlike other studies in Table II, we also use the frontal variations due to the distance from the camera. The mean,
videos of the subjects for emotion/valence/arousal/liking clas- 95th percentile (more robust than maximum), and standard
sification.Facialexpressionscanbeveryreliableindicatorsof deviation of these 30 features across the frames in a single
one’s emotions based on his/her personality i.e. willingness to trial are then calculated. These 90 parameters were then used
show emotions by various facial expressions. For each frontal for evaluation.
video trial, we first extracted a single frame for every second
inthetrialbyextractingthefirstframeforeverysecondofthe 2) DeepLearning-basedfeatures: Theuseofdeep-learning
video.Weexcludedtheextremeendsoftheimageandplaceda has transformed computer vision in multiple ways. This is
threshold on the minimum face size to be 50×50 pixels. This because such deep networks are capable of extracting fea-
was done to reduce computational complexity and increase ture representations from images that capture both uniform
face detector’s accuracy. Face detection was done using Haar- (contrast etc.) and complex (small changes in texture etc.)
likefeaturesbasedonViola-Jonesobjectdetector[61].Asmall types of features. Hence, we utilized these networks on face-
portion of images had a majority of the face occluded due to imagesusingadeepnetworkpre-trainedonVGG-facesdataset
subject putting his/her hand over their face. The face detector [63]. The extracted face region was resized to 224×224×3
failed on these instances and hence these were discarded. for each selected frame in the trial. Similar to the CNN-
1) Facialpointslocalizationbasedfeatures: Weappliedthe based deep-learning method used above for the bio-sensing
state-of-the-art Chehra algorithm [62] to the extracted facial modalities, we extracted 4,096 most meaningful features on
regions to obtain 49 localized points on the face representing these resized images. But, unlike the bio-sensing method, we
thesignificantpartsasshowninFig.4.Thisalgorithmdoesnot employed a different VGG network that has been specifically
need any human input or a dataset-specific training model for trained on more than 2.6 million face images from more than
predicting the localized face points, making the process fully 2,600 people [63]. This was done to extract features that are
automated. Previous research studies have reported promising more relevant to the face-dependent feature space. The mean,
results using the face action units (AUs) based on such facial 95th percentile, and standard deviation of the features across
landmarks[14].Weusedthese49localizedpointstocalculate the images in every trial were computed and the subsequent
30featuresbasedondistancessuchasthatbetweenthecenter features space was reduced to 30 using PCA.7
E. Dynamics of the EEG/Face features using deep-learning
The above-mentioned methods for extracting deep-learning
features from EEG/face-videos are special cases in which
a single trial is represented by a single image (EEG-PSD
image/Single feature space for face images in a video). But,
these methods do not fully take into account the temporal
dynamics of the features over time within the trial. Hence,
we propose a new method in which such images (EEG-PSD
or face region) are utilized for every second within a trial.
Fig. 3 shows the network architecture for this method for the
EEG-PSD images. Multiple EEG-PSD images were formed
for each trial by generating one image for each second, all
of which went through the pre-trained VGG network. The
4,096 features from the off-the-shelf deep-learning network
werethenobtainedforeachimage.Inaddition,theconditional
entropy features for every second were also calculated. PCA
was then used to reduce the dimensionality of the feature
space comprising features from EEG and face-videos. The
resultant feature space has 60 most representative features. Fig.5. Distributionofemotionclassesinthefourdatasets.
These 60×N (N = trial length in seconds) features are then
sent to a Long-Short Term Memory (LSTM) network [64].
in Table V. Finally, we used transfer learning among the
However, this method could only be employed on the DEAP
datasets i.e. training on one dataset and testing on another
dataset since the AMIGOS and MAHNOB-HCI datasets have
(Table VI). For these two latter evaluations of combining
varying trial length and DREAMER dataset does not contain
the datasets and using transfer learning, we randomly divide
any video data. The huge variations in the trial length in the
the datasets into two parts with an 80/20 ratio and perform
AMIGOS and MAHNOB-HCI datasets meant that during the
10-fold cross-validation. The classification was done using
data preparation phase of LSTM, a large amount of padding
extremelearningmachines(ELM)[65]withvariablenumbers
was needed. This may be possible in data from physical
of neurons, which has been shown to perform better than
sensors (like temperature, luminous, pressure, etc.) where
support vector machines (SVM) for various cases [35]. All
interpolation is easy to perform. But, for bio-signals, this is
the features were re-scaled between -1 and 1 before training
not desirable because we do not have affective labels reported
the ELM. A single-layer ELM was used with a sigmoid
by the subject during the course of each video trial i.e. we do
activationfunction.Forthetrend-baseddeep-learningmethod,
not know which parts of the video contributed most towards
twohiddenlayersintheLSTMwereusedwiththenumberof
the affective response. Hence, we could not use LSTMs on
neurons being 200 and 100, respectively. Stochastic gradient
these datasets.
descent with a momentum (SGDM) optimizer was used to
train the LSTM network.
IV. EVALUATION
A. Visualizing class-separability using the traditional vs.
In this section, we evaluate the various feature-extraction
deep-learning features
methods described above. First, we compare the performance
of the classification of affective states using the deep-learning One of the hypotheses of our study is that the traditional
features from the pre-trained convolution network with that methods for analyzing EEG can be improved by using deep-
using traditional EEG features. We also report the classi- learning based features obtained from pre-trained convolution
fication performance when features from these modalities networks. This is important because training convolution net-
are fused together. Thereafter, we evaluate the classification works requires huge datasets, which are usually unavailable
performance using each modality individually on the four in the bio-sensing domain. Hence, the Deep-Learning method
datasets, on combining the datasets together, and for transfer described in section III.A3 should be able to extract more
learning. Finally, we present results for a novel deep-learning meaningful features from EEG-PSD features (III.A1). We
based technique to identify the most important brain regions used t-SNE [66] to visualize the dimensionally reduced space
associated with emotional responses. usingtraditionalEEG-PSDfeaturesfor2-classvalenceand4-
Fig. 5 shows the distribution of self-reported valence and class emotions on the DEAP dataset with fixed trial length.
arousalforthefourdatasets.ItisevidentthattheDEAPdataset Kullback-Leibler (KL) divergence was used for measuring
has a higher concentration of trials closer to neutral emotion similarity and Euclidean distance was used as the distance
i.e. near the center of the graph. For each individual dataset measure for the t-SNE implementation. We then applied the
separately, we perform leave-one-subject-out evaluation and same approach to the features obtained by the VGG network,
show results for single modality classification in Table III and which were computed after using the EEG-PSD features to
multi-modalityclassificationinTableIV.Then,weperformed create a combined RGB image in Fig. 1. Fig. 6 shows
anevaluationbycombiningdatasetstogetherandshowresults that trials in both 2-valence and 4-emotion classes can be8
separatedtoabetterdegree(althoughnotoptimal)whenusing TABLEIII
the VGG features from the EEG-PSD combined image than INDIVIDUALMODALITYPERFORMANCEEVALUATION
directly using the EEG-PSD features. The EEG-PSD features
Response EEG Cardiac GSR Face-1 Face-2
only form distinct clusters for each subject and are unable DEAPDataset
to separate the valence/emotion classes whereas the VGG Valence 71.09/0.68 70.86/0.69 70.70/0.68 71.08/0.68 72.28/0.70
Arousal 72.58/0.65 71.09/0.63 71.64/0.65 72.21/0.65 74.47/0.68
features allow for better separation.
Liking 74.77/0.65 74.77/0.64 75.23/0.64 75.60/0.62 76.69/0.62
Emotion 48.83/0.26 45.55/0.31 45.94/0.25 43.52/0.28 46.27/0.27
AMIGOSDataset
Valence 83.02/0.80 81.89/0.80 80.63/0.79 80.58/0.77 77.28/0.74
Arousal 79.13/0.74 82.74/0.76 80.94/0.74 83.10/0.76 77.28/0.72
Liking 85.27/0.81 82.53/0.77 80.47/0.72 80.27/0.72 79.81/0.72
Emotion 55.71/0.30 58.08/0.36 56.41/0.34 57.74/0.28 56.79/0.27
MAHNOB-HCIDataset
Valence 80.77/0.76 78.76/0.73 78.98/0.73 83.04/0.79 85.13/0.82
Arousal 80.42/0.72 78.76/0.74 81.84/0.75 82.15/0.77 81.57/0.76
Emotion 57.86/0.33 57.23/0.35 57.84/0.32 60.41/0.35 63.42/0.35
DREAMERDataset
Valence 78.99/0.75 80.43/0.78 — — —
Arousal 79.23/0.77 80.68/0.77 — — —
Emotion 54.83/0.33 57.73/0.36 — — —
(a) t-SNE on two valence classes (low-valence in blue and high-valence in
Cardiac features refer to features extracted using PPG in the DEAP dataset
red)
andusingECGintheAMIGOS,MAHNOB-HCI,andDREAMERdatasets.
Face-1andFace-2refertothemethodsIIID.1andIIID.2respectively.Valence,
Arousal, and Liking have been classified into two classes (50% chance
accuracy)whereasEmotionhasbeenclassifiedintofourclasses(25%chance
accuracy). All values denote the mean percentage accuracy followed by the
mean F1-score (separated by “/”) whereas missing values represent missing
modalitydata.
the previous best results obtained by using only individual
modalitiesfortheDEAPandMAHNOB-HCIdatasetsandthe
baselineaccuraciesfortheAMIGOSandDREAMERdatasets.
Furthermore,wefindhigheraccuracyforLikingclassification
than for Valence/Arousal for DEAP and AMIGOS datasets,
(b) t-SNE on four emotion classes (HVHA in blue, LVHA in red, LVLA in suggesting that it might be easier for subjects to rate their
magenta,andHVLAingreen)
likenessforthevideocontentsthanratingvalenceandarousal.
Fig. 6. Visualization of feature spaces using t-SNE [66] in trials from the
This is understandable since the latter terms are difficult to
DEAPdatasetontheEEG-PSDfeaturesandtheVGGfeaturesderivedfrom
thecombinedRGBimage.TheVGGfeaturesallowforbetterseparation. comprehend than Liking and depend highly on the physiolog-
ical baseline of the subject at any particular time.
B. Evaluating individual modality performance C. Evaluating multi-modality performance
This section presents the classification results obtained by This section presents the results of combining different
using individual modalities on the four datasets. Table III modalities for affective state classification. Specifically, as
shows accuracy and mean F1-score results for individual shown in Table IV, we first combine the three bio-sensing
modalities. modalities (only two for the DREAMER since it does not
It is clear from Table III that our results in multiple cate- containGSRdata)toevaluatetheirjointperformanceandthen
gories for all the four datasets are better than those reported the EEG and Face-video modalities through the CNN-VGG-
previously,asshowninTableII.TheCNNbasedfeaturesthat extracted features. Finally, for the DEAP dataset, we present
we extracted for all modalities contribute most towards this theresultsoftraininganLSTMnetworkwiththetime-varying
classificationimprovementforallthemodalities.Furthermore, featuresfromtheEEGandFace-videomodalities(seeSection
for all four datasets and for all modalities, the performance is III.E).
substantially greater than the chance accuracy. EEG proves to In almost all cases, we find that combining features from
be the best performing bio-sensing modality whereas Cardiac multiple modalities increases classification accuracy. The fu-
and GSR features also perform very well despite containing sion of features from bio-sensing modalities increases the
fewer channels. Furthermore, the frontal-video-based showed accuracy in many cases for all the four datasets. We also
high accuracy in the affective classification for the three note that by training the LSTM network with the features
datasets and surpassed the accuracy obtained by the bio- from EEG and Face-video modalities not only increases the
sensing modalities in many cases. accuracyascomparedtotheindividualmodalities(fromTable
The classification performance using various modalities III) but also outperform the best accuracy on the DEAP
even for varying trial length is consistently better than that dataset reported in Table II. For AMIGOS, MAHNOB-HCI,
reported in previous studies (Table II). Our results surpass andDREAMERdatasets,weseethatusingmultiplemodalities9
TABLEIV TABLEV
MULTI-MODALITYPERFORMANCEEVALUATION COMBINEDDATASETPERFORMANCEEVALUATION
Response Bio- EEGand EEG Previous Response EEG Cardiac GSR Face-1 Face-2
sensing Face and Face Best DEAP+AMIGOSCombinedDataset
(LSTM) Accuracy Valence 62.80/0.58 59.69/0.59 59.64/0.58 63.04/0.62 62.38/0.62
DEAPDataset Arousal 62.27/0.61 63.61/0.61 61.98/0.62 67.66/0.65 68.65/0.66
Valence 71.87/0.68 73.94/0.69 79.52/0.70 77.19 Liking 69.13/0.59 69.27/0.61 69.27/0.55 67.99/0.64 68.65/0.64
Arousal 73.05/0.68 74.13/0.66 78.34/0.69 76.17 Emotion 37.47/0.27 37.50/0.22 37.24/0.31 40.92/0.36 42.24/0.36
Liking 75.86/0.69 76.74/0.63 80.95/0.70 68.40 DEAP+AMIGOS+MAHNOB-HCICombinedDataset
Emotion 49.53/0.27 48.11/0.28 54.22/0.31 50.80 Valence 61.24/0.60 58.57/0.59 58.98/0.57 61.59/0.61 62.56/0.63
AMIGOSDataset Arousal 65.15/0.63 61.84/0.61 61.02/0.59 65.94/0.65 67.15/0.66
Valence 83.94/0.82 78.23/0.74 — — Emotion 40.21/0.35 36.33/0.31 35.71/0.28 42.51/0.33 43.00/0.32
Arousal 82.76/0.76 81.47/0.72 — —
Liking 83.53/0.77 81.49/0.75 — — The DEAP + AMIGOS combined dataset consists of the data from 72
Emotion 58.56/0.40 58.02/0.29 — — subjectsandmorethan1,900trials.TheDEAP+AMIGOS+MAHNOB-HCI
MAHNOB-HCIDataset combineddatasetconsistsofthedatafrom99subjectsandmorethan2,400
Valence 80.36/0.75 85.49/0.82 — 73.00 trials.Onlythedeep-learning-basedmethodsareusedforextractingfeatures
Arousal 80.61/0.71 82.93/0.77 — 68.50 for evaluation from various modalities because these can be extracted from
Emotion 58.07/0.30 62.07/0.35 — — alldatasetsinthesamemanner.
DREAMERDataset
Valence 79.95/0.77 — — 62.49
Arousal 79.95/0.77 — — 62.32 TABLEVI
Emotion 55.56/0.33 — — — TRANSFERLEARNINGPERFORMANCEEVALUATION
Response EEG Cardiac GSR Face-1 Face-2
Bio-sensing refers to combining features from EEG, ECG/PPG, and GSR
DEAP+AMIGOS(TrainDataset),MAHNOB-HCI(TestDataset)
signals.
Valence 63.55/0.60 64.77/0.54 64.96/0.55 55.02/0.52 62.01/0.62
EEG + Face refers to combining features from EEG- and video-based
Arousal 58.37/0.55 62.50/0.52 62.50/0.52 59.32/0.54 58.60/0.58
modalities.
Emotion 36.65/0.32 39.58/0.28 38.64/0.28 36.38/0.39 34.05/0.37
EEG+Face(LSTM)referstocombiningfeaturesfromEEG-andvideo-based
DEAP(TrainDataset),MAHNOB-HCI(TestDataset)
modalitiesforeverysecondinthetrialtotrainanLSTMmodel.Duetothe
Valence 62.70/0.54 63.59/0.46 65.19/0.47 56.48/0.49 59.86/0.59
triallengthvaryingwidelyintheAMIGOSandMAHNOB-HCIdatasets,the
Arousal 61.99/0.55 61.46/0.48 63.23/0.52 59.33/0.56 61.99/0.60
LSTM-basedmethodcouldnotbeappliedtothem.TheDREAMERdataset
Emotion 35.88/0.23 38.01/0.24 39.08/0.24 33.57/0.33 32.50/0.22
doesnothavevideodata.
Only the deep-learning-based methods are used for extracting features for
evaluation from various modalities because these can be extracted from all
datasetsinthesamemanner.
D. Evaluatingtheclassificationperformanceusingcombining
datasets and transfer learning
To show that the proposed deep-learning-based features are
independent of the number of EEG channels, trial length, the
image resolution of the video, ECG/PPG cardiac modality,
etc., we trained the ELM classifier with data from more than
Fig.7. ConfusionmatrixforDEAPDatasetvalenceusingEEGalone(left),
one datasets. We also use a transfer-learning approach to train
Faces alone (middle), and EEG + Faces (right) for a sample 80/20 dataset
distribution. the ELM classifier with data from some of the four datasets
and then test it against the remaining dataset. The combined
datasetswererandomlydividedintoan80:20ratiofortraining
and testing. This allows us to verify how scalable our feature
outperform single-modality accuracy in many cases and sets extraction methods across datasets having discrepancies in
up new benchmarks by beating previous best results. As an recording devices (e.g. ECG vs PPG) and parameters (e.g.
example of the increased performance by combining multiple channel numbers). Table V shows that despite all these dis-
modalities over single modalities, we also show a confusion crepancies across the datasets, our methods work well and
matrix of the classification in Fig. 7. We also performed two- always perform considerably better than the chance accuracy
sample t-Test between Bio-Sensing and EEG plus Face multi- and the baseline accuracies for individual datasets [30], [31],
modal combinations for the datasets. The p-values of the t- [32] reported in Table II. Table VI shows the results of
Test analysis for the valence, arousal, liking, and emotion training with two datasets and testing on the third. The above
classification for the DEAP dataset were 0.676, 0.543, 0.939 combinations of datasets were chosen because all the sensor
and 0.347 respectively. The p-values for the valence, arousal, modalities were used in the datasets and the DEAP dataset
liking, and emotion classification for the AMIGOS dataset contains more trials (1,280 trials) than the other two datasets
were 0.003, 0.266, 0.134 and 0.026 respectively. The p-values combined together (AMIGOS and MAHNOB-HCI containing
for the valence, arousal, and emotion classification for the 640 and 540 trials respectively). Even when we test the ELM
MAHNOB-HCI Dataset were 0.0134, 0.293 and 0.149. We on a dataset, the trials from which were not used for training,
could not perform similar t-Test on the DREAMER dataset theresultswereconsistentlybetter(moresoforECG/PPGand
since it does not contain Video (Face) modality data. GSR modalities) than many previous studies and far above10
the chance accuracy. The slight decrease in performance for
somemodalitiescomparedtothosetrainedwiththedatafrom
the same dataset might be due to two factors, namely, the
varying trial length between the datasets and only using the
VGG-based features common to the datasets (for consistency
among the datasets) as opposed to combining features from
othermethodslikeconditionalentropy,HRV,face-localization,
etc.
Fig.9. Convolution-DeconvolutionnetworkonEEG-PSDimagestoidentify
salient brain regions corresponding to affective states. Pixels in individual
imageswerescaledbetween0and1.
accordingtotheEEG10-20system.Thisisconsistentwiththe
textbookevidenceregardingtheprocessingofhumanemotions
Fig. 8. Salient brain regions corresponding to low/high valence/arousal in [68], [69]. More interestingly, we observe from the difference
DEAPdataset.Thefrontallobehashighactivation.
image between high and low arousal that the processing of
arousal affective state is much more widely distributed across
the brain than valence. Hence, this method allows us to use
E. Identifyingthesalientbrainregionsthatcontributetowards
a single image to represent such areas across the brain, and
processing various emotions
across all subjects and trials, that are most activated for a
Asisclearfromtheperformanceevaluationsectionsabove,
particular affective measure rather than using multiple such
the deep-learning-based methods are able to extract more
EEG images. We present these results as a starting point to
meaningful features and perform better than traditional fea-
take this work towards using the EEG for investigating the
tures. This section aims to explore what insights the proposed
generation and processing of emotions inside the brain.
deep-learning-based method can provide on the brain regions
contributing to emotional responses. To this end, we added a
reverse VGG network (before the final max pooling step) to
V. DISCUSSIONANDCONCLUSION
the pre-trained VGG network that extracted the informative Advances in deep-learning have not translated into bio-
features we used above. That is, we added a deconvolving sensingandmulti-modalaffectivecomputingresearchdomain
network to the convolving network. As shown in [67], the mostly due to the absence of very-large-scale datasets. Such
convolution-deconvolutionnetworkcanbeusedtoidentifythe datasets are available for vision/audio modalities due to the
most salient areas in the images in both static and dynamic easeofdatacollection.Hence,forthetimebeing,itseemsthat
manner.Weutilizethisnetwork(Fig.8)todetectthoseregions the only viable solution is to use “off-the-shelf” pre-trained
in the EEG-PSD brain images that contribute most towards deep-learning networks to extract features from bio-sensing
processing various emotions. The pairs of EEG-PSD images modalities. The proposed methods present the advantages of
for consecutive seconds for a trial I ,I were sent to the being scalable and able to extract features from different
t t+1
dynamic convolution-deconvolution network along with the datasets. Such “off-the-shelf” features prove to work better
output of the static saliency for the image at I . The static than the traditionally used features of various bio-sensing
t
saliency network identified the most salient areas whereas the modalities.
dynamic saliency network was able to learn the variations This study proposed novel methods to affective computing
between these image areas for every consecutive second. This research by employing deep-learning features across various
procedure was done for every second for all the trials. We modalities.Weshowedthatthesemethodsperformbetterthan
report results only from the DEAP dataset for this method previously reported results on four different datasets contain-
because of its fixed trial length. ing various recording discrepancies. The methods were also
We used the RGB combined images (Figure 1) for ev- evaluated on the combined datasets. Furthermore, the various
ery second for every trial of the low/high valence/arousal modalities were fused to augment the performance of our
instances from the DEAP dataset by first convolving and then models. The LSTM was used to learn the temporal dynamics
deconvolving them in the network described above. Hence, of the features during stimulus presentation and increase the
theoretically, the areas with most salient variations across classification accuracy, compared to averaging the features
the trials would represent the brain regions that are most across that trial. We also showed that features extracted from
receptive to the particular affective state. Fig. 9 shows the bio-sensing modalities such as EEG can be combined with
brain activity for these affective states after averaging the those from the video-based modality to increase the accuracy
output of the network across all the trials for the affective further. In the future, we will investigate the elicitation of
state (valence/arousal). Most of the activity is over the frontal emotions and its dependence on the physiological baseline.
andcentrallobesaroundtheFC3,FCz,FC4,andCzlocations We also plan to work on “real-world” emotion recognition11
problems where the subjects are mobile while responding to [16] Yeasin, M., Bullot, B. and Sharma, R., 2006. Recognition of facial
audio-visual stimuli present in the environment as opposed to expressions and measurement of levels of interest from video. IEEE
TransactionsonMultimedia,8(3),pp.500-508.
being displayed on a screen in a well-controlled laboratory.
[17] Poria, S., Cambria, E., Hussain, A. and Huang, G.B., 2015. Towards
an intelligent framework for multimodal affective data analysis. Neural
ACKNOWLEDGMENT Networks,63,pp.104-116.
[18] Kahou,S.E.,Pal,C.,Bouthillier,X.,Froumenty,P.,Glehre,.,Memise-
Theauthorswouldliketothanktheresearchgroupscollect-
vic, R., Vincent, P., Courville, A., Bengio, Y., Ferrari, R.C. and Mirza,
ing and disseminating datasets used in this research and for M.,2013,December.Combiningmodalityspecificdeepneuralnetworks
granting us access to the datasets. This work was supported for emotion recognition in video. In Proceedings of the 15th ACM on
Internationalconferenceonmultimodalinteraction(pp.543-550).ACM.
in part by the Army Research Laboratory under Cooperative
[19] Yu, Z. and Zhang, C., 2015, November. Image based static facial ex-
AgreementNumberW911NF-10-2-0022,NSFNCS-1734883, pressionrecognitionwithmultipledeepnetworklearning.InProceedings
NSF 1540943, and a seed grant from UC San Diego Center ofthe2015ACMonInternationalConferenceonMultimodalInteraction
(pp.435-442).ACM.
for Wearable Sensors.
[20] Gudi, A., Tasli, H.E., Den Uyl, T.M. and Maroulis, A., 2015, May.
Deeplearningbasedfacsactionunitoccurrenceandintensityestimation.
REFERENCES In Automatic Face and Gesture Recognition (FG), 2015 11th IEEE
InternationalConferenceandWorkshopson(Vol.6,pp.1-5).IEEE.
[1] Bamidis, P.D., Papadelis, C., Kourtidou-Papadeli, C., Pappas, C. and
[21] Redmon,J.,Divvala,S.,Girshick,R.andFarhadi,A.,2016.Youonly
B. Vivas, A., 2004. Affective computing in the era of contemporary
lookonce:Unified,real-timeobjectdetection.InProceedingsoftheIEEE
neurophysiology and health informatics. Interacting with Computers,
conferenceoncomputervisionandpatternrecognition(pp.779-788).
16(4),pp.715-721.
[22] Krizhevsky,A.,Nair,V.andHinton,G.,2014.TheCIFAR-10dataset.
[2] Coyle, S., Wu, Y., Lau, K.T., Brady, S., Wallace, G. and Diamond, D.,
online:http://www.cs.toronto.edu/kriz/cifar.html.
2007.Bio-sensingtextiles-wearablechemicalbiosensorsforhealthmoni-
[23] Deng, L., 2012. The MNIST database of handwritten digit images for
toring.In4thInternationalWorkshoponWearableandImplantableBody
machine learning research [best of the web]. IEEE Signal Processing
SensorNetworks(BSN2007)(pp.35-39).Springer,Berlin,Heidelberg.
Magazine,29(6),pp.141-142.
[3] Riva, G., Mantovani, F., Capideville, C.S., Preziosa, A., Morganti, F.,
Villani, D., Gaggioli, A., Botella, C. and Alcaiz, M., 2007. Affective [24] Alarcao,S.M.andFonseca,M.J.,2017.EmotionsrecognitionusingEEG
interactionsusingvirtualreality:thelinkbetweenpresenceandemotions. signals:asurvey.IEEETransactionsonAffectiveComputing.
CyberPsychology&Behavior,10(1),pp.45-56. [25] Udovii, G., erek, J., Russo, M. and Sikora, M., 2017, October. Wear-
[4] LaFleur,Karl,etal.“Quadcoptercontrolinthree-dimensionalspaceusing able Emotion Recognition System based on GSR and PPG Signals.
anoninvasivemotorimagery-basedbraincomputerinterface.”Journalof In MMHealth 2017: Workshop on Multimedia for Personal Health and
neuralengineering10.4(2013):046003. HealthCare.
[5] Milln,J.R.,Renkens,F.,Mourino,J.andGerstner,W.,2004.Noninvasive [26] Soleymani,M.,Pantic,M.andPun,T.,2012.Multimodalemotionrecog-
brain-actuatedcontrolofamobilerobotbyhumanEEG.IEEETransac- nition in response to videos. IEEE transactions on affective computing,
tionsonbiomedicalEngineering,51(6),pp.1026-1033. 3(2),pp.211-223.
[6] Samant, S.S., Chapko, M.J. and Seo, H.S., 2017. Predicting consumer [27] Russell,J.A.,1980.Acircumplexmodelofaffect.Journalofpersonality
likingandpreferencebasedonemotionalresponsesandsensorypercep- andsocialpsychology,39(6),p.1161.
tion:Astudywithbasictastesolutions.FoodResearchInternational,100, [28] Liu, Y. and Sourina, O., 2012, September. EEG-based valence level
pp.325-334. recognitionforreal-timeapplications.InCyberworlds(CW),2012Inter-
[7] Duvinage,M.,Castermans,T.,Petieau,M.,Hoellinger,T.,Cheron,G.and nationalConferenceon(pp.53-60).IEEE.
Dutoit,T.,2013.PerformanceoftheEmotivEpocheadsetforP300-based [29] Gonuguntla, V., Mallipeddi, R. and Veluvolu, K.C., 2016, August.
applications.Biomedicalengineeringonline,12(1),p.56. Identificationofemotionassociatedbrainfunctionalnetworkwithphase
[8] Poh, M.Z., Swenson, N.C. and Picard, R.W., 2010. Motion-tolerant lockingvalue.InEngineeringinMedicineandBiologySociety(EMBC),
magneticearringsensorandwirelessearpieceforwearablephotoplethys- 2016IEEE38thAnnualInternationalConferenceofthe(pp.4515-4518).
mography.IEEETransactionsonInformationTechnologyinBiomedicine, IEEE.
14(3),pp.786-794. [30] Koelstra,S.,Muhl,C.,Soleymani,M.,Lee,J.S.,Yazdani,A.,Ebrahimi,
[9] Ehmen, H., Haesner, M., Steinke, I., Dorn, M., Gvercin, M. and T.,Pun,T.,Nijholt,A.andPatras,I.,2012.Deap:Adatabaseforemotion
Steinhagen-Thiessen, E., 2012. Comparison of four different mobile analysis; using physiological signals. IEEE Transactions on Affective
devices for measuring heart rate and ECG with respect to aspects of Computing,3(1),pp.18-31.
usability and acceptance by older people. Applied ergonomics, 43(3),
[31] Miranda-Correa,J.A.,Abadi,M.K.,Sebe,N.andPatras,I.,2017.AMI-
pp.582-587.
GOS:AdatasetforMood,personalityandaffectresearchonIndividuals
[10] Wyss, T., Roos, L., Beeler, N., Veenstra, B., Delves, S., Buller, M.
andGrOupS.arXivpreprintarXiv:1702.02510.
andFriedl,K.,2017.Thecomfort,acceptabilityandaccuracyofenergy
[32] Soleymani, M., Lichtenauer, J., Pun, T. and Pantic, M., 2012. A
expenditureestimationfromwearableambulatoryphysicalactivitymoni-
multimodal database for affect recognition and implicit tagging. IEEE
toringsystemsinsoldiers.JournalofScienceandMedicineinSport,20,
TransactionsonAffectiveComputing,3(1),pp.42-55.
pp.S133-S134.
[33] Katsigiannis,S.andRamzan,N.,2017.Dreamer:Adatabaseforemotion
[11] Lei, J., Sala, J. and Jasra, S.K., 2017. Identifying correlation between
recognition through eegand ecg signals from wirelesslow-cost off-the-
facial expression and heart rate and skin conductance with iMotions
shelfdevices.IEEEjournalofbiomedicalandhealthinformatics.
biometric platform. Journal of Emerging Forensic Sciences Research,
[34] Yin,Z.,Zhao,M.,Wang,Y.,Yang,J.andZhang,J.,2017.Recognition
2(2),pp.53-83.
ofemotionsusingmultimodalphysiologicalsignalsandanensembledeep
[12] Siddharth,S.,Patel,A.,Jung,T.P.andSejnowski,T.,2018.AWearable
learning model. Computer methods and programs in biomedicine, 140,
Multi-modalBio-sensingSystemTowardsReal-worldApplications.IEEE
pp.93-110.
TransactionsonBiomedicalEngineering.
[13] Siddharth, Patel, A., Jung, T.P. and Sejnowski, T.J., 2017, July. An [35] Zheng, W.L., Zhu, J.Y. and Lu, B.L., 2017. Identifying stable patterns
AffordableBio-SensingandActivityTaggingPlatformforHCIResearch. over time for emotion recognition from EEG. IEEE Transactions on
In International Conference on Augmented Cognition (pp. 399-409). AffectiveComputing.
Springer,Cham. [36] Yoon,H.J.andChung,S.Y.,2013.EEG-basedemotionestimationusing
[14] Tian, Y.I., Kanade, T. and Cohn, J.F., 2001. Recognizing action units Bayesian weighted-log-posterior function and perceptron convergence
forfacialexpressionanalysis.IEEETransactionsonpatternanalysisand algorithm.Computersinbiologyandmedicine,43(12),pp.2230-2237.
machineintelligence,23(2),pp.97-115. [37] Wang, D. and Shang, Y., 2013. Modeling physiological data with
[15] Bailenson,J.N.,Pontikakis,E.D.,Mauss,I.B.,Gross,J.J.,Jabon,M.E., deepbeliefnetworks.Internationaljournalofinformationandeducation
Hutcherson,C.A.,Nass,C.andJohn,O.,2008.Real-timeclassification technology(IJIET),3(5),p.505.
of evoked emotions using facial feature tracking and physiological re- [38] Atkinson, J. and Campos, D., 2016. Improving BCI-based emotion
sponses.Internationaljournalofhuman-computerstudies,66(5),pp.303- recognition by combining EEG feature selection and kernel classifiers.
317. ExpertSystemswithApplications,47,pp.35-41.12
[39] Koelstra, S. and Patras, I., 2013. Fusion of facial expressions and [62] Asthana,A.,Zafeiriou,S.,Cheng,S.andPantic,M.,2014.Incremental
EEGforimplicitaffectivetagging.ImageandVisionComputing,31(2), face alignment in the wild. In Proceedings of the IEEE Conference on
pp.164-174. ComputerVisionandPatternRecognition(pp.1859-1866).
[40] Ferdinando,H.,Seppnen,T.andAlasaarela,E.,2016,October.Compar- [63] Parkhi, O.M., Vedaldi, A. and Zisserman, A., 2015, September. Deep
ingfeaturesfromECGpatternandHRVanalysisforemotionrecognition FaceRecognition.InBMVC(Vol.1,No.3,p.6).
system. In Computational Intelligence in Bioinformatics and Computa- [64] Hochreiter, S. and Schmidhuber, J., 1997. Long short-term memory.
tionalBiology(CIBCB),2016IEEEConferenceon(pp.1-6).IEEE. Neuralcomputation,9(8),pp.1735-1780.
[41] Zhu, Y., Wang, S. and Ji, Q., 2014, July. Emotion recognition from [65] Huang, G.B., Zhu, Q.Y. and Siew, C.K., 2006. Extreme learning ma-
users’ eeg signals with the help of stimulus videos. In Multimedia and chine:theoryandapplications.Neurocomputing,70(1-3),pp.489-501.
Expo(ICME),2014IEEEInternationalConferenceon(pp.1-6).IEEE. [66] Maaten, L.V.D. and Hinton, G., 2008. Visualizing data using t-SNE.
[42] Gonuguntla,V.,Shafiq,G.,Wang,Y.andVeluvolu,K.C.,2015,August. Journalofmachinelearningresearch,9(Nov),pp.2579-2605.
EEG classification of emotions using emotion-specific brain functional [67] Wang, W., Shen, J. and Shao, L., 2018. Video salient object detection
network.InEngineeringinMedicineandBiologySociety(EMBC),2015 viafullyconvolutionalnetworks.IEEETransactionsonImageProcessing,
37thAnnualInternationalConferenceoftheIEEE(pp.2896-2899).IEEE. 27(1),pp.38-49.
[43] Mullen, T., Kothe, C., Chi, Y.M., Ojeda, A., Kerth, T., Makeig, S., [68] Rolls,E.T.,Hornak,J.,Wade,D.andMcGrath,J.,1994.Emotion-related
Cauwenberghs,G.andJung,T.P.,2013,July.Real-timemodelingand3D learning in patients with social and emotional changes associated with
visualizationofsourcedynamicsandconnectivityusingwearableEEG. frontal lobe damage. Journal of Neurology, Neurosurgery & Psychiatry,
In201335thannualinternationalconferenceoftheIEEEengineeringin 57(12),pp.1518-1524.
medicineandbiologysociety(EMBC)(pp.2184-2187).IEEE. [69] Phan, K.L., Wager, T., Taylor, S.F. and Liberzon, I., 2002. Functional
[44] Delorme,A.andMakeig,S.,2004.EEGLAB:anopensourcetoolbox neuroanatomyofemotion:ameta-analysisofemotionactivationstudies
foranalysisofsingle-trialEEGdynamicsincludingindependentcompo- inPETandfMRI.Neuroimage,16(2),pp.331-348.
nentanalysis.Journalofneurosciencemethods,134(1),pp.9-21.
[45] Peng, H., Long, F. and Ding, C., 2005. Feature selection based on
mutualinformationcriteriaofmax-dependency,max-relevance,andmin-
redundancy. IEEE Transactions on pattern analysis and machine intelli-
gence,27(8),pp.1226-1238. Siddharth (S’14) received B.Tech. in Electronics
[46] Siddharth,Jung,T.P.andSejnowski,T.J.,2018.Multi-modalApproach and Communications Engineering at the Indian In-
forAffectiveComputing.InEngineeringinMedicineandBiologySociety stituteofInformationTechnology,Allahabad,India
(EMBC),2018IEEE40thAnnualInternationalConferenceoftheIEEE. in 2015 and M.S. in Intelligent Systems, Robotics,
[47] Bashivan,P.,Rish,I.,Yeasin,M.andCodella,N.,2015.Learningrepre- and Control at the University of California San
sentationsfromEEGwithdeeprecurrent-convolutionalneuralnetworks. Diego (UCSD), La Jolla, USA in 2017. Currently,
arXivpreprintarXiv:1511.06448. he is pursuing Ph.D. at UCSD where his research
[48] Hunter,J.D.,2007.Matplotlib:A2Dgraphicsenvironment.Computing interestsincludecomputationalneuroscience,multi-
inscience&engineering,9(3),pp.90-95. modalbio-sensing,andaffectivecomputing.Hehas
[49] Simonyan, K. and Zisserman, A., 2014. Very deep convolutional net- alsoundertakenresearchinternshipsatNationalUni-
worksforlarge-scaleimagerecognition.arXivpreprintarXiv:1409.1556. versity of Singapore, French National Center for
[50] Russakovsky,O.,Deng,J.,Su,H.,etal.ImageNetLargeScaleVisual ScientificResearch,SamsungResearchAmerica,andFacebookRealityLabs.
RecognitionChallenge.InternationalJournalofComputerVision(IJCV).
Vol115,Issue3,2015,pp.211252
[51] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson,
CNN Features off-the-shelf: an Astounding Baseline for Recognition,
arXiv:1403.6382v3,2014. Tzyy-PingJung(S’91-M’92-SM’06-F’15)received
[52] Siddharth,Rangesh,A.,Ohn-Bar,E.andTrivedi,M.M.,2016,Novem- the B.S. degree in electronics engineering from
ber.Driverhandlocalizationandgraspanalysis:Avision-basedreal-time National Chiao Tung University, Hsinchu, Taiwan,
approach.InIntelligentTransportationSystems(ITSC),2016IEEE19th in1984,andtheM.S.andPh.D.degreesinelectrical
InternationalConferenceon(pp.2545-2550).IEEE. engineeringfromTheOhioStateUniversity,Colum-
[53] Wold, S., Esbensen, K. and Geladi, P., 1987. Principal component bus, OH, USA, in 1989 and 1993, respectively. He
analysis.Chemometricsandintelligentlaboratorysystems,2(1-3),pp.37- is an Associate Director of the Swartz Center for
52. Computational Neuroscience, Institute for Neural
[54] Orini, M., Bailn, R., Enk, R., Koelsch, S., Mainardi, L. and Laguna, Computation, and an Adjunct Professor of Bio-
P., 2010. A method for continuously assessing the autonomic response engineering at UCSD. His research interests are in
tomusic-inducedemotionsthroughHRVanalysis.Medical&biological theareasofbiomedicalsignalprocessing,cognitive
engineering&computing,48(5),pp.423-433. neuroscience, machine learning, human EEG, functional neuroimaging, and
[55] Billauer, E., 2012. peakdet: Peak detection using MATLAB. Detect brain-computerinterfacesandinteractions.
PeaksinaVector,Billauer,E.,Haifa,Israel,accessedJuly,20,p.2012.
[56] Ewing,D.J.,Neilson,J.M.andTravis,P.A.U.L.,1984.Newmethodfor
assessing cardiac parasympathetic activity using 24 hour electrocardio-
grams.Heart,52(4),pp.396-402.
[57] Umetani,K.,Singer,D.H.,McCraty,R.andAtkinson,M.,1998.Twenty- Terrence J. Sejnowski (F’00) is the Francis Crick
four hour time domain heart rate variability and heart rate: relations to ProfessoratTheSalkInstituteforBiologicalStud-
age and gender over nine decades. Journal of the American College of ies, where he directs the Computational Neurobi-
Cardiology,31(3),pp.593-601. ology Laboratory, and a Professor of Biology and
[58] Lin,C.H.,2008.Frequency-domainfeaturesforECGbeatdiscrimination ComputerScienceandEngineeringattheUniversity
usinggreyrelationalanalysis-basedclassifier.Computers&Mathematics of California San Diego, where he is co-Director
withApplications,55(4),pp.680-690. of the Institute for Neural Computation. The long-
[59] Fulop, S.A. and Fitz, K., 2006. Algorithms for computing the time- rangegoalofDr.Sejnowski’sresearchistounder-
correctedinstantaneousfrequency(reassigned)spectrogram,withapplica- stand the computational resources of brains and to
tions.TheJournaloftheAcousticalSocietyofAmerica,119(1),pp.360- buildlinkingprinciplesfrombraintobehaviorusing
371. computationalmodels.Dr.Sejnowskihaspublished
[60] Mera, K. and Ichimura, T., 2004. Emotion analyzing method using over500scientificpapersand12books,includingTheComputationalBrain,
physiological state. In Knowledge-Based Intelligent Information and withPatriciaChurchland.HereceivedtheWrightPrizeforInterdisciplinary
EngineeringSystems(pp.195-201).SpringerBerlin/Heidelberg. research in 1996, the Hebb Prize from the International Neural Network
[61] Viola, P. and Jones, M., 2001. Rapid object detection using a boosted Society in 1999, and the IEEE Neural Network Pioneer Award in 2002. He
cascadeofsimplefeatures.InComputerVisionandPatternRecognition, isamemberoftheNationalAcademyofSciences,theNationalAcademyof
2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Engineering,theNationalAcademyofMedicineandtheNationalAcademy
Conferenceon(Vol.1,pp.I-I).IEEE. ofInventors."
229,231,Video and image based emotion recognition challenges in the wild: Emotiw 2015,"['A Dhall', 'OV Ramana Murthy', 'R Goecke']",2015,384,"Acted Facial Expressions In The Wild, Expression in-the-Wild, Static Facial Expression in the Wild","classification, classifier, deep learning, facial expression recognition, machine learning","For the VReco sub-challenge, the facial features should  emotion recognition method on  the Acted Facial Expressions in the Wild database 5.0 and single image based facial expression",No DOI,Proceedings of the …,https://dl.acm.org/doi/10.1145/2818346.2829994,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,acm.org,
